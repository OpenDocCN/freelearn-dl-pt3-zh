<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Influential Classification Tools</h1>
                </header>
            
            <article>
                
<p>After the deep learning breakthrough in 2012, research toward more refined classification systems based on <strong><span>convolutional neural networks</span></strong> (<strong>CNNs</strong>) gained momentum. Innovation is moving at a frantic pace nowadays, as more and more companies are developing smart products. Among the numerous solutions developed over the years for object classification, some have became famous for their contributions to computer vision. They have been derived and adapted for so many different applications that they have achieved must-know status, and so deserve their own chapter.</p>
<p>In parallel with the advanced network architectures introduced by these solutions, other methods have been explored to better prepare CNNs for their specific tasks. So, in the second part of this chapter, we will look at how the knowledge acquired by networks on specific use cases can be transferred to new applications for enhanced performance.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li><span>What instrumental architectures such as VGG, inception, and ResNet have brought to computer vision</span></li>
<li>How these solutions can be reimplemented or directly reused for classification tasks</li>
<li>What transfer learning is, and how to efficiently repurpose trained networks</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Jupyter notebooks illustrating the concepts presented in this chapter can be found in the GitHub folder at <a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter04">github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter04</a><span>.</span></p>
<p>The only new package introduced in this chapter is <kbd>tensorflow-hub</kbd>. Installation instructions can be found at <a href="https://www.tensorflow.org/hub/installation">https://www.tensorflow.org/hub/installation</a> (though it is a single-line command with <kbd>pip</kbd>: <kbd>pip install tensorflow-hub</kbd>).</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding advanced CNN architectures</h1>
                </header>
            
            <article>
                
<p>Research in computer vision has been moving forward both through incremental contributions and large innovative leaps. Challenges organized by researchers and companies, inviting experts to submit new solutions in order to best solve a predefined task, have been playing a key role in triggering such instrumental contributions. The ImageNet classification contest (<strong>ImageNet Large Scale Visual Recognition Challenge</strong> <em>(</em><strong>ILSVRC</strong>); see <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>) is a perfect example. With its millions of images split into 1,000 fine-grained classes, it still represents a great challenge for daring researchers, even after the significant and symbolic victory of AlexNet in 2012.</p>
<p>In this section, we will present some of the classic deep learning methods that followed AlexNet in tackling ILSVRC, covering the reasons leading to their development and the contributions they made.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">VGG – a standard CNN architecture</h1>
                </header>
            
            <article>
                
<p>The first network architecture we will present is <strong>VGG</strong> (or <em>VGGNet</em>), developed by the <em>Visual Geometry Group</em> from <span>Oxford</span> University. Though the group only achieved second place in the ILSVRC classification task in 2014, their method influenced many later architectures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of the VGG architecture</h1>
                </header>
            
            <article>
                
<p>Looking at the motivation of the VGG authors, and then their contributions, we will present how the VGG architecture achieved higher accuracy with fewer parameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation</h1>
                </header>
            
            <article>
                
<p>AlexNet was a game changer, being the first CNN successfully trained for such a complex recognition task and making several contributions that are still valid nowadays, such as the following:</p>
<ul>
<li>The use of a <strong><span>rectified linear unit</span></strong> (<em><strong>ReLU</strong></em>) as an activation function, which prevents the vanishing gradient problem (explained later in this chapter), and thus improving training (compared to using sigmoid or tanh)</li>
<li>The application of <strong>dropout</strong> to CNNs (with all the benefits covered in <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>)</li>
<li>The typical CNN architecture combining blocks of convolution and pooling layers, with dense layers afterward for the final prediction</li>
<li>The application of random transformations (image translation, horizontal flipping, and more) to synthetically augment the dataset (that is, augmenting the number of different training images by randomly editing the original samples—see <a href="337ec077-c215-4782-b56c-beae4d94d718.xhtml">Chapter 7</a>, <em>Training on Complex and Scarce Datasets,</em> for more details)</li>
</ul>
<p>Still, even back then, it was clear that this prototype architecture had room for improvement. The main motivation of many researchers was to try going deeper (that is, building a network composed of a larger number of stacked layers), despite the challenges arising from this. Indeed, more layers typically means more parameters to train, making the learning process more complex. As we will describe in the next paragraph, however, Karen Simonyan and Andrew Zisserman from Oxford's VGG group tackled this challenge with success. The method they submitted to ILSVRC 2014 reached a top-5 error of 7.3%, dividing the 16.4% error of AlexNet <span>by more than two</span>!</p>
<div class="packt_infobox"><strong>Top-5 accuracy</strong> is one of the main classification metrics of ILSVRC. It considers that a method has predicted properly if the correct class is among its five first guesses. Indeed, for many applications, it is fine to have a method that's able to reduce a large number of class candidates to a lower number (for instance, to leave the final choice between the remaining candidates to an expert user). The top-5 metrics are a specific case of the more generic top-k metrics.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture</h1>
                </header>
            
            <article>
                
<p>In their paper (<em>Very Deep Convolutional Networks for Large-Scale Image Recognition</em>, <em>ArXiv, 2014</em>), Simonyan and Zisserman presented how they developed their network to be deeper than most previous ones. They actually introduced six different CNN architectures, from 11 to 25 layers deep. Each network is composed of five blocks of several consecutive convolutions followed by a max-pooling layer and three final dense layers (with dropout for training). All the convolutional and max-pooling layers have <kbd>SAME</kbd> for padding. The convolutions have <em><span>s</span> = 1</em> <span>for stride, </span>and are using the <em>ReLU</em> function for activation. All in all, a typical VGG network is represented in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/444b8da7-80c0-47b2-a276-ff2fee8922f8.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.1: VGG-16 architecture</div>
<p>The two most performant architectures, still commonly used nowadays, are called <strong>VGG-16</strong> and <strong>VGG-19</strong>. The numbers (16 and 19) represent the <em>depth</em> of these CNN architectures; that is, the number of <em>trainable</em> layers stacked together. For example, as shown in <em>Figure 4.1</em>, VGG-16 contains 13 convolutional layers and 3 dense ones, hence a depth of 16 (excluding the non-trainable operations; that is, the 5 max-pooling and 2 dropout layers). The same goes for VGG-19, which is composed of three additional convolutions. VGG-16 has approximately 138 million parameters, and VGG-19 has 144 million. Those numbers are quite high, although, as we will demonstrate in the following section, the VGG researchers took a new approach to keep these values in check despite the depth of their architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contributions – standardizing CNN architectures</h1>
                </header>
            
            <article>
                
<p>In the following paragraphs, we will summarize the most significant contributions introduced by these researchers while further detailing their architecture.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Replacing large convolutions with multiple smaller ones</h1>
                </header>
            
            <article>
                
<p>The authors began with a simple observation—a stack of two convolutions with <em>3</em> × <em>3</em> kernels has the same receptive field as a convolution with <em>5</em> × <em>5</em> kernels (refer to <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a><em>, Modern Neural Networks</em>, for the<span> </span><strong>effective receptive field</strong><span> (</span><strong>ERF</strong>) formula).</p>
<p>Similarly, three consecutive <em>3</em> × <em>3</em> convolutions result in a <em>7</em> × <em>7</em> receptive field, and five <em>3</em> × <em>3</em> operations result in an <em>11</em> × <em>11</em> receptive field. Therefore, while AlexNet has large filters (up to <em>11</em> × <em>11</em>), the VGG network contains more numerous but smaller convolutions for a larger ERF. The benefits of this change are twofold:</p>
<ul>
<li><strong>It decreases the number of parameters</strong>: Indeed, the <em>N</em> filters of an <em>11</em> × <em>11</em> convolution layer imply <em>11</em> <span class="ui_qtext_rendered_qtext"><span class="texhtml">×</span></span> <em>11</em> × <em>D</em> × <em>N</em> = <em>121D</em><em>N</em> values to train just for their kernels (for an input of depth <em>D</em>), while five <em>3</em> × <em>3</em> convolutions have a total of <em>1</em> <span class="ui_qtext_rendered_qtext"><span class="texhtml">×</span></span> <em>(3</em> × <span class="ui_qtext_rendered_qtext"><span class="texhtml"><em>3</em> × <em>D</em> × <em>N</em><em>)</em> + <em>4</em></span></span> <span class="ui_qtext_rendered_qtext"><span class="texhtml">×</span></span> <em>(3</em> <span class="ui_qtext_rendered_qtext"><span class="texhtml">×</span></span> <span class="ui_qtext_rendered_qtext"><span class="ui_qtext_rendered_qtext"><span class="texhtml"><em>3</em> × <em>N</em> × <em>N</em><em>)</em> = <em>9</em><em>D</em><em>N</em> + <em>36</em><em>N<sup>2</sup></em> weights for their kernels. As long as <em>N</em> &lt; <em>3.6</em><em>D</em>, this means fewer parameters. For instance, for <em>N</em> = <em>2</em><em>D</em>, the number of parameters drops from <em>242</em><em>D<sup>2</sup></em> to <em>153</em><em>D<sup>2</sup></em> (refer to the previous equations). This makes the network easier to optimize, as well as much lighter (we invite you to look at the decrease for the replacements of the <em>7</em> × <em>7</em> and <em>5</em> × <em>5</em> convolutions).</span></span></span></li>
<li><strong>It increases the non-linearity</strong>: Having a larger number of convolution layers—each followed by a <em>non-linear</em> activation function such as <em>ReLU</em>—increases the networks' capacity to learn complex features (that is, by combining more non-linear operations).</li>
</ul>
<p class="mce-root"/>
<p>Overall, replacing larger convolutions with small, consecutive ones allowed the VGG authors to effectively go deeper.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Increasing the depth of the feature maps</h1>
                </header>
            
            <article>
                
<p>Based on another intuition, the VGG authors doubled the depth of the feature maps for each block of convolutions (from 64 after the first convolution to 512). As each set is followed by a max-pooling layer with a <em>2</em> <span class="ui_qtext_rendered_qtext"><span class="texhtml">× <em>2</em> window size and a stride of 2, the depth doubles while the spatial dimensions are halved.</span></span></p>
<p>This allows the encoding of spatial information into more and more complex and discriminative features for classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Augmenting data with scale jittering</h1>
                </header>
            
            <article>
                
<p>Simonyan and Zisserman also introduced a <strong>data augmentation</strong> mechanism that they named <strong>scale jittering</strong>. At each training iteration, they randomly scale the batched images (from 256 pixels to 512 pixels for their smaller side) before cropping them to the proper input size (<em>224</em> <span class="ui_qtext_rendered_qtext"><span class="texhtml">×</span></span> <em>224</em> for their ILSVRC submission). With this random transformation, the network will be confronted with samples with different scales and will learn to properly classify them despite this scale jittering (refer to <em>Figure 4.2</em>). The network becomes more robust as a result, as it is trained on images covering a larger range of realistic transformations.</p>
<div class="packt_infobox">Data augmentation is the procedure of synthetically increasing the size of training datasets by applying random transformations to their images in order to create different versions. Details and concrete examples are provided in <a href="337ec077-c215-4782-b56c-beae4d94d718.xhtml">Chapter 7</a>, <em>Training on Complex and Scarce Datasets</em>.</div>
<p>The authors also suggested applying random scaling and cropping at test time. The idea is to generate several versions of the query image this way and to feed them all to the network, with the intuition that it increases the chance of feeding content on a scale the network is particularly used to. The final prediction is obtained by averaging the results for each version.</p>
<p class="mce-root"/>
<p>In their paper, they demonstrate how this process tends to also improve accuracy:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8a127497-cc9f-4714-b826-45b6fd79ada8.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.2: Example of scale jittering. Notice that it is common to not preserve the aspect ratio of the content to further transform the images</div>
<div class="packt_tip packt_infobox"><span>The same principle was previously used by the AlexNet authors. During both training and testing, they were generating several versions of each image with different combinations of cropping and flipping transformations.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Replacing fully connected layers with convolutions</h1>
                </header>
            
            <article>
                
<p>While the classic VGG architecture ends with several <strong>fully connected</strong> (<strong>FC</strong>) layers (such as AlexNet), the authors suggest an alternative version. In this version, the dense layers are replaced by convolutional ones.</p>
<p>The first set of convolutions with larger kernels (<em>7</em> × <em>7</em> and <em>3</em> × <em>3</em>) reduces the spatial size of the feature maps to <em>1</em> × <em>1</em> (with no padding applied beforehand) and increases their depth to 4,096. Finally, a <em>1</em> × <em>1</em> convolution is used with as many filters as classes to predict from (that is, <em>N</em> = 1,000 for ImageNet). The resulting <em>1</em> × <em>1</em> × <em>N</em> vector is normalized with the <kbd>softmax</kbd> function, and then flattened into the final class predictions (with each value of the vector representing the predicted class probability).</p>
<div class="packt_infobox"><em>1</em> × <em>1</em> convolutions are commonly used to change the depth of the input volume without affecting its spatial structure. For each spatial position, the new values are interpolated from all the depth values at that position.</div>
<p>Such a network without any dense layers is called a <strong>fully convolutional network </strong>(<strong>FCN</strong>). As mentioned in <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>, and as has been highlighted by the VGG authors, FCNs can be applied to images of different sizes, with no need for cropping beforehand.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_infobox">Interestingly, to achieve the best accuracy for ILSVRC, the authors trained and used both versions (normal and FCN), once again averaging their results to obtain the final predictions. This technique is named <strong>model averaging</strong> and is frequently used in production.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementations in TensorFlow and Keras</h1>
                </header>
            
            <article>
                
<p>Thanks to the efforts that the authors put into creating a clear architecture, VGG-16 and VGG-19 are among the simplest classifiers to reimplement. Example code can be found in the GitHub folder for this chapter, for educational purposes. However, in computer vision, as in many domains, it is always preferable not to reinvent the wheel and to instead reuse existing tools that are available. The following paragraphs present different preimplemented VGG solutions that you can directly adapt and reuse.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The TensorFlow model</h1>
                </header>
            
            <article>
                
<p>While TensorFlow itself does not offer any official implementation of the VGG architectures, neatly implemented VGG-16 and VGG-19 networks are available in the <kbd>tensorflow/models</kbd> GitHub repository (<a href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a>). This repository, maintained by TensorFlow contributors, contains numerous well-curated state-of-the-art or experimental models. It is often recommended that you should search this repository when looking for a specific network.</p>
<p>We invite our readers to have a look at the VGG code there (currently available at <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py">https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py</a>), as it reimplements the FCN version we described earlier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Keras model</h1>
                </header>
            
            <article>
                
<p>The Keras API has an official implementation of these architectures, accessible via its <kbd>tf.keras.applications</kbd> package (refer to the documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications">https://www.tensorflow.org/api_docs/python/tf/keras/applications</a>). This package contains several other well-known models and provides <em>pre trained</em> parameters for each (that is, parameters saved from prior training on a specific dataset). For instance, you can instantiate a VGG network with the following command:</p>
<pre>vgg_net = tf.keras.applications.VGG16(<br/>    include_top=True, weights='imagenet', input_tensor=None, <br/>    input_shape=None, pooling=None, classes=1000)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>With these default arguments, Keras instantiates the VGG-16 network and loads the persisted parameter values obtained after a complete training cycle on ImageNet. With this single command, we have a network ready to classify images into the 1,000 ImageNet categories. If we would like to retrain the network from scratch instead, we should fix <kbd>weights=None</kbd> and Keras will randomly set the weights.</p>
<div class="packt_tip">In Keras terminology, the <em>top</em> layers correspond to the final consecutive dense layers. Therefore, if we set <kbd>include_top=False</kbd>, the VGG dense layers will be excluded, and the network's outputs will be the feature maps of the last convolution/max-pooling block. This can be useful if we want to reuse the pre trained VGG network to extract meaningful features (which can be applied to more advanced tasks), and not just for classification. The <kbd>pooling</kbd> function parameter can be used in those cases (that is, when <kbd>include_top=False</kbd>) to specify an optional operation to be applied to the feature maps before returning them (<kbd>pooling='avg'</kbd> or <kbd>pooling='max'</kbd> to apply a global average- or max- pooling).</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">GoogLeNet and the inception module</h1>
                </header>
            
            <article>
                
<p>Developed by researchers at Google, the architecture we will now present was also applied to ILSVRC 2014 and won first place for the classification task ahead of VGGNet. <strong>GoogLeNet</strong> (for <em>Google</em> and <em>LeNet</em>, as an homage to this pioneering network) is structurally very different from its linear challenger, introducing the notion of <em>inception blocks</em> (the network is also commonly called an <strong>inception network</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of the GoogLeNet architecture</h1>
                </header>
            
            <article>
                
<p>As we will see in the following section, the GoogLeNet authors, Christian Szegedy <span>and others</span>, approached the conception of a more efficient CNN from a very different angle than the VGG researchers (<em>Going Deeper with Convolutions</em>, Proceedings of the CVPR IEEE conference, 2014).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation</h1>
                </header>
            
            <article>
                
<p>While VGG's authors took AlexNet and worked on standardizing and optimizing its structure in order to obtain a clearer and deeper architecture, researchers at Google took a different approach. Their first consideration, as mentioned in the paper, was the optimization of the CNN computational footprint<span>.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Indeed, in spite of careful engineering (refer to VGG), the deeper CNNs are, the larger their number of trainable parameters and their number of computations per prediction become (it is costly with respect to memory and time). For instance, VGG-16 weighs approximately 93 MB (in terms of parameter storage), and the VGG submission for ILSVRC took two to three weeks to train on four GPUs. With approximately 5 million parameters, GoogLeNet is 12 times lighter than AlexNet and 21 times lighter than VGG-16, and the network was trained within a week. As a result, GoogLeNet—and more recent inception networks—can even run on more modest machines (such as smartphones), which contributed to their lasting popularity.</span></p>
<p>We have to keep in mind that, despite this impressive reduction in the numbers of parameters and operations, GoogLeNet did win the classification challenge in 2014 with a top-5 error of 6.7% (against 7.3% with VGG). This performance is the result of the second target of Szegedy <span>and others—</span>the conception of a network that was not only deeper but also larger, with blocks of parallel layers for <em>multiscale processing</em>. While we will detail this solution later in this chapter, the intuition behind it is simple. Building a CNN is a complex, iterative task. How do we know which layer (such as convolutional or pooling) should be added to the stack in order to improve the accuracy? How do we know which kernel size would work best for a given layer? After all, kernels of different sizes will not react to features of the same scale. How can we avoid such a trade-off? A solution, according to the authors, is to use the <em>inception modules</em> they developed, composed of several different layers working in parallel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture</h1>
                </header>
            
            <article>
                
<p>As shown in <em>Figure 4.3</em>, GoogLeNet architecture is not as straightforward as the previous architectures we studied, although it can be analyzed region by region. The input images are first processed by a classic series of convolutional and max-pooling layers. Then, the information goes through a stack of nine inception modules. These modules (often called <strong>subnetworks</strong>; further detailed in <em>Figure 4.4</em>), are blocks of layers stacked vertically and horizontally. For each module, the input feature maps are passed to four parallel sub-blocks composed of one or two different layers (convolutions with different kernel sizes and max-pooling).</p>
<p class="CDPAlignLeft CDPAlign">The results of these four parallel operations are then concatenated together along the depth dimension and into a single feature volume:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/994f1060-1599-4d5a-96e6-c61b06fc087e.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.3: GoogLeNet architecture. The inception modules are detailed in <em>Figure 4.4</em></div>
<p>In the preceding figure, all the convolutional and max-pooling layers have <kbd>SAME</kbd><span> for padding.</span> The convolutions have <em>s = 1</em> for stride if unspecified and are using the <em>ReLU</em> function for activation.</p>
<p>This network is composed of several layer blocks sharing a similar structure with parallel layers—the inception modules. For instance, the first inception module, represented in <em>Figure 4.3</em>, receives a feature volume of size <em><span>28</span></em> × <em><span>28</span></em> × <span><em>192 </em>for input. Its first parallel sub-block, composed of a single <em>1</em> × <em>1</em> convolution output (<em>N</em> = 64 and <em>s</em> = 1), thus generates a <em>28</em> × <em>28</em> × <em>64</em> tensor. Similarly, the second sub-module, composed of two convolutions, outputs a <em>28</em> × <em>28</em> × <em>128</em> tensor; and the two remaining ones output a <em>28</em> × <em>28</em> × <em>32</em> and a <em>28</em> × <em>28</em> × <em>32</em> feature volume, respectively. Therefore, by stacking these four results together along the last dimension, the first inception module outputs a <em>28</em> × <em>28</em> × <em>256</em> tensor, which is then passed to the second module, and so on. In the following diagram, the naive solution is represented on the left, and the module used in GoogLeNet (that is, the inception module v1) is shown on the right (note that in GoogLeNet, the number of filters <em>N</em> increases the deeper the module is):<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/79d097f6-6abf-4bc6-8e4f-48bf0a4c09c2.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 4.4: Inception modules: naive versus actual</span></div>
<p class="mce-root">The features of the last module are average, pooled from <em>7</em> <span>× <em>7</em> × <em>1,024</em></span> to <span><em>1</em> × <em>1</em> × <em>1,024</em>,</span> and are finally densely converted into the prediction vector. As shown in <em>Figure 4.3</em>, the network is further composed of two auxiliary branches, also leading to predictions. Their purpose will be detailed in the next section.</p>
<p>In total, GoogLeNet is a 22-layer deep architecture (counting the trainable layers only), with a total of more than 60 convolutional and FC layers. And yet, this much larger network has 12 times fewer parameters than AlexNet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contributions – popularizing larger blocks and bottlenecks</h1>
                </header>
            
            <article>
                
<p>The low number of parameters, as well as the network's performance, are the results of several concepts implemented by the GoogLeNet authors. We will cover the main ones in this section.</p>
<div class="packt_infobox">In this section, we will present only the key concepts differentiating the inception networks from the ones we introduced <span>previously</span>. Note that the GoogLeNet authors reapplied several other techniques that we have already covered, such as the prediction of multiple crops for each input image and the use of other image transformations during training.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capturing various details with inception modules</h1>
                </header>
            
            <article>
                
<p>Introduced by Min Lin and others in their influential <strong>Network in Network</strong> (<strong>NIN</strong>) paper in 2013, the idea of having a CNN composed of sub-network modules was adapted and fully exploited by the Google team. As previously mentioned and shown in <em>Figure 4.4</em>, the basic inception modules they developed are composed of four parallel layers—three convolutions with filters of size <em>1</em> × <em>1</em>, <em>3</em> × <em>3</em>, and <em>5</em> × <em>5</em>, respectively, and one max-pooling layer with stride <kbd>1</kbd>. The advantages of this parallel processing, with the results concatenated together after,<span> </span>are numerous.</p>
<p class="mce-root"/>
<p>As explained in the Motivation sub-section, this architecture allows for the multiscale processing of the data. The results of each inception module combine features of different scales, capturing a wider range of information. We do not have to choose which kernel size may be the best (such a choice would require several iterations of training and testing cycles), that is, the network learns by itself which convolutions to rely on more for each module.</p>
<p>Additionally, while we presented how vertically stacking layers with non-linear activation functions positively affects a network's performance, this is also true for horizontal combinations. The concatenation of features mapped from different layers further adds to the non-linearity of the CNN.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using 1 x 1 convolutions as bottlenecks</h1>
                </header>
            
            <article>
                
<p>Though not a contribution <em>per se</em>, Szegedy et al. made the following technique notorious by efficiently applying it to their network.</p>
<p>As previously mentioned in the <em>Replacing fully connected layers with convolutions</em> section, <em>1</em> × <em>1</em> convolutional layers (with a stride of 1) are often used to change the overall depth of input volumes without affecting their spatial structures. Such a layer with <em>N</em> filters would take an input of shape <em>H</em> × <em>W</em> × <em>D</em> and return an interpolated <em>H</em> × <em>W</em> × <em>N</em> tensor. For each pixel in the input image, its <em>D</em> channel values will be interpolated by the layer (according to its filter weights) into <em>N</em> channel values.</p>
<p>This property can be applied to reduce the number of parameters required for larger convolutions by compressing the features' depth beforehand (using <em>N</em> &lt; <em>D</em>). This technique basically uses <em>1</em> × <em>1</em> convolutions as <strong>bottlenecks</strong> (that is, as intermediary layers reducing the dimensionality and, thus, the number of parameters). Since activations in neural networks are often redundant or left unused, such bottlenecks usually barely affect the performance (as long as they do not drastically reduce the depth). Moreover, GoogLeNet has its parallel layers to compensate for the depth reduction. Indeed, in inception networks, bottlenecks are present in every module, before all larger convolutions and after max-pooling operations, as illustrated in <em>Figure 4.4</em>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Given the <em>5</em> <span>× <em>5</em> convolution in the first inception module (taking as input a <em>28</em> × <em>28</em> × <em>192</em> volume) for example, the tensor containing its filters would be of the dimension <em>5</em> × <em>5</em> × <em>192</em> × <em>32</em> in the naive version. This represents 153,600 parameters just for this convolution. In the first version of the inception module (that is, with bottlenecks), a <em>1</em> × <em>1</em> convolution is introduced before the 5 × 5 one, with <em>N</em> = 16. As a result, the two convolutions require a total of <span class="ui_qtext_rendered_qtext"><em>1</em> <span class="texhtml">×</span> <em>1</em> × <em>192</em> × <span class="texhtml"><em>16</em> + <em>5</em> × <em>5</em> × <em>16</em> × <em>32</em> = <em>15,872</em> trainable values for their kernels. This is 10 times fewer parameters than the previous version (just for this single <em>5</em> × <em>5</em> layer), for the same output size!</span></span></span> Furthermore, as mentioned already, the addition of layers with a non-linear activation function (<em>ReLU</em>) further improves the networks' ability to grasp complex concepts.</p>
<div class="packt_infobox">We are presenting GoogLeNet as submitted to ILSVRC 2014<span> in this chapter</span>. More commonly named <strong>Inception V1</strong>, this architecture has been refined by its authors since then. <strong>Inception V2</strong> and <strong>Inception V3</strong> contain several improvements, such as replacing the <em><span>5 × 5</span></em> and <em>7 <span>× 7</span></em> convolutions by smaller ones (as done in VGG), improving the bottlenecks' hyperparameters to reduce the information loss, and adding <em>BatchNorm</em> layers.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pooling instead of fully connecting</h1>
                </header>
            
            <article>
                
<p>Another solution used by the inception authors to reduce the number of parameters was to use an average-pooling layer instead of a fully connected one after the last convolutional block. With a <span><em>7</em> × <em>7</em> window size and stride of 1, this layer reduces the feature volume from <em>7</em> × <em>7</em> × <em>1,024</em> to <em>1</em> × <em>1</em> × <em>1,024</em> without any parameter to train. A dense layer would have added (<em>7</em> × <span class="ui_qtext_rendered_qtext"><em>7</em> × <em>1,024</em>) × <span class="texhtml">1,024 = <span class="cwcot gsrt"><em>51,380,224</em> parameters. Though the network loses a bit in expressiveness with this replacement, the computational gain is enormous (and the network already contains enough non-linear operations to capture the information it needs for the final prediction).</span></span></span></span></p>
<div class="packt_infobox">The last and only FC layer in GoogLeNet has <em>1,024</em> × <span><span class="ui_qtext_rendered_qtext"><span class="texhtml"><em>1,000</em> = <em>1,024,000</em> parameters, a fifth of the total number the network has!</span></span></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fighting vanishing gradient with intermediary losses</h1>
                </header>
            
            <article>
                
<p>As briefly mentioned when introducing the architecture, GoogLeNet has two auxiliary branches at training time (removed after), also leading to predictions.</p>
<p class="mce-root"/>
<p>Their purpose is to improve the propagation of the loss through the network during training. Indeed, deeper CNNs are often plagued with <strong>vanishing gradient</strong>. Many CNN operations (for instance, <em>sigmoid</em>) have derivatives with small amplitudes (below one). Therefore, the higher the number of layers, the smaller the product of the derivatives becomes when backpropagating (as more values below one are multiplied together, the closer to zero the result will become). Often, the gradient simply vanishes/shrinks to zero when reaching the first layers. Since the gradient values are directly used to update the parameters, these layers won't effectively learn if the gradient is too small.</p>
<div class="packt_infobox">The opposite phenomenon—<span>the</span> <strong>exploding gradient</strong> <span>problem—can also happen with deeper networks. When operations whose derivatives can take on larger magnitudes are used, their product during backpropagation can become so big that it makes the training unstable (with huge, erratic weight updates) or it can even sometimes overflow (</span><kbd>NaN</kbd> <span>values).</span></div>
<p>The down-to-earth, yet effective, solution to this problem implemented here is to reduce the distance between the first layers and predictions, by introducing additional classification losses at various network depths. If the gradient from the final loss cannot flow properly to the first layers, these will still be trained to help with classification thanks to the closer intermediary losses. Incidentally, this solution also slightly improves the robustness of the layers affected by multiple losses, as they must learn to extract discriminative features that are not only useful to the main network, but also to the shorter branches.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementations in TensorFlow and Keras</h1>
                </header>
            
            <article>
                
<p>While the inception architecture may look complex to implement at first glance, we already have most of the tools to do so. Moreover, several pretrained versions are also made available by TensorFlow and Keras.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Inception module with the Keras Functional API</h1>
                </header>
            
            <article>
                
<p>The networks we have implemented so far were purely sequential, with a single path from inputs to predictions. The inception model differs from those, with its multiple parallel layers and branches. This gives us the opportunity to demonstrate that such operational graphs are not much more difficult to instantiate with the available APIs. In the following section, we will write an inception module using the Keras Functional API (refer to the documentation at <a href="https://keras.io/getting-started/sequential-model-guide/">https://keras.io/getting-started/sequential-model-guide/</a>).</p>
<p class="mce-root"/>
<p>So far, we have mostly been using the Keras Sequential API, which is not well-adapted for multipath architectures (as its name implies). The Keras Functional API is closer to the TensorFlow paradigm, with Python variables for the layers being passed as parameters to the next ones to build a graph. The following code presents a simplistic model implemented with both APIs:</p>
<pre>from keras.models import Sequential, Model<br/>from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input<br/><br/># Sequential version:<br/>model = Sequential()<br/>model.add(Conv2D(32, kernel_size=(5, 5), input_shape=input_shape))<br/>model.add(MaxPooling2D(pool_size=(2, 2)))<br/>model.add(Flatten())<br/>model.add(Dense(10, activation='softmax'))<br/><br/># Functional version:<br/>inputs = Input(shape=input_shape)<br/>conv1 = Conv2D(32, kernel_size=(5, 5))(inputs)<br/>maxpool1 = MaxPooling2D(pool_size=(2, 2))(conv1)<br/>predictions = Dense(10, activation='softmax')(Flatten()(maxpool1))<br/>model = Model(inputs=inputs, outputs=predictions)</pre>
<p class="mce-root">With the functional API, a layer can easily be passed to multiple others, which is what we need for the parallel blocks of the inception modules. Their results can then be merged together using a <kbd>concatenate</kbd> layer (refer to the documentation at <a href="https://keras.io/layers/merge/#concatenate_1">https://keras.io/layers/merge/#concatenate_1</a>). Therefore, the naive inception block presented in <em>Figure 4.4</em> can be implemented as follows:</p>
<pre>from keras.layers import Conv2D, MaxPooling2D, concatenate<br/><br/>def naive_inception_block(previous_layer, filters=[64, 128, 32]):<br/>    conv1x1 = Conv2D(filters[0], kernel_size=(1, 1), padding='same', <br/>                     activation='relu')(previous_layer)<br/>    conv3x3 = Conv2D(filters[1], kernel_size=(3, 3), padding='same',<br/>                     activation='relu')(previous_layer)<br/>    conv5x5 = Conv2D(filters[2], kernel_size=(5, 5), padding='same', <br/>                     activation='relu')(previous_layer)<br/>    max_pool = MaxPooling2D((3, 3), strides=(1, 1), <br/>                            padding='same')(previous_layer)<br/>    return concatenate([conv1x1, conv3x3, conv5x5, max_pool], axis=-1)</pre>
<p>We will leave it to you to adapt this code to implement the proper modules for Inception V1 by adding the bottleneck layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow model and TensorFlow Hub</h1>
                </header>
            
            <article>
                
<p>Google offers several scripts and tutorials explaining how to directly use its inception networks, or how to retrain them for new applications. The directory dedicated to this architecture in the <kbd>tensorflow/models</kbd> Git repository (<a href="https://github.com/tensorflow/models/tree/master/research/inception">https://github.com/tensorflow/models/tree/master/research/inception</a>) is also rich and well-documented. Moreover, a pretrained version of Inception V3 is available on <strong>TensorFlow Hub</strong>, which gives us the opportunity to introduce this platform.</p>
<p>TensorFlow Hub is a repository of pretrained models. In a similar way to how Docker allows people to easily share and reuse software packages, removing the need to reconfigure distributions, TensorFlow Hub gives access to pretrained models so that people do not have to spend time and resources reimplementing and retraining. It combines a website (<a href="https://tfhub.dev">https://tfhub.dev</a>) where people can search for specific models (depending, for example, on the target recognition task), and a Python package to easily download and start using these models. For instance, we can fetch and set up an Inception V3 network as follows:</p>
<pre>import tensorflow as tf<br/>import tensorflow_hub as hub<br/><br/>url = "https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2"<br/>hub_feature_extractor = hub.KerasLayer( # TF-Hub model as Layer<br/>    <span>url</span><span>, # URL of the TF-Hub model (here, an InceptionV3 extractor)<br/></span><span>    trainable</span>=<span>False, </span><span># Flag to set the layers as trainable or not<br/></span><span>    </span><span>input_shape</span>=(299, 299, 3)<span>, </span><span># Expected input shape (found on tfhub.dev)<br/></span><span>    </span><span>output_shape</span>=(2048,)<span>, </span><span># Output shape (same, found on the model's page)<br/></span><span>    </span><span>dtype</span>=tf.float32) <span># Expected dtype<br/></span><br/>inception_model = Sequential(<br/>    [hub_feature_extractor<span>, </span>Dense(num_classes<span>, </span><span>activation</span>=<span>'softmax'</span>)]<span>, <br/>    </span><span>name</span>=<span>"inception_tf_hub"</span>)</pre>
<p class="mce-root">Though this code is quite succinct, a lot is happening. A preliminary step was to browse the <a href="https://tfhub.dev">tfhub.dev</a> website and decide on a model there. On the page presenting the selected model (<a href="https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2">https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2</a>; stored in <kbd>model_url</kbd>), we can read that the inception model we chose is defined as an <strong>image feature vector</strong> that expects <em>299 </em><span>×</span> <em>299 </em><span>×</span> <em>3</em> inputs, among other details. To use a TensorFlow Hub model, we need to know how to interface with it.</p>
<p class="mce-root">The <em>image feature vector</em> type tells us that this network returns extracted features; that is, the results of the last convolutional block before the dense operations. With such a model, it is up to us to add the final layers (for instance, so that the output size corresponds to the number of considered classes).</p>
<p>The latest versions of the TensorFlow Hub interface seamlessly with Keras, and a complete pretrained TensorFlow Hub model can be fetched and instantiated as a Keras layer thanks to <kbd>tensorflow_hub.KerasLayer(model_url, trainable, ...)</kbd>. Like any Keras layer, it can then be used inside larger Keras models or TensorFlow estimators.</p>
<p class="mce-root">Though this may not seem as straightforward as using the Keras Applications API, TensorFlow Hub has an exotic catalog of models, which is destined to increase over time.</p>
<div class="packt_tip">One of the Jupyter notebooks available in the Git repository is dedicated to TensorFlow Hub and its usage.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Keras model</h1>
                </header>
            
            <article>
                
<p>As with VGG, Keras provides an implementation of Inception V3, optionally, with weights pretrained on ImageNet. <kbd>tf.keras.applications.InceptionV3()</kbd> (refer to the documentation at <a href="https://keras.io/applications/#inceptionv3">https://keras.io/applications/#inceptionv3</a>) has the same signature as the one presented for VGG.</p>
<p class="mce-root"/>
<div class="packt_infobox">We have mentioned AlexNet, the winning solution of ILSVRC 2012, as well as VGGNet and GoogLeNet, which prevailed during the 2014 edition. You might be wondering who won in 2013. The challenge that year was dominated by the <strong>ZFNet</strong> architecture (named after its creators, Matthew Zeiler and Rob Fergus from New York University). If ZFNet is not covered in this chapter, it is because its architecture was not particularly innovative, and has not really been reused afterward.<br/>
<br/>
However, Zeiler and Fergus' significant contribution lay somewhere else—they developed and applied several operations to the visualization of CNNs (such as <strong>unpooling</strong> and <strong>transposed convolution</strong>, also known as <strong>deconvolution</strong>, which are both detailed in <a href="c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml">Chapter 6</a>, <em>Enhancing and Segmenting Images</em>). Indeed, a common criticism of neural networks was that they behave like <em>black boxes</em>, and that no one can really grasp why and how they work so well. Zeiler and Fergus' work was an important first step toward opening up CNNs to reveal their inner processes (such as how they end up reacting to particular features and how they learn more abstract concepts as they go deeper.) Visualizing how each layer of their network reacted to specific images and contributed to the final prediction, the authors were able to optimize its hyperparameters and thus improve its performance (<em>Visualizing and Understanding Convolutional Networks</em>, Springer, 2014).<br/>
<br/>
Research toward understanding neural networks is still ongoing (for instance, with a multitude of recent work capturing and analyzing the <em>attention</em> of networks toward specific elements) and has already greatly helped to improve current systems.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ResNet – the residual network</h1>
                </header>
            
            <article>
                
<p>The last architecture we will address in this chapter won the 2015 edition of ILSVRC. Composed of a new kind of module, the residual module, <strong>ResNet</strong> (<strong>residual network</strong>) provides an efficient approach to creating very deep networks, beating larger models such as Inception in terms of performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of the ResNet architecture</h1>
                </header>
            
            <article>
                
<p>Developed by Kaiming He et al., researchers at Microsoft, the ResNet architecture is an interesting solution to learning problems affecting CNNs. Following the structure of previous sections, we will first clarify the author's targets and introduce their novel architecture (refer to <em>Deep Residual Learning for Image Recognition</em>, Proceedings of the CVPR IEEE conference, 2016).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation</h1>
                </header>
            
            <article>
                
<p>Inception networks demonstrated that going larger is a valid strategy in image classification, as well as other recognition tasks. Nevertheless, experts still kept trying to increase networks in order to solve more and more complex tasks. However, the question <em>Is learning better networks as easy as stacking more layers?</em>, asked in the preamble of the paper written by He et al.<span>,</span> is justified.</p>
<p>We know already that the deeper a network goes, the harder it becomes to train it. But besides the <em>vanishing/exploding gradient</em> problems (covered by other solutions already), He <span>et al.</span> pointed out another problem that deeper CNNs face—<em>performance degradation</em>. It all started with a simple observation—the accuracy of CNNs does not linearly increase with the addition of new layers. A degradation problem appears as the networks' depth increases. Accuracy starts saturating and even degrading. Even the training loss starts decreasing when negligently stacking too many layers, proving that the problem is not caused by overfitting. For instance, the authors compared the accuracy of an 18-layer-deep CNN with a 34-layer one, showing that the latter performs worse than the shallower version during and after training. In their paper, He <span>et al.</span> proposed a solution to build very deep and performant networks.</p>
<div class="packt_infobox">With <em>model averaging</em> (applying ResNet models of various depths) and <em>prediction averaging</em> (over multiple crops of each input image), the ResNet authors reached a historically low 3.6% top-5 error rate for the ILSVRC challenge. This was the first time an algorithm beat humans on that dataset<span>. Human performance had been measured by the challenge organizers, with the best human candidate reaching a 5.1% error rate (refer to </span><em>ImageNet Large-Scale Visual Recognition Challenge</em><span>, Springer, 2015). Achieving</span> super-human performance <span>on such a task was a huge milestone for deep learning. We should, however, keep in mind that, while algorithms can expertly solve a specific task, they still do not have the human ability to extend that knowledge to others, or to grasp the context of the data they are to deal with.</span></div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Architecture</h1>
                </header>
            
            <article>
                
<p>Like Inception, ResNet has known several iterative improvements to its architecture, for instance, with the addition of bottleneck convolutions or the use of smaller kernels. Like VGG, ResNet also has several pseudo-standardized versions characterized by their depth: ResNet-18, ResNet-50, ResNet-101, ResNet-152, and others. Indeed, the winning ResNet network for ILSVRC 2015 vertically stacked 152 trainable layers (with a total of 60 million parameters), which was an impressive feat at that time:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1cdf0cd3-2fb2-4866-bd2c-76b2b0aa52bf.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.5: Exemplary ResNet architecture</div>
<p>In the preceding diagram, all the convolutional and max-pooling layers have <kbd>SAME</kbd> for padding, and for stride <em>s = 1</em> if unspecified. Batch normalization is applied after each <em>3</em> × <em>3</em> convolution (on the residual path, in gray), and <em>1</em> × <em>1</em> convolutions (on the mapping path in black) have no activation function (identity).</p>
<p>As we can see in <em>Figure 4.5</em>, the ResNet architecture is slimmer than the Inception architecture, though it is similarly composed of layer blocks with parallel operations. Unlike Inception, where each parallel layer non-linearly processes the input information, ResNet blocks are composed of one non-linear path, and one identity path. The former (represented by the thinner gray arrows in <em>Figure 4.5</em>) applies a couple of convolutions with batch normalization and <em>ReLU</em> activation to the input feature maps. The latter (represented by the thicker black arrows) simply forward the features without applying any transformation.</p>
<div class="packt_infobox">The last statement is not always true. As shown in <em>Figure 4.5</em>, <em>1</em> <span>× <em>1</em> convolution</span>s are applied in order to adapt the depth of the features, when the depth is increased in parallel by the non-linear branches. On those occasions, to avoid a large increase in the number of parameters, the spatial dimensionality is also reduced <span>on both sides</span> using a stride of <span><em><span class="ui_qtext_rendered_qtext">s</span></em> = 2.</span></div>
<p class="mce-root"/>
<p>As in inception modules, the feature maps from each branch (that is, the transformed features and the original ones) are merged together before being passed to the next block. Unlike inception modules, however, this merging is not performed through depth concatenation, but through element-wise addition (a simple operation that does not require any additional parameters). We will cover, in the following section, the benefits of these residual blocks.</p>
<div class="packt_tip">Note that, in most implementations, the last <em>3</em> <span>× <em>3</em> convolution</span> of each residual block is not followed directly by <em>ReLU</em> activation. Instead, the non-linear function is applied after merging with the identity branch is done.</div>
<p>Finally, the features from the last block are average-pooled and densely converted into predictions, as in GoogLeNet.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Contributions – forwarding the information more deeply</h1>
                </header>
            
            <article>
                
<p>Residual blocks have been a significant contribution to machine learning and computer vision. In the following section, we will cover the reasons for this.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Estimating a residual function instead of a mapping</h1>
                </header>
            
            <article>
                
<p><span>As the ResNet authors pointed out, the degradation phenomenon would not happen if layers could easily learn <strong>identity mapping</strong> (that is, if a set of layers could learn weights so that their series of operations finally return the same tensors as the input layers).</span></p>
<p>Indeed, the authors argue that, when adding some layers on top of a CNN, we should at least obtain the same training/validation errors if these additional layers were able to converge to the identity function. They would learn to at least pass the result of the original network without degrading it. Since that is not the case—as we can often observe a degradation—it means that identity mapping is not easy to learn for CNN layers.</p>
<p>This led to the idea of introducing residual blocks, with two paths:</p>
<ul>
<li>One path further processes the data with some additional convolutional layers</li>
<li>One path performs the identity mapping (that is, forwarding the data with no changes)</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>We may intuitively grasp how this can solve the degradation problem. When adding a residual block on top of a CNN, its original performance can at least be preserved by setting the weights of the processing branch to zero, leaving only the predefined identity mapping. The processing path will only be considered if it benefits loss minimization.</p>
<p>The data forwarding path is usually called <strong>skip</strong> or <strong>shortcut</strong>. The processing one is commonly called <strong>residual</strong> <strong>path</strong>, since the output of its operations is then added to the original input, with the magnitude of the processed tensor being much smaller than the input one when the identity mapping is close to optimal (hence, the term <em>residual</em>). Overall, this residual path only introduces small changes to the input data, making it possible to forward patterns to deeper layers.</p>
<p>In their paper, He <span>et al.</span> demonstrate that their architecture not only tackles the degradation problem, but their ResNet models achieve better accuracy than traditional ones for the same number of layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Going ultra-deep</h1>
                </header>
            
            <article>
                
<p>It is also worth noting that residual blocks do not contain more parameters than traditional ones, as the skip and addition operations do not require any. They can, therefore, be efficiently used as building blocks for <em>ultra-deep</em> networks.</p>
<p>Besides the 152-layer network applied to the ImageNet challenge, the authors illustrated their contributions by training an impressive 1,202-layer one. They reported no difficulty training such a massive CNN (although its validation accuracy was slightly lower than for the 152-layer network, allegedly because of overfitting).</p>
<p>More recent works have been exploring the use of residual computations to build deeper and more efficient networks, <span>such as</span> <strong>Highway</strong> networks (with a trainable switch value to decide which path should be used for each residual block) or <strong>DenseNet</strong> models (adding further skip connections between blocks).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementations in TensorFlow and Keras</h1>
                </header>
            
            <article>
                
<p>As with previous architectures, we already have the tools needed to reimplement ResNet ourselves, while also having direct access to preimplemented/pretrained versions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Residual blocks with the Keras Functional API</h1>
                </header>
            
            <article>
                
<p>As practice, let's implement a basic residual block ourselves. As shown in <em>Figure 4.5</em>, the residual path consists of two convolutional layers, each one followed by batch normalization. The <em>ReLU</em> activation function is applied directly after the first convolution. For the second, the function is only applied after merging with the other path. Using the Keras Functional API, the residual path can thus be implemented in a matter of five or six lines, as demonstrated in the following code.</p>
<p>The shortcut path is even simpler. It contains either no layer at all, or a single <em>1</em> × <em>1</em> convolution to reshape the input tensor when the residual path is altering its dimensions (for instance, when a larger stride is used).</p>
<p>Finally, the results of the two paths are added together, and the <em>ReLU</em> function is applied to the sum. All in all, a basic residual block can be implemented as follows:</p>
<pre><span>from tf.keras.layers import Activation, Conv2D, BatchNormalization, add<br/><br/></span>def residual_block_basic(x, filters, kernel_size=3, strides=1):<br/>    # Residual Path:<br/>    conv_1 = Conv2D(filters=filters, kernel_size=kernel_size, <br/>                    padding='same', strides=strides)(x)<br/>    bn_1 = BatchNormalization(axis=-1)(conv_1)<br/>    act_1 = Activation('relu')(bn_1)<br/>    conv_2 = Conv2D(filters=filters, kernel_size=kernel_size, <br/>                    padding='same', strides=strides)(act_1)<br/>    residual = BatchNormalization(axis=-1)(conv_2)<br/>    # Shortcut Path:<br/>    shortcut = x if strides == 1 else Conv2D(<br/>        filters, kernel_size=1, padding='valid', strides=strides)(x)<br/>    # Merge and return :<br/>    return Activation('relu')(add([shortcut, residual]))</pre>
<div class="packt_tip">A more elegant function is presented in one of the Jupyter notebooks. This notebook also contains a complete implementation of the ResNet architecture and a brief demonstration of a classification problem.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The TensorFlow model and TensorFlow Hub</h1>
                </header>
            
            <article>
                
<p>Like the Inception networks, ResNet ones have their own official implementation provided in the <kbd>tensorflow/models</kbd> Git repository, as well as their own pretrained TensorFlow Hub modules.</p>
<div class="packt_tip">We invite you to check out the official <kbd>tensorflow/models</kbd> implementation, as it offers several types of residual blocks from more recent research efforts.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The Keras model</h1>
                </header>
            
            <article>
                
<p>Finally, Keras once again provides its own ResNet implementations—for instance, <kbd>tf.keras.applications.ResNet50()</kbd> (refer to the documentation at <a href="https://keras.io/applications/#resnet50">https://keras.io/applications/#resnet50</a>)—with the option to load parameters pretrained on ImageNet. These methods have the same signature as previously covered Keras applications.</p>
<div class="mce-root packt_tip">The complete code for the usage of this Keras application is also provided in the Git repository.</div>
<p>The list of CNN architectures presented in this chapter does not pretend to be exhaustive. It has been curated to cover solutions both instrumental to the computer vision domain and of pedagogical value.</p>
<p>As research in visual recognition keeps moving forward at a fast pace, more advanced architectures are being proposed, building upon previous solutions (as Highway and DenseNet methods do for ResNet, for instance), merging them (as with the Inception-ResNet solution), or optimizing them for particular use cases (such as the lighter MobileNet, which was made to run on smartphones). It is, therefore, always a good idea to check what the state of the art has to offer (for example, on official repositories or research journals) before trying to reinvent the wheel.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Leveraging transfer learning</h1>
                </header>
            
            <article>
                
<p>This idea of reusing knowledge provided by others is not only important in computer science. The development of human technology over the millennia is the result of our ability to transfer knowledge from one generation to another, and from one domain to another. Many researchers believe that applying this guidance to machine learning could be one of the keys to developing more proficient systems that will be able to solve new tasks without having to relearn everything from scratch.</p>
<p>Therefore, this section will present what <strong>transfer learning</strong> means for artificial neural networks, and how it can be applied to our models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview</h1>
                </header>
            
            <article>
                
<p>We will first introduce what transfer learning is<span> and how it is performed in deep learning, depending on the use cases.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Definition</h1>
                </header>
            
            <article>
                
<p>In the first part of this chapter, we presented several well-known CNNs, developed for the ImageNet classification challenge. We mentioned that these models are commonly repurposed for a broader range of applications. In the following pages, we will finally elaborate on the reasons behind this reconditioning and how it is performed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Human inspiration</h1>
                </header>
            
            <article>
                
<p>Like many developments in machine learning, transfer learning is inspired by our own human way of tackling complex tasks and gathering knowledge.</p>
<p>As mentioned in the introduction of this section, the first inspiration is our ability as a species to transfer knowledge from one individual to another. Experts can efficiently transfer the precious knowledge they have gathered over the years to a large number of students through oral or written teaching. By harnessing the knowledge that has been accumulated and distilled generation after generation, human civilizations have been able to continuously refine and extend their technical abilities. Phenomena that took millennia for our ancestors to understand— such as <span>human biology, the solar system, and more—became common knowledge.</span></p>
<p>Furthermore, as individuals, we also have the ability to transfer some expertise from one task to another. For example, people mastering one foreign language have an easier time learning similar ones. Similarly, people who have been driving a car for some time already have knowledge of the rules of the road and some related reflexes, which are useful if they want to learn how to drive other vehicles.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>These abilities to master complex tasks by building upon available knowledge, and to repurpose acquired skills to similar activities, are central to human intelligence. Researchers in machine learning dream of reproducing them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation</h1>
                </header>
            
            <article>
                
<p>Unlike humans, most machine learning systems have been designed, so far, for single, specific tasks. Directly applying a trained model to a different dataset would yield poor results, especially if the data samples do not share the same semantic content (for instance, MNIST digit images versus ImageNet photographs) or the same image quality/distribution (for instance, a dataset of smartphone pictures versus a dataset of high-quality pictures). As CNNs are trained to extract and interpret specific features, their performance will be compromised if the feature distribution changes. Therefore, some transformations are necessary to apply networks to new tasks.</p>
<p>Solutions have been investigated for decades. In 1998, Sebastian Thrun and Lorien Pratt edited <em>Learning to Learn</em>, a book compiling the prevalent research stands on the topic. More recently, in their <em>Deep Learning</em> book (<a href="http://www.deeplearningbook.org/contents/representation.html">http://www.deeplearningbook.org/contents/representation.html</a> on page 534, MIT Press), Ian Goodfellow, Yoshua Bengio, and Aaron Courville defined transfer learning as follows:</p>
<div class="t m2 x4e h3 yd9 ff3 fs2 fc0 sc0 ls0 ws0 packt_quote">[...] the situation where what has been learned in one setting (for example, distribution p<sub>1</sub>) is exploited to improve generalization in another setting (say, distribution p<sub>2</sub>).</div>
<p>It makes sense for researchers to suppose that, for example, some of the features a CNN is extracting to classify hand-written digits could be partially reused for the classification of hand-written texts. Similarly, a network that learned to detect human faces could be partially repurposed for the evaluation of facial expressions. Indeed, even though the inputs (full images for face detection versus cropped ones for the new task) and outputs (detection results versus classification values) are different, some of the network's layers are already trained to extract facial features, which is useful for both tasks.</p>
<p class="mce-root"/>
<div class="packt_infobox">In machine learning, a <strong>task</strong> is defined by the inputs <span>provided </span>(for example, pictures from smartphones) and the expected outputs (for example, prediction results for a specific set of classes). For instance, classification and detection on ImageNet are two different tasks with the same input images but different outputs.<br/>
<br/>
In some cases, algorithms can target similar tasks (for example, pedestrian detection) but have access to different sets of data (for example, CCTV images from different locations, or from cameras of different quality). These methods are thus trained on different <strong>domains</strong> (that is, data distributions).<br/>
<br/>
It is the goal of transfer learning to apply the knowledge either from one task to another or from one domain to another. The latter type of transfer learning is called <strong>domain adaptation</strong> and will be more specifically covered in <a href="337ec077-c215-4782-b56c-beae4d94d718.xhtml">Chapter 7</a>, <em>Training on Complex and Scarce Datasets</em>.</div>
<p>Transfer learning is especially interesting when not enough data is available to properly learn the new task (that is, there are not enough image samples to estimate the distribution). Indeed, deep learning methods are data hungry; they require large datasets for their training. Such datasets—especially labeled ones for supervised learning—are often tedious, if not impossible, to gather. For example, experts building recognition systems to automate industries cannot go to every plant to take hundreds of pictures of every new manufactured product and its components. They often have to deal with much smaller datasets, which are not large enough for the CNNs to satisfactorily converge. Such limitations explain the efforts to reuse knowledge acquired on well-documented visual tasks for those other cases.</p>
<div class="packt_tip">With their millions of annotated images from a large number of categories, ImageNet—and, more recently, COCO—are particularly rich datasets. It is assumed that CNNs trained on those have acquired quite an expertise in visual recognition, hence the availability in Keras and TensorFlow Hub of standard models (Inception, ResNet-50, and others) already trained on these datasets. People looking for models to transfer knowledge from commonly use these.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transferring CNN knowledge</h1>
                </header>
            
            <article>
                
<p>So, how can you transfer some knowledge from one model to another? Artificial neural networks have one advantage over human brains that facilitates this operation: they can be easily stored and duplicated. The expertise of a CNN is nothing but the values taken by its parameters after training—values that can easily be restored and transferred to similar networks.</p>
<p>Transfer learning for CNNs mostly consists of reusing the complete or partial architecture and weights of a performant network trained on a rich dataset to instantiate a new model for a different task. From this conditioned instantiation, the new model can then be <em>fine-tuned</em>; that is, it can be further trained on the available data for the new task/domain.</p>
<p>As we highlighted in the previous chapters, the first layers of a network tend to extract low-level features (such as lines, edges, or color gradients), whereas final convolutional layers react to more complex notions (such as specific shapes and patterns). For classification tasks, the final pooling and/or fully connected layers then process these high-level feature maps (often called <strong>bottleneck features</strong>) to make their class predictions.</p>
<p>This typical setup and related observations led to various transfer learning strategies. Pretrained CNNs, with their final prediction layers removed, started being used as efficient <em>feature extractors</em>. When the new task is similar enough to the ones these extractors were trained for, they can directly be used to output pertinent features (the <em>image feature vector</em> models on TensorFlow Hub are available for that exact purpose). These features can then be processed by one or two new dense layers, which are trained to output the task-related predictions. To preserve the quality of the extracted features, the layers of the feature extractors are often <em>frozen</em> during this training phase; that is, their parameters are not updated during the gradient descent. In other cases, when the tasks/domains are less similar, some of the last layers of the feature extractors—or all of them—are <em>fine-tuned</em>; that is, trained along with the new prediction layers on the task data. These different strategies are further explained in the next paragraphs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use cases</h1>
                </header>
            
            <article>
                
<p>In practice, which pretrained model should we reuse? Which layers should be frozen or fine-tuned? The answers to these questions depend on the similarity between the target task and the tasks that models have already been trained on, as well as the abundance of training samples for the new application.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Similar tasks with limited training data</h1>
                </header>
            
            <article>
                
<p>Transfer learning is especially useful when you want to solve a particular task and do not have enough training samples to properly train a performant model, but do have access to a larger and similar training dataset.</p>
<p>The model can be pretrained on this larger dataset until convergence (or, if available and pertinent, we can fetch an available pretrained model). Then, its final layers should be removed (when the target task is different, that is, its output differs from the pretraining task) and replaced with layers adapted to the target task. For example, imagine that we want to train a model to distinguish between pictures of bees and pictures of wasps. ImageNet contains images for these two classes, which could be used as a training dataset, but their number is not high enough for an efficient CNN to learn without overfitting. However, we could first train this network on the full ImageNet dataset to classify from the 1,000 categories to develop broader expertise. After this pretraining, its final dense layers can be removed and replaced by layers configured to output predictions for our two target classes.</p>
<p>As we mentioned earlier, the new model can finally be prepared for its task by freezing the pretrained layers and by training only the dense ones on top. Indeed, since the target training dataset is too small, the model would end up overfitting if we do not freeze its feature extractor component. By fixing these parameters, we make sure that the network keeps the expressiveness it developed on the richer dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Similar tasks with abundant training data</h1>
                </header>
            
            <article>
                
<p>The bigger the training dataset available for the target task, the smaller the chances of the network overfitting if we completely retrain it. Therefore, in such cases, people commonly unfreeze the latest layers of the feature extractor. In other words, the bigger the target dataset is, the more layers there are that can be safely fine-tuned. This allows the network to extract features that are more relevant to the new task, and thus to better learn how to perform it.</p>
<div class="packt_tip">The model has already been through a first training phase on a similar dataset and is probably close to convergence already. Therefore, it is common practice to use a smaller learning rate for the fine-tuning phase.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dissimilar tasks with abundant training data</h1>
                </header>
            
            <article>
                
<p>If we have access to a rich enough training set for our application, does it even make sense to use a pretrained model? This question is legitimate if the similarity between the original and target tasks is too low. Pretraining a model, or even downloading pretrained weights, can be costly. However, researchers demonstrated through various experiments that, in most cases, it is better to initialize a network with pretrained weights (even from a dissimilar use case) than with random ones.</p>
<div class="packt_infobox">Transfer learning makes sense when the tasks or their domains share at least some basic similarities. For instance, images and audio files can both be stored as two-dimensional tensors, and CNNs (such as ResNet ones) are commonly applied to both. However, the models are relying on completely different features for visual and audio recognition. It would typically not benefit a model for visual recognition to receive the weights from a network trained for an audio-related task.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dissimilar tasks with limited training data</h1>
                </header>
            
            <article>
                
<p>Finally, what if the target task is so specific that training samples are barely available and using pretrained weights does not make much sense? First, it would be necessary to reconsider applying or repurposing a deep model. Training such a model on a small dataset would lead to overfitting, and a deep pretrained extractor would return features that are too irrelevant for the specific task. However, we can still benefit from transfer learning if we keep in mind that the first layers of CNNs react to low-level features. Instead of only removing the final prediction layers of a pretrained model, we can also remove some of the last convolutional blocks, which are too task-specific. A shallow classifier can then be added on top of the remaining layers, and the new model can finally be fine-tuned.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning with TensorFlow and Keras</h1>
                </header>
            
            <article>
                
<p>To conclude this chapter, we will briefly cover how transfer learning can be performed with TensorFlow and Keras. We invite our readers to go through the related Jupyter notebook in parallel, to have transfer learning illustrated on classification tasks.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model surgery</h1>
                </header>
            
            <article>
                
<p>Indirectly, we have already presented how standard pretrained models provided through TensorFlow Hub and Keras applications can be fetched and easily transformed into feature extractors for new tasks. However, it is also common to reuse non-standard networks; for example, more specific state-of-the-art CNNs provided by experts, or custom models already trained for some previous tasks. We will demonstrate how any models can be edited for transfer learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing layers</h1>
                </header>
            
            <article>
                
<p>The first task is to remove the final layers of the pretrained model to transform it into a feature extractor. As usual, Keras makes this operation quite easy. For <kbd>Sequential</kbd> models, the list of layers is accessible through the <kbd>model.layers</kbd> attribute. This structure has a <kbd>pop()</kbd> method, which removes the last layer of the model. Therefore, if we know the number of final layers we need to remove to transform a network into a specific feature extractor (for instance, two layers for a standard ResNet model), this can be done as follows:</p>
<pre class="lang-py prettyprint prettyprinted"><span class="pln">for i in range(num_layers_to_remove):<br/>    model</span><span class="pun">.</span><span class="pln">layers</span><span class="pun">.</span><span class="pln">pop</span><span class="pun">()</span></pre>
<p>In pure TensorFlow, editing an operational graph supporting a model is neither simple nor recommended. However, we have to keep in mind that unused graph operations are not executed at runtime. So, still having the old layers present in the compiled graph will not affect the computational performance of the new model, as long as they are not called anymore. Therefore, instead of removing layers, we simply need to pinpoint the last layer/operation of the previous model we want to keep. If we <span>somehow </span>lost track of its corresponding Python object, but know its name (for instance, by checking the graph in Tensorboard), its representative tensor can be recovered by looping over the layers of the model and checking their names:</p>
<pre>for layer in model.layers:<br/>    if layer.name == name_of_last_layer_to_keep:<br/>        bottleneck_feats = layer.output<br/>        break</pre>
<p>However, Keras provides additional methods to simplify this process. Knowing the name of the last layer to keep (for instance, after printing the names with <kbd>model.summary()</kbd>), a feature extractor model can be built in a couple of lines:</p>
<pre class="lang-py prettyprint prettyprinted"><span class="pun">bottleneck_feats =</span><span class="pln"> </span><span class="pln">model</span><span class="pun">.</span><span class="pln">get_layer</span><span class="pun">(last_</span><span class="pln">layer_name</span><span class="pun">).output</span><span class="pun"><br/></span><span class="pun">feature_extractor =</span><span class="pln"> </span><span class="typ">Model</span><span class="pun">(</span><span class="pln">inputs</span><span class="pun">=</span><span class="pln">model</span><span class="pun">.</span><span class="pln">input</span><span class="pun">,</span><span class="pln"> outputs</span><span class="pun">=</span><span class="pln">bottleneck_feats</span><span class="pun">)</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Sharing its weights with the original model, this feature-extraction model is ready for use.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Grafting layers</h1>
                </header>
            
            <article>
                
<p>Adding new prediction layers on top of a feature extractor is straightforward (compared with previous examples with TensorFlow Hub), as it is just a matter of adding new layers on top of the corresponding model. For example, this can be done as follows, using the Keras API:</p>
<pre>dense1 = Dense(...)(feature_extractor.output) # ...<br/>new_model = Model(model.input, dense1)</pre>
<p>As we can see, through Keras, TensorFlow 2 makes it straightforward to shorten, extend, or combine models!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Selective training</h1>
                </header>
            
            <article>
                
<p class="mce-root">Transfer learning makes the training phase a bit more complex because we should first restore the pretrained layers and define which ones should be frozen. Thankfully, several tools are available that simplify these operations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Restoring pretrained parameters</h1>
                </header>
            
            <article>
                
<p>TensorFlow has some utility functions to warm-start estimators; that is, to initialize some of their layers with pretrained weights. The following snippet tells TensorFlow to use the saved parameters of a pretrained estimator for the new one for the layers sharing the same name:</p>
<pre>def model_function():<br/>    # ... define new model, reusing pretrained one as feature extractor.<br/><br/>ckpt_path = '/path/to/pretrained/estimator/model.ckpt'<br/>ws = tf.estimator.WarmStartSettings(ckpt_path)<br/>estimator = tf.estimator.Estimator(model_fn, warm_start_from=ws)</pre>
<div class="packt_tip">The <kbd>WarmStartSettings</kbd> initializer takes an optional <kbd>vars_to_warm_start</kbd> parameter, which can also be used to provide the names of the specific variables (as a list or a regex) that you want to restore from the checkpoint files (refer to the documentation for more details at <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings">https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings</a>).</div>
<p class="mce-root"/>
<p>With Keras, we can simply restore the pretrained model before its transformation for the new task:</p>
<pre># Assuming the pretrained model was saved with `model.save()`:<br/>model = tf.keras.models.load_model('/path/to/pretrained/model.h5')<br/># ... then pop/add layers to obtain the new model.</pre>
<p>Although it is not exactly optimal to restore the complete model before removing some of its layers, this solution has the advantage of being concise.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Freezing layers</h1>
                </header>
            
            <article>
                
<p>In TensorFlow, the most versatile method for freezing layers consists of removing their <kbd>tf.Variable</kbd> attributes from the list of variables passed to the optimizer:</p>
<pre># For instance, we want to freeze the model's layers with "conv" in their name:<br/>vars_to_train = model.trainable_variables<br/>vars_to_train = [v for v in vars_to_train if "conv" in v.name]<br/><br/># Applying the optimizer to the remaining model's variables:<br/>optimizer.apply_gradients(zip(gradient, vars_to_train))</pre>
<p>In Keras, layers have a <kbd>.trainable</kbd> attribute, which can simply be set to <kbd>False</kbd> in order to freeze them:</p>
<pre>for layer in feature_extractor_model.layers:<br/>    layer.trainable = False  # freezing the complete extractor</pre>
<p>Again, for complete transfer learning examples, we invite you to go through the Jupyter notebooks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="mce-root">Classification challenges, such as ILSVRC, are great playgrounds for researchers, leading to the development of more advanced deep learning solutions. In their own way, each of the architectures we detailed in this chapter became instrumental in computer vision and are still applied to increasingly complex applications. As we will see in the following chapters, their technical contributions inspired other methods for a wide range of visual tasks.</p>
<p class="mce-root"/>
<p class="mce-root">Moreover, not only did we learn to reuse state-of-the-art solutions, but we also discovered how algorithms themselves can benefit from the knowledge acquired from previous tasks. With transfer learning, the performance of CNNs can be greatly improved for specific applications. This is especially true for tasks such as object detection, which will be the topic of our next chapter. Annotating datasets for object detection is more tedious than for image-level recognition, so methods usually have access to smaller training datasets. It is, therefore, important to keep transfer learning in mind as a solution to obtain efficient models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Which TensorFlow Hub module can be used to instantiate an inception classifier for ImageNet?</li>
<li>How can you freeze the first three residual macro-blocks of a ResNet-50 model from Keras applications?</li>
<li>When is transfer learning not recommended?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Hands-On Transfer Learning with Python</em> (<a href="https://www.packtpub.com/big-data-and-business-intelligence/hands-transfer-learning-python">https://www.packtpub.com/big-data-and-business-intelligence/hands-transfer-learning-python</a>), by Dipanjan Sarkar, Raghav Bali, and Tamoghna Ghosh: This book covers transfer learning in more detail, while applying deep learning to domains other than computer vision.</li>
</ul>


            </article>

            
        </section>
    </body></html>