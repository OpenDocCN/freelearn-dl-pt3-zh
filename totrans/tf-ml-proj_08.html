<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generating Uncertainty in Traffic Signs Classifier Using Bayesian Neural Networks</h1>
                </header>
            
            <article>
                
<p>As humans, we love the uncertainty that comes with predictions. For example, we always want to know what the chances are of it raining before we leave the house. However, with traditional deep learning, we only have a point prediction and no notion of uncertainty. Predictions from these networks are assumed to be accurate, which is not always the case. Ideally, we would like to know the level of confidence of predictions from neural networks before making a decision.</p>
<p>For example, having uncertainty in the model could have potentially avoided the following disastrous consequences:</p>
<ul>
<li>In May 2016, the Tesla Model S crashed in northern Florida into a truck that was turning left in front of it. According to the official Tesla blog (<a href="https://www.tesla.com/en_GB/blog/tragic-loss">https://www.tesla.com/en_GB/blog/tragic-loss</a>), <em>Neither Autopilot nor the driver noticed the white side of the tractor trailer against a brightly lit sky, so the brake was not applied</em>.</li>
<li>In July 2015, two African Americans were classified as gorillas by Google's image classifier, raising concerns of racial discrimination. Here (<a href="https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/">https://www.usatoday.com/story/tech/2015/07/01/google-apologizes-after-photos-identify-black-people-as-gorillas/29567465/</a>) is the press release.</li>
</ul>
<p>The thing that is pretty clear from these examples is that quantifying uncertainty in predictions could have avoided these disasters. The question now is: If it's so obvious, why didn't Tesla or Google implement it in the first place?</p>
<p>Bayesian algorithms <span>(like Gaussian processes) </span>can quantify uncertainty, but cannot scale to large datasets such as images and videos, whereas deep learning is able to produce much better accuracy—except that it lacks any notion of uncertainty.</p>
<p>In this chapter, we will explore the concept of Bayesian neural networks, which combines the concepts of deep learning and Bayesian learning with model uncertainty in the predictions of deep neural networks. We will cover the following topics:</p>
<ul>
<li>Introduction to Bayesian deep learning</li>
<li>What are Bayesian neural networks?</li>
<li>Building a Bayesian neural network with the German Traffic Sign Image dataset</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding Bayesian deep learning</h1>
                </header>
            
            <article>
                
<p>We've all understood the basics of Bayes<span>'</span> rule, as explained in Chapter 6, <em>Predicting Stock Prices using Gaussian Process Regression</em>.</p>
<p>For Bayesian machine learning, we use the same formula as Bayes' rule to learn model parameters (<img class="fm-editor-equation" src="assets/f5d0830f-24cd-4bde-a545-131fa87abb6c.png" style="width:1.08em;height:1.00em;"/>) from the given data, <img class="fm-editor-equation" src="assets/839c249a-4c94-4cc9-9f42-a5eefc53e68f.png" style="width:1.08em;height:1.08em;"/>. The formula, then, looks like this:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1ca7cadc-8435-47da-9f98-f28d27c21001.png" style="width:13.17em;height:3.17em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/b4360234-8459-431a-8283-360b9fd20244.png" style="width:1.83em;height:0.92em;"/> or the probability of observed data is also called evidence. This is always difficult to compute. One brute-force way is to integrate out <img class="fm-editor-equation" src="assets/a534dbd7-1960-4d32-9cb1-d54dabe39401.png" style="width:3.83em;height:1.33em;"/> for all the values of model parameters, but this is obviously too expensive to evaluate.<img class="fm-editor-equation" src="assets/610582f1-80be-46ca-b3d8-625b3794d4a0.png" style="width:2.58em;height:1.33em;"/> is the prior on parameters, which is nothing but some randomly initialized value of parameters in most cases. Generally, we don't care about setting the priors perfectly as we expect the inference procedure to converge to the right value of parameters.</p>
<p><img class="fm-editor-equation" src="assets/614510dd-708d-4882-a251-6cb8cfec80bf.png" style="width:3.83em;height:1.33em;"/> is known as the likelihood of data, given the modeling parameters. Effectively, it shows how likely it is to obtain the given observations in the data when given the model parameters. We use likelihood as a measure to evaluate different models. The higher the likelihood, the better the model.</p>
<p>Finally, <img class="fm-editor-equation" src="assets/18494e17-11f3-433a-ab12-e82727d65da9.png" style="width:2.83em;height:1.00em;"/>, a posterior, is what we want to calculate. It's a probability distribution over model parameters that's obtained from the given data. Once we obtain the uncertainty in model parameters, we can use them to quantify the uncertainty in model predictions.</p>
<p>Generally, in machine learning, we use <strong>Maximum Likelihood estimation</strong> (<strong>MLE</strong>) (<a href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf">https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading10b.pdf</a>) to get the estimates of model parameters. However, in the case of Bayesian deep learning, we estimate a posterior from the prior and the procedure is known as <strong>Maximum a posteriori</strong> <span>(</span><strong>MAP</strong><span>) </span>estimation (<a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf">https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Bayes' rule in neural networks</h1>
                </header>
            
            <article>
                
<p>Traditionally, neural networks produce a point estimate by optimizing weights and biases to minimize a loss function, such as the mean squared error in regression problems. As mentioned earlier, this is similar to finding parameters using the Maximum likelihood estimation criteria:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/df3f26a9-ede4-43c3-886b-97655eb0ffda.png" style="width:19.50em;height:2.08em;"/></p>
<p>Typically, we obtain the best parameters through backpropagation in neural networks. To avoid overfitting, we introduce a regularizer of <img class="fm-editor-equation" src="assets/9346cd9b-1b59-4039-aa0f-1d75ac2bada1.png" style="width:1.33em;height:1.17em;"/> norm over weights. If you are not aware of regularization, please refer to the following Andrew Ng video: <a href="http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning">http://openclassroom.stanford.edu/MainFolder/CoursePage.php?course=MachineLearning</a>. It has been shown that <img class="fm-editor-equation" src="assets/16aa6075-3e33-4e93-a7f5-6c6beeb2117c.png" style="width:1.25em;height:1.08em;"/> normalization is equivalent to placing a normal prior on weights <img class="fm-editor-equation" src="assets/b2ee0746-0cee-44a6-8dbc-fc6269a8f8f7.png" style="width:5.67em;height:1.17em;"/>. With a prior on weights, the MLE estimation problem can be framed as a MAP estimation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/88c60ffd-1803-4bce-b8de-88ee81a6a1b3.png" style="width:19.58em;height:2.08em;"/></p>
<p class="mce-root"/>
<p>Using Bayes' rule, the preceding equation can be written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/989188bb-023e-4889-a734-f91ebb74ebe7.png" style="width:26.92em;height:2.08em;"/></p>
<div class="packt_tip">The exact proof of equivalence of regularization to the Bayesian framework is outside the scope of this chapter. If you are interested, you can read more about it at the following MIT lecture: <a href="http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf">http://www.mit.edu/~9.520/spring09/Classes/class15-bayes.pdf</a>.</div>
<p>From this, we can observe that traditional neural networks with regularization can be framed as a problem of inference using Bayes' rule. Bayesian neural networks aim to determine the posterior distribution using Monte Carlo or Variational inference techniques. In the rest of this chapter, we will look at how to build a Bayesian neural network using TensorFlow Probability.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding TensorFlow probability, variational inference, and Monte Carlo methods</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow Probability</strong> (<strong>tfp</strong> in code <span>– </span><a href="https://www.tensorflow.org/probability/overview#layer_2_model_building">https://www.tensorflow.org/probability/overview#layer_2_model_building</a>) was recently released by Google to perform probabilistic reasoning in a scalable manner. It provides tools and functionalities to define distributions, build neural networks with prior on weights, and perform probabilistic inference tasks such as Monte Carlo or Variational Inference.</p>
<p>Let's take a look at some of the functions/utilities we will be using for building our model:</p>
<ul>
<li><strong>Tfp.distributions.categorical</strong>: This is a standard categorical distribution that's characterized by probabilities or log-probabilities over K classes. In this project, we have Traffic Sign images from 43 different traffic signs. We will define a categorical distribution over 43 classes in this project.</li>
<li><strong>Probabilistic layers</strong>:<span> </span>Built on top of the TensorFlow layers implementation, probabilistic layers incorporate uncertainty over the functions they represent. Effectively, they incorporate uncertainty in the weights of the neural networks. They have the functionality to forward pass through the inputs by sampling from the posterior of weight distributions (<img class="fm-editor-equation" src="assets/3dc23fbc-1c62-49e1-afff-1457bbdbb861.png" style="width:4.50em;height:1.58em;"/>). Specifically, we will use<span> the </span>Convolutional2DFlipout<span> </span>(<a href="https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout">https://www.tensorflow.org/probability/api_docs/python/tfp/layers/Convolution2DFlipout</a>) layer, which can compute the forward pass by sampling from the posterior of the weight parameters of the model.</li>
<li><strong>Kullback-leibler (KL) divergence</strong>: If we want to measure the difference between two numbers, we just subtract them. What if we want to obtain a difference between two probability distributions? What is the equivalent of subtraction in this case? Often in the case in probability and statistics, we will<span> replace the observed data or complex distributions with a simpler, approximating distribution. KL Divergence helps us measure just how much information we lose when we choose an approximation. Essentially, it is a measure of one probability distribution from others. A KL divergence of 0 indicates that two distributions are identical. If you want to know more about the mathematics of KL divergence, please refer to a great explanation from MIT open courseware, which can be found at <a href="https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf">https://ocw.mit.edu/courses/sloan-school-of-management/15-097-prediction-machine-learning-and-statistics-spring-2012/lecture-notes/MIT15_097S12_lec15.pdf</a>.</span></li>
<li class="CDPAlignLeft CDPAlign"><strong>Variational inference</strong>:<span> Variational inference is a machine learning method that's used to approximate complex, intractable integrals in Bayesian learning through optimization</span>. </li>
</ul>
<p style="padding-left: 60px">As we know, our aim in Bayesian learning is to calculate the posterior <img class="fm-editor-equation" src="assets/13cfd9ef-c6bd-486c-b10f-32f04f36f217.png" style="width:4.50em;height:1.58em;"/>, given prior <img class="fm-editor-equation" src="assets/580456b9-ca75-4a74-8ff2-2d7062f886e1.png" style="width:2.58em;height:1.33em;"/> and data <img class="fm-editor-equation" src="assets/f1181935-aeb9-47fc-95b9-606593c46334.png" style="width:1.33em;height:1.33em;"/>. A prerequisite for computing the posterior is the computation of distribution of <img class="fm-editor-equation" src="assets/eafed642-dc26-40cf-b555-99b5c617f851.png" style="width:1.33em;height:1.33em;"/> (data) in order to obtain <img class="fm-editor-equation" src="assets/f0ce5197-f81a-4652-95e4-f2a4f361b607.png" style="width:2.50em;height:1.25em;"/>, or <em>evidence</em>. As mentioned earlier, the distribution of X is intractable as it is too expensive to compute using a brute-force approach. To address this problem, we will use something called Variational Inference (VI). In VI, we define a family of distributions <img class="fm-editor-equation" src="assets/787fd8b6-1a02-4263-a9b9-4cb5b05afa8f.png" style="width:4.25em;height:1.50em;"/><span>, </span><span>parameterized by</span> <span><img class="fm-editor-equation" src="assets/8f23cfcf-a85c-4cc1-b76c-7261bd3f8d4f.png" style="width:0.92em;height:1.42em;"/></span><span>. The core idea is to optimize</span> <span><img class="fm-editor-equation" src="assets/740b2ea1-bf88-4e90-a83a-406a68d690f4.png" style="width:0.92em;height:1.42em;"/></span><span> </span><span>so that the approximate distribution is as close to the true posterior as possible. We measure the difference between two distributions using KL divergence. </span>As it turns out, it is not easy to minimize the KL divergence. We can show you that this KL divergence is always positive and that it comprises two parts using the following mathematical formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7644d761-4949-4895-b510-906255b0c046.png" style="width:25.83em;height:1.75em;"/></p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">Here, <strong>ELBO</strong> is <strong>Evidence Lower Bound</strong> (<a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf">https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf</a>).</p>
<ul>
<li><strong>Monte Carlo method: </strong>Monte Carlo methods are computational methods which rely on repeated random sampling to obtain the statistical behavior of some phenomenon (behavior). They are typically used to model uncertainties or generate what-if scenarios for business.</li>
</ul>
<p style="padding-left: 60px">Let's say you commute by train every day to work. You are thinking about whether to take the company shuttle to the office instead. Now, there are many random variables associated with a bus ride, such as time of arrival, traffic, number of passengers boarding the bus, and so on.</p>
<p style="padding-left: 60px">One way that we can look at this what-if scenario is if we take the mean of these random variables and calculate the arrival time. However, that will be too naive as it doesn't take into account variance in these variables. Another way is to sample from these random variables (somehow, if you are able to do that!) and generate what-if scenarios of reaching the office.</p>
<p style="padding-left: 60px">To make a decision, you will need an acceptable criteron. For instance, if you observe that in 80% of these what-if scenarios you reach office on or before time, you can continue forward. This approach is also known as the Monte Carlo simulation. </p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px">In this project, we will model the weights of neural networks as random variables. To determine the final prediction, we will sample from the distributions of these weights repeatedly to obtain the distribution of predictions.</p>
<div class="packt_infobox">Note that we have skipped some mathematical details. Feel free to read more about<span> </span>Variational Inference<span> </span>(<a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf">https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf</a><a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf">)</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a Bayesian neural network</h1>
                </header>
            
            <article>
                
<p>For this project, we will use the German Traffic Sign Dataset (<a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset">http://benchmark.ini.rub.de/?section=gtsrb&amp;amp;subsection=dataset</a>) to build a Bayesian neural network. The training dataset contains 26,640 images in 43 classes. Similarly, the testing dataset contains 12,630 images.</p>
<div class="packt_infobox">Please read the <kbd>README.md</kbd> file in this book's repository before executing the code to install the appropriate dependencies and for instructions on how to run the code.</div>
<p>The following is an image that's present in this dataset: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-588 image-border" src="assets/bca6d66c-486e-4b01-8b71-8501f9a16c56.png" style="width:23.50em;height:42.50em;"/></p>
<p>You can see that there are different kinds of traffic sign depicted by different classes in the dataset.</p>
<p>We begin by p<span>re-processing our dataset and making it conform to the requirements of the learning algorithm. This is done by reshaping the images to a uniform size via histogram equalization, which is used to enhance contrast, and cropping them to only focus on the traffic signs in the image. Also, we convert the images to grayscale as traffic signs are identified by shape and not color in</span> <span>the image. </span></p>
<p>For modeling, we define a standard Lenet model (<a href="http://yann.lecun.com/exdb/lenet/">http://yann.lecun.com/exdb/lenet/</a>), which was<span> developed by Yann Lecun. Lenet is one of the first convolutional neural networks that was designed. It is</span><span> small and easy to understand, yet large enough to provide interesting results.</span></p>
<p>A standard Lenet model has the following properties: </p>
<ul>
<li>Three convolutional layers with increasing filter sizes</li>
<li>Four fully connected layers</li>
<li>No dropout layers</li>
<li><strong>Rectified linear</strong> (<strong>ReLU</strong>) after every fully connected or convolutional layer</li>
<li>Max pooling after every convolutional layer</li>
</ul>
<p>We train this model to minimize the negative of ELBO loss that is defined in the <em><span>Understanding TensorFlow Probability, Variational Inference, and Monte Carlo methods</span></em><span> section of this chapter</span>. Specifically, we define ELBO loss as a combination of two terms:</p>
<ul>
<li>Expected log likelihood or cross entropy that can be estimated through the Monte Carlo method </li>
<li><span>KL divergence</span></li>
</ul>
<p><span>Once the model is trained, we</span><span> evaluate the predictions on the hold-out dataset. One of the major differences in Bayesian neural network evaluation is that there is no single set of parameters (weights of the model) that we can obtain from training. Instead, we obtain a distribution of all the parameters. For evaluation, we will have to sample values from the distribution of each parameter to obtain the accuracy on the testing set. We will sample the parameters of the model multiple times to obtain a confidence interval on our predictions.</span></p>
<p>Lastly, we will show some uncertainty in our predictions in sample images from the testing dataset and also plot the distribution of the weight parameters we obtain.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining, training, and testing the model</h1>
                </header>
            
            <article>
                
<p>Download both the training and testing datasets from <a href="http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=dataset">http://benchmark.ini.rub.de/?section=gtsrb&amp;amp;subsection=dataset</a>. Let's look at the steps to build the project after you have downloaded the dataset:</p>
<ol>
<li>Begin by transforming the images present in the dataset using histogram equalization. This is essential as each image in the dataset may have a different scale of illumination. You can see from the following two images how images of the same traffic sign have very different illumination. Histogram equalization helps to normalize these differences and makes the training data more consistent:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1015 image-border" src="assets/564c6534-4274-47d0-844e-7337c1db8f30.png" style="width:20.08em;height:20.50em;"/><img class="alignnone size-full wp-image-1016 image-border" src="assets/fabef10e-3b86-4de1-a49d-3f0d9f745e8a.png" style="width:19.25em;height:19.92em;"/></p>
<p style="padding-left: 60px">Once we have performed the equalization, crop the image to focus on just the sign, and resize the image to 32 x 32 as desired by our learning algorithm:</p>
<div class="packt_tip">Note that we use 32 x 32 as the shape of images for training as it is big enough to preserve the nuances of the image for detection and small enough to train the model faster. </div>
<pre style="padding-left: 60px">def normalize_and_reshape_img(img):<br/># Histogram normalization in v channel<br/>hsv = color.rgb2hsv(img)<br/>hsv[:, :, 2] = exposure.equalize_hist(hsv[:, :, 2])<br/>img = color.hsv2rgb(hsv)<br/># Crop of the centre<br/>min_side = min(img.shape[:-1])<br/>centre = img.shape[0] // 2, img.shape[1] // 2<br/>img = img[centre[0] - min_side // 2:centre[0] + min_side // 2,<br/>centre[1] - min_side // 2:centre[1] + min_side // 2,<br/>:]<br/># Rescale to the desired size<br/>img = transform.resize(img, (IMG_SIZE, IMG_SIZE))<br/>return<br/> img</pre>
<ol start="2">
<li>Create dictionaries with label and image information for the train/test dataset and store them as pickle files so that we don't have to run the pre-processing code every time we run the model. This means that we essentially pre-process the transformed data to create our train and test datasets:</li>
</ol>
<pre style="padding-left: 60px">def preprocess_and_save_data(data_type ='train'):<br/>'''<br/>Preprocesses image data and saves the image features and labels as pickle files to be used for the model<br/>:param data_type: data_type is 'train' or 'test'<br/>:return: None<br/>'''<br/>if data_type =='train':<br/>root_dir = os.path.join(DATA_DIR, 'GTSRB/Final_Training/Images/')<br/>imgs = []<br/>labels = []<br/>all_img_paths = glob.glob(os.path.join(root_dir, '*/*.ppm'))<br/>np.random.shuffle(all_img_paths)<br/>for img_path in all_img_paths:<br/>img = normalize_and_reshape_img(io.imread(img_path))<br/>label = get_class(img_path)<br/>imgs.append(img)<br/>labels.append(label)<br/>X_train = np.array(imgs, dtype='float32')<br/># Make one hot targets<br/>Y_train = np.array(labels, dtype = 'uint8')<br/>train_data = {"features": X_train, "labels": Y_train}<br/>if not os.path.exists(os.path.join(DATA_DIR,"Preprocessed_Data")):<br/>os.makedirs(os.path.join(DATA_DIR,"Preprocessed_Data"))<br/>pickle.dump(train_data,open(os.path.join(DATA_DIR,"Preprocessed_Data","preprocessed_train.p"),"wb"))<br/>return train_data<br/>elif data_type == 'test':<br/># Reading the test file<br/>test = pd.read_csv(os.path.join(DATA_DIR, "GTSRB", 'GT-final_test.csv'), sep=';')<br/>X_test = []<br/>y_test = []<br/>i = 0<br/>for file_name, class_id in zip(list(test['Filename']), list(test['ClassId'])):<br/>img_path = os.path.join(DATA_DIR, 'GTSRB/Final_Test/Images/', file_name)<br/>X_test.append(normalize_and_reshape_img(io.imread(img_path)))<br/>y_test.append(class_id)<br/>test_data = {"features": np.array(X_test,dtype ='float32'), "labels": np.array(y_test,dtype = 'uint8')}<br/>if not os.path.exists(os.path.join(DATA_DIR,"Preprocessed_Data")):<br/>os.makedirs(os.path.join(DATA_DIR,"Preprocessed_Data"))<br/>pickle.dump(test_data,open(os.path.join(DATA_DIR,"Preprocessed_Data","preprocessed_test.p"),"wb"))<br/>return test_data</pre>
<div class="packt_infobox">We will use <span>grayscale images</span> in our project as our task throughout this project is to classify traffic sign images into one of the 43 classes and provide a measure of uncertainty in our classification. We do not care about the color of the image.</div>
<ol start="3">
<li>Define the model using the LeNet architecture in Keras. Finally, we will assign the 43 sized vector of outputs from the LeNet model into a categorical distribution function <span>(<kbd>tfd.categorical</kbd>)</span> from TensorFlow probability. This will help us generating the uncertainty in predictions afterwards:</li>
</ol>
<pre style="padding-left: 60px">with tf.name_scope("BNN", values=[images]):<br/>model = tf.keras.Sequential([<br/>tfp.layers.Convolution2DFlipout(10,<br/>kernel_size=5,<br/>padding="VALID",<br/>activation=tf.nn.relu),<br/>tf.keras.layers.MaxPooling2D(pool_size=[3, 3],<br/>strides=[1, 1],<br/>padding="VALID"),<br/>tfp.layers.Convolution2DFlipout(15,<br/>kernel_size=3,<br/>padding="VALID",<br/>activation=tf.nn.relu),<br/>tf.keras.layers.MaxPooling2D(pool_size=[2, 2],<br/>strides=[2, 2],<br/>padding="VALID"),<br/>tfp.layers.Convolution2DFlipout(30,<br/>kernel_size=3,<br/>padding="VALID",<br/>activation=tf.nn.relu),<br/>tf.keras.layers.MaxPooling2D(pool_size=[2, 2],<br/>strides=[2, 2],<br/>padding="VALID"),<br/>tf.keras.layers.Flatten(),<br/>tfp.layers.DenseFlipout(400, activation=tf.nn.relu),<br/>tfp.layers.DenseFlipout(120, activation = tf.nn.relu),<br/>tfp.layers.DenseFlipout(84, activation=tf.nn.relu),<br/>tfp.layers.DenseFlipout(43) ])<br/>logits = model(images)<br/>targets_distribution = tfd.Categorical(logits=logits)</pre>
<ol start="4">
<li>We define the loss to minimize the KL divergence up to ELBO. C<span>ompute the ELBO loss that is defined in the <em>Understanding TensorFlow Probability, Variational Inference, and Monte Carlo methods</em></span><span> section of this chapter. As you can see, we use the <kbd>model.losses</kbd> attribute to compute the KL divergence. This is because the <kbd>losses</kbd> attribute of a TensorFlow Keras Layer represents a side-effect computation such as regularizer penalties. Unlike regularizer penalties on specific TensorFlow variables, here the <kbd>losses</kbd> represent the KL divergence computation:</span></li>
</ol>
<pre style="padding-left: 60px"># Compute the -ELBO as the loss, averaged over the batch size.<br/>neg_log_likelihood = -<br/>  tf.reduce_mean(targets_distribution.log_prob(targets))<br/>kl = sum(model.losses) / X_train.shape[0]<br/>   elbo_loss = neg_log_likelihood + kl</pre>
<ol start="5">
<li>Use the Adam optimizer, as defined in <a href="60549866-497e-4dfa-890c-6651f34cf8e4.xhtml" target="_blank">Chapter 3</a>, <em>Sentiment Analysis in your browser using TensorFlow.js</em>, to optimize the ELBO loss:</li>
</ol>
<pre style="padding-left: 60px">with tf.name_scope("train"):<br/>optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)<br/>train_op = optimizer.minimize(elbo_loss)</pre>
<div class="packt_infobox"><span>Note that we are using the Adam optimizer because it generally performs better than other optimizers with default parameters.</span></div>
<ol start="6">
<li>Train the model with the following parameters:
<ul>
<li>Epochs = 1,000</li>
<li>Batch size = 128</li>
<li>Learning rate = 0.001:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 60px">with<span> tf.Session() </span>as<span> sess:<br/></span>sess.run(init_op)<br/># Run the training loop.<br/>train_handle = sess.run(train_iterator.string_handle())<br/>test_handle = sess.run(test_iterator.string_handle())<br/>for step in range(EPOCHS):<br/>_ = sess.run([train_op, accuracy_update_op],<br/>feed_dict={iter_handle: train_handle})<br/>if step % 5== 0:<br/>loss_value, accuracy_value = sess.run(<br/>     [elbo_loss, accuracy], feed_dict={iter_handle: train_handle})<br/>print("Epoch: {:&gt;3d} Loss: {:.3f} Accuracy: {:.3f}".format(<br/>step, loss_value, accuracy_value))</pre>
<ol start="7">
<li>Once the model is trained, each weight in the Bayesian neural network will have a distribution and not a fixed value. Sample each weight multiple times (50, in the code) and obtain different predictions for each sample. Sampling, although usef<span>ul, is expensive. Therefore, we should only use Bayesian neural networks where we require some measure of uncertainty in our predictions. Here is the code for Monte Carlo sampling:</span></li>
</ol>
<pre style="padding-left: 60px">#Sampling from the posterior and obtaining mean probability for held out dataset<br/>probs = np.asarray([sess.run((targets_distribution.probs),<br/>        feed_dict={iter_handle: test_handle})<br/>for _ in range(NUM_MONTE_CARLO)])</pre>
<ol start="8">
<li> Once you have the samples, obtain the mean probability for each image in the test dataset and compute the mean accuracy like in usual machine learning classifiers.<br/>
The mean accuracy we obtain for this dataset is ~<span> </span>89%<span> </span><span>for 1,000 Epochs. You can tune the parameters further or create a deeper model to obtain better accuracy.</span><br/>
Here is the code for getting the mean accuracy:</li>
</ol>
<pre style="padding-left: 60px">mean_probs = np.mean(probs, axis=0)<br/># Get the average accuracy<br/>Y_pred = np.argmax(mean_probs, axis=1)<br/>print("Overall Accuracy in predicting the test data = percent", round((Y_pred == y_test).mean() * 100,2))</pre>
<ol start="9">
<li>The next step is to calculate the distribution of accuracy for each Monte Carlo sample of each test image. For that, compute the predicted class and compare it with the test label. The predicted class can be obtained by assigning the label to the class with the maximum probability for a given network parameter sample. This way, you can get the range of accuracies and can also plot those accuracies on a histogram. Here is the code for obtaining accuracy and generating a histogram:</li>
</ol>
<pre style="padding-left: 60px">test_acc_dist = []<br/>for prob in probs:<br/>y_test_pred = np.argmax(prob, axis=1).astype(np.float32)<br/>accuracy = (y_test_pred == y_test).mean() * 100<br/>test_acc_dist.append(accuracy)<br/>plt.hist(test_acc_dist)<br/>plt.title("Histogram of prediction accuracies on test dataset")<br/>plt.xlabel("Accuracy")<br/>plt.ylabel("Frequency")<br/>save_dir = os.path.join(DATA_DIR, "..", "Plots")<br/>plt.savefig(os.path.join(save_dir, "Test_Dataset_Prediction_Accuracy.png"))</pre>
<p style="padding-left: 60px">The histogram that's generated will look something like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-592 image-border" src="assets/467b468e-0ee2-4b1f-a89b-c39554f745f6.png" style="width:24.58em;height:39.83em;"/></p>
<p style="padding-left: 60px">As you can see, we have a distribution of accuracies. This distribution can help us obtain the confidence interval over the accuracy of our model on the test dataset.</p>
<div class="packt_infobox"><span>Note that the plot might look differently when you run the code, since it is obtained through random sampling.</span></div>
<ol start="10">
<li>Take a few images from the test dataset and see their predictions for different samples in Monte Carlo. Use the following function <kbd>plot_heldout_prediction</kbd> to generate the histogram of predictions from different samples in Monte Carlo:</li>
</ol>
<pre style="padding-left: 60px">def plot_heldout_prediction(input_vals, probs , fname, title=""):<br/>save_dir = os.path.join(DATA_DIR, "..", "Plots")<br/>fig = figure.Figure(figsize=(1, 1))<br/>canvas = backend_agg.FigureCanvasAgg(fig)<br/>ax = fig.add_subplot(1,1,1)<br/>ax.imshow(input_vals.reshape((IMG_SIZE,IMG_SIZE)), interpolation="None")<br/>canvas.print_figure(os.path.join(save_dir, fname + "_image.png"), format="png")<br/>fig = figure.Figure(figsize=(10, 5))<br/>canvas = backend_agg.FigureCanvasAgg(fig)<br/>ax = fig.add_subplot(1,1,1)<br/>#Predictions<br/>y_pred_list = list(np.argmax(probs,axis=1).astype(np.int32))<br/>bin_range = [x for x in range(43)]<br/>ax.hist(y_pred_list,bins = bin_range)<br/>ax.set_xticks(bin_range)<br/>ax.set_title("Histogram of predicted class: " + title)<br/>ax.set_xlabel("Class")<br/>ax.set_ylabel("Frequency")<br/>fig.tight_layout()<br/>save_dir = os.path.join(DATA_DIR, "..", "Plots")<br/>canvas.print_figure(os.path.join(save_dir, fname + "_predicted_class.png"), format="png")<br/>print("saved {}".format(fname))</pre>
<p style="padding-left: 60px">Let's look at some of the images and their predictions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-593 image-border" src="assets/0032af64-7c08-4a41-a99e-77a001e86ba7.png" style="font-size: 1em;width:7.33em;height:7.42em;"/></p>
<p style="padding-left: 60px">For the preceding image, all of the predictions belonged to the correct class 02, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-594 image-border" src="assets/ce216508-385a-4f51-bf4b-56aec0ed1850.png" style="width:82.67em;height:40.67em;"/></p>
<p style="padding-left: 60px">In the following two cases, although our mean prediction was correct, some samples in Monte Carlo predicted the wrong class. You can imagine how quantifying uncertainty in such cases can make a self-driving car make better decisions on the road.</p>
<p style="padding-left: 60px"><strong>Case 1: </strong></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-595 image-border" src="assets/68be4b03-477a-477c-95c0-9947f85ef0cf.png" style="width:7.42em;height:7.50em;"/></p>
<p class="CDPAlignLeft CDPAlign"/>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 60px"><span>In the preceding image, some of the Monte Carlo predictions belonged to the wrong class, as shown in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-597 image-border" src="assets/e2070f84-669b-4fcd-affa-82c0922332c2.png" style="width:82.67em;height:40.83em;"/></p>
<p style="padding-left: 60px"><strong>Case 2: </strong></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-598 image-border" src="assets/232995e8-3458-4c34-a5b7-05ddc7da72ca.png" style="width:7.75em;height:7.83em;"/></p>
<p style="padding-left: 60px"><span>In the preceding image, some of the Monte Carlo predictions belonged to th</span>e wrong class, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-599 image-border" src="assets/bfd0ab71-71a0-45b9-81b6-8c389e0b4cd8.png" style="width:82.58em;height:40.75em;"/></p>
<p style="padding-left: 60px"><strong>Case 3:</strong></p>
<p style="padding-left: 60px">In the following case, average prediction is incorrect, but <span><span>some samples were </span></span>correctly predicted:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-600 image-border" src="assets/01afa093-37b6-469d-b53f-02db3dd06cf1.png" style="width:7.50em;height:7.50em;"/></p>
<p style="padding-left: 60px"><span>For the preceding image, we obtained the following histogram: </span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/efffb17d-c64b-4d65-a4cd-621a1eb9fff3.png"/></p>
<p style="padding-left: 60px"><strong>Case 4:</strong></p>
<p style="padding-left: 60px">Obviously, we will get cases where we didn't predict correctly for any sample:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-603 image-border" src="assets/1775ee39-0175-48a1-961c-99fa49b20639.png" style="width:7.50em;height:7.67em;"/></p>
<p style="padding-left: 60px">For this image, we obtained the following histogram: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-604 image-border" src="assets/06c3c145-42c9-4d7e-b3f6-40442e7fbc36.png" style="width:82.75em;height:40.92em;"/></p>
<ol start="11">
<li>Finally, visualize the posterior of weights in the network. In the following plot we are showing both the posterior mean and standard deviation of the different weights in the network:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-605 image-border" src="assets/ec78307f-11b7-4edc-a408-17e9f7d78b43.png" style="width:38.42em;height:18.67em;"/></p>
<p>Having a distribution on weights enables us to develop predictions for the same image, which is extremely useful in developing a confidence interval around our predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Neural networks, as we know, are great for point predictions, but can't help us identify the uncertainty in their predictions. On the other hand, Bayesian learning is great for quantifying uncertainty, but doesn't scale well in multiple dimensions or problems with big unstructured datasets such as images.</p>
<p>In this chapter, we looked at how we can combine neural networks with Bayesian learning using Bayesian neural networks.</p>
<p>We used the dataset of German Traffic Signs to develop a Bayesian neural network classifier using Google's recently released tool: TensorFlow probability. TF probability provides high-level APIs and functions to perform Bayesian modeling and inference.</p>
<p>We trained the Lenet model on the dataset. Finally, we used Monte Carlo to sample from the posterior of the parameters of the network to obtain predictions for each sample of the test dataset to quantify uncertainty.</p>
<p>However, we have only scratched the surface in terms of the complexity of Bayesian neural networks. If we want to develop safe AI, then understanding uncertainty in our predictions is of the utmost importance.</p>
<p>In the next chapter, we will learn about a new concept in machine learning known as autoencoders. We will look at how to detect credit card fraud using them. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is TensorFlow probability?</li>
<li>What is Variational inference and why is it important?</li>
<li>What is KL divergence?</li>
<li>What do we mean by prior and posterior on the weights of neural networks?</li>
</ol>


            </article>

            
        </section>
    </body></html>