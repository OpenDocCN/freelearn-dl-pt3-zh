- en: Understanding Convolutional Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解卷积网络
- en: In this chapter, we'll discuss **Convolutional Neural Networks** (**CNNs**)
    and their applications in **Computer Vision** (**CV**). CNNs started the modern
    deep learning revolution. They are at the base of virtually all recent CV advancements,
    including **Generative Adversarial Networks** (**GANs**), object detection, image
    segmentation, neural style transfer, and much more. For this reason, we believe
    CNNs deserve an in-depth look that's beyond our basic understanding of them.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论**卷积神经网络**（**CNNs**）及其在**计算机视觉**（**CV**）中的应用。CNNs启动了现代深度学习的革命。它们是几乎所有最近CV进展的基础，包括**生成对抗网络**（**GANs**）、目标检测、图像分割、神经风格迁移等。因此，我们认为CNNs值得深入探讨，超越我们对它们的基本理解。
- en: To do this, we'll start with a short recap of the CNN building blocks, that
    is, the convolutional and pooling layers. We'll discuss the various types of convolutions
    in use today since they are reflected in a large number of CNN applications. We'll
    also learn how to visualize the internal state of CNNs. Then, we'll focus on regularization
    techniques and implement a transfer learning example.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将从简短回顾CNN的构建块开始，也就是卷积层和池化层。我们将讨论今天使用的各种类型的卷积，因为它们在大量的CNN应用中有所体现。我们还将学习如何可视化CNN的内部状态。接着，我们将重点讲解正则化技术并实现一个迁移学习的例子。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding CNNs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解CNN
- en: Introducing transfer learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入迁移学习
- en: Understanding CNNs
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解CNN
- en: 'In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The* *Nuts and
    Bolts of Neural Networks,* we discussed that many NN operations have solid mathematical
    foundations, and convolutions are no exception. Let''s start by defining the mathematical
    convolution:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml)中，*《神经网络的基础》*，我们讨论了许多NN操作都有坚实的数学基础，卷积也不例外。让我们从定义数学卷积开始：
- en: '![](img/f3bce005-8da4-479a-bb99-b56cc6234765.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f3bce005-8da4-479a-bb99-b56cc6234765.png)'
- en: 'Here, we have the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: The convolution operation is denoted with *.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积运算用*表示。
- en: '*f* and *g* are two functions with a common parameter, *t*.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*f*和*g*是具有共同参数*t*的两个函数。'
- en: The result of the convolution is a third function, *s(t)* (not just a single
    value).
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积的结果是第三个函数，*s(t)*（而不仅仅是一个单一的值）。
- en: The convolution of *f* and *g* at value *t* is the integral of the product of *f(t)*
    and the reversed (mirrored) and shifted value of *g(t-τ)*, where *t-τ* represents
    the shift. That is, for a single value of *f* at time *t*, we shift *g* in the
    range ![](img/27cdbe02-f140-4acb-bcea-3114eae42a4a.png) and we compute the product
    *f(t)**g(t-τ)* continuously because of the integral. The integral (and hence the
    convolution) is equivalent to the area under the curve of the product of the two
    functions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*f*和*g*在值*t*处的卷积是*f(t)*与*g(t-τ)*的逆向（镜像）和位移后的值的乘积的积分，其中*t-τ*表示位移。也就是说，对于时间*t*上*f*的单个值，我们在范围内平移*g*
    ![](img/27cdbe02-f140-4acb-bcea-3114eae42a4a.png)，并且由于积分的缘故，我们不断计算乘积*f(t)**g(t-τ)*。积分（因此卷积）等同于两个函数乘积曲线下的面积。'
- en: 'This is best illustrated in the following diagram:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示最能说明这一点：
- en: '![](img/c8d8a4c1-7a56-460c-a0c3-7460343330a1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8d8a4c1-7a56-460c-a0c3-7460343330a1.png)'
- en: 'Left: a convolution, where *g* is shifted and reversed; right: a step-by-step
    illustration of a convolution operation'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 左：一个卷积，其中*g*被平移并反转；右：卷积运算的逐步演示
- en: In the convolution operation, *g* is shifted and reversed in order to preserve
    the operation's commutative property. In the context of CNNs, we can ignore this
    property and we can implement it without reversing *g*. In this case, the operation
    is called cross-correlation. These two terms are used interchangeably.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积运算中，为了保持运算的交换性，*g*被平移并反转。在CNN的上下文中，我们可以忽略这个特性，并且我们可以在不反转*g*的情况下实现它。在这种情况下，这个运算称为互相关。这两个术语是可以互换使用的。
- en: 'We can define the convolution for discrete (integer) values of *t* with the
    following formula (which is very similar to the continuous case):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下公式为离散（整数）值*t*定义卷积（这与连续情况非常相似）：
- en: '![](img/93ecc38f-ba32-4cc5-b628-ec8f975db95b.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93ecc38f-ba32-4cc5-b628-ec8f975db95b.png)'
- en: 'We can also generalize it to the convolution of functions with two shared input
    parameters, *i* and *j*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将其推广到具有两个共享输入参数*i*和*j*的函数卷积：
- en: '![](img/16ae140a-3930-4c85-99f1-04f0c49ff727.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/16ae140a-3930-4c85-99f1-04f0c49ff727.png)'
- en: We can derive the formula in a similar manner for three parameters.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以以类似的方式推导出三个参数的公式。
- en: 'In CNNs, the function *f* is the input of the convolution operation (also referred
    to as the convolutional layer). Depending on the number of input dimensions, we
    have 1D, 2D, or 3D convolutions. А time series input is a 1D vector, an image
    input is a 2D matrix, and a 3D point cloud is a 3D tensor. The function *g*, on
    the other hand, is called a kernel (or filter). It has the same number of dimensions
    as the input data and it is defined by a set of learnable weights. For example,
    a filter of size *n* for a 2D convolution is an *n×n* matrix. The following diagram
    illustrates a 2D convolution with a 2×2filter applied over a single 3×3 slice:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积神经网络（CNNs）中，函数 *f* 是卷积操作的输入（也称为卷积层）。根据输入维度的数量，我们有 1D、2D 或 3D 卷积。时间序列输入是 1D
    向量，图像输入是 2D 矩阵，3D 点云是 3D 张量。另一方面，函数 *g* 被称为核（或滤波器）。它的维度与输入数据相同，并由一组可学习的权重定义。例如，2D
    卷积的大小为 *n* 的滤波器是一个 *n×n* 矩阵。下图展示了 2D 卷积，使用 2×2 滤波器应用于单个 3×3 切片：
- en: '![](img/a52c6aef-cfd2-4f27-bc85-20649b591ca1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a52c6aef-cfd2-4f27-bc85-20649b591ca1.png)'
- en: 2D convolution with a 2×2filter applied over a single 3×3 slice
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 卷积，使用 2×2 滤波器应用于单个 3×3 切片
- en: 'The convolution works as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积的工作原理如下：
- en: We slide the filter along all of the dimensions of the input tensor.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将滤波器滑动通过输入张量的所有维度。
- en: At every input position, we multiply each filter weight by its corresponding
    input tensor cell at the given location. The input cells, which contribute to
    a single output cell, are called **receptive fields**. We sum all of these values
    to produce the value of a single output cell.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个输入位置，我们将每个滤波器权重与给定位置对应的输入张量单元相乘。贡献到单个输出单元的输入单元称为 **感受野**。我们将所有这些值相加，得到单个输出单元的值。
- en: Unlike fully-connected layers, where each output unit gathers information from
    all of the inputs, the activation of a convolution output cell is determined by
    the inputs in its receptive field. This principle works best for hierarchically
    structured data such as images. For example, neighboring pixels form meaningful
    shapes and objects, but a pixel at one end of the image is unlikely to have a
    relationship with a pixel at another end. Using a fully-connected layer to connect
    all of the input pixels with each output unit is like asking the network to find
    a needle in a haystack. It has no way of knowing whether an input pixel is in
    the receptive field of the output unit or not.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 与全连接层不同，在全连接层中，每个输出单元都从所有输入中收集信息，而卷积输出单元的激活是由其感受野中的输入决定的。这个原理最适用于层次结构数据，例如图像。例如，邻近的像素形成有意义的形状和物体，但图像一端的像素与另一端的像素之间不太可能有关系。使用全连接层将所有输入像素与每个输出单元连接，就像是让网络在大海捞针。它无法知道某个输入像素是否在输出单元的感受野内。
- en: The filter highlights some particular features in the receptive field. The output
    of the operation is a tensor (known as a feature map), which marks the locations
    where the feature is detected. Since we apply the same filter throughout the input
    tensor, the convolution is translation invariant; that is, it can detect the same
    features, regardless of their location on the image. However, the convolution
    is neither rotation invariant (it is not guaranteed to detect a feature if it's
    rotated), nor scale invariant (it is not guaranteed to detect the same artifact
    in different scales).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该滤波器在感受野中突出显示某些特定特征。操作的输出是一个张量（称为特征图），标记了检测到特征的位置。由于我们在整个输入张量上应用相同的滤波器，卷积是平移不变的；也就是说，它可以检测到相同的特征，无论它们在图像中的位置如何。然而，卷积既不是旋转不变的（如果特征旋转，它不保证检测到该特征），也不是尺度不变的（它不保证在不同尺度下检测到相同的特征）。
- en: 'In the following diagram, we can see examples of 1D and 3D convolutions (we''ve
    already introduced an example of a 2D convolution):'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，我们可以看到 1D 和 3D 卷积的示例（我们已经介绍了 2D 卷积的一个示例）：
- en: '![](img/6f84dd7f-8286-4cd7-9442-c79457e6322e.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f84dd7f-8286-4cd7-9442-c79457e6322e.png)'
- en: '1D convolution: The filter (denoted with multicolored lines) slides over a
    single axis; 3D convolution: The filter (denoted with dashed lines) slides over
    three axes'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 1D 卷积：滤波器（用多色线表示）沿单一轴滑动；3D 卷积：滤波器（用虚线表示）沿三个轴滑动
- en: The CNN convolution can have multiple filters, highlighting different features,
    which results in multiple output feature maps (one for each filter). It can also
    gather input from multiple feature maps, for example, the output of a previous
    convolution. The combination of feature maps (input or output) is called a volume.
    In this context, we can also refer to the feature maps as slices. Although the
    two terms refer to the same thing, we can think of the slice as part of the volume,
    whereas the feature map highlights its role as, well, a feature map.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 卷积可以有多个滤波器，用来突出不同的特征，这样就会生成多个输出特征图（每个滤波器一个）。它也可以从多个特征图中收集输入，例如，来自先前卷积的输出。特征图（输入或输出）的组合称为体积。在这个上下文中，我们也可以将特征图称为切片。虽然这两个术语指的是同一事物，但我们可以把切片看作是体积的一部分，而特征图则突出了它作为特征图的角色。
- en: As we mentioned earlier in this section, each volume (as well as the filter)
    is represented by a tensor. For example, a red, green, and blue (RGB) image is
    represented by a 3D tensor of three 2D slices (one slice per color channel). But
    in the context of CNNs, we add one more dimension for the sample index in the
    mini-batch. Here, a 1D convolution would have 3D input and output tensors. Their
    axes can be in either *NCW* or *NWC* order, where *N* is the index of the sample
    in the mini-batch, *C* is the index of the depth slice in the volume, and *W*
    is the vector size of each sample. In the same way, a 2D convolution will be represented
    by *NCHW* or *NHWC* tensors, where *H* and *W* are the height and width of the
    slices. A 3D convolution will have an *NCLHW* or *NLHWC* order, where *L* stands
    for the depth of the slice.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节前面提到的，每个体积（以及滤波器）都是通过张量表示的。例如，一个红、绿、蓝（RGB）图像通过一个包含三个 2D 切片（每个颜色通道一个切片）的
    3D 张量表示。但是，在 CNN 的上下文中，我们为每个样本的 mini-batch 添加了一个维度。在这种情况下，1D 卷积将具有 3D 输入和输出张量。它们的轴可以是
    *NCW* 或 *NWC* 顺序，其中 *N* 是 mini-batch 中样本的索引，*C* 是体积中深度切片的索引，*W* 是每个样本的向量大小。以相同的方式，2D
    卷积将由 *NCHW* 或 *NHWC* 张量表示，其中 *H* 和 *W* 分别是切片的高度和宽度。3D 卷积将采用 *NCLHW* 或 *NLHWC*
    顺序，其中 *L* 表示切片的深度。
- en: We use 2D convolutions to work with RGB images. However, we may consider the
    three colors an additional dimension, hence making the RGB image 3D. Why didn't
    we use 3D convolutions, then? The reason for this is that, even though we can
    think of the input as 3D, the output is still a 2D grid. Had we used 3D convolution,
    the output would also be 3D, which doesn't carry any meaning in the case of 2D
    images.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 2D 卷积来处理 RGB 图像。然而，我们也可以将三种颜色视为一个额外的维度，从而将 RGB 图像视为 3D。那么为什么我们不使用 3D 卷积呢？原因是，尽管我们可以把输入看作是
    3D 的，但输出仍然是一个 2D 网格。如果我们使用 3D 卷积，输出也会是 3D 的，而在 2D 图像的情况下，这没有任何意义。
- en: Let's say we have *n* input and *m* output slices. In this case, we'll apply
    *m* filters across the set of *n* input slices. Each filter will generate a unique
    output slice that highlights the feature that was detected by the filter (*n*
    to *m* relationship).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有 *n* 个输入切片和 *m* 个输出切片。在这种情况下，我们会对 *n* 个输入切片应用 *m* 个滤波器。每个滤波器将生成一个独特的输出切片，突出显示该滤波器检测到的特征（*n*
    到 *m* 的关系）。
- en: 'Depending on the relationship of the input and output slice, we get cross-channel
    and depth-wise convolutions, as illustrated in the following diagram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据输入和输出切片的关系，我们可以得到跨通道卷积和深度卷积，如下图所示：
- en: '![](img/56593c82-220f-46c9-a73f-346edd79558d.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/56593c82-220f-46c9-a73f-346edd79558d.png)'
- en: 'Left: cross-channel convolution; right: depthwise convolution'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 左图：跨通道卷积；右图：深度卷积
- en: 'Let''s discuss their properties:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论它们的属性：
- en: '**Cross-channel convolutions**: One output slice receives input from all of
    the input slices (*n*-to-one relationship). With multiple output slices, the relationship
    becomes *n*-to-*m*. In other words, each input slice contributes to the output
    of each output slice. Each pair of input/output slices uses a separate filter
    slice that''s unique to that pair. Let''s denote the size of the filter (equal width
    and height) with *F*, the depth of the input volume with *C[in]*, and the depth
    of the output volume with *C[out]*. With this, we can compute the total number
    of weights, *W*, in a 2D convolution with the following equation:'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**跨通道卷积**：一个输出切片从所有输入切片接收输入（*n* 到一的关系）。当有多个输出切片时，关系变为 *n* 到 *m*。换句话说，每个输入切片都会对每个输出切片的输出产生贡献。每对输入/输出切片都会使用一个与该对独特的滤波器切片。我们用
    *F* 来表示滤波器的大小（宽度和高度相等），用 *C[in]* 来表示输入体积的深度，用 *C[out]* 来表示输出体积的深度。有了这些，我们可以通过以下公式计算
    2D 卷积的总权重数 *W*：'
- en: '![](img/cbefc433-a0d8-46ef-98c5-9392bf144203.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cbefc433-a0d8-46ef-98c5-9392bf144203.png)'
- en: Here, +1 represents the bias weight for each filter. Let's say we have three
    slices and want to apply four 5*×*5 filters to them. If we did this, the convolution
    filter would have a total of *(3*5*5 + 1) * 4 = 304* weights, four output slices
    (output volume with a depth of 4), and one bias per slice. The filter for each
    output slice will have three 5*×*5 filter patches for each of the three input
    slices and one bias for a total of 3*5*5 + 1 = 76 weights.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，+1 表示每个滤波器的偏置权重。假设我们有三个切片，并且希望对它们应用四个 5*×*5 的滤波器。如果这样做，卷积滤波器将有总计 *(3*5*5
    + 1) * 4 = 304* 个权重，四个输出切片（深度为4的输出体积），每个切片有一个偏置。每个输出切片的滤波器将对每个输入切片应用三个 5*×*5 的滤波器补丁，并有一个偏置，总共有
    3*5*5 + 1 = 76 个权重。
- en: '**Depthwise convolutions**: Each output slice receives input from a single
    input slice. It''s a kind of reversal of the previous case. In its most simple
    form, we apply a filter over a single input slice to produce a single output slice.
    In this case, the input and output volumes have the same depth, that is, *C*.
    We can also specify a **channel multiplier** (an integer, *m*), where we apply *m* filters
    over a single output slice to produce *m* output slices. This is a case of a one-to-*m*
    relationship. In this case, the total number of output slices is *n * m*. We can
    compute the number of weights, *W*, in a 2D depthwise convolution with the following
    formula:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Depthwise convolutions**：每个输出切片仅接收来自单个输入切片的输入。这是之前情况的逆转。在最简单的形式中，我们对单个输入切片应用一个滤波器，生成一个单一的输出切片。在这种情况下，输入和输出的体积具有相同的深度，即
    *C*。我们还可以指定一个 **通道倍增器**（一个整数，*m*），我们在单个输出切片上应用 *m* 个滤波器，从而生成 *m* 个输出切片。这是一个一对-*m*
    的关系。在这种情况下，输出切片的总数是 *n * m*。我们可以用以下公式计算 2D 深度卷积中的权重 *W*：'
- en: '![](img/86e8086d-cbb7-430e-b548-164c7b897256.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/86e8086d-cbb7-430e-b548-164c7b897256.png)'
- en: Here, *m* is the channel multiplier and *+C* represents the biases of each output
    slice.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*m* 是通道倍增器，*+C* 表示每个输出切片的偏置。
- en: 'The convolution operation is also described by two other parameters:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作还通过其他两个参数来描述：
- en: '**S****tride** is the number of positions that we slide the filter over on
    the input slice on each step. By default, the stride is 1\. If it''s larger than
    1, then we call it a **stride convolution**. The largest stride increases the
    receptive field of the output neurons. With stride 2, the size of the output slice
    will be roughly four times smaller than the input. In other words, one output
    neuron will cover the area, which is four times larger, compared to a stride 1
    convolution. The neurons in the following layers will gradually capture input
    from the larger regions of the input image.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Stride** 是在每一步中，我们滑动滤波器在输入切片上的位置数。默认情况下，步幅为 1。如果大于 1，则称之为 **步幅卷积**。较大的步幅增加了输出神经元的感受野。步幅为
    2 时，输出切片的大小大约是输入的四分之一。换句话说，步幅为 1 的卷积中，一个输出神经元的感受野是步幅为 2 时的四倍。后续层中的神经元将逐渐捕获来自输入图像较大区域的信息。'
- en: '**Padding** the edges of the input slice with rows and columns of zeros before
    the convolution operation. The most common way to use padding is to produce output
    with the same dimensions as the input. The newly padded zeros will participate
    in the convolution operation with the slice, but they won''t affect the result.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Padding** 在卷积操作之前，用零填充输入切片的边缘，增加行和列。最常见的填充方式是使输出的尺寸与输入相同。新增的零将参与与切片的卷积操作，但不会影响结果。'
- en: 'Knowing the input dimensions and the filter size, we can compute the dimensions
    of the output slices. Let''s say the size of the input slice is *I* (equal height
    and width), the size of the filter is *F*, the stride is *S*, and the padding is *P*.
    Here, the size, *O*, of the output slice is given by the following equation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 知道输入的尺寸和滤波器的大小后，我们可以计算输出切片的尺寸。假设输入切片的大小为 *I*（高度和宽度相等），滤波器的大小为 *F*，步幅为 *S*，填充为
    *P*。此时，输出切片的大小 *O* 由以下公式给出：
- en: '![](img/288c3c31-1000-4908-8163-171bbc19a0d3.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/288c3c31-1000-4908-8163-171bbc19a0d3.png)'
- en: 'Besides stride convolutions, we can also use **pooling** operations to increase
    the receptive field of the deeper neurons and reduce the size of the future slices. The
    pooling splits the input slice into a grid, where each grid cell represents a
    receptive field of a number of neurons (just like it does with the convolution).
    Then, a pooling operation is applied over each cell of the grid. Similar to convolution,
    pooling is described by the stride, *S*, and the size of the receptive field, *F*.
    If the size of input slice is *I*, then the formula for the output size of the
    pooling is as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 除了步幅卷积，我们还可以使用**池化**操作来增加深层神经元的感受野，并减少未来切片的大小。池化将输入切片划分为网格，每个网格单元代表多个神经元的感受野（就像卷积操作一样）。然后，在网格的每个单元上应用池化操作。类似于卷积，池化通过步幅，*S*，和感受野的大小，*F*，来描述。如果输入切片的大小为
    *I*，则池化输出大小的公式如下：
- en: '![](img/643541b1-b7a2-44b3-a546-cd28d1b32542.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/643541b1-b7a2-44b3-a546-cd28d1b32542.png)'
- en: 'In practice, only two combinations are used. The first is a 2*×*2 receptive field
    with stride 2, while the second is a 3*×*3 receptive field with stride 2 (overlapping). The
    most common pooling operations are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，通常只使用两种组合。第一种是步幅为2的2*×*2感受野，第二种是步幅为2的3*×*3感受野（重叠）。最常见的池化操作如下：
- en: '**Max pooling**: This propagates the maximum value of the input values of the
    receptive field.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化**: 这个操作传播感受野内输入值的最大值。'
- en: '**Average pooling**: This propagates the average value of the inputs in the
    receptive field.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均池化**: 这个操作传播感受野内输入值的平均值。'
- en: '**Global Average Pooling** (**GAP**): This is the same as average pooling,
    but the pooling region has the same size as the feature map, *I×I*. GAP performs
    an extreme type of dimensionality reduction: the output is a single scalar, which
    represents the average value of the whole feature map.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局平均池化** (**GAP**): 这与平均池化相同，但池化区域与特征图的大小相同，*I×I*。GAP执行一种极端的降维操作：输出是一个单一的标量，表示整个特征图的平均值。'
- en: Typically, we would alternate one or more convolutional layers with one pooling
    (or stride convolution) layer. In this way, the convolutional layers can detect
    features at every level of the receptive field size because the aggregated receptive
    field size of deeper layers is larger than the ones at the beginning of the network.
    The deeper layers also have more filters (hence, a higher volume depth) compared
    to the initial ones. The feature detector at the beginning of the network works
    on a small receptive field. It can only detect a limited number of features, such
    as edges or lines, that are shared among all classes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们会交替使用一个或多个卷积层与一个池化（或步幅卷积）层。通过这种方式，卷积层可以在每个感受野大小的层次上检测特征，因为较深层的感受野大小要大于网络初始层的感受野。深层的层也比最初的层拥有更多的滤波器（因此，具有更高的体积深度）。网络初始阶段的特征检测器工作在较小的感受野上，它只能检测到有限的特征，如边缘或线条，这些特征在所有类别中都有共享。
- en: On the other hand, a deeper layer would detect more complex and numerous features.
    For example, if we have multiple classes, such as cars, trees, or people, each
    would have its own set of features, such as tires, doors, leaves, and faces. This
    would require more feature detectors. The output of the final convolution (or
    pooling) is "translated" to the target labels by adding one or more fully connected
    layers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，较深的层会检测到更复杂和更多的特征。例如，如果我们有多个类别，如汽车、树木或人，每个类别都会有一组特征，如轮胎、车门、树叶和面孔。这将需要更多的特征检测器。最终卷积（或池化）的输出通过添加一个或多个全连接层“翻译”到目标标签。
- en: Now that we have had an overview of convolutions, pooling operations, and CNNs,
    in the next section, we'll focus on different types of convolution operations.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对卷积、池化操作和卷积神经网络（CNN）有了一个概述，在下一节中，我们将重点讨论不同类型的卷积操作。
- en: Types of convolutions
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积类型
- en: So far, we've discussed the most common type of convolution. In the upcoming
    sections, we'll talk about a few of its variations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了最常见的卷积类型。在接下来的章节中，我们将讨论它的一些变体。
- en: Transposed convolutions
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转置卷积
- en: In the convolutional operations we've discussed so far, the output dimensions
    are either equal or smaller than the input dimensions. In contrast, transposed
    convolutions (first proposed in *Deconvolutional Networks* by Matthew D. Zeiler,
    Dilip Krishnan, Graham W. Taylor, and Rob Fergus: [https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf))
    allow us to upsample the input data (their output is larger than the input). This
    operation is also known as **deconvolution**, **fractionally strided convolution**,
    or **sub-pixel convolution**. These names can sometimes lead to confusion. To
    clarify things, note that the transposed convolution is, in fact, a regular convolution
    with a slightly modified input slice or convolutional filter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'For the longer explanation, we''ll start with a 1D regular convolution over
    a single input and output slice:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6a3a798-3dc8-441a-be05-3b5043baa376.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
- en: 1D regular convolution
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: It uses a filter with a size of 4, stride 2, and padding 2 (denoted with gray
    in the preceding diagram). The input is a vector of size 6 and the output is a
    vector of size 4\. The filter, a vector **f** = [1, 2, 3, 4], is always the same,
    but it's denoted with different colors for each position we apply it to. The respective
    output cells are denoted with the same color. The arrows show which input cells
    contribute to one output cell.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: The example that is being discussed in this section is inspired by the paper; *Is
    the deconvolution layer the same as a convolutional layer? *([https://arxiv.org/abs/1609.07009](https://arxiv.org/abs/1609.07009)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll discuss the same example (1D, single input and output slices,
    filter of size 4, padding 2, and stride 2), but for transposed convolution. The
    following diagram shows two ways we can implement it:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31980d51-3120-43f3-bf47-ac3ec266ef8a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
- en: 'Left: A convolution with stride 2, applied with the transposed filter **f**.
    The 2 pixels at the beginning and the end of the output are cropped; right: A
    convolution with stride 0.5, applied over input data, padded with subpixels. The
    input is filled with 0-valued pixels (gray).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss them in detail:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first case, we have a regular convolution with stride 2 and a filter represented
    as transposed row matrix (equivalent to column matrix) with size 4: ![](img/88d1c5eb-7650-46c4-b378-438a9f3fa008.png) (shown
    in the preceding diagram, left). Note that the stride is applied over the output
    layer as opposed to the regular convolution, where we stride over the input. By
    setting the stride larger than 1, we can increase the output size, compared to
    the input. Here, the size of the input slice is *I*, the size of the filter is *F*,
    the stride is *S*, and the input padding is *P*. Due to this, the size, *O*, of
    the output slice of a transposed convolution is given by the following formula:'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5c6c084c-a942-44ce-adbd-190f97c82f4e.png)'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: In this scenario, an input of size 4 produces an output of size *2*(4 - 1) +
    4 - 2*2 = 6*. We also crop the two cells at the beginning and the end of the output
    vector because they only gather input from a single input cell.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，大小为4的输入产生的输出大小为*2*(4 - 1) + 4 - 2*2 = 6*。我们还会裁剪输出向量的开始和结束部分的两个单元，因为它们只收集来自单个输入单元的信息。
- en: In the second case, the input is filled with imaginary 0-valued subpixels between
    the existing ones (shown in the preceding diagram, right). This is where the name
    subpixel convolution comes from. Think of it as padding but within the image itself
    and not only along the borders. Once the input has been transformed in this way,
    a regular convolution is applied.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二种情况下，输入填充了存在的像素之间的虚拟0值子像素（如前面的图所示，右侧）。这就是“子像素卷积”这一名称的由来。可以将其视为一种填充，但它发生在图像内部，而不仅仅是在边缘上。一旦输入以这种方式被转换，就会应用常规卷积。
- en: Let's compare the two output cells, *o*[1] and *o*[3], in both scenarios. As
    shown in the preceding diagram, in either case, *o*[1] receives input from the
    first and the second input cells and *o*[3] receives input from the second and
    third cells. In fact, the only difference between these two cases is the index
    of the weight, which participates in the computation. However, the weights are
    learned during training and, because of this, the index is not important. Therefore,
    the two operations are equivalent.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较两种情况中的两个输出单元，*o*[1]和*o*[3]。如前面的图所示，在这两种情况下，*o*[1]接收来自第一个和第二个输入单元的信息，*o*[3]接收来自第二个和第三个单元的信息。实际上，这两种情况之间唯一的区别是参与计算的权重索引。然而，权重是在训练过程中学习的，因此，索引并不重要。因此，这两种操作是等效的。
- en: 'Next, let''s take a look at a 2D transposed convolution from a subpixel point
    of view (the input is at the bottom). As with the 1D case, we insert 0-valued
    pixels and padding in the input slice to achieve upsampling:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从子像素的角度来看一个2D转置卷积（输入在底部）。与1D情况类似，我们在输入切片中插入0值像素和填充，以实现上采样：
- en: '![](img/1d3ef15e-d1d6-407d-bcad-b696e2b6f812.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1d3ef15e-d1d6-407d-bcad-b696e2b6f812.png)'
- en: The first three steps of a 2D transpose convolution with padding 1 and stride
    2: Source: https://github.com/vdumoulin/conv_arithmetic, https://arxiv.org/abs/1603.07285
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 具有填充为1和步幅为2的2D转置卷积的前三个步骤：来源：[https://github.com/vdumoulin/conv_arithmetic](https://github.com/vdumoulin/conv_arithmetic)，[https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)
- en: The backpropagation operation of a regular convolution is a transposed convolution.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 常规卷积的反向传播操作是转置卷积。
- en: 1×1 convolutions
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1×1卷积
- en: A 1*×*1 (or pointwise) convolution is a special case of convolution where each
    dimension of the convolution filter is of size 1 (1*×*1 in 2D convolutions and
    1*×*1*×*1 in 3D). At first, this doesn't make sense—a 1*×*1 filter doesn't increase
    the receptive field size of the output neurons. The result of such a convolution would
    be pointwise scaling. But it can be useful in another way—we can use them to change
    the depth between the input and output volumes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 1×1（或逐点）卷积是卷积的一种特殊情况，其中卷积滤波器的每个维度的大小为1（2D卷积中的1×1和3D卷积中的1×1×1）。起初，这似乎没有意义——1×1的滤波器并不会增加输出神经元的感受野大小。此类卷积的结果将是逐点缩放。但它在另一个方面是有用的——我们可以用它们来改变输入和输出体积之间的深度。
- en: To understand this, let's recall that, in general, we have an input volume with
    a depth of *D* slices and *M* filters for *M* output slices. Each output slice
    is generated by applying a unique filter over all of the input slices. If we use
    a 1*×*1 filter and *D != M*, we'll have output slices of the same size, but with
    different volume depths. At the same time, we won't change the receptive field
    size between the input and output. The most common use case is to reduce the output
    volume, or *D > M* (dimension reduction), nicknamed the "bottleneck" layer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这一点，我们回顾一下，通常情况下，我们有一个深度为*D*切片的输入体积，以及*M*个滤波器用于生成*M*个输出切片。每个输出切片都是通过将唯一的滤波器应用于所有输入切片生成的。如果我们使用1×1的滤波器且*D
    != M*，我们会得到相同大小的输出切片，但深度不同。同时，我们不会改变输入和输出之间的感受野大小。最常见的用例是减少输出体积，或*D > M*（降维），俗称“瓶颈”层。
- en: Depth-wise separable convolutions
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度可分离卷积
- en: An output slice in a cross-channel convolution receives input from all of the
    input slices using a single filter. The filter tries to learn features in a 3D
    space, where two of the dimensions are spatial (the height and width of the slice)
    and the third is the channel. Therefore, the filter maps both spatial and cross-channel
    correlations.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '**Depthwise separable convolutions** (**DSC**, *Xception: Deep Learning with
    Depthwise Separable Convolutions*, [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)) can
    completely decouple cross-channel and spatial correlations. A DSC combines two
    operations: a depthwise convolution and a 1*×*1 convolution. In a depthwise convolution,
    a single input slice produces a single output slice, so it only maps spatial (and
    not cross-channel) correlations. With 1*×*1 convolutions, we have the opposite.
    The following diagram represents the DSC:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79686796-d667-416c-a52f-9c40fd7631d4.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: A depthwise separable convolution
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The DSC is usually implemented without non-linearity after the first (depthwise)
    operation.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Let's compare the standard and depthwise separable convolutions. Imagine that
    we have 32 input and output channels and a filter with a size of 3*×*3\. In a
    standard convolution, one output slice is the result of applying one filter for
    each of the 32 input slices for a total of *32 * 3 * 3 = 288* weights (excluding
    bias). In a comparable depthwise convolution, the filter has only *3 * 3 = 9*
    weights and the filter for the 1*×*1 convolution has *32 * 1 * 1 = 32* weights.
    The total number of weights is *32 + 9 = 41*. Therefore, the depthwise separable
    convolution is faster and more memory-efficient compared to the standard one.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: Dilated convolutions
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall the discrete convolution formula we introduced at the beginning of the *A
    quick recap of CNNs* section. To explain dilated convolutions (*Multi-Scale Context
    Aggregation by Dilated Convolutions*, [https://arxiv.org/abs/1511.07122](https://arxiv.org/abs/1511.07122)),
    let''s start with the following formula:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61fd057f-883a-456b-85d4-d77cd8589e36.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'We''ll denote the dilated convolution with **[l]*, where *l* is a positive
    integer value called the dilation factor. The key is the way we apply the filter
    over the input. Instead of applying the *n×n* filter over the *n×n* receptive
    field, we apply the same filter sparsely over a receptive field of size *(n*l-1)×(n*l-1)*.
    We still multiply each filter weight by one input slice cell, but these cells
    are at a distance of *l* away from each other. The regular convolution is a special
    case of dilated convolution with *l=1*. This is best illustrated with the following
    diagram:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0807af5e-ce74-4b7d-8399-52a33bab68d4.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'A dilated convolution with a dilation factor of l=2: Here, the first two steps
    of the operation are displayed. The bottom layer is the input while the top layer
    is the output. Source: https://github.com/vdumoulin/conv_arithmetic'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Dilated convolutions can increase the receptive field size exponentially without
    losing resolution or coverage. We can also increase the receptive field with stride
    convolutions or pooling but at the cost of resolution and/or coverage. To understand
    this, let's imagine that we have a stride convolution with stride *s>1*. In this
    case, the output slice is *s* times smaller than the input (loss of resolution).
    If we increase *s>n* further (*n* is the size of either the pooling or convolutional
    kernel), we get loss of coverage because some of the areas of the input slice
    will not participate in the output at all. Additionally, dilated convolutions
    don't increase the computation and memory costs because the filter uses the same
    number of weights as the regular convolution.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Improving the efficiency of CNNs
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main reasons for the advances in recent **Deep Learning** (**DL**)
    is its ability to run **Neural Networks** (**NNs**) very fast. This is in large
    part because of the good match between the nature of NN algorithms and the specifics
    of **Graphical Processing Units** (**GPUs**). In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*, we underscored the importance of matrix multiplication
    in NNs. As a testament to this, it is possible to transform the convolution into
    a matrix multiplication as well. Matrix multiplication is embarrassingly parallel
    (trust me, this is a term—you can Google it!). The computation of each output
    cell is not related to the computation of any other output cell. Therefore, we
    can compute all of the outputs in parallel.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Not coincidentally, GPUs are well suited for highly parallel operations like
    this. On the one hand, a GPU has a high number of computational cores compared
    to a **Central Processing Unit** (**CPU**). Even though a GPU core is faster than
    a CPU one, we can still compute a lot more output cells in parallel. But what's
    even more important is that GPUs are optimized for memory bandwidth, while CPUs
    are optimized for latency. This means that a CPU can fetch small chunks of memory
    very quickly but will be slow when it comes to fetching large chunks. The GPU does
    the opposite. Because of this, in tasks such as large matrix multiplication for
    NNs, the GPU has an advantage.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Besides hardware specifics, we can optimize CNNs on the algorithmic side as
    well. The majority of computational time in a CNN is devoted to the convolutions
    themselves. Although the implementation of the convolution is straightforward
    enough, in practice, there are more efficient algorithms to achieve the same result.
    Although contemporary DL libraries such as TensorFlow or PyTorch shield the developer
    from such details, in this book, we are aiming for a deeper (pun intended) understanding
    of DL.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, in the next section, we'll discuss two of the most popular
    fast convolution algorithms.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Convolution as matrix multiplication
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll describe the algorithm that we use to transform convolutions
    into matrix multiplication, just like how it''s implemented in the cuDNN library
    (*cuDNN: Efficient Primitives for Deep Learning*, [https://arxiv.org/abs/1410.0759](https://arxiv.org/abs/1410.0759)).
    To understand this, let''s assume that we perform a cross-channel 2D convolution
    over an RGB input image. Let''s look at the following table for the parameters
    of the convolution:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Notation** | **Value** |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
- en: '| Mini-batch size | N | 1 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
- en: '| Input feature maps (volume depth) | C | 3 (one for each RGB channel) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
- en: '| Input image height | H | 4 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| Input image width | W | 4 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| Output feature maps (volume depth) | K | 2 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| Filter height | R | 2 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| Filter width | S | 2 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
- en: '| Output feature map height | P | 2 (based on the input/filter sizes) |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
- en: '| Output feature map width | Q | 2 (based on the input/filter sizes) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
- en: 'For the sake of simplicity, we''ll assume we have zero padding and stride 1\.
    We''ll denote the input tensor with *D* and the convolution filter tensor with
    *F*. The matrix convolution works in the following way:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: We unfold the tensors, *D* and *F*, into the ![](img/b51588a3-c401-4c1c-8b20-b91726e14de5.png) and
    ![](img/93fd8178-c6d7-4996-a1dd-94cf21f0a6a4.png) matrices, respectively.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we multiply the matrices to get the output matrix, ![](img/bc1fddcd-c0be-424f-9acb-47604f8d3a17.png).
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We discussed matrix multiplication in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*. Now, let''s focus on the way we can unfold
    the tensors in matrices. The following diagram shows how to do this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa86d420-5efa-43ad-bd02-cecad396cc09.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: Convolution as matrix multiplication; Inspired by https://arxiv.org/abs/1410.0759
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Each feature map has a different color (R, G, B). In the regular convolution,
    the filter has a square shape and we apply it over a square input region. In the
    transformation, we unfold each possible square region of *D* into one column of **D***[m]*.
    Then, we unfold each square component of *F* into one row of **F***[m]*. In this
    way, the input and filter data for each output cell is situated in a single column/row
    of the matrices **D***[m]* and **F***[m]*. This makes it possible to compute the
    output value as a matrix multiplication. The dimensions of the transformed input/filter/output
    are as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: dim(**D***[m]*) = CRS*×*NPQ = 12*×*4
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dim(**F***[m]*) = K*×*CRS = 2*×*12
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dim(**O***[m]*) = K*×*NPQ = 2*×*4
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand this transformation, let''s learn how to compute the first output
    cell with the regular convolution algorithm:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9834db46-e86b-421e-8de9-8c1789e9ed47.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s observe the same formula, but this time, in matrix multiplication
    form:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89ec53d1-8092-46af-9893-2faa7aea4d0f.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
- en: If we compare the components of the two equations, we'll see that they are exactly
    the same. That is, *D*[0,0,0,0] = **D**[m][0,0], *F*[0,0,0,0] = **F**[m][0,0], *D*[0,0,0,1]
    = **D**[m][0,1], *F*[0,0,0,1] = **F**[m][0,1], and so on. We can do the same for
    the rest of the output cells. Therefore, the output of the two approaches is the
    same.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: One disadvantage of the matrix convolution is increased memory usage. In the
    preceding diagram, we can see that some of the input elements are duplicated multiple
    times (up to RS = 4 times, like D4).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Winograd convolutions
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Winograd algorithm (*Fast Algorithms for Convolutional Neural Networks*, [https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308))
    can provide 2 or 3*×* speedup compared to the direct convolution. To explain this,
    we'll use the same notations that we used in the *Convolution as matrix multiplication* section
    but with a *3×3* (*R=S=3*) filter. We'll also assume that the input slices are
    bigger than *4×4* (*H>4, W>4*).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how to compute Winograd convolutions:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the input image into 4*×*4 tiles that overlap with stride 2, as shown
    in the following diagram:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cd4aa445-8508-4aab-bea4-72effd713822.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: The input is split into tiles
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: The tile size can vary, but for the sake of simplicity, we'll only focus on
    4*×*4 tiles.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform each tile using the following two matrix multiplications:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b0268ace-bb0f-44da-bbbe-28165f6b82fa.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, the matrix **D** is the input slice (the one with
    circle values) while **B** is a special matrix, which results from the specifics
    of the Winograd algorithm (you can find more information about them in the paper
    linked at the beginning of this section).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform the filter using the following two matrix multiplications:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/72a0e638-cfe5-4a34-a05e-83cfe8af3884.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, the matrix **F** (the one with dot values) is the
    3*×*3 convolution filter between one input and one output slice. **G** and its
    transpose, ![](img/c62285e7-2871-435d-92d0-75ab2fc9df82.png), are, again, special
    matrices, which result from the specifics of the Winograd algorithm. Note that
    the transformed filter matrix, **F***[t]*, has the same dimensions as the input
    tile, **D***[t]*.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the transformed output as an **element-wise** multiplication (the ![](img/8c5ea580-95d6-4abf-b93b-a2dc5ba2824b.png)symbol)
    of the transformed input and filter:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8c2d3f58-125b-4bc5-9cdc-0d72c1e1b1a6.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'Transform the output back into its original form:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c425497c-3ed4-4453-a663-75e676f44a5d.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: '**A** is a transformation matrix, which makes it possible to transform ![](img/17eb8275-2f67-47dc-ba8d-e764d4956c73.png)
    back into the form, which would have resulted from a direct convolution. As shown
    in the preceding formula and in the following diagram, the Winograd convolution
    allows us to compute 2*×*2 output tile simultaneously (four output cells):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f77fb1f-a7f4-41a6-a5ff-7a52ce537597.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: The Winograd convolution allows us to compute four output cells simultaneously
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, it seems that the Winograd algorithm performs a lot more operations
    than direct convolution. So, how is it faster, then? To find out, let's focus
    on the ![](img/199877db-609c-4220-bfef-fea0b783f935.png) transformation. The key
    here is that we have to perform ![](img/62533cbb-4149-4b6f-9a6b-65a4401988db.png) only
    once and then **D***[t]* can participate in the outputs of all *K* (following
    the notation) output slices. Therefore, ![](img/75e04c2e-80be-4bf1-9aa6-3ce4c72140c4.png) is
    amortized among all of the outputs and it doesn't affect the performance as much.
    Next, let's take a look at the ![](img/de88f956-c9af-4ff3-aea8-91343b168a3e.png) transformation. This
    one is even better because, once we compute **F***[t]*, we can apply it *N×P×Q*
    times (across all of the cells of the output slice and all of the images in the
    batch). Therefore, the performance penalty for this transformation is negligible.
    Similarly, the output transformation ![](img/4833fd5c-3623-4e16-be16-96617865bf20.png) is
    amortized over the number of input channels C.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll discuss the element-wise multiplication, ![](img/f2884fb8-3671-4696-86f3-c113be25c24d.png), which
    is applied *P×Q* times across all of the cells of the output slice and takes the
    bulk of the computational time. It consists of 16 scalar multiply operations and
    allows us to compute 2*×*2 output tile, which results in four multiplications
    for one output cell. Let's compare this with the direct convolution, where we
    have to perform *3*3=9* scalar multiplications (each filter element is multiplied
    by each receptive field input cell) for a single output. Therefore, the Winograd
    convolution requires *9/4 = 2.25* fewer operations.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The Winograd convolution has the most benefits when working with smaller filter
    sizes (for example, 3*×*3). Convolutions with larger filters (for example, 11*×*11)
    can be efficiently implemented with Fast Fourier Transform (FFT) convolutions,
    which are beyond the scope of this book.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll try to understand the inner workings of CNNs by visualizing
    their internal state.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing CNNs
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the criticisms of NNs is that their results aren't interpretable. It's
    common to think of a NN as a black box whose internal logic is hidden from us.
    This could be a serious problem. On the one hand, it's less likely we trust an
    algorithm that works in a way we don't understand, while on the other hand, it's
    hard to improve the accuracy of CNNs if we don't know how they work. Because of
    this, in the upcoming sections, we'll discuss two methods of visualizing the internal
    layers of a CNN, both of which will help us to gain insight into the way they
    learn.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Guided backpropagation
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Guided backpropagation (*Striving for Simplicity: The All Convolutional Net*, [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806))
    allows us to visualize the features that are learned by a single unit of one layer
    of a CNN. The following diagram shows how the algorithm works:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f578ee79-db08-41d5-82dd-e246b5197edb.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: Guided backpropagation visualization; Inspired by https://arxiv.org/abs/1412.6806.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the step-by-step execution:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: First, we start with a regular CNN (for example, AlexNet, VGG, and so on) with
    ReLU activations.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we feed the network with a single image *f^((0))* and propagate it forward
    until we get to the layer, *l*, we're interested in. This could be any network
    layer—hidden or output, convolutional or fully-connected.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set all but one activation of the output tensor *f^((l))* of that layer to 0\.
    For example, if we're interested in the output layer of a classification network,
    we'll select the unit with maximum activation (equivalent to the predicted class)
    and we'll set its value to 1\. All of the other units will be set to 0\. By doing
    this, we can isolate the unit in question and see which parts of the input image
    impact it the most.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we propagate the activation value of the selected unit backward until
    we reach the input layer and the reconstructed image *R^((0))*. The backward pass
    is very similar to the regular backpropagation (but not the same), that is, we
    still use transposed convolution as the backward operation of the forward convolution.
    In this case, though, we are interested in its image restoration properties rather
    than error propagation. Because of this, we aren't limited by the requirement
    to propagate the first derivative (gradient) and we can modify the signal in a
    way that will improve the visualization.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To understand the backward pass, we''ll use an example convolution with a single
    3*×*3 input and output slices. Let''s assume that we''re using a 1*×*1 filter
    with a single weight equal to 1 (we repeat the input). The following diagram shows
    this convolution, as well as three different ways to implement the backward pass:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/319cc3f9-9d51-4e9e-828b-e962de89b88c.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
- en: Convolution and the three different ways to reconstruct the image; Inspired
    by https://arxiv.org/abs/1412.6806.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss these three different ways to implement the backward pass in
    detail:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular backpropagation**: The backward signal is preconditioned on the input
    image since it also depends on the forward activations ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*, in the *Backpropagation* section). Our network
    uses a ReLU activation function, so the signal will only pass through the units
    that had positive activations in the forward pass.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deconvolutional network** (*deconvnet*, [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)):
    The backward signal of layer *l* depends only on the backward signal of layer
    *l+1*. A deconvnet will only route the positive values of *l+1* to *l*, regardless
    of what the forward activations are. Theoretically, the signal is not preconditioned
    on the input image at all. In this case, the deconvnet tries to restore the image
    based on its internal knowledge and the image class. However, this is not entirely
    true—if the network contains max-pooling layers, the deconvnet will store the
    so-called **switches** for each pooling layer. Each switch represents a map of
    the units with max activations of the forward pass. This map determines how to
    route the signal through the backward pass (you can read more about this in the
    source paper).'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guided backpropagation**: This is a combination of deconvnet and regular
    backprop. It will only route signals that have positive forward activations in
    *l* and positive backward activations in *l+1*. This adds additional guidance
    signal (hence the name) from the higher layers to the regular backprop. In essence,
    this step prevents negative gradients from flowing through the backward pass.
    The rationale is that the units that act as suppressors of our starting unit will
    be blocked and the reconstructed image will be free of their influence. Guided
    backpropagation performs so well that it doesn''t need to use deconvnet switches
    and instead routes the signal to all the units in each pooling region.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows a reconstructed image that was generated using
    guided backpropagation and AlexNet:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5ea6a22-529b-43c2-b783-e5cff83efdd8.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: original image, color reconstruction, and grayscale reconstruction
    using guided backpropagation on AlexNet; these images were generated using https://github.com/utkuozbulak/pytorch-cnn-visualizations.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-weighted class activation mapping
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand gradient-weighted class activation mapping (*Grad-CAM: Visual
    Explanations from Deep Networks via Gradient-Based Localization*, [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)),
    let''s quote the source paper itself:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '"Grad-CAM uses the gradients of any target concept (say, the logits for ''dog''
    or even a caption) flowing into the final convolutional layer to produce a coarse
    localization map highlighting the important regions in the image for predicting
    the concept."'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Grad-CAM algorithm:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcef2eda-a402-4ce0-871f-60bdd70bc65b.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: 'Grad-CAM schema; Source: https://arxiv.org/abs/1610.02391'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how it works:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: First, you start with a classification CNN model (for example, VGG).
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you feed the CNN with a single image and propagate it to the output layer.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Like we did in guided backpropagation, we take the output unit with maximum
    activation (equivalent to the predicted class ***c***), set its value to 1, and
    set all of the other outputs to 0\. In other words, create a one-hot encoded vector, *y^c*,
    of the prediction.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, compute the gradient of *y^c* with respect to the feature maps, *A^k*,
    of the final convolutional layer, ![](img/01d05e9f-c408-4523-b0aa-7ca630c33961.png),
    using backpropagation. *i* and *j* are the cell coordinates in the feature map.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, compute the scalar weight, ![](img/7bd05250-83fc-45f6-ae8d-3900c9a2fce7.png),
    which measures the "importance" of feature map *k* for the predicted class, *c*:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/423b3d51-d95b-493b-ace2-5735fb791876.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'Finally, compute a weighted combination between the scalar weights and the
    forward activation feature maps of the final convolutional layer and follow this
    with a ReLU:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8ac9e118-0da1-4eab-8b7d-0716b076b07b.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: 'Note that we multiply the scalar importance weight, ![](img/5c1c7470-1634-4c96-8d46-5d40c6d68f9a.png),
    by the tensor feature map, *A^k*. The result is a heatmap with the same dimensions
    as the feature map (14*×*14 in the case of VGG and AlexNet). It will highlight
    the areas of the feature map with the highest importance to class *c*. The ReLU
    discards the negative activations because we''re only interested in the features
    that increase *y^c*. We can upsample this heatmap back to the size of the input
    image and then superimpose it on it, as shown in the following screenshot:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/decb3007-cf16-4fad-835a-52e5462ac541.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
- en: 'Left to right: input image; upsampled heat-map; heat-map superimosed on the
    input (RGB); grayscale heat-map. The images were generated using https://github.com/utkuozbulak/pytorch-cnn-visualizations.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'One problem with Grad-CAM is upsampling the heatmap from 14*×*14 to 224*×*224
    because it doesn''t provide a fine-grained perspective of the important features
    for each class. To mitigate this problem, the authors of the paper proposed a
    combination of Grad-CAM and guided backpropagation (displayed in the Grad-CAM
    schema at the beginning of this section). We take the upsampled heatmap and combine
    it with the guided backprop visualization with element-wise multiplication. The
    input image contains two objects: a dog and a cat. Therefore, we can run Grad-CAM
    with both classes (the two rows of the diagram). This example shows how different
    classes detect different relevant features in the same image.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll discuss how to optimize CNNs with the help of regularization.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: CNN regularization
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*, a NN can approximate any function. But
    with great power comes great responsibility. The NN may learn to approximate the
    noise of the target function rather than its useful components. For example, imagine
    that we are training a NN to classify whether an image contains a car or not,
    but for some reason, the training set contains mostly red cars. It may turn out
    that the NN will associate the color red with the car, rather than its shape.
    Now, if the network sees a green car in inference mode, it may not recognize it
    as such because the color doesn't match. This problem is referred to as overfitting
    and it is central in machine learning (and even more so in deep networks). In
    this section, we'll discuss several ways to prevent it. Such techniques are collectively known
    as **regularization**.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of NNs, these regularization techniques usually impose some
    artificial limitations or obstacles on the training process to prevent the network
    from approximating the target function too closely. They try to guide the network
    to learn generic rather than specific approximation of the target function in
    the hope that this representation will generalize well on previously unseen examples
    of the test dataset. You may already be familiar with many of these techniques,
    so we''ll keep it short:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '**Input feature scaling**: ![](img/566be761-e32f-489f-a3f5-cb215784bef2.png). This
    operation scales all of the inputs in the [0, 1] range. For example, a pixel with
    intensity 125 would have a scaled value of ![](img/b247a222-8b4d-4945-9732-e5444b75478b.png).
    Feature scaling is fast and easy to implement.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input standard score**:![](img/a42dd245-b0f6-4a2a-8fc6-e991a4b1f37e.png). Here,
    μ and σ are the mean and standard deviation of all of the training data. They are
    usually computed separately for each input dimension. For example, in an RGB image,
    we would compute the mean *μ* and *σ* for each channel. We should note that *μ*
    and *σ* have to be computed on the training data and then applied to the test
    data. Alternatively, we can compute *μ* and *σ* per sample if it''s not practical
    to compute them over the entire dataset.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data augmentation**: This is where we artificially increase the size of the
    training set by applying random modifications (rotation, skew, scaling, and so
    on) on the training samples before feeding them to the network.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 regularization** (or **weight decay**): Here, we add a special regularization
    term to the cost function. Let''s assume that we''re using MSE ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The
    Nuts and Bolts of NNs*, *Gradient descent* section). Here, the MSE + L2 regularization
    formula is as follows:'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f0238ce8-2b02-4940-8b97-e734088e66ae.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
- en: Here, *w[j]* is one of *k* total network weights and λ is the weight decay coefficient.
    The rationale is that if the network weights, *w[j]*, are large, then the cost
    function will also increase. In effect, weight decay penalizes large weights (hence
    the name). This prevents the network from relying too heavily on a few features
    associated with these weights. There is less chance of overfitting when the network
    is forced to work with multiple features. In practical terms, when we compute
    the derivative of the weight decay cost function (the preceding formula) with
    respect to each weight and then propagate it to the weights themselves, the weight
    update rule changes from ![](img/616423c0-4765-4633-992c-40af9f782b5c.png) to
    ![](img/97768311-0e57-46f7-ab19-d63b6d727f0d.png).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropout**: Here, we randomly and periodically remove some of the neurons
    (along with their input and output connections) from the network. During a training
    mini-batch, each neuron has a probability, *p*, of being stochastically dropped.
    This is to ensure that no neuron ends up relying too much on other neurons and
    "learns" something useful for the network instead.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Normalization** (**BN**, *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*, [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)): This
    is a way to apply data processing, similar to the standard score, for the hidden
    layers of the network. It normalizes the outputs of the hidden layer for each
    mini-batch (hence the name) in a way that maintains its mean activation value
    close to 0 and its standard deviation close to 1. Let''s say ![](img/b32be79b-c498-426d-ab86-63943dc09652.png) is
    a mini-batch of size *n*. Each sample of *D* is a vector, ![](img/a3c7bc7c-e67d-4c0d-9550-05aa36f4a3a0.png),
    and ![](img/d8826d10-f9e0-4724-a977-eb5da755842e.png)is a cell with an index *k*
    of that vector. For the sake of clarity, we''ll omit the (*k*) superscript in
    the following formulas; that is, we''ll write *x[i]*, but we''ll mean ![](img/a7f2a55b-b380-4a97-92fa-5458f6ff024d.png).
    We can compute BN for each activation, *k*, over the whole minibatch in the following
    way:'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/a7b5373c-098a-413c-833d-83609bcd6a1e.png)]: This is the mini-batch
    mean. We compute *μ* separately for each location, *k*, over all samples.'
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d86ffa01-2a3f-43f1-8a3e-efbc2731d556.png): This is the mini-batch standard
    deviation. We compute *σ* separately for each location, *k*, over all samples.'
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7acf3e8f-0a11-47d6-8cc1-0aae11396f2d.png): We normalize each sample. *ε*
    is a constant that''s added for numerical stability.'
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7a6d44a8-9740-4439-9462-217732c87adc.png): *γ* and *β* are learnable
    parameters and we compute them over each location, *k* (*γ^((k))* and *β^((k))*),
    over all of the samples of the mini-batch (the same applies for *μ* and *σ*).
    In convolutional layers, each sample, *x*, is a tensor with multiple feature maps.
    To preserve the convolutional property, we compute *μ* and *σ* per location over
    all of the samples, but we use the same *μ* and *σ* in the matching locations
    across all of the feature maps. On the other hand, we compute *γ* and *β* per
    feature map, rather than per location.'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This section concludes our analysis of the structure and inner workings of CNNs.
    At this point, we would normally proceed with some sort of CNN coding example.
    But in this book, we want to do things a little differently. Therefore, we won't
    implement a plain old feed-forward CNN, which you may have already done before.
    Instead, in the next section, you will be introduced to the technique of transfer
    learning—a way to use pretrained CNN models for new tasks. But don't worry—we'll
    still implement a CNN from scratch. We'll do this in [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks.* In this way, we'll be able to create a more
    complex network architecture using our knowledge from that chapter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Introducing transfer learning
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say that we want to train a model on a task that doesn't have readily
    available labeled training data like ImageNet does. Labeling training samples
    could be expensive, time-consuming, and error-prone. So, what does a humble engineer
    do when they want to solve a real ML problem with limited resources? Enter **Transfer
    Learning** (**TL**).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: TL is the process of applying an existing trained ML model to a new, but related,
    problem. For example, we can take a network trained on ImageNet and repurpose
    it to classify grocery store items. Alternatively, we could use a driving simulator
    game to train a neural network to drive a simulated car and then use the network
    to drive a real car (but don't try this at home!). TL is a general ML concept
    that's applicable to all ML algorithms, but in this context, we'll talk about
    CNNs. Here's how it works.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: We start with an existing pretrained network. The most common scenario is to
    take a pretrained network from ImageNet, but it could be any dataset. TensorFlow
    and PyTorch both have popular ImageNet pretrained neural architectures that we
    can use. Alternatively, we can train our own network with a dataset of our choice.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: The fully-connected layers at the end of a CNN act as translators between the
    network's language (the abstract feature representations learned during training)
    and our language, which is the class of each sample. You can think of TL as a
    translation into another language. We start with the network's features, which
    is the output of the last convolutional or pooling layer. Then, we translate them
    into a different set of classes of the new task. We can do this by removing the
    last fully-connected layer (or all the fully-connected layers) of an existing
    pretrained network and replacing it with another layer, which represents the classes
    of the new problem.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the TL scenario shown in the following diagram:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b755ece2-264b-4069-b880-ce0486c1ffef.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
- en: In TL, we can replace the fully-connected layer(s) of a pretrained net and repurpose
    it/them for a new problem
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we cannot do this mechanically and expect the new network to work
    because we still have to train the new layer with data related to the new task.
    Here, we have two options:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '**Use the original part of the network as a feature extractor and only train
    the new layer(s)**: In this scenario, we feed the network a training batch of
    the new data and propagate it forward to see the network''s output. This part
    works just like regular training would. But in the backward pass, we lock the
    weights of the original network and only update the weights of the new layers.
    This is the recommended way to do things when we have limited training data for
    the new problem. By locking most of the network weights, we prevent overfitting
    on the new data.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tune the whole network**: In this scenario, we''ll train the whole network
    and not just the newly added layers at the end. It is possible to update all of
    the network weights, but we can also lock some of the weights in the first layers.
    The idea here is that the initial layers detect general features—not related to
    a specific task—and it makes sense to reuse them. On the other hand, the deeper
    layers may detect task-specific features and it would be better to update them.
    We can use this method when we have more training data and don''t need to worry
    about overfitting.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing transfer learning with PyTorch
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what TL is, let's look at whether it works in practice. In
    this section, we'll apply an advanced ImageNet pretrained network on the CIFAR-10
    images with **PyTorch 1.3.1** and the `torchvision` 0.4.2 package. We'll use both
    types of TL. It's preferable to run this example on a GPU.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: This example is partially based on [https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the following imports:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define `batch_size` for convenience:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the training dataset. We have to consider a few things:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CIFAR-10 images are 32*×*32, while the ImageNet network expects 224*×*224
    input. Since we are using an ImageNet-based network, we'll upsample the 32*×*32
    CIFAR images to 224*×*224.
  id: totrans-235
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardize the CIFAR-10 data using the ImageNet mean and standard deviation
    since this is what the network expects.
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll also add some data augmentation in the form of random horizontal or
    vertical flips:'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Follow the same steps with the validation/test data, but this time without
    augmentation:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Choose `device`, preferably a GPU with a fallback on CPU:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define the training of the model. Unlike TensorFlow, in PyTorch, we have to
    iterate over the training data manually. This method iterates once over the whole
    training set (one epoch) and applies the optimizer after each forward pass:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the testing/validation of the model. This is very similar to the training
    phase, but we will skip the backpropagation part:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Define the first TL scenario, where we use the pretrained network as a feature
    extractor:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll use a popular network known as ResNet-18\. We'll talk about this in detail
    in the *Advanced network architectures* section. PyTorch will automatically download
    the pretrained weights.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the last network layer with a new layer with 10 outputs (one for each
    CIFAR-10 class).
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exclude the existing network layers from the backward pass and only pass the
    newly added fully-connected layer to the Adam optimizer.
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the training for `epochs` and evaluate the network accuracy after each epoch.
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the test accuracy with the help of the `plot_accuracy` function. Its definition
    is trivial and you can find it in this book's code repository.
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the `tl_feature_extractor` function, which implements all
    of this:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Implement the fine-tuning approach. This function is similar to `tl_feature_extractor`,
    but here, we''re training the whole network:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Finally, we can run the whole thing in one of two ways:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call `tl_fine_tuning()` to use the fine-tuning TL approach for five epochs.
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Call `tl_feature_extractor()` to train the network with the feature extractor
    approach for five epochs.
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the accuracy of the networks after five epochs for the two scenarios:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17c9041d-7c61-440d-abc9-4065268247ba.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Left: Feature extraction TL accuracy; right: Fine-tuning TL accuracy'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: Due to the large size of the chosen `ResNet18` pretrained model, the network
    starts to overfit in the feature extraction scenario.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning with TensorFlow 2.0
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll implement the two transfer learning scenarios again,
    but this time using **TensorFlow 2.0.0 (TF)**. In this way, we can compare the
    two libraries. Instead of `ResNet18`, we'll use the `ResNet50V2` architecture
    (more on that in the [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks*). In addition to TF, this example also requires
    the TF Datasets 1.3.0 package ([https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)),
    a collection of various popular ML datasets.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: This example is partially based on [https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, let''s get started:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, first, we need to do the imports:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we''ll define the mini-batch and input images sizes (the image size is
    determined by the network architecture):'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next, we''ll load the CIFAR-10 dataset with the help of TF datasets. The `repeat()`
    method allows us to reuse the dataset for multiple epochs:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we''ll define the `train_format_sample` and `test_format_sample` functions,
    which will transform the input images into suitable CNN inputs. These functions
    play the same roles that the `transforms.Compose` object plays, which we defined
    in the *Implementing transfer learning with PyTorch* section. The input is transformed
    as follows:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The images are resized to 96*×*96, which is the expected network input size.
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each image is standardized by transforming its values so that it's in the (-1;
    1) interval.
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The labels are transformed for one-hot encoding.
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The training images are randomly flipped horizontally and vertically.
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the actual implementation:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next is some boilerplate code that assigns these transformers to the train/test
    datasets and splits them into mini-batches:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, we need to define the feature extraction model:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll use Keras for the pretrained network and model definition since it is
    an integral part of TF 2.0.
  id: totrans-284
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the `ResNet50V2` pretrained net, excluding the final fully-connected
    layers.
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we call `base_model.trainable = False`, which *freezes* all of the network
    weights and prevents them from training.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we add a `GlobalAveragePooling2D` operation, followed by a new and
    trainable fully-connected trainable layer at the end of the network.
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code implements this:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we''ll define the fine-tuning model. The only difference it has from
    the feature extraction is that we only freeze some of the bottom pretrained network
    layers (as opposed to all of them). The following is the implementation:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we''ll implement the `train_model` function, which trains and evaluates
    the models that are created by either the `build_fe_model` or `build_ft_model`
    function:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can run either the feature extraction or fine-tuning TL using the following
    code:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`train_model(build_ft_model())`'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_model(build_fe_model())`'
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TF will automatically use the machine GPU if one is available; otherwise, it
    will revert to the CPU. The following diagram shows the accuracy of the networks
    after five epochs for the two scenarios:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c73d77d-462f-4bfe-8c5d-2e9e7d7f9c64.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Left: Feature extraction TL; right: Fine-tuning TL'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-300
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with a quick recap of CNNs and discussed transposed,
    depthwise separable, and dilated convolutions. Next, we talked about improving
    the performance of CNNs by representing the convolution as a matrix multiplication
    or with the Winograd convolution algorithm. Then, we focused on visualizing CNNs
    with the help of guided backpropagation and Grad-CAM. Next, we discussed the most
    popular regularization techniques. Finally, we learned about transfer learning
    and implemented the same TL task with both PyTorch and TF as a way to compare
    the two libraries.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss some of the most popular advanced CNN architectures.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
