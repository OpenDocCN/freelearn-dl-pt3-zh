- en: Understanding Convolutional Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll discuss **Convolutional Neural Networks** (**CNNs**)
    and their applications in **Computer Vision** (**CV**). CNNs started the modern
    deep learning revolution. They are at the base of virtually all recent CV advancements,
    including **Generative Adversarial Networks** (**GANs**), object detection, image
    segmentation, neural style transfer, and much more. For this reason, we believe
    CNNs deserve an in-depth look that's beyond our basic understanding of them.
  prefs: []
  type: TYPE_NORMAL
- en: To do this, we'll start with a short recap of the CNN building blocks, that
    is, the convolutional and pooling layers. We'll discuss the various types of convolutions
    in use today since they are reflected in a large number of CNN applications. We'll
    also learn how to visualize the internal state of CNNs. Then, we'll focus on regularization
    techniques and implement a transfer learning example.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The* *Nuts and
    Bolts of Neural Networks,* we discussed that many NN operations have solid mathematical
    foundations, and convolutions are no exception. Let''s start by defining the mathematical
    convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3bce005-8da4-479a-bb99-b56cc6234765.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation is denoted with *.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f* and *g* are two functions with a common parameter, *t*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The result of the convolution is a third function, *s(t)* (not just a single
    value).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The convolution of *f* and *g* at value *t* is the integral of the product of *f(t)*
    and the reversed (mirrored) and shifted value of *g(t-τ)*, where *t-τ* represents
    the shift. That is, for a single value of *f* at time *t*, we shift *g* in the
    range ![](img/27cdbe02-f140-4acb-bcea-3114eae42a4a.png) and we compute the product
    *f(t)**g(t-τ)* continuously because of the integral. The integral (and hence the
    convolution) is equivalent to the area under the curve of the product of the two
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is best illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8d8a4c1-7a56-460c-a0c3-7460343330a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: a convolution, where *g* is shifted and reversed; right: a step-by-step
    illustration of a convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: In the convolution operation, *g* is shifted and reversed in order to preserve
    the operation's commutative property. In the context of CNNs, we can ignore this
    property and we can implement it without reversing *g*. In this case, the operation
    is called cross-correlation. These two terms are used interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can define the convolution for discrete (integer) values of *t* with the
    following formula (which is very similar to the continuous case):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93ecc38f-ba32-4cc5-b628-ec8f975db95b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also generalize it to the convolution of functions with two shared input
    parameters, *i* and *j*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16ae140a-3930-4c85-99f1-04f0c49ff727.png)'
  prefs: []
  type: TYPE_IMG
- en: We can derive the formula in a similar manner for three parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In CNNs, the function *f* is the input of the convolution operation (also referred
    to as the convolutional layer). Depending on the number of input dimensions, we
    have 1D, 2D, or 3D convolutions. А time series input is a 1D vector, an image
    input is a 2D matrix, and a 3D point cloud is a 3D tensor. The function *g*, on
    the other hand, is called a kernel (or filter). It has the same number of dimensions
    as the input data and it is defined by a set of learnable weights. For example,
    a filter of size *n* for a 2D convolution is an *n×n* matrix. The following diagram
    illustrates a 2D convolution with a 2×2filter applied over a single 3×3 slice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a52c6aef-cfd2-4f27-bc85-20649b591ca1.png)'
  prefs: []
  type: TYPE_IMG
- en: 2D convolution with a 2×2filter applied over a single 3×3 slice
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: We slide the filter along all of the dimensions of the input tensor.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At every input position, we multiply each filter weight by its corresponding
    input tensor cell at the given location. The input cells, which contribute to
    a single output cell, are called **receptive fields**. We sum all of these values
    to produce the value of a single output cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike fully-connected layers, where each output unit gathers information from
    all of the inputs, the activation of a convolution output cell is determined by
    the inputs in its receptive field. This principle works best for hierarchically
    structured data such as images. For example, neighboring pixels form meaningful
    shapes and objects, but a pixel at one end of the image is unlikely to have a
    relationship with a pixel at another end. Using a fully-connected layer to connect
    all of the input pixels with each output unit is like asking the network to find
    a needle in a haystack. It has no way of knowing whether an input pixel is in
    the receptive field of the output unit or not.
  prefs: []
  type: TYPE_NORMAL
- en: The filter highlights some particular features in the receptive field. The output
    of the operation is a tensor (known as a feature map), which marks the locations
    where the feature is detected. Since we apply the same filter throughout the input
    tensor, the convolution is translation invariant; that is, it can detect the same
    features, regardless of their location on the image. However, the convolution
    is neither rotation invariant (it is not guaranteed to detect a feature if it's
    rotated), nor scale invariant (it is not guaranteed to detect the same artifact
    in different scales).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see examples of 1D and 3D convolutions (we''ve
    already introduced an example of a 2D convolution):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6f84dd7f-8286-4cd7-9442-c79457e6322e.png)'
  prefs: []
  type: TYPE_IMG
- en: '1D convolution: The filter (denoted with multicolored lines) slides over a
    single axis; 3D convolution: The filter (denoted with dashed lines) slides over
    three axes'
  prefs: []
  type: TYPE_NORMAL
- en: The CNN convolution can have multiple filters, highlighting different features,
    which results in multiple output feature maps (one for each filter). It can also
    gather input from multiple feature maps, for example, the output of a previous
    convolution. The combination of feature maps (input or output) is called a volume.
    In this context, we can also refer to the feature maps as slices. Although the
    two terms refer to the same thing, we can think of the slice as part of the volume,
    whereas the feature map highlights its role as, well, a feature map.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier in this section, each volume (as well as the filter)
    is represented by a tensor. For example, a red, green, and blue (RGB) image is
    represented by a 3D tensor of three 2D slices (one slice per color channel). But
    in the context of CNNs, we add one more dimension for the sample index in the
    mini-batch. Here, a 1D convolution would have 3D input and output tensors. Their
    axes can be in either *NCW* or *NWC* order, where *N* is the index of the sample
    in the mini-batch, *C* is the index of the depth slice in the volume, and *W*
    is the vector size of each sample. In the same way, a 2D convolution will be represented
    by *NCHW* or *NHWC* tensors, where *H* and *W* are the height and width of the
    slices. A 3D convolution will have an *NCLHW* or *NLHWC* order, where *L* stands
    for the depth of the slice.
  prefs: []
  type: TYPE_NORMAL
- en: We use 2D convolutions to work with RGB images. However, we may consider the
    three colors an additional dimension, hence making the RGB image 3D. Why didn't
    we use 3D convolutions, then? The reason for this is that, even though we can
    think of the input as 3D, the output is still a 2D grid. Had we used 3D convolution,
    the output would also be 3D, which doesn't carry any meaning in the case of 2D
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have *n* input and *m* output slices. In this case, we'll apply
    *m* filters across the set of *n* input slices. Each filter will generate a unique
    output slice that highlights the feature that was detected by the filter (*n*
    to *m* relationship).
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on the relationship of the input and output slice, we get cross-channel
    and depth-wise convolutions, as illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56593c82-220f-46c9-a73f-346edd79558d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: cross-channel convolution; right: depthwise convolution'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss their properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cross-channel convolutions**: One output slice receives input from all of
    the input slices (*n*-to-one relationship). With multiple output slices, the relationship
    becomes *n*-to-*m*. In other words, each input slice contributes to the output
    of each output slice. Each pair of input/output slices uses a separate filter
    slice that''s unique to that pair. Let''s denote the size of the filter (equal width
    and height) with *F*, the depth of the input volume with *C[in]*, and the depth
    of the output volume with *C[out]*. With this, we can compute the total number
    of weights, *W*, in a 2D convolution with the following equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cbefc433-a0d8-46ef-98c5-9392bf144203.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, +1 represents the bias weight for each filter. Let's say we have three
    slices and want to apply four 5*×*5 filters to them. If we did this, the convolution
    filter would have a total of *(3*5*5 + 1) * 4 = 304* weights, four output slices
    (output volume with a depth of 4), and one bias per slice. The filter for each
    output slice will have three 5*×*5 filter patches for each of the three input
    slices and one bias for a total of 3*5*5 + 1 = 76 weights.
  prefs: []
  type: TYPE_NORMAL
- en: '**Depthwise convolutions**: Each output slice receives input from a single
    input slice. It''s a kind of reversal of the previous case. In its most simple
    form, we apply a filter over a single input slice to produce a single output slice.
    In this case, the input and output volumes have the same depth, that is, *C*.
    We can also specify a **channel multiplier** (an integer, *m*), where we apply *m* filters
    over a single output slice to produce *m* output slices. This is a case of a one-to-*m*
    relationship. In this case, the total number of output slices is *n * m*. We can
    compute the number of weights, *W*, in a 2D depthwise convolution with the following
    formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/86e8086d-cbb7-430e-b548-164c7b897256.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *m* is the channel multiplier and *+C* represents the biases of each output
    slice.
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution operation is also described by two other parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S****tride** is the number of positions that we slide the filter over on
    the input slice on each step. By default, the stride is 1\. If it''s larger than
    1, then we call it a **stride convolution**. The largest stride increases the
    receptive field of the output neurons. With stride 2, the size of the output slice
    will be roughly four times smaller than the input. In other words, one output
    neuron will cover the area, which is four times larger, compared to a stride 1
    convolution. The neurons in the following layers will gradually capture input
    from the larger regions of the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Padding** the edges of the input slice with rows and columns of zeros before
    the convolution operation. The most common way to use padding is to produce output
    with the same dimensions as the input. The newly padded zeros will participate
    in the convolution operation with the slice, but they won''t affect the result.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knowing the input dimensions and the filter size, we can compute the dimensions
    of the output slices. Let''s say the size of the input slice is *I* (equal height
    and width), the size of the filter is *F*, the stride is *S*, and the padding is *P*.
    Here, the size, *O*, of the output slice is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/288c3c31-1000-4908-8163-171bbc19a0d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Besides stride convolutions, we can also use **pooling** operations to increase
    the receptive field of the deeper neurons and reduce the size of the future slices. The
    pooling splits the input slice into a grid, where each grid cell represents a
    receptive field of a number of neurons (just like it does with the convolution).
    Then, a pooling operation is applied over each cell of the grid. Similar to convolution,
    pooling is described by the stride, *S*, and the size of the receptive field, *F*.
    If the size of input slice is *I*, then the formula for the output size of the
    pooling is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/643541b1-b7a2-44b3-a546-cd28d1b32542.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In practice, only two combinations are used. The first is a 2*×*2 receptive field
    with stride 2, while the second is a 3*×*3 receptive field with stride 2 (overlapping). The
    most common pooling operations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Max pooling**: This propagates the maximum value of the input values of the
    receptive field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average pooling**: This propagates the average value of the inputs in the
    receptive field.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Global Average Pooling** (**GAP**): This is the same as average pooling,
    but the pooling region has the same size as the feature map, *I×I*. GAP performs
    an extreme type of dimensionality reduction: the output is a single scalar, which
    represents the average value of the whole feature map.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Typically, we would alternate one or more convolutional layers with one pooling
    (or stride convolution) layer. In this way, the convolutional layers can detect
    features at every level of the receptive field size because the aggregated receptive
    field size of deeper layers is larger than the ones at the beginning of the network.
    The deeper layers also have more filters (hence, a higher volume depth) compared
    to the initial ones. The feature detector at the beginning of the network works
    on a small receptive field. It can only detect a limited number of features, such
    as edges or lines, that are shared among all classes.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, a deeper layer would detect more complex and numerous features.
    For example, if we have multiple classes, such as cars, trees, or people, each
    would have its own set of features, such as tires, doors, leaves, and faces. This
    would require more feature detectors. The output of the final convolution (or
    pooling) is "translated" to the target labels by adding one or more fully connected
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have had an overview of convolutions, pooling operations, and CNNs,
    in the next section, we'll focus on different types of convolution operations.
  prefs: []
  type: TYPE_NORMAL
- en: Types of convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've discussed the most common type of convolution. In the upcoming
    sections, we'll talk about a few of its variations.
  prefs: []
  type: TYPE_NORMAL
- en: Transposed convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the convolutional operations we've discussed so far, the output dimensions
    are either equal or smaller than the input dimensions. In contrast, transposed
    convolutions (first proposed in *Deconvolutional Networks* by Matthew D. Zeiler,
    Dilip Krishnan, Graham W. Taylor, and Rob Fergus: [https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf](https://www.matthewzeiler.com/mattzeiler/deconvolutionalnetworks.pdf))
    allow us to upsample the input data (their output is larger than the input). This
    operation is also known as **deconvolution**, **fractionally strided convolution**,
    or **sub-pixel convolution**. These names can sometimes lead to confusion. To
    clarify things, note that the transposed convolution is, in fact, a regular convolution
    with a slightly modified input slice or convolutional filter.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the longer explanation, we''ll start with a 1D regular convolution over
    a single input and output slice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a6a3a798-3dc8-441a-be05-3b5043baa376.png)'
  prefs: []
  type: TYPE_IMG
- en: 1D regular convolution
  prefs: []
  type: TYPE_NORMAL
- en: It uses a filter with a size of 4, stride 2, and padding 2 (denoted with gray
    in the preceding diagram). The input is a vector of size 6 and the output is a
    vector of size 4\. The filter, a vector **f** = [1, 2, 3, 4], is always the same,
    but it's denoted with different colors for each position we apply it to. The respective
    output cells are denoted with the same color. The arrows show which input cells
    contribute to one output cell.
  prefs: []
  type: TYPE_NORMAL
- en: The example that is being discussed in this section is inspired by the paper; *Is
    the deconvolution layer the same as a convolutional layer? *([https://arxiv.org/abs/1609.07009](https://arxiv.org/abs/1609.07009)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll discuss the same example (1D, single input and output slices,
    filter of size 4, padding 2, and stride 2), but for transposed convolution. The
    following diagram shows two ways we can implement it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/31980d51-3120-43f3-bf47-ac3ec266ef8a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: A convolution with stride 2, applied with the transposed filter **f**.
    The 2 pixels at the beginning and the end of the output are cropped; right: A
    convolution with stride 0.5, applied over input data, padded with subpixels. The
    input is filled with 0-valued pixels (gray).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss them in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first case, we have a regular convolution with stride 2 and a filter represented
    as transposed row matrix (equivalent to column matrix) with size 4: ![](img/88d1c5eb-7650-46c4-b378-438a9f3fa008.png) (shown
    in the preceding diagram, left). Note that the stride is applied over the output
    layer as opposed to the regular convolution, where we stride over the input. By
    setting the stride larger than 1, we can increase the output size, compared to
    the input. Here, the size of the input slice is *I*, the size of the filter is *F*,
    the stride is *S*, and the input padding is *P*. Due to this, the size, *O*, of
    the output slice of a transposed convolution is given by the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/5c6c084c-a942-44ce-adbd-190f97c82f4e.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: In this scenario, an input of size 4 produces an output of size *2*(4 - 1) +
    4 - 2*2 = 6*. We also crop the two cells at the beginning and the end of the output
    vector because they only gather input from a single input cell.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the second case, the input is filled with imaginary 0-valued subpixels between
    the existing ones (shown in the preceding diagram, right). This is where the name
    subpixel convolution comes from. Think of it as padding but within the image itself
    and not only along the borders. Once the input has been transformed in this way,
    a regular convolution is applied.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's compare the two output cells, *o*[1] and *o*[3], in both scenarios. As
    shown in the preceding diagram, in either case, *o*[1] receives input from the
    first and the second input cells and *o*[3] receives input from the second and
    third cells. In fact, the only difference between these two cases is the index
    of the weight, which participates in the computation. However, the weights are
    learned during training and, because of this, the index is not important. Therefore,
    the two operations are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s take a look at a 2D transposed convolution from a subpixel point
    of view (the input is at the bottom). As with the 1D case, we insert 0-valued
    pixels and padding in the input slice to achieve upsampling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d3ef15e-d1d6-407d-bcad-b696e2b6f812.png)'
  prefs: []
  type: TYPE_IMG
- en: The first three steps of a 2D transpose convolution with padding 1 and stride
    2: Source: https://github.com/vdumoulin/conv_arithmetic, https://arxiv.org/abs/1603.07285
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation operation of a regular convolution is a transposed convolution.
  prefs: []
  type: TYPE_NORMAL
- en: 1×1 convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A 1*×*1 (or pointwise) convolution is a special case of convolution where each
    dimension of the convolution filter is of size 1 (1*×*1 in 2D convolutions and
    1*×*1*×*1 in 3D). At first, this doesn't make sense—a 1*×*1 filter doesn't increase
    the receptive field size of the output neurons. The result of such a convolution would
    be pointwise scaling. But it can be useful in another way—we can use them to change
    the depth between the input and output volumes.
  prefs: []
  type: TYPE_NORMAL
- en: To understand this, let's recall that, in general, we have an input volume with
    a depth of *D* slices and *M* filters for *M* output slices. Each output slice
    is generated by applying a unique filter over all of the input slices. If we use
    a 1*×*1 filter and *D != M*, we'll have output slices of the same size, but with
    different volume depths. At the same time, we won't change the receptive field
    size between the input and output. The most common use case is to reduce the output
    volume, or *D > M* (dimension reduction), nicknamed the "bottleneck" layer.
  prefs: []
  type: TYPE_NORMAL
- en: Depth-wise separable convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An output slice in a cross-channel convolution receives input from all of the
    input slices using a single filter. The filter tries to learn features in a 3D
    space, where two of the dimensions are spatial (the height and width of the slice)
    and the third is the channel. Therefore, the filter maps both spatial and cross-channel
    correlations.
  prefs: []
  type: TYPE_NORMAL
- en: '**Depthwise separable convolutions** (**DSC**, *Xception: Deep Learning with
    Depthwise Separable Convolutions*, [https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357)) can
    completely decouple cross-channel and spatial correlations. A DSC combines two
    operations: a depthwise convolution and a 1*×*1 convolution. In a depthwise convolution,
    a single input slice produces a single output slice, so it only maps spatial (and
    not cross-channel) correlations. With 1*×*1 convolutions, we have the opposite.
    The following diagram represents the DSC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79686796-d667-416c-a52f-9c40fd7631d4.png)'
  prefs: []
  type: TYPE_IMG
- en: A depthwise separable convolution
  prefs: []
  type: TYPE_NORMAL
- en: The DSC is usually implemented without non-linearity after the first (depthwise)
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: Let's compare the standard and depthwise separable convolutions. Imagine that
    we have 32 input and output channels and a filter with a size of 3*×*3\. In a
    standard convolution, one output slice is the result of applying one filter for
    each of the 32 input slices for a total of *32 * 3 * 3 = 288* weights (excluding
    bias). In a comparable depthwise convolution, the filter has only *3 * 3 = 9*
    weights and the filter for the 1*×*1 convolution has *32 * 1 * 1 = 32* weights.
    The total number of weights is *32 + 9 = 41*. Therefore, the depthwise separable
    convolution is faster and more memory-efficient compared to the standard one.
  prefs: []
  type: TYPE_NORMAL
- en: Dilated convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recall the discrete convolution formula we introduced at the beginning of the *A
    quick recap of CNNs* section. To explain dilated convolutions (*Multi-Scale Context
    Aggregation by Dilated Convolutions*, [https://arxiv.org/abs/1511.07122](https://arxiv.org/abs/1511.07122)),
    let''s start with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61fd057f-883a-456b-85d4-d77cd8589e36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We''ll denote the dilated convolution with **[l]*, where *l* is a positive
    integer value called the dilation factor. The key is the way we apply the filter
    over the input. Instead of applying the *n×n* filter over the *n×n* receptive
    field, we apply the same filter sparsely over a receptive field of size *(n*l-1)×(n*l-1)*.
    We still multiply each filter weight by one input slice cell, but these cells
    are at a distance of *l* away from each other. The regular convolution is a special
    case of dilated convolution with *l=1*. This is best illustrated with the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0807af5e-ce74-4b7d-8399-52a33bab68d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A dilated convolution with a dilation factor of l=2: Here, the first two steps
    of the operation are displayed. The bottom layer is the input while the top layer
    is the output. Source: https://github.com/vdumoulin/conv_arithmetic'
  prefs: []
  type: TYPE_NORMAL
- en: Dilated convolutions can increase the receptive field size exponentially without
    losing resolution or coverage. We can also increase the receptive field with stride
    convolutions or pooling but at the cost of resolution and/or coverage. To understand
    this, let's imagine that we have a stride convolution with stride *s>1*. In this
    case, the output slice is *s* times smaller than the input (loss of resolution).
    If we increase *s>n* further (*n* is the size of either the pooling or convolutional
    kernel), we get loss of coverage because some of the areas of the input slice
    will not participate in the output at all. Additionally, dilated convolutions
    don't increase the computation and memory costs because the filter uses the same
    number of weights as the regular convolution.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the efficiency of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main reasons for the advances in recent **Deep Learning** (**DL**)
    is its ability to run **Neural Networks** (**NNs**) very fast. This is in large
    part because of the good match between the nature of NN algorithms and the specifics
    of **Graphical Processing Units** (**GPUs**). In [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*, we underscored the importance of matrix multiplication
    in NNs. As a testament to this, it is possible to transform the convolution into
    a matrix multiplication as well. Matrix multiplication is embarrassingly parallel
    (trust me, this is a term—you can Google it!). The computation of each output
    cell is not related to the computation of any other output cell. Therefore, we
    can compute all of the outputs in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Not coincidentally, GPUs are well suited for highly parallel operations like
    this. On the one hand, a GPU has a high number of computational cores compared
    to a **Central Processing Unit** (**CPU**). Even though a GPU core is faster than
    a CPU one, we can still compute a lot more output cells in parallel. But what's
    even more important is that GPUs are optimized for memory bandwidth, while CPUs
    are optimized for latency. This means that a CPU can fetch small chunks of memory
    very quickly but will be slow when it comes to fetching large chunks. The GPU does
    the opposite. Because of this, in tasks such as large matrix multiplication for
    NNs, the GPU has an advantage.
  prefs: []
  type: TYPE_NORMAL
- en: Besides hardware specifics, we can optimize CNNs on the algorithmic side as
    well. The majority of computational time in a CNN is devoted to the convolutions
    themselves. Although the implementation of the convolution is straightforward
    enough, in practice, there are more efficient algorithms to achieve the same result.
    Although contemporary DL libraries such as TensorFlow or PyTorch shield the developer
    from such details, in this book, we are aiming for a deeper (pun intended) understanding
    of DL.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this, in the next section, we'll discuss two of the most popular
    fast convolution algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution as matrix multiplication
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll describe the algorithm that we use to transform convolutions
    into matrix multiplication, just like how it''s implemented in the cuDNN library
    (*cuDNN: Efficient Primitives for Deep Learning*, [https://arxiv.org/abs/1410.0759](https://arxiv.org/abs/1410.0759)).
    To understand this, let''s assume that we perform a cross-channel 2D convolution
    over an RGB input image. Let''s look at the following table for the parameters
    of the convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Notation** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| Mini-batch size | N | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Input feature maps (volume depth) | C | 3 (one for each RGB channel) |'
  prefs: []
  type: TYPE_TB
- en: '| Input image height | H | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Input image width | W | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Output feature maps (volume depth) | K | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Filter height | R | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Filter width | S | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Output feature map height | P | 2 (based on the input/filter sizes) |'
  prefs: []
  type: TYPE_TB
- en: '| Output feature map width | Q | 2 (based on the input/filter sizes) |'
  prefs: []
  type: TYPE_TB
- en: 'For the sake of simplicity, we''ll assume we have zero padding and stride 1\.
    We''ll denote the input tensor with *D* and the convolution filter tensor with
    *F*. The matrix convolution works in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: We unfold the tensors, *D* and *F*, into the ![](img/b51588a3-c401-4c1c-8b20-b91726e14de5.png) and
    ![](img/93fd8178-c6d7-4996-a1dd-94cf21f0a6a4.png) matrices, respectively.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we multiply the matrices to get the output matrix, ![](img/bc1fddcd-c0be-424f-9acb-47604f8d3a17.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We discussed matrix multiplication in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*. Now, let''s focus on the way we can unfold
    the tensors in matrices. The following diagram shows how to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa86d420-5efa-43ad-bd02-cecad396cc09.png)'
  prefs: []
  type: TYPE_IMG
- en: Convolution as matrix multiplication; Inspired by https://arxiv.org/abs/1410.0759
  prefs: []
  type: TYPE_NORMAL
- en: 'Each feature map has a different color (R, G, B). In the regular convolution,
    the filter has a square shape and we apply it over a square input region. In the
    transformation, we unfold each possible square region of *D* into one column of **D***[m]*.
    Then, we unfold each square component of *F* into one row of **F***[m]*. In this
    way, the input and filter data for each output cell is situated in a single column/row
    of the matrices **D***[m]* and **F***[m]*. This makes it possible to compute the
    output value as a matrix multiplication. The dimensions of the transformed input/filter/output
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: dim(**D***[m]*) = CRS*×*NPQ = 12*×*4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dim(**F***[m]*) = K*×*CRS = 2*×*12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: dim(**O***[m]*) = K*×*NPQ = 2*×*4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand this transformation, let''s learn how to compute the first output
    cell with the regular convolution algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9834db46-e86b-421e-8de9-8c1789e9ed47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let''s observe the same formula, but this time, in matrix multiplication
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/89ec53d1-8092-46af-9893-2faa7aea4d0f.png)'
  prefs: []
  type: TYPE_IMG
- en: If we compare the components of the two equations, we'll see that they are exactly
    the same. That is, *D*[0,0,0,0] = **D**[m][0,0], *F*[0,0,0,0] = **F**[m][0,0], *D*[0,0,0,1]
    = **D**[m][0,1], *F*[0,0,0,1] = **F**[m][0,1], and so on. We can do the same for
    the rest of the output cells. Therefore, the output of the two approaches is the
    same.
  prefs: []
  type: TYPE_NORMAL
- en: One disadvantage of the matrix convolution is increased memory usage. In the
    preceding diagram, we can see that some of the input elements are duplicated multiple
    times (up to RS = 4 times, like D4).
  prefs: []
  type: TYPE_NORMAL
- en: Winograd convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Winograd algorithm (*Fast Algorithms for Convolutional Neural Networks*, [https://arxiv.org/abs/1509.09308](https://arxiv.org/abs/1509.09308))
    can provide 2 or 3*×* speedup compared to the direct convolution. To explain this,
    we'll use the same notations that we used in the *Convolution as matrix multiplication* section
    but with a *3×3* (*R=S=3*) filter. We'll also assume that the input slices are
    bigger than *4×4* (*H>4, W>4*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s how to compute Winograd convolutions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Divide the input image into 4*×*4 tiles that overlap with stride 2, as shown
    in the following diagram:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/cd4aa445-8508-4aab-bea4-72effd713822.png)'
  prefs: []
  type: TYPE_IMG
- en: The input is split into tiles
  prefs: []
  type: TYPE_NORMAL
- en: The tile size can vary, but for the sake of simplicity, we'll only focus on
    4*×*4 tiles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform each tile using the following two matrix multiplications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/b0268ace-bb0f-44da-bbbe-28165f6b82fa.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, the matrix **D** is the input slice (the one with
    circle values) while **B** is a special matrix, which results from the specifics
    of the Winograd algorithm (you can find more information about them in the paper
    linked at the beginning of this section).
  prefs: []
  type: TYPE_NORMAL
- en: 'Transform the filter using the following two matrix multiplications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/72a0e638-cfe5-4a34-a05e-83cfe8af3884.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, the matrix **F** (the one with dot values) is the
    3*×*3 convolution filter between one input and one output slice. **G** and its
    transpose, ![](img/c62285e7-2871-435d-92d0-75ab2fc9df82.png), are, again, special
    matrices, which result from the specifics of the Winograd algorithm. Note that
    the transformed filter matrix, **F***[t]*, has the same dimensions as the input
    tile, **D***[t]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the transformed output as an **element-wise** multiplication (the ![](img/8c5ea580-95d6-4abf-b93b-a2dc5ba2824b.png)symbol)
    of the transformed input and filter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8c2d3f58-125b-4bc5-9cdc-0d72c1e1b1a6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Transform the output back into its original form:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c425497c-3ed4-4453-a663-75e676f44a5d.png)'
  prefs: []
  type: TYPE_IMG
- en: '**A** is a transformation matrix, which makes it possible to transform ![](img/17eb8275-2f67-47dc-ba8d-e764d4956c73.png)
    back into the form, which would have resulted from a direct convolution. As shown
    in the preceding formula and in the following diagram, the Winograd convolution
    allows us to compute 2*×*2 output tile simultaneously (four output cells):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f77fb1f-a7f4-41a6-a5ff-7a52ce537597.png)'
  prefs: []
  type: TYPE_IMG
- en: The Winograd convolution allows us to compute four output cells simultaneously
  prefs: []
  type: TYPE_NORMAL
- en: At first glance, it seems that the Winograd algorithm performs a lot more operations
    than direct convolution. So, how is it faster, then? To find out, let's focus
    on the ![](img/199877db-609c-4220-bfef-fea0b783f935.png) transformation. The key
    here is that we have to perform ![](img/62533cbb-4149-4b6f-9a6b-65a4401988db.png) only
    once and then **D***[t]* can participate in the outputs of all *K* (following
    the notation) output slices. Therefore, ![](img/75e04c2e-80be-4bf1-9aa6-3ce4c72140c4.png) is
    amortized among all of the outputs and it doesn't affect the performance as much.
    Next, let's take a look at the ![](img/de88f956-c9af-4ff3-aea8-91343b168a3e.png) transformation. This
    one is even better because, once we compute **F***[t]*, we can apply it *N×P×Q*
    times (across all of the cells of the output slice and all of the images in the
    batch). Therefore, the performance penalty for this transformation is negligible.
    Similarly, the output transformation ![](img/4833fd5c-3623-4e16-be16-96617865bf20.png) is
    amortized over the number of input channels C.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll discuss the element-wise multiplication, ![](img/f2884fb8-3671-4696-86f3-c113be25c24d.png), which
    is applied *P×Q* times across all of the cells of the output slice and takes the
    bulk of the computational time. It consists of 16 scalar multiply operations and
    allows us to compute 2*×*2 output tile, which results in four multiplications
    for one output cell. Let's compare this with the direct convolution, where we
    have to perform *3*3=9* scalar multiplications (each filter element is multiplied
    by each receptive field input cell) for a single output. Therefore, the Winograd
    convolution requires *9/4 = 2.25* fewer operations.
  prefs: []
  type: TYPE_NORMAL
- en: The Winograd convolution has the most benefits when working with smaller filter
    sizes (for example, 3*×*3). Convolutions with larger filters (for example, 11*×*11)
    can be efficiently implemented with Fast Fourier Transform (FFT) convolutions,
    which are beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll try to understand the inner workings of CNNs by visualizing
    their internal state.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the criticisms of NNs is that their results aren't interpretable. It's
    common to think of a NN as a black box whose internal logic is hidden from us.
    This could be a serious problem. On the one hand, it's less likely we trust an
    algorithm that works in a way we don't understand, while on the other hand, it's
    hard to improve the accuracy of CNNs if we don't know how they work. Because of
    this, in the upcoming sections, we'll discuss two methods of visualizing the internal
    layers of a CNN, both of which will help us to gain insight into the way they
    learn.
  prefs: []
  type: TYPE_NORMAL
- en: Guided backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Guided backpropagation (*Striving for Simplicity: The All Convolutional Net*, [https://arxiv.org/abs/1412.6806](https://arxiv.org/abs/1412.6806))
    allows us to visualize the features that are learned by a single unit of one layer
    of a CNN. The following diagram shows how the algorithm works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f578ee79-db08-41d5-82dd-e246b5197edb.png)'
  prefs: []
  type: TYPE_IMG
- en: Guided backpropagation visualization; Inspired by https://arxiv.org/abs/1412.6806.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the step-by-step execution:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we start with a regular CNN (for example, AlexNet, VGG, and so on) with
    ReLU activations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we feed the network with a single image *f^((0))* and propagate it forward
    until we get to the layer, *l*, we're interested in. This could be any network
    layer—hidden or output, convolutional or fully-connected.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set all but one activation of the output tensor *f^((l))* of that layer to 0\.
    For example, if we're interested in the output layer of a classification network,
    we'll select the unit with maximum activation (equivalent to the predicted class)
    and we'll set its value to 1\. All of the other units will be set to 0\. By doing
    this, we can isolate the unit in question and see which parts of the input image
    impact it the most.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we propagate the activation value of the selected unit backward until
    we reach the input layer and the reconstructed image *R^((0))*. The backward pass
    is very similar to the regular backpropagation (but not the same), that is, we
    still use transposed convolution as the backward operation of the forward convolution.
    In this case, though, we are interested in its image restoration properties rather
    than error propagation. Because of this, we aren't limited by the requirement
    to propagate the first derivative (gradient) and we can modify the signal in a
    way that will improve the visualization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To understand the backward pass, we''ll use an example convolution with a single
    3*×*3 input and output slices. Let''s assume that we''re using a 1*×*1 filter
    with a single weight equal to 1 (we repeat the input). The following diagram shows
    this convolution, as well as three different ways to implement the backward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/319cc3f9-9d51-4e9e-828b-e962de89b88c.png)'
  prefs: []
  type: TYPE_IMG
- en: Convolution and the three different ways to reconstruct the image; Inspired
    by https://arxiv.org/abs/1412.6806.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s discuss these three different ways to implement the backward pass in
    detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Regular backpropagation**: The backward signal is preconditioned on the input
    image since it also depends on the forward activations ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The
    Nuts and Bolts of Neural Networks*, in the *Backpropagation* section). Our network
    uses a ReLU activation function, so the signal will only pass through the units
    that had positive activations in the forward pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deconvolutional network** (*deconvnet*, [https://arxiv.org/abs/1311.2901](https://arxiv.org/abs/1311.2901)):
    The backward signal of layer *l* depends only on the backward signal of layer
    *l+1*. A deconvnet will only route the positive values of *l+1* to *l*, regardless
    of what the forward activations are. Theoretically, the signal is not preconditioned
    on the input image at all. In this case, the deconvnet tries to restore the image
    based on its internal knowledge and the image class. However, this is not entirely
    true—if the network contains max-pooling layers, the deconvnet will store the
    so-called **switches** for each pooling layer. Each switch represents a map of
    the units with max activations of the forward pass. This map determines how to
    route the signal through the backward pass (you can read more about this in the
    source paper).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guided backpropagation**: This is a combination of deconvnet and regular
    backprop. It will only route signals that have positive forward activations in
    *l* and positive backward activations in *l+1*. This adds additional guidance
    signal (hence the name) from the higher layers to the regular backprop. In essence,
    this step prevents negative gradients from flowing through the backward pass.
    The rationale is that the units that act as suppressors of our starting unit will
    be blocked and the reconstructed image will be free of their influence. Guided
    backpropagation performs so well that it doesn''t need to use deconvnet switches
    and instead routes the signal to all the units in each pooling region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows a reconstructed image that was generated using
    guided backpropagation and AlexNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d5ea6a22-529b-43c2-b783-e5cff83efdd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From left to right: original image, color reconstruction, and grayscale reconstruction
    using guided backpropagation on AlexNet; these images were generated using https://github.com/utkuozbulak/pytorch-cnn-visualizations.'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-weighted class activation mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand gradient-weighted class activation mapping (*Grad-CAM: Visual
    Explanations from Deep Networks via Gradient-Based Localization*, [https://arxiv.org/abs/1610.02391](https://arxiv.org/abs/1610.02391)),
    let''s quote the source paper itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Grad-CAM uses the gradients of any target concept (say, the logits for ''dog''
    or even a caption) flowing into the final convolutional layer to produce a coarse
    localization map highlighting the important regions in the image for predicting
    the concept."'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Grad-CAM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bcef2eda-a402-4ce0-871f-60bdd70bc65b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Grad-CAM schema; Source: https://arxiv.org/abs/1610.02391'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: First, you start with a classification CNN model (for example, VGG).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, you feed the CNN with a single image and propagate it to the output layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Like we did in guided backpropagation, we take the output unit with maximum
    activation (equivalent to the predicted class ***c***), set its value to 1, and
    set all of the other outputs to 0\. In other words, create a one-hot encoded vector, *y^c*,
    of the prediction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, compute the gradient of *y^c* with respect to the feature maps, *A^k*,
    of the final convolutional layer, ![](img/01d05e9f-c408-4523-b0aa-7ca630c33961.png),
    using backpropagation. *i* and *j* are the cell coordinates in the feature map.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, compute the scalar weight, ![](img/7bd05250-83fc-45f6-ae8d-3900c9a2fce7.png),
    which measures the "importance" of feature map *k* for the predicted class, *c*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/423b3d51-d95b-493b-ace2-5735fb791876.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, compute a weighted combination between the scalar weights and the
    forward activation feature maps of the final convolutional layer and follow this
    with a ReLU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/8ac9e118-0da1-4eab-8b7d-0716b076b07b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that we multiply the scalar importance weight, ![](img/5c1c7470-1634-4c96-8d46-5d40c6d68f9a.png),
    by the tensor feature map, *A^k*. The result is a heatmap with the same dimensions
    as the feature map (14*×*14 in the case of VGG and AlexNet). It will highlight
    the areas of the feature map with the highest importance to class *c*. The ReLU
    discards the negative activations because we''re only interested in the features
    that increase *y^c*. We can upsample this heatmap back to the size of the input
    image and then superimpose it on it, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/decb3007-cf16-4fad-835a-52e5462ac541.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left to right: input image; upsampled heat-map; heat-map superimosed on the
    input (RGB); grayscale heat-map. The images were generated using https://github.com/utkuozbulak/pytorch-cnn-visualizations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One problem with Grad-CAM is upsampling the heatmap from 14*×*14 to 224*×*224
    because it doesn''t provide a fine-grained perspective of the important features
    for each class. To mitigate this problem, the authors of the paper proposed a
    combination of Grad-CAM and guided backpropagation (displayed in the Grad-CAM
    schema at the beginning of this section). We take the upsampled heatmap and combine
    it with the guided backprop visualization with element-wise multiplication. The
    input image contains two objects: a dog and a cat. Therefore, we can run Grad-CAM
    with both classes (the two rows of the diagram). This example shows how different
    classes detect different relevant features in the same image.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll discuss how to optimize CNNs with the help of regularization.
  prefs: []
  type: TYPE_NORMAL
- en: CNN regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we discussed in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*, a NN can approximate any function. But
    with great power comes great responsibility. The NN may learn to approximate the
    noise of the target function rather than its useful components. For example, imagine
    that we are training a NN to classify whether an image contains a car or not,
    but for some reason, the training set contains mostly red cars. It may turn out
    that the NN will associate the color red with the car, rather than its shape.
    Now, if the network sees a green car in inference mode, it may not recognize it
    as such because the color doesn't match. This problem is referred to as overfitting
    and it is central in machine learning (and even more so in deep networks). In
    this section, we'll discuss several ways to prevent it. Such techniques are collectively known
    as **regularization**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of NNs, these regularization techniques usually impose some
    artificial limitations or obstacles on the training process to prevent the network
    from approximating the target function too closely. They try to guide the network
    to learn generic rather than specific approximation of the target function in
    the hope that this representation will generalize well on previously unseen examples
    of the test dataset. You may already be familiar with many of these techniques,
    so we''ll keep it short:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input feature scaling**: ![](img/566be761-e32f-489f-a3f5-cb215784bef2.png). This
    operation scales all of the inputs in the [0, 1] range. For example, a pixel with
    intensity 125 would have a scaled value of ![](img/b247a222-8b4d-4945-9732-e5444b75478b.png).
    Feature scaling is fast and easy to implement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input standard score**:![](img/a42dd245-b0f6-4a2a-8fc6-e991a4b1f37e.png). Here,
    μ and σ are the mean and standard deviation of all of the training data. They are
    usually computed separately for each input dimension. For example, in an RGB image,
    we would compute the mean *μ* and *σ* for each channel. We should note that *μ*
    and *σ* have to be computed on the training data and then applied to the test
    data. Alternatively, we can compute *μ* and *σ* per sample if it''s not practical
    to compute them over the entire dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data augmentation**: This is where we artificially increase the size of the
    training set by applying random modifications (rotation, skew, scaling, and so
    on) on the training samples before feeding them to the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**L2 regularization** (or **weight decay**): Here, we add a special regularization
    term to the cost function. Let''s assume that we''re using MSE ([Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),* The
    Nuts and Bolts of NNs*, *Gradient descent* section). Here, the MSE + L2 regularization
    formula is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f0238ce8-2b02-4940-8b97-e734088e66ae.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *w[j]* is one of *k* total network weights and λ is the weight decay coefficient.
    The rationale is that if the network weights, *w[j]*, are large, then the cost
    function will also increase. In effect, weight decay penalizes large weights (hence
    the name). This prevents the network from relying too heavily on a few features
    associated with these weights. There is less chance of overfitting when the network
    is forced to work with multiple features. In practical terms, when we compute
    the derivative of the weight decay cost function (the preceding formula) with
    respect to each weight and then propagate it to the weights themselves, the weight
    update rule changes from ![](img/616423c0-4765-4633-992c-40af9f782b5c.png) to
    ![](img/97768311-0e57-46f7-ab19-d63b6d727f0d.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropout**: Here, we randomly and periodically remove some of the neurons
    (along with their input and output connections) from the network. During a training
    mini-batch, each neuron has a probability, *p*, of being stochastically dropped.
    This is to ensure that no neuron ends up relying too much on other neurons and
    "learns" something useful for the network instead.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch Normalization** (**BN**, *Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift*, [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)): This
    is a way to apply data processing, similar to the standard score, for the hidden
    layers of the network. It normalizes the outputs of the hidden layer for each
    mini-batch (hence the name) in a way that maintains its mean activation value
    close to 0 and its standard deviation close to 1. Let''s say ![](img/b32be79b-c498-426d-ab86-63943dc09652.png) is
    a mini-batch of size *n*. Each sample of *D* is a vector, ![](img/a3c7bc7c-e67d-4c0d-9550-05aa36f4a3a0.png),
    and ![](img/d8826d10-f9e0-4724-a977-eb5da755842e.png)is a cell with an index *k*
    of that vector. For the sake of clarity, we''ll omit the (*k*) superscript in
    the following formulas; that is, we''ll write *x[i]*, but we''ll mean ![](img/a7f2a55b-b380-4a97-92fa-5458f6ff024d.png).
    We can compute BN for each activation, *k*, over the whole minibatch in the following
    way:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[![](img/a7b5373c-098a-413c-833d-83609bcd6a1e.png)]: This is the mini-batch
    mean. We compute *μ* separately for each location, *k*, over all samples.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d86ffa01-2a3f-43f1-8a3e-efbc2731d556.png): This is the mini-batch standard
    deviation. We compute *σ* separately for each location, *k*, over all samples.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7acf3e8f-0a11-47d6-8cc1-0aae11396f2d.png): We normalize each sample. *ε*
    is a constant that''s added for numerical stability.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/7a6d44a8-9740-4439-9462-217732c87adc.png): *γ* and *β* are learnable
    parameters and we compute them over each location, *k* (*γ^((k))* and *β^((k))*),
    over all of the samples of the mini-batch (the same applies for *μ* and *σ*).
    In convolutional layers, each sample, *x*, is a tensor with multiple feature maps.
    To preserve the convolutional property, we compute *μ* and *σ* per location over
    all of the samples, but we use the same *μ* and *σ* in the matching locations
    across all of the feature maps. On the other hand, we compute *γ* and *β* per
    feature map, rather than per location.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: This section concludes our analysis of the structure and inner workings of CNNs.
    At this point, we would normally proceed with some sort of CNN coding example.
    But in this book, we want to do things a little differently. Therefore, we won't
    implement a plain old feed-forward CNN, which you may have already done before.
    Instead, in the next section, you will be introduced to the technique of transfer
    learning—a way to use pretrained CNN models for new tasks. But don't worry—we'll
    still implement a CNN from scratch. We'll do this in [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks.* In this way, we'll be able to create a more
    complex network architecture using our knowledge from that chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's say that we want to train a model on a task that doesn't have readily
    available labeled training data like ImageNet does. Labeling training samples
    could be expensive, time-consuming, and error-prone. So, what does a humble engineer
    do when they want to solve a real ML problem with limited resources? Enter **Transfer
    Learning** (**TL**).
  prefs: []
  type: TYPE_NORMAL
- en: TL is the process of applying an existing trained ML model to a new, but related,
    problem. For example, we can take a network trained on ImageNet and repurpose
    it to classify grocery store items. Alternatively, we could use a driving simulator
    game to train a neural network to drive a simulated car and then use the network
    to drive a real car (but don't try this at home!). TL is a general ML concept
    that's applicable to all ML algorithms, but in this context, we'll talk about
    CNNs. Here's how it works.
  prefs: []
  type: TYPE_NORMAL
- en: We start with an existing pretrained network. The most common scenario is to
    take a pretrained network from ImageNet, but it could be any dataset. TensorFlow
    and PyTorch both have popular ImageNet pretrained neural architectures that we
    can use. Alternatively, we can train our own network with a dataset of our choice.
  prefs: []
  type: TYPE_NORMAL
- en: The fully-connected layers at the end of a CNN act as translators between the
    network's language (the abstract feature representations learned during training)
    and our language, which is the class of each sample. You can think of TL as a
    translation into another language. We start with the network's features, which
    is the output of the last convolutional or pooling layer. Then, we translate them
    into a different set of classes of the new task. We can do this by removing the
    last fully-connected layer (or all the fully-connected layers) of an existing
    pretrained network and replacing it with another layer, which represents the classes
    of the new problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the TL scenario shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b755ece2-264b-4069-b880-ce0486c1ffef.png)'
  prefs: []
  type: TYPE_IMG
- en: In TL, we can replace the fully-connected layer(s) of a pretrained net and repurpose
    it/them for a new problem
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we cannot do this mechanically and expect the new network to work
    because we still have to train the new layer with data related to the new task.
    Here, we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use the original part of the network as a feature extractor and only train
    the new layer(s)**: In this scenario, we feed the network a training batch of
    the new data and propagate it forward to see the network''s output. This part
    works just like regular training would. But in the backward pass, we lock the
    weights of the original network and only update the weights of the new layers.
    This is the recommended way to do things when we have limited training data for
    the new problem. By locking most of the network weights, we prevent overfitting
    on the new data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tune the whole network**: In this scenario, we''ll train the whole network
    and not just the newly added layers at the end. It is possible to update all of
    the network weights, but we can also lock some of the weights in the first layers.
    The idea here is that the initial layers detect general features—not related to
    a specific task—and it makes sense to reuse them. On the other hand, the deeper
    layers may detect task-specific features and it would be better to update them.
    We can use this method when we have more training data and don''t need to worry
    about overfitting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing transfer learning with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know what TL is, let's look at whether it works in practice. In
    this section, we'll apply an advanced ImageNet pretrained network on the CIFAR-10
    images with **PyTorch 1.3.1** and the `torchvision` 0.4.2 package. We'll use both
    types of TL. It's preferable to run this example on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: This example is partially based on [https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py](https://github.com/pytorch/tutorials/blob/master/beginner_source/transfer_learning_tutorial.py).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Do the following imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Define `batch_size` for convenience:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the training dataset. We have to consider a few things:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The CIFAR-10 images are 32*×*32, while the ImageNet network expects 224*×*224
    input. Since we are using an ImageNet-based network, we'll upsample the 32*×*32
    CIFAR images to 224*×*224.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Standardize the CIFAR-10 data using the ImageNet mean and standard deviation
    since this is what the network expects.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We''ll also add some data augmentation in the form of random horizontal or
    vertical flips:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow the same steps with the validation/test data, but this time without
    augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Choose `device`, preferably a GPU with a fallback on CPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the training of the model. Unlike TensorFlow, in PyTorch, we have to
    iterate over the training data manually. This method iterates once over the whole
    training set (one epoch) and applies the optimizer after each forward pass:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the testing/validation of the model. This is very similar to the training
    phase, but we will skip the backpropagation part:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the first TL scenario, where we use the pretrained network as a feature
    extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll use a popular network known as ResNet-18\. We'll talk about this in detail
    in the *Advanced network architectures* section. PyTorch will automatically download
    the pretrained weights.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace the last network layer with a new layer with 10 outputs (one for each
    CIFAR-10 class).
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Exclude the existing network layers from the backward pass and only pass the
    newly added fully-connected layer to the Adam optimizer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Run the training for `epochs` and evaluate the network accuracy after each epoch.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Plot the test accuracy with the help of the `plot_accuracy` function. Its definition
    is trivial and you can find it in this book's code repository.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the `tl_feature_extractor` function, which implements all
    of this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Implement the fine-tuning approach. This function is similar to `tl_feature_extractor`,
    but here, we''re training the whole network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can run the whole thing in one of two ways:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Call `tl_fine_tuning()` to use the fine-tuning TL approach for five epochs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Call `tl_feature_extractor()` to train the network with the feature extractor
    approach for five epochs.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is the accuracy of the networks after five epochs for the two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17c9041d-7c61-440d-abc9-4065268247ba.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: Feature extraction TL accuracy; right: Fine-tuning TL accuracy'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the large size of the chosen `ResNet18` pretrained model, the network
    starts to overfit in the feature extraction scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning with TensorFlow 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll implement the two transfer learning scenarios again,
    but this time using **TensorFlow 2.0.0 (TF)**. In this way, we can compare the
    two libraries. Instead of `ResNet18`, we'll use the `ResNet50V2` architecture
    (more on that in the [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks*). In addition to TF, this example also requires
    the TF Datasets 1.3.0 package ([https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets)),
    a collection of various popular ML datasets.
  prefs: []
  type: TYPE_NORMAL
- en: This example is partially based on [https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb](https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, first, we need to do the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll define the mini-batch and input images sizes (the image size is
    determined by the network architecture):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll load the CIFAR-10 dataset with the help of TF datasets. The `repeat()`
    method allows us to reuse the dataset for multiple epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll define the `train_format_sample` and `test_format_sample` functions,
    which will transform the input images into suitable CNN inputs. These functions
    play the same roles that the `transforms.Compose` object plays, which we defined
    in the *Implementing transfer learning with PyTorch* section. The input is transformed
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The images are resized to 96*×*96, which is the expected network input size.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each image is standardized by transforming its values so that it's in the (-1;
    1) interval.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The labels are transformed for one-hot encoding.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The training images are randomly flipped horizontally and vertically.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at the actual implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is some boilerplate code that assigns these transformers to the train/test
    datasets and splits them into mini-batches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to define the feature extraction model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll use Keras for the pretrained network and model definition since it is
    an integral part of TF 2.0.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We load the `ResNet50V2` pretrained net, excluding the final fully-connected
    layers.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we call `base_model.trainable = False`, which *freezes* all of the network
    weights and prevents them from training.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we add a `GlobalAveragePooling2D` operation, followed by a new and
    trainable fully-connected trainable layer at the end of the network.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code implements this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll define the fine-tuning model. The only difference it has from
    the feature extraction is that we only freeze some of the bottom pretrained network
    layers (as opposed to all of them). The following is the implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we''ll implement the `train_model` function, which trains and evaluates
    the models that are created by either the `build_fe_model` or `build_ft_model`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We can run either the feature extraction or fine-tuning TL using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`train_model(build_ft_model())`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_model(build_fe_model())`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TF will automatically use the machine GPU if one is available; otherwise, it
    will revert to the CPU. The following diagram shows the accuracy of the networks
    after five epochs for the two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c73d77d-462f-4bfe-8c5d-2e9e7d7f9c64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: Feature extraction TL; right: Fine-tuning TL'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter with a quick recap of CNNs and discussed transposed,
    depthwise separable, and dilated convolutions. Next, we talked about improving
    the performance of CNNs by representing the convolution as a matrix multiplication
    or with the Winograd convolution algorithm. Then, we focused on visualizing CNNs
    with the help of guided backpropagation and Grad-CAM. Next, we discussed the most
    popular regularization techniques. Finally, we learned about transfer learning
    and implemented the same TL task with both PyTorch and TF as a way to compare
    the two libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll discuss some of the most popular advanced CNN architectures.
  prefs: []
  type: TYPE_NORMAL
