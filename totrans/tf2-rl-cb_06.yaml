- en: '*Chapter 6*: Reinforcement Learning in the Real World – Building Intelligent
    Agents to Complete Your To-Dos'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An RL Agent needs to interact with the environment to learn and train. Training
    RL Agents for real-world applications usually comes with physical limitations
    and challenges. This is because the Agent could potentially cause damage to the
    real-world system it is dealing with while learning. Fortunately, there are a
    lot of tasks in the real world that do not necessarily have such challenges, and
    yet can be very useful for completing the day-to-day real-world tasks that are
    available in our To-Do lists!
  prefs: []
  type: TYPE_NORMAL
- en: The recipes in this chapter will help you build RL Agents that can complete
    tasks on the internet, ranging from responding to annoying popups, booking flights
    on the web, managing emails and social media accounts, and more. We can do all
    of this without using a bunch of APIs that change over time or utilizing hardcoded
    scripts that stop working when a web page is updated. You will be training the
    Agents to complete such To-Do tasks by using the mouse and keyboard, just like
    how a human would! This chapter will also help you build the **WebGym** API, which
    is an OpenAI Gym-compatible generic RL learning environment interface that you
    can use to convert more than 50+ web tasks into training environments for RL and
    train your own RL Agents.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following recipes will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Building learning environments for real-world RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building an RL Agent to complete tasks on the web – Call to Action
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a visual auto-login bot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an RL Agent to automate flight booking for your travel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an RL Agent to manage your emails
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an RL Agent to automate your social media account management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in this book has been extensively tested on Ubuntu 18.04 and Ubuntu
    20.04, which means it should work with later versions of Ubuntu if Python 3.6+
    is available. With Python 3.6+ installed, along with the necessary Python packages
    listed in the Getting ready sections of each recipe, the code should run fine
    on Windows and Mac OSX too. It is advised that you create and use a Python virtual
    environment named `tf2rl-cookbook` to install the packages and run the code in
    this book. Installing Miniconda or Anaconda for Python virtual environment management
    is recommended. You will also need to install the Chromium chrome driver on your
    system. On Ubuntu 18.04+, you can install it by using the `sudo apt-get install
    chromium-chromedriver` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code for each recipe in each chapter will be available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Building learning environments for real-world RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This recipe will teach you how to set up and build WebGym, a **World of Bits**
    (**WoB**)-based OpenAI Gym compatible learning platform for training RL Agents
    for world wide web-based real-world tasks. WoB is an open domain platform for
    web-based Agents. For more information about WoB, check out the following link:
    [http://proceedings.mlr.press/v70/shi17a/shi17a.pdf](http://proceedings.mlr.press/v70/shi17a/shi17a.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: WebGym provides learning environments for Agents to perceive the world wide
    web how we (humans) perceive it – using the pixels rendered on our display screen.
    The Agent interacts with the environment using keyboard and mouse events as actions.
    This allows the Agent to experience the world wide web how we do, which means
    we don't need to make any additional modifications for the Agents to train. This
    allows us to train RL Agents that can directly work with web-based pages and applications
    to complete real-world tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows a sample **Click-To-Action** (**CTA**) environment,
    where the task is to click on a specific link to get to the next page or step
    in the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Sample CTA task requiring a specific link to be clicked ](img/B15074_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Sample CTA task requiring a specific link to be clicked
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example of a CTA task is depicted in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Sample CTA task requiring a specific option to be selected and
    submitted ](img/B15074_06_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Sample CTA task requiring a specific option to be selected and
    submitted
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment. Make sure that you update the environment so that it matches
    the latest conda environment specification file (`tfrl-cookbook.yml`) in this
    cookbook's code repository. WebGym is built on top of the miniwob-plusplus benchmark,
    which has also been made available as part of this book's code repository for
    ease of use.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will build WebGym by defining the custom `reset` and `step` methods. Then,
    we will define the state and action spaces for the training environments. First,
    we''ll look at the implementation of the `miniwob_env` module. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by importing the necessary Python modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s specify the directory where we will import the local `miniwob` environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can start to subclass `MiniWoBEnvironment`. We can then call the super
    class''s initialization function to initialize the environment and set the values
    for `base_url` before we configure the `miniwob` environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s time to customize the `reset(…)` method. To allow environments to be
    randomized, we will use a `seeds` argument to take a random seed. This can be
    used to generate random start states and tasks so that the Agent we train does
    not overfit to a fixed/static web page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will redefine the `step(…)` method. Let''s complete the implementation
    in two steps. First, we will define the method with docstrings that explain the
    arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will complete our implementation of the `step(…)` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our `MiniWoBEnv` class implementation! To test our class implementation
    and to understand how to use the class, we will write a quick `main()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can save the preceding script as `miniwob_env.py` and execute it to see
    the sample environment being acted on by a random Agent. In the next few steps,
    we will extend `MiniWoBEnv` in order to create an OpenAI Gym-compatible learning
    environment interface. Let''s begin by creating a new file named `envs.py` and
    include with the following imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the first environment, we will implement the `MiniWoBVisualClickEnv` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also define the observation and action space for this environment in
    the `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will further extend the `reset(…)` method to provide an OpenAI Gym-compatible
    interface method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next important piece is the `step` method. We will implement it in the
    following two steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To complete the `step` method''s implementation, let''s check if the dimensions
    of the actions are as expected and then bind the actions if necessary. Finally,
    we must execute a step in the environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can use a descriptive name for the class to register the environment with
    the Gym registry:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, to register the environment with OpenAI Gym''s registry locally, we
    must add the environment registration information to the `__init__.py` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, we have completed this recipe!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have extended the implementation of `MiniWoB-plusplus` in `MiniWoBEnv` so
    that we can use file-based web pages to represent tasks. We extended the `MiniWoBEnv`
    class even further to provide an OpenAI Gym-compatible interface in `MiniWoBVisualClickEnv`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a clear picture of how an RL Agent will be learning to complete the
    task in this environment, consider the following screenshot. Here, the Agent tries
    to understand the objective of the task by trying out different actions, which
    in this environment translates to clicking on different areas of the web page
    (represented on the right-hand side by blue dots). Eventually, the RL Agent clicks
    on the correct button and starts to understand what the task description means,
    as well as what the buttons are intended for, since it was rewarded for clicking
    on the correct spot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Visualizing the Agent''s actions while it''s learning to complete
    the CTA task ](img/B15074_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Visualizing the Agent's actions while it's learning to complete
    the CTA task
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to move on to the next recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Building an RL Agent to complete tasks on the web – Call to Action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This recipe will teach you how to implement an RL training script so that you
    can train an RL Agent to handle `OK`/`Cancel` dialog boxes, where you need you
    to click to acknowledge/dismiss the pop-up notification, and the `Click to learn
    more` button. In this recipe, you will instantiate a RL training environment that
    provides visual rendering for the web pages containing a CTA task. You will be
    training a **proximal policy optimization** (**PPO**)-based deep RL Agent that's
    been implemented using TensorFlow 2.x to learn how to complete the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image illustrates a set of observations from a randomized CTA
    environment (with different seeds) so that you understand the task that the Agent
    will be solving:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Screenshot of the Agent''s observations from a randomized CTA
    environment ](img/B15074_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Screenshot of the Agent's observations from a randomized CTA environment
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment. Make sure that you update the environment so that it matches
    the latest conda environment specification file (`tfrl-cookbook.yml`) in this
    cookbook''s code repository. If the following `import` statements run without
    any issues, then you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will be implementing a complete training script, including
    command-line argument parsing for training hyperparameter configuration. As you
    may have noticed from the `import` statements, we will be using Keras's functional
    API for TensorFlow 2.x to implement the **deep neural networks** (**DNNs**) we
    will be using as part of the Agent's algorithm implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will guide you through the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by defining the command-line arguments for the CTA Agent training
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create a TensorBoard logger so that we can log and visualize
    the live training progress of the CTA Agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following steps, we will implement the `Actor` class. However, we will
    begin by implementing the `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define the DNN that will represent the Actor''s model. We will
    split the implementation of the DNN into multiple steps as it''s going to be a
    bit long due to several neural network layers being stacked together. As the first
    and main processing step, we will implement a block by stacking convolution-pooling-convolution-pooling
    layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will flatten the output from the pooling layer so that we can start
    using fully connected or dense layers with dropout to generate the output we expect
    from the Actor network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to scale and clip the predicted value so that the values are bounded
    and lie within the range we expect the actions to be in. Let''s use the **Lambda
    layer** to implement custom clipping and scaling, as shown in the following code
    snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our `nn_model` implementation. Now, let''s define a convenience
    function to get an action, given a state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it''s time to implement the main train method. This will update the parameters
    of the Actor network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Although we are using `compute_loss` and `log_pdf` in the preceding `train`
    method, we haven''t really defined them yet! Let''s implement them one after the
    other, starting with the `compute_loss` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will implement the `log_pdf` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous step concludes out Actor implementation. Now, it''s time to start
    implementing the `Critic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next up is the `Critic` class''s neural network model. Like the Actor''s neural
    network model, this is going to be a DNN. We will split the implementation into
    a few steps. First, let''s implement a convolution-pooling-convolution-pooling
    block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While we could stack more blocks or layers to deepen the neural network, for
    our current task, we already have a sufficient number of parameters in the DNN
    to learn how to perform well at the CTA task. Let''s add the fully connected layers
    so that we can eventually produce the state-conditioned action value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement a method that will compute the Critic''s learning loss, which
    is essentially the mean-squared error between the temporal difference learning
    targets and the values predicted by the Critic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s finalize our `Critic` class by implementing the `train` method to update
    the Critic''s parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can utilize the Actor and the Critic implementation to build our PPO
    Agent so that it can work with high-dimensional (image) observations. Let''s begin
    by defining the `PPOAgent` class''s `__init__` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will be using **Generalized Advantage Estimates** (**GAE**) to update our
    policy. So, let''s implement a method that will calculate the GAE target values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are at the core of this script! Let''s define the training routine for the
    deep PPO Agent. We will split the implementation into multiple steps to make it
    easy to follow. We will begin with the outermost loop, which must be running for
    a configurable maximum number of episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement the logic for stepping through the environment and
    handling the end of an episode by checking the `done` values from the environments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement the logic that will check for the end of an episode
    or if it is time to update and perform an update step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the updated GAE targets, we can train the Actor and Critic
    networks and log the losses and other training metrics for tracking purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s implement the `__main__` function to train the CTA Agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That completes this recipe! Let's briefly recap on how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a PPO-based deep RL Agent and provided a training
    mechanism to develop a CTA Agent. Note that for simplicity, we used one instance
    of the environment, though the code can scale for a greater number of environment
    instances to speed up training.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand how the Agent training progresses, consider the following sequence
    of images. During the initial stages of training, when the Agent is trying to
    understand the task and the objective of the task, the Agent may just be executing
    random actions (exploration) or even clicking outside the screen, as shown in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Agent clicking outside the screen (no visible blue dot) during
    initial exploration](img/B15074_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Agent clicking outside the screen (no visible blue dot) during
    initial exploration
  prefs: []
  type: TYPE_NORMAL
- en: 'As the Agent learns by stumbling upon the correct button to click, it starts
    to make progress. The following screenshot shows the Agent making some progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Deep PPO Agent making progress in the CTA task ](img/B15074_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Deep PPO Agent making progress in the CTA task
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, when the episode is complete or ends (due to a time limit), the Agent
    receives an observation similar to the one shown in the following screenshot (left):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – End of episode observation (left) and summary of performance
    (right) ](img/B15074_06_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – End of episode observation (left) and summary of performance (right)
  prefs: []
  type: TYPE_NORMAL
- en: Now, it's time to move on to the next recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Building a visual auto-login bot
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Imagine that you have an Agent or a bot that watches what you are doing and
    automatically logs you into websites whenever you click on a login screen. While
    browser plugins exist that can automatically log you in, they do so using hardcoded
    scripts that only work on the pre-programmed website's login URLs. But what if
    you had an Agent that only relied on the rendered web page – just like you do
    to perform a task – and worked even when the URL changes and when you are on a
    new website with no prior saved data? How cool would that be?! This recipe will
    help you develop a script that will train an Agent to log in on a web page! You
    will learn how to randomize, customize, and increase the generality of the Agent
    to get it to work on any login screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of randomizing and customizing the usernames and passwords for a
    task can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Sample observations from a randomized user login task ](img/B15074_06_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Sample observations from a randomized user login task
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, make sure you have the latest version. First, you
    will need to activate the `tf2rl-cookbook` Python/conda virtual environment. Make
    sure that you update the environment so that it matches the latest conda environment
    specification file (`tfrl-cookbook.yml`) in this cookbook''s code repository.
    If the following `import` statements run without any issues, then you are ready
    to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will implement the deep RL-based Login Agent using the PPO
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s set up the training script''s command-line arguments and logging:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now directly jump into the `Critic` class''s definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s define the DNN for the Critic model. We''ll begin by implementing
    a perception block composed of convolution-pooling-convolution-pooling. In the
    subsequent steps, we''ll add more depth to the network by stacking another perception
    block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will add another perception block so that we can extract more features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will add a flattening layer, followed by fully connected (dense) layers,
    to bring down the shape of the network''s output to a single action value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To finalize our Critic implementation, let''s define the `compute_loss` method
    and the `update` method in order to train the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now work on implementing the `Actor` class. We''ll initialize the `Actor`
    class in this step and continue our implementation in the subsequent steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will use a similar DNN architecture for our Actor as we did in our Critic
    implementation. So, the `nn_model` method''s implementation will remain the same
    except for the last few layers, where the Actor and Critic''s implementation will
    vary. The Actor network model produces the mean and the standard deviation as
    output. This depends on the action space dimensions. On the other hand, the Critic
    network produces a state-conditioned action value, irrespective of the dimensions
    of the action space. The layers that differ from the Critic''s DNN implementation
    are listed here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement some methods that will compute the Actor''s loss and `log_pdf`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the help of these helper methods, our training method implementation becomes
    simpler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s implement a method that will get an action from the Actor when
    it''s given a state as input:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our Actor implementation. We can now tie both the Actor and
    the Critic together using the `PPOAgent` class implementation. Since the GAE target
    calculations were discussed in the previous recipe, we will skip this and focus
    on the training method''s implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Agent''s update is performed at a preset frequency in terms of the number
    of samples collected or at the end of every episode – whichever occurs first:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can run `MiniWoBLoginUserVisualEnv-v0` and train the Agent using
    the following snippet of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That completes our script for the auto-login Agent. It's time for you to run
    the script to see the Agent's training process in action!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The login task involves clicking on the correct form field and typing in the
    correct username and/or password. For an Agent to be able to do this, it needs
    to master how to use a mouse and keyboard, in addition to processing the visual
    web page to understand the task and the web login form. With enough samples, the
    deep RL Agent will learn a policy to complete this task. Let's take a look at
    the state of the Agent's progress, snapshotted at different stages.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows the Agent successfully entering the username and
    correctly clicking on the password field to enter the password, but not being
    able to complete the task yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Screenshot of a trained Agent successfully entering the username
    but not a password ](img/B15074_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Screenshot of a trained Agent successfully entering the username
    but not a password
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following image, you can see that the Agent has learned to enter both
    the username and password, but they are not quite right for the task to be classed
    as complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Agent entering both the username and password but incorrectly
    ](img/B15074_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Agent entering both the username and password but incorrectly
  prefs: []
  type: TYPE_NORMAL
- en: The same Agent with a different checkpoint, after several thousand more episodes
    of learning, is close to completing the task, as shown in the following image
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – A well-trained Agent model about to complete the login task
    successfully ](img/B15074_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – A well-trained Agent model about to complete the login task successfully
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand how the Agent works and behaves, you can customize it
    to your liking and use use cases to train the Agent to automatically log into
    any custom website you want!
  prefs: []
  type: TYPE_NORMAL
- en: Training an RL Agent to automate flight booking for your travel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, you will learn how to implement a deep RL Agent based on the
    `MiniWoBBookFlightVisualEnv` flight booking environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Sample start-state observations from the randomized MiniWoBBookFlightVisualEnv
    environment ](img/B15074_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – Sample start-state observations from the randomized MiniWoBBookFlightVisualEnv
    environment
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment. Make sure that you update the environment so that it matches
    the latest conda environment specification file (`tfrl-cookbook.yml`) in this
    cookbook''s code repository. If the following `import` statements run without
    any issues, then you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will be implementing a complete training script that you
    will be able to customize and train to book flights!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s expose the hyperparameters as configurable arguments to the training
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll set up TensorBoard logging for live visualization of the training
    progress:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll be using a Replay Buffer to implement Experience Reply. Let''s implement
    a simple `ReplayBuffer` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s start by implementing the `Actor` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The DNN model for the Actor will be composed of two perception blocks, each
    containing convolution-pooling-convolution-pooling layers, as in our previous
    recipe. We''ll skip this here and look at the implementation of the `train` method
    instead. The full source code, as always, will be available in this cookbook''s
    code repository. Let''s continue with our `train` and `predict` method implementations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last piece of our `Actor` class is to implement a function to get the action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, our `Actor` class is ready. Now, we can move on and implement the
    `Critic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Similar to the `Actor` class''s DNN model, we will be reusing a similar architecture
    for our `Critic` class from the previous recipe, with two perception blocks. You
    can refer to the full source code of this recipe or the DNN implementation in
    the previous recipe for completeness. Let''s jump into the implementation of the
    `predict` and `g_gradients` computations for the Q function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to update our Critic model, we need a loss to drive the parameter
    updates and an actual training step to perform the update. In this step, we will
    implement these two core methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s time to bring the Actor and Critic together to implement the DDPGAgent!
    Let''s dive into it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s implement a method that will update the target models of our Actor and
    Critic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement a method that will compute the temporal difference
    targets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because we are using a deterministic policy gradient and a policy without a
    distribution to sample from, we will be using a noise function to sample around
    the action predicted by the Actor network. The **Ornstein Uhlenbeck** (**OU**)
    noise process is a popular choice for DDPG Agents. We''ll implement this here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement a method that will replay the experience from the replay
    buffer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last but most crucial thing we must do in our Agent implementation is implement
    the `train` method. We will split the implementation into a few steps. First,
    we will start with the outermost loop, which must run for a maximum number of
    episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement the inner loop, which will run until the end of an
    episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are not done yet! We still need to update our Replay Buffer with the new
    experience that the Agent has collected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Are we done?! Almost! We just have to remember to replay the experience when
    the buffer size is bigger than the batch size we used for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our implementation. Now, we can launch the Agent training on
    the Visual Flight Booking environment using the following `__main__` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That's it!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DDPG Agent collects a series of samples from the flight booking environment
    as it explores and uses this experience to update its policy parameters through
    the Actor and Critic updates. The OU noise we discussed earlier allows the Agent
    to explore while using a deterministic action policy. The flight booking environment
    is quite complex as it requires the Agent to master both the keyboard and the
    mouse, in addition to understanding the task by looking at visual images of the
    task description (visual text parsing), inferring the intended task objective,
    and executing the actions in the correct sequence. The following screenshot shows
    the performance of the Agent upon completing a sufficiently large number of episodes
    of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – A screenshot of the Agent performing the flight booking task
    at different stages of learning ](img/B15074_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – A screenshot of the Agent performing the flight booking task at
    different stages of learning
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the Agent''s screen after the Agent progressed
    to the final stage of the task (although it''s not close to completing the task):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Screenshot of the Agent progressing all the way to the final
    stage of the flight booking task ](img/B15074_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Screenshot of the Agent progressing all the way to the final stage
    of the flight booking task
  prefs: []
  type: TYPE_NORMAL
- en: With that, we will move on to the next recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Training an RL Agent to manage your emails
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Email has become an integral part of many people''s lives. The number of emails
    that an average working professional goes through in a workday is growing daily.
    While a lot of email filters exist for spam control, how nice would it be to have
    an intelligent Agent that can perform a series of email management tasks that
    just provide a task description (through text or speech via speech-to-text) and
    are not limited by any APIs that have rate limits? In this recipe, you will develop
    a deep RL Agent and train it on email management tasks! A set of sample tasks
    can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – A sample set of observations from the randomized MiniWoBEmailInboxImportantVisualEnv
    environment ](img/B15074_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – A sample set of observations from the randomized MiniWoBEmailInboxImportantVisualEnv
    environment
  prefs: []
  type: TYPE_NORMAL
- en: Let's get into the details!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, make sure you have the latest version. First, you
    will need to activate the `tf2rl-cookbook` Python/conda virtual environment. Make
    sure that you update the environment so that it matches the latest conda environment
    specification file (`tfrl-cookbook.yml`) in this cookbook''s code repository.
    If the following `import` statements run without any issues, then you are ready
    to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to implement a deep RL Agent and train it to manage important
    emails:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will define an `ArgumentParser` so that we can configure the script
    from the command line. For a complete list of configurable hyperparameters, please
    refer to the source code for this recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s set up TensorBoard logging:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can initialize the `Actor` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because the observations in our email management environment are visual (images),
    we will need perception capabilities for the Actor in our Agent. For this, we
    must make use of convolution-based perception blocks, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s add more perception blocks comprising convolutions, followed by
    max pooling layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to flatten the DNN output to produce the mean (mu) and standard
    deviation that we want as the output from the Actor. First, let''s add the flattening
    layer and the dense layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to define the final layers of our Actor network. These will
    helps us produce `mu` and `std`, as we discussed in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our Actor''s DNN model implementation. To implement the remaining
    methods to complete the Actor class, please refer to the full code for this recipe,
    which can be found in this cookbook''s code repository. We will now focus on defining
    the interfaces for the `Critic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Critic''s DNN model is also based on the same convolutional neural network
    architecture that we used for the `Actor`. For completeness, please refer to this
    recipe''s full source code, which is available in this cookbook''s code repository.
    We will implement the loss computation and training method here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, we can now define our Agent''s class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code should be familiar to you from the previous Agent implementations
    in this chapter. You can complete the remaining methods (and the training loop)
    based on our previous implementations. If you get stuck, you can refer to the
    full source code for this recipe by going to this cookbook''s code repository.
    We will now write the `__main__` function so that we can train the Agent in `MiniWoBEmailInboxImportantVisualEnv`.
    This will allow us to see the Agent''s learning process in action:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PPO Agent uses convolutional neural network layers to process the high-dimensional
    visual inputs in the Actor and Critic classes. The PPO algorithm updates the Agent's
    policy parameters using a surrogate loss function that prevents the policy parameters
    from being drastically updated. It then keeps the policy updates within the trust
    region, which makes it robust to hyperparameter choices and a few other factors
    that may lead to instability during the Agent's training regime. The email management
    environment poses as a nice sequential decision-making problem for the deep RL
    Agent. First, the Agent has to choose the correct email from a series of emails
    in an inbox and then perform the desired action (starring the email and so on).
    The Agent only has access to the visual rendering of the inbox, so it needs to
    extract the task specification details, interpret the task specification, and
    then plan and execute the actions!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot of the Agent''s performance at different stages
    of learning (loaded from different checkpoints):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – A series of screenshots showing the Agent''s learning progress
    ](img/B15074_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – A series of screenshots showing the Agent's learning progress
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the next recipe!
  prefs: []
  type: TYPE_NORMAL
- en: Training an RL Agent to automate your social media account management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By the end of this recipe, you will have built a complete deep RL Agent training
    script that can be trained to perform management tasks on your social media account!
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows a series of (randomized) tasks from the environment
    that we will be training the Agent in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – A sample set of social media account management tasks that
    the Agent has been  asked to solve ](img/B15074_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – A sample set of social media account management tasks that the
    Agent has been asked to solve
  prefs: []
  type: TYPE_NORMAL
- en: Note that there is a scroll bar in this task that the Agent needs to learn how
    to use! The tweet that's relevant to this task may be hidden from the visible
    part of the screen, so the Agent will have to actively explore (by sliding the
    scroll bar up/down) in order to progress!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will need to activate the `tf2rl-cookbook` Python/conda
    virtual environment. Make sure that you update the environment so that it matches
    the latest conda environment specification file (`tfrl-cookbook.yml`) in this
    cookbook''s code repository. If the following `import` statements run without
    any issues, then you are ready to get started:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's start by configuring the Agent training script. After that, you will be
    shown how to complete the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s jump right into the implementation! We will begin with our `ReplayBuffer`
    implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement our `Actor` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next core piece is the DNN definition for our Actor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Depending on the complexity of the task, we can modify (add/reduce) the depth
    of the DNN. We will start by connecting the output of the pooling layers to the
    fully connected layers with dropout:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That completes our DNN model implementation for the Actor. Now, let''s implement
    the methods that will train the Actor and get the predictions from the Actor model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now get the actions from our Actor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s get started with our Critic implementation. Here, we will need to implement
    the Agent class that we are after:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that the Critic''s model is initialized with `self.nn_model()`. Let''s
    implement this here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will complete our DNN architecture for the Critic by funneling the output
    through the fully connected layers with dropout. This way, we receive the necessary
    action values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s implement the `g_gradients` and `compute_loss` methods. This should
    be pretty straightforward:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can complete the Critic''s implementation by implementing the `predict`
    and `train` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now utilize the Actor and Critic to implement our Agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will implement the `update_target` method, as per the DDPG algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will not look at the implementation of the `train` method here. Instead,
    we will start the outer loop''s implementation, before completing it in the following
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The main inner loop''s implementation is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That completes our training method implementation. For the implementation of
    the `replay_experience`, `add_ou_noise`, and `get_td_targets` methods, please
    refer to the full source code of this recipe, which can be found in this cookbook's
    code repository.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s write our `__main__` function so that we can start training the Agent
    in our social media environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s visually explore how a well-trained Agent progresses through social
    media management tasks. The following screenshot shows the Agent learning to use
    the scroll bar to "navigate" in this environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – The Agent learning to navigate using the scroll bar  ](img/B15074_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – The Agent learning to navigate using the scroll bar
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the task specification does not imply anything related to the scroll
    bar or the navigation, and that the Agent was able to explore and figure out that
    it needs to navigate in order to progress with the task! The following screenshot
    shows the Agent progressing much further by choosing the correct tweet but clicking
    on the wrong action; that is, `Embed Tweet` instead of the `Mute` button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – The Agent clicking on Embed Tweet when the goal was to click
    on Mute ](img/B15074_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – The Agent clicking on Embed Tweet when the goal was to click on
    Mute
  prefs: []
  type: TYPE_NORMAL
- en: After 96 million episodes of training, the Agent was sufficiently able to solve
    the task. The following screenshot shows the Agent's performance on an evaluation
    episode (the Agent was loaded from a checkpoint)
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – The Agent loaded from trained parameters about to complete
    the task successfully  ](img/B15074_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – The Agent loaded from trained parameters about to complete the
    task successfully
  prefs: []
  type: TYPE_NORMAL
- en: That concludes this recipe and this chapter. Happy training!
  prefs: []
  type: TYPE_NORMAL
