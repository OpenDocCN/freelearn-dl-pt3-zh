- en: '*Chapter 10*: Road Ahead'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the final chapter of the book. We have learned about and implemented
    many generative models; and yet there are a lot more models and applications that
    we have not covered as they are beyond the scope of this book. In this chapter,
    we will start by summarizing some of the important techniques that we have learned,
    such as **optimizer and activation functions**, **adversarial loss**, **auxiliary
    loss**, **normalization**, and **regularization**.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will look at some of the common pitfalls when using generative models
    in real-world settings. After that, we will go over some interesting image/video
    generative models and applications. There is no coding in this chapter, but you
    will find that many of the new models that we introduce in this chapter are built
    using techniques we have learned previously. There are also a few links to resources
    where you can read papers and code to explore the technology.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Putting your skills into practice
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text to image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Video retargeting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural rendering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from PixelCNN, which we covered in [*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017),
    *Getting Started with Image Generation Using TensorFlow*, which is a CNN, all
    the other generative models we have learned about are based on (variational) autoencoders
    or **generative adversarial networks** (**GANs**). Strictly speaking, a GAN is
    not a network but a training method that makes use of two networks – a generator
    and a discriminator. I tried to fit a lot of content into this book; so, the information
    can be overwhelming. We will now go over a summary of the important techniques
    we have learned, by grouping them into the following categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer and activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auxiliary loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizer and activation functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`0` and the second moment is set to `0.999`. The learning rate for the generator
    is set to `0.0001`, while the discriminator uses a learning rate that is two to
    four times larger than that. The discriminator is the key component in a GAN and
    it needs to learn well before the generator does. WGAN trains the discriminator
    more times than the generator in the training step, and an alternative to that
    is to use a higher learning rate for the discriminator.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the de facto activation function for internal layers is leaky
    ReLU with an alpha of `0.01` or `0.02`. The choice of the generator's output activation
    functions depends on the image normalization, that is, sigmoid for a pixel range
    of `[0, 1]` or Tanh for `[-1, 1]`. On the other hand, the discriminator uses a
    linear output activation function for most adversarial losses, apart from sigmoid
    for the earlier non-saturated loss.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have seen that an autoencoder can be used as a generator in a GAN setting.
    GANs are trained using adversarial loss (sometimes called GAN loss). The following
    table lists some of the popular adversarial losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Important adversarial loss; σ refers to the sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_10_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Important adversarial loss; σ refers to the sigmoid function
  prefs: []
  type: TYPE_NORMAL
- en: Non-saturated loss is used in vanilla GANs but it is unstable due to disjoining
    gradients. Wasserstein loss has theory underpinning it that proves it is more
    stable to train with. However, many GAN models choose to use least-square loss,
    which has shown to be stable too. In recent years, hinge loss has become the favorite
    choice of many state-of-the-art models. It is not clear which loss is the best.
    Nevertheless, we have used least-square and hinge loss in many models in this
    book and they seem to train well. So, I would suggest you try them first when
    designing your new GANs.
  prefs: []
  type: TYPE_NORMAL
- en: Auxiliary loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apart from adversarial loss, which acts as the main loss in GAN training, there
    are various auxiliary losses that help generate better images. Some of them are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reconstruction loss** *(*[*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Variational Autoencoder)* to encourage pixel-wise accuracy, this is usually L1
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KL divergence loss** *(*[*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Variational Autoencoder)* for the **variational autoencoder** (**VAE**) to bring
    the latent vector to a standard, multivariate normal distribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cycle consistency loss** *(*[*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation)* for bi-direction image translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Perceptual loss** *(*[*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*,
    Style Transfer)* measures high-level perceptual and semantic differences between
    images. It can be further divided into two losses:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: a) **Feature-matching loss**, which is normally the L2 loss of image features
    extracted from VGG layers. This is also called **perceptual loss**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) **Style loss** features are normally derived from VGG features, such as the
    Gram matrix or activation statistics, and are calculated using L2 loss.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Layer activations are normalized to stabilize network training. Normalization
    takes the following general form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_10_001.jpg)![](img/Formula_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *x* is the activation, *µ* is the mean of activations, *σ* is the standard
    deviation of the activations, and *ε* is the fudge factor for numerical stability.
    *γ* and *β* are learnable parameters; there is one pair for each activation channel.
    Many of the different normalizations differ only in how *µ* and *σ* are obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: In **batch normalization** *(*[*Chapter 3*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)*,
    Generative Adversarial Network)*, the mean and standard deviation are calculated
    across both batch (*N*) and spatial (*H*, *W*) locations, or in other words, (*N*,
    *H*, *W*).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instance normalization** *(*[*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation)* which is the preferred method nowadays, only uses
    the spatial dimension (*H*, *W*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptive instance normalization** (**AdaIN**) *(*[*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*,
    Style Transfer)* serves a different purpose to merge the content and style activation.
    It still uses the equation, except that now the parameters have different meanings.
    *X* is still the activation we consider from the content feature. *γ* and *β*
    are no longer learnable parameters but the mean and standard deviation from the
    style features. Like with instance normalization, the statistics are calculated
    across a spatial dimension of (*H*, *W*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spatially-adaptive normalization (SPADE)** ([*Chapter 6*](B14538_06_Final_JM_ePub.xhtml#_idTextAnchor124)*,
    AI Painter*) has one *γ* and *β* value for each of the features (pixels), in other
    words, they have dimension of (H, W, C). They are produced by running convolutional
    layers across segmentation map to normalize pixels from different semantic objects
    separately.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conditional batch normalization** *(*[*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation)* is just like batch normalization except
    that *γ* and *β* are now multi-dimensional of (LABELS, C), so there is one set
    of them per class label.'
  prefs: []
  type: TYPE_NORMAL
- en: '`1`.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Apart from adversarial loss and normalization, the other important factor in
    stabilizing GAN training is regularization. Regularization aims to constrain the
    growth of the network weights in order to keep the competition between the generator
    and discriminator in check. This is normally done by adding a loss function that
    uses weights. The two common regularizations used in GANs aim to enforce the 1-Lipschitz
    constraint:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gradient penalty** *(*[*Chapter 3*](B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060)*,
    Generative Adversarial Network)* penalizes the growth of gradients, hence the
    weights. However, it is not very commonly used due to the additional backpropagation
    required to calculate the gradients against the input. This slows down the computation
    considerably.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Orthogonal regularization ***(*[*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation)* aims to make the weights to be orthonormal
    matrices, this is because the matrix norm doesn''t change if it multiplies with
    an orthogonal matrix. This can avoid the vanishing or exploding gradient problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spectral normalization** *(*[*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation)* normalizes the layer weights by dividing
    it by its spectral norm. This is different from usual regularizations that use
    loss function to constrain the weights. Spectral normalization is computationally
    efficient, easy to implement, and independent of the training loss. You should
    use it when designing a new GAN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes the summary of GANs techniques. We will now look at new applications
    and models that we have not explored.
  prefs: []
  type: TYPE_NORMAL
- en: Putting your skills into practice
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you can apply the skills you have learned to implement your own image generation
    projects. Before you start, there are some pitfalls you should look out for and
    also some practical advice that you can follow.
  prefs: []
  type: TYPE_NORMAL
- en: Don't trust everything you read
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A new academic paper is published and shows astonishing images generated by
    their model! Take it with a pinch of salt. Usually, these papers handpick the
    best result to showcase and hide the failed examples. Furthermore, the images
    are shrunk down to fit onto the paper, thus the image artifacts may not be visible
    from the paper. Before investing your time in using or re-implementing the information
    in the paper, try to find other resources of the claimed results. This can be
    the author's website or GitHub repository, which may contain the raw, high-definition
    images and videos.
  prefs: []
  type: TYPE_NORMAL
- en: How big is your GPU?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning models, especially GANs, are computationally expensive. Many of
    the state-of-the-art results are produced after training tons of data on multiple
    GPUs for weeks. You will almost certainly need that sort of computing power just
    to attempt to reproduce those results. Therefore, pay attention to the computation
    resources used in the papers to avoid disappointment.
  prefs: []
  type: TYPE_NORMAL
- en: If you don't mind waiting, you can use a single GPU and wait four times longer
    (assuming the original implementation used four GPUs). However, this will usually
    mean the batch size will also have to be reduced by four times, and this can have
    an effect on the results and convergence rate. You may have to reduce the learning
    rate to match the reduced batch size, which further slows down the training time.
  prefs: []
  type: TYPE_NORMAL
- en: Build your model using existing models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The renowned AI scientist Dr. Andrej Karpathy, in one of his talks in 2019,
    said *"don't be a hero."* When you want to create an AI project, do not invent
    your own model; always start from existing models. Researchers have spent huge
    amounts of time and resources on creating models. Along the way, they may have
    thrown in some tricks as well. Therefore, you should start from existing models,
    then tweak or build on top of them to suit your requirements.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen throughout this book, most often, state-of-the-art models do
    not come out of thin air but have been built on top of pre-existing models or
    techniques. There are usually implementations of the model available online, either
    officially by the authors or by re-implementation by enthusiasts in various different
    machine learning frameworks. One useful web resource to find them is the [http://paperswithcode.com/](http://paperswithcode.com/)
    website.
  prefs: []
  type: TYPE_NORMAL
- en: Understand the model's limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of the AI companies I know do not create their own model architecture,
    for the reasons mentioned in the preceding sections. So, what is the point of
    learning to code TensorFlow to create image generation models? The first answer
    to that is that by writing from scratch, you now understand what the layers and
    models are, as well as their limitations. Say someone without knowledge of GANs
    was amazed by what AI could do, so they downloaded pix2pix to train on their own
    dataset to translate images of cats into trees. That did not work, and they had
    no clue why it failed; AI was a black box to them.
  prefs: []
  type: TYPE_NORMAL
- en: As an AI-educated person, we know that pix2pix requires a paired image dataset,
    and we will need to use CycleGAN for unpaired datasets. The knowledge that you
    have learned will help you choose the right model and the right data to use. Furthermore,
    you will now know how to tweak the model architecture for different image sizes,
    different conditioning, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We have now looked at some of the common pitfalls in using generative models.
    Now, we will look at some of the interesting applications and models that you
    could use generative models for.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Out of all the things that image generative models can do, **image processing**
    is probably the one that produces the best results for commercial use. In our
    context, image processing refers to applying some transformation to existing images
    to produce new ones. We will look at the three applications of image processing
    in this section – **image inpainting**, **image compression**, and **image super-resolution**
    (**ISR**).
  prefs: []
  type: TYPE_NORMAL
- en: Image inpainting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image inpainting is the process of filling in missing pixels of an image so
    that the result is visually realistic. It has practical applications in image
    editing, such as restoring a damaged image or removing obstructing objects. In
    the following example, you can see how image inpainting is used to remove people
    in the background. We first fill the people in with white pixels, then we use
    a generative model to fill in the pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Image inpainting using DeepFillv2 to remove people in the background
    (left) original image, (middle) people filled with white masks, (right) restored
    image'
  prefs: []
  type: TYPE_NORMAL
- en: '(source: J. Yu et al., 2018, "Free-Form Image Inpainting with Gated Convolution,"'
  prefs: []
  type: TYPE_NORMAL
- en: https://arxiv.org/abs/1806.03589) ](img/B14538_10_02.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.2 – Image inpainting using DeepFillv2 to remove people in the background
    (left) original image, (middle) people filled with white masks, (right) restored
    image (source: J. Yu et al., 2018, "Free-Form Image Inpainting with Gated Convolution,"
    [https://arxiv.org/abs/1806.03589](https://arxiv.org/abs/1806.03589))'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional image inpainting works by finding a background patch with a similar
    texture and then pasting it into the missing regions. However, this usually only
    works for simple textures in a small area. One of the first GANs designed for
    image inpainting is the **context encoder**. Its architecture is similar to an
    autoencoder but trained with adversarial loss in addition to the usual L2 reconstruction
    loss. The result can appear blurry if there is a large area to be filled.
  prefs: []
  type: TYPE_NORMAL
- en: One approach to tackle this is to use two networks (course and fine) to train
    on different scales. Using this approach, **DeepFill** (J. Yu et al., 2018, *Generative
    Image Inpainting with Contextual Attention*, [https://arxiv.org/abs/1801.07892](https://arxiv.org/abs/1801.07892))
    adds an attention layer to better capture the features from a distant spatial
    location.
  prefs: []
  type: TYPE_NORMAL
- en: 'In earlier GANs, a dataset for image inpainting was created by randomly cutting
    out square masks (holes), but the technique does not translate well to real-world
    applications. Yu et al. propose a partial convolution layer to create irregular
    masks. The layer contains a masked convolution like the one we implemented in
    PixelCNN in [*Chapter 1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017), *Getting
    Started with Image Generation Using TensorFlow*. The following image examples
    show the results of using a partial convolution-based network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Irregular masks and the inpainted results (source: G. Liu et
    al., 2018, "Image Inpainting for Irregular Holes Using Partial Convolutions,"
    https://arxiv.org/abs/1804.07723)](img/B14538_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.3 – Irregular masks and the inpainted results (source: G. Liu et
    al., 2018, "Image Inpainting for Irregular Holes Using Partial Convolutions,"
    [https://arxiv.org/abs/1804.07723](https://arxiv.org/abs/1804.07723))'
  prefs: []
  type: TYPE_NORMAL
- en: '**DeepFillv2** uses gated convolution to improve and generalize masked convolutions.
    DeepFill uses only a standard discriminator that predicts real or fake images.
    However, this does not work well when there can be many holes in free-form inpainting.
    Therefore, it uses **spectral-normalized PatchGAN** (**SN-PatchGAN**) to encourage
    more realistic inpainting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some additional resources on this topic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The TensorFlow v1 source code for DeepFillv1 and v2: [https://github.com/JiahuiYu/generative_inpainting](https://github.com/JiahuiYu/generative_inpainting)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Interactive inpainting demo where you can use your own photo to play with:
    [https://www.nvidia.com/research/inpainting/](https://www.nvidia.com/research/inpainting/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image compression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Image compression is the process of transforming images from raw pixels into
    encoded data that is much smaller in size for storage or communication. For example,
    a JPEG file is a compressed image. When we open a JPEG file, the computer will
    reverse the compression process to restore the image pixels. The simplified image
    compression pipeline is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Segmentation**: Divide the image into small blocks and each of them will
    be processed individually.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transformation**: Transform raw pixels into representations that are more
    compressible. At this stage, a higher compression rate is normally achieved by
    removing high-frequency content that makes the restored image blurrier. For example,
    consider a segment of a grayscale image containing [255, 250, 252, 251, ...] pixel
    values that are nearly white. The differences between them are so small that the
    human eye cannot pick up on them, so we can just transform all the pixels into
    255\. This will make the data easier to compress.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Quantization**: Use a lower bit number to represent the data. An example
    is to convert a grayscale image with 256-pixel values between [0, 255] into two
    values of black and white of [0, 1].'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Symbol encoding** is used to encode data using some efficient coding. One
    of the common ones is known as **run-length coding**. Instead of saving every
    8-bit pixel, we can save just the difference between the pixels. Therefore, instead
    of saving white pixels of [255, 255, 255, …], we can just encode it into something
    such as [255] x 100, which says the white pixel repeats 100 times.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A higher compression rate is achieved by using more extreme quantization or
    removing more frequency contents. As a result, this information is lost (hence,
    this is known as lossy compression). The following diagram shows one such GAN
    for image compression:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Generative compression network. The encoder (E) maps the image
    into latent feature'
  prefs: []
  type: TYPE_NORMAL
- en: w. It is quantized by a finite quantizer, q, to obtain representation ŵ , which
    can be encoded
  prefs: []
  type: TYPE_NORMAL
- en: into a bitstream. The decoder (G) reconstructs the image and D is the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: E. Agustsson et al., 2018, "Generative Adversarial Networks for Extreme
    Learned Image Compression," https://arxiv.org/abs/1804.02958)](img/B14538_10_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.4 – Generative compression network. The encoder (E) maps the image
    into latent feature w. It is quantized by a finite quantizer, q, to obtain representation
    ŵ , which can be encoded into a bitstream. The decoder (G) reconstructs the image
    and D is the discriminator. (Source: E. Agustsson et al., 2018, "Generative Adversarial
    Networks for Extreme Learned Image Compression," [https://arxiv.org/abs/1804.02958](https://arxiv.org/abs/1804.02958))'
  prefs: []
  type: TYPE_NORMAL
- en: In general, generative compression uses autoencoder architecture to compress
    an image into small, latent code and that is restored using the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Image super-resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have used the upsampling layer a lot to increase the spatial resolution of
    the activation in the generator (GAN) or decoder (autoencoder). It works by spacing
    out the pixels and filling in the gaps by interpolation. As a result, the enlarged
    image is usually blurry.
  prefs: []
  type: TYPE_NORMAL
- en: In a lot of image applications, we want to enlarge the image while keeping its
    crispness, and this can be done via **image super resolution** (**ISR**). ISR
    aims to increase the image from **low resolution** (**LR**) to **high resolution**
    (**HR**). **Super-Resolution Generative Adversarial Network** (**SRGAN**) (C.
    Ledig et al., 2016, *Photo-Realistic Single Image Super-Resolution Using a Generative
    Adversarial Network*, [https://arxiv.org/abs/1609.04802](https://arxiv.org/abs/1609.04802))
    was the first to use a GAN to do that.
  prefs: []
  type: TYPE_NORMAL
- en: SRGAN's architecture is similar to that of DCGAN but uses residual blocks instead
    of a plain convolutional layer. It borrows the perception loss from style transfer
    literature, that is, the content loss calculated from VGG features. In retrospect,
    we knew this was a better measure of visual perception quality rather than pixel-wise
    loss. We can now see how versatile the autoencoder is for various image processing
    tasks. Similar autoencoder architecture can be repurposed for other image processing
    tasks, such as image denoising or deblurring. Next, we will look at an application
    where the input to the model is not images but words.
  prefs: []
  type: TYPE_NORMAL
- en: Text to image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Text-to-image GANs are conditional GANs. However, instead of using class labels
    as conditions, they use words as the condition to generate images. In earlier
    practice, GANs used word embeddings as the conditions into the generator and discriminator.
    Their architectures are similar to conditional GANs, which we learned about in
    [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084), *Image-to-Image
    Translation*. The difference is merely that the embedding of text is generated
    using a **natural language processing** (**NLP**) preprocessing pipeline. The
    following diagram shows the architecture of a text-conditional GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Text-conditional convolutional GAN architecture where text
    encoding'
  prefs: []
  type: TYPE_NORMAL
- en: is used by both the generator and discriminator
  prefs: []
  type: TYPE_NORMAL
- en: '(Redrawn from: S. Reed et al., 2016, "Generative Adversarial Text to Image
    Synthesis,"'
  prefs: []
  type: TYPE_NORMAL
- en: https://arxiv.org/abs/1605.05396)](img/B14538_10_05.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.5 – Text-conditional convolutional GAN architecture where text encoding
    is used by both the generator and discriminator (Redrawn from: S. Reed et al.,
    2016, "Generative Adversarial Text to Image Synthesis," [https://arxiv.org/abs/1605.05396](https://arxiv.org/abs/1605.05396))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Like normal GANs, generated high-resolution images tend to be blurry. **StackGAN**
    resolves this by stacking two networks together. The following diagram shows the
    text and the generated images at different stages of StackGAN and a vanilla GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Images generated by StackGAN at different generator stages'
  prefs: []
  type: TYPE_NORMAL
- en: '(source: H. Zhang et al., 2017, "StackGAN: Text to Photo-realistic Image Synthesis
    with Stacked Generative Adversarial Networks," https://arxiv.org/abs/1612.03242)](img/B14538_10_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.6 – Images generated by StackGAN at different generator stages (source:
    H. Zhang et al., 2017, "StackGAN: Text to Photo-realistic Image Synthesis with
    Stacked Generative Adversarial Networks," [https://arxiv.org/abs/1612.03242](https://arxiv.org/abs/1612.03242))'
  prefs: []
  type: TYPE_NORMAL
- en: The first generator produces a low-resolution image from the word embedding.
    The second generator then takes the generated image and word embedding as input
    conditions to the second generator to produce refined images. The coarse-to-fine
    architecture has appeared in different forms in many high-resolution GANs, as
    we have learned in this book.
  prefs: []
  type: TYPE_NORMAL
- en: '**AttnGAN** (T. Xu et al., 2017, *AttnGAN: Fine-Grained Text to Image Generation
    with Attentional Generative Adversarial Networks, at* [https://arxiv.org/abs/1711.10485](https://arxiv.org/abs/1711.10485))
    further improves text-to-image synthesis by using an attention module. The attention
    module is different from the one used in SAGAN ([*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*,
    Self-Attention for Image Generation*) but the principle is the same. There are
    two inputs into the attention module at the start of every stage of the generator
    – word features and image features. It learns to pay attention to different words
    and image regions when moving from coarse to fine generators. Most text-to-image
    models after that have some form of attention mechanism.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Text to image is still an unsolved problem; it still struggles to generate
    complex real-world images from text. As we can see in the following figure, the
    generated images are still far from perfect. Researchers are beginning to bring
    in recent advancement from NLP to improve text-to-image performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Examples of images generated from the given caption from the
    MS-COCO dataset (A) original images and their image caption in the dataset (B)
    images generated by StackGAN + object pathway (C) images generated by StackGAN
    (source: T. Hinz et al., 2019, "Generating Multiple Objects at Spatially Distinct
    Locations," https://arxiv.org/abs/1901.00686)](img/B14538_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7 – Examples of images generated from the given caption from the
    MS-COCO dataset (A) original images and their image caption in the dataset (B)
    images generated by StackGAN + object pathway (C) images generated by StackGAN
    (source: T. Hinz et al., 2019, "Generating Multiple Objects at Spatially Distinct
    Locations," [https://arxiv.org/abs/1901.00686](https://arxiv.org/abs/1901.00686))'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at the exciting application of video retargeting.
  prefs: []
  type: TYPE_NORMAL
- en: Video retargeting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Video synthesis is a broad term used for describing all forms of video generation.
    This can include generating video from random noise or words, to colorize black-and-white
    video, and so on, much like image generation.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at a subgroup of video synthesis known as **video
    retargeting**. We will first look at two applications – face reenactment and pose
    transfer – and then introduce a powerful model that uses motion to generalize
    video targeting.
  prefs: []
  type: TYPE_NORMAL
- en: Face reenactment
  prefs: []
  type: TYPE_NORMAL
- en: '**Face reenactment** was introduced along with face swapping in [*Chapter 9*](B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175),
    *Video Synthesis*. Face reenactment in video synthesis involves transferring the
    facial expression of the driving video to the face in the target video. This is
    useful in animation and movie making. Recently, Zakharov et al. proposed a generative
    model that requires only a few target 2D images. This is done by using facial
    landmarks as intermediate features, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Transferring the facial expression from the target image to
    the source image'
  prefs: []
  type: TYPE_NORMAL
- en: '(source: E. Zakharov et al., 2019, "Few-Shot Adversarial Learning of Realistic
    Neural Talking Head Models," https://arxiv.org/abs/1905.08233)](img/B14538_10_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.8 – Transferring the facial expression from the target image to the
    source image (source: E. Zakharov et al., 2019, "Few-Shot Adversarial Learning
    of Realistic Neural Talking Head Models," [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s briefly look into the model architecture, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Architecture of few-shot adversarial learning'
  prefs: []
  type: TYPE_NORMAL
- en: '(source: E. Zakharov et al., 2019, "Few-Shot Adversarial Learning of Realistic
    Neural Talking Head Models," https://arxiv.org/abs/1905.08233)](img/B14538_10_09.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.9 – Architecture of few-shot adversarial learning (source: E. Zakharov
    et al., 2019, "Few-Shot Adversarial Learning of Realistic Neural Talking Head
    Models," [https://arxiv.org/abs/1905.08233](https://arxiv.org/abs/1905.08233))'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing that you should notice in the preceding diagram is **AdaIN**,
    which we immediately know is a style-based model. Therefore, we can see that the
    landmarks at the top are the content (the target's face shape and pose), while
    the style (the source's face attributes and expression) is extracted from the
    **embedder**. The generator then uses AdaIN to fuse the content and style to reconstruct
    the face.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, a similar model has been deployed by NVIDIA to slash the bit rate
    of teleconferencing video transmission. You can view their blog at [https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/](https://blogs.nvidia.com/blog/2020/10/05/gan-video-conferencing-maxine/)
    to learn how they use many of the AI techniques, such as ISR, face alignment,
    and face reenactment, in real-world deployment. Next, we will look at how to use
    AI to transfer the pose of a person.
  prefs: []
  type: TYPE_NORMAL
- en: Pose transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pose transfer** is similar to face reenactment except that now it will transfer
    the body (and head) pose. There are many ways to perform pose transfer but all
    of them involve the use of **body joints** (also known as **keypoints**) as features.
    The following diagram shows one example of images generated from a condition image
    and target pose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Transferring target poses onto the condition image'
  prefs: []
  type: TYPE_NORMAL
- en: '(source: Z. Zhu et al., 2019, "Progressive Pose Attention Transfer for Person
    Image'
  prefs: []
  type: TYPE_NORMAL
- en: Generation," https://arxiv.org/abs/1904.03349 )](img/B14538_10_10.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.10 – Transferring target poses onto the condition image (source:
    Z. Zhu et al., 2019, "Progressive Pose Attention Transfer for Person Image Generation,"
    [https://arxiv.org/abs/1904.03349](https://arxiv.org/abs/1904.03349) )'
  prefs: []
  type: TYPE_NORMAL
- en: Pose transfer has many potential applications, including generating a fashion
    modeling video from single 2D images. This task is more challenging than face
    reenactment due to the huge variety of human poses. Next, we will look at a motion
    model that could generalize both face reenactment and pose transfer.
  prefs: []
  type: TYPE_NORMAL
- en: Motion transfer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The face reenactment and pose transfer models introduced in the preceding section
    require object-specific priors, in other words, the facial landmark and human
    pose keypoints. Those features are normally extracted by separate models trained
    using a lot of data, which can be expensive to acquire and annotate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Recently, an object-agnostic model known as a **first-order motion model**
    (A. Siarohin et al., 2019, *First Order Motion Model for Image Animation*, [https://arxiv.org/abs/2003.00196](https://arxiv.org/abs/2003.00196))
    was introduced. It has rapidly gained popularity for its ease of use as it doesn''t
    need a lot of annotated training data. The following screenshot shows the overall
    architecture of the model, which exploits the motion in video frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – First-order motion model that disentangles appearance and
    motion (source:'
  prefs: []
  type: TYPE_NORMAL
- en: https://aliaksandrsiarohin.github.io/first-order-model-website/)](img/B14538_10_11.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.11 – First-order motion model that disentangles appearance and motion
    (source: [https://aliaksandrsiarohin.github.io/first-order-model-website/](https://aliaksandrsiarohin.github.io/first-order-model-website/))'
  prefs: []
  type: TYPE_NORMAL
- en: In style transfer, an image is disentangled into content and style. Using the
    same terminology, **motion transfer** disentangles a video into appearance and
    motion. The motion module captures the motion of the object in the driving video.
    The generator network uses the appearance from the source image (similar to VGG
    content features) and motion information to create a new target video.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, this model requires only a single source image and a driving video.
    It could do many of the video tasks we discussed, including face reenactment,
    pose transfer, and face swapping. You should definitely check out the website
    to see the demo video.
  prefs: []
  type: TYPE_NORMAL
- en: Although video retargeting GANs have improved dramatically in recent years,
    they are still not quite there yet to generate high-resolution images that are
    perfect for video production. An alternative is to combine 3D modeling with 2D
    GANs, which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Neural rendering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rendering** is the process of generating photo-realistic images from 2D or
    3D computer models. The term **neural rendering** has recently emerged to describe
    rendering using a neural network. In traditional 3D rendering, we will need to
    first create a 3D model with a polygon mesh that describes the object''s shape,
    color, and texture. Then, the lighting and camera position will be set and render
    the view into a 2D image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There has been an ongoing research on 3D object generation, but it is still
    not able to generate satisfying results. We can take advantage of the advancement
    of GANs by projecting part of the 3D objects into 2D space. We then use GANs to
    enhance the image in 2D space, for example, to generate a realistic texture using
    style transfer before projecting that back into the 3D model. The top diagram
    in the following figure shows the general pipeline of this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Two common frameworks for neural rendering'
  prefs: []
  type: TYPE_NORMAL
- en: '(Redrawn from: M-Y. Liu et al., 2020, "Generative Adversarial Networks for
    Image and Video Synthesis: Algorithms and Applications," https://arxiv.org/abs/2008.02793)
    ](img/B14538_10_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.12 – Two common frameworks for neural rendering (Redrawn from: M-Y.
    Liu et al., 2020, "Generative Adversarial Networks for Image and Video Synthesis:
    Algorithms and Applications," [https://arxiv.org/abs/2008.02793](https://arxiv.org/abs/2008.02793))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagram *(b)* shows the framework that uses 3D data as input and 3D differentiable
    operations, such as 3D convolution. Apart from 3D polygons, 3D data can also exist
    in the form of a point cloud that can be obtained from lidar/radar or computer
    vision techniques such as structure from motion. A point cloud is made up of points
    in 3D space that depict the object''s surface. One application of a 3D to 2D deep
    network framework is to render the point cloud into a 2D image, as shown in the
    following figure, where the input is the cloud points obtained from a room:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – (left) 3D point cloud to 2D rendering, (middle) point cloud
    synthesis'
  prefs: []
  type: TYPE_NORMAL
- en: image, (right) ground truth
  prefs: []
  type: TYPE_NORMAL
- en: '(source: F. Pittaluga et al., 2019, "Revealing Scenes by Inverting Structure
    from Motion Reconstructions," https://arxiv.org/abs/1904.03303)](img/B14538_10_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.13 – (left) 3D point cloud to 2D rendering, (middle) point cloud
    synthesis image, (right) ground truth (source: F. Pittaluga et al., 2019, "Revealing
    Scenes by Inverting Structure from Motion Reconstructions," [https://arxiv.org/abs/1904.03303](https://arxiv.org/abs/1904.03303))'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also perform rendering in the reverse direction, that is, from a 2D
    image to a 3D object. This is often known as **inverse rendering**. The following
    figure shows examples of 2D to 3D inverse rendering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Given an input of 2D images (first column), the model predicts
    the 3D shape'
  prefs: []
  type: TYPE_NORMAL
- en: and texture and renders them into the same viewpoint (second column). Images
    on
  prefs: []
  type: TYPE_NORMAL
- en: the right show the rendering in three different viewpoints.
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: Y. Zhang et al., 2020, "Image GANs Meet Differentiable Rendering for
    Inverse Graphics and Interpretable 3D Neural Rendering," https://arxiv.org/abs/2010.09125)](img/B14538_10_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10.14 – Given an input of 2D images (first column), the model predicts
    the 3D shape and texture and renders them into the same viewpoint (second column).
    Images on the right show the rendering in three different viewpoints. (Source:
    Y. Zhang et al., 2020, "Image GANs Meet Differentiable Rendering for Inverse Graphics
    and Interpretable 3D Neural Rendering," [https://arxiv.org/abs/2010.09125](https://arxiv.org/abs/2010.09125))'
  prefs: []
  type: TYPE_NORMAL
- en: The model by Y. Zhang et al., 2020, uses two *renderers*. One is a differentiable
    graphics renderer to render 2D into 3D, which is outside the scope of this book.
    The other one is a GAN to generate multi-view image data, or more specifically,
    StyleGAN. It is interesting to know why they chose to use StyleGAN. The authors
    learned that StyleGAN could generate faces of slightly different viewing angles
    by changing the latent code. Then, they did an extensive study to find that styles
    in early layers control the camera viewpoint, making it ideal for this task. This
    is also a good example that shows how we could leverage 2D generative models into
    the 3D world.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our introduction to neural rendering. It is an active area and
    there are many more use cases that are yet to be explored.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the inception of GANs and VAEs in 2014, significant advancement has been
    made in 2D image generation. Generating high-fidelity images is still challenging
    in practice as it requires huge amounts of data, computing power, and hyperparameter
    tuning. However, as demonstrated by StyleGAN, it seems that we now have the technology
    to do this, especially in face generation.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, at the time of writing this book, there haven't really been any major
    breakthroughs in this area since 2018\. With this book, we have included all the
    important techniques leading to BigGAN. These techniques include the use of AdaIN
    and self-attention modules, which are now commonplace even in adjacent fields
    such as video synthesis. This gives us a solid foundation to explore other emerging
    generative technologies.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we looked back at the things we have learned and summarized
    them in different groups, such as losses and normalization techniques. We then
    looked at some practical advice in training generative models. Finally, we touched
    upon some of the upcoming technologies, especially in the area of video retargeting.
    I believe you now have the knowledge, skills, and confidence to explore the new
    and exciting AI world and I wish you all the best in your new adventure. I hope
    you have enjoyed reading this book. I welcome your feedback, which will help me
    improve my writing skills for my next book. Thanks!
  prefs: []
  type: TYPE_NORMAL
