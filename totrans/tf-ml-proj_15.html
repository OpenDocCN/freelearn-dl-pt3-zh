<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">What is Next?</h1>
                </header>
            
            <article>
                
<p>Congratulations on making it this far. So far, have learned to implement a variety of cutting-edge AI algorithms in TensorFlow and built cool projects on the side. Specifically, we have built projects on reinforcement learning, Bayesian neural networks, capsule networks, and <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>), among others. We have also learned about several modules of TensorFlow, including TensorFlow.js, TensorFlow Lite, and TensorFlow Probability, among others. This surely deserves a pat on the back and a well-earned rest.</p>
<p>Before we go out to play, there are a few more things that we should consider reading about before we are prime-time ready to deploy these cutting edge techniques in production. As we will realize in this chapter, there is more to deploying a machine learning model in production than just implementing the latest research paper in AI. To understand what I mean by this, let's read through the following topics:</p>
<ul>
<li>TensorFlow utilities to deploy models in production</li>
<li>General rules for building AI applications</li>
<li>Limitations of deep learning</li>
<li>Applications of AI across different industries</li>
<li>Ethical Considerations in AI</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing TensorFlow in production</h1>
                </header>
            
            <article>
                
<p>When it comes to software engineering, we see several best practices, like version control through GitHub, reusable libraries, continuous integration, and others, which have made developers more productive. Machine learning is a new field where there is a definite need for some tooling to make model deployment simple and improve a data scientist's productivity. In that respect, TensorFlow has released a host of tools recently.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding TensorFlow Hub</h1>
                </header>
            
            <article>
                
<p>Software repositories have a real benefit in the field of software engineering as they enhance the reusability of code. This not only helps to improve developer productivity, but also helps in sharing expertise among different developers. Also, because developers now want to share their code, they develop their code in a manner that is more clean and modular so that it can benefit the entire community.</p>
<p>Google introduced TensorFlow Hub to achieve the similar purpose of reusability in machine learning. It is designed so that you can create, share, and reuse the components of machine learning models. Reusability in machine learning is even more important than software engineering because we are not only using the algorithm/architecture and the expertise—we are also using an enormous amount of compute power that went into training the model and all of the data as well.</p>
<p> TF hub comprises of several machine learning models which are trained using state-of-the-art algorithms and huge amounts of data by experts at Google. Each of this trained model is termed as <strong>Module </strong>in TF hub. A module and can be shared on the <strong><span>TensorFlow </span>Hub</strong>, where they can then be imported by anyone into their code. The following diagram depicts the flow of what how a trained TensorFlow model can be used by other applications/models:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-684 image-border" src="assets/c851d7e2-fe42-404d-aae8-2a87f7f0424c.png" style="width:43.42em;height:9.50em;"/></p>
<p><strong>Modules</strong> in <strong><span>TensorFlow </span>Hub</strong> contain both the model's architecture or the <span>TensorFlow </span>graph and the weights of the trained <strong>Model</strong>. <strong>Modules</strong> have the following properties:</p>
<ul>
<li><strong>Composable:</strong> Composable means that we can use modules as building blocks and add stuff on top of them.</li>
<li><strong>Reusable:</strong> Reusable means that modules have a common signature so that we can swap one with another. This is mainly useful when we are iterating over models to get the best accuracy on our dataset.</li>
<li><strong>Retrainable:</strong> Modules come with pre-trained weights, but they are flexible enough to be retrained on the new dataset. This means that we can back propagate through the model to generate new set of weights.</li>
</ul>
<p>Let's understand this with the help of an example. Say we have a dataset of different Toyota cars such as the Camry, the Corolla, and other models. If we don't have a lot of images of each category, it won't be prudent to train the entire model from scratch.</p>
<p>Instead, what we can do is take a general purpose model that has been trained on a giant set of images from TensorFlow Hub and take the reusable part of the model, such as its architecture and pre-trained weights. On top of the pre-trained model, we can add a classifier that classifies the images that are present in our dataset appropriately. This procedure is sometimes also referred to as transfer learning. This is illustrated in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-685 image-border" src="assets/cbb61268-5dd9-446e-8095-20c8abb8be51.png" style="width:41.83em;height:9.75em;"/></p>
<div class="packt_tip">If you want to learn more about Transfer Learning, please refer to the Stanford's course notes <a href="http://cs231n.github.io/transfer-learning/">(</a><a href="http://cs231n.github.io/transfer-learning/">http://cs231n.github.io/transfer-learning/</a><a href="http://cs231n.github.io/transfer-learning/">)</a></div>
<p>You can visit TensorFlow Hub (<a href="https://www.tensorflow.org/hub/">https://www.tensorflow.org/hub/</a>) to get state-of-the-art, research-oriented image models that you can directly import into your custom models. Let's say we are using NasNet (<a href="https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/1">https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/1</a>), which is an image module that's trained through architecture search. Here we are going to use the URL for the <span><span>NasNet module </span></span>in our code to import the module, as follows:</p>
<pre>```<br/>module = hub.Module(“https://tfhub.dev/google/imagenet/nasnet_large/feature_vect<br/>            or/1”)<br/>features = module(toyota_images)<br/>logits = tf.layers.dense(features, NUM_CLASSES)<br/>probabilities = tf.nn.softmax(logits)<br/>```</pre>
<p><span>We added a dense layer with softmax non-linearity on top of the module. We train the weights of the dense layer through</span> backpropagation<span> to classify the Toyota car images.</span></p>
<div class="packt_infobox"><span>Note that we don't need to download the module, nor do we need to instantiate it.</span></div>
<p><span>TensorFlow takes care of all of those low-level details, which makes the module reusable in a true sense. Another great thing about using this module is that you get thousands of hours of compute required to train NasNet for free.</span></p>
<p>Let's say we do have a large dataset. In that case, we can train the reusable part of the module as follows:</p>
<pre>```<br/>module = hub.Module(“https://tfhub.dev/google/imagenet/nasnet_large/feature_vector/1”,trainable = True, tags {“train”})features = module(toyota_images)<br/>logits = tf.layers.dense(features, NUM_CLASSES)<br/>probabilities = tf.nn.softmax(logits)<br/>```</pre>
<p><span>TensorFlow Hub </span>has pre-trained models for image classification, word embeddings, sentence embeddings, and other applications. Let's consider our movie sentiment detection project from <a href="60549866-497e-4dfa-890c-6651f34cf8e4.xhtml" target="_blank">Chapter 3</a>, <em>Sentiment Analysis in your browser using Tensorflow.js</em> in this book. We could have used a pre-trained embedding for each piece of work in the dataset from <span>TensorFlow </span><span>Hub</span>. This availability of pre-trained modules across domains will potentially help many developers build new applications without having to worry about the math behind the models.</p>
<p>You can find more details about this on the official web page of TensorFlow Hub (<a href="https://www.tensorflow.org/hub/">https://www.tensorflow.org/hub/</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Serving</h1>
                </header>
            
            <article>
                
<p><strong><span>TensorFlow </span>Serving</strong> is a highly flexible serving system for deploying machine learning models in production. Before we go into the details, first let's try to understand what serving is by taking a look at its architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1045 image-border" src="assets/d0b5b79d-79ad-4299-b98e-3a3f6f66e7b9.png" style="width:74.33em;height:20.08em;"/></p>
<p>We have some <strong>Data</strong> and we use that to train a machine learning <strong>Model</strong>. Once the <strong>Model</strong> is trained, it needs to be deployed onto a web or mobile <strong>App</strong> to serve the end users. One way to do that is through a <strong>remote procedure call</strong> (<strong>RPC</strong>) server (<a href="https://www.ibm.com/support/knowledgecenter/en/ssw_aix_72/com.ibm.aix.progcomc/ch8_rpc.htm">https://www.ibm.com/support/knowledgecenter/en/ssw_aix_72/com.ibm.aix.progcomc/ch8_rpc.htm</a>). <span>TensorFlow </span>Serving can be used both as an <strong>RPC Server</strong> and as a set of libraries, both inside an app or embedded device.</p>
<p><span>TensorFlow </span>Serving has three pillars:</p>
<ul>
<li><strong>C++ libraries:</strong> Low-level C++ libraries primarily contain the functions and methods required for <span>TensorFlow </span>serving. These are the libraries that Google uses to generate the binaries used by applications. They are also open sourced.</li>
<li><strong>Binaries:</strong> If we want standard settings for our serving architecture, we can use pre-defined binaries, that incorporate all the best practices from Google. Google also provides Docker containers (<a href="https://www.docker.com/">https://www.docker.com/</a>) to scale the binaries on Kubernetes (<a href="https://kubernetes.io/">https://kubernetes.io/</a>).</li>
<li><strong>Hosted services:</strong> <span>TensorFlow </span>Serving also has hosted services across Google Cloud ML, which makes it pretty easy to use and deploy.</li>
</ul>
<p>Here are some of the advantages of <span>TensorFlow </span>Serving:</p>
<ul>
<li><strong>Online and low latency:</strong> Users don't want to wait to see predictions on their application. With TF serving, predictions are not only fast—they are consistently fast.</li>
<li><strong>Multiple models in a single process:</strong> TF serving lets you load multiple models in the same process. Let's say we have a model that is serving great predictions to the customers. However, if we want to run an experiment, then we might want to load another model, along with the production model.</li>
<li><strong>Auto-loading and training of versions of the same model:</strong> TF serving has support for auto-loading a newly trained model without downtime and switching from an old version of the same model in production.</li>
<li><strong>Scalable:</strong> TF Serving is auto-scalable with Cloud ML, Docker, and Kubernetes.</li>
</ul>
<p>For more details on how to deploy your models using TF Serving, refer to the official documentation <a href="https://www.tensorflow.org/serving/">here</a> (<a href="https://www.tensorflow.org/serving/">https://www.tensorflow.org/serving/</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Extended</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow Extended</strong> (<strong>TFX</strong>) is a general-purpose machine learning platform that was built at Google.<span> Some components of it are open sourced, and there was a recent paper (</span><a href="https://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform">https://www.kdd.org/kdd2017/papers/view/tfx-a-tensorflow-based-production-scale-machine-learning-platform</a><span>) in a KDD conference illustrating the capabilities and vision of TFX.</span></p>
<p>In this book, we primarily understood the semantics of building a <span>TensorFlow</span> model. However, when we look at the actual machine learning applications in production, there are many more components. The following diagram illustrates these components:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1044 image-border" src="assets/4e01138c-711e-49eb-8e39-c41fb9d698c8.png" style="width:43.08em;height:17.00em;"/></p>
<p>As we can see, ML code is a very small component of the overall system. Other blocks take the maximum amount of time to build and occupy the maximum lines of code. TFX provides the libraries and tools to construct the other components of machine learning pipeline.</p>
<p>Let's look at an example of the machine learning process to understand the different open source components of <span>TensorFlow </span>Extended:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-686 image-border" src="assets/949f43e6-ceb7-495a-9177-1fac8f01f203.png" style="width:42.25em;height:6.08em;"/></p>
<ul>
<li><strong>Analyze Data</strong>: Exploratory data analysis is a requirement for building any machine learning model. TFX has a tool named Facets (<a href="https://github.com/PAIR-code/facets">https://github.com/PAIR-code/facets</a>), which lets us visualize the distribution of each variable and identify missing data or outliers, or inform others about what data transformations might be required on the data.</li>
<li><strong>Transform</strong>: TensorFlow transforms (<a href="https://www.tensorflow.org/tfx/transform/get_started">https://www.tensorflow.org/tfx/transform/get_started</a>) provide out-of-the-box functions to perform a full transform on the base data to make it suitable for training a model. It is also very much attached to the TF graph itself, which ensures that you are applying the same transforms in training, as well as serving.</li>
<li><strong>Train TF estimator</strong>: After transforming the data, we can use the TF Estimator (<a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator</a>), which provides a high-level API to quickly define, train, and export a model. TF Estimators also let you export models in different formats for inference and serving.</li>
<li><strong>Analyze Model</strong>: Once the model is built, we can directly push it to production, but that would be a very bad idea. Instead, we should analyze the model predictions and make sure that the model is predicting things that we want it to predict. TF Model Analysis (<a href="https://www.tensorflow.org/tfx/model_analysis/get_started">https://www.tensorflow.org/tfx/model_analysis/get_started</a>) lets us evaluate the model over a large dataset and provides a UI to slice the predictions by different values of attributes.</li>
<li><strong>Serve Model</strong>: After analyzing our model and getting comfortable with its model predictions, we want to serve the model to production. One way this can be achieved is by using <span>TensorFlow </span>Serving, which was described in the previous section.</li>
</ul>
<p><span>TensorFlow </span>Extended is heavily used inside Google for building products. It definitely has more features than the ones that are open sourced. For people working at startups or companies that don't have their own internal ML platform, TFX is highly recommended for building end-to-end ML applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recommendations for building AI applications</h1>
                </header>
            
            <article>
                
<p>Now that we understand some of the tools from <span>TensorFlow </span>that can help us in developing and deploying models at scale, let's try to understand the general rules of thumb when building AI applications.</p>
<ul>
<li><strong>Engineering <span>over</span> machine learning</strong>: Almost all the solutions to problems start with engineering. It is very important to get the data pipeline right before building any machine learning model.</li>
<li><strong>Keep it simple</strong>: Generally, data scientists have a natural tendency to build the most complex model for the problem. However, it is great to start with a simple, interpretable model—say, a logistic regression model for classification. It helps in discovering and debugging data or engineering pipeline issues better. Only when you are not satisfied with the results of the basic model should you use advanced techniques like deep learning.</li>
<li><strong>Distributed processing</strong>: In the era of big data, you will almost always run into issues where you can't fit the data into RAM. Learning about distributed frameworks like Spark can help a lot in processing and building scalable machine learning applications.</li>
<li><strong>Automated model retraining</strong>: Once the model is deployed, its performance can degrade over time. It is important to keep checking the model's accuracy so that automated training of the model can be kicked off with new data. This will help in maintaining prediction accuracy for the product.</li>
<li><strong>Training and testing pipelines</strong>: With separate pipelines for training and testing, there is always a possibility of divergence between training and testing features. Try to have as much overlap as possible between training and testing pipelines. This can help make debugging model predictions easier.</li>
<li><strong>Launch new models with A/B testing</strong>: A/B testing is a method of comparing two versions of model/webpage and others. It is a statistical experiment conducted where two different versions are shown to the users at random. You can read more about them in lecture notes from Purdue (<a href="https://www.cs.purdue.edu/homes/ribeirob/courses/Fall2016/lectures/hyp_tests.pdf">https://www.cs.purdue.edu/homes/ribeirob/courses/Fall2016/lectures/hyp_tests.pdf</a>)</li>
</ul>
<p>If you build a better model than the one already existing in production, you will see some uplift in accuracy for testing datasets. However, there is a real chance that you might not see the same uplift in production compared to the existing model because of a variety of issues like correlations versus causation, change in user behavior, and so on. It is very important to perform A/B tests with a new model in production before rolling them out to all users.</p>
<ul>
<li><strong>One model <span>over</span> ensemble</strong>: Ensemble models (a combination of many single models) might give better accuracy over a single model. However, if the gain is not significant, always prefer using a single model. This is because ensemble models are difficult to maintain, debug, and scale in production systems.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Limitations of deep learning</h1>
                </header>
            
            <article>
                
<p>In this project, almost all of the projects involved some sort of deep learning. Deep learning has been pivotal in powering most of the advances in the last few years. However, there are obvious limitations to deep learning that we should understand before applying them to real-world situations. Here are some of them:</p>
<ul>
<li><strong>Data-hungry</strong>: Usually, we don't have big datasets for every problem we want to solve using machine learning. On the contrary, deep learning algorithms only work when we have huge datasets for the problem.</li>
<li><strong>Compute intensive</strong>: Deep learning training usually requires GPU support and a huge amount of RAM. However, this makes it impossible to train deep neural networks on edge devices like mobiles and tablets.</li>
<li><strong>No prediction uncertainty</strong>: Deep learning algorithms are, by default, poor at representing uncertainty. Deep neural networks can confidently misclassify a cat image as that of a dog.</li>
</ul>
<p style="padding-left: 60px"><span>There is no notion of confidence intervals or uncertainty in predictions. For applications like self-driving cars, it is very important to take uncertainty into account before making any decision. In this book, we touched on concepts like Bayesian neural networks, which are an attempt to incorporate uncertainty in deep neural networks.</span></p>
<ul>
<li><strong>Uninterpretable black boxes</strong>: Deep learning models are hard to interpret and trust. For example, the loan department at a bank decides whether to give a loan to an individual based on their past purchases or credit history using a deep neural network.</li>
</ul>
<p style="padding-left: 60px"><span>If the model denies the loan, the bank has to give an explanation to the individual regarding why their loan was denied. However, with deep neural networks, it is almost impossible to provide an explicit reason about why the loan was denied. Uninterpretability is a major reason why these models are not ubiquitous across different industries.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AI applications in industries</h1>
                </header>
            
            <article>
                
<p>AI is the new paradigm that every company is trying to move to. As per the Mckinsey report (<a href="https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-modeling-the-impact-of-ai-on-the-world-economy">https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-frontier-modeling-the-impact-of-ai-on-the-world-economy</a>), by 2030, 70% of companies are expected to adopt at least one AI technology. Let's look at different applications of AI by industries:</p>
<ul>
<li><strong>Retail</strong>:
<ul>
<li>Supply chain optimization</li>
<li>Customization of shopping experiences by micro targeting</li>
<li>Pricing of products and holiday discount calculations</li>
<li>Custom product placement in retail stores to increase sales</li>
</ul>
</li>
<li><strong>Social Networks (Facebook, LinkedIn, Twitter)</strong>:
<ul>
<li>Friend/follower recommendations</li>
<li>Home feed customization to increase engagement based on past history</li>
<li>Fake news/fraud detection</li>
</ul>
</li>
<li><strong>Healthcare</strong>:
<ul>
<li>New drug discovery</li>
<li>Automated medical imaging</li>
<li>Recommending workouts/food through data stored in Apple Watch or other devices</li>
</ul>
</li>
<li><strong>Finance</strong>:
<ul>
<li>Stock market prediction</li>
<li>Credit card fraud detection</li>
<li>Loan qualification</li>
<li>Chatbots for customer support</li>
</ul>
</li>
</ul>
<ul>
<li><strong>Manufacturing</strong>:
<ul>
<li>Predictive maintenance</li>
<li>Demand forecasting</li>
<li>Inventory management</li>
</ul>
</li>
<li><strong>Logistics</strong>:
<ul>
<li>ETA optimization</li>
<li>Surge pricing</li>
<li>Shared rides/pool</li>
<li>Pricing</li>
<li>Self-driving cars</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Ethical considerations in AI</h1>
                </header>
            
            <article>
                
<p>We are seeing an extraordinary rise in Artificial Intelligence and its applications. However, the growing sophistication of AI applications has raised a number of concerns around bias, fairness, safety, transparency, and accountability. This is mainly because AI models don't have a conscience and can't distinguish good from bad all by themselves. They are as good as the data they are trained on. So, if the data is biased in some sense, so will the predictions be. There are other concerns around rising unemployment due to automation, the use of AI for terrorism, and racist predictions from AI models, among others.</p>
<p>The good news is that many universities are spending time and resources to come up with solutions on how to make AI more fair and free from bias. At the same time, regulators are trying to frame new rules so that AI applications are safe and secure for humans.</p>
<p>As an AI practitioner, it is imperative that we understand these issues before using AI in our own products. I urge you to make yourself aware of the ethical issues in your products and correct them accordingly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at various extensions of <span>TensorFlow </span>for improving the productivity of data scientists and enabling the easier deployment of cutting-edge models in production at a large scale.</p>
<p>We looked at <span>TensorFlow </span>Hub, which is similar to the GitHub repository of trained deep learning models from various areas like Computer Vision, Natural Language Processing, and so on. Thereafter, we understood how <span>TensorFlow </span>Serving provide tools and libraries to deploy deep learning models at scale. Lastly, we learned about the open source components of <strong><span>TensorFlow </span>Extended</strong> (<strong>TFX</strong>), which is a machine learning platform from Google. TFX helps in the entire model building pipeline, from data analysis to model deployment.</p>
<p>Next, we learned about several best practices when building scalable AI products. Building a robust engineering pipeline and trying out simple models before deep learning and always launching new models with A/B tests are few of them.</p>
<p>Thereafter, we dispensed the hype around deep learning by understanding the limitations of deep neural networks. Specifically, we learned that they require huge amounts of data and compute power for building good, accurate models. Also, the fact that they are not interpretable makes them unusable in many AI applications. We also looked at various applications of AI across different industries and learned about the importance of ethics in AI.</p>
<p>Lastly, if you have made it this far and completed the projects, I thank you and congratulate you for your awesome achievement. You have acquired the necessary skills to build practical AI applications using state-of-the-art techniques in reinforcement learning, computer vision, and natural language processing, among others. I would request that you now use your knowledge for good and make this world a better place.</p>
<p>I would like to end this book with my favorite quote:</p>
<div class="packt_quote">The future is not some place we are going, but one we are creating.</div>
<p class="packt_quote CDPAlignRight CDPAlign">- John H. Schaar</p>


            </article>

            
        </section>
    </body></html>