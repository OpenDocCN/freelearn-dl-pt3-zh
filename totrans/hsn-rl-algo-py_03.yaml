- en: Implementing RL Cycle and OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In every machine learning project, an algorithm learns rules and instructions
    from a training dataset, with a view to performing a task better. In **reinforcement
    learning** (**RL**), the algorithm is called the agent, and it learns from the
    data provided by an environment. Here, the environment is a continuous source
    of information that returns data according to the agent's actions. And, because
    the data returned by an environment could be potentially infinite, there are many
    conceptual and practical differences among the supervised settings that arise
    while training. For the purpose of this chapter, however, it is important to highlight
    the fact that different environments not only provide different tasks to accomplish,
    but can also have different types of input, output, and reward signals, while
    also requiring the adaptation of the algorithm in each case. For example, a robot
    could either sense its state from a visual input, such as an RGB camera, or from
    discrete internal sensors.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you'll set up the environment required to code RL algorithms
    and build your first algorithm. Despite being a simple algorithm that plays CartPole,
    it offers a useful baseline to master the basic RL cycle before moving on to more
    advanced RL algorithms. Also, because, in the later chapters, you'll code many
    deep neural networks, here, we'll give you a brief recap about TensorFlow and
    introduce TensorBoard, a visualization tool.
  prefs: []
  type: TYPE_NORMAL
- en: Almost all the environments used throughout the book are based on the interface
    open sourced by OpenAI called **Gym**. Therefore, we'll take a look at it and
    use some of its built-in environments. Then, before moving on to an in-depth examination
    of RL algorithms in the next chapters, we'll list and explain the strengths and
    differences of a number of open source environments. In this way, you'll have
    a broad and practical overview of the problems that can be tackled with RL.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI Gym and RL cycles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Types of RL environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the three main tools required to create deep RL algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Programming language**: Python is the first choice for the development of
    machine learning algorithms on account of its simplicity and the third-party libraries
    that are built around it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep learning framework**: In this book, we use TensorFlow because, as we''ll
    see in the *TensorFlow* section, it is scalable, flexible, and very expressive.
    Despite this, many other frameworks can be used in its place, including PyTorch
    and Caffe.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment**: Throughout the book, we''ll use many different environments
    to demonstrate how to deal with different types of problems and to highlight the
    strengths of RL algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this book, we use Python 3.7, but all versions above 3.5 should work. We
    also assume that you've already installed `numpy` and `matplotlib`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you haven''t already installed TensorFlow, you can do so through their website
    or by typing the following in a Terminal window:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, you can type the following command, if your machine has GPUs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can find all the installation instructions and the exercises relating to
    this chapter on the GitHub repository, which can be found here: [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)[.](https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-Algorithms-with-Python)
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how to install the environments.
  prefs: []
  type: TYPE_NORMAL
- en: Installing OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI Gym offers a general interface as well as a broad variety of environments.
  prefs: []
  type: TYPE_NORMAL
- en: To install it, we will use the following commands.
  prefs: []
  type: TYPE_NORMAL
- en: 'On OSX, we can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu 16.04, we will use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'On Ubuntu 18.04, we will use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding command for your respective OS, the following command
    is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Certain Gym environments also require the installation of `pybox2d`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Installing Roboschool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The final environment we are interested in is Roboschool, a simulator for robots.
    It''s easy to install, but if you encounter any errors, take a look at its GitHub
    repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: OpenAI Gym and RL cycles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since RL requires an agent and an environment to interact with each other, the
    first example that may spring to mind is the earth, the physical world we live
    in. Unfortunately, for now, it is actually used in only a few cases. With the
    current algorithms, the problems stem from the large number of interactions that
    an agent has to execute with the environment in order to learn good behaviors.
    It may require hundreds, thousands, or even millions of actions, requiring way too
    much time to be feasible. One solution is to use simulated environments to start
    the learning process and, only at the end, fine-tune it in the real world. This
    approach is way better than learning just from the world around it, but still
    requires slow real-world interactions. However, in many cases, the task can be
    fully simulated. To research and implement RL algorithms, games, video games,
    and robot simulators are a perfect testbed because, in order to be solved, they
    require capabilities such as planning, strategy, and long-term memory. Moreover,
    games have a clear reward system and can be completely simulated in an artificial
    environment (computers), allowing fast interactions that accelerate the learning
    process. For these reasons, in this book, we'll use mostly video games and robot
    simulators to demonstrate the capabilities of RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Gym, an open source toolkit for developing and researching RL algorithms,
    was created to provide a common and shared interface for environments, while making
    a large and diverse collection of environments available. These include Atari
    2600 games, continuous control tasks, classic control theory problems, simulated
    robotic goal-based tasks, and simple text games. Owing to its generality, many
    environments created by third parties are using the Gym interface.
  prefs: []
  type: TYPE_NORMAL
- en: Developing an RL cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A basic RL cycle is shown in the following code block. This essentially makes
    the RL model play for 10 moves while rendering the game at each step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This leads to the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b77b90c-3aad-4e87-a1fc-2de8c5ec6805.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Rendering of CartPole'
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a closer look at the code. It starts by creating a new environment
    named `CartPole-v1`, a classic game used in control theory problems. However,
    before using it, the environment is initialized by calling `reset()`*.* After
    doing so, the cycle loops 10 times. In each iteration, `env.action_space.sample()`
    samples a random action, executes it in the environment with `env.step()`*,* and
    displays the result with the `render()` method; that is, the current state of
    the game, as in the preceding screenshot. In the end, the environment is closed
    by calling `env.close()`.
  prefs: []
  type: TYPE_NORMAL
- en: Don't worry if the following code outputs deprecation warnings; they are there
    to notify you that some functions have been changed. The code will still be functioning
    correctly.
  prefs: []
  type: TYPE_NORMAL
- en: This cycle is the same for every environment that uses the Gym interface, but
    for now, the agent can only play random actions without having any feedback, which
    is essential to any RL problem.
  prefs: []
  type: TYPE_NORMAL
- en: In RL, you may see the terms **state** and **observation** being used almost
    interchangeably, but they are not the same. We talk about state when all the information
    pertaining to the environment is encoded in it. We talk about observation when
    only a part of the actual state of the environment is visible to the agent, such
    as the perception of a robot. To simplify this, OpenAI Gym always uses the term
    observation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the flow of the cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/81b29847-365f-4f0f-be9f-48fdc91fe028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Basic RL cycle according to OpenAI Gym. The environment returns
    the next state, a reward, a done flag, and some additional information'
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the `step()` method returns four variables that provide information
    about the interaction with the environment. The preceding diagram shows the loop
    between the agent and environment, as well as the variables exchanged; namely, **Observation**,
    **Reward**, **Done**, and **Info**. **Observation** is an object that represents
    the new observation (or state) of the environment. **Reward** is a float number
    that represents the number of rewards obtained in the last action. **Done** is
    a Boolean value that is used on tasks that are episodic; that is, tasks that are limited
    in terms of the number of interactions. Whenever `done`is `True`, this means that
    the episode has terminated and that the environment should be reset. For example,
    `done` is `True` when the task has been completed or the agent has died. **Info**,
    on the other hand, is a dictionary that provides extra information about the environment
    but that usually isn't used.
  prefs: []
  type: TYPE_NORMAL
- en: If you have never heard of CartPole, it's a game with the goal of balancing
    a pendulum acting on a horizontal cart. A reward of +1 is provided for every timestep
    when the pendulum is in the upright position. The episode ends when it is too
    unbalanced or it manages to balance itself for more than 200 timesteps (collecting
    a maximum cumulative reward of 200).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now create a more complete algorithm that plays 10 games and prints
    the accumulated reward for each game using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following table shows the output of the `step()` method over the last four
    actions of a game:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Observation** | **Reward** | **Done** | **Info** |'
  prefs: []
  type: TYPE_TB
- en: '| [-0.05356921, -0.38150626, 0.12529277, 0.9449761 ] | 1.0  | False  | {} |'
  prefs: []
  type: TYPE_TB
- en: '| [-0.06119933, -0.57807287, 0.14419229, 1.27425449] | 1.0 | False  | {} |'
  prefs: []
  type: TYPE_TB
- en: '| [-0.07276079, -0.38505429, 0.16967738, 1.02997704] | 1.0 | False  | {} |'
  prefs: []
  type: TYPE_TB
- en: '| [-0.08046188, -0.58197758, 0.19027692, 1.37076617] | 1.0 | False  | {} |'
  prefs: []
  type: TYPE_TB
- en: '| [-0.09210143, -0.3896757, 0.21769224, 1.14312384] | 1.0 | True | {} |'
  prefs: []
  type: TYPE_TB
- en: Notice that the environment's observation is encoded in a 1 x 4 array; that
    the reward, as we expected, is always 1; and that `done` is `True` only in the
    last row when the game is terminated. Also, **Info**, in this case, is empty.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming chapters, we'll create agents that play CartPole by taking more
    intelligent actions depending on the current state of the pole.
  prefs: []
  type: TYPE_NORMAL
- en: Getting used to spaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In OpenAI Gym, actions and observations are mostly instances of the `Discrete`
    or `Box` class. These two classes represent different spaces. `Box` represents
    an *n*-dimensional array, while `Discrete`, on the other hand, is a space that
    allows a fixed range of non-negative numbers. In the preceding table, we have
    already seen that the observation of CartPole is encoded by four floats, meaning
    that it''s an instance of the `Box` class. It is possible to check the type and
    dimension of the observation spaces by printing the `env.observation_space` variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Indeed, as we expected, the output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this book, we mark the output of `print()` by introducing the printed text
    with `>>`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way, it is possible to check the dimension of the action space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In particular, `Discrete(2)` means that the actions could either have the value
    `0` or `1`. Indeed, if we use the sampling function used in the preceding example,
    we obtain `0` or `1` (in CartPole, this means left or right*)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The `low`and `high` instance attributes return the minimum and maximum values
    allowed by a `Box` space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Development of ML models using TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow is a machine learning framework that performs high-performance numerical
    computations. TensorFlow owes its popularity to its high quality and vast amount
    of documentation, its ability to easily serve models at scale in production environments,
    and the friendly interface to GPUs and TPUs.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow, to facilitate the development and deployment of ML models, has many
    high-level APIs, including Keras, Eager Execution, and Estimators. These APIs
    are very useful in many contexts, but, in order to develop RL algorithms, we'll
    only use low-level APIs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s code immediately using **TensorFlow**. The following lines of code
    execute the sum of the constants, `a` and `b`, created with `tf.constant()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'A particularity of TensorFlow is the fact that it expresses all computations
    as a computational graph that has to first be defined and later executed. Only
    after execution will the results be available. In the following example, after
    the operation, `c = a + b`, `c` doesn''t hold the end value. Indeed, if you print
    `c` before creating the session, you''ll obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This is the class of the `c` variable, not the result of the addition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, execution has to be done inside a session that is instantiated with `tf.Session()`.
    Then, to perform the computation, the operation has to be passed as input to the
    `run` function of the session just created. Thus, to actually compute the graph
    and consequently sum `a` and `b`, we need to create a session and pass `c` as
    an input to `session.run`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you are using Jupyter Notebook, make sure to reset the previous graph by
    running `tf.reset_default_graph()`.
  prefs: []
  type: TYPE_NORMAL
- en: Tensor
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The variables in TensorFlow are represented as tensors that are arrays of any
    number of dimensions. There are three main types of tensors—`tf.Variable`, `tf.constant`,
    and `tf.placeholder`. Except for `tf.Variable`, all the other tensors are immutable.
  prefs: []
  type: TYPE_NORMAL
- en: 'To check the shape of a tensor, we will use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The elements of a tensor are easily accessible, and the mechanisms are similar
    to those employed by Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Constant
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we have already seen, a constant is an immutable type of tensor that can
    be easily created using `tf.constant`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Placeholder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A placeholder is a tensor that is fed at runtime. Usually, placeholders are
    used as input for models. Every input passed to a computational graph at runtime
    is fed with `feed_dict`. `feed_dict` is an optional argument that allows the caller
    to override the value of tensors in the graph. In the following snippet, the `a` placeholder
    is overridden by `[[0.1,0.2,0.3]]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If the size of the first dimension of the input is not known during the creation
    of the graph, TensorFlow can take care of it. Just set it to `None`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This feature is useful when the number of training examples is not known initially.
  prefs: []
  type: TYPE_NORMAL
- en: Variable
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **variable** is a mutable tensor that can be trained using an optimizer. For
    example, they can be the free variables that constitute the weights and biases
    of a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now create two variables, one uniformly initialized, and one initialized
    with constant values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The variables aren't initialized until `global_variables_initializer()` is called.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the variables created in this way are set as `trainable`, meaning that
    the graph can modify them, for example, after an optimization operation. The variables
    can be set as non-trainable, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'An easy way to access all the variables is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Creating a graph
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A **graph** represents low-level computations in terms of the dependencies between
    operations. In TensorFlow, you first define a graph, and then create a session
    that executes the operations in the graph.
  prefs: []
  type: TYPE_NORMAL
- en: The way a graph is built, computed, and optimized in TensorFlow allows a high
    degree of parallelism, distributed execution, and portability, all very important
    properties when building machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give you an idea of the structure of a graph produced internally by TensorFlow,
    the following program produces the computational graph demonstrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This results in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/34e69931-f42d-439c-b29e-656073e965d4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Example of a computational graph'
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To better digest all the concepts, let''s now create a simple linear regression
    model. First, we have to import all the libraries and set a random seed, both
    for NumPy and TensorFlow (so that we''ll all have the same results):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create a synthetic dataset consisting of 100 examples, as shown
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a558b08f-c230-46e9-b908-aae0172ee7e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Dataset used in the linear regression example'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because this is a linear regression example, *y = W * X + b*, where *W*and
    *b* are arbitrary values. In this example, we set `W` = `0.5` and `b` = `1.4`.
    Additionally, we add some normal random noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step involves creating the placeholders for the input and the output,
    and the variables of the weight and bias of the linear model. During training,
    these two variables will be optimized to be as similar as possible to the weight
    and bias of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we build the computational graph defining the linear operation and the
    **mean squared error** (**MSE**) loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now instantiate the optimizer and call `minimize()` to minimize the
    MSE loss. `minimize()` first computes the gradients of the variables (`v_weight`
    and `v_bias`) and then applies the gradient, updating the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create a session and initialize all the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The training is done by running the optimizer multiple times while feeding
    the dataset to the graph. To keep track of the state of the model, the MSE loss
    and the model variables (weight and bias) are printed every 40 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, we can print the final values of the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: During the training phase, it's possible to see that the MSE loss would decrease
    toward a non-zero value (of about 3.71). That's because we added random noise
    to the dataset that prevents the MSE from reaching a perfect value of 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, as anticipated, with regard to the weight and bias of the model approach,
    the values of `0.500` and `1.473` are precisely the values around which the dataset
    has been built. The blue line visible in the following screenshot is the prediction
    of the trained linear model, while the points are our training examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1eeea8a-b731-4fd5-bff9-6679bd86cdea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Linear regression model predictions'
  prefs: []
  type: TYPE_NORMAL
- en: For all the color references in the chapter, please refer to the color images
    bundle: [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf.).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keeping track of how variables change during the training of a model can be
    a tedious job. For instance, in the linear regression example, we kept track of
    the MSE loss and of the parameters of the model by printing them every 40 epochs.
    As the complexity of the algorithms increases, there is an increase in the number
    of variables and metrics to be monitored. Fortunately, this is where TensorBoard
    comes to the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorBoard is a suite of visualization tools that can be used to plot metrics,
    visualize TensorFlow graphs, and visualize additional information. A typical TensorBoard
    screen is similar to the one shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c541147-d088-4565-9993-2e32ab112850.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Scalar TensorBoard page'
  prefs: []
  type: TYPE_NORMAL
- en: 'The integration of TensorBoard with TensorFlow code is pretty straightforward as
    it involves only a few tweaks to the code. In particular, to visualize the MSE
    loss over time and monitor the weight and bias of our linear regression model
    using TensorBoard, it is first necessary to attach the loss tensor to `tf.summar.scalar()`
    and the model''s parameters to `tf.summary.histogram()`*. *The following snippet
    should be added after the call to the optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, to simplify the process and handle them as a single summary, we can merge
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we have to instantiate a `FileWriter` instance that will log
    all the summary information in a file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines create a unique filename using the current date and time.
    In the third line, the path of the file and the TensorFlow graph are passed to `FileWriter()`.
    The second parameter is optional and represents the graph to visualize.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final change is done in the training loop by replacing the previous line, `train_loss,
    _ = session.run(..)`, with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: First, `all_summary` is executed in the current session, and then the result
    is added to `file_writer` to be saved in the file. This procedure will run the
    three summaries that were merged previously and log them in the log file. TensorBoard
    will then read from this file and visualize the scalar, the two histograms, and
    the computation graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember to close `file_writer` at the end, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can open TensorBoard by going to the working directory and typing
    the following in a terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This command creates a web server that listens to port `6006`. To start TensorBoard,
    you have to go to the link that TensorBoard shows you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ca1b057-02de-40bc-994b-2822f44fecae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Histogram of the linear regression model''s parameters'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can now browse TensorBoard by clicking on the tabs at the top of the page
    to access the plots, the histograms, and the graph. In the preceding—as well as
    the following—screenshots, you can see some of the results visualized on those
    pages. The plots and the graphs are interactive, so take some time to explore
    them in order to improve your understanding of their use. Also check the TensorBoard
    official documentation ([https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard)) to
    learn more about the additional features included in TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a568965-c104-40d1-94b3-536ae8e0bc0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Scalar plot of the MSE loss'
  prefs: []
  type: TYPE_NORMAL
- en: Types of RL environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Environments, similar to labeled datasets in supervised learning, are the essential
    part of RL as they dictate the information that has to be learned and the choice
    of algorithms. In this section, we'll take a look at the main differences between
    the types of environments and list some of the most important open source environments.
  prefs: []
  type: TYPE_NORMAL
- en: Why different environments?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While, for real applications, the choice of environment is dictated by the task
    to be learned, for research applications, usually, the choice is dictated by intrinsic
    features of the environment. In this latter case, the end goal is not to train
    the agent on a specific task, but to show some task-related capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if the goal is to create a multi-agent RL algorithm, the environment
    should have at least two agents with a means to communicate with one another,
    regardless of the end task. Instead, to create a lifelong learner (agents that
    continuously create and learn more difficult tasks using the knowledge acquired
    in previous easier tasks), the primary quality that the environment should have
    is the ability to adapt to new situations and a realistic domain.
  prefs: []
  type: TYPE_NORMAL
- en: 'Task aside, environments can differ by other characteristics, such as complexity,
    observation space, action space, and reward function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Complexity**: Environments can spread across a wide spectrum, from the balance
    of a pole to the manipulation of physical objects with a robot hand. More complex
    environments can be chosen to show the capability of an algorithm to deal with
    a large state space that mimics the complexity of the world. On the other hand,
    simpler ones can be used to show only some specific qualities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observation space**: As we have already seen, the observation space can range
    from the full state of the environment to only a partial observation perceived
    by the perception systems, such as row images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action space**: Environments with a large continuous action space challenge
    the agent to deal with real-value vectors, whereas discrete actions are easier
    to learn as they have only a limited number of actions available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward function**: Environments with hard explorations and delayed rewards,
    such as Montezuma''s revenge, are very challenging to solve. Surprisingly, only
    a few algorithms are able to reach human levels. For this reason, these environments
    are used as a test bed for algorithms that propose to address the exploration
    problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open source environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How can we design an environment that meets our requirements? Fortunately,
    there are many open source environments that are built to tackle specific or broader
    problems. By way of an example, CoinRun, shown in the following screenshot, was
    created to measure the generalization capabilities of an algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a82a2e04-618c-49bb-b0b8-3f7a74f3cb1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: The CoinRun environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now list some of the main open source environments available. These
    are created by different teams and companies, but almost all of them use the OpenAI
    Gym interface:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63951647-17fb-4b34-b107-63834c6408da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Roboschool environment'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gym Atari** ([https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari)):
    Includes Atari 2600 games with screen images as input. They are useful for measuring
    the performance of RL algorithms on a wide variety of games with the same observation
    space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gym Classic control** ([https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control)):
    Classic games that can be used for the easy evaluation and debugging of an algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gym MuJoCo** ([https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco)):
    Includes continuous control tasks (such as Ant, and HalfCheetah) built on top
    of MuJoCo, a physics engine that requires a paid license (a free license is available
    for students).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MalmoEnv** ([https://github.com/Microsoft/malmo](https://github.com/Microsoft/malmo)):
    An environment built on top of Minecraft.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pommerman** ([https://github.com/MultiAgentLearning/playground](https://github.com/MultiAgentLearning/playground)):
    A great environment for training multi-agent algorithms. Pommerman is a variant
    of the famous Bomberman.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Roboschool** ([https://github.com/openai/roboschool](https://github.com/openai/roboschool)):
    A robot simulation environment integrated with OpenAI Gym. It includes an environment
    replica of MuJoCo, as shown in the preceding screenshot, two interactive environments
    to improve the robustness of the agent, and one multiplayer environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Duckietown** ([https://github.com/duckietown/gym-duckietown](https://github.com/duckietown/gym-duckietown)):
    A self-driving car simulator with different maps and obstacles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PLE** ([https://github.com/ntasfi/PyGame-Learning-Environment](https://github.com/ntasfi/PyGame-Learning-Environment)):
    PLE includes many different arcade games, such as Monster Kong, FlappyBird, and
    Snake.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unity ML-Agents** ([https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents)):
    Environments built on top of Unity with realistic physics. ML-agents allow a great
    degree of freedom and the possibility to create your own environment using Unity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CoinRun** ([https://github.com/openai/coinrun](https://github.com/openai/coinrun)):
    An environment that addresses the problem of overfitting in RL. It generates different
    environments for training and testing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepMind Lab** ([https://github.com/deepmind/lab](https://github.com/deepmind/lab)):
    Provides a suite of 3D environments for navigation and puzzle tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DeepMind PySC2** ([https://github.com/deepmind/pysc2](https://github.com/deepmind/pysc2)):
    An environment for learning the complex game, StarCraft II.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hopefully, in this chapter, you have learned about all the tools and components
    needed to build RL algorithms. You set up the Python environment required to develop
    RL algorithms and programmed your first algorithm using an OpenAI Gym environment.
    As the majority of state-of-the-art RL algorithms involve deep learning, you have
    been introduced to TensorFlow, a deep learning framework that you'll use throughout
    the book. The use of TensorFlow speeds up the development of deep RL algorithms
    as it deals with complex parts of deep neural networks such as backpropagation.
    Furthermore, TensorFlow is provided with TensorBoard, a visualization tool that
    is used to monitor and help the algorithm debugging process.
  prefs: []
  type: TYPE_NORMAL
- en: Because we'll be using many environments in the subsequent chapters, it's important
    to have a clear understanding of their differences and distinctiveness. By now,
    you should also be able to choose the best environments for your own projects,
    but bear in mind that despite the fact that we provided you with a comprehensive
    list, there may be many others that could better suit your problem.
  prefs: []
  type: TYPE_NORMAL
- en: That being said, in the following chapters, you'll finally learn how to develop
    RL algorithms. Specifically, in the next chapter, you will be presented with algorithms
    that can be used in simple problems where the environment is completely known.
    After those, we'll build more sophisticated ones that can deal with more complex
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What's the output of the `step()` function in Gym?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you sample an action using the OpenAI Gym interface?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the main difference between the `Box` and `Discrete` classses?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are deep learning frameworks used in RL?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a tensor?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What can be visualized in TensorBoard?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To create a self-driving car, which of the environments mentioned in the chapter
    would you use?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the TensorFlow official guide, refer to the following link: [https://www.tensorflow.org/guide/low_level_intro](https://www.tensorflow.org/guide/low_level_intro).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the TensorBoard official guide, refer to the following link: [https://www.tensorflow.org/guide/summaries_and_tensorboard](https://www.tensorflow.org/guide/summaries_and_tensorboard).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
