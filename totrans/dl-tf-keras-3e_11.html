<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer501">
<h1 class="chapterNumber">11</h1>
<h1 class="chapterTitle" id="_idParaDest-314">Reinforcement Learning</h1>
<p class="normal">This chapter introduces <strong class="keyWord">Reinforcement Learning</strong> (<strong class="keyWord">RL</strong>)—the least explored and yet most promising learning paradigm. Reinforcement learning is very different from the supervised and unsupervised learning models we covered in earlier chapters. Starting from a clean slate (that is, having no prior information), the RL agent can go through multiple stages of trial and error, and learn to achieve a goal, all the while the only input being the feedback from the environment. The research in RL by OpenAI seems to suggest that continuous competition can be a cause for the evolution of intelligence. Many deep learning practitioners believe that RL will play an important role in the big AI dream: <strong class="keyWord">Artificial General Intelligence</strong> (<strong class="keyWord">AGI</strong>). This chapter will delve into different RL algorithms. The following topics will be covered:</p>
<ul>
<li class="bulletList">What RL is and its lingo</li>
<li class="bulletList">Learn how to use the OpenAI Gym interface</li>
<li class="bulletList">Applications of RL</li>
<li class="bulletList">Deep Q-Networks</li>
<li class="bulletList">Policy gradients</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp11"><span class="url">https://packt.link/dltfchp11</span></a>.</p>
</div>
<h1 class="heading-1" id="_idParaDest-315">An introduction to RL</h1>
<p class="normal">What is <a id="_idIndexMarker1165"/>common between a baby learning to walk, birds learning to fly, and an RL agent learning to play an Atari game? Well, all three involve:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Trial and error</strong>: The child (or the bird) tries various ways, fails many times, and succeeds in some ways<a id="_idIndexMarker1166"/> before it can really walk (or fly). The RL agent plays many games, winning some and losing many, before it can become reliably successful.</li>
<li class="bulletList"><strong class="keyWord">Goal</strong>: The child has the goal<a id="_idIndexMarker1167"/> to walk, the bird to fly, and the RL agent to win the game.</li>
<li class="bulletList"><strong class="keyWord">Interaction with the environment</strong>: The only <a id="_idIndexMarker1168"/>feedback they have is from their environment.</li>
</ul>
<p class="normal">So, the first questions that arise are what is RL, and how is it different from supervised and unsupervised learning? Anyone who owns a pet knows that the best strategy to train a pet is rewarding it for desirable behavior and disciplining it for bad behavior. RL, also called <strong class="keyWord">learning with a critic</strong>, is a<a id="_idIndexMarker1169"/> learning paradigm where the agent learns in the same manner. The agent here corresponds to our network (program); it can perform a set of <strong class="keyWord">actions</strong> (<strong class="keyWord">a</strong>), which brings about a change in the <strong class="keyWord">state</strong> (<strong class="keyWord">s</strong>) of the environment, and, in turn, the agent receives a reward or punishment from the environment.</p>
<p class="normal">For example, consider the case of training a dog to fetch a ball: here, the dog is our agent, the voluntary muscle movements that the dog makes are the actions, and the ground (as well as the person and ball) is the environment; the dog perceives our reaction to its action in terms of giving it a treat as a reward. RL can be defined as a computational approach to goal-directed learning and decision making, from interaction with the environment, under some idealized conditions. The agent can sense the state of the environment, and the agent can perform specific well-defined actions on the environment. This causes two things: first, a change in the state of the environment, and second, a reward is generated (under ideal conditions). This cycle continues, and in theory the agent learns how to more frequently generate a reward over time:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with medium confidence" height="520" src="../Images/B18331_11_01.png" width="877"/></figure>
<p class="packt_figref">Figure 11.1: Reinforcement learning: interaction between agent and environment</p>
<p class="normal">Unlike supervised<a id="_idIndexMarker1170"/> learning, the agent is not presented with any training examples; it does not know what the correct action is.</p>
<p class="normal">And unlike unsupervised learning, the agent’s goal is not to find some inherent structure in the input (the learning may find some structure, but that isn’t the goal); instead, its only goal is to maximize the rewards (in the long run) and reduce the punishments.</p>
<h2 class="heading-2" id="_idParaDest-316">RL lingo</h2>
<p class="normal">Before learning various RL algorithms, it is important we understand a few important terms. We will illustrate the terms with the help of two examples, first a robot in a maze, and second an agent <a id="_idIndexMarker1171"/>controlling the wheels of a <strong class="keyWord">Self-Driving Car</strong> (<strong class="keyWord">SDC</strong>). The two RL agents are shown as follows:</p>
<figure class="mediaobject"><img alt="A picture containing diagram  Description automatically generated" height="326" src="../Images/B18331_11_02.png" width="568"/></figure>
<p class="packt_figref">Figure 11.2: State for a robot trying to find a path in a maze (LHS). State for an agent trying to control the steering wheel of a self-driving car (RHS)</p>
<p class="normal"><em class="italic">Figure 11.2</em> shows the two examples we will be considering. Let us start with the terms:</p>
<ul>
<li class="bulletList"><strong class="keyWord">State</strong>, <em class="italic">S</em>: State is the set <a id="_idIndexMarker1172"/>of tokens (or representations) that can define all of the possible states the environment can be in. It can be continuous or discrete. In the case of the robot finding its path through a maze, the state can be represented by a 4×4 matrix, with elements indicating whether that block is empty, occupied, or blocked. A block with a value of 1 means it is occupied by the robot, 0 means it is empty, and <em class="italic">X</em> represents that the block is impassable. Each element in this array, <em class="italic">S</em>, can have one of these three discrete values, so the state is discrete in nature. Next, consider the agent controlling the steering wheel of a self-driving car. The agent takes as input the front-view image. The image contains continuous valued pixels, so here the state is continuous.</li>
<li class="bulletList"><strong class="keyWord">Action</strong>, <em class="italic">A(S)</em>: Actions are the <a id="_idIndexMarker1173"/>set of all possible things that the agent can do in a particular state. The set of possible actions, <em class="italic">A</em>, depends on the present state, <em class="italic">S</em>. Actions may or may not result in a change of state. Like states, they can be discrete or continuous. The robot finding a path in the maze can perform five discrete actions [<strong class="keyWord">up</strong>, <strong class="keyWord">down</strong>, <strong class="keyWord">left</strong>, <strong class="keyWord">right</strong>, <strong class="keyWord">no change</strong>]. The SDC agent, on the other hand, can rotate the steering wheel at a continuous range of angles.</li>
<li class="bulletList"><strong class="keyWord">Reward </strong><em class="italic">R(S,A,S’)</em>: Rewards are <a id="_idIndexMarker1174"/>a scalar value returned by the environment based on the agent’s action(s). Here <em class="italic">S</em> is the present state and <em class="italic">S’</em> is the state of the environment after action <em class="italic">A</em> is taken. It is determined by the goal; the agent gets a higher reward if the action brings it near the goal, and a low (or even negative) reward otherwise. How we define a reward is totally up to us—in the case of the maze, we can define the reward as the Euclidean distance between the agent’s current position and goal. The SDC agent reward can be that the car is on the road (positive reward) or off the road (negative reward).</li>
<li class="bulletList"><strong class="keyWord">Policy</strong> <img alt="" height="46" src="../Images/B18331_11_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="79"/>: Policy defines a mapping between each state and the action to take in that state. The <a id="_idIndexMarker1175"/>policy can be <em class="italic">deterministic</em>, that is, for each state, there is a well-defined policy. In the case of the maze robot, a policy can be that if the top block is empty, move up. The policy can also be <em class="italic">stochastic</em>, that is, where an action is taken by some probability. It can be implemented as a simple look-up table, or it can be a function dependent on the present state. The policy is the core of the RL agent. In this chapter, we’ll learn about different algorithms that help the agent to learn the policy.</li>
<li class="bulletList"><strong class="keyWord">Return </strong><em class="italic">G</em><sub class="italic">t</sub>: This is the <a id="_idIndexMarker1176"/>discounted sum of all future rewards starting from the current time, mathematically defined as:</li>
</ul>
<p class="center"><img alt="" height="133" src="../Images/B18331_11_002.png" style="height: 3.33em !important;" width="300"/></p>
<ul>
<li class="bulletList">Here <em class="italic">R</em><sub class="italic">t</sub> is the reward at time <em class="italic">t</em> and <img alt="" height="46" src="../Images/B18331_11_003.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> is the discount factor; its value lies between 0 and 1. The discount factor determines how important future rewards are in deciding the policy. If it is near zero, the agent gives importance to the immediate rewards. A high discount factor, however, means the agent is looking far into the future. It may give up immediate reward in favor of high future rewards, just as in the game chess, you may sacrifice a pawn to later checkmate the opponent.</li>
<li class="bulletList"><strong class="keyWord">Value function </strong><em class="italic">V(S)</em>: This defines the “goodness” of a state in the long run. It can be thought of as the total <a id="_idIndexMarker1177"/>amount of reward the agent can expect to accumulate over time, starting from the state, <em class="italic">S</em>. You can think of it as long-term good, as opposed to an immediate but short-lived good. What do you think is more important, maximizing the immediate reward or the value function? You probably guessed right: just as in chess, we sometimes lose a pawn to <a id="_idIndexMarker1178"/>win the game a few steps later, and so the agent should try to maximize the value function.</li>
<li class="bulletList">Normally, the value is defined either as<a id="_idIndexMarker1179"/> the <strong class="keyWord">state-value function</strong> <img alt="" height="46" src="../Images/B18331_11_004.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/> or the <strong class="keyWord">action-value function</strong> <img alt="" height="50" src="../Images/B18331_11_005.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="142"/>, where <img alt="" height="42" src="../Images/B18331_11_006.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/> is the policy followed. The state-value<a id="_idIndexMarker1180"/> function is the expected return from the state <em class="italic">S</em> after following policy <img alt="" height="42" src="../Images/B18331_11_006.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/>:</li>
</ul>
<p class="center"><img alt="" height="46" src="../Images/B18331_11_008.png" style="height: 1.15em !important; vertical-align: 0.10em !important;" width="379"/></p>
<ul>
<li class="bulletList">Here <em class="italic">E</em> is the expectation, and <em class="italic">S</em><sub class="italic">t</sub><em class="italic">=s</em> is the state at time <em class="italic">t</em>. The action-value function is the expected return from the state <em class="italic">S</em>, taking an action <em class="italic">A=a</em> and following the policy <img alt="" height="42" src="../Images/B18331_11_006.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/>:</li>
</ul>
<p class="center"><img alt="" height="46" src="../Images/B18331_11_010.png" style="height: 1.15em !important; vertical-align: 0.10em !important;" width="546"/></p>
<ul>
<li class="bulletList"><strong class="keyWord">Model of the environment</strong>: This is an<a id="_idIndexMarker1181"/> optional element. It mimics the behavior of the environment, and it contains the physics of the environment; in other words, it indicates how the environment will behave. The model of the environment is defined by the transition probability to the next state. This is an optional component; we can have <strong class="keyWord">model-free</strong> reinforcement<a id="_idIndexMarker1182"/> learning as well where the transition probability is not needed to define the RL process.</li>
</ul>
<p class="normal">In RL, we assume that the state of the environment follows the <strong class="keyWord">Markov property</strong>, that is, each state is dependent solely on the preceding<a id="_idIndexMarker1183"/> state, the action taken from the action space, and the corresponding reward. </p>
<p class="normal">That is, if <em class="italic">S</em><sup class="italic">t</sup><sup class="superscript">+1</sup> is the state of the environment at time <em class="italic">t+1</em>, then it is a function of <em class="italic">S</em><sup class="italic">t</sup> state at time <em class="italic">t</em>, <em class="italic">A</em><sup class="italic">t</sup> is the action taken at time <em class="italic">t</em>, and <em class="italic">R</em><sup class="italic">t</sup> is the corresponding reward received at time <em class="italic">t</em>, no prior history is needed. If <em class="italic">P(S</em><sup class="italic">t</sup><sup class="superscript">+1</sup>|<em class="italic">S</em><sup class="superscript">t</sup><em class="italic">)</em> is the transition probability, mathematically the Markov property can be written as:</p>
<p class="center"><img alt="" height="46" src="../Images/B18331_11_011.png" style="height: 1.15em !important; vertical-align: 0.10em !important;" width="575"/></p>
<p class="normal">And thus, RL can be <a id="_idIndexMarker1184"/>assumed to be a <strong class="keyWord">Markov Decision Process</strong> (<strong class="keyWord">MDP</strong>).</p>
<h2 class="heading-2" id="_idParaDest-317">Deep reinforcement learning algorithms</h2>
<p class="normal">The basic idea of <strong class="keyWord">Deep Reinforcement Learning</strong> (<strong class="keyWord">DRL</strong>) is that we can use a deep neural network to approximate either the<a id="_idIndexMarker1185"/> policy function or the value function. In this chapter, we will be studying some popular DRL algorithms. These algorithms can be classified into two classes, depending upon what they approximate:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Value-based methods</strong>: In these methods, the algorithms take the action that maximizes the value<a id="_idIndexMarker1186"/> function. The agent here learns to predict how good a given state or action would be. An example of the value-based method is the Deep Q-Network. Consider, for example, our robot in a maze: assuming that the value of each state is the negative of the number of steps needed to go from that box to the goal, then, at each time step, the agent will choose the action that takes it to a state with optimal value, as in the following diagram. So, starting from a value of <strong class="keyWord">-6</strong>, it’ll move to <strong class="keyWord">-5</strong>, <strong class="keyWord">-4</strong>, <strong class="keyWord">-3</strong>, <strong class="keyWord">-2</strong>, <strong class="keyWord">-1</strong>, and eventually reach the goal with the value <strong class="keyWord">0</strong>: </li>
</ul>
<figure class="mediaobject"><img alt="Table  Description automatically generated" height="335" src="../Images/B18331_11_03.png" width="282"/></figure>
<p class="packt_figref">Figure 11.3: Demo value function values for the maze-finding robot</p>
<ul>
<li class="bulletList"><strong class="keyWord">Policy-based methods</strong>: In these methods, the algorithms predict the optimal policy (the one that maximizes the expected return), without maintaining the value function estimates. The aim is to find the optimal policy, instead of the <a id="_idIndexMarker1187"/>optimal action. An example of the policy-based method is policy gradients. Here, we approximate the policy function, which allows us to map each state to the best corresponding action. One advantage of policy-based methods over value-based is that we can use them even for continuous action spaces.</li>
</ul>
<p class="normal">Besides the algorithms approximating either policy or value, there are a few questions we need to answer to make reinforcement learning work.</p>
<h3 class="heading-3" id="_idParaDest-318">How does the agent choose its actions, especially when untrained?</h3>
<p class="normal">When the agent starts learning, it has<a id="_idIndexMarker1188"/> no idea what the best way in which to determine an action is, or which action will provide the best <em class="italic">Q</em> value. So how do we go about it? We take a leaf out of nature’s book. Like bees and ants, the agent makes a balance between exploring new actions and exploiting learned ones. Initially, when the agent starts, it has no idea which action among the possible actions is better, so it makes random choices, but as it<a id="_idIndexMarker1189"/> learns, it starts making use of the learned policy. This is called the <strong class="keyWord">exploration vs exploitation </strong>[2] tradeoff. Using exploration, the agent gathers more information, and later exploits the gathered information to make the best decision.</p>
<h3 class="heading-3" id="_idParaDest-319">How does the agent maintain a balance between exploration and exploitation?</h3>
<p class="normal">There are various strategies; one<a id="_idIndexMarker1190"/> of the most employed is the <strong class="keyWord">epsilon-greedy</strong> (<img alt="" height="50" src="../Images/B18331_11_012.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="196"/>) policy. Here, the agent explores unceasingly, and depending upon the value of <img alt="" height="46" src="../Images/B18331_11_013.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="146"/>, at each step the agent selects a random action with probability <img alt="" height="42" src="../Images/B18331_11_014.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/>, and with probability <img alt="" height="42" src="../Images/B18331_11_015.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="88"/> selects an action that maximizes the value function. Normally, the value of <img alt="" height="42" src="../Images/B18331_11_014.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> decreases asymptotically. In Python the <img alt="" height="50" src="../Images/B18331_11_012.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="196"/> policy can be implemented as:</p>
<pre class="programlisting code"><code class="hljs-code">  <span class="hljs-keyword">if</span> np.random.rand() &lt;= epsilon:
        a = random.randrange(action_size)
  <span class="hljs-keyword">else</span>:
        a = np.argmax(model.predict(s))
</code></pre>
<p class="normal">where <code class="inlineCode">model</code> is the deep neural network approximating the value/policy function, <code class="inlineCode">a</code> is the action chosen from the <a id="_idIndexMarker1191"/>action space of size <code class="inlineCode">action_size</code>, and <code class="inlineCode">s</code> is the state. Another way to perform exploration is to use noise; researchers have experimented with both Gaussian and Ornstein-Uhlenbeck noise with success.</p>
<h3 class="heading-3" id="_idParaDest-320">How to deal with the highly correlated input state space</h3>
<p class="normal">The input to our RL model is the present state<a id="_idIndexMarker1192"/> of the environment. Each action results in some change in the environment; however, the correlation between two consecutive states is very high. Now if we make our network learn based on the sequential states, the high correlation between consecutive inputs results in what is known as <strong class="keyWord">catastrophic forgetting</strong>. To mitigate<a id="_idIndexMarker1193"/> the effect of catastrophic forgetting, in 2018, David Isele <a id="_idIndexMarker1194"/>and Akansel Cosgun proposed the <strong class="keyWord">experience replay </strong>method.</p>
<p class="normal">In simplest terms, the learning algorithm first stores the MDP tuple—state, action, reward, and next state <em class="italic">&lt;S, A, R, S’&gt;</em>—in a buffer/memory. Once a significant amount of memory is built, a batch is selected randomly to train the agent. The memory is continuously refreshed with new additions and old deletions. The use of experience replay provides three benefits:</p>
<ul>
<li class="bulletList">First, it allows the same experience to be potentially used in many weight updates, hence increasing data efficiency.</li>
<li class="bulletList">Second, the random selection of batches of experience removes the correlations between consecutive states presented to the network for training.</li>
<li class="bulletList">Third, it stops any unwanted feedback loops that may arise and cause the network to get stuck in local minima or diverge.</li>
</ul>
<p class="normal">A modified version of experience replay is the <strong class="keyWord">Prioritized Experience Replay</strong> (<strong class="keyWord">PER</strong>). Introduced in 2015 by Tom Schaul et al. [4], it derives from the idea that not all experiences (or, you might say, attempts) are equally important. Some attempts are better lessons than<a id="_idIndexMarker1195"/> others. Thus, instead of selecting the experiences randomly, it will be much more efficient to assign higher priority to more educational experiences in selection for training. In the Schaul paper, it was proposed that experiences in which the difference between the prediction and target is high should be given priority, as the agent could learn a lot in these cases.</p>
<h3 class="heading-3" id="_idParaDest-321">How to deal with the problem of moving targets</h3>
<p class="normal">Unlike supervised learning, the target is <a id="_idIndexMarker1196"/>not previously known in RL. With a moving target, the agent tries to maximize the expected return, but the maximum value goes on changing as the agent learns. In essence, this is like trying to catch a butterfly yet each time you approach it, it moves to a new location. The major reason to have a moving target is that the same networks are used to estimate the action and the target values, and this can cause oscillations in learning.</p>
<p class="normal">A solution to this was proposed by the DeepMind team in their 2015 paper, titled <em class="italic">Human-level Control through Deep Reinforcement Learning</em>, published in Nature. The solution is that now, instead of a moving target, the agent has short-term fixed targets. The agent now maintains two networks, both are exactly the same in architecture, one called the local network, which is used at each step to estimate the present action, and one the target network, which is used to get the target value. However, both networks have their own set of weights. At each time step, the local network learns in the direction such that its estimate and target are near to each other. After some number of time steps, the target network weights are updated. The update<a id="_idIndexMarker1197"/> can be a <strong class="keyWord">hard update</strong>, where the weights of the local network are copied completely to the target network after <em class="italic">N</em> time steps, or it can be <a id="_idIndexMarker1198"/>a <strong class="keyWord">soft update</strong>, in which the target network slowly (by a factor of Tau <img alt="" height="46" src="../Images/B18331_11_018.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="117"/>) moves its weight toward the local network.</p>
<h2 class="heading-2" id="_idParaDest-322">Reinforcement success in recent years</h2>
<p class="normal">In the last few years, DRL has been successfully used in a variety of tasks, especially in game playing and<a id="_idIndexMarker1199"/> robotics. Let us acquaint ourselves with some success stories of RL before learning its algorithms:</p>
<ul>
<li class="bulletList"><strong class="keyWord">AlphaGo Zero</strong>: Developed by Google’s DeepMind team, the AlphaGo Zero paper <em class="italic">Mastering the game of Go without any human knowledge</em> starts from an absolutely blank slate (<strong class="keyWord">tabula rasa</strong>). The AlphaGo Zero uses one neural network to approximate both the move probabilities and value.</li>
<li class="bulletList">This neural network takes as an input the raw board representation. It uses a Monte Carlo tree <a id="_idIndexMarker1200"/>search guided by the neural network to select the moves. The reinforcement learning algorithm incorporates a look-ahead search inside the training loop. It was trained for 40 days using a 40-block residual CNN and, over the course of training, it played about 29 million games (a big number!). The neural network was optimized on Google Cloud using TensorFlow, with 64 GPU workers and 19 CPU parameter servers. You can access the paper here: <a href="https://www.nature.com/articles/nature24270"><span class="url">https://www.nature.com/articles/nature24270</span></a>.</li>
<li class="bulletList"><strong class="keyWord">AI-controlled sailplanes</strong>: Microsoft has developed a controller system that can run on many different autopilot hardware platforms, such as Pixhawk and Raspberry Pi 3. It can keep the sailplane in the air without using a motor, by autonomously finding and catching rides on naturally occurring thermals. The controller helps the sailplane to operate on its own by detecting and using these thermals to travel without the aid of a motor or a person. They implemented it as a partially observable Markov decision process. They employed Bayesian reinforcement learning and used the Monte Carlo tree search to search for the best action. They’ve divided the whole system into level planners—a high-level planner that makes a decision based on experience and a low-level planner that uses Bayesian reinforcement learning to detect and latch onto thermals in real time. You can see the sailplane in action at Microsoft News: <a href="https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/"><span class="url">https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/</span></a>.</li>
<li class="bulletList"><strong class="keyWord">Locomotion behavior</strong>: In the paper <em class="italic">Emergence of Locomotion Behaviours in Rich Environments</em> (<a href="https://arxiv.org/pdf/1707.02286.pdf"><span class="url">https://arxiv.org/pdf/1707.02286.pdf</span></a>), DeepMind researchers provided the agents with rich and diverse environments. The environments presented a spectrum of challenges at different levels of difficulty. The agent was provided with difficulties in increasing order; this led the agent to learn sophisticated locomotion skills without performing any reward engineering (that is, designing special reward functions).</li>
<li class="bulletList"><strong class="keyWord">Data center cooling using reinforcement learning</strong>: Data centers are workhorses of the present digital/internet revolution. With their large servers and networking devices, they facilitate data storage, data transfer, and the processing of information over the internet. Data centers account for about ~1.5% of all global energy consumption and if nothing is done about it, the consumption will only increase. DeepMind, along with Google Research in 2016, employed reinforcement learning models to reduce the energy consumption of their data centers by 40%. Using the historical data collected from the<a id="_idIndexMarker1201"/> sensors within the data center, they trained a deep neural network to predict future energy efficiency and propose optimal action. You can read the details of the models and approach in the paper <em class="italic">Data center cooling using model-predictive control</em> (<a href="https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf</span></a>).</li>
<li class="bulletList"><strong class="keyWord">Controlling nuclear fusion plasma</strong>: A recent (2022) and interesting application of RL is in controlling nuclear fusion plasma with the help of reinforcement learning. The results are published in a Nature paper: <em class="italic">Magnetic control of tokamak plasmas through reinforcement learning</em>.</li>
</ul>
<p class="normal">It is really amazing to see how the DRL agent, without any implicit knowledge, learns to perform, and even beat, humans – in many specialized tasks. In the coming sections, we will explore these fabulous DRL algorithms and see them play games with almost human efficiency within a few thousand epochs.</p>
<h1 class="heading-1" id="_idParaDest-323">Simulation environments for RL</h1>
<p class="normal">As mentioned earlier, <strong class="keyWord">trial and error</strong> is an<a id="_idIndexMarker1202"/> important component of any RL algorithm. Therefore, it makes sense to train our RL agent firstly in a simulated environment.</p>
<p class="normal">Today there exists a large number of platforms that can be used for the creation of an environment. Some popular ones are:</p>
<ul>
<li class="bulletList"><strong class="keyWord">OpenAI Gym</strong>: This contains a collection of <a id="_idIndexMarker1203"/>environments that <a id="_idIndexMarker1204"/>we can use to train our RL agents. In this chapter, we’ll be using the OpenAI Gym interface.</li>
<li class="bulletList"><strong class="keyWord">Unity ML-Agents SDK</strong>: It allows developers to<a id="_idIndexMarker1205"/> transform games and simulations created using the Unity editor into environments where intelligent agents can be trained using DRL, evolutionary strategies, or other machine learning methods through a simple-to-use Python API. It works with TensorFlow and provides the <a id="_idIndexMarker1206"/>ability to train intelligent agents for 2D/3D and VR/AR games. You can learn more about it here: <a href="https://github.com/Unity-Technologies/ml-agents"><span class="url">https://github.com/Unity-Technologies/ml-agents</span></a>.</li>
<li class="bulletList"><strong class="keyWord">Gazebo</strong>: In Gazebo, we can build three-dimensional worlds with physics-based simulation. The <code class="inlineCode">gym-gazebo</code> toolkit uses<a id="_idIndexMarker1207"/> Gazebo along <a id="_idIndexMarker1208"/>with the <strong class="keyWord">Robot Operating System</strong> (<strong class="keyWord">ROS</strong>) and the OpenAI Gym interface<a id="_idIndexMarker1209"/> and can be used to train RL agents. To find out more about this, you can refer to the white paper: <a href="https://arxiv.org/abs/1608.05742"><span class="url">https://arxiv.org/abs/1608.05742</span></a>.</li>
<li class="bulletList"><strong class="keyWord">Blender learning environment</strong>: This is a Python interface for the Blender game engine, and it also works with OpenAI Gym. It has at its base Blender: a free 3D modeling software with an integrated game engine. This provides an easy-to-use, powerful set of tools for <a id="_idIndexMarker1210"/>creating games. It provides an interface to the Blender game engine, and the games themselves are designed in Blender. We can then create a custom virtual environment to train an RL agent on a specific problem (<a href="https://github.com/LouisFoucard/gym-blender"><span class="url">https://github.com/LouisFoucard/gym-blender</span></a>).</li>
<li class="bulletList"><strong class="keyWord">Malmo</strong>: Built by the<a id="_idIndexMarker1211"/> Microsoft team, Malmo is a platform for AI experimentation and research built on top of Minecraft. It provides a <a id="_idIndexMarker1212"/>simple API for creating tasks and missions. You can learn more about Project Malmo here: <a href="https://www.microsoft.com/en-us/research/project/project-malmo/"><span class="url">https://www.microsoft.com/en-us/research/project/project-malmo/</span></a>.</li>
</ul>
<h1 class="heading-1" id="_idParaDest-324">An introduction to OpenAI Gym</h1>
<p class="normal">We will be using OpenAI Gym to provide an environment for our agent. OpenAI Gym is an open source toolkit to develop and<a id="_idIndexMarker1213"/> compare RL algorithms. It contains a variety of simulated environments that can be used to train agents and develop new RL algorithms.</p>
<p class="normal">The first thing to do is install OpenAI Gym. The following command will install the minimal <code class="inlineCode">gym</code> package:</p>
<pre class="programlisting con"><code class="hljs-con">pip install gym
</code></pre>
<p class="normal">If you want to install all (free) <code class="inlineCode">gym</code> modules, add <code class="inlineCode">[all]</code> after it:</p>
<pre class="programlisting con"><code class="hljs-con">pip install gym[all]
</code></pre>
<div class="note">
<p class="normal">The MuJoCo environment requires a purchasing license. For Atari-based games, you will need to install Atari dependencies (Box2D and ROM):</p>
<pre class="programlisting con"><code class="hljs-con">pip install box2d-py
</code></pre>
</div>
<p class="normal">OpenAI Gym provides a variety of environments, from simple text-based to three-dimensional games. The environments supported can be grouped as follows:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Algorithms</strong>: Contains environments that involve performing computations such as addition. While we can <a id="_idIndexMarker1214"/>easily perform the computations on a computer, what makes these problems interesting as RL problems is that the agent learns these tasks purely by example.</li>
<li class="bulletList"><strong class="keyWord">Atari</strong>: This environment provides a wide variety of classic Atari/arcade games.</li>
<li class="bulletList"><strong class="keyWord">Box2D</strong>: Contains robotics tasks in two dimensions such as a car racing agent or bipedal robot walk.</li>
<li class="bulletList"><strong class="keyWord">Classic control</strong>: This contains the classical control theory problems, such as balancing a cart pole.</li>
<li class="bulletList"><strong class="keyWord">MuJoCo</strong>: This is proprietary (you can get a one-month free trial). It supports various robot simulation tasks. The environment includes a physics engine; hence, it’s used for training robotic tasks.</li>
<li class="bulletList"><strong class="keyWord">Robotics</strong>: This environment also uses the physics engine of MuJoCo. It simulates goal-based tasks for fetch and shadow-hand robots.</li>
<li class="bulletList"><strong class="keyWord">Toy text</strong>: A simple text-based environment—very good for beginners.</li>
</ul>
<p class="normal">You can get a complete list of environments from the Gym website: <a href="https://gym.openai.com"><span class="url">https://gym.openai.com</span></a>. To find a list of all available environments in your installation, you can use the following code:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> gym <span class="hljs-keyword">import</span> envs
   
envall = envs.registry.<span class="hljs-built_in">all</span>()
<span class="hljs-built_in">len</span>(envall)
</code></pre>
<p class="normal">At the time of writing this book, it resulted in 859, that is, there are 859 different environments present in the <code class="inlineCode">gym</code> module. Let us see more details of these environments. Each environment is created<a id="_idIndexMarker1215"/> by using the <code class="inlineCode">make</code> function. Associated with each environment is a unique ID, its observation space, its action space, and a default reward range. Gym allows you to access them through dot notation, as shown in the following code. We go through all the environments in the <code class="inlineCode">envall</code> list and note down its unique ID, which is used to create the environment using the <code class="inlineCode">make</code> method, its observation space, reward range, and the action space:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tqdm <span class="hljs-keyword">import</span> tqdm
<span class="hljs-type">List</span> = []
<span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> tqdm(envall):
    <span class="hljs-keyword">try</span>:
        env = e.make()
        <span class="hljs-type">List</span>.append([e.<span class="hljs-built_in">id</span>, env.observation_space, env.action_space, env.reward_range])
        env.close() 
    <span class="hljs-keyword">except</span>:
        <span class="hljs-keyword">continue</span>  
</code></pre>
<p class="normal"><em class="italic">Figure 11.4</em> shows a random sample from the list:</p>
<figure class="mediaobject"><img alt="Table  Description automatically generated" height="849" src="../Images/B18331_11_04.png" width="826"/></figure>
<p class="packt_figref">Figure 11.4: Random list of environments available in OpenAI Gym</p>
<p class="normal">You can use these commands to <a id="_idIndexMarker1216"/>find out details about any environment in Gym. For example, the following code prints details of the MountainCar environment:</p>
<pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'MountainCar-v0'</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"The Observation space is        </span><span class="hljs-subst">{env.observation_space}</span><span class="hljs-string">"</span> )
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Upper Bound for Env Observation </span><span class="hljs-subst">{env.observation_space.high}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Lower Bound for Env Observation </span><span class="hljs-subst">{env.observation_space.low}</span><span class="hljs-string">"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"Action Space                    </span><span class="hljs-subst">{env.action_space}</span><span class="hljs-string">"</span>)
env.seed(<span class="hljs-number">0</span>)
obs = env.reset()
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"The initial observation is      </span><span class="hljs-subst">{obs}</span><span class="hljs-string">"</span>)
<span class="hljs-comment"># Take a random actionget the new observation space</span>
new_obs, reward, done, info = env.step(env.action_space.sample())
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"The new observation is          </span><span class="hljs-subst">{new_obs}</span><span class="hljs-string">"</span>)
env.close()
</code></pre>
<p class="normal">The core interface provided by <a id="_idIndexMarker1217"/>OpenAI Gym is the unified environment interface. The agent can interact with the environment using three basic methods, that is, <code class="inlineCode">reset</code>, <code class="inlineCode">step</code>, and <code class="inlineCode">render</code>. The <code class="inlineCode">reset</code> method resets the environment and returns the observation. The <code class="inlineCode">step</code> method steps the environment by one time step and returns <code class="inlineCode">new_obs</code>, <code class="inlineCode">reward</code>, <code class="inlineCode">done</code>, and <code class="inlineCode">info</code>. The <code class="inlineCode">render</code> method renders one frame of the environment, like popping a window. Let us try and view some different environments and view their initial frame:</p>
<table class="table-container" id="table001-5">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Physics Engine</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Classic Control</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Atari</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<pre class="programlisting code"><code class="hljs-code">e = <span class="hljs-string">'LunarLander-v2'</span>
env = gym.make(e)
obs = env.reset() 
img = env.render(mode=<span class="hljs-string">'rgb_array'</span>)
env.close()
plt.imshow(img)
</code></pre>
</td>
<td class="table-cell">
<pre class="programlisting code"><code class="hljs-code">e = <span class="hljs-string">'CartPole-v0'</span>
env = gym.make(e)
env.reset()
img = env.render(mode=<span class="hljs-string">'rgb_array'</span>)
env.close()
plt.imshow(img)
</code></pre>
</td>
<td class="table-cell">
<pre class="programlisting code"><code class="hljs-code">e = <span class="hljs-string">'SpaceInvaders-v0'</span>
env = gym.make(e)
env.reset()
img = env.render(mode=<span class="hljs-string">'rgb_array'</span>)
env.close()
plt.imshow(img)
</code></pre>
</td>
</tr>
<tr>
<td class="table-cell">
<figure class="mediaobject"><img alt="Icon  Description automatically generated" height="189" src="../Images/B18331_11_05.png" width="275"/></figure>
</td>
<td class="table-cell">
<figure class="mediaobject"><img alt="Chart, box and whisker chart  Description automatically generated" height="183" src="../Images/B18331_11_06.png" width="264"/></figure>
</td>
<td class="table-cell">
<figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" height="234" src="../Images/B18331_11_07.png" width="191"/></figure>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 11.1: Different environments of OpenAI Gym and their initial state</p>
<p class="normal">The preceding code uses <a id="_idIndexMarker1218"/>Matplotlib to display the environment; alternatively, you can directly use the <code class="inlineCode">render</code> method:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
env_name = <span class="hljs-string">'Breakout-v0'</span>
env = gym.make(env_name)
obs = env.reset()
env.render()
</code></pre>
<p class="normal">You can see the Breakout environment in <em class="italic">Figure 11.5</em>; the <code class="inlineCode">render</code> function pops up the environment window:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="340" src="../Images/B18331_11_08.png" width="237"/></figure>
<p class="packt_figref">Figure 11.5: Initial state of the Breakout environment</p>
<p class="normal">We can use <code class="inlineCode">env.observation_space</code> and <code class="inlineCode">env.action_space</code> to find out more about the state space and action space for the <a id="_idIndexMarker1219"/>Breakout game. The results show the state consists of a three-channel image of size 210 × 160, and the action space is discrete with four possible actions. Once you are done, do not forget to close OpenAI using:</p>
<pre class="programlisting code"><code class="hljs-code">env.close()
</code></pre>
<h2 class="heading-2" id="_idParaDest-325">Random agent playing Breakout</h2>
<p class="normal">Let’s have some fun and <a id="_idIndexMarker1220"/>play the Breakout game. When I first played the game, I had no idea of the rules or how to play, so I randomly chose the control buttons. Our novice agent will do the same; it will choose the actions randomly from the action space. Gym provides a function called <code class="inlineCode">sample()</code>, which chooses a random action from the action space – we will be using this function. Also, we can save a replay of the game, to view it later. There are two ways to save the play, one using Matplotlib and another using an OpenAI Gym Monitor wrapper. Let us first see the Matplotlib method.</p>
<p class="normal">We will first import the necessary modules; we will only need <code class="inlineCode">gym</code> and <code class="inlineCode">matplotlib</code> for now, as the agent will be playing random moves:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> matplotlib.animation <span class="hljs-keyword">as</span> animation
</code></pre>
<p class="normal">We create the Gym environment:</p>
<pre class="programlisting code"><code class="hljs-code">env_name = <span class="hljs-string">'Breakout-v0'</span>
env = gym.make(env_name)
</code></pre>
<p class="normal">Next, we will run the game, one step at a time, choosing a random action, either for 300 steps or until the game<a id="_idIndexMarker1221"/> is finished (whichever is earlier). The environment state (observation) space is saved at each step in the list <code class="inlineCode">frames</code>:</p>
<pre class="programlisting code"><code class="hljs-code">frames = [] <span class="hljs-comment"># array to store state space at each step</span>
env.reset()
done = <span class="hljs-literal">False</span>
<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">300</span>): 
    <span class="hljs-comment">#print(done)</span>
    frames.append(env.render(mode=<span class="hljs-string">'rgb_array'</span>))
    obs,reward,done, _ = env.step(env.action_space.sample())
    <span class="hljs-keyword">if</span> done:
        <span class="hljs-keyword">break</span>
</code></pre>
<p class="normal">Now comes the part of combining all the frames into a GIF image using Matplotlib Animation. We create an image object, patch, and then define a function that sets image data to a particular frame index. The function is used by the Matplotlib <code class="inlineCode">Animation</code> class to create an animation, which we finally save in the file <code class="inlineCode">random_agent.gif</code>:</p>
<pre class="programlisting code"><code class="hljs-code">patch = plt.imshow(frames[<span class="hljs-number">0</span>])
plt.axis(<span class="hljs-string">'off'</span>)
<span class="hljs-keyword">def</span> <span class="hljs-title">animate</span>(<span class="hljs-params">i</span>):
    patch.set_data(frames[i])
    anim = animation.FuncAnimation(plt.gcf(), animate, \
        frames=<span class="hljs-built_in">len</span>(frames), interval=<span class="hljs-number">10</span>)
    anim.save(<span class="hljs-string">'random_agent.gif'</span>, writer=<span class="hljs-string">'imagemagick'</span>)
</code></pre>
<p class="normal">The code above will generate a GIF image. Below are some screen grabs from the image:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="272" src="../Images/B18331_11_09.png" width="630"/> </figure>
<p class="packt_figref">Figure 11.6: Some screenshots from the saved GIF image </p>
<p class="normal">Now that we are familiar with<a id="_idIndexMarker1222"/> OpenAI Gym, we’ll move on to wrappers—which you can use to create your own custom environments.</p>
<h2 class="heading-2" id="_idParaDest-326">Wrappers in Gym</h2>
<p class="normal">Gym provides various <a id="_idIndexMarker1223"/>wrappers for us to modify the existing environment. For example, if you have image-based inputs with<a id="_idIndexMarker1224"/> the RGB intensity value lying between 0 and 255, but the RL agent you use is a neural network, which works best if the input is in the range 0-1, you can use the Gym wrapper class to preprocess the state space. Below we define a wrapper that concatenates observations:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque
<span class="hljs-keyword">from</span> gym <span class="hljs-keyword">import</span> spaces
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-comment">#Class to concat observations</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">ConcatObservations</span>(gym.Wrapper):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, env, n</span>):
        gym.Wrapper.__init__(self, env)
        shape = env.observation_space.shape
        self.n = n
        self.frames = deque([], maxlen=n)
        self.observation_space = \
            spaces.Box(low=<span class="hljs-number">0</span>, high=<span class="hljs-number">255</span>, shape=((n,) + shape), dtype=env.observation_space.dtype)
    <span class="hljs-keyword">def</span> <span class="hljs-title">reset</span>(<span class="hljs-params">self</span>):  <span class="hljs-comment">#reset function</span>
        obs = self.env.reset()
        <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(self.n):
            self.frames.append(obs)
        <span class="hljs-keyword">return</span> self._get_obs()
    <span class="hljs-keyword">def</span> <span class="hljs-title">step</span>(<span class="hljs-params">self, action</span>): <span class="hljs-comment">#step function</span>
        obs, reward, done, info = self.env.step(action)
        self.frames.append(obs)
        <span class="hljs-keyword">return</span> self._get_obs(), reward, done, info
    <span class="hljs-keyword">def</span> <span class="hljs-title">_get_obs</span>(<span class="hljs-params">self</span>):
        <span class="hljs-keyword">return</span> np.array(self.frames)
</code></pre>
<p class="normal">You can see that we need to change the default <code class="inlineCode">reset</code> function, <code class="inlineCode">step</code> function, and observation function <code class="inlineCode">_get_obs</code>. We also need to modify the default observation space.</p>
<p class="normal">Let us see how it<a id="_idIndexMarker1225"/> works. If you take the <code class="inlineCode">"BreakoutNoFrameskip-v4"</code> environment, then the initial observation space is 210 x 160 x 3: </p>
<pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"BreakoutNoFrameskip-v4"</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"The original observation space is  </span><span class="hljs-subst">{env.observation_space}</span><span class="hljs-string">"</span>)
</code></pre>
<pre class="programlisting gen"><code class="hljs">### OUTPUT:
</code></pre>
<pre class="programlisting con"><code class="hljs-con">&gt;&gt;&gt;The original observation space is  Box(0, 255, (210, 160, 3), uint8)
</code></pre>
<p class="normal">And now if you use the wrapper we just created:</p>
<pre class="programlisting code"><code class="hljs-code">env = ConcatObservations(env, <span class="hljs-number">4</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">f"The new observation space is  </span><span class="hljs-subst">{env.observation_space}</span><span class="hljs-string">"</span>)
</code></pre>
<pre class="programlisting gen"><code class="hljs">### OUTPUT:
</code></pre>
<pre class="programlisting con"><code class="hljs-con">The new observation space is  Box(0, 255, (4, 210, 160, 3), uint8)
</code></pre>
<p class="normal">You can see that now a dimension is added—it has four frames, with each frame of size 210 x 160 x 3. You can use a wrapper to modify the rewards as well. In this case, you use the superclass <code class="inlineCode">RewardWrapper</code>. Below is sample code that can clip the reward to lie within the range [-10, 10]:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">ClippedRewards</span>(gym.RewardWrapper):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, env</span>):
        gym.RewardWrapper.__init__(self, env)
        self.reward_range = (-<span class="hljs-number">10</span>,<span class="hljs-number">10</span>)
    <span class="hljs-keyword">def</span> <span class="hljs-title">reward</span>(<span class="hljs-params">self, reward</span>):
        <span class="hljs-string">"""Clip to {+10, 0, -10} by its sign."""</span>
        <span class="hljs-keyword">return</span> reward <span class="hljs-keyword">if</span> reward &gt;= -<span class="hljs-number">10</span> <span class="hljs-keyword">and</span> reward &lt;= <span class="hljs-number">10</span> <span class="hljs-keyword">else</span> <span class="hljs-number">10</span> * np.sign(reward)
</code></pre>
<p class="normal">Let us try using it in the CartPole environment, which has the reward range <img alt="" height="50" src="../Images/B18331_11_019.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="133"/>:</p>
<pre class="programlisting code"><code class="hljs-code">env = ClippedRewards(gym.make(<span class="hljs-string">"CartPole-v0"</span>))
<span class="hljs-built_in">print</span>(<span class="hljs-string">f'Clipped reward range: </span><span class="hljs-subst">{env.reward_range}</span><span class="hljs-string">'</span>)
env.close()
</code></pre>
<pre class="programlisting gen"><code class="hljs">### OUTPUT:
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Clipped reward range: (-10, 10)
</code></pre>
<p class="normal">Another useful application<a id="_idIndexMarker1226"/> of wrappers is when you want to save the state space as an agent is learning. Normally, an RL agent requires lots of steps for proper training, and as a result, it is not feasible to store the state space at each step. Instead, we can choose to store after every 500th step (or any other number you wish) in the preceding algorithm. OpenAI Gym provides the <code class="inlineCode">Wrapper Monitor</code> class to save the game as a video. To do so, we need to first import wrappers, then create the environment, and finally use <code class="inlineCode">Monitor</code>.</p>
<p class="normal">By default, it will store the video of 1, 8, 27, 64, (episode numbers with perfect cubes), and so on, and then every 1,000th episode; each training, by default, is saved in one folder. The code to do this is:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
env = gym.make(<span class="hljs-string">"Breakout-v0"</span>)
env = gym.wrappers.Monitor(env, <span class="hljs-string">'recording'</span>, force=<span class="hljs-literal">True</span>)
observation = env.reset()
<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>):
    <span class="hljs-comment">#env.render()</span>
    action = env.action_space.sample()
    <span class="hljs-comment"># your agent here (this takes random actions)</span>
    observation, reward, done, info = env.step(action)
    <span class="hljs-keyword">if</span> done:
        observation = env.reset()
env.close()
</code></pre>
<p class="normal">For <code class="inlineCode">Monitor</code> to work, we require FFmpeg support. We may need to install it depending upon our OS, if it is missing.</p>
<p class="normal">This will save the videos in <code class="inlineCode">.mp4</code> format in the folder <code class="inlineCode">recording</code>. An important thing to note here is that you have to set the <code class="inlineCode">force=True</code> option if you want to use the same folder for the next<a id="_idIndexMarker1227"/> training session.</p>
<div class="note">
<p class="normal">If you want to train your agent on Google Colab, you will need to add the following drivers to be able to visualize the Gym output:</p>
<pre class="programlisting code"><code class="hljs-code">!pip install pyglet
!apt-get install -y xvfb python-opengl &gt; /dev/null <span class="hljs-number">2</span>&gt;&amp;<span class="hljs-number">1</span>
!pip install gym pyvirtualdisplay &gt; /dev/null <span class="hljs-number">2</span>&gt;&amp;<span class="hljs-number">1</span>
</code></pre>
<p class="normal">After installing the Python virtual display, you need to start it—Gym uses the virtual display to set observations. The following code can help you in starting a display of size 600 x 400:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> pyvirtualdisplay <span class="hljs-keyword">import</span> Display
display = Display(visible=<span class="hljs-number">0</span>, size=(<span class="hljs-number">600</span>, <span class="hljs-number">400</span>))
display.start()
</code></pre>
<p class="normal">And to be able to play around with Atari games, use:</p>
<pre class="programlisting code"><code class="hljs-code">!wget http://www.atarimania.com/roms/Roms.rar
!mkdir /content/ROM/
!unrar e /content/Roms.rar /content/ROM/
!python -m atari_py.import_roms /content/ROM/
</code></pre>
</div>
<h1 class="heading-1" id="_idParaDest-327">Deep Q-networks</h1>
<p class="normal"><strong class="keyWord">Deep Q-Networks</strong>, <strong class="keyWord">DQNs</strong> for short, are<a id="_idIndexMarker1228"/> deep learning neural networks designed to approximate the Q-function (value-state function). They are one of the most popular value-based reinforcement learning algorithms. The model was proposed by Google’s DeepMind in NeurIPS 2013, in the paper entitled <em class="italic">Playing Atari with Deep Reinforcement Learning</em>. The most important contribution of this paper was that they used the raw state space directly as input to the network; the input features were not hand-crafted as done in earlier RL implementations. Also, they could train the agent with exactly the same architecture to play different Atari games and obtain state-of-the-art results.</p>
<p class="normal">This model is an extension of the simple Q-learning algorithm. In Q-learning algorithms, a Q-table is maintained as a cheat sheet. After each action, the Q-table is updated using the Bellman equation [5]:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_11_020.png" style="height: 1.35em !important; vertical-align: 0.04em !important;" width="1008"/></p>
<p class="normal"><img alt="" height="42" src="../Images/B18331_11_021.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/> is the learning rate, and its<a id="_idIndexMarker1229"/> value lies in the range [0,1]. The first term represents the component of the old <em class="italic">Q</em> value and the second term the target <em class="italic">Q</em> value. Q-learning is good if the number of states and the number of possible actions is small, but for large state spaces and action spaces, Q-learning is simply not scalable. A better alternative would be to use a deep neural network as a function approximator, approximating the target Q-function for each possible action. The weights of the deep neural network in this case store the Q-table information. There is a separate output unit for each possible action. The network takes the state as its input and returns the predicted target <em class="italic">Q</em> value for all possible actions. The question arises: how do we train this network, and what should be the loss function? Well, since our network has to predict the target <em class="italic">Q</em> value:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_11_022.png" style="height: 1.35em !important;" width="592"/></p>
<p class="normal">the loss function should try and reduce the difference between the <em class="italic">Q</em> value predicted, <em class="italic">Q</em><sub class="italic">predicted</sub>, and the target <em class="italic">Q</em>, <em class="italic">Q</em><sub class="italic">target</sub>. We can do this by defining the loss function as:</p>
<p class="center"><img alt="" height="58" src="../Images/B18331_11_023.png" style="height: 1.45em !important;" width="758"/></p>
<p class="normal">where <em class="italic">W</em> is the training parameters of our deep <em class="italic">Q</em> network, learned using gradient descent, such that the loss function is minimized.</p>
<p class="normal">The following is the general architecture of a DQN. The network takes the <em class="italic">n</em>-dimensional state as input and outputs the <em class="italic">Q</em> value of each possible action in the <em class="italic">m</em>-dimensional action space. Each layer (including the input) can be a convolutional layer (if we are taking the raw pixels as input, convolutional layers make more sense) or a dense layer:</p>
<figure class="mediaobject"><img alt="A picture containing icon  Description automatically generated" height="3115" src="../Images/B18331_11_10.png" style="width:20em;" width="2500"/></figure>
<p class="packt_figref">Figure 11.7: The figure shows a simple DQN network, the input layer taking State vector S, and the output predicting Q for all possible actions for the state</p>
<p class="normal">In the next section, we will try<a id="_idIndexMarker1230"/> training a DQN. Our agent task will be to stabilize a pole on a cart. The agent can move the cart left or right to maintain balance.</p>
<h2 class="heading-2" id="_idParaDest-328">DQN for CartPole</h2>
<p class="normal">CartPole is a classic OpenAI problem<a id="_idIndexMarker1231"/> with continuous state space and discrete action space. In it, a pole is attached by an un-actuated joint to a cart; the cart moves along a frictionless track. The goal is to keep the pole standing on the cart by moving the cart left or right. A reward of +1 is given for each time step the pole is standing. Once the pole is more than 15 degrees from the vertical, or the cart moves beyond 2.4 units from the center, the game is over:</p>
<figure class="mediaobject"><img alt="Box and whisker chart  Description automatically generated" height="357" src="../Images/B18331_11_11.png" width="536"/></figure>
<p class="packt_figref">Figure 11.8: A screenshot from the CartPole Gym environment</p>
<p class="normal">You can check the leaderboard of OpenAI Gym for some cool entries for the CartPole environment: <a href="https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0"><span class="url">https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0</span></a>.</p>
<p class="normal">We start with importing the <a id="_idIndexMarker1232"/>necessary modules. We require <code class="inlineCode">gym</code>, obviously, to provide us with the CartPole environment, and <code class="inlineCode">tensorflow</code> to build our DQN network. Besides these, we need the <code class="inlineCode">random</code> and <code class="inlineCode">numpy</code> modules:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> random
<span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> math
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> deque
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense
<span class="hljs-keyword">from</span> tensorflow.keras.optimizers <span class="hljs-keyword">import</span> Adam
</code></pre>
<p class="normal">We set up the global values for the maximum episodes for which we will be training the agent (<code class="inlineCode">EPOCHS</code>), the threshold value when we consider the environment solved (<code class="inlineCode">THRESHOLD</code>), and a bool to indicate if we want to record the training or not (<code class="inlineCode">MONITOR</code>). Please note that as per the official OpenAI documentation, the CartPole environment is considered solved when the agent is able to maintain the pole in the vertical position for 195 time steps (ticks). In the following code, for the sake of time, we have reduced the <code class="inlineCode">THRESHOLD</code> to 45:</p>
<pre class="programlisting code"><code class="hljs-code">EPOCHS = <span class="hljs-number">1000</span>
THRESHOLD = <span class="hljs-number">45</span>
MONITOR = <span class="hljs-literal">True</span>
</code></pre>
<p class="normal">Now let us build our DQN. We declare a class <code class="inlineCode">DQN</code> and in its <code class="inlineCode">__init__()</code> function declare all the hyperparameters and our model. We are also creating the environment inside the <code class="inlineCode">DQN</code> class. As you can <a id="_idIndexMarker1233"/>see, the class is quite general, and you can use it to train any Gym environment whose state space information can be encompassed in a 1D array:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">DQN</span>():
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, env_string, batch_size=</span><span class="hljs-number">64</span>):
        self.memory = deque(maxlen=<span class="hljs-number">100000</span>)
        self.env = gym.make(env_string)
        input_size = self.env.observation_space.shape[<span class="hljs-number">0</span>]
        action_size = self.env.action_space.n
        self.batch_size = batch_size
        self.gamma = <span class="hljs-number">1.0</span>
        self.epsilon = <span class="hljs-number">1.0</span>
        self.epsilon_min = <span class="hljs-number">0.01</span>
        self.epsilon_decay = <span class="hljs-number">0.995</span>
        
        alpha=<span class="hljs-number">0.01</span>
        alpha_decay=<span class="hljs-number">0.01</span>
        <span class="hljs-keyword">if</span> MONITOR: self.env = gym.wrappers.Monitor(self.env,
<span class="hljs-keyword">        </span><span class="hljs-string">'data/'</span>+env_string, force=<span class="hljs-literal">True</span>)
        
        <span class="hljs-comment"># Init model</span>
        self.model = Sequential()
        self.model.add(Dense(<span class="hljs-number">24</span>, input_dim=input_size,
        activation=<span class="hljs-string">'tanh'</span>))
        self.model.add(Dense(<span class="hljs-number">48</span>, activation=<span class="hljs-string">'tanh'</span>))
        self.model.add(Dense(action_size, activation=<span class="hljs-string">'linear'</span>))
        self.model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'mse'</span>, optimizer=Adam(lr=alpha,
        decay=alpha_decay))
</code></pre>
<p class="normal">The DQN that we have built is a three-layered perceptron; in the following output, you can see the model summary. We use the Adam optimizer with learning rate decay:</p>
<pre class="programlisting con"><code class="hljs-con">Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 24)                120       
                                                                 
 dense_1 (Dense)             (None, 48)                1200      
                                                                 
 dense_2 (Dense)             (None, 2)                 98        
                                                                 
=================================================================
Total params: 1,418
Trainable params: 1,418
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p class="normal">The variable list <code class="inlineCode">self.memory</code> will contain our experience replay buffer. We need to add a method for saving the <em class="italic">&lt;S,A,R,S’&gt; </em>tuple into<a id="_idIndexMarker1234"/> the memory and a method to get random samples from it in batches to train the agent. We perform these two functions by defining the class methods <code class="inlineCode">remember</code> and <code class="inlineCode">replay</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">remember</span>(<span class="hljs-params">self, state, action, reward, next_state, done</span>):
        self.memory.append((state, action, reward, next_state, done))
<span class="hljs-keyword">def</span> <span class="hljs-title">replay</span>(<span class="hljs-params">self, batch_size</span>):
        x_batch, y_batch = [], []
        minibatch = random.sample(self.memory, <span class="hljs-built_in">min</span>(<span class="hljs-built_in">len</span>(self.memory),
        batch_size))
        <span class="hljs-keyword">for</span> state, action, reward, next_state, done <span class="hljs-keyword">in</span> minibatch:
             y_target = self.model.predict(state)
             y_target[<span class="hljs-number">0</span>][action] = reward <span class="hljs-keyword">if</span> done <span class="hljs-keyword">else</span> reward + self.gamma * np.<span class="hljs-built_in">max</span>(self.model.predict(next_state)[<span class="hljs-number">0</span>])
             x_batch.append(state[<span class="hljs-number">0</span>])
             y_batch.append(y_target[<span class="hljs-number">0</span>])
        
        self.model.fit(np.array(x_batch), np.array(y_batch),
        batch_size=<span class="hljs-built_in">len</span>(x_batch), verbose=<span class="hljs-number">0</span>)
</code></pre>
<p class="normal">Our agent will use the <strong class="keyWord">epsilon-greedy policy</strong> when choosing the action. This is implemented in the following method:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">choose_action</span>(<span class="hljs-params">self, state, epsilon</span>):
        <span class="hljs-keyword">if</span> np.random.random() &lt;= epsilon:
            <span class="hljs-keyword">return</span> self.env.action_space.sample()
        <span class="hljs-keyword">else</span>:
            <span class="hljs-keyword">return</span> np.argmax(self.model.predict(state))
</code></pre>
<p class="normal">Next, we write a method to train the agent. We define two lists to keep track of the scores. First, we fill the experience replay buffer and then we choose some samples from it to train the agent <a id="_idIndexMarker1235"/>and hope that the agent will slowly learn to do better:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self</span>):
    scores = deque(maxlen=<span class="hljs-number">100</span>)
    avg_scores = []
    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):
        state = self.env.reset()
        state = self.preprocess_state(state)
        done = <span class="hljs-literal">False</span>
        i = <span class="hljs-number">0</span>
        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
            action = self.choose_action(state,self.epsilon)
            next_state, reward, done, _ = self.env.step(action)
            next_state = self.preprocess_state(next_state)
            self.remember(state, action, reward, next_state, done)
            state = next_state
            self.epsilon = <span class="hljs-built_in">max</span>(self.epsilon_min,
            self.epsilon_decay*self.epsilon) <span class="hljs-comment"># decrease epsilon</span>
            i += <span class="hljs-number">1</span>
        scores.append(i)
        mean_score = np.mean(scores)
        avg_scores.append(mean_score)
        <span class="hljs-keyword">if</span> mean_score &gt;= THRESHOLD <span class="hljs-keyword">and</span> e &gt;= <span class="hljs-number">100</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">'Ran {} episodes. Solved after {} trials </span><span class="hljs-string">✔</span><span class="hljs-string">'</span>.<span class="hljs-built_in">format</span>(e, e - <span class="hljs-number">100</span>))
            <span class="hljs-keyword">return</span> avg_scores
        <span class="hljs-keyword">if</span> e % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">'[Episode {}] - Mean survival time over last 100 episodes was {} ticks.'</span>.<span class="hljs-built_in">format</span>(e, mean_score))
    self.replay(self.batch_size)
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'Did not solve after {} episodes :('</span>.<span class="hljs-built_in">format</span>(e))
    <span class="hljs-keyword">return</span> avg_scores
</code></pre>
<p class="normal">Now that all necessary functions are done, we just need one more helper function to reshape the state of the CartPole environment so that the input to the model is in the correct shape. The state of the environment is described by four continuous variables: cart position ([-2.4-2.4]), cart velocity, pole angle ([-41.8o-41.8o]), and pole velocity :</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_state</span>(<span class="hljs-params">self, state</span>):
    <span class="hljs-keyword">return</span> np.reshape(state, [<span class="hljs-number">1</span>, self.input_size])
</code></pre>
<p class="normal">Let us now instantiate our <a id="_idIndexMarker1236"/>agent for the CartPole environment and train it:</p>
<pre class="programlisting code"><code class="hljs-code">env_string = <span class="hljs-string">'CartPole-v0'</span>
agent = DQN(env_string)
scores = agent.train()
</code></pre>
<pre class="programlisting con"><code class="hljs-con">[Episode 0] - Mean survival time over last 100 episodes was 28.0 ticks.
[Episode 100] - Mean survival time over last 100 episodes was 15.71 ticks.
[Episode 200] - Mean survival time over last 100 episodes was 27.81 ticks.
Ran 259 episodes. Solved after 159 trials ✔
</code></pre>
<p class="normal">Let’s plot the average reward as the agent learns:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
plt.plot(scores)
plt.show()
</code></pre>
<p class="normal"><em class="italic">Figure 11.9</em> shows the agent being trained on my system. The agent was able to achieve our set threshold of 45 in 254 steps:</p>
<figure class="mediaobject"><code class="inlineCode"><img alt="Chart  Description automatically generated" height="420" src="../Images/B18331_11_12.png" width="621"/></code></figure>
<p class="packt_figref">Figure 11.9: Average agent reward plot</p>
<p class="normal">Once the training is done, you can close the environment:</p>
<pre class="programlisting code"><code class="hljs-code">agent.env.close()
</code></pre>
<p class="normal">You can see that starting <a id="_idIndexMarker1237"/>with no information about how to balance the pole, the agent, using a DQN, is able to balance the pole for more and more time (on average) as it learns. Starting from a blank slate, the agent is able to build information/knowledge to fulfill the required goal. Remarkable!</p>
<h2 class="heading-2" id="_idParaDest-329">DQN to play a game of Atari</h2>
<p class="normal">In the preceding section, we <a id="_idIndexMarker1238"/>trained a DQN to balance a pole in CartPole. It was a simple problem, and thus we could solve it using a perceptron model. But imagine if the environment state was just the CartPole visual as we humans see it. With raw pixel values as the input state space, our previous DQN will not work. What we need is a convolutional neural network. Next, we build one based on the seminal paper on DQNs, <em class="italic">Playing Atari with Deep Reinforcement Learning</em>.</p>
<p class="normal">Most of the code will be similar to the DQN for CartPole, but there will be significant changes in the DQN network itself, and how we preprocess the state that we obtain from the environment.</p>
<p class="normal">First, let us see the change in the way state space is processed. <em class="italic">Figure 11.10</em> shows one of the Atari games, Breakout:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="388" src="../Images/B18331_11_13.png" width="283"/></figure>
<p class="packt_figref">Figure 11.10: A screenshot of the Atari game, Breakout</p>
<p class="normal">Now, if you look at the image, not all of it contains relevant information: the top part has redundant information about the score, the bottom part has unnecessary blank space, and the image is <a id="_idIndexMarker1239"/>colored. To reduce the burden on our model, it is best to remove the unnecessary information, so we crop the image, convert it to grayscale, and make it a square of size 84 × 84 (as in the paper). Here is the code to preprocess the input raw pixels:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">preprocess_state</span>(<span class="hljs-params">self, img</span>):
    img_temp = img[<span class="hljs-number">31</span>:<span class="hljs-number">195</span>]  <span class="hljs-comment"># Choose the important area of the image</span>
    img_temp = tf.image.rgb_to_grayscale(img_temp)
    img_temp = tf.image.resize(img_temp, [self.IM_SIZE, self.IM_SIZE],
    method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)
    img_temp = tf.cast(img_temp, tf.float32)
    <span class="hljs-keyword">return</span> img_temp[:,:,<span class="hljs-number">0</span>]
</code></pre>
<p class="normal">Another important issue is that just by looking at the image at one time step, how can the agent know whether the ball is going up or down? One way could be to use LSTM along with a CNN to keep a record of the past and hence the ball movement. The paper, however, used a simple technique. Instead of a single state frame, it concatenated the state space for the past four<a id="_idIndexMarker1240"/> time steps together as one input to the CNN; that is, the network sees four past frames of the environment as its input. The following is the code for combining the present and previous states:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">combine_images</span>(<span class="hljs-params">self, img1, img2</span>):
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(img1.shape) == <span class="hljs-number">3</span> <span class="hljs-keyword">and</span> img1.shape[<span class="hljs-number">0</span>] == self.m:
        im = np.append(img1[<span class="hljs-number">1</span>:,:, :],np.expand_dims(img2,<span class="hljs-number">0</span>), axis=<span class="hljs-number">2</span>)
        <span class="hljs-keyword">return</span> tf.expand_dims(im, <span class="hljs-number">0</span>)
    <span class="hljs-keyword">else</span>:
        im = np.stack([img1]*self.m, axis = <span class="hljs-number">2</span>)
        <span class="hljs-keyword">return</span> tf.expand_dims(im, <span class="hljs-number">0</span>)
</code></pre>
<p class="normal">The model was defined in the <code class="inlineCode">__init__</code> function. We modify the function to now have a CNN with an input of (84 × 84 × 4) representing four state frames each of size 84 × 84:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, env_string,batch_size=</span><span class="hljs-number">64</span><span class="hljs-params">, IM_SIZE = </span><span class="hljs-number">84</span><span class="hljs-params">, m = </span><span class="hljs-number">4</span>):
    self.memory = deque(maxlen=<span class="hljs-number">5000</span>)
    self.env = gym.make(env_string)
    input_size = self.env.observation_space.shape[<span class="hljs-number">0</span>]
    action_size = self.env.action_space.n
    self.batch_size = batch_size
    self.gamma = <span class="hljs-number">1.0</span>
    self.epsilon = <span class="hljs-number">1.0</span>
    self.epsilon_min = <span class="hljs-number">0.01</span>
    self.epsilon_decay = <span class="hljs-number">0.995</span>
    self.IM_SIZE = IM_SIZE
    self.m = m
    
    
    alpha=<span class="hljs-number">0.01</span>
    alpha_decay=<span class="hljs-number">0.01</span>
    <span class="hljs-keyword">if</span> MONITOR: self.env = gym.wrappers.Monitor(self.env, <span class="hljs-string">'../data/'</span>+env_string, force=<span class="hljs-literal">True</span>)
    
    <span class="hljs-comment"># Init model</span>
    self.model = Sequential()
    self.model.add( Conv2D(<span class="hljs-number">32</span>, <span class="hljs-number">8</span>, (<span class="hljs-number">4</span>,<span class="hljs-number">4</span>), activation=<span class="hljs-string">'relu'</span>,padding=<span class="hljs-string">'valid'</span>, input_shape=(IM_SIZE, IM_SIZE, m)))
    self.model.add( Conv2D(<span class="hljs-number">64</span>, <span class="hljs-number">4</span>, (<span class="hljs-number">2</span>,<span class="hljs-number">2</span>), activation=<span class="hljs-string">'relu'</span>,padding=<span class="hljs-string">'</span><span class="hljs-string">valid'</span>))
    self.model.add( Conv2D(<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, (<span class="hljs-number">1</span>,<span class="hljs-number">1</span>), activation=<span class="hljs-string">'relu'</span>,padding=<span class="hljs-string">'valid'</span>))
    self.model.add(Flatten())
    self.model.add(Dense(<span class="hljs-number">512</span>, activation=<span class="hljs-string">'elu'</span>))
    self.model.add(Dense(action_size, activation=<span class="hljs-string">'linear'</span>))
    self.model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'mse'</span>, optimizer=Adam(lr=alpha, decay=alpha_decay))
</code></pre>
<p class="normal">Lastly, we will need to <a id="_idIndexMarker1241"/>make a minor change in the <code class="inlineCode">train</code> function. We will need to call the new <code class="inlineCode">preprocess</code> function, along with the <code class="inlineCode">combine_images</code> function to ensure that four frames are concatenated:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">self</span>):
    scores = deque(maxlen=<span class="hljs-number">100</span>)
    avg_scores = []
    
    <span class="hljs-keyword">for</span> e <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(EPOCHS):
        state = self.env.reset()
        state = self.preprocess_state(state)
        state = self.combine_images(state, state)
        done = <span class="hljs-literal">False</span>
        i = <span class="hljs-number">0</span>
        <span class="hljs-keyword">while</span> <span class="hljs-keyword">not</span> done:
            action = self.choose_action(state,self.epsilon)
            next_state, reward, done, _ = self.env.step(action)
            next_state = self.preprocess_state(next_state)
            next_state = self.combine_images(next_state, state)
            <span class="hljs-comment">#print(next_state.shape)</span>
            self.remember(state, action, reward, next_state, done)
            state = next_state
            self.epsilon = <span class="hljs-built_in">max</span>(self.epsilon_min, self.epsilon_decay*self.epsilon) <span class="hljs-comment"># decrease epsilon</span>
            i += reward
        scores.append(i)
        mean_score = np.mean(scores)
        avg_scores.append(mean_score)
        <span class="hljs-keyword">if</span> mean_score &gt;= THRESHOLD <span class="hljs-keyword">and</span> e &gt;= <span class="hljs-number">100</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">'Ran {} episodes. Solved after {} trials </span><span class="hljs-string">✔</span><span class="hljs-string">'</span>.<span class="hljs-built_in">format</span>(e, e - <span class="hljs-number">100</span>))
            <span class="hljs-keyword">return</span> avg_scores
        <span class="hljs-keyword">if</span> e % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">'[Episode {}] - Score over last 100 episodes was {}.'</span>.<span class="hljs-built_in">format</span>(e, mean_score))
        self.replay(self.batch_size)
    
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'Did not solve after {} episodes :('</span>.<span class="hljs-built_in">format</span>(e))
    <span class="hljs-keyword">return</span> avg_scores
</code></pre>
<p class="normal">That’s all. We can now train<a id="_idIndexMarker1242"/> the agent for playing Breakout. The complete code is available on GitHub repository (<a href="https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition/tree/main/Chapter_11"><span class="url">https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition/tree/main/Chapter_11</span></a>) of this chapter in the file <code class="inlineCode">DQN_Atari_v2.ipynb</code>.</p>
<h2 class="heading-2" id="_idParaDest-330">DQN variants</h2>
<p class="normal">After the unprecedented success of DQNs, the<a id="_idIndexMarker1243"/> interest in RL increased and many new RL algorithms came into being. Next, we see some of the algorithms that are based on DQNs. They all use DQNs as the base and build upon it.</p>
<h3 class="heading-3" id="_idParaDest-331">Double DQN</h3>
<p class="normal">In DQNs, the agent uses the same <em class="italic">Q</em> value to<a id="_idIndexMarker1244"/> both select and evaluate an action. This can cause a maximization bias in learning. For example, let us consider that for a state, <em class="italic">S</em>, all possible actions have true <em class="italic">Q</em> values of zero. Now, our DQN estimates will have<a id="_idIndexMarker1245"/> some values above and some values below zero, and since we are choosing the action with the maximum <em class="italic">Q</em> value and later evaluating the <em class="italic">Q</em> value of each action using the same (maximized) estimated value function, we are overestimating <em class="italic">Q</em>—or in other words, our agent is over-optimistic. This can lead to unstable training and a low-quality policy. To deal with this issue, Hasselt et al. from DeepMind proposed the Double DQN algorithm in their paper <em class="italic">Deep Reinforcement Learning with Double Q-Learning</em>. In Double DQN, we have two Q-networks with the same architecture but different weights. One of the Q-networks is used to determine the action using the epsilon-greedy policy and the other is used to determine its value (Q-target).</p>
<p class="normal">If you recall in DQNs, the Q-target was given by:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_11_024.png" style="height: 1.35em !important;" width="583"/></p>
<p class="normal">Here, the action <em class="italic">A</em> was selected using the same DQN, <em class="italic">Q(S,A; W)</em>, where <em class="italic">W</em> is the training parameters of the<a id="_idIndexMarker1246"/> network; that is, we are writing the <em class="italic">Q</em> value function along with its training parameter to emphasize the difference between vanilla DQNs and Double DQN:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_11_025.png" style="height: 1.35em !important;" width="938"/></p>
<p class="normal">In Double DQN, the equation for the target will now change. Now, the DQN <em class="italic">Q(S,A;W)</em> is used for determining the action <a id="_idIndexMarker1247"/>and the DQN <em class="italic">Q(S,A;W’)</em> is used for calculating the target (notice the different weights). So, the preceding equation will change to:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_11_026.png" style="height: 1.35em !important;" width="946"/></p>
<p class="normal">This simple change reduces the overestimation and helps us to train the agent faster and more reliably.</p>
<h3 class="heading-3" id="_idParaDest-332">Dueling DQN</h3>
<p class="normal">This architecture was proposed by Wang et al. in their paper <em class="italic">Dueling Network Architectures for Deep Reinforcement Learning</em> in 2015. Like the DQN and Double DQN, it is also a model-free algorithm.</p>
<p class="normal">Dueling DQN decouples the<a id="_idIndexMarker1248"/> Q-function into the value function and advantage function. The value function, which we discussed <a id="_idIndexMarker1249"/>earlier, represents the value of the state independent of any action. The advantage function, on the other hand, provides a relative measure of the utility (advantage/goodness) of action <em class="italic">A</em> in the state <em class="italic">S</em>. The Dueling DQN uses convolutional networks in the initial layers to extract the features from raw pixels. However, in the later stages, it is separated into two different networks, one approximating the value and another approximating the advantage. This ensures that the network produces separate estimates for the value function and the advantage function:</p>
<p class="center"><img alt="" height="46" src="../Images/B18331_11_027.png" style="height: 1.15em !important; vertical-align: 0.10em !important;" width="600"/></p>
<p class="normal">Here, <img alt="" height="42" src="../Images/B18331_10_024.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> is an array of the training parameters of the shared convolutional network (it is shared by both <em class="italic">V</em> and <em class="italic">A</em>), and <img alt="" height="42" src="../Images/B18331_11_021.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/> and <img alt="" height="46" src="../Images/B18331_11_030.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="25"/> are the training parameters for the <em class="italic">Advantage</em> and <em class="italic">Value</em> estimator networks. Later, the<a id="_idIndexMarker1250"/> two networks are recombined using an aggregating layer to estimate the <em class="italic">Q</em> value.</p>
<p class="normal">In <em class="italic">Figure 11.11</em>, you can <a id="_idIndexMarker1251"/>see the architecture of Dueling DQN:</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="326" src="../Images/B18331_11_14.png" width="683"/></figure>
<p class="packt_figref">Figure 11.11: Visualizing the architecture of a Dueling DQN</p>
<p class="normal">You may be wondering, what is the advantage of doing all of this? Why decompose <em class="italic">Q</em> if we will just be putting it back together? Well, decoupling the value and advantage functions allows us to know which states are valuable, without having to take into account the effect of each action for each state. There are many states that, irrespective of the action taken, are good or bad states: for example, having breakfast with your loved ones in a good resort is always a good state, and being admitted to a hospital emergency ward is always a bad state. Thus, separating value and advantage allows one to get a more robust approximation of the value function. Next, you can see a figure from the paper highlighting how in the Atari game Enduro, the value network learns to pay attention to the road, and the advantage network learns to pay attention only when there are cars immediately in front, so as to avoid a collision:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="399" src="../Images/B18331_11_15.png" width="293"/></figure>
<p class="packt_figref">Figure 11.12: In the Atari game Enduro, the value network learns to pay attention to the road (red spot), and the advantage network focuses only when other vehicles are immediately in front. Image source: https://arxiv.org/pdf/1511.06581.pdf</p>
<p class="normal">The aggregate layer is implemented in a manner that allows one to recover both <em class="italic">V</em> and <em class="italic">A</em> from the given <em class="italic">Q</em>. This is <a id="_idIndexMarker1252"/>achieved by enforcing that the advantage function<a id="_idIndexMarker1253"/> estimator has zero advantage at the chosen action:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_11_031.png" style="height: 1.35em !important;" width="1138"/></p>
<p class="normal">In the paper, Wang et al. reported that the network is more stable if the max operation is replaced by the average operation. This is so because the speed of change in advantage is now the same as the change in average, instead of the optimal (max) value.</p>
<h3 class="heading-3" id="_idParaDest-333">Rainbow</h3>
<p class="normal">Rainbow is the<a id="_idIndexMarker1254"/> current state-of-the-art DQN variant. Technically, to call it a DQN variant would <a id="_idIndexMarker1255"/>be wrong. In essence, it is an ensemble of many DQN variants combined together into a single algorithm. It modifies the distributional RL [6] loss to multi-step loss and combines it with Double DQN using a greedy action. Quoting from the paper:</p>
<blockquote class="packt_quote">
<p class="quote">The network architecture is a dueling network architecture adapted for use with return distributions. The network has a shared representation <img alt="" height="50" src="../Images/B18331_11_032.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="92"/>, which is then fed into a value stream <img alt="" height="54" src="../Images/B18331_11_033.png" style="height: 1.35em !important; vertical-align: -0.43em !important;" width="38"/> with <em class="italic">N</em><sub class="subscript">atoms</sub> outputs, and into an advantage stream <img alt="" height="46" src="../Images/B18331_11_034.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="42"/> with <em class="italic">N</em><sub class="subscript">atoms</sub>×<em class="italic">N</em><sub class="subscript">actions</sub> outputs, where a1ξ(fξ(s), a)will denote the output corresponding to atom i and action a. For each atom <em class="italic">z</em><sub class="subscript">i</sub>, the value and advantage streams are aggregated, as in Dueling DQN, and then passed through a softmax layer to obtain the normalised parametric distributions used to estimate the returns’ distributions.</p>
</blockquote>
<p class="normal">Rainbow combines six different RL algorithms:</p>
<ul>
<li class="bulletList">N-step returns</li>
<li class="bulletList">Distributional state-action value learning</li>
<li class="bulletList">Dueling networks</li>
<li class="bulletList">Noisy networks</li>
<li class="bulletList">Double DQN</li>
<li class="bulletList">Prioritized experience replay</li>
</ul>
<p class="normal">Till now, we’ve<a id="_idIndexMarker1256"/> considered value-based reinforcement learning algorithms. In the next section, we will learn about<a id="_idIndexMarker1257"/> policy-based reinforcement learning algorithms. </p>
<h1 class="heading-1" id="_idParaDest-334">Deep deterministic policy gradient</h1>
<p class="normal">The DQN and its variants have been very successful in solving problems where the state space is continuous and action space is <a id="_idIndexMarker1258"/>discrete. For example, in Atari games, the input space consists of raw pixels, but actions are discrete—[<strong class="keyWord">up</strong>, <strong class="keyWord">down</strong>, <strong class="keyWord">left</strong>, <strong class="keyWord">right</strong>, <strong class="keyWord">no-op</strong>]. How do we solve a problem with continuous action space? For instance, say an RL agent driving a car needs to turn its wheels: this action has a continuous action space. </p>
<p class="normal">One way to handle this situation is by discretizing the action space and continuing with a DQN or its variants. However, a better solution would be to use a policy gradient algorithm. In policy gradient methods, the policy <img alt="" height="46" src="../Images/B18331_11_035.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="117"/> is approximated directly.</p>
<p class="normal">A neural network is used to approximate the policy; in the simplest form, the neural network learns a policy for selecting actions that maximize the rewards by adjusting its weights using the steepest gradient ascent, hence the name: policy gradients.</p>
<p class="normal">In this section, we will focus on the <strong class="keyWord">Deep Deterministic Policy Gradient</strong> (<strong class="keyWord">DDPG</strong>) algorithm, another successful RL algorithm by Google’s DeepMind in 2015. DDPG is implemented using two<a id="_idIndexMarker1259"/> networks; one called the actor network and the other called the critic network.</p>
<p class="normal">The actor network <a id="_idIndexMarker1260"/>approximates the optimal policy deterministically, that is, it outputs the most preferred action for any given input state. In essence, the actor is learning. The critic on the other hand evaluates the optimal action value function using the actor’s most preferred action. Before going further, let us contrast this with the DQN algorithm that we discussed in the preceding section. In <em class="italic">Figure 11.13</em>, you can see the general architecture of DDPG:</p>
<figure class="mediaobject"><img alt="A screenshot of a computer  Description automatically generated with low confidence" height="393" src="../Images/B18331_11_16.png" width="607"/></figure>
<p class="packt_figref">Figure 11.13: Architecture of the DDPG model</p>
<p class="normal">On the left-hand side of <em class="italic">Figure 11.13</em> is the critic network, it takes as input the state vector, <em class="italic">S</em>, and action taken, <em class="italic">A</em>. The output of the network is the <em class="italic">Q</em> value for that state and action. The right-hand figure shows the actor network. It takes as input the state vector, S, and predicts the optimum action, A, to be taken. In the figure, we have shown both the actor and critic to be of four layers. This is only for demonstration purposes. </p>
<p class="normal">The actor network outputs the most preferred action; the critic takes as input both the input state and action taken and evaluates its <em class="italic">Q</em> value. To train the critic network, we follow the same procedure as with a DQN; that is, we try to minimize the difference between the estimated <em class="italic">Q</em> value and<a id="_idIndexMarker1261"/> the target <em class="italic">Q</em> value. The gradient of the <em class="italic">Q</em> value over actions is then propagated back to train the actor network. So, if the critic is good enough, it will force the actor to choose actions with optimal value functions.</p>
<h1 class="heading-1" id="_idParaDest-335">Summary</h1>
<p class="normal">Reinforcement learning has in recent years seen a lot of progress. To summarize all of that in a single chapter is not possible. However, in this chapter, we focused on the recent successful RL algorithms. The chapter started by introducing the important concepts in the RL field, its challenges, and the solutions to move forward. Next, we delved into two important RL algorithms: the DQN and DDPG algorithms. Toward the end of this chapter, we covered important topics in the field of deep learning.</p>
<p class="normal">In the next chapter, we will move on to applying what we have learned to production.</p>
<h1 class="heading-1" id="_idParaDest-336">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">MIT Technology Review covers OpenAI experiments on reinforcement learning: <a href="https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/"><span class="url">https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/</span></a></li>
<li class="numberedList">Coggan, Melanie. (2014). <em class="italic">Exploration and Exploitation in Reinforcement Learning</em>. Research supervised by Prof. Doina Precup, CRA-W DMP Project at McGill University.</li>
<li class="numberedList">Lin, Long-Ji. (1993). <em class="italic">Reinforcement learning for robots using neural networks</em>. No. CMU-CS-93-103. Carnegie-Mellon University Pittsburgh PA School of Computer Science.</li>
<li class="numberedList">Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. (2015). <em class="italic">Prioritized Experience Replay</em>. arXiv preprint arXiv:1511.05952</li>
<li class="numberedList">Sutton R., Barto A.<em class="italic"> Chapter 4, Reinforcement Learning</em>. MIT Press: <a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf"><span class="url">https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf</span></a></li>
<li class="numberedList">Dabney W., Rowland M., Bellemare M G., and Munos R. (2018). <em class="italic">Distributional Reinforcement Learning with Quantile Regression</em>. In Thirty-Second AAAI Conference on Artificial Intelligence.</li>
<li class="numberedList">Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney, W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018). <em class="italic">Rainbow: Combining improvements in Deep Reinforcement Learning</em>. In Thirty-Second AAAI Conference on Artificial Intelligence.</li>
<li class="numberedList">Details about different environments can be obtained from <a href="https://www.gymlibrary.ml/"><span class="url">https://www.gymlibrary.ml/</span></a> </li>
<li class="numberedList">Wiki pages are maintained for some environments at <a href="https://github.com/openai/gym/wiki"><span class="url">https://github.com/openai/gym/wiki</span></a></li>
<li class="numberedList">Details regarding installation instructions and dependencies can be obtained from <a href="https://github.com/openai/gym"><span class="url">https://github.com/openai/gym</span></a></li>
<li class="numberedList">Link to the paper by DeepMind, <em class="italic">Asynchronous Methods for Deep Reinforcement Learning</em>: <a href="https://arxiv.org/pdf/1602.01783.pdf"><span class="url">https://arxiv.org/pdf/1602.01783.pdf</span></a> </li>
<li class="numberedList">This is a blog post by Andrej Karpathy on reinforcement learning: <a href="http://karpathy.github.io/2016/05/31/rl/"><span class="url">http://karpathy.github.io/2016/05/31/rl/</span></a></li>
<li class="numberedList">Glorot X. and Bengio Y. (2010). <em class="italic">Understanding the difficulty of training deep feedforward neural networks</em>. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics: <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf"><span class="url">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</span></a></li>
<li class="numberedList">A good read on why RL is still hard to crack: <a href="https://www.alexirpan.com/2018/02/14/rl-hard.xhtml"><span class="url">https://www.alexirpan.com/2018/02/14/rl-hard.xhtml</span></a></li>
<li class="numberedList">Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., ... &amp; Wierstra, D. (2015). <em class="italic">Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971</em>.</li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>