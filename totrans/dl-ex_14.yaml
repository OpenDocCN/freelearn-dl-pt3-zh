- en: Generative Adversarial Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**) are deep neural net architectures
    that consist of two networks pitted against each other (hence the name **adversarial**).'
  prefs: []
  type: TYPE_NORMAL
- en: GANs were introduced in a paper ([https://arxiv.org/abs/1406.2661](https://arxiv.org/abs/1406.2661))
    by Ian Goodfellow and other researchers, including Yoshua Bengio, at the University
    of Montreal in 2014\. Referring to GANs, Facebook's AI research director, Yann
    LeCun, called **adversarial training** the most interesting idea in the last 10
    years in machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The potential of GANs is huge, because they can learn to mimic any distribution
    of data. That is, GANs can be taught to create worlds eerily similar to our own
    in any domain: images, music, speech, or prose. They are robot artists in a sense,
    and their output is impressive ([https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html](https://www.nytimes.com/2017/08/14/arts/design/google-how-ai-creates-new-music-and-new-artists-project-magenta.html))—and
    poignant too.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive introduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simple implementation of GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep convolutional GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An intuitive introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to introduce GANs in a very intuitive way. To
    get an idea of how GANs work, we will adopt a fake scenario of getting a ticket
    for a party.
  prefs: []
  type: TYPE_NORMAL
- en: 'The story starts with a very interesting party or event being held somewhere,
    and you are very interested in attending it. You hear about this event very late
    and all the tickets are sold out, but you will do anything to get into the party.
    So you come up with an idea! You will try to fake a ticket that needs to be exactly
    the same as the original one, or very, very similar to it. But because life is
    not easy, there''s another challenge: you don''t know what the original ticket
    looks like. So from your experience of going to such parties, you start to imagine
    what the ticket might look like and start to design the ticket based on your imagination.'
  prefs: []
  type: TYPE_NORMAL
- en: You will try to design the ticket and then go to the event and show the ticket
    to the security guys. Hopefully, they will be convinced and will let you in. But
    you don't want to show your face multiple times to the security guards, so you
    decide to get help from your friend, who will take your initial guess about the
    original ticket and show it to the security guards. If they don't let him in,
    he will get some information for you about what the ticket might look like, based
    on seeing some people getting in with the actual ticket. You will refine the ticket
    based on your friend's comments until the security guards let him in. At this
    point—and at this point only—you will design another one that has exactly the
    same look and get yourself in.
  prefs: []
  type: TYPE_NORMAL
- en: Do, think too much about how unrealistic this story is, but the way GANs work
    is very similar to this story. GANs are very trendy nowadays, and people are using
    them for many applications in the field of computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: There are many interesting applications that you can use GANs for, and we will
    implement and mention some of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'In GANs, there are two main components that have made a breakthrough in many
    computer vision fields. The first component is called **Generator** and the second
    one is called **Discriminator**:'
  prefs: []
  type: TYPE_NORMAL
- en: The Generator will try to generate data samples out of a specific probability
    distribution, which is very similar to the guy who was trying to replicate a ticket
    for the event
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Discriminator will judge (like the security guys who are trying to find
    flaws in the ticket to decide whether it''s original or fake) whether its input
    is coming from the original training set (an original ticket) or from the generator
    part (designed by the guy who''s trying to replicate the original ticket):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/4961129f-2166-4bfd-9304-70a05d5d46c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: GANs – general architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Simple implementation of GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: From the story of faking a ticket to an event, the idea of GANs seems to be
    very intuitive. So to get a clear understanding of how GANs work and how to implement
    them, we are going to demonstrate a simple implementation of a GAN on the MNIST
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to build the core of the GAN network, which is comprised of
    two major components: the generator and the discriminator. As we said, the generator
    will try to imagine or fake data samples from a specific probability distribution;
    the discriminator, which has access to and sees the actual data samples, will
    judge whether the generator''s output has any flaws in the design or it''s very
    close to the original data samples. Similar to the scenario of the event, the
    whole purpose of the generator is to try to convince the discriminator that the
    generated image is from the real dataset and hence try to fool him.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The training process has a similar end to the event story; the generator will
    finally manage to generate images that look very similar to the original data
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45302309-65dd-4e06-96f8-c1c5ca5d1569.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: GAN general architecture for the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The typical structure of any GAN is shown in *Figure 2*, which will be trained
    on the MNIST dataset. The `Latent sample` part in this figure is a random thought
    or vector that the generator uses to replicate the real images with fake ones.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned, the discriminator works as a judge and it will try to separate
    the real images from the fake ones that were designed by the generator. So the
    output of this network will be binary, which can be represented by a sigmoid function
    with 0 (meaning the input is a fake image) and 1 (meaning that the input is a
    real image).
  prefs: []
  type: TYPE_NORMAL
- en: Let's go ahead and start implementing this architecture to see how it performs
    on the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start of by importing the required libraries for this implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be using the MNIST dataset, so we are going to use TensorFlow helpers
    to get the dataset and store it somewhere:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Model inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before diving into building the core of the GAN, which is represented by the
    generator and discriminator, we are going to define the inputs of our computational
    graph. As shown in *Figure 2*, we need two inputs. The first one will be the real
    images, which will be fed to the discriminator. The other input is called **latent
    space**, which will be fed to the generator and used to generate its fake images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/c738df95-3bb7-40dd-9862-b5d6f2d3292a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Architecture of the MNIST GAN implementation'
  prefs: []
  type: TYPE_NORMAL
- en: Now it's time to dive into building the two core components of our architecture.
    We will start by building the generator part. As shown in *Figure 3*, the generator
    will consist of at least one hidden layer, which will work as an approximator.
    Also, instead of using the normal ReLU activation function, we will use something
    called a leaky ReLU. This will allow the gradient values to flow through the layer
    without any constraints (more in the next section about leaky RelU).
  prefs: []
  type: TYPE_NORMAL
- en: Variable scope
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Variable scope is a feature of TensorFlow that helps us to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that we have some naming conventions to retrieve them later, for example,
    by making them start with the word generator or discriminator, which will help
    us during the training of the network. We could have used the name scope feature,
    but this feature won't help us for the second purpose.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To be able to reuse or retrain the same network but with different inputs. For
    example, we are going to sample fake images from the generator to see how good
    the generator is for replicating the original ones. Also, the discriminator will
    have access to the real and fake images, which will make it easy for us to reuse
    the variables instead of creating new ones while building the computational graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following statement will show how to use the variable scope feature of
    TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: You can read more about the benefits of using the variable scope feature at
    [https://www.tensorflow.org/programmers_guide/variable_scope#the_problem](https://www.tensorflow.org/programmers_guide/variable_scope#the_problem).
  prefs: []
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We mentioned that we will be using a different version than the ReLU activation
    function, which is called leaky ReLU. The traditional version of the ReLU activation
    function will just take the maximum between the input value and zero, by other
    means truncating negative values to zero. Leaky ReLU, which is the version that
    we will be using, allows some negative values to exist, hence the name **leaky
    ReLU**.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, if we use the traditional ReLU activation function, the network gets
    stuck in a popular state called the dying state, and that's because the network
    produces nothing but zeros for all the outputs.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of using leaky ReLU is to prevent this dying state by allowing some
    negative values to pass through.
  prefs: []
  type: TYPE_NORMAL
- en: The whole idea behind making the generator work is to receive gradient values
    from the discriminator, and if the network is stuck in a dying situation, the
    learning process won't happen.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figures illustrate the difference between traditional ReLU and
    its leaky version:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/282a88ea-0437-4ea8-b383-342103b56ce6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1138c8e8-e7cd-47a0-b5d0-7a899bc7e5e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Leaky ReLU activation functions'
  prefs: []
  type: TYPE_NORMAL
- en: The leaky ReLU activation function is not implemented in TensorFlow, so we need
    to implement it ourselves. The output of this activation function will be positive
    if the input is positive, and will be a controlled negative value if the input
    is negative. We will control the negative value by a parameter called **alpha**,
    which will introduce tolerance of the network by allowing some negative values
    to pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following equation represents the leaky ReLU that we will be implementing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87b4a5a5-f789-4a60-b7a6-30053c8fb0cd.png)'
  prefs: []
  type: TYPE_IMG
- en: Generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MNIST images are normalized between 0 and 1, where the `sigmoid` activation
    function can work best. But in practice, the `tanh` activation function is found
    to give better performance than any other function. So, in order to use the `tanh`
    activation function, we will need to re-scale the range of the pixel values of
    these images to be between -1 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now we have the generator part ready. Let's go ahead and define the second component
    of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next up, we will build the second main component in the generative adversarial
    network, which is the discriminator. The discriminator is pretty much the same
    as the generator, but instead of using the `tanh` activation function, we will
    be using the `sigmoid` activation function; it will produce a binary output that
    will represent the judgment of the discriminator on the input image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Building the GAN network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After defining the main functions that will build the generator and discriminator
    parts, it's time to stack them up and define the model losses and optimizers for
    this implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Model hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can fine-tune the GANs by changing the following set of hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Defining the generator and discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'After defining the two main parts of our architecture that will be used to
    generate fake MNIST images (which will look exactly the same as the real ones),
    it''s time to build the network using the functions that we have defined so far.
    In order to build the network, we are going to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the inputs for our model, which will consist of two variables. One
    of these variables will be the real images, which will be fed to the discriminator,
    and the second will be the latent space to be used by the generator to replicate
    the original images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calling the defined generator function to build the generator part of the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calling the defined discriminator function to build the discriminator part of
    the network, but we are going to call this function twice. One call will be for
    the real data and the second call will be for the fake data from the generator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keeping the weights of real and fake images the same by reusing the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Discriminator and generator losses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, we need to define the discriminator and generator losses, and
    this can be considered to be the most tricky part of this implementation.
  prefs: []
  type: TYPE_NORMAL
- en: We know that the generator tries to replicate the original images and that the
    discriminator works as a judge, receiving both images from the generator and the
    original input images. So while designing our loss for each part, we need to target
    two things.
  prefs: []
  type: TYPE_NORMAL
- en: First, we need the discriminator part of the network to be able to distinguish
    between the fake images generated by the generator and the real images coming
    from the original training examples. During training time, we will feed the discriminator
    part with a batch that is divided into two categories. The first category will
    be images from the original input and the second category will be images from
    the fake ones that got generated by the generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the final general loss of the discriminator will be the sum of its ability
    to accept the real ones as real and detect the fake ones as fake; then the final
    total loss will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85448d3e-1c11-4804-a615-f1b6463e6b26.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: So we need to calculate two losses to come up with the final discriminator loss.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first loss, `disc_loss_real`, will be calculated based on the `logits`
    values that we will get from the discriminator and the `labels`, which will be
    all ones in this case since we know that all the images in this mini-batch are
    all coming from the real input images of the MNIST dataset. To enhance the ability
    of the model to generalize on the test set and give better results, people have
    found that practically changing the value of 1 to 0.9 is better. This kind of
    change to the label introduces something called **label smooth**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: For the second part of the discriminator loss, which is the ability of the discriminator
    to detect fake images, the loss will be between the logits values that we will
    get from the discriminator and labels; all of these are zeros since we know that
    all the images in this mini-batch are coming from the generator, and not from
    the original input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have discussed the discriminator loss, we need to calculate the
    generator loss as well. The generator loss will be called `gen_loss`, which will
    be the loss between `disc_logits_fake` (the output of the discriminator for the
    fake images) and the labels (which will be all ones since the generator is trying
    to convince the discriminator with its design of the fake image):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Optimizers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, the optimizers part! In this section, we will define the optimization
    criteria that will be used during the training process. First off, we are going
    to update the variables of the generator and discriminator separately, so we need
    to be able to retrieve the variables of each part.
  prefs: []
  type: TYPE_NORMAL
- en: For the first optimizer, the generator one, we will retrieve all the variables
    that start with the name `generator` from the trainable variables of the computational
    graph; then we can check which variable is which by referring to its name.
  prefs: []
  type: TYPE_NORMAL
- en: We'll do the same for the discriminator variables as well, by letting in all
    variables that start with `discriminator`. After that, we can pass the list of
    variables that we want to be optimized to the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: 'So the variable scope feature of TensorFlow gave us the ability to retrieve
    variables that start with a certain string, and then we can have two different
    lists of variables, one for the generator and another one for the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let''s kick off the training process and see how GANs will manage to generate
    images similar to the MNIST ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the model for 100 epochs, we have a trained model that will be
    able to generate images similar to the original input images that we fed to the
    discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f56c476d-c084-4820-81e9-d1571c8a5033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Discriminator and Generator Losses'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding figure, you can see that the model losses, which are
    represented by the Discriminator and Generator lines, are converging.
  prefs: []
  type: TYPE_NORMAL
- en: Generator samples from training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s test the performance of the model and even see how the generation skills
    (designing tickets for the event) of the generator got enhanced while approaching
    the end of the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Before plotting some generated images from the last epoch in the training process,
    we need to load the persisted file that contains generated samples at each epoch
    during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s plot the 16 generated images from the last epoch of the training
    process and see how the generator was able to generate meaningful numbers such
    as 3, 7, and 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2e23c0b1-d92c-4a30-8967-03d5a3da995e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Samples from the final training epoch'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can even see the designing skills of the generator over different epochs.
    So let''s visualize the images that are generated by it in every 10 epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0f84e24e-868e-4667-a52c-52afb81d2dc9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Images generated as the network was training, for every 10 epochs'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the designing skills of the generator and its ability to generate
    fake images were very limited at first, and then it was enhanced towards the end
    of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling from the generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous section, we went through some examples that were generated
    during the training process of this GAN architecture. We can also generate completely
    new images from the generator by loading the checkpoints that we have saved and
    feeding the generator with a new latent space that it can use to generate new
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a01f9682-b0f7-4eff-8906-4baaacce3ec9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Samples from the generator'
  prefs: []
  type: TYPE_NORMAL
- en: There are some observations that you can come up with while implementing this
    example. During the first epochs of the training process, the generator doesn't
    have any skills to produce similar images to the real one because it doesn't know
    what they look like. Even the discriminator doesn't know how to distinguish between
    fake images made by the generator and the. At the beginning of training, two interesting
    situations occur. First, the generator does not know how to create images like
    the real ones that we fed originally to the network. Second, the discriminator
    doesn't know the difference between the real and fake images.
  prefs: []
  type: TYPE_NORMAL
- en: Later on, the generator starts to fake images that make sense to some extent,
    and that's because the generator will learn the data distribution that the original
    input images are coming from. In parallel, the discriminator will be able to distinguish
    between fake and real images and it will be fooled by the end of the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs are being used nowadays for lots of interesting applications. GANs can
    be used for different setups such as semi-supervised and unsupervised tasks. Also,
    because of the huge number of researchers working on GANs, these models are progressing
    day by day and their ability to generate images or videos is getting better and
    better.
  prefs: []
  type: TYPE_NORMAL
- en: These kinds of models can be used for many interesting commercial applications,
    such as adding a plugin to Photoshop that can take commands like `make my smile
    more appealing`. They can also be used for image denoising.
  prefs: []
  type: TYPE_NORMAL
