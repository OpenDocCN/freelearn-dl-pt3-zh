<html><head></head><body>
  <div id="_idContainer040">
    <h1 class="chapterNumber">3</h1>
    <h1 id="_idParaDest-88" class="chapterTitle">Keras</h1>
    <p class="normal">In this chapter, we will focus on the high-level TensorFlow API named Keras.</p>
    <p class="normal">By the end of this chapter, you should have a better understanding of:</p>
    <ul>
      <li class="bullet">The Keras Sequential API</li>
      <li class="bullet">The Keras Functional API</li>
      <li class="bullet">The Keras Subclassing API</li>
      <li class="bullet">The Keras Preprocessing API</li>
    </ul>
    <h1 id="_idParaDest-89" class="title">Introduction </h1>
    <p class="normal">In the previous chapter, we covered TensorFlow's fundamentals, and we are now able to set up a computational graph. This chapter will introduce Keras, a high-level neural network API written in Python with multiple backends. TensorFlow is one of them. François Chollet, a French software engineer and AI researcher currently working at Google, created Keras for his own personal use before it was open-sourced in 2015. Keras's primary goal is to provide an easy-to-use and accessible library to enable fast experiments.</p>
    <p class="normal">TensorFlow v1 suffers from usability issues; in particular, a sprawling and sometimes confusing API. For example, TensorFlow v1 offers two high-level APIs:</p>
    <ul>
      <li class="bullet">The Estimator API (added in release 1.1) is used for training models on localhost or distributed environments</li>
      <li class="bullet">The Keras API was then added later (release 1.4.0) and intended to be used for fast prototyping</li>
    </ul>
    <p class="normal">With TensorFlow v2, Keras became the official high-level API. Keras can scale and suit various user profiles, from research to application development and from model training to deployment. Keras provides four key advantages: it's user-friendly (without sacrificing flexibility and performance), modular, composable, and scalable.</p>
    <p class="normal">The TensorFlow Keras APIs are the same as the Keras API. However, the implementation of Keras in its TensorFlow version of the backend has been optimized for TensorFlow. It integrates TensorFlow-specific functionality, such as eager execution, data pipelines, and Estimators.</p>
    <p class="normal">The difference between Keras, the independent library, and Keras' implementation as integrated with TensorFlow is only the way to import it.</p>
    <p class="normal">Here is the command to import the Keras API specification:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> keras
</code></pre>
    <p class="normal">Here is TensorFlow's implementation of the Keras API specification:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
</code></pre>
    <p class="normal">Now, let's start by discovering the basic building blocks of Keras.</p>
    <h1 id="_idParaDest-90" class="title">Understanding Keras layers</h1>
    <p class="normal">Keras layers are the <a id="_idIndexMarker139"/>fundamental building blocks of Keras models. Each layer receives data as input, does a specific task, and returns an output.</p>
    <p class="normal">Keras includes a wide range of built-in layers:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Core layers:</strong> Dense, Activation, Flatten, Input, Reshape, Permute, RepeatVector, SpatialDropOut, and many more.</li>
      <li class="bullet"><strong class="keyword">Convolutional layers for Convolutional Neural Networks:</strong> Conv1D, Conv2D, SeparableConv1D, Conv3D, Cropping2D, and many more.</li>
      <li class="bullet"><strong class="keyword">Pooling</strong> layers that perform a downsampling operation to reduce feature maps: MaxPooling1D, AveragePooling2D, and GlobalAveragePooling3D.</li>
      <li class="bullet"><strong class="keyword">Recurrent layers for recurrent neural networks to process recurrent or sequence data:</strong> RNN, SimpleRNN, GRU, LSTM, ConvLSTM2D, etc.</li>
      <li class="bullet"><strong class="keyword">The embedding layer</strong>, only used as the first layer in a model and turns positive integers into dense vectors of fixed size.</li>
      <li class="bullet"><strong class="keyword">Merge layers:</strong> Add, Subtract, Multiply, Average, Maximum, Minimum, and many more.</li>
      <li class="bullet"><strong class="keyword">Advanced activation layers:</strong> LeakyReLU, PReLU, Softmax, ReLU, etc.</li>
      <li class="bullet"><strong class="keyword">The batch normalization layer</strong>, which normalizes the activation of the previous layer at each batch.</li>
      <li class="bullet"><strong class="keyword">Noise layers:</strong> GausianNoise, GausianDropout, and AlphaDropout.</li>
      <li class="bullet"><strong class="keyword">Layer wrappers:</strong> TimeDistributed applies a layer to every temporal slice of an input and bidirectional wrapper for RNNs.</li>
      <li class="bullet"><strong class="keyword">Locally-connected layers:</strong> LocallyConnected1D and LocallyConnected2D. They work like Conv1D or Conv2D without sharing their weights.</li>
    </ul>
    <p class="normal">We can also write our Keras layers as explained in the Keras Subclassing API section of this chapter.</p>
    <h2 id="_idParaDest-91" class="title">Getting ready </h2>
    <p class="normal">To start, we'll review some methods that are common in all Keras layers. These methods are very useful to know the configuration and the state of a layer.</p>
    <h2 id="_idParaDest-92" class="title">How to do it... </h2>
    <ol>
      <li class="numbered">Let's start with the <a id="_idIndexMarker140"/>layer's weights. The weights are possibly the most essential concept in a layer; it decides how much influence the input will have on the output. It represents the state of a layer. The <code class="Code-In-Text--PACKT-">get_weights()</code> function returns the weights of the layer as a list of NumPy arrays:
        <pre class="programlisting code"><code class="hljs-code">layer.get_weights()
</code></pre>
        <p class="bullet-para">The <code class="Code-In-Text--PACKT-">set_weights()</code> method fixes the weights of the layer from a list of Numpy arrays:</p>
        <pre class="programlisting code"><code class="hljs-code">layer.set_weights(weights)
</code></pre>
      </li>
      <li class="numbered">As we'll explain in the Keras Functional API recipe, sometimes neural network topology isn't linear. In this case, a layer can be used several times in the network (shared layer). We can easily get the inputs and outputs of a layer by using this command if the layer is a single node (no shared layer):
        <pre class="programlisting code"><code class="hljs-code">layer.input
layer.output
</code></pre>
        <p class="bullet-para">Or this one, if the layer has multiple nodes:</p>
        <pre class="programlisting code"><code class="hljs-code">layer.get_input_at(node_index)
layer.get_output_at(node_index)
</code></pre>
      </li>
      <li class="numbered">We can also easily get the layer's input and output shapes by using this command if a layer is a single node (no shared layer):
        <pre class="programlisting code"><code class="hljs-code">layer.input_shape
layer.output_shape
</code></pre>
        <p class="bullet-para">Or this one, if the layer has multiple nodes:</p>
        <pre class="programlisting code"><code class="hljs-code">layer.get_input_shape_at(node_index)
layer.get_output_shape_at(node_index)
</code></pre>
      </li>
      <li class="numbered">Now, we'll be discussing the layer's configuration. As the same layer could be instantiating several times, the configuration doesn't include the weights or connectivity information. The <code class="Code-In-Text--PACKT-">get_config()</code> function returns a dictionary containing the configuration of the layer:
        <pre class="programlisting code"><code class="hljs-code">layer.get_config()
</code></pre>
        <p class="bullet-para">The <code class="Code-In-Text--PACKT-">from_config()</code> method instantiates a layer's configuration:</p>
        <pre class="programlisting code"><code class="hljs-code">layer.from_config(config)
</code></pre>
        <p class="bullet-para">Note that the layer configuration is stored in an associative array (Python dictionary), a data structure that maps keys to values.</p>
      </li>
    </ol>
    <h2 id="_idParaDest-93" class="title">How it works... </h2>
    <p class="normal">The layers are <a id="_idIndexMarker141"/>the building blocks of the models. Keras offers a wide range of building layers and useful methods to know more about what's happening and get inside the models.</p>
    <p class="normal">With Keras, we can build models in three ways: with the Sequential, the Functional, or the Subclassing API. We'll later see that only the last two APIs allow access to the layers.</p>
    <h2 id="_idParaDest-94" class="title">See also</h2>
    <p class="normal">For some <a id="_idIndexMarker142"/>references on the Keras Layers API, see the following documentation:</p>
    <ul>
      <li class="bullet">Keras layers API documentation: <a href="https://keras.io/layers/about-keras-layers/"><span class="url">https://keras.io/layers/about-keras-layers/</span></a></li>
      <li class="bullet">TensorFlow Keras layers API documentation: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/layers</span></a></li>
    </ul>
    <h1 id="_idParaDest-95" class="title">Using the Keras Sequential API</h1>
    <p class="normal">The main goal of <a id="_idIndexMarker143"/>Keras is to make it easy to create deep learning models. The Sequential API allows us to create Sequential models, which are a linear stack of layers. Models that are connected layer by layer can solve many problems. To create a Sequential model, we have to create an instance of a Sequential class, create some model layers, and add them to it.</p>
    <p class="normal">We will go from the creation of our Sequential model to its prediction via the compilation, training, and evaluation steps. By the end of this recipe, you will have a Keras model ready to be deployed in production.</p>
    <h2 id="_idParaDest-96" class="title">Getting ready</h2>
    <p class="normal">This recipe will cover the main ways of creating a Sequential model and assembling layers to build a model with the Keras Sequential API.</p>
    <p class="normal">To start, we load TensorFlow and NumPy, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">We are ready to proceed with an explanation of how to do it.</p>
    <h2 id="_idParaDest-97" class="title">How to do it... </h2>
    <ol>
      <li class="numbered" value="1">First, we will create a Sequential model. Keras offers two equivalent ways of creating a Sequential model. Let's start by passing a list of layer instances as an array to the constructor. We'll build a multi-class classifier (10 categories) fully connected model, aka a multi-layer perceptron, by entering the following code.
        <pre class="programlisting code"><code class="hljs-code">model = tf.keras.Sequential([
    <span class="hljs-comment"># Add a fully connected layer with 1024 units to the model</span>
    tf.keras.layers.Dense(<span class="hljs-number">1024</span>, input_dim=<span class="hljs-number">64</span>),
    <span class="hljs-comment"># Add an activation layer with ReLU activation function</span>
    tf.keras.layers.Activation(<span class="hljs-string">'relu'</span>),
    <span class="hljs-comment"># Add a fully connected layer with 256 units to the model</span>
    tf.keras.layers.Dense(<span class="hljs-number">256</span>),
    <span class="hljs-comment"># Add an activation layer with ReLU activation function</span>
    tf.keras.layers.Activation(<span class="hljs-string">'relu'</span>),
    <span class="hljs-comment"># Add a fully connected layer with 10 units to the model</span>
    tf.keras.layers.Dense(<span class="hljs-number">10</span>),
    <span class="hljs-comment"># Add an activation layer with softmax activation function</span>
    tf.keras.layers.Activation(<span class="hljs-string">'softmax'</span>)
])
</code></pre>
        <p class="bullet-para">Another way to create a Sequential model is to instantiate a Sequential class and then add <a id="_idIndexMarker144"/>layers via the <code class="Code-In-Text--PACKT-">.add()</code> method.</p>
        <pre class="programlisting code"><code class="hljs-code">model = tf.keras.Sequential()
<span class="hljs-comment"># Add a fully connected layer with 1024 units to the model</span>
model.add(tf.keras.layers.Dense(<span class="hljs-number">1024</span>, input_dim=<span class="hljs-number">64</span>))
<span class="hljs-comment"># Add an activation layer with ReLU activation function</span>
model.add(tf.keras.layers.Activation(relu))
<span class="hljs-comment"># Add a fully connected layer with 256 units to the model</span>
model.add(tf.keras.layers.Dense(<span class="hljs-number">256</span>))
<span class="hljs-comment"># Add an activation layer with ReLU activation function</span>
model.add(tf.keras.layers.Activation(<span class="hljs-string">'relu'</span>))
<span class="hljs-comment"># Add a fully connected Layer with 10 units to the model</span>
model.add(tf.keras.layers.Dense(<span class="hljs-number">10</span>))
<span class="hljs-comment"># Add an activation layer with softmax activation function</span>
model.add(tf.keras.layers.Activation(<span class="hljs-string">'softmax'</span>))
</code></pre>
      </li>
      <li class="numbered">Let's take a closer look at the layer configuration. The <code class="Code-In-Text--PACKT-">tf.keras.layers</code> API offers a lot of built-in layers and also provides an API to create our layers. In most of them, we can set these parameters to the layer's constructor:<ul>
          <li class="bullet-l2">We can add an activation function by specifying the name of a built-in function or as a callable object. This function decides whether a neuron should be activated or not. By default, a layer has no activation function. Below are the two ways to create a layer with an activation function. Note that you don't have to run the following code; these layers are not assigned to variables.
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Creation of a dense layer with a sigmoid activation function:</span>
Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">'sigmoid'</span>)
<span class="hljs-comment"># Or:</span>
Dense(<span class="hljs-number">256</span>, activation=tf.keras.activations.sigmoid)
</code></pre></li>
        </ul>
        <ul>
          <li class="bullet-l2">We can also specify an initialization strategy for the initial weights (kernel and bias) by passing the string identifier of built-in initializers or a callable object. The kernel is by default set to the "Glorot uniform" initializer, and the bias is set to zeros.
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># A dense layer with a kernel initialized to a truncated normal distribution:</span>
Dense(<span class="hljs-number">256</span>, kernel_initializer=<span class="hljs-string">'random_normal'</span>)
<span class="hljs-comment"># A dense layer with a bias vector initialized with a constant value of 5.0:</span>
Dense(<span class="hljs-number">256</span>, bias_initializer=tf.keras.initializers.Constant(value=<span class="hljs-number">5</span>))
</code></pre></li>
        </ul>
        <ul>
          <li class="bullet-l2">We can also specify regularizers for kernel and <a id="_idIndexMarker145"/>bias, such as L1 (also called Lasso) or <a id="_idIndexMarker146"/>L2 (also called Ridge) regularization. By default, no regularization is applied. A regularizer aims to prevent overfitting by penalizing a model <a id="_idIndexMarker147"/>for having large weights. These penalties are incorporated in the loss function that the network optimizes.
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># A dense layer with L1 regularization of factor 0.01 applied to the kernel matrix:</span>
Dense(<span class="hljs-number">256</span>, kernel_regularizer=tf.keras.regularizers.l1(<span class="hljs-number">0.01</span>))
<span class="hljs-comment"># A dense layer with L2 regularization of factor 0.01 applied to the bias vector:</span>
Dense(<span class="hljs-number">256</span>, bias_regularizer=tf.keras.regularizers.l2(<span class="hljs-number">0.01</span>))
</code></pre></li>
        </ul>
      </li>
      <li class="numbered">In Keras, it's strongly recommended to set the input shape for the first layer. Yet, contrary to appearances, the input layer isn't a layer but a tensor. Its shape must be the same as our training data. The following layers perform automatic shape inference; their shapes are calculated based on the unit of the previous layer.<p class="bullet-para">Each type of layer requires input with a certain number of dimensions, so there are different ways to specify the input shape depending on the kind of layer. Here, we'll focus on the Dense layer, so we'll use the <code class="Code-In-Text--PACKT-">input_dim</code> parameter. Since the shape of the weights depends on the input size, if the input shape isn't specified in advance, the model has no weights: the model is not built. In this case, you can't call any methods of the <code class="Code-In-Text--PACKT-">Layer</code> class such as <code class="Code-In-Text--PACKT-">summary</code>, <code class="Code-In-Text--PACKT-">layers</code>, <code class="Code-In-Text--PACKT-">weights</code>, and so on.</p>
        <p class="bullet-para">In this recipe, we'll create datasets with 64 features, and we'll process batches of 10 samples. The shape of our input data is (10,64), aka (<code class="Code-In-Text--PACKT-">batch_size</code>, <code class="Code-In-Text--PACKT-">number_of_features</code>). By default, a Keras model is defined to support any batch size, so the batch size isn't mandatory. We just have to specify the number of features through the <code class="Code-In-Text--PACKT-">input_dim</code> parameter to our first layer.</p>
        <pre class="programlisting code"><code class="hljs-code">Dense(<span class="hljs-number">256</span>, input_dim=(<span class="hljs-number">64</span>))
</code></pre>
        <p class="bullet-para">However, we can force the batch size for efficiency reasons with the <code class="Code-In-Text--PACKT-">batch_size</code> argument.</p>
        <pre class="programlisting code"><code class="hljs-code"> Dense(<span class="hljs-number">256</span>, input_dim=(<span class="hljs-number">64</span>), batch_size=<span class="hljs-number">10</span>)
</code></pre>
      </li>
      <li class="numbered">Before the learning phase, our model needs to be configured. This is done by the <code class="Code-In-Text--PACKT-">compile</code> method. We have to specify:<ul>
          <li class="bullet-l2">An optimization algorithm for the training of our neural network. We can pass an optimizer instance from the <code class="Code-In-Text--PACKT-">tf.keras.optimizers</code> module. For example, we can use an instance of <code class="Code-In-Text--PACKT-">tf.keras.optimizers.RMSprop</code> or <code class="Code-In-Text--PACKT-">'RMSprop'</code>, which is an optimizer that implements the <code class="Code-In-Text--PACKT-">RMSprop</code> algorithm.</li>
          <li class="bullet-l2">A loss function <a id="_idIndexMarker148"/>called an objective function <a id="_idIndexMarker149"/>or optimization score function aims at minimizing the model. It <a id="_idIndexMarker150"/>can be the name of an existing loss function (such as <code class="Code-In-Text--PACKT-">categorical_crossentropy</code> or <code class="Code-In-Text--PACKT-">mse</code>), a symbolic TensorFlow loss function (<code class="Code-In-Text--PACKT-">tf.keras.losses.MAPE</code>), or a custom loss function, which takes as input two tensors (true tensors and predicted tensors) and returns a scalar for each data point.</li>
          <li class="bullet-l2">A list of metrics used to judge our model's performance that aren't used in the model training process. We can either pass the string names or callables from the <code class="Code-In-Text--PACKT-">tf.keras.metrics</code> module.</li>
          <li class="bullet-l2">If you want to be sure that the model trains and evaluates eagerly, we can set the argument <code class="Code-In-Text--PACKT-">run_eagerly</code> to true.</li>
        </ul>
        <p class="bullet-para">Note that the graph is finalized with the <code class="Code-In-Text--PACKT-">compile</code> method. </p>
        <p class="bullet-para">Now, we'll compile the model using the Adam optimizer for categorical cross-entropy loss and display the accuracy metric.</p>
        <pre class="programlisting code"><code class="hljs-code">model.compile(
    optimizer=<span class="hljs-string">"adam"</span>, 
    loss=<span class="hljs-string">"categorical_crossentropy"</span>,
    metrics=[<span class="hljs-string">"accuracy"</span>]
)
</code></pre>
      </li>
      <li class="numbered">Now, we'll generate three toy datasets of 64 features with random values. One will be used to train the model (2,000 samples), another one to validate (500 samples), and the last one to test (500 samples).
        <pre class="programlisting code"><code class="hljs-code">data = np.random.random((<span class="hljs-number">2000</span>, <span class="hljs-number">64</span>))
labels = np.random.random((<span class="hljs-number">2000</span>, <span class="hljs-number">10</span>))
val_data = np.random.random((<span class="hljs-number">500</span>, <span class="hljs-number">64</span>))
val_labels = np.random.random((<span class="hljs-number">500</span>, <span class="hljs-number">10</span>))
test_data = np.random.random((<span class="hljs-number">500</span>, <span class="hljs-number">64</span>))
test_labels = np.random.random((<span class="hljs-number">500</span>, <span class="hljs-number">10</span>))
</code></pre>
      </li>
      <li class="numbered">After the model has been configured, the learning phase begins by calling the <code class="Code-In-Text--PACKT-">fit</code> method. The training configuration is done by these three arguments:<ul>
          <li class="bullet-l2">We have to set the number of epochs, aka the number of iterations over the entire input data.</li>
          <li class="bullet-l2">We have to specify the number of samples per gradient, called the <code class="Code-In-Text--PACKT-">batch_size</code> argument. Note <a id="_idIndexMarker151"/>that the last batch may be smaller if the total number of samples is not divisible by the batch size.</li>
          <li class="bullet-l2">We can specify a validation dataset by setting the <code class="Code-In-Text--PACKT-">validation_data</code> argument (a tuple of inputs and labels). This dataset makes it easy to monitor the performance of the model. The loss and metrics are computed in inference mode at the end of each epoch.</li>
        </ul>
        <p class="bullet-para">Now, we'll train the model on our toy datasets by calling the <code class="Code-In-Text--PACKT-">fit</code> method:</p>
        <pre class="programlisting code"><code class="hljs-code">model.fit(data, labels, epochs=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">50</span>,
          validation_data=(val_data, val_labels))
</code></pre>
      </li>
      <li class="numbered">Then, we'll evaluate our model on the test dataset. We'll call the <code class="Code-In-Text--PACKT-">model.evaluate</code> function, which predicts the loss value and the metric values of the model in test mode. Computation is done in batches. It has three important arguments: the input data, the target data, and the batch size. This function predicts the output for a given input. Then, it computes the <code class="Code-In-Text--PACKT-">metrics</code> function (specified in the <code class="Code-In-Text--PACKT-">model.compile</code> based on the target data) and the model's prediction and returns the computed metric value as the output.
        <pre class="programlisting code"><code class="hljs-code">model.evaluate(data, labels, batch_size=<span class="hljs-number">50</span>)
</code></pre>
      </li>
      <li class="numbered">We can also just use the model to make a prediction. The <code class="Code-In-Text--PACKT-">tf.keras.Model.predict</code> method takes as input only data and returns a prediction. And here's how to predict the output of the last layer of inference for the data provided, as a NumPy array:
        <pre class="programlisting code"><code class="hljs-code">result = model.predict(data, batch_size=<span class="hljs-number">50</span>)
</code></pre>
        <p class="bullet-para">Analyzing this model's performance is of no interest in this recipe because we randomly generated a dataset.</p>
        <p class="bullet-para">Now, let's move on to an analysis of this recipe.</p>
      </li>
    </ol>
    <h2 id="_idParaDest-98" class="title">How it works... </h2>
    <p class="normal">Keras provides the Sequential <a id="_idIndexMarker152"/>API to create models composed of a linear stack of layers. We can either pass a list of layer instances as an array to the constructor or use the <code class="Code-In-Text--PACKT-">add</code> method. </p>
    <p class="normal">Keras provides different kinds of layers. Most of them share some common constructor arguments such as <code class="Code-In-Text--PACKT-">activation</code>, <code class="Code-In-Text--PACKT-">kernel_initializer</code> and <code class="Code-In-Text--PACKT-">bias_initializer</code>, and <code class="Code-In-Text--PACKT-">kernel_regularizer</code> and <code class="Code-In-Text--PACKT-">bias_regularizer</code>.</p>
    <p class="normal">Take care with the delayed-build pattern: if no input shape is specified on the first layer, the model gets built the first time the model is called on some input data or when methods such as <code class="Code-In-Text--PACKT-">fit</code>, <code class="Code-In-Text--PACKT-">eval</code>, <code class="Code-In-Text--PACKT-">predict</code>, and <code class="Code-In-Text--PACKT-">summary</code> are called. The graph is finalized with the <code class="Code-In-Text--PACKT-">compile</code> method, which configures the model before the learning phase. Then, we can evaluate the model or make predictions.</p>
    <h2 id="_idParaDest-99" class="title">See also </h2>
    <p class="normal">For some references on <a id="_idIndexMarker153"/>the Keras Sequential API, visit the following websites:</p>
    <ul>
      <li class="bullet">tf.keras.Sequential model API documentation: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Sequential "><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/Sequential</span></a></li>
      <li class="bullet">Keras Sequential model API documentation: <a href="https://keras.io/models/sequential/ "><span class="url">https://keras.io/models/sequential/ </span></a></li>
    </ul>
    <h1 id="_idParaDest-100" class="title">Using the Keras Functional API</h1>
    <p class="normal">The Keras <a id="_idIndexMarker154"/>Sequential API is great for developing deep learning models in most situations. However, this API has some limitations, such as a linear topology, that could be overcome with the Functional API. Note that many high-performing networks are based on a non-linear topology such as Inception, ResNet, etc. </p>
    <p class="normal">The Functional API allows defining complex models with a non-linear topology, multiple inputs, multiple outputs, residual connections with non-sequential flows, and shared and reusable layers.</p>
    <p class="normal">The deep learning model is usually a directed acyclic graph (DAG). The Functional API is a way to build a graph of layers and create more flexible models than the <code class="Code-In-Text--PACKT-">tf.keras.Sequential</code> API.</p>
    <h2 id="_idParaDest-101" class="title">Getting ready </h2>
    <p class="normal">This recipe will cover <a id="_idIndexMarker155"/>the main ways of creating a Functional model, using callable models, manipulating complex graph topologies, sharing layers, and finally introducing the concept of the layer "node" with the Keras Sequential API.</p>
    <p class="normal">As usual, we just need to import TensorFlow as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Input, Dense, TimeDistributed
<span class="hljs-keyword">import</span> keras.models
</code></pre>
    <p class="normal">We are ready to proceed with an explanation of how to do it.</p>
    <h2 id="_idParaDest-102" class="title">How to do it... </h2>
    <p class="normal">Let's go and make a Functional model for recognizing the MNIST dataset of handwritten digits. We will predict the handwritten digits from grayscale images.</p>
    <h3 id="_idParaDest-103" class="title">Creating a Functional model</h3>
    <ol>
      <li class="numbered" value="1">First, we will <a id="_idIndexMarker156"/>load the MNIST dataset.
        <pre class="programlisting code"><code class="hljs-code">mnist = tf.keras.datasets.mnist
(X_mnist_train, y_mnist_train), (X_mnist_test, y_mnist_test) = mnist.load_data()
</code></pre>
      </li>
      <li class="numbered">Then, we will create an input node with a 28x28 dimensional shape. Remember that in Keras, the input layer is not a layer but a tensor, and we have to specify the input shape for the first layer. This tensor must have the same shape as our training data. By default, a Keras model is defined to support any batch size, so the batch size isn't mandatory. <code class="Code-In-Text--PACKT-">Input()</code> is used to instantiate a Keras tensor.
        <pre class="programlisting code"><code class="hljs-code">inputs = tf.keras.Input(shape=(<span class="hljs-number">28</span>,<span class="hljs-number">28</span>))
</code></pre>
      </li>
      <li class="numbered">Then, we will flatten the images of size (28,28) using the following command. This will produce an array of 784 pixels.
        <pre class="programlisting code"><code class="hljs-code">flatten_layer = keras.layers.Flatten()
</code></pre>
      </li>
      <li class="numbered">We'll add a new node in the graph of layers by calling <code class="Code-In-Text--PACKT-">the flatten_layer</code> on the <code class="Code-In-Text--PACKT-">inputs</code> object:
        <pre class="programlisting code"><code class="hljs-code">flatten_output = flatten_layer(inputs)
</code></pre>
        <p class="bullet-para">The "layer call" action is like drawing an arrow from <code class="Code-In-Text--PACKT-">inputs</code> to the <code class="Code-In-Text--PACKT-">flatten_layer</code>. We're "passing" the inputs to the flatten layer, and as a result, it produces outputs. A layer instance is callable (on a tensor) and returns a tensor.</p>
      </li>
      <li class="numbered">Then, we'll create a new layer instance:
        <pre class="programlisting code"><code class="hljs-code">dense_layer = tf.keras.layers.Dense(<span class="hljs-number">50</span>, activation=<span class="hljs-string">'relu'</span>)
</code></pre>
      </li>
      <li class="numbered">We'll add a <a id="_idIndexMarker157"/>new node:
        <pre class="programlisting code"><code class="hljs-code">dense_output = dense_layer(flatten_output)
</code></pre>
      </li>
      <li class="numbered">To build a model, multiple layers are stacked. In this example, we will add another <code class="Code-In-Text--PACKT-">dense</code> layer to do a classification task between 10 classes:
        <pre class="programlisting code"><code class="hljs-code">predictions = tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)(dense_output)
</code></pre>
      </li>
      <li class="numbered">Input tensor(s) and output tensor(s) are used to define a model. The model is a function of one or more input layers and one or more output layers. The model instance formalizes the computational graph on how the data flows from input(s) to output(s).
        <pre class="programlisting code"><code class="hljs-code">model = keras.Model(inputs=inputs, outputs=predictions)
</code></pre>
      </li>
      <li class="numbered">Now, we'll print the summary.
        <pre class="programlisting code"><code class="hljs-code">model.summary()
</code></pre>
      </li>
      <li class="numbered">This results in the following output:<figure class="mediaobject"><img src="../Images/B16254_03_01.png" alt=""/></figure>
        <p class="packt_figref">Figure 3.1: Summary of the model</p>
      </li>
      <li class="numbered">Such <a id="_idIndexMarker158"/>a model can be trained and evaluated by the same <code class="Code-In-Text--PACKT-">compile, fit</code>, <code class="Code-In-Text--PACKT-">evaluate</code>, and <code class="Code-In-Text--PACKT-">predict</code> methods used in the Keras Sequential model.
        <pre class="programlisting code"><code class="hljs-code">model.compile(optimizer=<span class="hljs-string">'sgd'</span>,
             loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>,
             metrics=[<span class="hljs-string">'accuracy'</span>])
model.fit(X_mnist_train, y_mnist_train,
          validation_data=(X_mnist_train, y_mnist_train),
          epochs=<span class="hljs-number">10</span>)
</code></pre>
      </li>
    </ol>
    <p class="normal">In this recipe, we have built a model using the Functional API.</p>
    <h3 id="_idParaDest-104" class="title">Using callable models like layers</h3>
    <p class="normal">Let's go into the details of the Functional API with callable models.</p>
    <ol>
      <li class="numbered" value="1">With the Functional API, it is <a id="_idIndexMarker159"/>easy to reuse trained models: any model can be treated as a layer, by calling it on a tensor. We will reuse the model defined in the previous section as a layer to see this in action. It's a classifier for 10 categories. This model returns 10 probabilities: 1 for each category. It's called a 10-way <a id="_idIndexMarker160"/>softmax. So, by calling the model defined above, the model will predict for each input one of the 10 classes.
        <pre class="programlisting code"><code class="hljs-code">x = Input(shape=(<span class="hljs-number">784</span>,))
<span class="hljs-comment"># y will contain the prediction for x</span>
y = model(x)
</code></pre>
        <div class="packt_tip">
          <p class="Tip--PACKT-">Note that by calling a model, we are not just reusing the model architecture, we are also reusing its weights.</p>
        </div>
      </li>
      <li class="numbered">If we're facing a sequence problem, creating a model will become very easy with the Functional API. For example, instead of processing one image, we want to process a video composed of many images. We could turn an image classification model into a video classification model in just one line using the <code class="Code-In-Text--PACKT-">TimeDistributed</code> layer wrapper. This wrapper applies our previous model to every temporal slice of <a id="_idIndexMarker161"/>the input sequence, or in other words, to each image of our video.
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> TimeDistributed
<span class="hljs-comment"># Input tensor for sequences of 50 timesteps,</span>
<span class="hljs-comment"># Each containing a 28x28 dimensional matrix.</span>
input_sequences = tf.keras.Input(shape=(<span class="hljs-number">10</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>))
<span class="hljs-comment"># We will apply the previous model to each sequence so one for each timestep.</span>
<span class="hljs-comment"># The MNIST model returns a vector with 10 probabilities (one for each digit).</span>
<span class="hljs-comment"># The TimeDistributed output will be a sequence of 50 vectors of size 10.</span>
processed_sequences = tf.keras.layers.TimeDistributed(model)(input_sequences)
</code></pre>
      </li>
    </ol>
    <p class="normal">We have seen that models are callable like layers. Now, we'll learn how to create complex models with a non-linear topology.</p>
    <h3 id="_idParaDest-105" class="title">Creating a model with multiple inputs and outputs</h3>
    <p class="normal">The Functional API <a id="_idIndexMarker162"/>makes it easy to manipulate a large number of intertwined datastreams with multiple inputs and outputs and non-linear connectivity topologies. These cannot be handled with the Sequential API, which isn't able to create a model with layers that aren't connected sequentially or with multiple inputs or outputs.</p>
    <p class="normal">Let's go with an example. We're going to build a system for predicting the price of a specific house and the elapsed time before its sale.</p>
    <p class="normal">The model will have two inputs:</p>
    <ul>
      <li class="bullet-l2">Data about the house such as the number of bedrooms, house size, air conditioning, fitted kitchen, etc.</li>
      <li class="bullet-l2">A recent picture of the house</li>
    </ul>
    <p class="normal">This model will have two outputs:</p>
    <ul>
      <li class="bullet-l2">The elapsed time before the sale (two categories – slow or fast)</li>
      <li class="bullet-l2">The predicted price</li>
    </ul>
    <ol>
      <li class="numbered" value="1">To build this system, we'll start by building the first block to process tabular data about the house.
        <pre class="programlisting code"><code class="hljs-code">house_data_inputs = tf.keras.Input(shape=(<span class="hljs-number">128</span>,), name=<span class="hljs-string">'house_data'</span>)
x = tf.keras.layers.Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>)(house_data_inputs)
block_1_output = tf.keras.layers.Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>)(x)
</code></pre>
      </li>
      <li class="numbered">Then, we'll build <a id="_idIndexMarker163"/>the second block to process the house image data.
        <pre class="programlisting code"><code class="hljs-code">house_picture_inputs = tf.keras.Input(shape=(<span class="hljs-number">128</span>,<span class="hljs-number">128</span>,<span class="hljs-number">3</span>), name=<span class="hljs-string">'house_picture'</span>)
x = tf.keras.layers.Conv2D(<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>)(house_picture_inputs)
x = tf.keras.layers.Conv2D(<span class="hljs-number">64</span>, <span class="hljs-number">3</span>, activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>)(x)
block_2_output = tf.keras.layers.Flatten()(x)
</code></pre>
      </li>
      <li class="numbered">Now, we'll merge all available features into a single large vector via concatenation.
        <pre class="programlisting code"><code class="hljs-code">x = tf.keras.layers.concatenate([block_1_output, block_2_output])
</code></pre>
      </li>
      <li class="numbered">Then, we'll stick a logistic regression for price prediction on top of the features.
        <pre class="programlisting code"><code class="hljs-code">price_pred = tf.keras.layers.Dense(<span class="hljs-number">1</span>, name=<span class="hljs-string">'price'</span>, activation=<span class="hljs-string">'relu'</span>)(x)
</code></pre>
      </li>
      <li class="numbered">And, we'll stick a time classifier on top of the features.
        <pre class="programlisting code"><code class="hljs-code">time_elapsed_pred = tf.keras.layers.Dense(<span class="hljs-number">2</span>, name=<span class="hljs-string">'elapsed_time'</span>, activation=<span class="hljs-string">'softmax'</span>)(x)
</code></pre>
      </li>
      <li class="numbered">Now, we'll build the model.
        <pre class="programlisting code"><code class="hljs-code">model = keras.Model([house_data_inputs, house_picture_inputs],
                   [price_pred, time_elapsed_pred],
                   name=<span class="hljs-string">'toy_house_pred'</span>)
</code></pre>
      </li>
      <li class="numbered">Now, we'll plot the model.
        <pre class="programlisting code"><code class="hljs-code">keras.utils.plot_model(model, <span class="hljs-string">'multi_input_and_output_model.png'</span>, show_shapes=<span class="hljs-literal">True</span>)
</code></pre>
      </li>
      <li class="numbered">This <a id="_idIndexMarker164"/>results in the following output:<figure class="mediaobject"><img src="../Images/B16254_03_02.png" alt=""/></figure>
    
    <p class="packt_figref">Figure 3.2: Plot of a model with multiple inputs and outputs</p></li>
    </ol>
    <p class="normal">In this recipe, we have <a id="_idIndexMarker165"/>created a complex model using the Functional API with multiple inputs and outputs that predicts the price of a specific house and the elapsed time before its sale. Now, we'll introduce the concept of shared layers.</p>
    <h3 id="_idParaDest-106" class="title">Shared layers </h3>
    <p class="normal">Some models reuse the <a id="_idIndexMarker166"/>same layer multiple times inside their architecture. These layer instances learn features that correspond to multiple paths in the graph of layers. Shared layers are often used to encode inputs from similar spaces.</p>
    <p class="normal">To share a layer (weights and all) across different inputs, we only need to instantiate the layer once and call it on as many inputs as we want.</p>
    <p class="normal">Let's consider two different sequences of text. We will apply the same embedding layer to these two sequences, which feature similar vocabulary.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Variable-length sequence of integers</span>
text_input_a = tf.keras.Input(shape=(<span class="hljs-literal">None</span>,), dtype=<span class="hljs-string">'int32'</span>)
<span class="hljs-comment"># Variable-length sequence of integers</span>
text_input_b = tf.keras.Input(shape=(<span class="hljs-literal">None</span>,), dtype=<span class="hljs-string">'int32'</span>)
<span class="hljs-comment"># Embedding for 1000 unique words mapped to 128-dimensional vectors</span>
shared_embedding = tf.keras.layers.Embedding(<span class="hljs-number">1000</span>, <span class="hljs-number">128</span>)
<span class="hljs-comment"># Reuse the same layer to encode both inputs</span>
encoded_input_a = shared_embedding(text_input_a)
encoded_input_b = shared_embedding(text_input_b)
</code></pre>
    <p class="normal">In this recipe, we have <a id="_idIndexMarker167"/>learned how to reuse a layer multiple times in the same model. Now, we'll introduce the concept of extracting and reusing a layer.</p>
    <h3 id="_idParaDest-107" class="title">Extracting and reusing nodes in the graph of layers </h3>
    <p class="normal">In the first <a id="_idIndexMarker168"/>recipe of this chapter, we saw <a id="_idIndexMarker169"/>that a layer is an instance that takes a tensor as an argument and returns another tensor. A model is composed of several layer instances. These layer instances are objects that are chained one to another by their layer input and output tensors. Each time we instantiate a layer, the output of the layer is a new tensor. By adding a "node" to the layer, we link the input to the output tensor. </p>
    <p class="normal">The graph of layers is a static data structure. With the Keras Functional API, we can easily access and inspect the model.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">tf.keras.application</code> module contains canned architectures with pre-trained weights.</p>
    <ol>
      <li class="numbered" value="1">Let's go to download the ResNet 50 pre-trained model.
        <pre class="programlisting code"><code class="hljs-code">resnet = tf.keras.applications.resnet.ResNet50()
</code></pre>
      </li>
      <li class="numbered">Then, we'll display the intermediate layers of the model by querying the graph data structure:
        <pre class="programlisting code"><code class="hljs-code">intermediate_layers = [layer.output <span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> resnet.layers]
</code></pre>
      </li>
      <li class="numbered">Then, we'll display the top 10 intermediate layers of the model by querying the graph data structure:
        <pre class="programlisting code"><code class="hljs-code">intermediate_layers[:<span class="hljs-number">10</span>]
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code"> [&lt;tf.Tensor 'input_7:0' shape=(None, 224, 224, 3) dtype=float32&gt;,
 &lt;tf.Tensor 'conv1_pad/Pad:0' shape=(None, 230, 230, 3) dtype=float32&gt;,
 &lt;tf.Tensor 'conv1_conv/BiasAdd:0' shape=(None, 112, 112, 64) dtype=float32&gt;,
 &lt;tf.Tensor 'conv1_bn/cond/Identity:0' shape=(None, 112, 112, 64) dtype=float32&gt;,
 &lt;tf.Tensor 'conv1_relu/Relu:0' shape=(None, 112, 112, 64) dtype=float32&gt;,
 &lt;tf.Tensor 'pool1_pad/Pad:0' shape=(None, 114, 114, 64) dtype=float32&gt;,
 &lt;tf.Tensor 'pool1_pool/MaxPool:0' shape=(None, 56, 56, 64) dtype=float32&gt;,
 &lt;tf.Tensor 'conv2_block1_1_conv/BiasAdd:0' shape=(None, 56, 56, 64) dtype=float32&gt;,
 &lt;tf.Tensor 'conv2_block1_1_bn/cond/Identity:0' shape=(None, 56, 56, 64) dtype=float32&gt;,
 &lt;tf.Tensor 'conv2_block1_1_relu/Relu:0' shape=(None, 56, 56, 64) dtype=float32&gt;]
</code></pre>
      </li>
      <li class="numbered">Now, we'll select all <a id="_idIndexMarker170"/>the feature <a id="_idIndexMarker171"/>layers. We'll go into the details in the convolution neural network chapter.
        <pre class="programlisting code"><code class="hljs-code">feature_layers = intermediate_layers[:<span class="hljs-number">-2</span>]
</code></pre>
      </li>
      <li class="numbered">Then, we'll reuse the nodes in order to create our feature-extraction model.
        <pre class="programlisting code"><code class="hljs-code">feat_extraction_model = keras.Model(inputs=resnet.input, outputs=feature_layers)
</code></pre>
      </li>
    </ol>
    <p class="normal">One of the interesting benefits of a deep learning model is that it can be reused partly or wholly on similar predictive modeling problems. This <a id="_idIndexMarker172"/>technique is called "transfer learning": it significantly improves the training phase by decreasing the training time and the model's performance on a related problem.</p>
    <p class="normal">The new model architecture is based on one or more layers from a pre-trained model. The weights of the pre-trained model may be used as the starting point for the training process. They <a id="_idIndexMarker173"/>can be either fixed or fine-tuned, or <a id="_idIndexMarker174"/>totally adapted during the learning phase. The two main approaches to implement transfer learning are weight initialization and feature extraction. Don't worry, we'll go into the details later in this book.</p>
    <p class="normal">In this recipe, we have loaded a pretrained model based on the VGG19 architecture. We have extracted nodes from this model and reused them in a new model.</p>
    <h2 id="_idParaDest-108" class="title">How it works... </h2>
    <p class="normal">The Keras Sequential API is <a id="_idIndexMarker175"/>appropriate in the vast majority of cases but is limited to creating layer-by-layer models. The Functional API is more flexible and allows extracting and reusing nodes, sharing layers, and creating non-linear models with multiple inputs and multiple outputs. Note that many high-performing networks are based on a non-linear topology.</p>
    <p class="normal">In this recipe, we have learned how to build models using the Keras Functional API. These models are trained and evaluated by the same <code class="Code-In-Text--PACKT-">compile</code>, <code class="Code-In-Text--PACKT-">fit</code>, <code class="Code-In-Text--PACKT-">evaluate</code>, and <code class="Code-In-Text--PACKT-">predict</code> methods used by the Keras Sequential model.</p>
    <p class="normal">We have also viewed how to reuse trained models as a layer, how to share layers, and also how to extract and reuse nodes. This last approach is used in transfer learning techniques that speed up training and improve performance.</p>
    <h2 id="_idParaDest-109" class="title">There's more...</h2>
    <p class="normal">As we can access every layer, models built with the Keras Functional API have specific features such as model plotting, whole-model saving, etc.</p>
    <p class="normal">Models built with the Functional API could be complex, so here are some tips to consider to avoid pulling your hair out during the process:</p>
    <ul>
      <li class="bullet">Name the layers: It will be quite useful when we display summaries and plots of the model graph.</li>
      <li class="bullet">Separate submodels: Consider each submodel as being like a Lego brick that we will combine together with the others at the end.</li>
      <li class="bullet">Review the layer summary: Use the <code class="Code-In-Text--PACKT-">summary</code> method to check the outputs of each layer.</li>
      <li class="bullet">Review graph plots: Use the <code class="Code-In-Text--PACKT-">plot</code> method to display and check the connection between the layers.</li>
      <li class="bullet">Consistent variable names: Use the same variable name for the input and output layers. It avoids copy-paste mistakes.</li>
    </ul>
    <h2 id="_idParaDest-110" class="title">See also</h2>
    <p class="normal">For some references on the <a id="_idIndexMarker176"/>Keras Functional API, visit the following websites:</p>
    <ul>
      <li class="bullet">Keras Functional API documentation: <a href="https://keras.io/getting-started/functional-api-guide/"><span class="url">https://keras.io/getting-started/functional-api-guide/</span></a></li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">tf.keras.Model</code> API: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/Model "><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/Model</span></a></li>
      <li class="bullet">Machine Learning Mastery: <a href="https://machinelearningmastery.com/keras-functional-api-deep-learning/"><span class="url">https://machinelearningmastery.com/keras-functional-api-deep-learning/</span></a></li>
      <li class="bullet">Inside TensorFlow: tf.Keras by François Chollet (part1) <a href="https://www.youtube.com/watch?v=UYRBHFAvLSs"><span class="url">https://www.youtube.com/watch?v=UYRBHFAvLSs</span></a></li>
      <li class="bullet">Inside TensorFlow: tf.Keras (part2) <a href="https://www.youtube.com/watch?v=uhzGTijaw8A"><span class="url">https://www.youtube.com/watch?v=uhzGTijaw8A</span></a></li>
    </ul>
    <h1 id="_idParaDest-111" class="title">Using the Keras Subclassing API</h1>
    <p class="normal">Keras is based on <a id="_idIndexMarker177"/>object-oriented design principles. So, we can subclass the <code class="Code-In-Text--PACKT-">Model</code> class and create our model architecture definition.</p>
    <p class="normal">The Keras Subclassing API is the third way proposed by Keras to build deep neural network models.</p>
    <p class="normal">This API is fully customizable, but this flexibility also brings complexity! So, hold on to your hats, it's harder to use than the Sequential or Functional API.</p>
    <p class="normal">But you're probably wondering why we need this API if it's so hard to use. Some model architectures and some custom layers can be extremely challenging. Some researchers and some developers hope to have full control of their models and the way to train them. The Subclassing API provides these features. Let's go into the details.</p>
    <h2 id="_idParaDest-112" class="title">Getting ready </h2>
    <p class="normal">Here, we will cover the main ways of creating a custom layer and a custom model using the Keras Subclassing API.</p>
    <p class="normal">To start, we load TensorFlow, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
</code></pre>
    <p class="normal">We are ready to <a id="_idIndexMarker178"/>proceed with an explanation of how to do it.</p>
    <h2 id="_idParaDest-113" class="title">How to do it... </h2>
    <p class="normal">Let's start by creating our layer.</p>
    <h3 id="_idParaDest-114" class="title">Creating a custom layer</h3>
    <p class="normal">As explained <a id="_idIndexMarker179"/>in the <em class="italic">Understanding Keras layers</em> section, Keras provides various built-in layers such as dense, convolutional, recurrent, and normalization layers through its layered API.</p>
    <p class="normal">All layers are subclasses of the <code class="Code-In-Text--PACKT-">Layer</code> class and implement these methods:</p>
    <ul>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">build</code> method, which defines the weights of the layer.</li>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">call</code> method, which specifies the transformation from inputs to outputs done by the layer.</li>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">compute_output_shape</code> method, if the layer modifies the shape of its input. This allows Keras to perform automatic shape inference.</li>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">get_config</code> and <code class="Code-In-Text--PACKT-">from_config</code> methods, if the layer is serialized and deserialized.</li>
    </ul>
    <ol>
      <li class="numbered" value="1">Let's put the theory into action. First, we'll make a subclass layer for a custom dense layer:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">MyCustomDense</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.layers.Layer</span><span class="hljs-class">):</span>
    <span class="hljs-comment"># Initialize this class with the number of units</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, units</span><span class="hljs-function">):</span>
        super(MyCustomDense, self).__init__()
        self.units = units
 
    <span class="hljs-comment"># Define the weights and the bias</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build</span><span class="hljs-function">(</span><span class="hljs-params">self, input_shape</span><span class="hljs-function">):</span>
        self.w = self.add_weight(shape=(input_shape[<span class="hljs-number">-1</span>], self.units),
                            initializer=<span class="hljs-string">'random_normal'</span>,
                            trainable=<span class="hljs-literal">True</span>)
        self.b = self.add_weight(shape=(self.units,),
                            initializer=<span class="hljs-string">'random_normal'</span>,
                            trainable=<span class="hljs-literal">True</span>)
 
    <span class="hljs-comment"># Applying this layer transformation to the input tensor</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, inputs</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">return</span> tf.matmul(inputs, self.w) + self.b
    
    <span class="hljs-comment"># Function to retrieve the configuration</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_config</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">return</span> {<span class="hljs-string">'units'</span>: self.units}
</code></pre>
      </li>
      <li class="numbered">Then, we'll <a id="_idIndexMarker180"/>create a model using the <code class="Code-In-Text--PACKT-">MyCustomDense</code> layer created in the previous step:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Create an input layer</span>
inputs = keras.Input((<span class="hljs-number">12</span>,<span class="hljs-number">4</span>))
<span class="hljs-comment"># Add an instance of MyCustomeDense layer</span>
outputs = MyCustomDense(<span class="hljs-number">2</span>)(inputs)
<span class="hljs-comment"># Create a model</span>
model = keras.Model(inputs, outputs)
<span class="hljs-comment"># Get the model config</span>
config = model.get_config()
</code></pre>
      </li>
      <li class="numbered">Next, we will reload the model from the config:
        <pre class="programlisting code"><code class="hljs-code">new_model = keras.Model.from_config(config, 
                              custom_objects={<span class="hljs-string">'MyCustomDense'</span>: MyCustomDense})
</code></pre>
      </li>
    </ol>
    <p class="normal">In this recipe, we have created our <code class="Code-In-Text--PACKT-">Layer</code> class. Now, we'll create our model.</p>
    <h3 id="_idParaDest-115" class="title">Creating a custom model</h3>
    <p class="normal">By subclassing <a id="_idIndexMarker181"/>the <code class="Code-In-Text--PACKT-">tf.keras.Model</code> class, we can build a fully customizable model. </p>
    <p class="normal">We define our layers in the <code class="Code-In-Text--PACKT-">__init__</code> method, and we can have full, complete control over the forward pass of the model by implementing the <code class="Code-In-Text--PACKT-">call</code> method. The <code class="Code-In-Text--PACKT-">training</code> Boolean argument can be used to specify different behavior during the training or inference phase.</p>
    <ol>
      <li class="numbered" value="1">First, we will load the MNIST dataset and normalize the grayscale:
        <pre class="programlisting code"><code class="hljs-code">mnist = tf.keras.datasets.mnist
(X_mnist_train, y_mnist_train), (X_mnist_test, y_mnist_test) = mnist.load_data()
train_mnist_features = X_mnist_train/<span class="hljs-number">255</span>
test_mnist_features = X_mnist_test/<span class="hljs-number">255</span>
</code></pre>
      </li>
      <li class="numbered">Let's go and make a subclass <code class="Code-In-Text--PACKT-">Model</code> for recognizing MNIST data:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">MyMNISTModel</span><span class="hljs-class">(</span><span class="hljs-params">tf.keras.Model</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, num_classes</span><span class="hljs-function">):</span>
        super(MyMNISTModel, self).__init__(name=<span class="hljs-string">'my_mnist_model'</span>)
        self.num_classes = num_classes
        self.flatten_1 = tf.keras.layers.Flatten()
        self.dropout = tf.keras.layers.Dropout(<span class="hljs-number">0.1</span>)
        self.dense_1 = tf.keras.layers.Dense(<span class="hljs-number">50</span>, activation=<span class="hljs-string">'relu'</span>)
        self.dense_2 = tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">call</span><span class="hljs-function">(</span><span class="hljs-params">self, inputs, training=False</span><span class="hljs-function">):</span>
        x = self.flatten_1(inputs)
        <span class="hljs-comment"># Apply dropout only during the training phase</span>
        x = self.dense_1(x)
        <span class="hljs-keyword">if</span> training:
            x = self.dropout(x, training=training)
        <span class="hljs-keyword">return</span> self.dense_2(x)
</code></pre>
      </li>
      <li class="numbered">Now, we <a id="_idIndexMarker182"/>are going to instantiate the model and process the training:
        <pre class="programlisting code"><code class="hljs-code">my_mnist_model = MyMNISTModel(<span class="hljs-number">10</span>)
<span class="hljs-comment"># Compile</span>
my_mnist_model.compile(optimizer=<span class="hljs-string">'sgd'</span>,
                      loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>,
                      metrics=[<span class="hljs-string">'accuracy'</span>])
<span class="hljs-comment"># Train</span>
my_mnist_model.fit(train_features, y_train,
                  validation_data=(test_features, y_test),
                  epochs=<span class="hljs-number">10</span>)
</code></pre>
      </li>
    </ol>
    <h2 id="_idParaDest-116" class="title">How it works... </h2>
    <p class="normal">The Subclassing API is a way for deep learning practitioners to build their layers or models using object-oriented <a id="_idIndexMarker183"/>Keras design principles. We recommend using this API only if your model cannot be achieved using the Sequential or the Functional API. Although this way can be complicated to implement, it remains useful in a few cases, and it is interesting for all developers and researchers to know how layers and models are implemented in Keras.</p>
    <h2 id="_idParaDest-117" class="title">See also</h2>
    <p class="normal">For some references on the <a id="_idIndexMarker184"/>Keras Subclassing API, see the following tutorials, papers, and articles:</p>
    <ul>
      <li class="bullet">Writing custom layers and models with Keras: <a href="https://www.tensorflow.org/guide/keras/custom_layers_and_models"><span class="url">https://www.tensorflow.org/guide/keras/custom_layers_and_models</span></a></li>
      <li class="bullet">Writing your own Keras layers: <a href="https://keras.io/layers/writing-your-own-keras-layers/"><span class="url">https://keras.io/layers/writing-your-own-keras-layers/</span></a></li>
    </ul>
    <h1 id="_idParaDest-118" class="title">Using the Keras Preprocessing API</h1>
    <p class="normal">The Keras Preprocessing <a id="_idIndexMarker185"/>API gathers modules for data processing and data augmentation. This API provides utilities for working with sequence, text, and image data. Data preprocessing is an essential step in machine learning and deep learning. It converts, transforms, or encodes raw data into an understandable, useful, and efficient format for learning algorithms.</p>
    <h2 id="_idParaDest-119" class="title">Getting ready </h2>
    <p class="normal">This recipe will cover some preprocessing methods provided by Keras for sequence, text, and image data.</p>
    <p class="normal">As usual, we just need to import TensorFlow as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.sequence <span class="hljs-keyword">import</span> TimeseriesGenerator, pad_sequences, skipgrams, make_sampling_table
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing.text <span class="hljs-keyword">import</span> text_to_word_sequence, one_hot, hashing_trick, Tokenizer
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense
</code></pre>
    <p class="normal">We are ready to <a id="_idIndexMarker186"/>proceed with an explanation of how to do it.</p>
    <h2 id="_idParaDest-120" class="title">How to do it... </h2>
    <p class="normal">Let's start with the sequence data.</p>
    <h3 id="_idParaDest-121" class="title">Sequence preprocessing</h3>
    <p class="normal">Sequence data is data <a id="_idIndexMarker187"/>where the order matters, such as text or a time series. So, a time series is defined by a series of data points ordered by time.</p>
    <h4 class="title">Time series generator</h4>
    <p class="normal">Keras provides utilities <a id="_idIndexMarker188"/>for preprocessing sequence data such as time series data. It takes in consecutive data points and applies transformations using time series parameters such as stride, length of history, etc., to return a TensorFlow dataset instance.</p>
    <ol>
      <li class="numbered" value="1">Let's go with a toy time series dataset of 10 integer values:
        <pre class="programlisting code"><code class="hljs-code">series = np.array([i <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)])
print(series)
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</code></pre>
      </li>
      <li class="numbered">We want to predict the next value from the last five lag observations. So, we'll define a generator with the <code class="Code-In-Text--PACKT-">length</code> argument set to 5. This argument specifies the length of the output sequences in a number of timesteps:
        <pre class="programlisting code"><code class="hljs-code">generator = TimeseriesGenerator(data = series,
                               targets = series,
                               length=<span class="hljs-number">5</span>,
                               batch_size=<span class="hljs-number">1</span>,
                               shuffle=<span class="hljs-literal">False</span>,
                               reverse=<span class="hljs-literal">False</span>)
</code></pre>
      </li>
      <li class="numbered">We want to generate samples composed of 5 lag observations for one prediction and the toy time series dataset contains 10 values. So, the number of samples generated is 5:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># number of samples</span>
print(<span class="hljs-string">'Samples: %d'</span> % len(generator))
</code></pre>
      </li>
      <li class="numbered">Then, we'll <a id="_idIndexMarker189"/>display the inputs and output of each sample and check that the data is well prepared:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(generator)):
    x, y = generator[i]
    print(<span class="hljs-string">'%s =&gt; %s'</span> % (x, y))
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">[[0 1 2 3 4]] =&gt; [5]
[[1 2 3 4 5]] =&gt; [6]
[[2 3 4 5 6]] =&gt; [7]
[[3 4 5 6 7]] =&gt; [8]
[[4 5 6 7 8]] =&gt; [9]
</code></pre>
      </li>
      <li class="numbered">Now, we'll create and compile a model:
        <pre class="programlisting code"><code class="hljs-code">model = Sequential()
model.add(Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'relu'</span>, input_dim=<span class="hljs-number">5</span>))
model.add(Dense(<span class="hljs-number">1</span>))
model.compile(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mse'</span>)
</code></pre>
      </li>
      <li class="numbered">And we'll train the model by giving the generator as input data:
        <pre class="programlisting code"><code class="hljs-code">model.fit(generator, epochs=<span class="hljs-number">10</span>)
</code></pre>
      </li>
    </ol>
    <p class="normal">Preparing time series data for modeling with deep learning methods can be very challenging. But fortunately, Keras provides a generator that will help us transform a univariate or multivariate time series dataset into a data structure ready to train models. This generator offers many options to prepare the data, such as the shuffle, the sampling rate, the start <a id="_idIndexMarker190"/>and end offsets, etc. We recommend consulting the official Keras API to get more details.</p>
    <p class="normal">Now, we'll focus on how to prepare data for variable-length input sequences.</p>
    <h4 class="title">Padding sequences</h4>
    <p class="normal">When processing <a id="_idIndexMarker191"/>sequence data, each sample often has different lengths. In order for all the sequences to fit the desired length, the solution is to pad them. Sequences shorter than the defined sequence length are padded with values at the end (by default) or the beginning of each sequence. Otherwise, if the sequence is greater than the desired length, the sequence is truncated.</p>
    <ol>
      <li class="numbered" value="1">Let's start with four sentences:
        <pre class="programlisting code"><code class="hljs-code">sentences = [[<span class="hljs-string">"What"</span>, <span class="hljs-string">"do"</span>, <span class="hljs-string">"you"</span>, <span class="hljs-string">"like"</span>, <span class="hljs-string">"?"</span>],
             [<span class="hljs-string">"I"</span>, <span class="hljs-string">"like"</span>, <span class="hljs-string">"basket-ball"</span>, <span class="hljs-string">"!"</span>],
             [<span class="hljs-string">"And"</span>, <span class="hljs-string">"you"</span>, <span class="hljs-string">"?"</span>],
             [<span class="hljs-string">"I"</span>, <span class="hljs-string">"like"</span>, <span class="hljs-string">"coconut"</span>, <span class="hljs-string">"and"</span>, <span class="hljs-string">"apple"</span>]]
</code></pre>
      </li>
      <li class="numbered">First, we'll build the vocabulary lookup table. We'll create two dictionaries to go from the words to integer identifiers and vice versa.
        <pre class="programlisting code"><code class="hljs-code">text_set = set(np.concatenate(sentences))
vocab_to_int = dict(zip(text_set, range(len(text_set))))
int_to_vocab = {vocab_to_int[word]:word <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> vocab_to_int.keys()}
</code></pre>
      </li>
      <li class="numbered">Then after building the vocabulary lookup table, we'll encode the sentences as integer arrays.
        <pre class="programlisting code"><code class="hljs-code">encoded_sentences = []
<span class="hljs-keyword">for</span> sentence <span class="hljs-keyword">in</span> sentences:
    encoded_sentence = [vocab_to_int[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence]
    encoded_sentences.append(encoded_sentence)
encoded_sentences
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">[[8, 4, 7, 6, 0], [5, 6, 2, 3], [10, 7, 0], [5, 6, 1, 9, 11]]
</code></pre>
      </li>
      <li class="numbered">Now, we'll use the <code class="Code-In-Text--PACKT-">pad_sequences</code> function to truncate and pad sequences to a common length easily. The pre-sequence padding is activated by default.
        <pre class="programlisting code"><code class="hljs-code">pad_sequences(encoded_sentences)
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">array([[ 8,  4,  7,  6,  0],
       [ 0,  5,  6,  2,  3],
       [ 0,  0, 10,  7,  0],
       [ 5,  6,  1,  9, 11]], dtype=int32)
</code></pre>
      </li>
      <li class="numbered">Then, we'll activate the post-sequence padding and set the <code class="Code-In-Text--PACKT-">maxlen</code> argument to the desired length – here, 7.
        <pre class="programlisting code"><code class="hljs-code">pad_sequences(encoded_sentences, maxlen = <span class="hljs-number">7</span>)
</code></pre>
      </li>
      <li class="numbered">This results in <a id="_idIndexMarker192"/>the following output:
        <pre class="programlisting code"><code class="hljs-code">array([[ 0,  0,  8,  4,  7,  6,  0],
       [ 0,  0,  0,  5,  6,  2,  3],
       [ 0,  0,  0,  0, 10,  7,  0],
       [ 0,  0,  5,  6,  1,  9, 11]], dtype=int32)
</code></pre>
      </li>
      <li class="numbered">The length of the sequence can also be trimmed to the desired length – here, 3. By default, this function removes timesteps from the beginning of each sequence.
        <pre class="programlisting code"><code class="hljs-code">pad_sequences(encoded_sentences, maxlen = <span class="hljs-number">3</span>)
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">array([[ 7,  6,  0],
       [ 6,  2,  3],
       [10,  7,  0],
       [ 1,  9, 11]], dtype=int32)
</code></pre>
      </li>
      <li class="numbered">Set the truncating argument to <code class="Code-In-Text--PACKT-">post</code> to remove timesteps from the end of each sequence.
        <pre class="programlisting code"><code class="hljs-code">pad_sequences(encoded_sentences, maxlen = <span class="hljs-number">3</span>, truncating=<span class="hljs-string">'post'</span>)
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">array([[ 8,  4,  7],
       [ 5,  6,  2],
       [10,  7,  0],
       [ 5,  6,  1]], dtype=int32)
</code></pre>
      </li>
    </ol>
    <p class="normal">Padding is very useful <a id="_idIndexMarker193"/>when we want all sequences in a list to have the same length.</p>
    <p class="normal">In the next section, we will cover a very popular technique for preprocessing text.</p>
    <h4 class="title">Skip-grams</h4>
    <p class="normal">Skip-grams is one of the <a id="_idIndexMarker194"/>unsupervised learning techniques in natural language processing. It finds the most related words for a given word and predicts the context word for this given word. </p>
    <p class="normal">Keras provides the <code class="Code-In-Text--PACKT-">skipgrams</code> pre-processing function, which takes in an integer-encoded sequence of words and returns the relevance for each pair of words in the defined window. If the pair of words is relevant, the sample is positive, and the associated label is set to 1. Otherwise, the sample is considered negative, and the label is set to 0.</p>
    <p class="normal">An example is better than thousands of words. So, let's take this sentence, <code class="Code-In-Text--PACKT-">"I like coconut and apple,"</code> select the first word as our "context word," and use a window size of two. We make pairs of the context word "I" with the word covered in the specified window. So, we have two pairs of words <code class="Code-In-Text--PACKT-">(I, like)</code> and <code class="Code-In-Text--PACKT-">(I, coconut)</code>, both of which equal <code class="Code-In-Text--PACKT-">1</code>.</p>
    <p class="normal">Let's put the theory into action:</p>
    <ol>
      <li class="numbered" value="1">First, we'll encode a sentence as a list of word indices:
        <pre class="programlisting code"><code class="hljs-code">sentence = <span class="hljs-string">"I like coconut and apple"</span>
encoded_sentence = [vocab_to_int[word] <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> sentence.split()]
vocabulary_size = len(encoded_sentence)
</code></pre>
      </li>
      <li class="numbered">Then, we'll call the <code class="Code-In-Text--PACKT-">skipgrams</code> function with a window size of 1:
        <pre class="programlisting code"><code class="hljs-code">pairs, labels = skipgrams(encoded_sentence, 
                          vocabulary_size, 
                          window_size=<span class="hljs-number">1</span>,
                          negative_samples=<span class="hljs-number">0</span>)
</code></pre>
      </li>
      <li class="numbered">Now, we'll print the results:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(len(pairs)):
    print(<span class="hljs-string">"({:s} , {:s} ) -&gt; {:d}"</span>.format(
          int_to_vocab[pairs[i][<span class="hljs-number">0</span>]], 
          int_to_vocab[pairs[i][<span class="hljs-number">1</span>]], 
          labels[i]))
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">(coconut , and ) -&gt; 1
(apple , ! ) -&gt; 0
(and , coconut ) -&gt; 1
(apple , and ) -&gt; 1
(coconut , do ) -&gt; 0
(like , I ) -&gt; 1
(and , apple ) -&gt; 1
(like , coconut ) -&gt; 1
(coconut , do ) -&gt; 0
(I , like ) -&gt; 1
(coconut , like ) -&gt; 1
(and , do ) -&gt; 0
(like , coconut ) -&gt; 0
(I , ! ) -&gt; 0
(like , ! ) -&gt; 0
(and , coconut ) -&gt; 0
</code></pre>
      </li>
    </ol>
    <p class="normal">Note that the <a id="_idIndexMarker195"/>non-word is defined by index 0 in the vocabulary and will be skipped. We recommend that readers consult the Keras API to find more details about padding.</p>
    <p class="normal">Now, let's introduce some tips to preprocess text data.</p>
    <h3 id="_idParaDest-122" class="title">Text preprocessing</h3>
    <p class="normal">In deep learning, we <a id="_idIndexMarker196"/>cannot feed raw text directly into our network. We have to encode our text as numbers and provide integers as input. Our model will generate integers as output. This module provides utilities for preprocessing text input.</p>
    <h4 class="title">Split text to word sequence</h4>
    <p class="normal">Keras provides <a id="_idIndexMarker197"/>the <code class="Code-In-Text--PACKT-">text_to_word_sequence</code> method, which transforms a sequence into a list of words or tokens.</p>
    <ol>
      <li class="numbered" value="1">Let's go with this sentence:
        <pre class="programlisting code"><code class="hljs-code">sentence = <span class="hljs-string">"I like coconut , I like apple"</span>
</code></pre>
      </li>
      <li class="numbered">Then, we'll call the method that converts a sentence into a list of words. By default, this method splits the text on whitespace.
        <pre class="programlisting code"><code class="hljs-code">text_to_word_sequence(sentence, lower=<span class="hljs-literal">False</span>) 
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">[<span class="hljs-string">'I'</span>, <span class="hljs-string">'like'</span>, <span class="hljs-string">'coconut'</span>, <span class="hljs-string">'I'</span>, <span class="hljs-string">'like'</span>, <span class="hljs-string">'apple'</span>]
</code></pre>
      </li>
      <li class="numbered">Now, we'll set the <code class="Code-In-Text--PACKT-">lower</code> argument to <code class="Code-In-Text--PACKT-">True</code>, and the text will be converted to lower case:
        <pre class="programlisting code"><code class="hljs-code">text_to_word_sequence(sentence, lower=<span class="hljs-literal">True</span>, filters=[])
</code></pre>
      </li>
      <li class="numbered">This results in the following output:
        <pre class="programlisting code"><code class="hljs-code">['i', 'like', 'coconut', ',', 'i', 'like', 'apple']
</code></pre>
      </li>
    </ol>
    <p class="normal">Note that by default, the <code class="Code-In-Text--PACKT-">filter</code> argument filters out a list of characters such as punctuation. In our last code execution, we removed all the predefined filters.</p>
    <p class="normal">Let's continue with a method to encode words or categorical features.</p>
    <h4 class="title">Tokenizer</h4>
    <p class="normal">The <code class="Code-In-Text--PACKT-">Tokenizer</code> class is <a id="_idIndexMarker198"/>the utility class for text tokenization. It's the preferred approach for preparing text in deep learning.</p>
    <p class="normal">This class takes as inputs:</p>
    <ul>
      <li class="bullet">The maximum number of words to keep. Only the most common words will be kept based on word frequency.</li>
      <li class="bullet">A list of characters to filter out.</li>
      <li class="bullet">A boolean to convert the text into lower case, or not.</li>
      <li class="bullet">The separator for word splitting.</li>
    </ul>
    <ol>
      <li class="numbered" value="1">Let's go with this sentence:
        <pre class="programlisting code"><code class="hljs-code">sentences = [[<span class="hljs-string">"What"</span>, <span class="hljs-string">"do"</span>, <span class="hljs-string">"you"</span>, <span class="hljs-string">"like"</span>, <span class="hljs-string">"?"</span>],
             [<span class="hljs-string">"I"</span>, <span class="hljs-string">"like"</span>, <span class="hljs-string">"basket-ball"</span>, <span class="hljs-string">"!"</span>],
             [<span class="hljs-string">"And"</span>, <span class="hljs-string">"you"</span>, <span class="hljs-string">"?"</span>],
             [<span class="hljs-string">"I"</span>, <span class="hljs-string">"like"</span>, <span class="hljs-string">"coconut"</span>, <span class="hljs-string">"and"</span>, <span class="hljs-string">"apple"</span>]]
</code></pre>
      </li>
      <li class="numbered">Now, we will create a <code class="Code-In-Text--PACKT-">Tokenizer</code> instance and fit it on the previous sentences:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># create the tokenizer</span>
t = Tokenizer()
<span class="hljs-comment"># fit the tokenizer on the documents</span>
t.fit_on_texts(sentences)
</code></pre>
      </li>
      <li class="numbered">The tokenizer creates several pieces of information about the document. We can get a dictionary containing the count for each word.
        <pre class="programlisting code"><code class="hljs-code">print(t.word_counts)
</code></pre>
      </li>
      <li class="numbered">This results in the <a id="_idIndexMarker199"/>following outputs:
        <pre class="programlisting code"><code class="hljs-code">OrderedDict([('what', 1), ('do', 1), ('you', 2), ('like', 3), ('?', 2), ('i', 2), ('basket-ball', 1), ('!', 1), ('and', 2), ('coconut', 1), ('apple', 1)])
</code></pre>
      </li>
      <li class="numbered">We can also get a dictionary containing, for each word, the number of documents in which it appears:
        <pre class="programlisting code"><code class="hljs-code">print(t.document_count)
</code></pre>
      </li>
      <li class="numbered">This results in the following outputs:
        <pre class="programlisting code"><code class="hljs-code">4
</code></pre>
      </li>
      <li class="numbered">A dictionary contains, for each word, its unique integer identifier:
        <pre class="programlisting code"><code class="hljs-code">print(t.word_index)
</code></pre>
      </li>
      <li class="numbered">This results in the following outputs:
        <pre class="programlisting code"><code class="hljs-code">{'like': 1, 'you': 2, '?': 3, 'i': 4, 'and': 5, 'what': 6, 'do': 7, 'basket-ball': 8, '!': 9, 'coconut': 10, 'apple': 11}
</code></pre>
      </li>
      <li class="numbered">The number of unique documents that were used to fit the <code class="Code-In-Text--PACKT-">Tokenizer</code>.
        <pre class="programlisting code"><code class="hljs-code">print(t.word_docs)
</code></pre>
      </li>
      <li class="numbered">This results in the following outputs:
        <pre class="programlisting code"><code class="hljs-code">defaultdict(&lt;class 'int'&gt;, {'do': 1, 'like': 3, 'what': 1, 'you': 2, '?': 2, '!': 1, 'basket-ball': 1, 'i': 2, 'and': 2, 'coconut': 1, 'apple': 1})
</code></pre>
      </li>
      <li class="numbered">Now, we are ready to encode our documents, thanks to the <code class="Code-In-Text--PACKT-">texts_to_matrix</code> function. This function provides four different document encoding schemes to compute the coefficient for each token.<p class="bullet-para">Let's start with the binary mode, which returns whether or not each token is present in the document.</p>
        <pre class="programlisting code"><code class="hljs-code">t.texts_to_matrix(sentences, mode=<span class="hljs-string">'binary'</span>)
</code></pre>
      </li>
      <li class="numbered">This results in the <a id="_idIndexMarker200"/>following outputs:
        <pre class="programlisting code"><code class="hljs-code"> [[0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1.]]
</code></pre>
      </li>
      <li class="numbered">The <code class="Code-In-Text--PACKT-">Tokenizer</code> API offers another mode based on word count – it returns the count of each word in the document:
        <pre class="programlisting code"><code class="hljs-code">t.texts_to_matrix(sentences, mode=<span class="hljs-string">'count'</span>)
</code></pre>
      </li>
      <li class="numbered">This results in the following outputs:
        <pre class="programlisting code"><code class="hljs-code">[[0. 1. 1. 1. 0. 0. 1. 1. 0. 0. 0. 0.]
 [0. 1. 0. 0. 1. 0. 0. 0. 1. 1. 0. 0.]
 [0. 0. 1. 1. 0. 1. 0. 0. 0. 0. 0. 0.]
 [0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1.]]
</code></pre>
      </li>
    </ol>
    <p class="normal">Note that we can also use the <code class="Code-In-Text--PACKT-">tfidf</code> mode or the frequency mode. The first returns the term frequency-inverse document frequency score for each word, and the second returns the frequency of each word in the document related to the total number of words in the document.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">Tokenizer</code> API can fit the training dataset and encode text data in the training, validation, and test datasets.</p>
    <p class="normal">In this section, we have covered a few techniques to prepare text data before training and prediction.</p>
    <p class="normal">Now, let's go on to prepare and augment images.</p>
    <h3 id="_idParaDest-123" class="title">Image preprocessing</h3>
    <p class="normal">The data preprocessing <a id="_idIndexMarker201"/>module provides a set of tools for real-time data augmentation on image data.</p>
    <p class="normal">In deep learning, the performance of a neural network is often improved by the number of examples available in the training dataset.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">ImageDataGenerator</code> class in the Keras preprocessing API allows the creation of new data from the training dataset. It isn't applied to the validation or test dataset because it aims to expand the number of examples in the training datasets with plausible new images. This <a id="_idIndexMarker202"/>technique is called data augmentation. Beware not to confuse data preparation with data normalization or image resizing, which is applied to all data in interaction with the model. Data augmentation includes many transformations from the field of image manipulation, such as rotation, horizontal and vertical shift, horizontal and vertical flip, brightness, and much more.</p>
    <p class="normal">The strategy may differ depending on the task to realize. For example, in the MNIST dataset, which contains images of handwritten digits, applying a horizontal flip doesn't make sense. Except for the figure 8, this transformation isn't appropriate. </p>
    <p class="normal">While in the case of a baby picture, applying this kind of transformation makes sense because the image could have been taken from the left or right.</p>
    <ol>
      <li class="numbered" value="1">Let's put the theory into action and perform a data augmentation on the <code class="Code-In-Text--PACKT-">CIFAR10</code> dataset. We will start by downloading the <code class="Code-In-Text--PACKT-">CIFAR</code> dataset.
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Load CIFAR10 Dataset</span>
(x_cifar10_train, y_cifar10_train), (x_cifar10_test, y_cifar10_test) = tf.keras.datasets.cifar10.load_data()
</code></pre>
      </li>
      <li class="numbered">Now, we'll create an image data generator that applies a horizontal flip, a random rotation between 0 and 15, and a shift of 3 pixels on the width and on the height.
        <pre class="programlisting code"><code class="hljs-code">datagen = tf.keras.preprocessing.image.ImageDataGenerator(
    rotation_range=<span class="hljs-number">15</span>,
    width_shift_range=<span class="hljs-number">3</span>,
    height_shift_range=<span class="hljs-number">3</span>,
    horizontal_flip=<span class="hljs-literal">True</span>)
</code></pre>
      </li>
      <li class="numbered">Create an iterator on the train dataset.
        <pre class="programlisting code"><code class="hljs-code">it= datagen.flow(x_cifar10_train, y_cifar10_train, batch_size = <span class="hljs-number">32</span>)
</code></pre>
      </li>
      <li class="numbered">Create a model and compile it.
        <pre class="programlisting code"><code class="hljs-code">model = tf.keras.models.Sequential([
   tf.keras.layers.Conv2D(filters=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"relu"</span>, input_shape=[<span class="hljs-number">32</span>, <span class="hljs-number">32</span>, <span class="hljs-number">3</span>]),
   tf.keras.layers.Conv2D(filters=<span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"relu"</span>),
   tf.keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>),
   tf.keras.layers.Conv2D(filters=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"relu"</span>),
   tf.keras.layers.Conv2D(filters=<span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-string">"same"</span>, activation=<span class="hljs-string">"relu"</span>),
   tf.keras.layers.MaxPool2D(pool_size=<span class="hljs-number">2</span>),
   tf.keras.layers.Flatten(),
   tf.keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">"relu"</span>),
   tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">"softmax"</span>)
])
model.compile(loss=<span class="hljs-string">"sparse_categorical_crossentropy"</span>,
             optimizer=tf.keras.optimizers.SGD(lr=<span class="hljs-number">0.01</span>),
             metrics=[<span class="hljs-string">"accuracy"</span>])
</code></pre>
      </li>
      <li class="numbered">And process the <a id="_idIndexMarker203"/>training by calling the <code class="Code-In-Text--PACKT-">fit</code> method. Take care to set the <code class="Code-In-Text--PACKT-">step_per_epoch</code> argument, which specifies the number of sample batches comprising an epoch.
        <pre class="programlisting code"><code class="hljs-code">history = model.fit(it, epochs=<span class="hljs-number">10</span>,
                    steps_per_epoch=len(x_cifar10_train) / <span class="hljs-number">32</span>,
                    validation_data=(x_cifar10_test,                                           y_cifar10_test))
</code></pre>
      </li>
    </ol>
    <p class="normal">With the image data generator, we have extended the size of our original dataset by creating new images. With more images, the training of a deep learning model can be improved.</p>
    <h2 id="_idParaDest-124" class="title">How it works... </h2>
    <p class="normal">The Keras <a id="_idIndexMarker204"/>Preprocessing API allows transforming, encoding, and augmenting data for neural networks. It makes it easier to work with sequence, text, and image data.</p>
    <p class="normal">First, we introduced the Keras Sequence Preprocessing API. We used the time series generator to transform a univariate or multivariate time series dataset into a data structure ready to train models. Then, we focused on the data preparation for variable-length input sequences, aka padding. And we finished this first part with the skip-gram technique, which finds the most related words for a given word and predicts the context word for that given word.</p>
    <p class="normal">Then, we covered the Keras Text Preprocessing API, which offers a complete turnkey solution to process natural language. We learned how to split text into words and tokenize the words using binary, word count, <code class="Code-In-Text--PACKT-">tfidf</code>, or frequency mode.</p>
    <p class="normal">Finally, we focused on the Image Preprocessing API using the <code class="Code-In-Text--PACKT-">ImageDataGenerator</code>, which is a real advantage to increase the size of your training dataset and to work with images.</p>
    <h2 id="_idParaDest-125" class="title">See also</h2>
    <p class="normal">For some <a id="_idIndexMarker205"/>references on the Keras Preprocessing API, visit the following websites:</p>
    <ul>
      <li class="bullet">Sequence Preprocessing Keras API: <a href="http://keras.io/preprocessing/sequence/"><span class="url">http://keras.io/preprocessing/sequence/</span></a></li>
      <li class="bullet">Text Processing Keras API: <a href="https://keras.io/preprocessing/text/"><span class="url">https://keras.io/preprocessing/text/</span></a></li>
      <li class="bullet">More about syntactic and semantic word similarities: Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean. (2013). Efficient Estimation of Word Representations in Vector Space <a href="https://arxiv.org/pdf/1301.3781v3.pdf"><span class="url">https://arxiv.org/pdf/1301.3781v3.pdf</span></a></li>
      <li class="bullet">Image Preprocessing Keras API: <a href="http://keras.io/preprocessing/image/"><span class="url">http://keras.io/preprocessing/image/</span></a></li>
      <li class="bullet">More examples of data image augmentation: <a href="https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/"><span class="url">https://machinelearningmastery.com/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks/</span></a></li>
    </ul>
  </div>
</body></html>