- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improving the Model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of modeling in machine learning is to ensure our model generalizes
    well on unseen data. Throughout our journey as data professionals who build models
    with neural networks, we are likely to come across two main issues: underfitting
    and overfitting. **Underfitting** is a scenario in which our model lacks the necessary
    complexity to capture underlying patterns in our data, while **overfitting** occurs
    when our model is too complex such that it not only learns the patterns but also
    picks up noise and outliers in our training data. In this case, our model performs
    exceptionally well on training data but fails to generalize well on unseen data.
    [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification with Neural
    Networks*, examined the science behind neural networks. Here, we will explore
    the art of fine-tuning neural networks to build optimally performing models for
    image classification. We will explore various network settings in a hands-on fashion
    to gain an understanding of the impact of each of these settings (hyperparameters)
    on our model’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond exploring the art of hyperparameter tuning, we will also explore various
    ways of improving our data quality using data normalization, data augmentation,
    and the use of synthetic data to improve the generalization capabilities of our
    model. In the past, there was a lot of emphasis on building complex networks.
    However, in more recent times, there has been an increased interest in enhancing
    the performance of neural networks using data-centric strategies. The use of these
    data-centric strategies does not erode the need for careful model design; rather,
    we can look at them as complementary strategies working in tandem toward a desired
    goal, thus enhancing our ability to build optimal models with good generalization
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data is key
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning hyperparameters of a neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be equipped to effectively navigate the
    challenges posed by overfitting and underfitting using a combination of model-centric
    and data-centric ideas when building models with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using a `python >= 3.8.0`, along with the following packages that
    can be installed using the `pip` `install` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`tensorflow>=2.7.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tensorflow-datasets==4.4.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas==1.3.4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy==1.21.4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data is key
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When it comes to improving the performance of a neural network, or any other
    machine learning model for that matter, the importance of good data preparation
    cannot be overemphasized. In [*Chapter 3*](B18118_03.xhtml#_idTextAnchor065),
    *Linear Regression with TensorFlow*, we saw the impact that normalizing our data
    had on the model’s performance. Beyond data normalization, there are other data
    preparation techniques that can make a difference in our modeling process.
  prefs: []
  type: TYPE_NORMAL
- en: As you must have recognized by now, machine learning requires investigating,
    experimenting, and applying different techniques, depending on the problem at
    hand. To ensure we have an optimally performing model, our journey should start
    by looking at our data thoroughly. Do we have enough representative samples from
    each of the target classes? Is our data balanced? Have we ensured the absence
    of incorrect labels? Do we have the right type of data? How are we dealing with
    missing data? These are some of the questions we have to ask and handle before
    the modeling phase.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the quality of our data is a multi-faceted endeavor involving different
    techniques, such as engineering new features from existing ones in our data by
    applying some data preprocessing techniques such as data normalization. When we
    are working with imbalanced datasets, where we are short of representative samples
    of the minority class, the logical thing would be to gather more data on the minority
    class; however, this is not practical in all instances. In such cases, synthetic
    data may be an effective alternative. Start-ups such as **Anyverse.ai** and **Datagen.tech**
    focus on synthetic data development, thus making it possible to mitigate issues
    around data imbalance and data scarcity. However, synthetic data may be costly,
    and it is important that we do a cost-benefit analysis before embarking on this
    route.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another problem we could face is when our collected samples are not representative
    enough for our model to function properly. Imagine you train your model to recognize
    human faces. You gather thousands of images of human faces and split your data
    into training and test sets. You train your model, and it predicts perfectly on
    your test set. However, when you ship this model as a product to the open market,
    you get a result such as *Figure 6**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The need for data augmentation](img/B18118_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – The need for data augmentation
  prefs: []
  type: TYPE_NORMAL
- en: Surprising, right? Even though you trained the model on thousands of images,
    the model failed to learn to recognize faces if the axis is flipped vertically,
    horizontally, or otherwise. To mitigate this type of problem, we employ a technique
    called data augmentation. Data augmentation is a technique that we use to create
    new training data by altering the existing data in some way, such as randomly
    cropping, zooming in or out, or rotating or flipping the initial image. The underlying
    idea behind data augmentation is to enable our model to recognize an object in
    the image, even under unpredictable conditions such as the ones we saw in *Figure
    6**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation is useful when we want more data samples from a limited training
    set; we can use data augmentation to efficiently increase the size of our dataset,
    hence giving our model more data to learn from. Also, because we can simulate
    various scenarios, our model is less likely to overfit as it learns the underlying
    patterns in the data rather than the noise in the data because our model learns
    about the data in multiple ways. Another important benefit of data augmentation
    is that it is a cost-saving technique that saves us from expensive and sometimes
    time-consuming data collection processes. We will be applying data augmentation
    in [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186)*, Handling Overfitting,* where
    we will be working on real-world image classification problems. Also, should you
    find yourself working on image or text data in the future, you may find data augmentation
    a very handy technique to know.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond addressing issues around data imbalance and data diversity, we may also
    want to refine our model itself to make it suitably complex enough to identify
    patterns in the data, and we want to do this without overfitting the model. Here,
    the aim is to improve the model’s quality by tweaking one or more settings, such
    as increasing the number of hidden layers, adding more neurons to each layer,
    changing the optimizer, or using a more sophisticated activation function. These
    settings can be tuned via experimentation until an optimal model is achieved.
  prefs: []
  type: TYPE_NORMAL
- en: We have discussed several ideas behind improving the performance of neural networks.
    Now, let’s see how we can improve the result achieved on the Fashion MNIST dataset
    in [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105), *Image Classification with*
    *Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning hyperparameters of a neural network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to establish a **baseline model** before making any improvements
    in machine learning. A baseline model is a simple model that we can use to evaluate
    the performance of more complex models. In [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105),
    *Image Classification with Neural Networks*, we achieved an accuracy of 88.50%
    on our training data and 85.67% on our test data in just five epochs. In our quest
    to try to improve our model’s performance, we will continue with our three-step
    (*build*, *compile*, and *fit*) process of constructing a neural network using
    **TensorFlow**. In each of the steps we use to build our neural network, there
    are settings that need to be configured before training. These settings are called
    **hyperparameters**. They control how the network will learn and perform, and
    mastering the art of fine-tuning them is an essential step in building successful
    deep learning models. Common hyperparameters include the number of neurons in
    each layer, the number of hidden layers, the learning rate, the activation functions,
    and the number of epochs. By iteratively experimenting with these hyperparameters,
    we can obtain the optimal setting best suited to our use case.
  prefs: []
  type: TYPE_NORMAL
- en: When building real-world models, especially when working on domain-specific
    problems, expert knowledge could prove very useful in pinpointing the best hyperparameter
    values for the task. Let’s return to our notebook and experiment with different
    hyperparameters, and see whether we can beat our baseline model by tweaking one
    or more hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the number of epochs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you are trying to teach a child the multiplication tables; each study
    session you have with the child can be likened to an epoch in machine learning.
    If you only have a small number of study sessions with the child, odds are they
    will not fully grasp the concept of multiplication. Hence, the child will be unable
    to attempt basic multiplication problems. This scenario in machine learning is
    underfitting, where the model hasn’t been able to grasp the underlying patterns
    in the data due to insufficient training.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, let’s say you spend a good amount of time teaching the child
    to memorize specific aspects of the times tables, say the 2s, 3s, and 4s. The
    child becomes skilled at reciting these tables; however, when faced with multiplying
    numbers such as 10 x 8, the child struggles. This happens because rather than
    understanding the principle of multiplication such that the child can apply the
    underlying idea when working with other numbers, the child simply memorized the
    examples learned during the study session. This scenario in machine learning is
    like the concept of overfitting, where our model performs well on the training
    data but fails to generalize well in new situations. In machine learning, when
    training our model, we need to strike a balance such that our model is trained
    well enough to learn the underlying patterns in our data and not to memorize the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what the impact of training the model for longer will have on our
    result. This time, let’s go for 40 epochs and observe what will happen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we change the number of epochs in `Step 3` from `5` to `40`, keeping
    all other hyperparameters of our base model constant. The last five epochs of
    the output are displayed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that, when we increase the number of epochs, it takes a much longer
    time to train our model. So, it can become computationally expensive when we have
    to train for a large number of epochs. After 40 epochs, we see that our model’s
    training accuracy has jumped up to `0.9524`, and it may appear to you that you
    have found a silver bullet for solving this problem. However, our goal is to ensure
    generalization; hence, the acid test for our model is to see how it will perform
    on the unseen data. Let’s check out what the results look like on our test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: When we run the code, we arrive at an accuracy of `0.8692` on our test data.
    We can see that the longer the model is trained, the more accurate the model can
    be on the training data. However, if we train the model for too long, it will
    reach a point of diminishing returns, which is evident when we compare the difference
    in performance between the training and test set accuracy. It is critical to strike
    the right balance of epochs so that the model can learn and improve, but not to
    the point of overfitting on our training data. A practical approach could be to
    start with a small number of epochs, then increase the number of epochs as required.
    This approach can be effective; however, it can also be time-consuming as multiple
    experiments are required to find the optimal number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: What if we could perhaps set a rule to stop training before the point of diminishing
    returns? Yes, this is possible. Let’s examine this idea next and see what difference
    it can make to our result.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping using callbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Early stopping** is a regularization technique that can be used to prevent
    overfitting when training neural networks. When we hardcode the number of epochs
    into our model, we cannot halt training when the desired metric is attained, or
    when training begins to degrade or fails to improve any further. We saw this when
    we increased the number of epochs. However, to mitigate against this scenario,
    TensorFlow provides us with early stopping callbacks such that we can either use
    the in-built callback functions or design our own custom callbacks. We can monitor
    our experiments in real time with more control such that we can halt training
    before our model begins to overfit, when our model stops learning during training,
    or in line with other defined criteria. Early stopping can be invoked at various
    points during training. It can be applied at the start or end of training or based
    on attaining a specific metric.'
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping with built-in callbacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s explore built-in callbacks for early stopping with TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start by importing early stopping from TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we initialize early stopping. TensorFlow allows us to pass in some arguments,
    which we use to create a `callbacks` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s unpack some of the arguments used in our early stopping function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`monitor` can be used to track a metric we want to keep an eye on; in our case,
    we want to keep track of the validation loss. We could also switch it to track
    the validation accuracy. It is a good idea to track your experiment on your validation
    split, hence we set our `callbacks` to monitor the validation loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `patience` argument is set at `5`. This means if there is no progress with
    regards to reducing the validation loss after five epochs, the training will end.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We add the `restore_best_weight` argument and set it to `True`. This allows
    the callback to monitor the entire process and restores the weights from the best
    epoch found during training. If we set `restore_best_weight` to `False`, the model
    weights from the last training step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we set `verbose` to `1`, this ensures we are informed when callback actions
    take place. If we set `verbose` to `0`, training stops but we get no output message.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are a few other arguments that can be used here, but these ones work well
    enough for many instances with regard to applying early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll continue with our three-step approach in which we build, compile, and
    fit the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Step 1` and `Step 2` are the same steps we previously implemented. When building
    out the model, we trained for longer epochs. However, in `Step 3`, we made a few
    tweaks to accommodate our validation split and callbacks. We use 20% of our training
    data for validation and we pass our `callbacks` object into `model.fit()`. This
    ensures our early stopping callbacks interrupt the training when our validation
    loss stops falling. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Because we set `verbose` to `1`, we can see that our experiment ends on epoch
    19\. Now, rather than worrying about how many epochs we need to train effectively,
    we can simply select a large number of epochs and implement early stopping. Next,
    we can also see that because we implemented `restore_best_weights`, the best weights
    are achieved on epoch 14 where we recorded the lowest validation loss (`0.3194`).
    With early stopping, we save compute time and take concrete steps against overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what our test accuracy looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we achieved a test accuracy of `0.8847`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s see how we can write our own custom callbacks to implement early
    stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping with custom callbacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can extend the capabilities of callbacks by writing our own custom callbacks
    for early stopping. This adds flexibility to callbacks so we can implement some
    desired logic during training. The TensorFlow documentation provides several ways
    to do this. Let’s implement a simple callback to track our validation accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, if we want to stop our training process when the model exceeds
    85% accuracy on the validation set, we can do this by crafting our own custom
    callback called `E``arlyStop`, which takes the `tf.keras.callbacks.Callback` parameter.
    We then define a function called `on_epoch_end`, which returns the logs for each
    epoch. We set `self.model.stop_training = True` and once the accuracy exceeds
    85%, training ends and displays a message similar to what we get with `verbose`
    set to `1` when we used built-in callbacks. Now we can pass in our `callback`
    into `model.fit()` as we did with built-in callbacks. We then train our model
    using our three-step approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: This time, at the end of the first epoch, we arrive at over 85% validation accuracy.
    Again, this is a smart way of achieving the desired metrics with minimal use of
    computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good grasp of how to select epochs and apply early stopping,
    let’s now set our sights on other hyperparameters and see whether by tweaking
    one or more of them, we can improve our test accuracy of 88%. Perhaps we can start
    by trying out a more complex model.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see what happens if we add more neurons to our hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Adding neurons in the hidden layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The hidden layers are responsible for the heavy lifting in neural networks,
    as we covered when discussing the anatomy of neural networks in [*Chapter 5*](B18118_05.xhtml#_idTextAnchor105),
    *Image Classification with Neural Networks*. Let’s try out different numbers of
    neurons in the hidden layer. We’ll define a function called `train_model` that
    will allow us to try out different numbers of neurons. The `train_model` function
    takes the `hidden_neurons` argument that represents the number of hidden neurons
    in the model. In addition, the function also takes training images, labels, callbacks,
    validation splits, and epochs. The function builds, compiles, and fits the model
    using these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'To try out a list of neurons, we created a `for` loop to iterate over the neuron
    list called `neuron_values`. Then it applies the `train_model` function to build
    and train a model for each of the neurons in the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The `print` statement returns a message indicating that the model has been
    trained with 1 and 500 neurons respectively. Let’s examine the results when we
    run the function, starting with the hidden layer with one neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'From our result, the model with one neuron in the hidden layer was not complex
    enough to identify patterns in the data. This model performed well below 50%,
    which is a clear case of underfitting. Next, let’s look at the result from the
    model with 500 neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: We can see that the model is overfitting with more neurons. The model recorded
    an accuracy of `0.9303` on the training set but `0.8838` on the test set. In general,
    a larger hidden layer can learn more complex patterns; however, it would require
    more computational resources and be more prone to overfitting. When selecting
    the number of neurons in the hidden layer, it is important to consider the size
    of the training data. If we have a large training sample, we can afford to have
    a large number of neurons in the hidden layer. However, when the training sample
    is quite small, it may be better to consider working with a smaller number of
    neurons in the hidden layer. A larger number of neurons could lead to overfitting,
    as we saw in our experiment, and this architecture may perform even worse than
    a model with a smaller number of neurons in the hidden layer.
  prefs: []
  type: TYPE_NORMAL
- en: Another consideration to bear in mind is the type of data we are working with.
    When we work with linear data, a small number of hidden layers may be sufficient
    for our neural network. However, with non-linear data, we will need a more complex
    model to learn the complexities in the data. Finally, we must bear in mind that
    models with more neurons require longer training time. It is important to consider
    the trade-off between performance and generalization. As a rule of thumb, you
    can start training a model with a small number of neurons. This will train faster
    and avoid overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, we can optimize the number of neurons in the hidden layer by
    identifying neurons that have little or no impact on the performance of the network.
    This approach is called **pruning**. This is outside the scope of the exam, so
    we will stop there.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see the impact of adding more layers to our baseline architecture. So
    far, we have looked at making the model more complex and training for longer.
    How about we try out changing the optimizers? Let’s mix things up a bit and see
    what happens.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the optimizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have used the **Adam optimizer** as our default optimizer; however, there
    are other prominent optimizers and they all have their pros and cons. In this
    book and for your exam, we will focus on Adam, **stochastic gradient descent**
    (**SGD**), and **Root Mean Squared Propagation** (**RMSprop**). RMSprop has low
    memory requirements and offers an adaptive learning rate; on the flip side, it
    takes a much longer time to converge in comparison to Adam and SGD. RMSprop works
    well on training very deep networks such as **recurrent neural networks** (**RNNs**),
    which we will talk about later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, SGD is another popular optimizer; it is simple to implement
    and efficient when the data is sparse. However, it is slow to converge and requires
    careful tuning of the learning rate. If the learning rate is too high, SGD will
    diverge; if the learning rate is too low, SGD will converge very slowly. SGD works
    well on a wide variety of problems and converges faster than other optimizers
    on large datasets, but it could sometimes converge slowly when training very large
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Adam is an improved version of SGD; it has low memory requirements, offers an
    adaptive learning rate, is a very efficient optimizer, and can converge to a good
    solution in fewer iterations than to SGD or RMSprop. Adam is also well suited
    for training large neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try out these three optimizers and see which one works best on our dataset.
    We have changed the optimizer from Adam to RMSprop and SGD, and used the same
    architecture with built-in callbacks. We can see the results in *Figure 6**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Adam** | **RMSProp** | **SGD** |'
  prefs: []
  type: TYPE_TB
- en: '| Number of epochs before early stopping | 13 | 9 | 39 |'
  prefs: []
  type: TYPE_TB
- en: '| Validation accuracy | 0.8867 | 0.8788 | 0.8836 |'
  prefs: []
  type: TYPE_TB
- en: '| Test accuracy | 0.8787 | 0.8749 | 0.8749 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.2 – The performance of different optimizers
  prefs: []
  type: TYPE_NORMAL
- en: Although Adam required more training epochs, its results were marginally better
    than those of the other optimizers. Of course, any of these optimizers can be
    used for this problem. In later chapters, we will work on real-world images that
    are more complex. There, we will revisit these optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Before we close this chapter, let’s look at the learning rate and its impact
    on our model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: Changing the learning rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Learning rate** is an important hyperparameter that controls how well our
    model will learn and improve during training. An optimal learning rate will ensure
    the model converges quickly and accurately, while on the other hand, a poorly
    selected learning rate can lead to a wide range of issues such as slow convergence,
    underfitting, overfitting, or network instability.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand the impact of the learning rate, we need to know how it affects
    the model’s training process. The learning rate is the step size taken to reach
    the point where the loss function is at its minimum. In *Figure 6**.3(a)*, we
    see that when we choose a low learning rate, the model requires too many steps
    to reach the minimum point. On the flip side, when the learning rate is too high,
    the model would likely learn too quickly, taking larger steps and likely overshooting
    the minimum point, as seen in *Figure 6**.3(c)*. A high learning rate can lead
    to instability and overfitting. However, when we find the ideal learning rate
    as in *Figure 6**.3(b)*, the model is likely to experience fast convergence and
    good generalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – A plot showing low, optimal, and high learning rates](img/B18118_06_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – A plot showing low, optimal, and high learning rates
  prefs: []
  type: TYPE_NORMAL
- en: 'The question that comes to mind is: how do we find the optimal learning rate?
    One way is to try out different learning rates and see what works based on evaluating
    the model on the validation set. Another way is to use a learning rate scheduler.
    This allows us to dynamically adjust the learning rate during training. We will
    explore this approach in the later chapters of the book. Here, let’s try out a
    few different learning rates to see how they impact on our network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s craft a function that will take a list of different learning rates. In
    this experiment, we will try out six different learning rates (1, 0.1, 0.01, 0.001,
    0.0001, 0.00001, and 0.000001). First, let’s create a function to create our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use the function to build, compile, and fit the model. It also takes
    in the learning rate as a variable that we pass into our function, and that returns
    the test accuracy as our result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We have now outlined the different learning rates. Here, we want to experiment
    with different learning rates, from a very high to a very low learning rate. We
    created an empty list and appended our test set accuracy. Next, let’s look at
    the numeric values in a tabular fashion. We use `pandas` to generate a DataFrame
    with the learning rate and accuracies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The output DataFrame is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Different learning rates and their test accuracies](img/B18118_06_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Different learning rates and their test accuracies
  prefs: []
  type: TYPE_NORMAL
- en: From the results, we can see that when working with a really high learning rate
    (1.0), the model performs poorly. As we reduce the learning rate value, we see
    that the model’s accuracy begins to rise; when the learning rate becomes too small,
    the model takes too long to converge. There is no silver bullet when it comes
    to choosing the ideal learning rate for a problem. It depends on several factors
    such as the model architecture, the data, and the type of optimization technique
    applied.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have seen various ways of tweaking a model to improve its performance,
    we have come to the end of the chapter. We’ve tried adjusting different hyperparameters
    to improve our model’s performance; however, we got stuck on 88% test accuracy.
    Perhaps this is a good time to try something else, which we will do in the next
    chapter. Take a break, and when you are ready, let’s see how we can improve this
    result and also try out real-world images.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at improving the performance of a neural network.
    Although we worked with a lightweight dataset, we have learned some important
    ideas around improving our model’s performance–ideas that will come in handy,
    both in the exam and on the job. You now know that data quality and model complexity
    are two sides of the machine learning coin. If you have good-quality data, a poor
    model will yield subpar results and, on the flip side, even the most advanced
    model will yield a suboptimal result with bad data.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should have a good understanding and hands-on experience of fine-tuning
    neural networks. Like a seasoned expert, you should be able to understand the
    art of fine-tuning hyperparameters and apply this to different machine learning
    problems and not just image classification. Also, you have seen that model building
    requires a lot of experimenting. There is no silver bullet, but having a good
    understanding of the moving parts and various techniques, and how and why to apply
    them, is what differentiates a star from the average Joe.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine convolutional neural networks. We will
    see why they are state-of-the-art when it comes to image classification tasks.
    We will look at the power of convolutions and examine in a hands-on fashion how
    they do things differently from the simple neural networks we have been using
    so far.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s test what we’ve learned in this chapter using the CIFAR-10 notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: Build a neural network using our three-step approach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the number of neurons from 5 to 100 in the hidden layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a custom callback to stop training when the training accuracy is 90%.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Try out the following learning rates: 5, 0.5, 0.01, 0.001\. What did you observe?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, you can check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Amr, T., 2020\. *Hands-On Machine Learning with scikit-learn and Scientific
    Python Toolkits*, Packt Publishing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gulli, A., Kapoor, A. and Pal, S., 2019\. *Deep Learning with TensorFlow 2 and
    Keras*, Packt Publishing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*How to Write Custom TensorFlow Callbacks — The Easy* *Way*: [https://towardsdatascience.com/how-to-write-custom-tensorflow-callbacks-the-easy-way-c7c4b0e31c1c](https://towardsdatascience.com/how-to-write-custom-tensorflow-callbacks-the-easy-way-c7c4b0e31c1c)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3](https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping](https://medium.com/geekculture/introduction-to-neural-network-2f8b8221fbd3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
