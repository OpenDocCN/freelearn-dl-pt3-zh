- en: '17'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning Frontiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! You have made it to the final chapter. We have come a long
    way. We started off with the fundamentals of reinforcement learning and gradually
    we learned about the state-of-the-art deep reinforcement learning algorithms.
    In this chapter, we will look at some exciting and promising research trends in
    reinforcement learning. We will start the chapter by learning what meta learning
    is and how it differs from other learning paradigms. Then, we will learn about
    one of the most used meta-learning algorithms, called **Model-Agnostic Meta Learning**
    (**MAML**).
  prefs: []
  type: TYPE_NORMAL
- en: We will understand MAML in detail, and then we will see how to apply it in a
    reinforcement learning setting. Following this, we will learn about hierarchical
    reinforcement learning, and we look into a popular hierarchical reinforcement
    learning algorithm called MAXQ value function decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we will look at an interesting algorithm called **Imagination Augmented
    Agents** (**I2As**), which makes use of both model-based and model-free learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Meta reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model-agnostic meta learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hierarchical reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAXQ value function decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imagination Augmented Agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin the chapter by understanding meta reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Meta reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to understand how meta reinforcement learning works, first let's understand
    meta learning.
  prefs: []
  type: TYPE_NORMAL
- en: Meta learning is one of the most promising and trending research areas in the
    field of artificial intelligence. It is believed to be a stepping stone for attaining
    **Artificial General Intelligence** (**AGI**). What is meta learning? And why
    do we need meta learning? To answer these questions, let's revisit how deep learning
    works.
  prefs: []
  type: TYPE_NORMAL
- en: We know that in deep learning, we train a deep neural network to perform a task.
    But the problem with deep neural networks is that we need to have a large training
    dataset to train our network, as it will fail to learn when we have only a few
    data points.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we trained a deep learning model to perform task **A**. Suppose we
    have a new task **B**, which is closely related to task **A**. Although task **B**
    is closely related to task **A**, we can't use the model we trained for task **A**
    to perform task **B**. We need to train a new model from scratch for task **B**.
    So, for each task, we need to train a new model from scratch although they might
    be related. But is this really true AI? Not really. How do we humans learn? We
    generalize our learning to multiple concepts and learn from there. But current
    learning algorithms master only one task. So, here is where meta learning comes
    in.
  prefs: []
  type: TYPE_NORMAL
- en: Meta learning produces a versatile AI model that can learn to perform various
    tasks without having to be trained from scratch. We train our meta-learning model
    on various related tasks with few data points, so for a new related task, it can
    make use of the learning achieved in previous tasks. Many researchers and scientists
    believe that meta learning can get us closer to achieving AGI. Learning to learn
    is the key focus of meta learning. We will understand how exactly meta learning
    works by looking at a popular meta learning algorithm called MAML in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Model-agnostic meta learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Model-Agnostic Meta Learning** (**MAML**) is one of the most popular meta-learning
    algorithms and it has been a major breakthrough in meta-learning research. The
    basic idea of MAML is to find a better initial model parameter so that with a good
    initial parameter, a model can learn quickly on new tasks with fewer gradient steps.'
  prefs: []
  type: TYPE_NORMAL
- en: So, what do we mean by that? Let's say we are performing a classification task
    using a neural network. How do we train the network? We start off by initializing
    random weights and train the network by minimizing the loss. How do we minimize
    the loss? We minimize the loss using gradient descent. Okay, but how do we use
    gradient descent to minimize the loss? We use gradient descent to find the optimal
    weights that will give us the minimal loss. We take multiple gradient steps to
    find the optimal weights so that we can reach convergence.
  prefs: []
  type: TYPE_NORMAL
- en: In MAML, we try to find these optimal weights by learning from the distribution
    of similar tasks. So, for a new task, we don't have to start with randomly initialized
    weights; instead, we can start with optimal weights, which will take fewer gradient
    steps to reach convergence and doesn't require more data points for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand how MAML works in simple terms. Let''s suppose we have three
    related tasks: *T*[1], *T*[2], and *T*[3].'
  prefs: []
  type: TYPE_NORMAL
- en: First, we randomly initialize our model parameter (weight), ![](img/B15558_09_118.png).
    We train our network on task *T*[1]. Then, we try to minimize the loss *L* by
    gradient descent. We minimize the loss by finding the optimal parameter. Let ![](img/B15558_17_002.png)
    be the optimal parameter for the task *T*[1]. Similarly, for tasks *T*[2] and
    *T*[3], we will start off with a randomly initialized model parameter ![](img/B15558_09_118.png)
    and minimize the loss by finding the optimal parameters by gradient descent. Let
    ![](img/B15558_12_208.png) and ![](img/B15558_17_005.png) be the optimal parameters
    for tasks *T*[2] and *T*[3], respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in the following figure, we start off each task with the randomly
    initialized parameter ![](img/B15558_09_054.png) and minimize the loss by finding
    the optimal parameters ![](img/B15558_17_007.png), ![](img/B15558_17_008.png),
    and ![](img/B15558_17_009.png) for the tasks *T*[1], *T*[2], and *T*[3] respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.1: ![](img/B15558_17_010.png) is initialized at a random position'
  prefs: []
  type: TYPE_NORMAL
- en: However, instead of initializing ![](img/B15558_09_098.png) in a random position,
    that is, with random values, if we initialize ![](img/B15558_09_123.png) in a
    position that is common to all three tasks, then we don't need to take many gradient
    steps and it will take us less time to train. MAML tries to do exactly this. MAML
    tries to find this optimal parameter ![](img/B15558_09_087.png) that is common
    to many of the related tasks, so we can train a new task relatively quick with
    few data points without having to take many gradient steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'As *Figure 17.2* shows, we shift ![](img/B15558_09_054.png) to a position that
    is common to all different optimal ![](img/B15558_09_107.png) values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.2: ![](img/B15558_17_016.png) is initialized at the optimal position'
  prefs: []
  type: TYPE_NORMAL
- en: So, for a new related task, say, *T*[4], we don't have to start with a randomly
    initialized parameter, ![](img/B15558_09_087.png). Instead, we can start with
    the optimal ![](img/B15558_09_123.png) value (shifted ![](img/B15558_10_037.png))
    so that it will take fewer gradient steps to attain convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in MAML, we try to find this optimal ![](img/B15558_09_098.png) value
    that is common to related tasks to help us learn from fewer data points and minimize
    our training time. MAML is model-agnostic, meaning that we can apply MAML to any
    models that are trainable with gradient descent. But how exactly does MAML work?
    How do we shift the model parameters to an optimal position? Now that we have
    a basic understanding of MAML, we will address all these questions in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MAML
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we have a model *f* parameterized by ![](img/B15558_09_118.png), that
    is, ![](img/B15558_17_022.png), and we have a distribution over tasks, *p(T)*.
    First, we initialize our parameter ![](img/B15558_09_056.png) with some random
    values. Next, we sample a batch of tasks *T*[i] from a distribution over tasks―that
    is, *T*[i]* ~ p(T)*. Let''s say we have sampled five tasks: *T*[1], *T*[2], *T*[3],
    *T*[4], *T*[5].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each task *T*[i], we sample *k* number of data points and train the
    model *f* parameterized by ![](img/B15558_09_098.png), that is, ![](img/B15558_17_025.png).
    We train the model by computing the loss ![](img/B15558_17_026.png) and we minimize
    the loss using gradient descent and find the optimal parameter ![](img/B15558_17_027.png).
    The parameter update rule using gradient descent is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_028.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_027.png) is the optimal parameter for a task *T*[i]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_17_030.png) is the initial parameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_07_025.png) is the learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_17_032.png) is the gradient of loss for a task *T*[i] with the
    model parameterized as ![](img/B15558_17_033.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: So, after the preceding parameter update using gradient descent, we will have
    optimal parameters for all five tasks that we have sampled. That is, for the tasks
    *T*[1], *T*[2], *T*[3], *T*[4], *T*[5], we will have the optimal parameters ![](img/B15558_17_034.png),
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before the next iteration, we perform a meta update or meta optimization.
    That is, in the previous step, we found the optimal parameter ![](img/B15558_17_035.png)
    by training on each of the tasks, *T*[i]. Now we take some new set of tasks and
    for each of these new tasks *T*[i], we don't have to start from the random position
    ![](img/B15558_09_098.png); instead, we can start from the optimal position ![](img/B15558_17_037.png)
    to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, for each of the new tasks *T*[i], instead of using the randomly initialized
    parameter ![](img/B15558_09_098.png), we use the optimal parameter ![](img/B15558_17_039.png).
    This implies that we train the model *f* parameterized by ![](img/B15558_17_040.png),
    that is, ![](img/B15558_17_041.png) instead of using ![](img/B15558_17_025.png).
    Then, we calculate the loss ![](img/B15558_17_043.png), compute the gradients,
    and update the parameter ![](img/B15558_09_054.png). This makes our randomly initialized
    parameter ![](img/B15558_09_087.png) move to an optimal position where we don''t
    have to take many gradient steps. This step is called a meta update, meta optimization,
    or meta training. It can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_046.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In equation (2), the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_14_004.png) is the initial parameter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_09_152.png) is the learning rate'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_17_049.png) is the gradient of loss for each of the new tasks
    *T*[i], with the model parameterized as ![](img/B15558_17_041.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: If you look at our previous meta update equation (2) closely, we can see that
    we are updating our model parameter ![](img/B15558_09_118.png) by merely taking
    an average of gradients of each new task *T*[i] with the model *f* parameterized
    by ![](img/B15558_17_037.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 17.3 helps us to understand the MAML algorithm better. As we can observe,
    our MAML algorithm has two loops—an **inner loop** where we find the optimal parameter
    ![](img/B15558_17_053.png) for each of the tasks *T*[i] using the model *f* parameterized
    by initial parameter ![](img/B15558_09_054.png), that is, ![](img/B15558_17_055.png),
    and an **outer loop** where we use the model *f* parameterized by the optimal
    parameter ![](img/B15558_17_056.png) obtained in the previous step, that is ![](img/B15558_17_057.png),
    and train the model on the new set of tasks, calculate the loss, compute the gradient
    of the loss, and update the randomly initialized model parameter ![](img/B15558_09_098.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.3: The MAML algorithm'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we should not use the same set of tasks we used to find the optimal
    parameter ![](img/B15558_17_059.png) when updating the model parameter ![](img/B15558_09_098.png)
    in the outer loop.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, in MAML, we sample a batch of tasks and for each task *T*[i]
    in the batch, we minimize the loss using gradient descent and get the optimal
    parameter ![](img/B15558_17_027.png). Then, we update our randomly initialized
    model parameter ![](img/B15558_09_054.png) by calculating gradients for each new
    task *T*[i] with the model parameterized as ![](img/B15558_17_041.png).
  prefs: []
  type: TYPE_NORMAL
- en: Still not clear how exactly MAML works? Worry not! Let's look in even more detail
    at the steps and understand how MAML works in a supervised learning setting in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: MAML in a supervised learning setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we learned, MAML is model-agnostic and so we can apply MAML to any model
    that can be trained with gradient descent. In this section, let's learn how to
    apply the MAML algorithm in a supervised learning setting. Before going ahead,
    let's define our loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we are performing regression, then we can use mean squared error as our
    loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_064.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we are performing classification, then we can use cross-entropy loss as
    our loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_065.png)'
  prefs: []
  type: TYPE_IMG
- en: Now let's see step by step how exactly MAML is used in supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a model *f* parameterized by a parameter ![](img/B15558_17_066.png),
    and we have a distribution over tasks *p(T)*. First, we randomly initialize the
    model parameter ![](img/B15558_09_008.png).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we sample a batch of tasks *T*[i] from a distribution of tasks, that is,
    *T*[i] *~ p(T)*. Let's say we have sampled three tasks; then, we have *T*[1],
    *T*[2], *T*[3].
  prefs: []
  type: TYPE_NORMAL
- en: '**Inner loop:** For each task *T*[i], we sample *k* data points and prepare
    our training and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_068.png)'
  prefs: []
  type: TYPE_IMG
- en: Wait! What are the training and test datasets? We use the training dataset in
    the inner loop for finding the optimal parameter ![](img/B15558_17_069.png) and
    the test set in the outer loop for finding the optimal parameter ![](img/B15558_09_087.png).
    The test dataset does not mean that we are checking the model's performance. It
    basically acts as a training set in the outer loop. We can also call our test
    set a meta-training set.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we train the model ![](img/B15558_17_055.png) on the training dataset ![](img/B15558_17_072.png),
    calculate the loss, minimize the loss using gradient descent, and get the optimal
    parameter ![](img/B15558_17_059.png) as ![](img/B15558_17_074.png).
  prefs: []
  type: TYPE_NORMAL
- en: That is, for each of the tasks *T*[i], we sample *k* data points and prepare
    ![](img/B15558_17_075.png) and ![](img/B15558_17_076.png). Next, we minimize the
    loss on the training dataset ![](img/B15558_17_077.png) and get the optimal parameter
    ![](img/B15558_17_078.png). As we sampled three tasks, we will have three optimal
    parameters, ![](img/B15558_17_079.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**Outer loop**: Now, we perform meta optimization on the test set (meta-training
    set); that is, we try to minimize the loss in the test set ![](img/B15558_17_080.png).
    Here, we parameterize our model *f* by the optimal parameter ![](img/B15558_17_069.png)
    calculated in the previous step. So, we compute the loss of the model ![](img/B15558_17_082.png)
    and the gradients of the loss and update our randomly initialized parameter ![](img/B15558_09_087.png)
    using our test dataset (meta-training dataset) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_084.png)'
  prefs: []
  type: TYPE_IMG
- en: We repeat the preceding steps for several iterations to find the optimal parameter.
    For a clear understanding of how MAML works in supervised learning, let's look
    into the algorithm in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – MAML in supervised learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The algorithm of MAML in a supervised learning setting is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Say that we have a model *f* parameterized by a parameter ![](img/B15558_09_056.png)
    and we have a distribution over tasks *p(T)*. First, we randomly initialize the
    model parameter ![](img/B15558_09_054.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a batch of tasks *T*[i] from a distribution of tasks, that is, *T*[i]
    *~ p(T)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each task *T*[i]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample *k* data points and prepare our training and test datasets:![](img/B15558_17_068.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model ![](img/B15558_17_088.png) on the training dataset ![](img/B15558_17_089.png)
    and compute the loss.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the loss using gradient descent and get the optimal parameter ![](img/B15558_17_053.png)
    as ![](img/B15558_17_091.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, minimize the loss on the test set ![](img/B15558_17_092.png). Parameterize
    the model *f* with the optimal parameter ![](img/B15558_17_069.png) calculated
    in the previous step, compute loss ![](img/B15558_17_094.png). Calculate gradients
    of the loss and update our randomly initialized parameter ![](img/B15558_17_066.png)
    using our test (meta-training) dataset as:![](img/B15558_17_084.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *4* for several iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following figure gives us an overview of how the MAML algorithm works in
    supervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.4: Overview of MAML'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to use MAML in a supervised learning setting, in
    the next section, we will see how to use MAML in a reinforcement learning setting.
  prefs: []
  type: TYPE_NORMAL
- en: MAML in a reinforcement learning setting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's learn how to apply the MAML algorithm in a reinforcement learning
    setting. We know that the objective of reinforcement learning is to find the optimal
    policy, that is, the policy that gives the maximum return. We've learned about
    several reinforcement learning algorithms for finding the optimal policy, and
    we've also learned about several deep reinforcement learning algorithms for finding
    the optimal policy, where we used the neural network parameterized by ![](img/B15558_10_095.png).
  prefs: []
  type: TYPE_NORMAL
- en: We can apply MAML to any algorithm that can be trained with gradient descent.
    For instance, let's take the policy gradient method. In the policy gradient method,
    we use a neural network parameterized by ![](img/B15558_09_106.png) to find the
    optimal policy and we train our network using gradient descent. So, we can apply
    the MAML algorithm to the policy gradient method.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand how MAML works in reinforcement learning step by step.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a model (policy network) *f* parameterized by a parameter
    ![](img/B15558_09_118.png). The model (policy network) *f* tries to find the optimal
    policy by learning the optimal parameter ![](img/B15558_09_087.png). Suppose,
    we have a distribution over tasks *p(T)*. First, we randomly initialize the model
    parameter ![](img/B15558_09_054.png).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we sample a batch of tasks *T*[i] from a distribution of tasks, that is,
    *T*[i] *~ p(T)*. Let's say we have sampled three tasks; then, we have *T*[1],
    *T*[2], *T*[3].
  prefs: []
  type: TYPE_NORMAL
- en: '**Inner loop**: For each task *T*[i], we prepare our train dataset ![](img/B15558_17_089.png).
    Okay, how can we create the training dataset in a reinforcement learning setting?'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the model (policy network) ![](img/B15558_17_025.png). So, we generate
    *k* number of trajectories using our model ![](img/B15558_17_025.png). We know
    that the trajectories consist of a sequence of state-action pairs. So, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_105.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, we compute the loss and minimize it using gradient descent and get the
    optimal parameter ![](img/B15558_17_053.png) as ![](img/B15558_17_074.png).
  prefs: []
  type: TYPE_NORMAL
- en: That is, for each of the tasks *T*[i], we sample *k* trajectories and prepare
    the training dataset ![](img/B15558_17_108.png). Next, we minimize the loss on
    the training dataset ![](img/B15558_17_109.png) and get the optimal parameter
    ![](img/B15558_17_053.png). As we sampled three tasks, we will have three optimal
    parameters, ![](img/B15558_17_111.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need the test dataset ![](img/B15558_17_112.png), which we use in the
    outer loop. How do we prepare our test dataset? Now, we use our model *f* parameterized
    by the optimal parameter ![](img/B15558_17_056.png); that is, we use ![](img/B15558_17_057.png)
    and generate *k* number of trajectories. So, we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_115.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember that ![](img/B15558_17_116.png) is created by ![](img/B15558_17_117.png)
    and the test (meta-training) dataset ![](img/B15558_17_118.png) is created by
    ![](img/B15558_17_119.png).
  prefs: []
  type: TYPE_NORMAL
- en: '**Outer loop**: Now, we perform meta optimization on the test (meta-training)
    dataset; that is, we try to minimize the loss in the test dataset ![](img/B15558_17_120.png).
    Here, we parameterize our model *f* by the optimal parameter ![](img/B15558_17_053.png)
    calculated in the previous step. So, we compute the loss of the model ![](img/B15558_17_043.png)
    and the gradients of the loss and update our randomly initialized parameter ![](img/B15558_09_098.png)
    using our test (meta-training) dataset as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_084.png)'
  prefs: []
  type: TYPE_IMG
- en: We repeat the preceding step for several iterations to find the optimal parameter.
    For a clear understanding of how MAML works in reinforcement learning, let's look
    into the algorithm in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – MAML in reinforcement learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The algorithm of MAML in a reinforcement learning setting is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Say, we have a model *f* parameterized by a parameter ![](img/B15558_10_066.png)
    and we have a distribution over tasks *p(T)*. First, we randomly initialize the
    model parameter ![](img/B15558_09_087.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a batch of tasks *T*[i] from a distribution of tasks, that is, *T*[i]
    *~ p(T).*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each task *T*[i]:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sample *k* trajectories using ![](img/B15558_17_025.png) and prepare the training
    dataset: ![](img/B15558_17_128.png).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model ![](img/B15558_17_129.png) on the training dataset ![](img/B15558_17_130.png)
    and compute the loss.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Minimize the loss using gradient descent and get the optimal parameter ![](img/B15558_17_131.png)
    as ![](img/B15558_17_132.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sample *k* trajectories using ![](img/B15558_17_133.png) and prepare the test
    dataset: ![](img/B15558_17_115.png).'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we minimize the loss on the test dataset ![](img/B15558_17_135.png). Parameterize
    the model *f* with the optimal parameter ![](img/B15558_17_136.png) calculated
    in the previous step and compute the loss ![](img/B15558_17_137.png). Calculate
    the gradients of the loss and update our randomly initialized parameter ![](img/B15558_17_066.png)
    using our test (meta-training) dataset as:![](img/B15558_17_139.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *4* for several iterations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: That's it! Meta learning is a growing field of research. Now that we have a
    basic idea of meta learning, you can explore more about meta learning and see
    how meta learning is used in reinforcement learning. In the next section, we will
    learn about hierarchical reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Hierarchical reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The problem with reinforcement learning is that it cannot scale well with a
    large number of state spaces and actions, which ultimately leads to the problem
    called curse of dimensionality. **Hierarchical reinforcement learning** (**HRL**)
    is proposed to solve the curse of dimensionality, where we decompose large problems
    into small subproblems in a hierarchy. Let's suppose the goal of our agent is
    to reach home from school. Now, our goal is split into a set of subgoals, such
    as going out of the school gate, booking a cab, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There are different methods used in HRL, such as state-space decomposition,
    state abstraction, and temporal abstraction. In state-space decomposition, we
    decompose the state space into different subspaces and try to solve the problem
    in a smaller subspace. Breaking down the state space also allows faster exploration,
    as the agent does not want to explore the entire state space. In state abstraction,
    the agent ignores the variables that are irrelevant to achieving the current subtasks
    in the current state space. In temporal abstraction, the action sequence and action
    sets are grouped, which divides the single step into multiple steps.
  prefs: []
  type: TYPE_NORMAL
- en: We will now look into one of the most commonly used algorithms in HRL, called
    MAXQ value function decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: MAXQ value function Decomposition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MAXQ value function decomposition is one of the most frequently used algorithms
    in HRL. In this section, let''s get a basic idea and overview of how MAXQ value
    function decomposition works. Let''s understand how MAXQ value function decomposition
    works with an example. Let''s take a taxi environment as shown in *Figure 17.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.5: Taxi environment'
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose our agent is driving a taxi. As *Figure 17.5* shows, the tiny,
    yellow-colored rectangle is the taxi driven by our agent. The letters (**R**,
    **G**, **Y**, **B**) represent the different locations. Thus we have four locations
    in total, and the agent has to pick up a passenger at one location and drop them
    off at another location. The agent will receive +20 points as a reward for a successful
    drop-off and -1 point for every time step it takes. The agent will also lose -10
    points for illegal pickups and drop-offs.
  prefs: []
  type: TYPE_NORMAL
- en: So the goal of our agent is to learn to pick up and drop off passengers at the
    correct location in a short time without adding illegal passengers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we break the goal of our agent into four subtasks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Navigate**: In the Navigate subtask, the goal of our agent is to drive the
    taxi from the current location to one of the target locations. The Navigate(t)
    subtask will use the four primitive actions: *north*, *south*, *east*, and *west*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Get**:In the Get subtask, the goal of our agent is to drive the taxi from
    its current location to the passenger''s location and pick up the passenger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Put**:In the Put subtask, the goal of our agent is to drive the taxi from
    its current location to the passenger''s destination and drop off the passenger.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Root**:Root is the whole task.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can represent all these subtasks in a directed acyclic graph called a task
    graph, as *Figure 17.6* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.6: Task graph'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can observe from the preceding figure, all the subtasks are arranged
    hierarchically. Each node represents a subtask or primitive action and each edge
    connects in such a way that a subtask can call its child subtask. As shown, the
    **Navigate(t)** subtask has four primitive actions: **East**, **West**, **North**,
    and **South**. The **Get** subtask has a **Pickup** primitive action and a **Navigate(t)**
    subtask. Similarly, the **Put** subtask has a **Putdown** (drop) primitive action
    and **Navigate(t)** subtask.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In MAXQ value function decomposition, we decompose the value function into
    a set of value functions for each of the subtasks. For the efficient designing
    and debugging of MAXQ decompositions, we can redesign our task graphs as *Figure 17.7 *shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.7: Task graph'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can observe from *Figure 17.7*, our redesigned graph contains two special
    types of nodes: max nodes and Q nodes. The max nodes define the subtasks in the
    task decomposition and the Q nodes define the actions that are available for each
    subtask.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in this section, we got a basic idea of MaxQ value function decomposition.
    In the next section, we will learn about I2A.
  prefs: []
  type: TYPE_NORMAL
- en: Imagination augmented agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Are you a fan of chess? If I asked you to play chess, how would you play it?
    Before moving any chess piece on the chessboard, you might imagine the consequences
    of moving a chess piece and move the chess piece that you think would help you
    to win the game. So, basically, before taking any action, we imagine the consequence
    and, if it is favorable, we proceed with that action, else we refrain from performing
    that action.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, **Imagination Augmented Agents** (**I2As**) are augmented with imagination.
    Before taking any action in an environment, the agent imagines the consequences
    of taking the action and if they think the action will provide a good reward,
    they will perform the action. The I2A takes advantage of both model-based and
    model-free learning. *Figure 17.8* shows the architecture of I2As:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.8: I2A architecture'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from *Figure 17.8*, I2A architecture has both model-based
    and model-free paths. Thus, the action the agent takes is the result of both the
    model-based and model-free paths. In the model-based path, we have rollout encoders.
  prefs: []
  type: TYPE_NORMAL
- en: 'These rollout encoders are where the agent performs imagination tasks, so let''s
    take a closer look at the rollout encoders. *Figure 17.9* shows a single rollout
    encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.9: Single imagination rollout'
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Figure 17.9*, we can observe that the rollout encoders have two layers:
    the imagine future layer and the encoder layer. The imagine future layer is where
    the imagination happens. The imagine future layer consists of the imagination
    core.'
  prefs: []
  type: TYPE_NORMAL
- en: When we feed the state *s*[t] to the imagination core, we get the next state
    ![](img/B15558_17_140.png) and the reward ![](img/B15558_17_141.png), and when
    we feed this next state ![](img/B15558_17_140.png) to the next imagination core,
    we get the next state ![](img/B15558_17_143.png) and reward ![](img/B15558_17_144.png).
    If we repeat these for *n* steps, we get a rollout, which is basically a pair
    of states and rewards, and then we use encoders such as **Long Short-Term Memory**
    (**LSTM**) to encode this rollout. As a result, we get rollout encodings. These
    rollout encodings are actually the embeddings describing the future imagined path.
    We will have multiple rollout encoders for different future imagined paths, and
    we use an aggregator to aggregate this rollout encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, but how exactly does the imagination happen in the imagination core?
    What is actually in the imagination core? *Figure 17.10* shows a single imagination
    core:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.10: The imagination core'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from *Figure 17.10*, the imagination core consists of a policy
    network and an environment model. The environment model learns from all the actions
    that the agent has performed so far. It takes information about the state ![](img/B15558_17_145.png),
    imagines all the possible futures considering the experience, and chooses the
    action ![](img/B15558_17_146.png) that gives a high reward.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 17.11* shows the complete architecture of I2As with all components
    expanded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.11: Full I2A architecture'
  prefs: []
  type: TYPE_NORMAL
- en: 'Have you played Sokoban before? Sokoban is a classic puzzle game where the
    player has to push boxes to a target location. The rules of the game are very
    simple: boxes can only be pushed and cannot be pulled. If we push a box in the
    wrong direction then the puzzle becomes unsolvable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_17_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17.12: Sokoban environment'
  prefs: []
  type: TYPE_NORMAL
- en: The I2A architecture provides good results in these kinds of environments, where
    the agent has to plan in advance before taking an action. The authors of the paper
    tested I2A performance on Sokoban and achieved great results.
  prefs: []
  type: TYPE_NORMAL
- en: There are various exciting research advancements happening around deep reinforcement
    learning. Now that you have finished reading the book, you can start exploring
    such advancements and experiment with various projects. Learn and reinforce!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started this chapter by understanding what meta learning is. We learned that
    with meta learning, we train our model on various related tasks with a few data
    points, such that for a new related task, our model can make use of the learning
    obtained from the previous tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned about a popular meta-learning algorithm called MAML. In MAML,
    we sample a batch of tasks and for each task *T*[i] in the batch, we minimize
    the loss using gradient descent and get the optimal parameter ![](img/B15558_17_053.png).
    Then, we update our randomly initialized model parameter ![](img/B15558_09_054.png)
    by calculating the gradients for each of the new tasks *T*[i] with the model parameterized
    as ![](img/B15558_17_041.png).
  prefs: []
  type: TYPE_NORMAL
- en: Moving on, we learned about HRL, where we decompose large problems into small
    subproblems in a hierarchy. We also looked into the different methods used in
    HRL, such as state-space decomposition, state abstraction, and temporal abstraction.
    Next, we got an overview of MAXQ value function decomposition, where we decompose
    the value function into a set of value functions for each of the subtasks.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about I2As, which are augmented with imagination.
    Before taking any action in an environment, the agent imagines the consequences
    of taking the action, and if they think the action will provide a good reward,
    they will perform the action.
  prefs: []
  type: TYPE_NORMAL
- en: Deep reinforcement learning is evolving every day with interesting advancements.
    Now that you have learned about the various state-of-the-art deep reinforcement
    learning algorithms, you can start building interesting projects and also contribute
    to deep reinforcement learning research.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s test the knowledge you gained in this chapter; try answering the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need meta learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is MAML?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the meta objective?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the meta training set?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define HRL.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does an I2A work?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, we can refer to the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks** by *Chelsea
    Finn*, *Pieter Abbeel, Sergey Levine*, [https://arxiv.org/pdf/1703.03400.pdf](https://arxiv.org/pdf/1703.03400.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition**
    by *Thomas G. Dietterich*, [https://arxiv.org/pdf/cs/9905014.pdf](https://arxiv.org/pdf/cs/9905014.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imagination-Augmented Agents for Deep Reinforcement Learning** by *Théophane
    Weber*, *et al*.,[https://arxiv.org/pdf/1707.06203.pdf](https://arxiv.org/pdf/1707.06203.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
