["```\nimport gensim.downloader as api\nfrom gensim.models import Word2Vec\ndataset = api.load(\"text8\")\nmodel = Word2Vec(dataset)\nmodel.save(\"data/text8-word2vec.bin\") \n```", "```\n$ mkdir data\n$ python create_embedding_with_text8.py \n```", "```\nfrom gensim.models import KeyedVectors\nmodel = KeyedVectors.load(\"data/text8-word2vec.bin\")\nword_vectors = model.wv \n```", "```\nwords = word_vectors.vocab.keys()\nprint([x for i, x in enumerate(words) if i < 10])\nassert(\"king\" in words) \n```", "```\n['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against'] \n```", "```\ndef print_most_similar(word_conf_pairs, k):\n   for i, (word, conf) in enumerate(word_conf_pairs):\n       print(\"{:.3f} {:s}\".format(conf, word))\n       if i >= k-1:\n           break\n   if k < len(word_conf_pairs):\n       print(\"...\")\nprint_most_similar(word_vectors.most_similar(\"king\"), 5) \n```", "```\n0.760 prince\n0.701 queen\n0.700 kings\n0.698 emperor\n0.688 throne\n... \n```", "```\nprint_most_similar(word_vectors.most_similar(\n   positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n) \n```", "```\n0.803 germany \n```", "```\nprint_most_similar(word_vectors.most_similar_cosmul(\n   positive=[\"france\", \"berlin\"], negative=[\"paris\"]), 1\n) \n```", "```\n0.984 germany \n```", "```\nprint(word_vectors.doesnt_match([\"hindus\", \"parsis\", \"singapore\", \"christians\"])) \n```", "```\nfor word in [\"woman\", \"dog\", \"whale\", \"tree\"]:\n   print(\"similarity({:s}, {:s}) = {:.3f}\".format(\n       \"man\", word,\n       word_vectors.similarity(\"man\", word)\n   )) \n```", "```\nsimilarity(man, woman) = 0.759\nsimilarity(man, dog) = 0.474\nsimilarity(man, whale) = 0.290\nsimilarity(man, tree) = 0.260 \n```", "```\nprint(print_most_similar(\n   word_vectors.similar_by_word(\"singapore\"), 5)\n) \n```", "```\n0.882 malaysia\n0.837 indonesia\n0.826 philippines\n0.825 uganda\n0.822 thailand\n... \n```", "```\nprint(\"distance(singapore, malaysia) = {:.3f}\".format(\n   word_vectors.distance(\"singapore\", \"malaysia\")\n)) \n```", "```\nvec_song = word_vectors[\"song\"]\nvec_song_2 = word_vectors.word_vec(\"song\", use_norm=True) \n```", "```\nimport argparse\nimport gensim.downloader as api\nimport numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, confusion_matrix \n```", "```\ndef download_and_read(url):\n   local_file = url.split('/')[-1]\n   p = tf.keras.utils.get_file(local_file, url,\n       extract=True, cache_dir=\".\")\n   labels, texts = [], []\n   local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n   with open(local_file, \"r\") as fin:\n       for line in fin:\n           label, text = line.strip().split('\\t')\n           labels.append(1 if label == \"spam\" else 0)\n           texts.append(text)\n   return texts, labels\nDATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\ntexts, labels = download_and_read(DATASET_URL) \n```", "```\n# tokenize and pad text\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(texts)\ntext_sequences = tokenizer.texts_to_sequences(texts)\ntext_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n    text_sequences)\nnum_records = len(text_sequences)\nmax_seqlen = len(text_sequences[0])\nprint(\"{:d} sentences, max length: {:d}\".format(\n    num_records, max_seqlen)) \n```", "```\n# labels\nNUM_CLASSES = 2\ncat_labels = tf.keras.utils.to_categorical(\n    labels, num_classes=NUM_CLASSES) \n```", "```\n# vocabulary\nword2idx = tokenizer.word_index\nidx2word = {v:k for k, v in word2idx.items()}\nword2idx[\"PAD\"] = 0\nidx2word[0] = \"PAD\"\nvocab_size = len(word2idx)\nprint(\"vocab size: {:d}\".format(vocab_size)) \n```", "```\n# dataset\ndataset = tf.data.Dataset.from_tensor_slices(\n    (text_sequences, cat_labels))\ndataset = dataset.shuffle(10000)\ntest_size = num_records // 4\nval_size = (num_records - test_size) // 10\ntest_dataset = dataset.take(test_size)\nval_dataset = dataset.skip(test_size).take(val_size)\ntrain_dataset = dataset.skip(test_size + val_size)\nBATCH_SIZE = 128\ntest_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\nval_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\ntrain_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True) \n```", "```\n>>> import gensim.downloader as api\n>>> api.info(\"models\").keys() \n```", "```\ndef build_embedding_matrix(sequences, word2idx, embedding_dim,\n       embedding_file):\n   if os.path.exists(embedding_file):\n       E = np.load(embedding_file)\n   else:\n       vocab_size = len(word2idx)\n       E = np.zeros((vocab_size, embedding_dim))\n       word_vectors = api.load(EMBEDDING_MODEL)\n       for word, idx in word2idx.items():\n           try:\n               E[idx] = word_vectors.word_vec(word)\n           except KeyError:   # word not in embedding\n               pass\n       np.save(embedding_file, E)\n   return E\nEMBEDDING_DIM = 300\nDATA_DIR = \"data\"\nEMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\nEMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\nE = build_embedding_matrix(text_sequences, word2idx, \n   EMBEDDING_DIM,\n   EMBEDDING_NUMPY_FILE)\nprint(\"Embedding matrix:\", E.shape) \n```", "```\nclass SpamClassifierModel(tf.keras.Model):\n   def __init__(self, vocab_sz, embed_sz, input_length,\n           num_filters, kernel_sz, output_sz,\n           run_mode, embedding_weights,\n           **kwargs):\n       super(SpamClassifierModel, self).__init__(**kwargs)\n       if run_mode == \"scratch\":\n           self.embedding = tf.keras.layers.Embedding(vocab_sz,\n               embed_sz,\n               input_length=input_length,\n               trainable=True)\n       elif run_mode == \"vectorizer\":\n           self.embedding = tf.keras.layers.Embedding(vocab_sz,\n               embed_sz,\n               input_length=input_length,\n               weights=[embedding_weights],\n               trainable=False)\n       else:\n           self.embedding = tf.keras.layers.Embedding(vocab_sz,\n               embed_sz,\n               input_length=input_length,\n               weights=[embedding_weights],\n               trainable=True)\n       self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n           kernel_size=kernel_sz,\n           activation=\"relu\")\n       self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n       self.pool = tf.keras.layers.GlobalMaxPooling1D()\n       self.dense = tf.keras.layers.Dense(output_sz,\n           activation=\"softmax\")\n   def call(self, x):\n       x = self.embedding(x)\n       x = self.conv(x)\n       x = self.dropout(x)\n       x = self.pool(x)\n       x = self.dense(x)\n       return x\n# model definition\nconv_num_filters = 256\nconv_kernel_size = 3\nmodel = SpamClassifierModel(\n   vocab_size, EMBEDDING_DIM, max_seqlen,\n   conv_num_filters, conv_kernel_size, NUM_CLASSES,\n   run_mode, E)\nmodel.build(input_shape=(None, max_seqlen)) \n```", "```\n# compile\nmodel.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \n```", "```\nNUM_EPOCHS = 3\n# data distribution is 4827 ham and 747 spam (total 5574), which\n# works out to approx 87% ham and 13% spam, so we take reciprocals\n# and this works out to being each spam (1) item as being \n# approximately 8 times as important as each ham (0) message.\nCLASS_WEIGHTS = { 0: 1, 1: 8 }\n# train model\nmodel.fit(train_dataset, epochs=NUM_EPOCHS,\n   validation_data=val_dataset,\n   class_weight=CLASS_WEIGHTS)\n# evaluate against test set\nlabels, predictions = [], []\nfor Xtest, Ytest in test_dataset:\n   Ytest_ = model.predict_on_batch(Xtest)\n   ytest = np.argmax(Ytest, axis=1)\n   ytest_ = np.argmax(Ytest_, axis=1)\n   labels.extend(ytest.tolist())\n   predictions.extend(ytest.tolist())\nprint(\"test accuracy: {:.3f}\".format(accuracy_score(labels, predictions)))\nprint(\"confusion matrix\")\nprint(confusion_matrix(labels, predictions)) \n```", "```\n$ python spam_classifier --mode [scratch|vectorizer|finetune] \n```", "```\nimport gensim\nimport logging\nimport numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\nfrom scipy.sparse import csr_matrix\nfrom sklearn.metrics.pairwise import cosine_similarity\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) \n```", "```\nDATA_DIR = \"./data\"\nUCI_DATA_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00371/NIPS_1987-2015.csv\"\ndef download_and_read(url):\n   local_file = url.split('/')[-1]\n   p = tf.keras.utils.get_file(local_file, url, cache_dir=\".\")\n   row_ids, col_ids, data = [], [], []\n   rid = 0\n   f = open(p, \"r\")\n   for line in f:\n       line = line.strip()\n       if line.startswith(\"\\\"\\\",\"):\n           # header\n           continue\n       # compute non-zero elements for current row\n       counts = np.array([int(x) for x in line.split(',')[1:]])\n       nz_col_ids = np.nonzero(counts)[0]\n       nz_data = counts[nz_col_ids]\n       nz_row_ids = np.repeat(rid, len(nz_col_ids))\n       rid += 1\n       # add data to big lists\n       row_ids.extend(nz_row_ids.tolist())\n       col_ids.extend(nz_col_ids.tolist())\n       data.extend(nz_data.tolist())\n   f.close()\n   TD = csr_matrix((\n       np.array(data), (\n           np.array(row_ids), np.array(col_ids)\n           )\n       ),\n       shape=(rid, counts.shape[0]))\n   return TD\n# read data and convert to Term-Document matrix\nTD = download_and_read(UCI_DATA_URL)\n# compute undirected, unweighted edge matrix\nE = TD.T * TD\n# binarize\nE[E > 0] = 1 \n```", "```\n0 1405 4845 754 4391 3524 4282 2357 3922 1667\n0 1341 456 495 1647 4200 5379 473 2311\n0 3422 3455 118 4527 2304 772 3659 2852 4515 5135 3439 1273\n0 906 3498 2286 4755 2567 2632\n0 5769 638 3574 79 2825 3532 2363 360 1443 4789 229 4515 3014 3683 2967 5206 2288 1615 1166\n0 2469 1353 5596 2207 4065 3100\n0 2236 1464 1596 2554 4021\n0 4688 864 3684 4542 3647 2859\n0 4884 4590 5386 621 4947 2784 1309 4958 3314\n0 5546 200 3964 1817 845 \n```", "```\nNUM_WALKS_PER_VERTEX = 32\nMAX_PATH_LENGTH = 40\nRESTART_PROB = 0.15\nRANDOM_WALKS_FILE = os.path.join(DATA_DIR, \"random-walks.txt\")\ndef construct_random_walks(E, n, alpha, l, ofile):\n   if os.path.exists(ofile):\n       print(\"random walks generated already, skipping\")\n       return\n   f = open(ofile, \"w\")\n   for i in range(E.shape[0]):  # for each vertex\n       if i % 100 == 0:\n           print(\"{:d} random walks generated from {:d} vertices\"\n               .format(n * i, i))\n       for j in range(n):       # construct n random walks\n           curr = i\n           walk = [curr]\n           target_nodes = np.nonzero(E[curr])[1]\n           for k in range(l):   # each of max length l\n               # should we restart?\n               if np.random.random() < alpha and len(walk) > 5:\n                   break\n               # choose one outgoing edge and append to walk\n               try:\n                   curr = np.random.choice(target_nodes)\n                   walk.append(curr)\n                   target_nodes = np.nonzero(E[curr])[1]\n               except ValueError:\n                   continue\n           f.write(\"{:s}\\n\".format(\" \".join([str(x) for x in walk])))\n   print(\"{:d} random walks generated from {:d} vertices, COMPLETE\"\n       .format(n * i, i))\n   f.close()\n# construct random walks (caution: very long process!)\nconstruct_random_walks(E, NUM_WALKS_PER_VERTEX, RESTART_PROB, MAX_PATH_LENGTH, RANDOM_WALKS_FILE) \n```", "```\n0 1405 4845 754 4391 3524 4282 2357 3922 1667\n0 1341 456 495 1647 4200 5379 473 2311\n0 3422 3455 118 4527 2304 772 3659 2852 4515 5135 3439 1273\n0 906 3498 2286 4755 2567 2632\n0 5769 638 3574 79 2825 3532 2363 360 1443 4789 229 4515 3014 3683 2967 5206 2288 1615 1166\n0 2469 1353 5596 2207 4065 3100\n0 2236 1464 1596 2554 4021\n0 4688 864 3684 4542 3647 2859\n0 4884 4590 5386 621 4947 2784 1309 4958 3314\n0 5546 200 3964 1817 845 \n```", "```\nW2V_MODEL_FILE = os.path.join(DATA_DIR, \"w2v-neurips-papers.model\")\nclass Documents(object):\n   def __init__(self, input_file):\n       self.input_file = input_file\n   def __iter__(self):\n       with open(self.input_file, \"r\") as f:\n           for i, line in enumerate(f):\n               if i % 1000 == 0:\n                   logging.info(\"{:d} random walks extracted\".format(i))\n               yield line.strip().split()\ndef train_word2vec_model(random_walks_file, model_file):\n   if os.path.exists(model_file):\n       print(\"Model file {:s} already present, skipping training\"\n           .format(model_file))\n       return\n   docs = Documents(random_walks_file)\n   model = gensim.models.Word2Vec(\n       docs,\n       size=128,    # size of embedding vector\n       window=10,   # window size\n       sg=1,        # skip-gram model\n       min_count=2,\n       workers=4\n   )\n   model.train(\n       docs,\n       total_examples=model.corpus_count,\n       epochs=50)\n   model.save(model_file)\n# train model\ntrain_word2vec_model(RANDOM_WALKS_FILE, W2V_MODEL_FILE) \n```", "```\ndef evaluate_model(td_matrix, model_file, source_id):\n   model = gensim.models.Word2Vec.load(model_file).wv\n   most_similar = model.most_similar(str(source_id))\n   scores = [x[1] for x in most_similar]\n   target_ids = [x[0] for x in most_similar]\n   # compare top 10 scores with cosine similarity \n   # between source and each target\n   X = np.repeat(td_matrix[source_id].todense(), 10, axis=0)\n   Y = td_matrix[target_ids].todense()\n   cosims = [cosine_similarity(X[i], Y[i])[0, 0] for i in range(10)]\n   for i in range(10):\n       print(\"{:d} {:s} {:.3f} {:.3f}\".format(\n           source_id, target_ids[i], cosims[i], scores[i]))\nsource_id = np.random.choice(E.shape[0])\nevaluate_model(TD, W2V_MODEL_FILE, source_id) \n```", "```\nsrc_id dst_id cosine_sim w2v_score\n1971   5443        0.000     0.348\n1971   1377        0.000     0.348\n1971   3682        0.017     0.328\n1971   51          0.022     0.322\n1971   857         0.000     0.318\n1971   1161        0.000     0.313\n1971   4971        0.000     0.313\n1971   5168        0.000     0.312\n1971   3099        0.000     0.311\n1971   462         0.000     0.310 \n```", "```\nimport tensorflow as tf\nimport tensorflow_hub as hub\n\nelmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\nembeddings = elmo.signatures[\"default\"](\n    tf.constant([\n      \"i like green eggs and ham\",\n      \"would you eat them in a box\"\n    ]))[\"elmo\"]\nprint(embeddings.shape) \n```", "```\nembed = hub.KerasLayer(\"https://tfhub.dev/google/elmo/3\",input_shape=[], dtype=tf.string)\nmodel = tf.keras.Sequential([embed])\nembeddings = model.predict([\n    \"i i like green eggs and ham\",\n    \"would you eat them in a box\"\n])\nprint(embeddings.shape) \n```", "```\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/4\")\nembeddings = embed([\n\"i like green eggs and ham\",\n\"would you eat them in a box\"\n])[\"outputs\"]\nprint(embeddings.shape) \n```", "```\n$ git clone https://github.com/google-research/bert.git\n$ cd bert \n```", "```\n$ mkdir data\n$ cd data\n$ wget \\ \nhttps://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n$ unzip -a uncased_L-12_H-768_A-12.zip \n```", "```\nuncased_L-12_H-768_A-12/\n ├── bert_config.json\n ├── bert_model.ckpt.data-00000-of-00001\n ├── bert_model.ckpt.index\n ├── bert_model.ckpt.meta\n └── vocab.txt \n```", "```\n$ export BERT_BASE_DIR=./data/uncased_L-12_H-768_A-12\n$ export CLASSIFIER_DATA=./data/my_data\n$ export TRAINED_CLASSIFIER=./data/my_classifier\n$ python extract_features.py \\\n    --input_file=${CLASSIFIER_DATA}/sentences.txt \\\n    --output_file=${CLASSIFIER_DATA}/embeddings.jsonl \\\n    --vocab_file=${BERT_BASE_DIR}/vocab.txt \\\n    --bert_config_file=${BERT_BASE_DIR}/bert_config.json \\\n    --init_checkpoint=${BERT_BASE_DIR}/bert_model.ckpt \\\n    --layers=-1,-2,-3,-4 \\\n    --max_seq_length=128 \\\n    --batch_size=8 \n```"]