<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Video and Recurrent Neural Networks</h1>
                </header>
            
            <article>
                
<p>So far in this book, we have only considered still images. However, in this chapter, we will introduce the techniques that are applied to video analysis. From self-driving cars to video streaming websites, computer vision techniques have been developed to enable sequences of images to be processed.</p>
<p>We will introduce a new type of neural network—<strong>recurrent neural networks</strong> (<strong>RNNs</strong>), which are designed specifically for sequential inputs such as video. As a practical application, we will combine them with <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) to detect actions included in short video clips.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Introduction to RNNs</li>
<li>Inner workings of long short-term memory networks</li>
<li>Applications of computer vision models to videos</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>Commented code in the form of Jupyter notebooks is available in this book's GitHub repository at</span> <a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter08">https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter08</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing RNNs</h1>
                </header>
            
            <article>
                
<p><span>RNNs</span> are a type of neural network that are suited for <em>sequential</em> (or <em>recurrent</em>) data. Examples of sequential data include sentences (sequences of words), time series (sequences of stock prices, for instance), or videos (sequences of frames). They qualify as recurrent data as each time step is related to the previous ones.</p>
<p>While RNNs were originally developed for time series analysis and natural language processing tasks, they are now applied to various computer vision tasks.</p>
<p>We will first introduce the basic concepts behind RNNs, before trying to get a general understanding of how they work. We will then describe how their weights can be learned.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Basic formalism</h1>
                </header>
            
            <article>
                
<p>To introduce RNNs, we will use the example of video recognition. A video is composed of <em>N</em> frames. The naive method to classify a video would be to apply a CNN to each frame, and then take the average of the outputs.</p>
<p>While this would provide decent results, it does not reflect the fact that some parts of the video are more important than others. Moreover, the important parts do not always take more frames than the meaningless ones. The risk of averaging the output would be to lose important information.</p>
<p>To circumvent this problem, an RNN is applied to all the frames of the video, one after the other, from the first one to the last one. The main attribute of RNNs is adequately combining features from all the frames in order to generate meaningful results.</p>
<div class="packt_infobox">We do not apply the RNN directly to the raw pixels of the frame. As described later in the chapter, we first use a CNN to generate a feature volume (a stack of feature maps). The concept of feature volume was detailed in <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>. As a reminder, a feature volume is the output of a CNN and usually represents the input with a smaller dimensionality.</div>
<p>To do so, RNNs introduce a new concept called the <strong>state</strong>. State can be pictured as the memory of the RNN. In practice, <em>state</em> is a float matrix. The <em>state</em> starts as a zero matrix and is updated with each frame of the video. At the end of the process, the final state is used to generate the output of the RNN.</p>
<p>The main component of an RNN is the <strong>RNN cell</strong>, which we will apply to every frame. A cell receives as inputs both the <em>current frame</em> and the <em>previous state</em>. For a video composed of <em>N</em> frames, an unfolded representation of a simple recurrent network is depicted in <em>Figure 8-1</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5169459a-a619-4a53-9a8b-4c3a7fe2928e.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-1: Basic RNN cell</div>
<p>In detail, we start with a null state (<em>h<sup>&lt;0&gt;</sup></em>). As a first step, the cell combines the current state (<em>h<sup>&lt;0&gt;</sup></em>) with the current frame (frame<sub>1</sub>) to generate a new state (<em>h<sup>&lt;1&gt;</sup></em>). Then, the same process is applied to the next frames. At the end of this process, we end up with the final state (<em>h<sup>&lt;n&gt;</sup></em>).</p>
<div class="packt_tip">Note the vocabulary here—<em>RNN</em> refers to the component that accepts an image and returns a final output. An <em>RNN cell</em> refers to the sub-component that combines a frame as well as a current state, and returns the next state.</div>
<p>In practice, the cell combines the current state and the frame to generate a new state. This combination happens according to the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a963e0e1-c4b2-4417-ace0-01520200eb48.png" style="width:21.08em;height:1.50em;"/></p>
<p>In the formula, the following applies:</p>
<ul>
<li><em>b</em> is the bias.</li>
<li><em>W<sub>rec</sub></em> is the recurrent weight matrix, and <em>W<sub>input</sub></em> is the weight matrix.</li>
<li><em>x<sup>&lt;t&gt;</sup></em> is the input.</li>
<li><em>h</em><sup><em>&lt;t-1</em></sup><sup>&gt;</sup> is the current state, and <em>h<sup>&lt;t&gt;</sup></em> is the new state.</li>
</ul>
<p>The hidden state is not used as is. A weight matrix, <em>V</em>, is used to compute the final prediction:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dbf994e9-96b7-42f6-876a-4dc1d6022356.png" style="width:10.25em;height:1.33em;"/></p>
<div class="packt_infobox">Throughout this chapter, we will make use of chevrons (<em>&lt; &gt;</em>) to denote temporal information. Other sources may use different conventions. Note, however, that the <em>y</em> with a hat (<img class="fm-editor-equation" src="assets/4d9d459e-d52e-4560-ad87-0679ff1768e4.png" style="width:0.75em;height:1.50em;"/>) commonly represents the prediction of a neural network, while <em>y</em> represents the ground truth.</div>
<p>When applied to videos, RNNs can be used to classify the whole video or every single frame. In the former case, for instance, when predicting whether a video is violent, only the final prediction, <img class="fm-editor-equation" src="assets/9a1a7978-6dd9-4bba-b4e5-b677fb274ad6.png" style="width:2.17em;height:1.33em;"/>, will be used. In the latter case, for instance, to detect which frames may contain nudity, predictions for each time step will be used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General understanding of RNNs</h1>
                </header>
            
            <article>
                
<p>Before we detail how the network learns the weights of <em>W<sub>input</sub></em>, <em>W<sub>rec</sub></em>, and <em>V</em>, let's try to get a broad understanding of how a basic RNN works. The general idea is that <em>W<sub>input</sub></em> will influence the results if some of the features from the input make it into the hidden state, and <em>W<sub>rec</sub></em> will influence the results if some features stay in the hidden state.</p>
<p>Let's use specific examples—classifying a violent video and a dance video.</p>
<p>As a gunshot can be quite sudden, it would represent only a few frames among all the frames of the video. Ideally, the network will learn W<sub>input</sub>, so that when <em>x<sup>&lt;t&gt;</sup></em> contains the information of a gunshot, the concept of <em>violent video</em> would be added to the state. Moreover, <em>W<sub>rec</sub></em> (defined in the previous equation) must be learned in a way that prevents the concept of <em>violent</em> from disappearing from the state. This way, even if the gunshot appears only in the first few frames, the video would still be classified as violent (see <em>Figure 8-2</em>).</p>
<p><span>However, to classify dance videos, we would adopt another behavior. Ideally, the network would learn</span> <em>W<sub>input</sub></em> so that, <span>for example, when</span> <em>x<sup>&lt;t&gt; </sup></em><span>contains people who appear to be dancing, the concept of <em>dance</em> would only be lightly incremented in the state (see <em>Figure 8-2</em>):</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b9eb6930-ae06-4161-9841-7c6e18e6696a.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-2: Simplified representation of how the hidden state should evolve, depending on the video content</div>
<p>Indeed, if the input is a sport video, we would not want a single frame mistakenly classified as <em>dancing people</em> to change our state to <em>dancing</em>. As a dancing video is mostly made of frames containing dancing people, by incrementing the state little by little, we would avoid misclassification.</p>
<p>Moreover, <em>W<sub>rec</sub></em> must be learned in order to make <em>dance</em> gradually disappear from the state. This way, if the introduction of the video is about dance, but the whole video is not, it would not be classified as such.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning RNN weights</h1>
                </header>
            
            <article>
                
<p>In practice, the state of the network is much more complex than a vector containing a weight for each class, as in the previous example. The weights of <em>W<sub>input</sub></em>, <em>W<sub>rec</sub></em>, and <em>V</em> cannot be engineered by hand. Thankfully, they can be learned through <strong>backpropagation</strong>. This technique was detailed in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>. The general idea is to learn the weights by correcting them based on the errors that the network makes.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagation through time</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">For RNNs, however, we not only backpropagate the error through the depth of the network, but also through time. First of all, we compute the total loss by summing the individual loss (<em>L</em>) over all the time steps:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dcef82b0-e784-40a7-9d94-5230a6aec74a.png" style="width:14.08em;height:2.25em;"/></p>
<p>This means that we can compute the gradient for each time step separately. To greatly simplify the calculations, we will assume that <em>tanh</em> = <em>identity</em> (that is, we assume that there is no activation function). For instance, at <em>t</em> = <em>4</em>, we will compute the gradient by applying the chain rule:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2dc89d41-4b50-41c7-9652-606b9839cec7.png" style="width:14.00em;height:2.75em;"/></p>
<p>Here, we stumble upon a complexity—the third term (in bold) on the right-hand side of the equation cannot be easily derived. Indeed, to take the derivative of <em>h<sup>&lt;4&gt;</sup></em> with respect to <em>W<sub>rec</sub></em>, all other terms must not depend on <em>W<sub>rec</sub></em>. However, <em>h<sup>&lt;4&gt;</sup></em> also depends on <em>h<sup>&lt;3&gt;</sup></em>. And <em>h<sup>&lt;3&gt;</sup></em> depends on <em>W<sub>rec</sub></em>, since <em><span>h</span><sup>&lt;3&gt;</sup></em><span>= <em>tanh</em> (<em>W</em></span><em><sub>rec</sub> <span>h</span><sup>&lt;2&gt;</sup></em> <span>+ <em>W</em></span><em><sub>input</sub> <span>x</span><sup>&lt;3&gt;</sup></em><span>+<em>b</em>), and so on and so forth until we reach</span> <em>h<sup>&lt;0&gt;</sup></em><span>, which is entirely composed of zeros.</span></p>
<p>To properly derive this term, we apply the total derivative formula on this partial derivative:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/2301b12c-ff98-425b-8cc0-5384c6a2a653.png" style="width:25.50em;height:2.50em;"/></p>
<div class="packt_infobox">It might seem weird that a term is equal to itself plus other (non-null) terms. However, since we are taking the total derivative of a partial derivative, we need to take into account all the terms in order to generate the gradient.</div>
<div>
<p>By noticing that all the other terms are remaining constant, we obtain the following equations:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0a6c7dd6-ae58-4567-b5fc-81350c329cf6.png" style="width:17.25em;height:2.67em;"/></p>
<p>Therefore, the partial derivative presented before can be expressed as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/165d3572-8480-436e-a48e-35b45fcc897e.png" style="width:19.17em;height:2.67em;"/></p>
<p>In conclusion, we notice that the gradient will depend on all the previous states as well as <em>W<sub>rec</sub></em>. This concept is called <strong>backpropagation through time</strong> (<strong>BPTT</strong>). Since the latest state depends on all the states before it, it only makes sense to consider them to compute the error. As we sum the gradient of each time step to compute the total gradient, and since, for each time step, we have to go back to the first time step to compute the gradient, a large amount of computation is implied. For this reason, RNNs are notoriously slow to train.</p>
</div>
<p>Moreover, we can generalize the previous formula to show that <img class="fm-editor-equation" src="assets/1fd22468-8206-4bb5-bd9b-35c90b6f2489.png" style="width:2.75em;height:2.17em;"/>depends on <em>W<sub>rec</sub></em> to the power of (<em>t-2</em>). This is very problematic when <em>t</em> is large. Indeed, if the terms of <em>W<sub>rec</sub></em> are below one, with the high exponent, they become very small. Worse, if the terms are above one, the gradient tends toward infinity. These phenomena are called <strong>gradient vanishing</strong> and <strong>gradient explosion</strong>, respectively (they were previously described in <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>). Thankfully, workarounds exist to avoid this problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Truncated backpropagation</h1>
                </header>
            
            <article>
                
<p>To circumvent the long training time, it is possible to compute the gradient every <em>k<sub>1</sub></em> time step instead of every step. This divides the number of gradient operations by <em>k<sub>1</sub></em>, making the training of the network faster.</p>
<p>Instead of backpropagating throughout all the time steps, we can also limit the propagation to <em>k<sub>2</sub></em> steps in the past. This effectively limits the gradient vanishing, since the gradient will depend on <em>W<sup>k<sub>2</sub></sup></em> at most. This also limits the computations that are necessary to compute the gradient. However, the network will be less likely to learn long-term temporal relations.</p>
<p>The combination of those two techniques is called <strong>truncated backpropagation</strong>, with its two parameters commonly referred to as <em>k<sub>1</sub></em> and <em>k<sub>2</sub></em>. They must be tuned to ensure a good trade-off between training speed and model performance.</p>
<p>This technique—while powerful—remains a workaround for a fundamental <span>RNN</span> problem. In the next section, we will introduce a change of architecture that can be used to solve this issue in its entirety.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Long short-term memory cells</h1>
                </header>
            
            <article>
                
<p>As we saw previously, regular RNNs suffer from gradient explosion. As such, it can sometimes be hard to teach them long-term relations in sequences of data. Moreover, they store information in a single-state matrix. For instance, if a gunshot happens at the very beginning of a very long video, it will be unlikely that the hidden state of the RNNs will not be overridden by noise by the time it reaches the end of the video. The video might not be classified as violent.</p>
<p>To circumvent those two problems, Sepp Hochreiter and Jürgen Schmidhuber proposed, in their paper (<em>Long Short-Term Memory</em>, <em>Neural Computation</em>, 1997), a variant of the basic RNN—the <strong>Long Short-Term Memory</strong> (<strong>LSTM</strong>) cell. This has improved markedly over the years, with many variants being introduced. In this section, we will give an overview of its inner workings, and we will show why gradient vanishing is less of an issue. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM general principles</h1>
                </header>
            
            <article>
                
<p>Before we detail the mathematics behind the LSTM cell, let's try to get a general understanding of how it works. To do so, we will use the example of a live classification system that is applied to the Olympic Games. The system has to detect, for every frame, which sport is being played during a long video from the Olympics.</p>
<p>If the network sees people standing in line, can it infer what sport it is? Is it soccer players singing the anthem, or is it athletes preparing to run a 100-meter race? Without information about what happened in the frames just prior to this, the prediction will not be accurate. The basic RNN architecture we presented earlier would be able to store this information in the hidden state. However, if the sports are alternating one after the other, it would be much harder. Indeed, the state is used to generate the current predictions. The basic RNN is unable to store information that it will not use immediately.</p>
<p>The LSTM architecture solves this by storing a memory matrix, which is called the <strong>cell state</strong> and is referred to as <em>C<sup>&lt;t&gt;</sup></em>. At every time step, <em>C<sup>&lt;t&gt;</sup></em> contains information about the current state. But this information will not be used <span>directly </span>to generate the output. Instead, it will be filtered by a <em>gate</em>.</p>
<div class="packt_tip">Note that the LSTM's cell state is different from the simple RNN's state, as outlined by the following equations. The LSTM's cell state is filtered before being transformed into the final state.</div>
<p>Gates are the core idea of LSTM's cell. A gate is a matrix that will be multiplied term by term to another element in the LSTM. If all the values of the gate are <em>0</em>, none of the information from the other element will pass through. On the other hand, if the gate values are all around <em>1</em>, all the information of the other element will pass through.</p>
<p>As a reminder, an example of term-by-term multiplication (also called <strong>element-wise multiplication</strong> or the <strong>Hadamard product</strong>) can be depicted as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/98e6f482-4d5e-4a76-a4c2-bf147d661b8f.png" style="width:18.00em;height:2.92em;"/></p>
<p>At each time step, three gate matrices are computed using the current input and the previous output:</p>
<ul>
<li><strong>The input gate</strong>: Applied to the input to decide which information gets through. In our example, if the video is showing members of the audience, we would not want to use this input to generate predictions. The gate would be mostly zeros.</li>
<li><strong>The forget gate</strong>: Applied to the cell state to decide which information to forget. In our example, if the video is showing presenters talking, we would want to forget about the current sport, as we are probably going to see a new sport next.</li>
<li><strong>The output gate</strong>: This will be multiplied by the cell state to decide which information to output. We might want to keep in the cell state the fact that the previous sport was soccer, but this information will not be useful for the current frame. Outputting this information would perturb the upcoming time steps. By setting the gate around zero, we would effectively keep this information for later.</li>
</ul>
<p>In the next section, we will cover how the gates and the candidate state are computed and demonstrate why LSTMs suffer less from gradient vanishing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LSTM inner workings</h1>
                </header>
            
            <article>
                
<p>First, let's detail how the gates are computed:</p>
<p style="padding-left: 180px"><img src="assets/b5777da1-74f4-4480-941d-d114c7f886f5.png" style="width:17.17em;height:5.00em;"/></p>
<p>As detailed in the previous equations, the three gates are computed using the same principle—by multiplying a weight matrix (<em>W</em>) by the previous output (<em>h<sup>&lt;t-1&gt;</sup></em>) and the current input (<em>x<sup>&lt;t&gt;</sup></em>). Notice that the activation function is the sigmoid (σ). As a consequence, the gate values are always between <em>0</em> and <em>1</em>.</p>
<p>The candidate state (<img class="fm-editor-equation" src="assets/729d7117-1700-4f15-b009-bc5a52b11a51.png" style="width:2.33em;height:1.33em;"/>) is computed in a similar fashion. However, the activation function used is a hyperbolic tangent instead of the sigmoid:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8c27adaf-69d5-4c6e-a822-9cb219f86b97.png" style="width:20.75em;height:1.75em;"/></p>
<p>Notice that this formula is exactly the same as the one used to compute <em>h<sup>&lt;t&gt;</sup></em> in the basic RNN architecture. However, <em>h<sup>&lt;t&gt;</sup></em> was the <em>hidden state</em> while, in this case, we are computing the <strong>candidate cell state</strong>. To compute the new cell state, we combine the previous one with the candidate cell state. Both states are gated by the forget and input gates, respectively:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1d704d09-e5d0-48fc-8c44-c5077a57ef37.png" style="width:21.50em;height:1.67em;"/></p>
<p>Finally, the LSTM hidden state (output) will be computed from the cell state as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/90069c9e-8d6f-4409-a98b-4a850ff902a7.png" style="width:15.92em;height:1.50em;"/></p>
<p>The simplified representation of the LSTM cell is depicted in <em>Figure 8-3</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/84f1e8a6-d776-431d-80d8-7c7fb6571eb3.png" style="width:32.92em;height:16.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-3: Simplified representation of the LSTM cell. Gate computation is omitted</div>
<p>LSTM weights are also computed using backpropagation through time. Due to the numerous information paths in LSTM cells, gradient computation is even more complex. However, we can observe that if the terms of the forget gate, <em>f<sup>&lt;t&gt;</sup></em>, are close to <em>1</em>, information can be passed from one cell state to the other, as shown in the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/63f36f34-b96e-48d6-8bfd-6c5e73186690.png" style="width:6.58em;height:2.17em;"/></p>
<p>For this reason, by initializing the forget gate bias to a vector of ones, we can ensure that the information backpropagates through numerous time steps. As such, LSTMs suffer less from gradient vanishing.</p>
<p>This concludes our introduction to RNNs; we can now begin with the hands-on classification of a video.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying videos</h1>
                </header>
            
            <article>
                
<p>From television to web streaming, the video format is getting more and more popular. Since the inception of computer vision, researchers have attempted to apply computer vision to more than one image at a time. While limited by computing power at first, they more recently have developed powerful techniques for video analysis. In this section, we will introduce video-related tasks and detail one of them—video classification.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying computer vision to video</h1>
                </header>
            
            <article>
                
<p>At 30 frames per second, processing every frame of a video implies analyzing <em>30 × 60</em> = <em>180</em> frames per minute. This problem was faced really early in computer vision, before the rise of deep learning. Techniques were then devised to analyze videos efficiently.</p>
<p>The most obvious technique is <strong>sampling</strong>. We can analyze only one or two frames per second instead of all the frames. While more efficient, we may lose information if an important scene appears very briefly, such as in the case of a gunshot, which was mentioned earlier.</p>
<p>A more advanced technique is <strong>scene extraction</strong>. This is particularly popular for analyzing movies. An algorithm detects when the video is changing from one scene to another. For instance, if the camera goes from a close-up view to a wide view, we would analyze a frame from each framing. Even if the close-up is really short and the wide view occurs over many frames, we would extract only one frame from each shot. <em>Scene extraction</em> can be done by using fast and efficient algorithms. They process the pixels of images and evaluate the variation between two consecutive frames. A large variation indicates a scene change.</p>
<p>In addition, all the image-related tasks described in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, also apply to video. For instance, super-resolution, segmentation, and style transfer are commonly targeted at video. However, the temporal aspect of a video creates new applications in the form of the following video-specific tasks:</p>
<ul>
<li><strong>Action detection</strong>: A variant of video classification, the goal here is to classify what actions a person is accomplishing. Actions range from running to playing soccer, but can also be as precise as the kind of dance being performed, or the musical instrument being played.</li>
<li><strong>Next-frame prediction</strong>: Given <em>N</em> consecutive frames, this predicts how frame <em>N+1</em> is going to look.</li>
<li><strong>Ultra slow motion</strong>: This is also called <strong>frame interpolation</strong>. The model has to generate intermediate frames to make slow motion look less jerky.</li>
<li><strong>Object tracking</strong>: This was executed historically using classical computer vision techniques such as descriptors. However, deep learning is now applied to track objects in videos.</li>
</ul>
<p>Of these video-specific tasks, we will focus on action detection. In the next section, we will introduce an action video dataset and cover how to apply an LSTM cell to videos.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Classifying videos with an LSTM</h1>
                </header>
            
            <article>
                
<p>We will make use of the <em>UCF1</em><em>01</em> dataset (<a href="https://www.crcv.ucf.edu/data/UCF101.php">https://www.crcv.ucf.edu/data/UCF101.php</a>), which was put together by K. Soomro <span><span>et al.</span></span> (refer to <em>UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild</em>, CRCV-TR-12-01, 2012). Here are a few examples from the dataset:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/deca8b2d-956c-4dae-ad97-7409d5742783.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-4: Example images from the UCF101 dataset</div>
<p><span>The dataset is composed of 13,320 segments of video. Each segment contains a person performing one of 101 possible actions.</span></p>
<p>To classify the video, we will use a two-step process. Indeed, a recurrent network is not fed the raw pixel images. While it could technically be fed with full images, CNN feature extractors are used beforehand in order to reduce the dimensionality, and to reduce the computations done by LSTMs. Therefore, our network architecture can be represented by <em>Figure 8-5</em>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d30c5edf-b57d-4a39-911e-8ee2255bfc67.png"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-5: Combination of a CNN and an RNN to categorize videos. In this simplified example, the sequence length is 3</div>
<p>As stated earlier, backpropagating errors through RNNs is difficult. While we could train the CNN from scratch, it would take a tremendous amount of time for sub-par results. Therefore, we use a pretrained network, applying the transfer learning technique that was introduced in <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>.</p>
<p>For the same reason, it is also common practice not to fine-tune the CNN and to keep its weights untouched, as this does not bring any performance improvement. Since the CNN will stay unchanged throughout all the epochs of training, a specific frame will always return the same feature vector. This allows us to <em>cache</em> the feature vectors. As the CNN step is the most time-consuming, caching the results means computing the feature vector only once instead of at each epoch, saving us a tremendous amount of training time.</p>
<p>Therefore, we will classify the videos in two steps. First, we will extract the features and cache them. Once this is done, we will train the LSTM on extracted features.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting features from videos</h1>
                </header>
            
            <article>
                
<p class="mce-root">To generate feature vectors, we will use a pretrained inception network trained on the ImageNet dataset to categorize images in different categories.</p>
<p class="mce-root">We will remove the last layer (the fully connected layer) and only keep the feature vector that is generated after a max-pooling operation.</p>
<p class="mce-root">Another option would be to keep the output of the layer just before average-pooling, that is, the higher-dimensional feature maps. However, in our example, we will not need spatial information—whether the action takes place in the middle of the frame or in the corner, the predictions will be the same. Therefore, we will use the output of the two-dimensional max-pooling layer. This will make the training faster, since the input of the LSTM will be 64 times smaller (<em>64</em> = <em>8</em> <span>×</span> <em>8</em> = the size of a feature map for an input image of size <em>299</em> × <em>299</em>).</p>
<p class="mce-root">TensorFlow allows us to access a pretrained model with a single line, as described in <a href="061eb54a-4e3f-44e8-afb1-bacf796511f4.xhtml">Chapter 4</a>, <em>Influential Classification Tools</em>:</p>
<pre>inception_v3 = tf.keras.applications.InceptionV3(include_top=False, weights='imagenet')</pre>
<p class="mce-root">We add the max-pooling operation to transform the <em>8</em> <span>×</span> <em>8</em> <span>×</span> <em>2,048</em> feature map into a <em>1</em> <span>×</span> <em>2,048</em> vector:</p>
<pre>x = inception_v3.output<br/>pooling_output = tf.keras.layers.GlobalAveragePooling2D()(x)<br/><br/>feature_extraction_model = tf.keras.Model(inception_v3.input, pooling_output)</pre>
<p class="mce-root">We will use the <kbd>tf.data</kbd> API to load the frames from the video. An initial problem arises—all the videos have different lengths. Here is the distribution of the number of frames:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/d45fba54-3893-4f07-908b-7854257096e0.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8-6: Distribution of the number of frames per video in the UCF101 dataset</div>
<div class="mce-root packt_tip">It is always good practice to run a quick analysis on data before using it. Manually reviewing it and plotting the distributions can save a lot of experimenting time.</div>
<p>With TensorFlow, as with most deep learning frameworks, all examples in a batch must be of the same length. The most common solution to fit this requirement is <em>padding</em>—we fill the first temporal time steps with actual data and the last ones with zeros.</p>
<p>In our case, we will not use all the frames from the video. At 25 frames per second, most of the frames look alike. By using only a subset of the frames, we will reduce the size of our input, and therefore speed up the training process. To select this subset, we can use any of the following options:</p>
<ul>
<li>Extract <em>N</em> frames per second.</li>
<li>Sample <em>N</em> frames out of all the frames.</li>
<li>Segment the videos in scenes and extract <em>N</em> frames per scene, as shown in the following diagram:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1ba1d585-f268-460d-b199-f2773f7e4606.png" style="width:39.25em;height:18.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8-7: Comparison of two sampling techniques. Dotted rectangles indicate zero padding</div>
<p>Due to the large variation in video length, extracting <em>N</em> frames per second would also result in a large variation in input length. Although this could be solved with padding, we would end up with some inputs mostly composed of zeros—this could lead to poor training performance. We will, therefore, sample <em>N</em> images per video.</p>
<p>We will use the TensorFlow dataset API to feed the input to our feature extraction network:</p>
<pre>dataset = tf.data.Dataset.from_generator(frame_generator,<br/>             output_types=(tf.float32, tf.string),<br/>             output_shapes=((299, 299, 3), ())</pre>
<p>In the previous code, we specify the input type and the input shape. Our generator will return images of shape <em>299</em> <span>×</span> <em>299</em> with three channels, as well as a string representing the filename. The filename will be used to group the frames by video later on.</p>
<p class="mce-root">The role of <kbd>frame_generator</kbd> is to select the frames that will be processed by the network. We use the OpenCV library to read from the video file. For each video, we will sample an image every <em>N</em> frames, where <em>N</em> equals <kbd>num_frames / SEQUENCE_LENGTH</kbd> and <kbd>SEQUENCE_LENGTH</kbd> is the size of the input sequence of the LSTM. A simplified version of this generator looks like this:</p>
<pre>def frame_generator():<br/>    video_paths = tf.io.gfile.glob(VIDEOS_PATH)<br/>    for video_path in video_paths:<br/>        capture = cv2.VideoCapture(video_path)<br/>        num_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))<br/>        sample_every_frame = max(1, num_frames // SEQUENCE_LENGTH)<br/>        current_frame = 0<br/><br/>        label = os.path.basename(os.path.dirname(video_path))<br/>        while True:<br/>            success, frame = capture.read()<br/>            if not success:<br/>                break<br/><br/>            if current_frame % sample_every_frame == 0:<br/>                img = preprocess_frame(frame)<br/>                yield img, video_path<br/><br/>            current_frame += 1</pre>
<p>We iterate over the frames of the video, processing only a subset. At the end of the video, the OpenCV library will return <kbd>success</kbd> as <kbd>False</kbd> and the loop will terminate.</p>
<div class="packt_tip">Note that just like in any Python generator, instead of using the <kbd>return</kbd> keyword, we use the <kbd>yield</kbd> keyword. This allows us to start returning frames before the end of the loop. This way, the network can start training without waiting for all the frames to be preprocessed.</div>
<p>Finally, we iterate over the dataset to generate video features:</p>
<pre>dataset = dataset.batch(16).prefetch(tf.data.experimental.AUTOTUNE)<br/>current_path = None<br/>all_features = []<br/><br/>for img, batch_paths in tqdm.tqdm(dataset):<br/>    batch_features = feature_extraction_model(img)<br/>    <br/>    for features, path in zip(batch_features.numpy(), batch_paths.numpy()):<br/>        if path != current_path and current_path is not None:<br/>            output_path = current_path.decode().replace('.avi', '')<br/>            np.save(output_path, all_features)<br/>            all_features = []<br/>            <br/>        current_path = path<br/>        all_features.append(features)</pre>
<p>In the previous code, note that we iterate over the batch output and compare video filenames. We do so because the batch size is not necessarily the same as <em>N</em> (the number of frames we sample per video). Therefore, a batch may contain frames from multiple consecutive sequences:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f5dd6d25-9070-4070-a4ee-7b942f513eee.png" style="width:28.75em;height:20.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 8-8: Representation of the input for a batch size of four and three sampled frames per video</div>
<p>We read the output of the network, and when we reach a different filename, we save the video features to file. Note that this technique will only work if the frames are in the correct order. If the dataset is shuffled, it would no longer work. Video features are saved at the same location as the video, but with a different extension (<kbd>.npy</kbd> instead of <kbd>.avi</kbd>).</p>
<p>This step iterates over the 13,320 videos of the dataset and generates features for every single one of them. Sampling 40 frames per video takes about one hour on a modern GPU.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the LSTM</h1>
                </header>
            
            <article>
                
<p>Now that the video features are generated, we can use them to train an LSTM. This step is very similar to the training steps described earlier in the book—we define a model and an input pipeline, and launch the training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the model</h1>
                </header>
            
            <article>
                
<p>Our model is a simple sequential model, defined using Keras layers:</p>
<pre>model = tf.keras.Sequential([<br/>    tf.keras.layers.Masking(mask_value=0.),<br/>    tf.keras.layers.LSTM(512, dropout=0.5, recurrent_dropout=0.5),<br/>    tf.keras.layers.Dense(256, activation='relu'),<br/>    tf.keras.layers.Dropout(0.5),<br/>    tf.keras.layers.Dense(len(LABELS), activation='softmax')<br/>])</pre>
<p>We apply a dropout, a concept introduced in <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks.</em> The <kbd>dropout</kbd> parameter of the LSTM controls how much dropout is applied to the input weight matrix. The <kbd>recurrent_dropout</kbd> parameter controls how much dropout is applied to the previous state. Similar to a mask, <kbd>recurrent_dropout</kbd> randomly ignores part of the previous state activations in order to avoid overfitting.</p>
<p>The very first layer of our model is a <kbd>Masking</kbd> layer. As we padded our image sequences with empty frames in order to batch them, our LSTM cell would needlessly iterate over those added frames. Adding the <kbd>Masking</kbd> layer ensures the LSTM layer stops at the actual end of the sequence, before it encounters a zero matrix.</p>
<div class="packt_infobox"><span>The model will categorize videos in 101 categories, such as <em>kayaking</em>, <em>rafting,</em> or <em>fencing</em>. However, it will only predict a vector representing the predictions. We need a way to convert those 101 categories into vector form. We will use a technique called</span> <strong>one-hot encoding</strong>, <span>described in</span> <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em><span>. Since we have 101 different labels, we will return a vector of size 101. For <em>kayaking</em>, the vector will be full of</span> zeros <span>except for the first item, which is set to</span> <em>1</em><span>. For <em>rafting</em>, it will be</span> <em>0</em> <span>except for the second element, which is set to</span> <em>1</em><span>, and so on for the other categories.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the data</h1>
                </header>
            
            <article>
                
<p>We will load the <kbd>.npy</kbd> files that are produced when generating frame features using a generator. The code ensures that all the input sequences have the same length, padding them with zeros if necessary:</p>
<pre>def make_generator(file_list):<br/>    def generator():<br/>        np.random.shuffle(file_list)<br/>        for path in file_list:<br/>            full_path = os.path.join(BASE_PATH, path)<br/>            full_path = full_path.replace('.avi', '.npy')<br/><br/>            label = os.path.basename(os.path.dirname(path))<br/>            features = np.load(full_path)<br/><br/>            padded_sequence = np.zeros((SEQUENCE_LENGTH, 2048))<br/>            padded_sequence[0:len(features)] = np.array(features)<br/><br/>            transformed_label = encoder.transform([label])<br/>            yield padded_sequence, transformed_label[0]<br/>    return generator</pre>
<p>In the previous code, we defined a Python <strong>closure function</strong>—a function that returns another function. This technique allows us to create <kbd>train_dataset</kbd>, returning training data, and <kbd>validation_dataset</kbd>, returning validation data with just one generator function:</p>
<pre>train_dataset = tf.data.Dataset.from_generator(make_generator(train_list),<br/>                 output_types=(tf.float32, tf.int16),<br/>                 output_shapes=((SEQUENCE_LENGTH, 2048), (len(LABELS))))<br/>train_dataset = train_dataset.batch(16)<br/>train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)<br/><br/>valid_dataset = tf.data.Dataset.from_generator(make_generator(test_list),<br/>                 output_types=(tf.float32, tf.int16),<br/>                 output_shapes=((SEQUENCE_LENGTH, 2048), (len(LABELS))))<br/>valid_dataset = valid_dataset.batch(16)<br/>valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)</pre>
<p>We also batch and prefetch the data according to the best practices that were described in <a href="337ec077-c215-4782-b56c-beae4d94d718.xhtml">Chapter 7</a>, <em><em>Training on Complex and Scarce Datasets</em></em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p>The training procedure is very similar to those previously described in the book, and we invite the readers to refer to the notebook attached to this chapter. Using the model described <span>previously</span>, we reach a precision level of 72% on the validation set.</p>
<p>This result can be compared to the state-of-the-art precision level of 94%, which is obtained when using more advanced techniques. Our simple model could be enhanced by improving frame sampling, using data augmentation, using a different sequence length, or by optimizing the size of the layers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We expanded our knowledge of neural networks by describing the general principles of RNNs. After covering the inner workings of the basic RNN, we extended backpropagation to apply it to recurrent networks. As presented in this chapter, BPTT suffers from gradient vanishing when applied to RNNs. This can be worked around by using truncated backpropagation, or <span>by using a different type of architecture—LSTM networks</span>.</p>
<p>We applied those theoretical principles to a practical problem—action recognition in videos. By combining CNNs and LSTMs, we successfully trained a network to classify videos in 101 categories, introducing video-specific techniques such as frame sampling and padding.</p>
<p>In the next chapter, we will broaden our knowledge of neural network applications by covering new platforms—mobile devices and web browsers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the main advantages of LSTMs over the simple RNN architecture?</li>
<li>What is the CNN used for when it is applied before the LSTM?</li>
<li>What is gradient vanishing and why does it occur? Why is it a problem?</li>
<li>What are some of the workarounds for gradient vanishing?</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><em>RNNs with Python Quick Start Guide</em> (<a href="https://www.packtpub.com/big-data-and-business-intelligence/recurrent-neural-networks-python-quick-start-guide">https://www.packtpub.com/big-data-and-business-intelligence/recurrent-neural-networks-python-quick-start-guide</a>), by Simeon Kostadinov: This book details RNN architectures, and applies them to examples using TensorFlow 1.</li>
</ul>
<ul>
<li><em>A Critical Review of RNNs for Sequence Learning</em> (<a href="https://arxiv.org/abs/1506.00019">https://arxiv.org/abs/1506.00019</a>), by Zachary C. Lipton et al.: This survey reviews and synthesizes three decades of RNN architectures.</li>
<li><em>Empirical Evaluation of Gated RNNs on Sequence Modeling</em> (<a href="https://arxiv.org/abs/1412.3555">https://arxiv.org/abs/1412.3555</a>), by Junyoung Chung et al.: This paper compares the performance of different RNN architectures.</li>
</ul>


            </article>

            
        </section>
    </body></html>