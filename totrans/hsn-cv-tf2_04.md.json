["```\n# Initializing the trainable variables (for instance, the filters with values from a Glorot distribution, and the bias with zeros):\nkernels_shape = [k, k, D, N]\nglorot_uni_initializer = tf.initializers.GlorotUniform()\n# ^ this object is defined to generate values following the Glorot distribution (note that other famous parameter more or less random initializers exist, also covered by TensorFlow)\nkernels = tf.Variable(glorot_uni_initializer(kernels_shape), \n                      trainable=True, name=\"filters\")\nbias = tf.Variable(tf.zeros(shape=[N]), trainable=True, name=\"bias\")\n\n# Defining our convolutional layer as a compiled function:\n@tf.function\ndef conv_layer(x, kernels, bias, s):\n    z = tf.nn.conv2d(x, kernels, strides=[1,s,s,1], padding='VALID')\n    # Finally, applying the bias and activation function (for instance, ReLU):\n    return tf.nn.relu(z + bias)\n```", "```\nclass SimpleConvolutionLayer(tf.keras.layers.Layer):\n    def __init__(self, num_kernels=32, kernel_size=(3, 3), stride=1):\n        \"\"\" Initialize the layer.\n        :param num_kernels: Number of kernels for the convolution\n        :param kernel_size: Kernel size (H x W)\n        :param stride: Vertical/horizontal stride\n        \"\"\"\n        super().__init__() \n        self.num_kernels = num_kernels\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n    def build(self, input_shape):\n        \"\"\" Build the layer, initializing its parameters/variables.\n        This will be internally called the 1st time the layer is used.\n        :param input_shape: Input shape for the layer (for instance, BxHxWxC)\n        \"\"\"\n        num_input_ch = input_shape[-1] # assuming shape format BHWC\n        # Now we know the shape of the kernel tensor we need:\n        kernels_shape = (*self.kernel_size, num_input_ch, self.num_kernels)\n        # We initialize the filter values fior instance, from a Glorot distribution:\n        glorot_init = tf.initializers.GlorotUniform()\n        self.kernels = self.add_weight( # method to add Variables to layer\n            name='kernels', shape=kernels_shape, initializer=glorot_init,\n            trainable=True) # and we make it trainable.\n        # Same for the bias variable (for instance, from a normal distribution):\n        self.bias = self.add_weight(\n            name='bias', shape=(self.num_kernels,), \n            initializer='random_normal', trainable=True)\n\n    def call(self, inputs):\n        \"\"\" Call the layer, apply its operations to the input tensor.\"\"\"\n        return conv_layer(inputs, self.kernels, self.bias, self.stride)\n```", "```\nconv = tf.keras.layers.Conv2D(filters=N, kernel_size=(k, k), strides=s,\n                              padding='valid', activation='relu')\n```", "```\navg_pool = tf.keras.layers.AvgPool2D(pool_size=k, strides=[s, s], padding='valid')\nmax_pool = tf.keras.layers.MaxPool2D(pool_size=k, strides=[s, s], padding='valid')\n```", "```\nfc = tf.keras.layers.Dense(units=output_size, activation='relu')\n```", "```\nfrom tensorflow.keras.model import Model, Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\nmodel = Sequential() # `Sequential` inherits from tf.keras.Model\n# 1st block:\nmodel.add(Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', \n input_shape=(img_height, img_width, img_channels))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# 2nd block:\nmodel.add(Conv2D(16, kernel_size=(5, 5), activation='relu')\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\n# Dense layers:\nmodel.add(Flatten())\nmodel.add(Dense(120, activation='relu'))\nmodel.add(Dense(84, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n```", "```\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n\nclass LeNet5(Model): # `Model` has the same API as `Layer` + extends it\n    def __init__(self, num_classes): # Create the model and its layers\n        super(LeNet5, self).__init__()\n        self.conv1 = Conv2D(6, kernel_size=(5, 5), padding='same', \n                            activation='relu')\n        self.conv2 = Conv2D(16, kernel_size=(5, 5), activation='relu')\n        self.max_pool = MaxPooling2D(pool_size=(2, 2))\n        self.flatten = Flatten()\n        self.dense1 = Dense(120, activation='relu')\n        self.dense2 = Dense(84, activation='relu')\n        self.dense3 = Dense(num_classes, activation='softmax')\n    def call(self, x): # Apply the layers in order to process the inputs\n        x = self.max_pool(self.conv1(x)) # 1st block\n        x = self.max_pool(self.conv2(x)) # 2nd block\n        x = self.flatten(x)\n        x = self.dense3(self.dense2(self.dense1(x))) # dense layers\n        return x\n```", "```\nmodel.compile(optimizer='sgd', loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n# We also instantiate some Keras callbacks, that is, utility functions automatically called at some points during training to monitor it:\ncallbacks = [\n    # To interrupt the training if `val_loss` stops improving for over 3 epochs:\n    tf.keras.callbacks.EarlyStopping(patience=3, monitor='val_loss'),\n    # To log the graph/metrics into TensorBoard (saving files in `./logs`):\n    tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)]\n# Finally, we launch the training:\nmodel.fit(x_train, y_train, batch_size=32, epochs=80, \n          validation_data=(x_test, y_test), callbacks=callbacks)\n```", "```\noptimizer = tf.optimizers.SGD(lr=0.01, momentum=0.9, # `momentum` = \"mu\"\n                              decay=0.0, nesterov=False)\n```", "```\n@tf.function\ndef train_step(batch_images, batch_gts): # typical training step\n    with tf.GradientTape() as grad_tape: # Tell TF to tape the gradients\n        batch_preds = model(batch_images, training=True) # forward\n        loss = tf.losses.MSE(batch_gts, batch_preds)     # compute loss\n    # Get the loss gradients w.r.t trainable parameters and back-propagate:\n    grads = grad_tape.gradient(loss, model.trainable_variables)\n    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n```", "```\nfrom functools import partial\n\ndef l2_reg(coef=1e-2): # reimplementation of tf.keras.regularizers.l2()\n    return lambda x: tf.reduce_sum(x ** 2) * coef\n\nclass ConvWithRegularizers(SimpleConvolutionLayer):\n    def __init__(self, num_kernels=32, kernel_size=(3, 3), stride=1,\n                 kernel_regularizer=l2_reg(), bias_regularizer=None):\n        super().__init__(num_kernels, kernel_size, stride)  \n        self.kernel_regularizer = kernel_regularizer\n        self.bias_regularizer = bias_regularizer\n\n    def build(self, input_shape):\n        super().build(input_shape)\n        # Attaching the regularization losses to the variables.\n        if self.kernel_regularizer is not None:\n            # for instance, we tell TF to compute and save\n            # `tf.nn.l1_loss(self.kernels)` at each call (that is iteration):\n            self.add_loss(partial(self.kernel_regularizer, self.kernels))\n        if self.bias_regularizer is not None:\n            self.add_loss(partial(self.bias_regularizer, self.bias))\n```", "```\n# We create a NN containing layers with regularization/additional losses:\nmodel = Sequential()\nmodel.add(ConvWithRegularizers(6, (5, 5), kernel_regularizer=l2_reg())\nmodel.add(...) # adding more layers\nmodel.add(Dense(num_classes, activation='softmax'))\n\n# We train it (c.f. function `training_step()` defined before):\nfor epoch in range(epochs):\n    for (batch_images, batch_gts) in dataset:\n        with tf.GradientTape() as grad_tape:\n            loss = tf.losses.sparse_categorical_crossentropy(\n                batch_gts, model(batch_images)) # main loss\n            loss += sum(model.losses)           # list of addit. losses\n        # Get the gradients of combined losses and back-propagate:\n        grads = grad_tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n```", "```\n# We instantiate a regularizer (L1 for example):\nl1_reg = tf.keras.regularizers.l1(0.01)\n# We can then pass it as a parameter to the target model's layers:\nmodel = Sequential()\nmodel.add(Conv2D(6, kernel_size=(5, 5), padding='same', activation='relu', \n                 input_shape=input_shape, kernel_regularizer=l1_reg))\nmodel.add(...) # adding more layers\nmodel.fit(...) # training automatically taking into account the reg. terms.\n\n```", "```\nmodel = Sequential([ # ...\n    Dense(120, activation='relu'),\n    Dropout(0.2),    # ...\n])\n```"]