- en: '*Chapter 8*: Distributed Training for Accelerated Development of Deep RL Agents'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：加速深度强化学习代理开发的分布式训练'
- en: Training Deep RL agents to solve a task takes enormous wall-clock time due to
    the high sample complexity. For real-world applications, iterating over agent
    training and testing cycles at a faster pace plays a crucial role in the market
    readiness of a Deep RL application. The recipes in this chapter provide instructions
    on how to speed up Deep RL agent development using the distributed training of
    deep neural network models by leveraging TensorFlow 2.x’s capabilities. Strategies
    for utilizing multiple CPUs and GPUs both on a single machine and across a cluster
    of machines are discussed. Multiple recipes for training distributed **Deep Reinforcement
    Learning** (**Deep RL**) agents using the **Ray**, **Tune**, and **RLLib** frameworks
    are also provided.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度强化学习代理解决任务需要大量的时间，因为其样本复杂度很高。对于实际应用，快速迭代代理训练和测试周期对于深度强化学习应用的市场就绪度至关重要。本章中的配方提供了如何利用
    TensorFlow 2.x 的能力，通过分布式训练深度神经网络模型来加速深度强化学习代理开发的说明。讨论了如何在单台机器以及跨机器集群上利用多个 CPU
    和 GPU 的策略。本章还提供了使用**Ray**、**Tune** 和 **RLLib** 框架训练分布式**深度强化学习**（**Deep RL**）代理的多个配方。
- en: 'Specifically, the following recipes are a part of this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章包含以下配方：
- en: Building distributed deep learning models using TensorFlow 2.x – Multi-GPU training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2.x 构建分布式深度学习模型 – 多 GPU 训练
- en: Scaling up and out – Multi-machine, multi-GPU training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展规模与范围 – 多机器、多 GPU 训练
- en: Training Deep RL agents at scale – Multi-GPU PPO agent
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大规模训练深度强化学习代理 – 多 GPU PPO 代理
- en: Building blocks for distributed Deep Reinforcement Learning for accelerated
    training
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为加速训练构建分布式深度强化学习的构建模块
- en: Large-scale Deep RL agent training using Ray, Tune, and RLLib
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ray、Tune 和 RLLib 进行大规模深度强化学习（Deep RL）代理训练
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04
    and should work with later versions of Ubuntu if Python 3.6+ is available. With
    Python 3.6+ installed along with the necessary Python packages, as listed before
    the start of each of the recipes, the code should run fine on Windows and Mac
    OSX too. It is advised to create and use a Python virtual environment named `tf2rl-cookbook`
    to install the packages and run the code in this book. Miniconda or Anaconda installation
    for Python virtual environment management is recommended.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码在 Ubuntu 18.04 和 Ubuntu 20.04 上经过广泛测试，且如果安装了 Python 3.6+，应该也能在之后版本的 Ubuntu
    上运行。只要安装了 Python 3.6+ 以及所需的 Python 包（每个配方开始前都会列出），代码也应该能够在 Windows 和 Mac OSX 上正常运行。建议创建并使用名为
    `tf2rl-cookbook` 的 Python 虚拟环境来安装本书中所需的包并运行代码。推荐使用 Miniconda 或 Anaconda 来管理 Python
    虚拟环境。
- en: 'The complete code for each recipe in each chapter will be available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 每个配方的完整代码可以在此获取：[https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook)。
- en: Distributed deep learning models using TensorFlow 2.x – Multi-GPU training
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 2.x 进行分布式深度学习模型训练 – 多 GPU 训练
- en: Deep RL utilizes a deep neural network for policy, value-function, or model
    representations. For higher-dimensional observation/state spaces, for example,
    in the case of image or image-like observations, it is typical to use **convolutional
    neural network** (**CNN**) architectures. While CNNs are powerful and enable training
    Deep RL policies for vision-based control tasks, training deep CNNs requires a
    lot of time, especially in the RL setting. This recipe will help you understand
    how we can leverage TensorFlow 2.x’s distributed training APIs to train deep **residual
    networks** (**ResNets**) using multiple GPUs. The recipe comes with configurable
    building blocks that you can use to build Deep RL components like deep policy
    networks or value networks.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习利用深度神经网络进行策略、价值函数或模型表示。对于高维观察/状态空间，例如图像或类似图像的观察，通常会使用**卷积神经网络**（**CNN**）架构。虽然
    CNN 强大且能训练适用于视觉控制任务的深度强化学习策略，但在强化学习的设置下，训练深度 CNN 需要大量时间。本配方将帮助你了解如何利用 TensorFlow
    2.x 的分布式训练 API，通过多 GPU 训练深度**残差网络**（**ResNets**）。本配方提供了可配置的构建模块，你可以用它们来构建深度强化学习组件，比如深度策略网络或价值网络。
- en: Let’s get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. Having access to a (local or cloud) machine with one or more GPUs will
    be beneficial for this recipe. We will be using the `tensorflow_datasets`. This
    should be already installed if you used `tfrl-cookbook.yml` to set up/update your
    conda environment.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个食谱，你需要首先激活 `tf2rl-cookbook` Python/conda 虚拟环境。确保更新环境，以匹配食谱代码库中最新的 conda
    环境规范文件（`tfrl-cookbook.yml`）。拥有一台（本地或云端）配备一个或多个 GPU 的机器将对这个食谱有帮助。我们将使用 `tensorflow_datasets`，如果你使用
    `tfrl-cookbook.yml` 来设置/更新了你的 conda 环境，它应该已经安装好了。
- en: Now, let’s begin!
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: How to do it...
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'The implementation in this recipe is based on the latest official TensorFlow
    documentation/tutorial. The following steps will help you get a good command over
    TensorFlow 2.x’s distributed execution capabilities. We will be using a ResNet
    model as an example of a large model that will benefit from being trained in a
    distributed fashion, utilizing multiple GPUs to speed up training. We will discuss
    the code snippets for the main components for building a ResNet. Please refer
    to the `resnet.py` file in the cookbook’s code repository for the full and complete
    implementation. Let’s get started:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中的实现基于最新的官方 TensorFlow 文档/教程。接下来的步骤将帮助你深入掌握 TensorFlow 2.x 的分布式执行能力。我们将使用
    ResNet 模型作为大模型的示例，它将从分布式训练中受益，利用多个 GPU 加速训练。我们将讨论构建 ResNet 的主要组件的代码片段。完整的实现请参考食谱代码库中的
    `resnet.py` 文件。让我们开始：
- en: 'Let’s jump right into the template for building residual neural networks:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们直接进入构建残差神经网络的模板：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'With the above template for a ResNet block, we can quickly build ResNets with
    multiple ResNet blocks. We will implement a ResNet with one ResNet block here
    in the book, and you will find the ResNet implemented with multiple configurable
    numbers and sizes of ResNet blocks in the code repository. Let’s get started and
    complete the ResNet implementation in the following several steps, focusing on
    one important concept at a time. First, let’s define the function signature:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上面的 ResNet 块模板，我们可以快速构建包含多个 ResNet 块的 ResNet。在本书中，我们将实现一个包含一个 ResNet 块的 ResNet，你可以在代码库中找到实现了多个可配置数量和大小的
    ResNet 块的 ResNet。让我们开始并在接下来的几个步骤中完成 ResNet 的实现，每次集中讨论一个重要的概念。首先，让我们定义函数签名：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, let’s handle the channel ordering in the input image data representation.
    The most common ordering of the dimensions is either: `batch_size` x `channels`
    x `width` x `height` or `batch_size` x `width` x `height` x `channels`. We will
    handle these two cases:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们处理输入图像数据表示中的通道顺序。最常见的维度顺序是：`batch_size` x `channels` x `width` x `height`
    或 `batch_size` x `width` x `height` x `channels`。我们将处理这两种情况：
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, let’s apply zero padding to the input and apply initial layers to start
    processing:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们对输入数据进行零填充，并应用初始层开始处理：
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'It’s time to add the ResNet blocks using the `resnet_block` function we created:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在是时候使用我们创建的 `resnet_block` 函数来添加 ResNet 块了：
- en: '[PRE4]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As the final layer, we want to add a `softmax` activated `Dense` (fully connected)
    layer with the number of nodes equal to the number of output classes needed for
    the task:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为最终层，我们希望添加一个经过 `softmax` 激活的 `Dense`（全连接）层，节点数量等于任务所需的输出类别数：
- en: '[PRE5]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The last step in the ResNet model building function is to wrap the layers as
    a TensorFlow 2.x Keras model and return the output:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 ResNet 模型构建函数中的最后一步是将这些层封装为一个 TensorFlow 2.x Keras 模型，并返回输出：
- en: '[PRE6]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Using the ResNet function that we just discussed, it becomes quite easy to
    build deep residual networks of varying layer depths by simply changing the number
    of blocks. For example, the following is possible:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们刚才讨论的 ResNet 函数，通过简单地改变块的数量，构建具有不同层深度的深度残差网络变得非常容易。例如，以下是可能的：
- en: '[PRE7]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'With our model defined, we can jump to the multi-GPU training code. The remaining
    steps in this recipe will guide you through the implementation that will allow
    you to speed up training the ResNet using all the available GPUs on a machine.
    Let’s start by importing the `ResNet` module that we built along with the `tensorflow_datasets`
    module:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义好我们的模型后，我们可以跳到多 GPU 训练代码。本食谱中的剩余步骤将引导你完成实现过程，帮助你利用机器上的所有可用 GPU 加速训练 ResNet。让我们从导入我们构建的
    `ResNet` 模块以及 `tensorflow_datasets` 模块开始：
- en: '[PRE8]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can now choose which dataset we want to use to exercise our distributed
    training pipeline. For this recipe, we will use the `dmlab` dataset that contains
    images typically observed by RL agents acting in the DeepMind Lab environment.
    Depending on the compute capabilities of the GPUs, RAM, and CPUs on your training
    machine, you may want to use a smaller dataset such as `CIFAR10`:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以选择使用哪个数据集来运行我们的分布式训练管道。在这个食谱中，我们将使用`dmlab`数据集，该数据集包含在DeepMind Lab环境中，RL代理通常观察到的图像。根据你训练机器的GPU、RAM和CPU的计算能力，你可能想使用一个更小的数据集，比如`CIFAR10`：
- en: '[PRE9]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The next step needs your full attention! We are going to choose the distributed
    execution strategy. TensorFlow 2.x has wrapped a lot of functionality into a simple
    API call like the one listed here:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步需要你全神贯注！我们将选择分布式执行策略。TensorFlow 2.x将许多功能封装成了一个简单的API调用，如下面所示：
- en: '[PRE10]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will declare the key hyperparameters in this step that you can adjust depending
    on your machine’s hardware (such as RAM and GPU memory):'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这一步中，我们将声明关键超参数，你可以根据机器的硬件（例如RAM和GPU内存）进行调整：
- en: '[PRE11]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Before we start preparing the datasets, let’s implement a preprocessing function
    that performs operations before we pass the images to the neural network. You
    can add your own custom preprocessing operations. In this recipe, we will only
    need to cast the image data to `float32` first and then convert the image pixel
    value ranges to be [0, 1] rather than the typical interval of [0, 255]:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在开始准备数据集之前，让我们实现一个预处理函数，该函数在将图像传递给神经网络之前执行操作。你可以添加你自己的自定义预处理操作。在这个食谱中，我们只需要首先将图像数据转换为`float32`，然后将图像像素值范围转换为[0,
    1]，而不是典型的[0, 255]区间：
- en: '[PRE12]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We are ready to create the dataset splits for training and validation/testing:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好为训练和验证/测试创建数据集划分：
- en: '[PRE13]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We are at the crucial step of this recipe! Let’s instantiate and compile our
    model within the scope of the distributed strategy:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经到了这个食谱的关键步骤！让我们在分布式策略的范围内实例化并编译我们的模型：
- en: '[PRE14]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s also create callbacks for logging to TensorBoard and checkpointing our
    model parameters during training:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们还创建一些回调，用于将日志记录到TensorBoard，并在训练过程中检查点保存我们的模型参数：
- en: '[PRE15]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With that, we have everything needed to train our model using the distributed
    strategy. With Keras’s user-friendly `fit()` API, it is as simple as the following:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有了这些，我们已经具备了使用分布式策略训练模型所需的一切。借助Keras用户友好的`fit()`API，它就像下面这样简单：
- en: '[PRE16]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'When the preceding line is executed, the training process will start. We can
    also manually save the model using the following lines:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当执行前面的行时，训练过程将开始。我们也可以使用以下几行手动保存模型：
- en: '[PRE17]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Once we have a saved checkpoint, it is easy to load the weights and start evaluating
    the model:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们保存了检查点，加载权重并开始评估模型就变得很容易：
- en: '[PRE18]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'To verify that the trained model using the distributed strategy works with
    and without replication, we will load it using two different methods in the following
    steps and evaluate. First, let’s load the model without replicating using the
    (same) strategy we used to train the model:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证使用分布式策略训练的模型在有复制和没有复制的情况下都能正常工作，我们将在接下来的步骤中使用两种不同的方法加载并评估它。首先，让我们使用我们用来训练模型的（相同的）策略加载不带复制的模型：
- en: '[PRE19]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, let’s load the model within the distributed execution strategy’s scope,
    which would create replicas and evaluate the model:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，让我们在分布式执行策略的范围内加载模型，这将创建副本并评估模型：
- en: '[PRE20]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When you execute the preceding two code blocks, you will notice that both the
    methods result in the same evaluation accuracy, which is a good sign and signifies
    that we can use the model for prediction without any constraints on the execution
    strategy!
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当你执行前面的两个代码块时，你会发现两种方法都会得到相同的评估准确度，这是一个好兆头，意味着我们可以在没有任何执行策略限制的情况下使用模型进行预测！
- en: That completes our recipe. Let’s recap and look at how the recipe works.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这完成了我们的食谱。让我们回顾一下并看看食谱是如何工作的。
- en: How it works...
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: A residual block in a neural network architecture applies convolution filters
    followed by multiple identity blocks. Specifically, a convolution block is applied
    once, followed by (size - 1) identity blocks where size is an integer representing
    the number of constituent convolutional-identity blocks. The identity block implements
    the shortcut or skip connections for the inputs to go through without being filtered
    through convolution operators. The convolutional block implements convolution
    layers followed by batch-normalization activation, followed by one or more sets
    of convolution-batchnorm-activation layers. The `resnet` module we built uses
    these convolution and identity building blocks to build a full ResNet with varying
    sizes that can be configured by simply changing the number of blocks. The size
    of the network is calculated as `6 * num_blocks + 2`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构中的残差块应用了卷积滤波器，后接多个恒等块。具体来说，卷积块应用一次，接着是(size - 1)个恒等块，其中size是一个整数，表示卷积-恒等块的数量。恒等块实现了跳跃连接或短路连接，使得输入可以绕过卷积操作直接通过。卷积块则包含卷积层，后接批量归一化激活，再接一个或多个卷积-批归一化-激活层。我们构建的`resnet`模块使用这些卷积和恒等构建块来构建一个完整的ResNet，并且可以通过简单地更改块的数量来配置不同大小的网络。网络的大小计算公式为`6
    * num_blocks + 2`。
- en: 'Once our ResNet model was ready, we used the `tensorflow_datasets` module to
    generate training and validation datasets. The TensorFlow Datasets module offers
    several popular datasets, such as CIFAR10, CIFAR100, and DMLAB, that have images
    and the associated labels for classification tasks. The list of all the available
    datasets can be found here: [https://tensorflow.org/datasets/catalog](https://tensorflow.org/datasets/catalog).'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的ResNet模型准备好，我们使用`tensorflow_datasets`模块生成训练和验证数据集。TensorFlow数据集模块提供了几个流行的数据集，如CIFAR10、CIFAR100和DMLAB，这些数据集包含图像及其相关标签，用于分类任务。所有可用数据集的列表可以在此找到：[https://tensorflow.org/datasets/catalog](https://tensorflow.org/datasets/catalog)。
- en: In this recipe, we used the Mirrored Strategy for distributed execution using
    `tf.distribute.MirroredStrategy`, which enables synchronous distributed training
    using multiple replicas on one machine. Even with distributed execution with multiple
    replicas, we saw that the usual logging and checkpointing using callbacks worked
    as expected. We also verified that loading a saved model and running inference
    for evaluation works both with and without replication, making it portable without
    any added constraints just because the training utilized a distributed execution
    strategy!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们使用了`tf.distribute.MirroredStrategy`的镜像策略进行分布式执行，它允许在一台机器上使用多个副本进行同步分布式训练。即使是在多副本的分布式执行下，我们发现使用回调进行常规的日志记录和检查点保存依然如预期工作。我们还验证了加载保存的模型并运行推理进行评估在有或没有复制的情况下都能正常工作，这使得模型在训练过程中使用了分布式执行策略后，依然具有可移植性，不会因增加任何额外限制而受影响！
- en: It’s time to advance to the next recipe!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候进入下一个食谱了！
- en: Scaling up and out – Multi-machine, multi-GPU training
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展与扩展 – 多机器，多GPU训练
- en: To reach the highest scale in terms of the distributed training of deep learning-based
    models, we need the capability to leverage compute resources across GPUs and across
    machines. This can significantly reduce the time it takes to iterate over or develop
    new models and architectures for the problem you are trying to solve. With easy
    access to cloud computing services such as Microsoft Azure, Amazon AWS, and Google’s
    GCP, renting multiple GPU-equipped machines for an hourly rate has become easier
    and much more common. It is also more economical than setting up and maintaining
    your own multi-GPU multi-machine node. This recipe will provide a quick walk-through
    of training deep models using TensorFlow 2.x’s multi-worker mirrored distributed
    execution strategy based on the official documentation, which you can use and
    easily customize for your use cases. For the multi-machine, multi-GPU distributed
    training example in this recipe, we will train a deep residual network (ResNet
    or resnet) for typical image classification tasks. The same network architecture
    can be used by RL agents for their policy or value-function representation with
    a slight modification to the output layer, as we will see in the later recipes
    of this chapter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. To exercise the distributed training pipeline, it is recommended to
    set up a cluster with two or more machines equipped with GPUs either locally or
    on a cloud instance such as Azure, AWS, or GCP. While the training script we will
    implement can utilize multiple machines in a cluster, it is not absolutely necessary
    to have a cluster set up, although it is encouraged.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s begin!
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since this distributed training setup involves multiple machines, we need a
    communication interface between the machines and a way to address the individual
    machines. This is typically done using the existing network infrastructure and
    IP address:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by setting up a configuration parameter describing the cluster
    where we would like to train the models. The following code block is commented
    out so that you can edit and uncomment based on your cluster setup or leave it
    commented if you want to simply try it out on a single machine setup:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'To leverage multi-machine setups, we will use TensorFlow 2.x’s `MultiWorkerMirroredStrategy`:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, let’s declare the basic hyperparameters for the training. Feel free to
    adjust the batch sizes and the `NUM_GPUS` values as per your cluster/computer
    configuration:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To prepare the dataset, let’s implement two quick functions for normalizing
    and augmenting the input images:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For the sake of simplicity and faster convergence, we will stick with the CIFAR10
    dataset as per the official TensorFlow 2.x sample for training, but feel free
    to choose a different dataset when you explore. Once you choose the dataset, we
    can generate the training and the testing sets:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To make the training results reproducible, we will use a fixed random seed
    to shuffle the dataset:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We are not ready to generate the training and validation/testing dataset. We
    will shuffle the dataset using the known and fixed random seed declared in the
    previous step and apply the augmentation to the training set:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Similarly, we will prepare the test dataset but we do not want to apply random
    cropping to the test images! So, we will skip the augmentation and use the normalization
    step for preprocessing:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Before we can start training, we need to create an instance of an optimizer
    and also prepare the input layer. Feel free to use a different optimizer, such
    as Adam, as per the needs of your task:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, we are ready to construct the model instance within the scope of the
    `MultiMachineMirroredStrategy`:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To train the model, we use the simple but powerful Keras API:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once the model is trained, we easily save, load, and evaluate:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 12.1 Save
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H1
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: That completes our recipe implementation! Let’s summarize what we implemented
    and how it works in the next section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For any distributed training runs with TensorFlow 2.x, the `TF_CONFIG` environment
    variable needs to be set on each of the (virtual) machines on your cluster. These
    configuration values inform each of the machines about the role and the training
    information each of the nodes will need to perform its job. You can read more
    about the details of **TF_CONFIG** configurations used by TensorFlow 2.x’s distributed
    training here: [https://cloud.google.com/ai-platform/training/docs/distributed-training-details](https://cloud.google.com/ai-platform/training/docs/distributed-training-details).'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: We used TensorFlow 2.x’s `MultiWorkerMirroredStrategy`, which is a strategy
    similar to the Mirrored Strategy we used in the previous recipe in this chapter.
    This strategy is useful for synchronous training across machines with each machine
    potentially having one or more GPUs. All the variables and computations required
    for training the model are replicated on each of the worker nodes as in the Mirrored
    Strategy and, additionally, a distributed collection routine (such as all-reduce)
    is used to collate results from multiple distributed nodes. The remaining workflow
    for training, saving the model, loading the model, and evaluating the model remains
    the same as in our previous recipe.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Ready for the next recipe? Let’s do it.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: Training Deep RL agents at scale – Multi-GPU PPO agent
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL agents in general require a large number of samples and gradient steps to
    be trained depending on the complexity of the state, action, and the problem space.
    With Deep RL, the computational complexity also increases drastically as the deep
    neural network used by the agent (for Q/value-function representation, for policy
    representation, or for both) has a lot more operations and parameters that need
    to be executed and updated, respectively. To speed up the training process, we
    need the capability to scale our Deep RL agent training to leverage the available
    compute resources, such as GPUs. This recipe will help you leverage multiple GPUs
    to train a PPO agent with a deep convolutional neural network policy in a distributed
    fashion in one of the procedurally generated RL environments using **OpenAI’s
    procgen** library.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，RL代理需要大量的样本和梯度步骤来进行训练，这取决于状态、动作和问题空间的复杂性。随着深度强化学习（Deep RL）的发展，计算复杂度也会急剧增加，因为代理使用的深度神经网络（无论是用于Q值函数表示，策略表示，还是两者都有）有更多的操作和参数需要分别执行和更新。为了加速训练过程，我们需要能够扩展我们的深度RL代理训练，以利用可用的计算资源，如GPU。这个食谱将帮助你利用多个GPU，以分布式的方式训练一个使用深度卷积神经网络策略的PPO代理，在使用**OpenAI的procgen**库的程序生成的RL环境中进行训练。
- en: Let’s get started!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Getting ready
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. Although not required, it is recommended to use a machine with two
    or more GPUs to execute this recipe.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这个食谱，首先你需要激活`tf2rl-cookbook` Python/conda虚拟环境。确保更新环境，以匹配食谱代码库中的最新conda环境规格文件(`tfrl-cookbook.yml`)。虽然不是必需的，但建议使用具有两个或更多GPU的机器来执行此食谱。
- en: Now, let’s begin!
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始吧！
- en: How to do it...
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We will implement a complete recipe to allow configurable training of a PPO
    agent with a deep convolutional neural network policy in a distributed fashion.
    Let’s start implementing it step by step:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将实现一个完整的食谱，允许以分布式方式配置训练PPO代理，并使用深度卷积神经网络策略。让我们一步一步地开始实现：
- en: 'We will begin by importing the necessary modules for our recipe:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从导入实现这一食谱所需的模块开始：
- en: '[PRE33]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We will be using the `procgen` environments from OpenAI. Let’s import that
    as well:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用OpenAI的`procgen`环境。让我们也导入它：
- en: '[PRE34]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In order to make this recipe easy to configure and run, let’s add support for
    command-line arguments with useful configuration flags:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使这个食谱更易于配置和运行，让我们添加对命令行参数的支持，并配置一些有用的配置标志：
- en: '[PRE35]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s use a TensorBoard summary writer for logging:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用TensorBoard摘要写入器进行日志记录：
- en: '[PRE36]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We will first implement the `Actor` class in the following several steps, starting
    with the `__init__` method. You will notice that we need to instantiate the models
    within the context of the execution strategy:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先在以下几个步骤中实现`Actor`类，从`__init__`方法开始。你会注意到我们需要在执行策略的上下文中实例化模型：
- en: '[PRE37]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'For the Actor’s policy network model, we will implement a deep convolutional
    neural network comprising of multiple `Conv2D` and `MaxPool2D` layers. We will
    start the implementation in this step and finish in the following few steps:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于Actor的策略网络模型，我们将实现一个包含多个`Conv2D`和`MaxPool2D`层的深度卷积神经网络。在这一步我们将开始实现，接下来的几步将完成它：
- en: '[PRE38]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We will add more Conv2D – Pool2D layers to stack up the processing layers depending
    on the needs for the task. In this recipe, we will be training policies for the
    procgen environment, which is somewhat visually rich, so we will stack a few more
    layers:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加更多的Conv2D - Pool2D层，以根据任务的需求堆叠处理层。在这个食谱中，我们将为procgen环境训练策略，该环境在视觉上较为丰富，因此我们将堆叠更多的层：
- en: '[PRE39]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, we can use a flattening layer and prepare the output heads for the policy
    network:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用一个扁平化层，并为策略网络准备输出头：
- en: '[PRE40]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'As the final step for building the neural model for the policy network, we
    will create the output layer and return a Keras model:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为构建策略网络神经模型的最后一步，我们将创建输出层并返回一个Keras模型：
- en: '[PRE41]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'With the model we have defined in the previous steps, we can start processing
    a state/observation image input and produce the logits (unnormalized probabilities)
    and the action that the Actor would take. Let’s implement a method to do that:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们在前面步骤中定义的模型，我们可以开始处理状态/观察图像输入，并生成logits（未归一化的概率）以及Actor将采取的动作。让我们实现一个方法来完成这个任务：
- en: '[PRE42]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, to compute the surrogate loss to drive the learning, we will implement
    the `compute_loss` method:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，为了计算驱动学习的替代损失，我们将实现`compute_loss`方法：
- en: '[PRE43]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next up is a core method that ties all the methods together to perform the
    training. Note that this is the train method per replica, and we will use it in
    our distributed training method, which will follow in the next steps:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是一个核心方法，它将所有方法连接在一起以执行训练。请注意，这是每个副本的训练方法，我们将在后续的分布式训练方法中使用它：
- en: '[PRE44]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'To implement the distributed training method, we will make use of the `tf.function`
    decorator to implement a TensorFlow 2.x function:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了实现分布式训练方法，我们将使用`tf.function`装饰器来实现一个TensorFlow 2.x函数：
- en: '[PRE45]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'That completes our `Actor` class implementation, and we will now start our
    implementation of the `Critic` class:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这就完成了我们的`Actor`类实现，接下来我们将开始实现`Critic`类：
- en: '[PRE46]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You must have noticed that we are creating the Critic’s value-function model
    instance within the scope of the execution strategy to support distributed training.
    We will now start implementing the Critic’s neural network model in the following
    few steps:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你一定注意到，我们在执行策略的作用域内创建了Critic的价值函数模型实例，以支持分布式训练。接下来，我们将开始在以下几个步骤中实现Critic的神经网络模型：
- en: '[PRE47]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Like our Actor’s model, we will have similar layering of Conv2D-MaxPool2D layers
    followed by flattening layers with dropout:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与我们的Actor模型类似，我们将有类似的Conv2D-MaxPool2D层的堆叠，后面跟着带有丢弃的扁平化层：
- en: '[PRE48]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We will add the value output head and return the model as a Keras model to
    complete our Critic’s neural network model:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将添加值输出头，并将模型作为Keras模型返回，以完成我们Critic的神经网络模型：
- en: '[PRE49]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As you may recall, the Critic’s loss is the mean squared error between the
    predicted temporal-difference target and the actual temporal-difference targets.
    Let’s implement a method to compute the loss:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如你所记得，Critic的损失是预测的时间差目标与实际时间差目标之间的均方误差。让我们实现一个计算损失的方法：
- en: '[PRE50]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Similar to our Actor implementation, we will implement a per-replica `train`
    method and then use it in a later step for distributed training:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与我们的Actor实现类似，我们将实现一个每个副本的`train`方法，然后在后续步骤中用于分布式训练：
- en: '[PRE51]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We will now finalize our `Critic` class implementation by implementing the
    `train_distributed` method that enables distributed training:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将通过实现`train_distributed`方法来完成`Critic`类的实现，该方法支持分布式训练：
- en: '[PRE52]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'With our `Actor` and `Critic` classes implemented, we can start our distributed
    `PPOAgent` implementation. We will implement the `PPOAgent` class in the following
    several steps. Let’s begin with the `__init__` method:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实现了我们的`Actor`和`Critic`类后，我们可以开始我们的分布式`PPOAgent`实现。我们将分几个步骤实现`PPOAgent`类。让我们从`__init__`方法开始：
- en: '[PRE53]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we will implement a method to calculate the target for the **generalized
    advantage estimate** (**GAE**):'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个方法来计算**广义优势估计**（**GAE**）的目标：
- en: '[PRE54]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We are all set to start our main `train(…)` method. We will split the implementation
    of this method into the following few steps. Let’s set up the scope, start the
    outer loop, and initialize varaibles:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经准备好开始我们的`train(…)`方法。我们将把这个方法的实现分为以下几个步骤。让我们设置作用域，开始外循环，并初始化变量：
- en: '[PRE55]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, we can start the loop that needs to be executed for each episode until
    the episode is done:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以开始为每个回合执行的循环，直到回合结束：
- en: '[PRE56]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Within each episode, if we have reached `update_freq` or just reached an end
    state, we need to compute the GAEs and TD targets. Let’s add the code for that:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个回合内，如果我们达到了`update_freq`或者刚刚到达了结束状态，我们需要计算GAE和TD目标。让我们添加相应的代码：
- en: '[PRE57]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Within the same execution context, we need to train the `Actor` and the `Critic`:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在相同的执行上下文中，我们需要训练`Actor`和`Critic`：
- en: '[PRE58]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Finally, we need to reset the tracking variables and update our episode reward
    values:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要重置跟踪变量并更新我们的回合奖励值：
- en: '[PRE59]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'With that, our distributed `main` method to finalize our recipe:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这样，我们的分布式`main`方法就完成了，来完成我们的配方：
- en: '[PRE60]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: That’s it for the recipe! Hope you enjoyed cooking it up. You can execute the
    recipe and watch the progress using the TensorBoard logs to see the training speedup
    you get with a greater number of GPUs!
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 配方完成了！希望你喜欢这个过程。你可以执行这个配方，并通过TensorBoard日志观看进度，以查看你在更多GPU的支持下获得的训练加速效果！
- en: Let’s recap what we accomplished and how the recipe works in the next section.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下我们完成的工作以及配方如何工作的下一部分。
- en: How it works...
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We implemented `Actor` and `Critic` classes where the Actor used a deep convolutional
    neural network for the policy representation and the Critic utilized a similar
    deep convolutional neural network for its value function representation. Both
    these models were instantiated under the scope of the distributed execution strategy
    using the `self.execution_strategy.scope()` construct.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: The procgen environments, such as coinrun, fruitbot, jumper, leaper, maze, and
    others, are visually (relatively) rich environments and therefore require convolutional
    layers that are relatively deep to process the visual observations. We therefore
    used a deep CNN model for the policy network of the Actor. For distributed training
    using multiple replicas on multiple GPUs, we first implemented a single-replica
    training method (train) and then used `Tensorflow.function` to run across replicas
    and reduce the results to arrive at the total loss.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, while training our PPO agent in the distributed setting, we performed
    all the training operations within the scope of the distributed execution strategy
    by using Python’s `with` statement for context management like this: `with self.distributed_execution_strategy.scope()`.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to move on to the next recipe!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Building blocks for distributed Deep Reinforcement Learning for accelerated
    training
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous recipes in this chapter discussed how you could scale your Deep
    RL training using TensorFlow 2.x’s distributed execution APIs. While it was straightforward
    after understanding the concepts and the implementation style, training Deep RL
    agents with more advanced architectures such as Impala and R2D2 requires RL building
    blocks such as distributed parameter servers and distributed experience replay.
    This chapter will walk through the implementation of such building blocks for
    distributed RL training. We will be using the Ray distributed computing framework
    to implement our building blocks.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. To test the building blocks we build in this recipe, we will be using
    the `sac_agent_base` module based on our SAC agent implemented in one of the book’s
    earlier recipes. If the following `import` statements run without issues, you
    are ready to start:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now, let’s begin!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will implement the building blocks one by one, starting with the distributed
    parameter server:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ParameterServer` class is a simple store for sharing the neural network
    parameters or weights between workers in a distributed training setting. We will
    implement the class as a Ray’s remote Actor:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let’s also add a method to save the weights to the disk:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'As the next building block, we will implement the `ReplayBuffer`, which can
    be used by a distributed set of agents. We will start the implementation in this
    step and continue in the next several steps:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Next, we will implement a method to store new experiences in the replay buffer:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'To sample a batch of experience data from the replay buffer, we will implement
    a method that randomly samples from the replay buffer and returns a dictionary
    containing the sampled experience data:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'That completes our `ReplayBuffer` class implementation. We will now start implementing
    a method to roll out, which essentially collects experiences in an RL environment
    using an exploration policy with parameters pulled from the distributed parameter
    server object and stores the collected experience in the distributed replay buffer.
    We will start our implementation in this step and complete the `rollout` method
    implementation in the following steps:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'With the agent intialized and loaded and the environment instance(s) ready,
    we can start our experience-gathering loop:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let’s handle the case when a `max_ep_len` is configured to indicate the maximum
    length of the episode and then store the collected experience in the distributed
    replay buffer:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Finally, at the end of the episode, sync the weights of the behavior policy
    using the parameter server:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'That completes the implementation of the `rollout` method and we can now implement
    a `train` method that runs the train loop:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The final module in our recipe is the `main` function, which puts together
    all the building blocks we have built so far in this recipe and exercises them.
    We will begin the implementation in this step and finish it in the remaining steps.
    Let’s start with the `main` function argument list and capture the arguments in
    a config dictionary:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Next, let’s create an instance of the desired environment, obtain the state
    and observation space, initialize ray, and also initialize a Stochastic Actor-Critic
    agent. Note that we are initializing a single-node ray cluster but feel free to
    initialize ray with a cluster of nodes (local or in the cloud):'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'In this step, we will initialize an instance of the `ParameterServer` class
    and an instance of the `ReplayBuffer` class:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We are now ready to exercise the building blocks we have built. We will first
    launch a series of rollout tasks based on the number of workers specified as a
    configuration argument that will launch the rollout process on the distributed
    ray cluster:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: The rollout task will launch the remote tasks that will populate the replay
    buffer with the gathered experience. The above line will return immediately even
    though the rollout tasks will take time to complete because of the asynchronous
    function call.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will launch a configurable number of learners that run the distributed
    training task on the ray cluster:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The above statement will launch the remote training process and return immediately
    even though the `train` function on the learners will take time to complete.
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Finally, let’s define our entry point. We will use the Python Fire library
    to expose our `main` function, and its arguments to look like an executable supporting
    command-line argument:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'With the preceding entry point, the script can be configured and launched from
    the command line. An example is provided here for your reference:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: That completes our implementation! Let’s briefly discuss how it works in the
    next section.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We built a distributed `ParameterServer`, `ReplayBuffer`, rollout worker, and
    learner processes. These building blocks are crucial for training distributed
    RL agents. We utilized Ray as the framework for distributed computing.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: After implementing the building blocks and the tasks, in the `main` function,
    we launched the two asynchronous, distributed tasks on the ray cluster. The `task_rollout`
    launched a (configurable) number of rollout workers and the `task_train` launched
    a (configurable) number of learners. Both the tasks run on the ray cluster asynchronously
    in a distributed manner. The rollout workers pull the latest weights from the
    parameter server and gather and store experiences in the replay memory buffer
    while, simultaneously, the learners train using batches of experiences sampled
    from the replay memory and push the updated (and potentially improved) set of
    parameters to the parameter server.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to move on to the next and final recipe of this chapter!
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Large-scale Deep RL agent training using Ray, Tune, and RLLib
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we got a flavor of how to implement distributed RL agent
    training routines from scratch. Since most of the components used as building
    blocks have become a standard way of building Deep RL training infrastructure,
    we can leverage an existing library that maintains a high-quality implementation
    of such building blocks. Fortunately, with our choice of ray as the framework
    for distributed computing, we are in a good place. Tune and RLLib are two libraries
    built on top of ray, and are available together with Ray, that provide highly
    scalable hyperparameter tuning (Tune) and RL training (RLLib). This recipe will
    provide a curated set of steps to get you acquainted with ray, Tune, and RLLib
    so that you can utilize them to scale your Deep RL training routines. In addition
    to the recipe discussed here in the text, the cookbook’s code repository for this
    chapter will have a handful of additional recipes for you.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started!
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Python/conda virtual environment. Make sure to update the environment to match
    the latest conda environment specification file (`tfrl-cookbook.yml`) in the cookbook’s
    code repo. Ray, Tune, and RLLib will be installed in your `tfrl-cookbook` conda
    environment when you use the provided conda YAML spec for the environment. If
    you want to install Tune and RLLib in a different environment, the easiest way
    is to install it using the following command:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Now, let’s begin!
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start with quick and basic commands and recipes to launch training
    on ray clusters using Tune and RLLib and progressively customize the training
    pipeline to provide you with a useful recipe:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'Launching typical training of RL agents in OpenAI Gym environments is as easy
    as specifying the algorithm name and the environment name. For example, to train
    a PPO agent in the CartPole-v4 Gym environment, all you need to execute is the
    following command:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Let’s try to train a PPO agent in the `coinrun` `procgen` environment, like
    in one of our previous recipes:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'You will notice that the preceding command fails with the following (shortened)
    error:'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: This is because, as the error states, RLLib by default supports observations
    of shapes (42, 42, k) or (84, 84, k). Observations of other shapes will need a
    custom model or a preprocessor. In the next few steps, we will see how we can
    implement a custom neural network model implemented using the TensorFlow 2.x Keras
    API, which can be used with ray RLLib.
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will start our custom model implementation (`custom_model.py`) in this step
    and complete it in the following few steps. In this step, let’s import the necessary
    modules and also implement a helper method to return a Conv2D layer with a certain
    filter depth:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Next, let’s implement a helper method to build and return a simple residual
    block:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Let’s implement another handy function to construct multiple residual block
    sequences:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'We can now start implementing the `CustomModel` class as a subclass of the
    TFModelV2 base class provided by RLLib to make it easy to integrate with RLLib:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'After the `__init__` method, we need to implement the `forward` method as it
    is not implemented by the base class (`TFModelV2`) but is necessary:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'We will also implement a one-line method to reshape the value function output:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: With that, our `CustomModel` implementation is complete and is ready to use!
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will implement a solution (`5.1_training_using_tune_run.py`) using ray,
    Tune, and RLLib’s Python API so that you also utilize the model in addition to
    their command-line usage. Let’s split the implementation into two steps. In this
    step, we will import the necessary modules and initialize ray:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'In this step, we will register our custom model in RLLib’s `ModelCatlog` and
    then use it to train a PPO agent with a custom set of parameters including the
    `framework` parameter that forces RLLib to use TensorFlow 2\. We will also shut
    down ray at the end of the script:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'We will look at another quick recipe (`5_2_custom_training_using_tune.py`)
    to customize the training loop. We will split the implementation into the following
    few steps to keep it simple. In this step, we will import the necessary libraries
    and initialize ray:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Now, let’s register our custom model with RLLib’s `ModelCatalog` and configure
    the **IMPALA agent**. We could very well use any other RLLib support agents, such
    as PPO or SAC:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'We can now implement our custom training loop and include any steps in the
    loop as we desire. We will keep the example loop simple by simply performing a
    training step and saving the agent’s model every n(100) epochs:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Note that we could continue to train the agent using the saved checkpoint and
    using the simpler ray tune’s run API as shown here as an example:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Finally, let’s shut down ray to free up system resources:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: That completes this recipe! In the next section, let’s recap what we discussed
    in this recipe.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We identified one of the common limitations with the simple but limited command-line
    interface of ray RLLib. We also discussed a solution to overcome the failure in
    step 2 where a custom model was needed to use RLLib’s PPO agent training and implemented
    it in steps 9 and 10.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: While the solution discussed in steps 9 and 10 looks elegant, it may not provide
    all the customization knobs you are looking for or are familiar with. For example,
    it abstracts away the basic RL loop that steps through the environment. We implemented
    another quick recipe starting from step 11 that allows the customization of the
    training loop. In step 12, we saw how we can register our custom model and use
    it with the IMPALA agent – which is a scalable, distributed Deep RL agent based
    on IMPortance-weighted Actor-Learner Architecture. IMPALA agent actors communicate
    sequences of states, actions, and rewards to a centralized learner where batch
    gradient updates take place, in contrast to the (asynchronous) Actor-Critic-based
    agents where the gradients are communicated to a centralized parameter server.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: For more information on Tune, you can refer to the Tune user guide and configuration
    documentation at [https://docs.ray.io/en/master/tune/user-guide.html](https://docs.ray.io/en/master/tune/user-guide.html).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: For more information on RLLib training APIs and configuration documentation,
    you can refer to [https://docs.ray.io/en/master/rllib-training.html](https://docs.ray.io/en/master/rllib-training.html).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: That completes the recipe and the chapter! Hope you feel empowered with the
    new skills and knowledge you have gained to speed up your Deep RL agent training.
    See you in the next chapter!
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
