["```\n    import tensorflow as tf\n    import numpy as np\n    import datetime \n    ```", "```\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n    x_train = x_train.reshape(-1, 28, 28, 1)\n    x_test = x_test.reshape(-1, 28, 28, 1)\n    # Padding the images by 2 pixels since in the paper input images were 32x32\n    x_train = np.pad(x_train, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n    x_test = np.pad(x_test, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n    # Normalize\n    x_train = x_train / 255\n    x_test = x_test/ 255\n    # Set model parameters\n    image_width = x_train[0].shape[0]\n    image_height = x_train[0].shape[1]\n    num_channels = 1 # grayscale = 1 channel\n    # Training and Test data variables\n    batch_size = 100\n    evaluation_size = 500\n    generations = 300\n    eval_every = 5\n    # Set for reproducible results\n    seed = 98\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    # Declare the model\n    input_data = tf.keras.Input(dtype=tf.float32, shape=(image_width,image_height, num_channels), name=\"INPUT\")\n    # First Conv-ReLU-MaxPool Layer\n    conv1 = tf.keras.layers.Conv2D(filters=6,\n                                   kernel_size=5,\n                                   padding='VALID',\n                                   activation=\"relu\",\n                                   name=\"C1\")(input_data)\n    max_pool1 = tf.keras.layers.MaxPool2D(pool_size=2,\n                                          strides=2, \n                                          padding='SAME',\n                                          name=\"S1\")(conv1)\n    # Second Conv-ReLU-MaxPool Layer\n    conv2 = tf.keras.layers.Conv2D(filters=16,\n                                   kernel_size=5,\n                                   padding='VALID',\n                                   strides=1,\n                                   activation=\"relu\",\n                                   name=\"C3\")(max_pool1)\n    max_pool2 = tf.keras.layers.MaxPool2D(pool_size=2,\n                                          strides=2, \n                                          padding='SAME',\n                                          name=\"S4\")(conv2)\n    # Flatten Layer\n    flatten = tf.keras.layers.Flatten(name=\"FLATTEN\")(max_pool2)\n    # First Fully Connected Layer\n    fully_connected1 = tf.keras.layers.Dense(units=120,\n                                             activation=\"relu\",\n                                             name=\"F5\")(flatten)\n    # Second Fully Connected Layer\n    fully_connected2 = tf.keras.layers.Dense(units=84,\n                                             activation=\"relu\",\n                                             name=\"F6\")(fully_connected1)\n    # Final Fully Connected Layer\n    final_model_output = tf.keras.layers.Dense(units=10,\n                                               activation=\"softmax\",\n                                               name=\"OUTPUT\"\n                                               )(fully_connected2)\n\n    model = tf.keras.Model(inputs= input_data, outputs=final_model_output) \n    ```", "```\n    model.compile(\n        optimizer=\"adam\", \n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    )\n    model.summary() \n    ```", "```\n    log_dir=\"logs/experiment-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n    ```", "```\n    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, \n                                                          write_images=True,\n                                                          histogram_freq=1 )\n    model.fit(x=x_train, \n              y=y_train, \n              epochs=5,\n              validation_data=(x_test, y_test), \n              callbacks=[tensorboard_callback]) \n    ```", "```\n    $ tensorboard --logdir=\"logs\" \n    ```", "```\n    # Create a FileWriter for the timestamped log directory.\n    file_writer = tf.summary.create_file_writer(log_dir)\n    with file_writer.as_default():\n        # Reshape the images and write image summary.\n        images = np.reshape(x_train[0:10], (-1, 32, 32, 1))\n        tf.summary.image(\"10 training data examples\", images, max_outputs=10, step=0) \n    ```", "```\n    import tensorflow as tf\n    from tensorboard.plugins.hparams import api as hp\n    import numpy as np\n    import datetime \n    ```", "```\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n    # Normalize\n    x_train = x_train / 255\n    x_test = x_test/ 255\n    ## Set model parameters\n    image_width = x_train[0].shape[0]\n    image_height = x_train[0].shape[1]\n    num_channels = 1 # grayscale = 1 channel \n    ```", "```\n    HP_ARCHITECTURE_NN = hp.HParam('archi_nn', \n    hp.Discrete(['128,64','256,128']))\n    HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.0, 0.1))\n    HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd'])) \n    ```", "```\n    def train_model(hparams, experiment_run_log_dir):\n\n        nb_units = list(map(int, hparams[HP_ARCHITECTURE_NN].split(\",\")))\n\n        model = tf.keras.models.Sequential()\n        model.add(tf.keras.layers.Flatten(name=\"FLATTEN\"))\n        model.add(tf.keras.layers.Dense(units=nb_units[0], activation=\"relu\", name=\"D1\"))\n        model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT], name=\"DROP_OUT\"))\n        model.add(tf.keras.layers.Dense(units=nb_units[1], activation=\"relu\", name=\"D2\"))\n        model.add(tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"OUTPUT\"))\n\n        model.compile(\n            optimizer=hparams[HP_OPTIMIZER], \n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"accuracy\"]\n        )\n\n        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=experiment_run_log_dir)\n        hparams_callback = hp.KerasCallback(experiment_run_log_dir, hparams)\n\n        model.fit(x=x_train, \n                  y=y_train, \n                  epochs=5,\n                  validation_data=(x_test, y_test),\n                  callbacks=[tensorboard_callback, hparams_callback]\n                 )\n    model = tf.keras.Model(inputs= input_data, outputs=final_model_output) \n    ```", "```\n    for archi_nn in HP_ARCHITECTURE_NN.domain.values:\n        for optimizer in HP_OPTIMIZER.domain.values:\n            for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n                hparams = {\n                    HP_ARCHITECTURE_NN : archi_nn, \n                    HP_OPTIMIZER: optimizer,\n                    HP_DROPOUT : dropout_rate\n                }\n\n                experiment_run_log_dir=\"logs/experiment-\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\n                train_model(hparams, experiment_run_log_dir) \n    ```", "```\n    $ tensorboard --logdir=\"logs\" \n    ```", "```\n    import tensorflow as tf\n    import numpy as np \n    ```", "```\n    class MyCustomGate(tf.keras.layers.Layer):\n\n        def __init__(self, units, a1, b1):\n            super(MyCustomGate, self).__init__()\n            self.units = units\n            self.a1 = a1\n            self.b1 = b1\n        # Compute f(x) = a1 * x + b1\n        def call(self, inputs):\n            return inputs * self.a1 + self.b1 \n    ```", "```\n    class MyCustomGateTest(tf.test.TestCase):\n        def setUp(self):\n            super(MyCustomGateTest, self).setUp()\n            # Configure the layer with 1 unit, a1 = 2 et b1=1\n            self.my_custom_gate = MyCustomGate(1,2,1)\n        def testMyCustomGateOutput(self):\n            input_x = np.array([[1,0,0,1],\n                               [1,0,0,1]])\n            output = self.my_custom_gate(input_x)\n            expected_output = np.array([[3,1,1,3], [3,1,1,3]])\n            self.assertAllEqual(output, expected_output) \n    ```", "```\n    tf.test.main() \n    ```", "```\n    $ python3 01_implementing_unit_tests.py\n    ...\n    [       OK ] MyCustomGateTest.testMyCustomGateOutput\n    [ RUN      ] MyCustomGateTest.test_session\n    [  SKIPPED ] MyCustomGateTest.test_session\n    ----------------------------------------------------------------------\n    Ran 2 tests in 0.016s\n    OK (skipped=1) \n    ```", "```\n    tf.debugging.set_log_device_placement(True)\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    c = tf.matmul(a, b)\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n    Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 \n    ```", "```\n    a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n    print(a.device)\n    b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    print(b.device)\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n    Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 \n    ```", "```\n    tf.debugging.set_log_device_placement(True)\n    with tf.device('/device:CPU:0'):\n        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n        c = tf.matmul(a, b)\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n    Executing op MatMul in device /job:localhost/replica:0/task:0/device:CPU:0 \n    ```", "```\n    tf.debugging.set_log_device_placement(True)\n    with tf.device('/device:CPU:0'):\n        a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name='a')\n        b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name='b')\n    c = tf.matmul(a, b)\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n    Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0 \n    ```", "```\n    gpu_devices = tf.config.list_physical_devices('GPU')\n    if gpu_devices:\n        try:\n            tf.config.experimental.set_memory_growth(gpu_devices[0], True)\n        except RuntimeError as e:\n            # Memory growth cannot be modified after GPU has been initialized\n            print(e) \n    ```", "```\n    gpu_devices = tf.config.list_physical_devices('GPU')\n    if gpu_devices:\n        try:\n    tf.config.experimental.set_virtual_device_configuration(gpu_devices[0],\n                                                       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n        except RuntimeError as e:\n            # Memory growth cannot be modified after GPU has been initialized\n            print(e) \n    ```", "```\n    gpu_devices = tf.config.list_physical_devices('GPU')\n    if gpu_devices:\n        try:\n            tf.config.experimental.set_virtual_device_configuration(gpu_devices[0],\n                                                       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\n                                                        tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024) ])\n        except RuntimeError as e:\n            # Memory growth cannot be modified after GPU has been initialized\n            print(e) \n    ```", "```\n    if tf.test.is_built_with_cuda(): \n        <Run GPU specific code here> \n    ```", "```\n    if tf.test.is_built_with_cuda():\n        with tf.device('/cpu:0'):\n            a = tf.constant([1.0, 3.0, 5.0], shape=[1, 3])\n            b = tf.constant([2.0, 4.0, 6.0], shape=[3, 1])\n\n            with tf.device('/gpu:0'):\n                c = tf.matmul(a,b)\n                c = tf.reshape(c, [-1])\n\n            with tf.device('/gpu:1'):\n                d = tf.matmul(b,a)\n                flat_d = tf.reshape(d, [-1])\n\n            combined = tf.multiply(c, flat_d)\n        print(combined)\n    Num GPUs Available:  2\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:CPU:0\n    Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:0\n    Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:1\n    Executing op Reshape in device /job:localhost/replica:0/task:0/device:GPU:1\n    Executing op Mul in device /job:localhost/replica:0/task:0/device:CPU:0\n    tf.Tensor([  88\\.  264\\.  440\\.  176\\.  528\\.  880\\.  264\\.  792\\. 1320.], shape=(9,), dtype=float32) \n    ```", "```\n    import tensorflow as tf\n    import tensorflow_datasets as tfds \n    ```", "```\n    # Create two virtual GPUs\n    gpu_devices = tf.config.list_physical_devices('GPU')\n    if gpu_devices:\n        try:\n            tf.config.experimental.set_virtual_device_configuration(gpu_devices[0],\n                                                       [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024),\n                                                        tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024) ])\n        except RuntimeError as e:\n            # Memory growth cannot be modified after GPU has been initialized\n            print(e) \n    ```", "```\n    datasets, info = tfds.load('mnist', with_info=True, as_supervised=True)\n    mnist_train, mnist_test = datasets['train'], datasets['test'] \n    ```", "```\n    def normalize_img(image, label):\n      \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n      return tf.cast(image, tf.float32) / 255., label\n    mnist_train = mnist_train.map(\n        normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    mnist_train = mnist_train.cache()\n    mnist_train = mnist_train.shuffle(info.splits['train'].num_examples)\n    mnist_train = mnist_train.prefetch(tf.data.experimental.AUTOTUNE)\n    mnist_test = mnist_test.map(\n        normalize_img, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    mnist_test = mnist_test.cache()\n    mnist_test = mnist_test.prefetch(tf.data.experimental.AUTOTUNE) \n    ```", "```\n    mirrored_strategy = tf.distribute.MirroredStrategy() \n    ```", "```\n    print('Number of devices: {}'.format(mirrored_strategy.num_replicas_in_sync)) \n    ```", "```\n    BATCH_SIZE_PER_REPLICA = 128\n    BATCH_SIZE = BATCH_SIZE_PER_REPLICA * mirrored_strategy.num_replicas_in_sync\n    mnist_train = mnist_train.batch(BATCH_SIZE)\n    mnist_test = mnist_test.batch(BATCH_SIZE) \n    ```", "```\n    with mirrored_strategy.scope():\n        model = tf.keras.Sequential()\n        model.add(tf.keras.layers.Flatten(name=\"FLATTEN\"))\n        model.add(tf.keras.layers.Dense(units=128 , activation=\"relu\", name=\"D1\"))\n        model.add(tf.keras.layers.Dense(units=64 , activation=\"relu\", name=\"D2\"))\n        model.add(tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"OUTPUT\"))\n\n        model.compile(\n            optimizer=\"sgd\", \n            loss=\"sparse_categorical_crossentropy\",\n            metrics=[\"accuracy\"]\n        ) \n    ```", "```\n    model.fit(mnist_train, \n              epochs=10,\n              validation_data= mnist_test\n              ) \n    ```", "```\n    import tensorflow as tf \n    ```", "```\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n    # Normalize\n    x_train = x_train / 255\n    x_test = x_test/ 255\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Flatten(name=\"FLATTEN\"))\n    model.add(tf.keras.layers.Dense(units=128 , activation=\"relu\", name=\"D1\"))\n    model.add(tf.keras.layers.Dense(units=64 , activation=\"relu\", name=\"D2\"))\n    model.add(tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"OUTPUT\"))\n\n    model.compile(optimizer=\"sgd\", \n                  loss=\"sparse_categorical_crossentropy\",\n                  metrics=[\"accuracy\"]\n                 )\n    model.fit(x=x_train, \n              y=y_train, \n              epochs=5,\n              validation_data=(x_test, y_test)\n             ) \n    ```", "```\n    model.save(\"SavedModel\") \n    ```", "```\n    SavedModel\n    |_ assets\n    |_ variables\n    |_ saved_model.pb \n    ```", "```\n    model2 = tf.keras.models.load_model(\"SavedModel\") \n    ```", "```\n    model.save(\"SavedModel.h5\")\n    model.save(\"model_save\", save_format=\"h5\") \n    ```", "```\n    checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"./checkpoint\",save_weights_only=True, save_freq='epoch')\n    model.fit(x=x_train, \n              y=y_train, \n              epochs=5,\n              validation_data=(x_test, y_test),\n              callbacks=[checkpoint_callback]\n             ) \n    ```", "```\n    model.load_weights(\"./checkpoint\") \n    ```", "```\n    import tensorflow as tf\n    import numpy as np\n    import requests\n    import matplotlib.pyplot as plt\n    import json \n    ```", "```\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n    # Normalize\n    x_train = x_train / 255\n    x_test = x_test/ 255\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Flatten(name=\"FLATTEN\"))\n    model.add(tf.keras.layers.Dense(units=128 , activation=\"relu\", name=\"D1\"))\n    model.add(tf.keras.layers.Dense(units=64 , activation=\"relu\", name=\"D2\"))\n    model.add(tf.keras.layers.Dense(units=10, activation=\"softmax\", name=\"OUTPUT\"))\n\n    model.compile(optimizer=\"sgd\", \n                  loss=\"sparse_categorical_crossentropy\",\n                  metrics=[\"accuracy\"]\n                 )\n    model.fit(x=x_train, \n              y=y_train, \n              epochs=5,\n              validation_data=(x_test, y_test)\n             ) \n    ```", "```\n    $ docker pull tensorflow/serving \n    ```", "```\n    $ docker run -p 8501:8501 \\\n      --mount type=bind,source=\"$(pwd)/my_mnist_model/\",target=/models/my_mnist_model \\\n      -e MODEL_NAME=my_mnist_model -t tensorflow/serving \n    ```", "```\n    num_rows = 4\n    num_cols = 3\n    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n    for row in range(num_rows):\n        for col in range(num_cols):\n            index = num_cols * row + col\n            image = x_test[index]\n            true_label = y_test[index]\n            plt.subplot(num_rows, 2*num_cols, 2*index+1)\n            plt.imshow(image.reshape(28,28), cmap=\"binary\")\n            plt.axis('off')\n            plt.title('\\n\\n It is a {}'.format(y_test[index]), fontdict={'size': 16})\n    plt.tight_layout()\n    plt.show() \n    ```", "```\n    json_request = '{{ \"instances\" : {} }}'.format(x_test[0:12].tolist())\n    resp = requests.post('http://localhost:8501/v1/models/my_mnist_model:predict', data=json_request, headers = {\"content-type\": \"application/json\"})\n    print('response.status_code: {}'.format(resp.status_code))     \n    print('response.content: {}'.format(resp.content))\n    predictions = json.loads(resp.text)['predictions'] \n    ```", "```\n    num_rows = 4\n    num_cols = 3\n    plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n    for row in range(num_rows):\n        for col in range(num_cols):\n            index = num_cols * row + col\n            image = x_test[index]\n            predicted_label = np.argmax(predictions[index])\n            true_label = y_test[index]\n            plt.subplot(num_rows, 2*num_cols, 2*index+1)\n            plt.imshow(image.reshape(28,28), cmap=\"binary\")\n            plt.axis('off')\n            if predicted_label == true_label:\n                color = 'blue'\n            else:\n                color = 'red'\n            plt.title('\\n\\n The model predicts a {} \\n and it is a {}'.format(predicted_label, true_label), fontdict={'size': 16}, color=color)\n    plt.tight_layout()\n    plt.show() \n    ```"]