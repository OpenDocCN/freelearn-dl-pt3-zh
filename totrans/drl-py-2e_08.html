<html><head></head><body>
  <div id="_idContainer992">
    <h1 class="chapterNumber">8</h1>
    <h1 id="_idParaDest-207" class="chapterTitle">A Primer on TensorFlow</h1>
    <p class="normal">TensorFlow is one of the most popular deep learning libraries. In upcoming chapters, we will use TensorFlow to build deep reinforcement models. So, in this chapter, we will get ourselves familiar with TensorFlow and its functionalities.</p>
    <p class="normal">We will learn about what computational graphs are and how TensorFlow uses them. We will also explore TensorBoard, which is a visualization tool provided by TensorFlow used for visualizing models. Going forward, we will understand how to build a neural network with TensorFlow to perform handwritten digit classification. </p>
    <p class="normal">Moving on, we will learn about TensorFlow 2.0, which is the latest version of TensorFlow. We will understand how TensorFlow 2.0 differs from its previous versions and how it uses Keras as its high-level API.</p>
    <p class="normal">In this chapter, we will learn about the following:</p>
    <ul>
      <li class="bullet">TensorFlow</li>
      <li class="bullet">Computational graphs and sessions</li>
      <li class="bullet">Variables, constants, and placeholders</li>
      <li class="bullet">TensorBoard</li>
      <li class="bullet">Handwritten digit classification in TensorFlow</li>
      <li class="bullet">Math operations in TensorFlow</li>
      <li class="bullet">TensorFlow 2.0 and Keras</li>
    </ul>
    <h1 id="_idParaDest-208" class="title">What is TensorFlow?</h1>
    <p class="normal">TensorFlow is an<a id="_idIndexMarker777"/> open source software library from Google, which is extensively used for numerical computation. It is one of the most used libraries for building deep learning models. It is highly scalable and runs on multiple platforms, such as Windows, Linux, macOS, and Android. It was originally developed by the researchers and engineers of the Google Brain team.</p>
    <p class="normal">TensorFlow supports execution on everything, including CPUs, GPUs, TPUs, which are tensor processing units, and mobile and embedded platforms. Due to its flexible architecture and ease of deployment, it has become a popular choice of library among many researchers and scientists for building deep learning models.</p>
    <p class="normal">In TensorFlow, every computation is represented by a data flow graph, also known as a <strong class="keyword">computational graph</strong>, where<a id="_idIndexMarker778"/> nodes represent operations, such as addition or multiplication, and edges represent tensors. Data flow graphs can also be shared and executed on many different platforms. TensorFlow provides a visualization tool, called TensorBoard, for visualizing data flow graphs.</p>
    <p class="normal">TensorFlow 2.0 is the latest version of TensorFlow. In the upcoming chapters, we will use TensorFlow 2.0 for building deep reinforcement learning models. However, it is important to understand how TensorFlow 1.x works. So, first, we will learn to use TensorFlow 1.x and then we will look into TensorFlow 2.0. </p>
    <p class="normal">You can install TensorFlow <a id="_idIndexMarker779"/>easily through <code class="Code-In-Text--PACKT-">pip</code> by just typing the following command in your terminal:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install tensorflow==1.13.1
</code></pre>
    <p class="normal">We can check the successful installation of TensorFlow by running the following simple <code class="Code-In-Text--PACKT-">Hello TensorFlow!</code> program:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
hello = tf.constant(<span class="hljs-string">"Hello TensorFlow!"</span>)
sess = tf.Session()
print(sess.run(hello))
</code></pre>
    <p class="normal">The preceding program should print <code class="Code-In-Text--PACKT-">Hello TensorFlow!</code>. If you get any errors, then you probably have not installed TensorFlow correctly.</p>
    <h1 id="_idParaDest-209" class="title">Understanding computational graphs and sessions</h1>
    <p class="normal">As we have <a id="_idIndexMarker780"/>learned, every computation in TensorFlow is represented by a computational graph. They consist of several nodes and edges, where nodes are mathematical operations, such as addition and multiplication, and edges are tensors. Computational graphs are very efficient at optimizing resources and promote distributed computing.</p>
    <p class="normal">A computational graph consists of several TensorFlow operations, arranged in a graph of nodes.</p>
    <p class="normal">A computational graph helps us to understand the network architecture when we work on building a really complex neural network. For instance, let's consider a simple layer, <em class="italic">h</em> = Relu(<em class="italic">WX</em> + <em class="italic">b</em>). Its computational graph would be represented as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.1: Computational graph</p>
    <p class="normal">There are two types <a id="_idIndexMarker781"/>of dependency in the computational graph, called direct and indirect dependency. Say we have node <code class="Code-In-Text--PACKT-">b</code>, the input of which is dependent on the output of node <code class="Code-In-Text--PACKT-">a</code>; this type<a id="_idIndexMarker782"/> of dependency is called <strong class="keyword">direct dependency</strong>, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">a = tf.multiply(<span class="hljs-number">8</span>,<span class="hljs-number">5</span>)
b = tf.multiply(a,<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">When node <code class="Code-In-Text--PACKT-">b</code> doesn't depend on node <code class="Code-In-Text--PACKT-">a</code> for its<a id="_idIndexMarker783"/> input, it is called <strong class="keyword">indirect dependency</strong>, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">a = tf.multiply(<span class="hljs-number">8</span>,<span class="hljs-number">5</span>)
b = tf.multiply(<span class="hljs-number">4</span>,<span class="hljs-number">3</span>)
</code></pre>
    <p class="normal">So, if we can understand these dependencies, we can distribute the independent computations in the available resources and reduce the computation time. Whenever we import TensorFlow, a default graph is created automatically and all of the nodes we create are associated with the default graph. We can also create our own graphs instead of using the default graph, and this is useful when building multiple models that do not depend on one another. A TensorFlow graph can be created using <code class="Code-In-Text--PACKT-">tf.Graph()</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">graph = tf.Graph()
<span class="hljs-keyword">with</span> graph.as_default():
     z = tf.add(x, y, name=<span class="hljs-string">'Add'</span>)
</code></pre>
    <p class="normal">If we want to clear the default graph (that is, if we want to clear the previously defined variables and <a id="_idIndexMarker784"/>operations in the graph), then we can do this using <code class="Code-In-Text--PACKT-">tf.reset_default_graph()</code>.</p>
    <h2 id="_idParaDest-210" class="title">Sessions</h2>
    <p class="normal">As mentioned in the <a id="_idIndexMarker785"/>previous section, a computational graph with operations on its nodes and tensors to its edges is created, and in order to execute the graph, we use a TensorFlow session.</p>
    <p class="normal">A TensorFlow session can be created using <code class="Code-In-Text--PACKT-">tf.Session()</code>, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">sess = tf.Session()
</code></pre>
    <p class="normal">After creating the session, we can execute our graph, using the <code class="Code-In-Text--PACKT-">sess.run()</code> method.</p>
    <p class="normal">Every computation in TensorFlow is represented by a computational graph, so we need to run a computational graph for everything. That is, in order to compute anything on TensorFlow, we need to create a TensorFlow session.</p>
    <p class="normal">Let's execute the following code to multiply two numbers:</p>
    <pre class="programlisting code"><code class="hljs-code">a = tf.multiply(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)
print(a)
</code></pre>
    <p class="normal">Instead of printing <code class="Code-In-Text--PACKT-">9</code>, the preceding code will print a TensorFlow object, <code class="Code-In-Text--PACKT-">Tensor("Mul:0", shape=(), dtype=int32)</code>.</p>
    <p class="normal">As we discussed earlier, whenever we import TensorFlow, a default computational graph is automatically created and all nodes are attached to the graph. Hence, when we print <code class="Code-In-Text--PACKT-">a</code>, it just returns the TensorFlow object because the value for <code class="Code-In-Text--PACKT-">a</code> is not computed yet, as the computation graph has not been executed.</p>
    <p class="normal">In order to execute the graph, we need to initialize and run the TensorFlow session, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">a = tf.multiply(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)
<span class="hljs-keyword">with</span> tf.Session <span class="hljs-keyword">as</span> sess:
    print(sess.run(a))
</code></pre>
    <p class="normal">The preceding code prints <code class="Code-In-Text--PACKT-">9</code>.</p>
    <p class="normal">Now that we have learned about sessions, in the next section, we will learn about variables, constants, and <a id="_idIndexMarker786"/>placeholders.</p>
    <h1 id="_idParaDest-211" class="title">Variables, constants, and placeholders</h1>
    <p class="normal">Variables, constants, and placeholders are fundamental elements of TensorFlow. However, there is always confusion between these three. Let's look at each element, one by one, and learn the difference between them.</p>
    <h2 id="_idParaDest-212" class="title">Variables</h2>
    <p class="normal">Variables are<a id="_idIndexMarker787"/> containers used to store values. Variables are used as input to several other operations in a computational graph. A variable can be created using the <code class="Code-In-Text--PACKT-">tf.Variable()</code> function, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.Variable(<span class="hljs-number">13</span>)
</code></pre>
    <p class="normal">Let's create a variable called <code class="Code-In-Text--PACKT-">W</code>, using <code class="Code-In-Text--PACKT-">tf.Variable()</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">W = tf.Variable(tf.random_normal([<span class="hljs-number">500</span>, <span class="hljs-number">111</span>], stddev=<span class="hljs-number">0.35</span>), name=<span class="hljs-string">"weights"</span>)
</code></pre>
    <p class="normal">As you can see in the preceding code, we create a variable, <code class="Code-In-Text--PACKT-">W</code>, by randomly drawing values from a normal distribution with a standard deviation of <code class="Code-In-Text--PACKT-">0.35</code>.</p>
    <p class="normal">What is that <code class="Code-In-Text--PACKT-">name</code> parameter in <code class="Code-In-Text--PACKT-">tf.Variable()</code>?</p>
    <p class="normal">It is used to set the name of the variable in the computational graph. So, in the preceding code, Python saves the variable as <code class="Code-In-Text--PACKT-">W</code> but in the TensorFlow graph, it will be saved as <code class="Code-In-Text--PACKT-">weights</code>.</p>
    <p class="normal">After defining a variable, we need to initialize all of the variables in the computational graph. That can be done using <code class="Code-In-Text--PACKT-">tf.global_variables_initializer()</code>.</p>
    <p class="normal">Once we create a session, we run the initialization operation, which initializes all of the defined variables, and only then can we run the other operations, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.Variable(<span class="hljs-number">1212</span>)
init = tf.global_variables_initializer()
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
  sess.run(init) 
  <span class="hljs-keyword">print</span>(sess.run(x))
</code></pre>
    <h2 id="_idParaDest-213" class="title">Constants</h2>
    <p class="normal">Constants, unlike<a id="_idIndexMarker788"/> variables, cannot have their values changed. That is, constants are immutable. Once they are assigned values, they cannot be changed throughout the program. We can create constants using <code class="Code-In-Text--PACKT-">tf.constant()</code>, as the following code shows:</p>
    <pre class="programlisting code"><code class="hljs-code"> x = tf.constant(<span class="hljs-number">13</span>)
</code></pre>
    <h2 id="_idParaDest-214" class="title">Placeholders and feed dictionaries</h2>
    <p class="normal">We can<a id="_idIndexMarker789"/> think of <a id="_idIndexMarker790"/>placeholders as variables, where we only define the type and dimension, but do not assign a value. Values for the placeholders will be fed at runtime. We feed data to computational graphs using placeholders. Placeholders are defined with no values.</p>
    <p class="normal">A placeholder can be defined using <code class="Code-In-Text--PACKT-">tf.placeholder()</code>. It takes an optional argument called <code class="Code-In-Text--PACKT-">shape</code>, which denotes the dimensions of the data. If <code class="Code-In-Text--PACKT-">shape</code> is set to <code class="Code-In-Text--PACKT-">None</code>, then we can feed data of any size at runtime. A placeholder can be defined as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"> x = tf.placeholder(<span class="hljs-string">"float"</span>, shape=<span class="hljs-literal">None</span>)
</code></pre>
    <div class="note">
      <p class="Information-Box--PACKT-">To put it in simple terms, we use <code class="Code-In-Text--PACKT-">tf.Variable</code> to store data and <code class="Code-In-Text--PACKT-">tf.placeholder</code> to feed in external data.</p>
    </div>
    <p class="normal">Let's consider a simple example to better understand placeholders:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.placeholder(<span class="hljs-string">"float"</span>, <span class="hljs-literal">None</span>)
y = x+<span class="hljs-number">3</span>
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    result = sess.run(y)
    print(result)
</code></pre>
    <p class="normal">If we run the preceding code, then it will return an error because we are trying to compute <code class="Code-In-Text--PACKT-">y</code>, where <code class="Code-In-Text--PACKT-">y= x+3</code> and <code class="Code-In-Text--PACKT-">x</code> is a placeholder whose value is not assigned. As we have learned, values for the placeholders will be assigned at runtime. We assign the values of the placeholder using the <code class="Code-In-Text--PACKT-">feed_dict</code> parameter. The <code class="Code-In-Text--PACKT-">feed_dict</code> parameter is basically a dictionary where the key represents the name of the placeholder, and the value represents the value of the placeholder.</p>
    <p class="normal">As you can see in the following code, we set <code class="Code-In-Text--PACKT-">feed_dict = {x:5}</code>, which implies that the value for the <code class="Code-In-Text--PACKT-">x</code> placeholder is <code class="Code-In-Text--PACKT-">5</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    result = sess.run(y, feed_dict={x: <span class="hljs-number">5</span>})
    print(result)
</code></pre>
    <p class="normal">The <a id="_idIndexMarker791"/>preceding <a id="_idIndexMarker792"/>code returns <code class="Code-In-Text--PACKT-">8.0</code>.</p>
    <p class="normal">That's it. In the next section, we will learn about TensorBoard.</p>
    <h1 id="_idParaDest-215" class="title">Introducing TensorBoard</h1>
    <p class="normal">TensorBoard is TensorFlow's <a id="_idIndexMarker793"/>visualization tool, which can be used to visualize a computational graph. It can also be used to plot various quantitative metrics and the results of several intermediate calculations. When we are training a really deep neural network, it becomes confusing when we have to debug the network. So, if we can visualize the computational graph in TensorBoard, we can easily understand such complex models, debug them, and optimize them. TensorBoard also supports sharing.</p>
    <p class="normal">As shown in <em class="italic">Figure 8.2</em>, the TensorBoard panel consists of several tabs—<strong class="keyword">SCALARS</strong>, <strong class="keyword">IMAGES</strong>, <strong class="keyword">AUDIO</strong>, <strong class="keyword">GRAPHS</strong>, <strong class="keyword">DISTRIBUTIONS</strong>, <strong class="keyword">HISTOGRAMS</strong>, and <strong class="keyword">EMBEDDINGS</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.2: TensorBoard</p>
    <p class="normal">The tabs are pretty self-explanatory. The <strong class="keyword">SCALARS</strong> tab shows useful information about the scalar variables we use in our program. For example, it shows how the value of a scalar variable called <code class="Code-In-Text--PACKT-">loss</code> changes over several iterations.</p>
    <p class="normal">The <strong class="keyword">GRAPHS</strong> tab shows the computational graph. The <strong class="keyword">DISTRIBUTIONS</strong> and <strong class="keyword">HISTOGRAMS</strong> tabs show the distribution of a variable. For example, our model's weight distribution and histogram can be seen under these tabs. The <strong class="keyword">EMBEDDINGS</strong> tab is used for visualizing high-dimensional vectors, such as word embeddings. </p>
    <p class="normal">Let's build a basic computational graph and visualize it in TensorBoard. Let's say we have four constants, shown as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant(<span class="hljs-number">1</span>,name=<span class="hljs-string">'x'</span>)
y = tf.constant(<span class="hljs-number">1</span>,name=<span class="hljs-string">'y'</span>)
a = tf.constant(<span class="hljs-number">3</span>,name=<span class="hljs-string">'a'</span>)
b = tf.constant(<span class="hljs-number">3</span>,name=<span class="hljs-string">'b'</span>)
</code></pre>
    <p class="normal">Let's multiply <code class="Code-In-Text--PACKT-">x</code> and <code class="Code-In-Text--PACKT-">y</code> and <code class="Code-In-Text--PACKT-">a</code> and <code class="Code-In-Text--PACKT-">b</code> and save them as <code class="Code-In-Text--PACKT-">prod1</code> and <code class="Code-In-Text--PACKT-">prod2</code>, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">prod1 = tf.multiply(x,y,name=<span class="hljs-string">'prod1'</span>)
prod2 = tf.multiply(a,b,name=<span class="hljs-string">'prod2'</span>)
</code></pre>
    <p class="normal">Add <code class="Code-In-Text--PACKT-">prod1</code> and <code class="Code-In-Text--PACKT-">prod2</code> and store them in <code class="Code-In-Text--PACKT-">sum</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">sum = tf.add(prod1,prod2,name=<span class="hljs-string">'sum'</span>)
</code></pre>
    <p class="normal">Now, we can visualize all of these operations in TensorBoard. To visualize in TensorBoard, we first need to save our event files. That can be done using <code class="Code-In-Text--PACKT-">tf.summary.FileWriter()</code>. It takes two important parameters, <code class="Code-In-Text--PACKT-">logdir</code> and <code class="Code-In-Text--PACKT-">graph</code>.</p>
    <p class="normal">As the name suggests, <code class="Code-In-Text--PACKT-">logdir</code> specifies the directory where we want to store the graph, and <code class="Code-In-Text--PACKT-">graph</code> specifies which graph we want to store:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    writer = tf.summary.FileWriter(logdir=<span class="hljs-string">'./graphs'</span>,graph=sess.graph)
    print(sess.run(sum))
</code></pre>
    <p class="normal">In the preceding <a id="_idIndexMarker794"/>code, <code class="Code-In-Text--PACKT-">./graphs</code> is the directory where we are storing our event file, and <code class="Code-In-Text--PACKT-">sess.graph</code> specifies the current graph in our TensorFlow session. So, we are storing the current graph of the TensorFlow session in the <code class="Code-In-Text--PACKT-">graphs</code> directory.</p>
    <p class="normal">To start TensorBoard, go to your Terminal, locate the working directory, and type the following:</p>
    <pre class="programlisting code"><code class="hljs-code">tensorboard --logdir=graphs --port=<span class="hljs-number">8000</span>
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">logdir</code> parameter indicates the directory where the event file is stored and <code class="Code-In-Text--PACKT-">port</code> is the port number. Once you run the preceding command, open your browser and type <code class="Code-In-Text--PACKT-">http://localhost:8000/</code>.</p>
    <p class="normal">In the TensorBoard panel, under the <strong class="keyword">GRAPHS</strong> tab, you can see the computational graph:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.3: Computational graph</p>
    <p class="normal">As you may notice, all <a id="_idIndexMarker795"/>of the operations we have defined are clearly shown in the graph.</p>
    <h2 id="_idParaDest-216" class="title">Creating a name scope</h2>
    <p class="normal">Scoping is used to<a id="_idIndexMarker796"/> reduce complexity and helps us to better understand a model by grouping related nodes together. Having a name scope helps us to group similar operations in a graph. This comes in handy when we are building a complex architecture. Scoping can be created using <code class="Code-In-Text--PACKT-">tf.name_scope()</code>. In the previous example, we performed two operations, <code class="Code-In-Text--PACKT-">Product</code> and <code class="Code-In-Text--PACKT-">sum</code>. We can simply group them into two different name scopes as <code class="Code-In-Text--PACKT-">Product</code> and <code class="Code-In-Text--PACKT-">sum</code>.</p>
    <p class="normal">In the previous section, we saw how <code class="Code-In-Text--PACKT-">prod1</code> and <code class="Code-In-Text--PACKT-">prod2</code> perform multiplication and compute the result. We'll define a name scope called <code class="Code-In-Text--PACKT-">Product</code>, and group the <code class="Code-In-Text--PACKT-">prod1</code> and <code class="Code-In-Text--PACKT-">prod2</code> operations, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">"Product"</span>):
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">"prod1"</span>):
        prod1 = tf.multiply(x,y,name=<span class="hljs-string">'prod1'</span>)
        
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">"prod2"</span>):
        prod2 = tf.multiply(a,b,name=<span class="hljs-string">'prod2'</span>)
</code></pre>
    <p class="normal">Now, define the name scope for <code class="Code-In-Text--PACKT-">sum</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">"sum"</span>):
    sum = tf.add(prod1,prod2,name=<span class="hljs-string">'sum'</span>)
</code></pre>
    <p class="normal">Store the file in the <code class="Code-In-Text--PACKT-">graphs</code> directory:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    writer = tf.summary.FileWriter(<span class="hljs-string">'./graphs'</span>, sess.graph)
    print(sess.run(sum))
</code></pre>
    <p class="normal">Visualize the graph in TensorBoard:</p>
    <pre class="programlisting code"><code class="hljs-code">tensorboard --logdir=graphs --port=<span class="hljs-number">8000</span>
</code></pre>
    <p class="normal">As you may notice, now, we have only two nodes, <strong class="keyword">sum</strong> and <strong class="keyword">Product</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.4: A computational graph</p>
    <p class="normal">Once we double-click<a id="_idIndexMarker797"/> on the nodes, we can see how the computation is happening. As you can see, the <strong class="keyword">prod1</strong> and <strong class="keyword">prod2</strong> nodes are grouped under the <strong class="keyword">Product</strong> scope, and their results are sent to the <strong class="keyword">sum</strong> node, where they will be added. You can see how the <strong class="keyword">prod1</strong> and <strong class="keyword">prod2</strong> nodes compute their value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.5: A computational graph in detail</p>
    <p class="normal">The preceding graph is just a simple example. When we are working on a complex project with a lot of operations, name scoping helps us to group similar operations together and <a id="_idIndexMarker798"/>enables us to understand the computational graph better.</p>
    <p class="normal">Now that we have learned about TensorFlow, in the next section, let's see how to build handwritten digit classification using TensorFlow.</p>
    <h1 id="_idParaDest-217" class="title">Handwritten digit classification using TensorFlow</h1>
    <p class="normal">Putting together<a id="_idIndexMarker799"/> all the concepts we have<a id="_idIndexMarker800"/> learned so far, we will see how we can use TensorFlow to build a neural network to recognize handwritten digits. If you have been playing around with deep learning of late, then you must have come across the MNIST dataset. It has been called the <em class="italic">hello world</em> of deep learning. It consists of 55,000 data points of handwritten digits (0 to 9).</p>
    <p class="normal">In this section, we will see how we can use our neural network to recognize these handwritten <a id="_idIndexMarker801"/>digits, and we will get the hang of<a id="_idIndexMarker802"/> TensorFlow and TensorBoard.</p>
    <h2 id="_idParaDest-218" class="title">Importing the required libraries</h2>
    <p class="normal">As a first <a id="_idIndexMarker803"/>step, let's import all of the required libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">'ignore'</span>)
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.examples.tutorials.mnist <span class="hljs-keyword">import</span> input_data
tf.logging.set_verbosity(tf.logging.ERROR)
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
%matplotlib inline
</code></pre>
    <h2 id="_idParaDest-219" class="title">Loading the dataset</h2>
    <p class="normal">Load the <a id="_idIndexMarker804"/>dataset, using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">mnist = input_data.read_data_sets(<span class="hljs-string">"data/mnist"</span>, one_hot=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">In the preceding code, <code class="Code-In-Text--PACKT-">data/mnist</code> implies the location where we store the MNIST dataset, and <code class="Code-In-Text--PACKT-">one_hot=True</code> implies that we are one-hot encoding the labels (0 to 9).</p>
    <p class="normal">We will see what we have in our data by executing the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">"No of images in training set {}"</span>.format(mnist.train.images.shape))
print(<span class="hljs-string">"No of labels in training set {}"</span>.format(mnist.train.labels.shape))
print(<span class="hljs-string">"No of images in test set {}"</span>.format(mnist.test.images.shape))
print(<span class="hljs-string">"No of labels in test set {}"</span>.format(mnist.test.labels.shape))
No of images <span class="hljs-keyword">in</span> training set (<span class="hljs-number">55000</span>, <span class="hljs-number">784</span>)
No of labels <span class="hljs-keyword">in</span> training set (<span class="hljs-number">55000</span>, <span class="hljs-number">10</span>)
No of images <span class="hljs-keyword">in</span> test set (<span class="hljs-number">10000</span>, <span class="hljs-number">784</span>)
No of labels <span class="hljs-keyword">in</span> test set (<span class="hljs-number">10000</span>, <span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">We have <code class="Code-In-Text--PACKT-">55000</code> images in the training set, each image is of size <code class="Code-In-Text--PACKT-">784</code>, and we have <code class="Code-In-Text--PACKT-">10</code> labels, which are actually 0 to 9. Similarly, we have <code class="Code-In-Text--PACKT-">10000</code> images in the test set.</p>
    <p class="normal">Now, we'll plot an input image to see what it looks like:</p>
    <pre class="programlisting code"><code class="hljs-code">img1 = mnist.train.images[<span class="hljs-number">0</span>].reshape(<span class="hljs-number">28</span>,<span class="hljs-number">28</span>)
plt.imshow(img1, cmap=<span class="hljs-string">'Greys'</span>)
</code></pre>
    <p class="normal">Thus, our input <a id="_idIndexMarker805"/>image looks like the following:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.6: Image of the digit 7 from the training set</p>
    <h2 id="_idParaDest-220" class="title">Defining the number of neurons in each layer</h2>
    <p class="normal">We'll build a four-layer <a id="_idIndexMarker806"/>neural network with three hidden layers and one output layer. As the size of the input image is <code class="Code-In-Text--PACKT-">784</code>, we set <code class="Code-In-Text--PACKT-">num_input</code> to <code class="Code-In-Text--PACKT-">784</code>, and since we have 10 handwritten digits (0 to 9), we set <code class="Code-In-Text--PACKT-">10</code> neurons in the output layer. </p>
    <p class="normal">We define the number of neurons in each layer as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#number of neurons in input layer</span>
num_input = <span class="hljs-number">784</span>
<span class="hljs-comment">#num of neurons in hidden layer 1</span>
num_hidden1 = <span class="hljs-number">512</span>
<span class="hljs-comment">#num of neurons in hidden layer 2</span>
num_hidden2 = <span class="hljs-number">256</span>
<span class="hljs-comment">#num of neurons in hidden layer 3</span>
num_hidden_3 = <span class="hljs-number">128</span>
<span class="hljs-comment">#num of neurons in output layer</span>
num_output = <span class="hljs-number">10</span>
</code></pre>
    <h2 id="_idParaDest-221" class="title">Defining placeholders</h2>
    <p class="normal">As we have<a id="_idIndexMarker807"/> learned, we<a id="_idIndexMarker808"/> first need to define the placeholders for <code class="Code-In-Text--PACKT-">input</code> and <code class="Code-In-Text--PACKT-">output</code>. Values for the placeholders will be fed in at runtime through <code class="Code-In-Text--PACKT-">feed_dict</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'input'</span>):
    X = tf.placeholder(<span class="hljs-string">"float"</span>, [<span class="hljs-literal">None</span>, num_input])
<span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'output'</span>):
    Y = tf.placeholder(<span class="hljs-string">"float"</span>, [<span class="hljs-literal">None</span>, num_output])
</code></pre>
    <p class="normal">Since we have a four-layer network, we have four weights and four biases. We initialize our weights by drawing values from the truncated normal distribution with a standard deviation of <code class="Code-In-Text--PACKT-">0.1</code>. Remember, the dimensions of the weight matrix should be <em class="italic">the</em> <em class="italic">number of neurons in the previous layer</em> x <em class="italic">the number of neurons in the current layer</em>. For instance, the dimensions of weight matrix <code class="Code-In-Text--PACKT-">w3</code> should be <em class="italic">the</em> <em class="italic">number of neurons in hidden layer 2</em> x <em class="italic">the number of neurons in hidden layer 3</em>.</p>
    <p class="normal">We often define all of the weights in a dictionary, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'weights'</span>):
 
 weights = {
 <span class="hljs-string">'w1'</span>: tf.Variable(tf.truncated_normal([num_input, num_hidden1], stddev=<span class="hljs-number">0.1</span>),name=<span class="hljs-string">'weight_1'</span>),
 <span class="hljs-string">'w2'</span>: tf.Variable(tf.truncated_normal([num_hidden1, num_hidden2], stddev=<span class="hljs-number">0.1</span>),name=<span class="hljs-string">'weight_2'</span>),
 <span class="hljs-string">'w3'</span>: tf.Variable(tf.truncated_normal([num_hidden2, num_hidden_3], stddev=<span class="hljs-number">0.1</span>),name=<span class="hljs-string">'weight_3'</span>),
 <span class="hljs-string">'out'</span>: tf.Variable(tf.truncated_normal([num_hidden_3, num_output], stddev=<span class="hljs-number">0.1</span>),name=<span class="hljs-string">'weight_4'</span>),
 }
</code></pre>
    <p class="normal">The shape of the bias should be the number of neurons in the current layer. For instance, the dimension of the <code class="Code-In-Text--PACKT-">b2</code> bias is the number of neurons in hidden layer 2. We set the bias<a id="_idIndexMarker809"/> value as a <a id="_idIndexMarker810"/>constant; <code class="Code-In-Text--PACKT-">0.1</code> in all of the layers:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'biases'</span>):
    biases = {
        <span class="hljs-string">'b1'</span>: tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=[num_hidden1]),name=<span class="hljs-string">'bias_1'</span>),
        <span class="hljs-string">'b2'</span>: tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=[num_hidden2]),name=<span class="hljs-string">'bias_2'</span>),
        <span class="hljs-string">'b3'</span>: tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=[num_hidden_3]),name=<span class="hljs-string">'bias_3'</span>),
        <span class="hljs-string">'out'</span>: tf.Variable(tf.constant(<span class="hljs-number">0.1</span>, shape=[num_output]),name=<span class="hljs-string">'bias_4'</span>)
    }
</code></pre>
    <h2 id="_idParaDest-222" class="title">Forward propagation</h2>
    <p class="normal">Now we'll define <a id="_idIndexMarker811"/>the forward <a id="_idIndexMarker812"/>propagation operation. We'll use ReLU activations in all layers. In the last layers, we'll apply <code class="Code-In-Text--PACKT-">sigmoid</code> activation, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'Model'</span>):
    
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'layer1'</span>):
        layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights[<span class="hljs-string">'w1'</span>]), biases[<span class="hljs-string">'b1'</span>]) )
    
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'layer2'</span>):
        layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights[<span class="hljs-string">'w2'</span>]), biases[<span class="hljs-string">'b2'</span>]))
        
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'layer3'</span>):
        layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights[<span class="hljs-string">'w3'</span>]), biases[<span class="hljs-string">'b3'</span>]))
        
    <span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'output_layer'</span>):
         y_hat = tf.nn.sigmoid(tf.matmul(layer_3, weights[<span class="hljs-string">'out'</span>]) + biases[<span class="hljs-string">'out'</span>])
</code></pre>
    <h2 id="_idParaDest-223" class="title">Computing loss and backpropagation</h2>
    <p class="normal">Next, we'll define<a id="_idIndexMarker813"/> our <a id="_idIndexMarker814"/>loss function. We'll use softmax cross-entropy as <a id="_idIndexMarker815"/>our loss function. TensorFlow provides the <code class="Code-In-Text--PACKT-">tf.nn.softmax_cross_entropy_with_logits()</code> function for computing softmax cross-entropy loss. It takes two parameters as inputs, <code class="Code-In-Text--PACKT-">logits</code> and <code class="Code-In-Text--PACKT-">labels</code>:</p>
    <ul>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">logits</code> parameter specifies the <code class="Code-In-Text--PACKT-">logits</code> predicted by our network; for example, <code class="Code-In-Text--PACKT-">y_hat</code></li>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">labels</code> parameter specifies the actual labels; for example, true labels, <code class="Code-In-Text--PACKT-">Y</code></li>
    </ul>
    <p class="normal">We take the mean of the <code class="Code-In-Text--PACKT-">loss</code> function using <code class="Code-In-Text--PACKT-">tf.reduce_mean()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'Loss'</span>):
        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_hat,labels=Y))
</code></pre>
    <p class="normal">Now, we need to minimize the loss using backpropagation. Don't worry! We don't have to calculate the derivatives of all the weights manually. Instead, we can use TensorFlow's <code class="Code-In-Text--PACKT-">optimizer</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">learning_rate = <span class="hljs-number">1e-4</span>
optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)
</code></pre>
    <h2 id="_idParaDest-224" class="title">Computing accuracy</h2>
    <p class="normal">We calculate the<a id="_idIndexMarker816"/> accuracy of our model as follows:</p>
    <ul>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">y_hat</code> parameter denotes the predicted probability for each class of our model. Since we have <code class="Code-In-Text--PACKT-">10</code> classes, we will have <code class="Code-In-Text--PACKT-">10</code> probabilities. If the probability is high at position <code class="Code-In-Text--PACKT-">7</code>, then it means that our network predicts the input image as digit <code class="Code-In-Text--PACKT-">7</code> with high probability. The <code class="Code-In-Text--PACKT-">tf.argmax()</code> function returns the index of the largest value. Thus, <code class="Code-In-Text--PACKT-">tf.argmax(y_hat,1)</code> gives the index where the probability is high. Thus, if the probability is high at index <code class="Code-In-Text--PACKT-">7</code>, then it returns <code class="Code-In-Text--PACKT-">7</code>.</li>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">Y</code> parameter denotes the actual labels, and they are the one-hot encoded values. That is, the <code class="Code-In-Text--PACKT-">Y</code> parameter consists of zeros everywhere except at the position of the actual image, where it has a value of <code class="Code-In-Text--PACKT-">1</code>. For instance, if the input image is <code class="Code-In-Text--PACKT-">7</code>, then <code class="Code-In-Text--PACKT-">Y</code> has a value of 0 at all indices except at index <code class="Code-In-Text--PACKT-">7</code>, where it has a value of <code class="Code-In-Text--PACKT-">1</code>. Thus, <code class="Code-In-Text--PACKT-">tf.argmax(Y,1)</code> returns <code class="Code-In-Text--PACKT-">7</code> because that is where we have a high value, <code class="Code-In-Text--PACKT-">1</code>.</li>
    </ul>
    <p class="normal">Thus, <code class="Code-In-Text--PACKT-">tf.argmax(y_hat,1)</code> gives the predicted digit, and <code class="Code-In-Text--PACKT-">tf.argmax(Y,1)</code> gives us the actual digit.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">tf.equal(x, y)</code> function takes <code class="Code-In-Text--PACKT-">x</code> and <code class="Code-In-Text--PACKT-">y</code> as inputs and returns the truth value of <em class="italic">(x == y)</em> element-wise. Thus, <code class="Code-In-Text--PACKT-">correct_pred = tf.equal(predicted_digit,actual_digit)</code> consists of <code class="Code-In-Text--PACKT-">True</code> where the actual and predicted digits are the same, and <code class="Code-In-Text--PACKT-">False</code> where the actual and predicted digits are not the same. We convert the Boolean values in <code class="Code-In-Text--PACKT-">correct_pred</code> into float <a id="_idIndexMarker817"/>values using TensorFlow's cast operation, <code class="Code-In-Text--PACKT-">tf.cast(correct_pred, tf.float32)</code>. After converting them into float values, we take the average using <code class="Code-In-Text--PACKT-">tf.reduce_mean()</code>.</p>
    <p class="normal">Thus, <code class="Code-In-Text--PACKT-">tf.reduce_mean(tf.cast(correct_pred, tf.float32))</code> gives us the average correct predictions:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.name_scope(<span class="hljs-string">'Accuracy'</span>):
    
    predicted_digit = tf.argmax(y_hat, <span class="hljs-number">1</span>)
    actual_digit = tf.argmax(Y, <span class="hljs-number">1</span>)
    
    correct_pred = tf.equal(predicted_digit,actual_digit)
    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))
</code></pre>
    <h2 id="_idParaDest-225" class="title">Creating a summary</h2>
    <p class="normal">We can also visualize<a id="_idIndexMarker818"/> how the loss and accuracy of our model changes during several iterations in TensorBoard. So, we use <code class="Code-In-Text--PACKT-">tf.summary()</code> to get the summary of the variable. Since the loss and accuracy are scalar variables, we use <code class="Code-In-Text--PACKT-">tf.summary.scalar()</code>, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.summary.scalar(<span class="hljs-string">"Accuracy"</span>, accuracy)
tf.summary.scalar(<span class="hljs-string">"Loss"</span>, loss)
</code></pre>
    <p class="normal">Next, we merge all of the summaries we use in our graph, using <code class="Code-In-Text--PACKT-">tf.summary.merge_all()</code>. We do this because when we have many summaries, running and storing them would become inefficient, so we run them once in our session instead of running them <a id="_idIndexMarker819"/>multiple times:</p>
    <pre class="programlisting code"><code class="hljs-code">merge_summary = tf.summary.merge_all()
</code></pre>
    <h2 id="_idParaDest-226" class="title">Training the model</h2>
    <p class="normal">Now, it is time to<a id="_idIndexMarker820"/> train our model. As we have learned, first, we need to initialize all of the variables:</p>
    <pre class="programlisting code"><code class="hljs-code">init = tf.global_variables_initializer()
</code></pre>
    <p class="normal">Define the batch size, number of iterations, and learning rate, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">learning_rate = <span class="hljs-number">1e-4</span>
num_iterations = <span class="hljs-number">1000</span>
batch_size = <span class="hljs-number">128</span>
</code></pre>
    <p class="normal">Start the TensorFlow session:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
</code></pre>
    <p class="normal">Initialize all the variables:</p>
    <pre class="programlisting code"><code class="hljs-code">    sess.run(init)
</code></pre>
    <p class="normal">Save the event files:</p>
    <pre class="programlisting code"><code class="hljs-code">    summary_writer = tf.summary.FileWriter(<span class="hljs-string">'./graphs'</span>, graph=tf.get_default_graph()
</code></pre>
    <p class="normal">Train the model for a number of iterations:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
</code></pre>
    <p class="normal">Get a batch of data according to the batch size:</p>
    <pre class="programlisting code"><code class="hljs-code">        batch_x, batch_y = mnist.train.next_batch(batch_size)
</code></pre>
    <p class="normal">Train the network:</p>
    <pre class="programlisting code"><code class="hljs-code">        sess.run(optimizer, feed_dict={ X: batch_x, Y: batch_y})
</code></pre>
    <p class="normal">Print <code class="Code-In-Text--PACKT-">loss</code> and <code class="Code-In-Text--PACKT-">accuracy</code> for every 100<sup class="Superscript--PACKT-">th</sup> iteration:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> i % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
            batch_loss, batch_accuracy,summary = sess.run(
                [loss, accuracy, merge_summary],
                feed_dict={X: batch_x, Y: batch_y}
                )
            <span class="hljs-comment">#store all the summaries    </span>
            summary_writer.add_summary(summary, i)
            print(<span class="hljs-string">'Iteration: {}, Loss: {}, Accuracy: {}'</span>.format(i,batch_loss,batch_accuracy))
</code></pre>
    <p class="normal">As you may notice from the following output, the loss decreases and the accuracy increases<a id="_idIndexMarker821"/> over various training iterations:</p>
    <pre class="programlisting code"><code class="hljs-code">Iteration: <span class="hljs-number">0</span>, Loss: <span class="hljs-number">2.30789709091</span>, Accuracy: <span class="hljs-number">0.1171875</span>
Iteration: <span class="hljs-number">100</span>, Loss: <span class="hljs-number">1.76062202454</span>, Accuracy: <span class="hljs-number">0.859375</span>
Iteration: <span class="hljs-number">200</span>, Loss: <span class="hljs-number">1.60075569153</span>, Accuracy: <span class="hljs-number">0.9375</span>
Iteration: <span class="hljs-number">300</span>, Loss: <span class="hljs-number">1.60388696194</span>, Accuracy: <span class="hljs-number">0.890625</span>
Iteration: <span class="hljs-number">400</span>, Loss: <span class="hljs-number">1.59523034096</span>, Accuracy: <span class="hljs-number">0.921875</span>
Iteration: <span class="hljs-number">500</span>, Loss: <span class="hljs-number">1.58489584923</span>, Accuracy: <span class="hljs-number">0.859375</span>
Iteration: <span class="hljs-number">600</span>, Loss: <span class="hljs-number">1.51407408714</span>, Accuracy: <span class="hljs-number">0.953125</span>
Iteration: <span class="hljs-number">700</span>, Loss: <span class="hljs-number">1.53311181068</span>, Accuracy: <span class="hljs-number">0.9296875</span>
Iteration: <span class="hljs-number">800</span>, Loss: <span class="hljs-number">1.57677125931</span>, Accuracy: <span class="hljs-number">0.875</span>
Iteration: <span class="hljs-number">900</span>, Loss: <span class="hljs-number">1.52060437202</span>, Accuracy: <span class="hljs-number">0.9453125</span>
</code></pre>
    <h2 id="_idParaDest-227" class="title">Visualizing graphs in TensorBoard</h2>
    <p class="normal">After training, we can<a id="_idIndexMarker822"/> visualize<a id="_idIndexMarker823"/> our computational graph in TensorBoard, as shown in <em class="italic">Figure 8.7</em>. As you can see, our <strong class="keyword">Model</strong> takes <strong class="keyword">input</strong>, <strong class="keyword">weights</strong>, and <strong class="keyword">biases</strong> as input and returns the output. We compute <strong class="keyword">Loss</strong> and <strong class="keyword">Accuracy</strong> based on the output of the model. We minimize the loss by calculating <strong class="keyword">gradients</strong> and updating <strong class="keyword">weights</strong>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.7: Computational graph</p>
    <p class="normal">If we double-click<a id="_idIndexMarker824"/> and <a id="_idIndexMarker825"/>expand <strong class="keyword">Model</strong>, we can see that we have three hidden layers and one output layer:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.8: Expanding the Model node</p>
    <p class="normal">Similarly, we can double-click and see every node. For instance, if we open <strong class="keyword">weights</strong>, we can see how the<a id="_idIndexMarker826"/> four <a id="_idIndexMarker827"/>weights are initialized using truncated normal distribution, and how it is updated using the Adam optimizer:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.9: Expanding the weights node</p>
    <p class="normal">As we have learned, the computational graph helps us to understand what is happening on each node. </p>
    <p class="normal">We can see how the accuracy is being calculated by double-clicking on the <strong class="keyword">Accuracy</strong> node:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.10: Expanding the Accuracy node</p>
    <p class="normal">Remember that we <a id="_idIndexMarker828"/>also stored <a id="_idIndexMarker829"/>a summary of our <code class="Code-In-Text--PACKT-">loss</code> and <code class="Code-In-Text--PACKT-">accuracy</code> variables. We can find them under the <strong class="keyword">SCALARS</strong> tab in TensorBoard. <em class="italic">Figure 8.11</em> shows how the loss decreases over iterations:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.11: Plot of the loss function</p>
    <p class="normal"><em class="italic">Figure 8.12</em> shows <a id="_idIndexMarker830"/>how <a id="_idIndexMarker831"/>accuracy increases over iterations:</p>
    <figure class="mediaobject"><img src="../Images/B15558_08_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.12 Plot of accuracy</p>
    <p class="normal">That's it. In the next section, we will learn about another interesting feature in TensorFlow called eager execution.</p>
    <h1 id="_idParaDest-228" class="title">Introducing eager execution</h1>
    <p class="normal">Eager execution in <a id="_idIndexMarker832"/>TensorFlow is more Pythonic and allows rapid prototyping. Unlike graph mode, where we need to construct a graph every time we want to perform any operation, eager execution follows the imperative programming paradigm, where any operation can be performed immediately, without having to create a graph, just like we do in Python. Hence, with eager execution, we can say goodbye to sessions and placeholders. It also makes the debugging process easier with an immediate runtime error, unlike graph mode. For instance, in graph mode, to compute anything, we run the session. As shown in the following code, to evaluate the value of <code class="Code-In-Text--PACKT-">z</code>, we have to run the TensorFlow session:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant(<span class="hljs-number">11</span>)
y = tf.constant(<span class="hljs-number">11</span>)
z = x*y
<span class="hljs-keyword">with</span> tf.Session() <span class="hljs-keyword">as</span> sess:
    <span class="hljs-keyword">print</span>(sess.run(z))
</code></pre>
    <p class="normal">With eager execution, we don't need to create a session; we can simply compute <code class="Code-In-Text--PACKT-">z</code>, just like we do in Python. In order to enable eager execution, just call the <code class="Code-In-Text--PACKT-">tf.enable_eager_execution()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant(<span class="hljs-number">11</span>)
y = tf.constant(<span class="hljs-number">11</span>)
z = x*y
<span class="hljs-keyword">print</span>(z)
</code></pre>
    <p class="normal">It will return the following:</p>
    <pre class="programlisting code"><code class="hljs-code">&lt;tf.Tensor: id=<span class="hljs-number">789</span>, shape=(), dtype=int32, numpy=<span class="hljs-number">121</span>&gt;
</code></pre>
    <p class="normal">In order to get the output value, we can print the following:</p>
    <pre class="programlisting code"><code class="hljs-code">z.numpy()
<span class="hljs-number">121</span>
</code></pre>
    <p class="normal">Although eager execution enables the imperative programming paradigm, in this book, we will investigate most of the examples in non-eager mode to better understand the algorithms from scratch. In the next section, we will see how to perform math operations using TensorFlow.</p>
    <h1 id="_idParaDest-229" class="title">Math operations in TensorFlow</h1>
    <p class="normal">Now, we will explore <a id="_idIndexMarker833"/>some of the operations in TensorFlow using eager execution mode:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>])
y = tf.constant([<span class="hljs-number">3.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">1.</span>])
</code></pre>
    <p class="normal">Let's start with some basic arithmetic operations.</p>
    <p class="normal">Use <code class="Code-In-Text--PACKT-">tf.add</code> to add two numbers:</p>
    <pre class="programlisting code"><code class="hljs-code">sum = tf.add(x,y)
sum.numpy()
array([<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>], dtype=float32)
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">tf.subtract</code> function is used for finding the difference between two numbers:</p>
    <pre class="programlisting code"><code class="hljs-code">difference = tf.subtract(x,y)
difference.numpy()
array([<span class="hljs-number">-2.</span>,  <span class="hljs-number">0.</span>,  <span class="hljs-number">2.</span>], dtype=float32)
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">tf.multiply</code> function is used for multiplying two numbers:</p>
    <pre class="programlisting code"><code class="hljs-code">product = tf.multiply(x,y)
product.numpy()
array([<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">3.</span>], dtype=float32)
</code></pre>
    <p class="normal">Divide two numbers using <code class="Code-In-Text--PACKT-">tf.divide</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">division = tf.divide(x,y)
division.numpy()
array([<span class="hljs-number">0.33333334</span>, <span class="hljs-number">1.</span>        , <span class="hljs-number">3.</span>        ], dtype=float32)
</code></pre>
    <p class="normal">The dot product can be computed as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">dot_product = tf.reduce_sum(tf.multiply(x, y))
dot_product.numpy()
<span class="hljs-number">10.0</span>
</code></pre>
    <p class="normal">Next, let's find the index of the minimum and maximum elements:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant([<span class="hljs-number">10</span>, <span class="hljs-number">0</span>, <span class="hljs-number">13</span>, <span class="hljs-number">9</span>])
</code></pre>
    <p class="normal">The index of the minimum value is computed using <code class="Code-In-Text--PACKT-">tf.argmin()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.argmin(x).numpy()
<span class="hljs-number">1</span>
</code></pre>
    <p class="normal">The index of the maximum value is computed using <code class="Code-In-Text--PACKT-">tf.argmax()</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.argmax(x).numpy()
<span class="hljs-number">2</span>
</code></pre>
    <p class="normal">Run the following <a id="_idIndexMarker834"/>code to find the squared difference between <code class="Code-In-Text--PACKT-">x</code> and <code class="Code-In-Text--PACKT-">y</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.Variable([<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">5</span>,<span class="hljs-number">7</span>,<span class="hljs-number">11</span>])
y = tf.Variable([<span class="hljs-number">1</span>])
tf.math.squared_difference(x,y).numpy()
[  <span class="hljs-number">0</span>,   <span class="hljs-number">4</span>,  <span class="hljs-number">16</span>,  <span class="hljs-number">36</span>, <span class="hljs-number">100</span>]
</code></pre>
    <p class="normal">Let's try typecasting; that is, converting from one data type into another.</p>
    <p class="normal">Print the type of <code class="Code-In-Text--PACKT-">x</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">print</span>(x.dtype)
tf.int32
</code></pre>
    <p class="normal">We can convert the type of <code class="Code-In-Text--PACKT-">x</code>, which is <code class="Code-In-Text--PACKT-">tf.int32</code>, into <code class="Code-In-Text--PACKT-">tf.float32</code>, using <code class="Code-In-Text--PACKT-">tf.cast</code>, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.cast(x, dtype=tf.float32)
</code></pre>
    <p class="normal">Now, check the <code class="Code-In-Text--PACKT-">x</code> type. It will be <code class="Code-In-Text--PACKT-">tf.float32</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">print</span>(x.dtype)
tf.float32
</code></pre>
    <p class="normal">Concatenate the two matrices:</p>
    <pre class="programlisting code"><code class="hljs-code">x = [[<span class="hljs-number">3</span>,<span class="hljs-number">6</span>,<span class="hljs-number">9</span>], [<span class="hljs-number">7</span>,<span class="hljs-number">7</span>,<span class="hljs-number">7</span>]]
y = [[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>], [<span class="hljs-number">5</span>,<span class="hljs-number">5</span>,<span class="hljs-number">5</span>]]
</code></pre>
    <p class="normal">Concatenate the matrices row-wise:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.concat([x, y], <span class="hljs-number">0</span>).numpy()
array([[<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>],
       [<span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>],
       [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
       [<span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]], dtype=int32)
</code></pre>
    <p class="normal">Use the following code to concatenate the matrices column-wise:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.concat([x, y], <span class="hljs-number">1</span>).numpy()
array([[<span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>],
       [<span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>]], dtype=int32)
</code></pre>
    <p class="normal">Stack the <code class="Code-In-Text--PACKT-">x</code> matrix <a id="_idIndexMarker835"/>using the <code class="Code-In-Text--PACKT-">stack</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.stack(x, axis=<span class="hljs-number">1</span>).numpy()
array([[<span class="hljs-number">3</span>, <span class="hljs-number">7</span>],
       [<span class="hljs-number">6</span>, <span class="hljs-number">7</span>],
       [<span class="hljs-number">9</span>, <span class="hljs-number">7</span>]], dtype=int32)
</code></pre>
    <p class="normal">Now, let's see how to perform the <code class="Code-In-Text--PACKT-">reduce_mean</code> operation:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.Variable([[<span class="hljs-number">1.0</span>, <span class="hljs-number">5.0</span>], [<span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>]])
x.numpy()
array([[<span class="hljs-number">1.</span>, <span class="hljs-number">5.</span>],
       [<span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>]]
</code></pre>
    <p class="normal">Compute the mean value of <code class="Code-In-Text--PACKT-">x</code>; that is, (<em class="italic">1.0 </em>+<em class="italic"> 5.0 </em>+<em class="italic"> 2.0 </em>+<em class="italic"> 3.0</em>) /<em class="italic"> 4</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.reduce_mean(input_tensor=x).numpy()
<span class="hljs-number">2.75</span>
</code></pre>
    <p class="normal">Compute the mean across the row; that is, (<em class="italic">1.0</em>+<em class="italic">5.0</em>)/<em class="italic">2, </em>(<em class="italic">2.0</em>+<em class="italic">3.0</em>)/<em class="italic">2</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.reduce_mean(input_tensor=x, axis=<span class="hljs-number">0</span>).numpy()
array([<span class="hljs-number">1.5</span>, <span class="hljs-number">4.</span> ], dtype=float32)
</code></pre>
    <p class="normal">Compute the mean across the column; that is, (<em class="italic">1.0</em>+<em class="italic">5.0</em>)/<em class="italic">2.0, </em>(<em class="italic">2.0</em>+<em class="italic">3.0</em>)/<em class="italic">2.0</em>:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.reduce_mean(input_tensor=x, axis=<span class="hljs-number">1</span>, keepdims=<span class="hljs-literal">True</span>).numpy()
array([[<span class="hljs-number">3.</span> ],
       [<span class="hljs-number">2.5</span>]], dtype=float32)
</code></pre>
    <p class="normal">Draw random values from the probability distributions:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.random.normal(shape=(<span class="hljs-number">3</span>,<span class="hljs-number">2</span>), mean=<span class="hljs-number">10.0</span>, stddev=<span class="hljs-number">2.0</span>).numpy()
tf.random.uniform(shape = (<span class="hljs-number">3</span>,<span class="hljs-number">2</span>), minval=<span class="hljs-number">0</span>, maxval=<span class="hljs-literal">None</span>, dtype=tf.float32,).numpy()
</code></pre>
    <p class="normal">Compute the <a id="_idIndexMarker836"/>softmax probabilities:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.constant([<span class="hljs-number">7.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">5.</span>])
tf.nn.softmax(x).numpy()
array([<span class="hljs-number">0.8756006</span> , <span class="hljs-number">0.00589975</span>, <span class="hljs-number">0.11849965</span>], dtype=float32)
</code></pre>
    <p class="normal">Now, we'll look at how to compute the gradients.</p>
    <p class="normal">Define the <code class="Code-In-Text--PACKT-">square</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">square</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
  <span class="hljs-keyword">return</span> tf.multiply(x, x)
</code></pre>
    <p class="normal">The gradients can be computed for the preceding <code class="Code-In-Text--PACKT-">square</code> function using <code class="Code-In-Text--PACKT-">tf.GradientTape</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">with</span> tf.GradientTape(persistent=<span class="hljs-literal">True</span>) <span class="hljs-keyword">as</span> tape:
     <span class="hljs-keyword">print</span>(square(6.).numpy())
<span class="hljs-number">36.0</span>
</code></pre>
    <h1 id="_idParaDest-230" class="title">TensorFlow 2.0 and Keras</h1>
    <p class="normal">TensorFlow 2.0 has got some really cool features. It sets the eager execution mode by default. It<a id="_idIndexMarker837"/> provides <a id="_idIndexMarker838"/>a simplified workflow and uses Keras as the main API for building deep learning models. It is also backward compatible with TensorFlow 1.x versions.</p>
    <p class="normal">To install TensorFlow 2.0, open your Terminal and type the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install tensorflow==2.0.0-alpha0
</code></pre>
    <p class="normal">Since TensorFlow 2.0 uses Keras as a high-level API, we will look at how Keras works in the next section.</p>
    <h2 id="_idParaDest-231" class="title">Bonjour Keras</h2>
    <p class="normal">Keras is another popularly <a id="_idIndexMarker839"/>used deep learning library. It was developed by François Chollet at Google. It is well known for its fast prototyping, and it makes model building simple. It is a high-level library, meaning that it does not perform any low-level operations on its own, such as convolution. It uses a backend engine for doing that, such as TensorFlow. The Keras API is available in <code class="Code-In-Text--PACKT-">tf.keras</code>, and TensorFlow 2.0 uses it as the primary API.</p>
    <p class="normal">Building a model in Keras involves four important steps:</p>
    <ol>
      <li class="numbered">Defining the model</li>
      <li class="numbered">Compiling the model</li>
      <li class="numbered">Fitting the model</li>
      <li class="numbered">Evaluating the model</li>
    </ol>
    <h3 id="_idParaDest-232" class="title">Defining the model</h3>
    <p class="normal">The first step is <a id="_idIndexMarker840"/>defining the model. Keras provides two different APIs to define the model:</p>
    <ul>
      <li class="bullet">The sequential API</li>
      <li class="bullet">The functional API</li>
    </ul>
    <h4 class="title">Defining a sequential model</h4>
    <p class="normal">In a sequential<a id="_idIndexMarker841"/> model, we stack each layer, one<a id="_idIndexMarker842"/> above another:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> keras.models <span class="hljs-keyword">import</span> Sequential
<span class="hljs-keyword">from</span> keras.layers <span class="hljs-keyword">import</span> Dense
</code></pre>
    <p class="normal">First, let's define our model as a <code class="Code-In-Text--PACKT-">Sequential()</code> model, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">model = Sequential()
</code></pre>
    <p class="normal">Now, define the first layer, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(Dense(<span class="hljs-number">13</span>, input_dim=<span class="hljs-number">7</span>, activation=<span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">In the preceding code, <code class="Code-In-Text--PACKT-">Dense</code> implies a fully connected layer, <code class="Code-In-Text--PACKT-">input_dim</code> implies the dimension of our input, and <code class="Code-In-Text--PACKT-">activation</code> specifies the activation function that we use. We can stack up as many layers as we want, one above another.</p>
    <p class="normal">Define the next layer with the <code class="Code-In-Text--PACKT-">relu</code> activation function, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(Dense(<span class="hljs-number">7</span>, activation=<span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">Define the output layer with the <code class="Code-In-Text--PACKT-">sigmoid</code> activation function:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>))
</code></pre>
    <p class="normal">The final code block of the <a id="_idIndexMarker843"/>sequential model is shown as follows. As you can see, the Keras code is much<a id="_idIndexMarker844"/> simpler than the TensorFlow code:</p>
    <pre class="programlisting code"><code class="hljs-code">model = Sequential()
model.add(Dense(<span class="hljs-number">13</span>, input_dim=<span class="hljs-number">7</span>, activation=<span class="hljs-string">'relu'</span>))
model.add(Dense(<span class="hljs-number">7</span>, activation=<span class="hljs-string">'relu'</span>))
model.add(Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>))
</code></pre>
    <h4 class="title">Defining a functional model</h4>
    <p class="normal">A functional model <a id="_idIndexMarker845"/>provides more flexibility than a <a id="_idIndexMarker846"/>sequential model. For instance, in a functional model, we can easily connect any layer to another layer, whereas, in a sequential model, each layer is in a stack, one above another. A functional model comes in handy when creating complex models, such as directed acyclic graphs, models with multiple input values, multiple output values, and shared layers. Now, we will see how to define a functional model in Keras.</p>
    <p class="normal">The first step is to define the input dimensions:</p>
    <pre class="programlisting code"><code class="hljs-code">input = Input(shape=(<span class="hljs-number">2</span>,))
</code></pre>
    <p class="normal">Now, we'll define our first fully connected layer with <code class="Code-In-Text--PACKT-">10</code> neurons and <code class="Code-In-Text--PACKT-">relu</code> activation, using the <code class="Code-In-Text--PACKT-">Dense</code> class, as shown:</p>
    <pre class="programlisting code"><code class="hljs-code">layer1 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'relu'</span>)
</code></pre>
    <p class="normal">We defined <code class="Code-In-Text--PACKT-">layer1</code>, but where is the input to <code class="Code-In-Text--PACKT-">layer1</code> coming from? We need to specify the input to <code class="Code-In-Text--PACKT-">layer1</code> in a bracket notation at the end, as shown:</p>
    <pre class="programlisting code"><code class="hljs-code">layer1 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'relu'</span>)(input)
</code></pre>
    <p class="normal">We define the next layer, <code class="Code-In-Text--PACKT-">layer2</code>, with <code class="Code-In-Text--PACKT-">13</code> neurons and <code class="Code-In-Text--PACKT-">relu</code> activation. The input to <code class="Code-In-Text--PACKT-">layer2</code> comes from <code class="Code-In-Text--PACKT-">layer1</code>, so that is added in the bracket at the end, as shown in the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">layer2 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'relu'</span>)(layer1)
</code></pre>
    <p class="normal">Now, we can define the <a id="_idIndexMarker847"/>output layer with the <code class="Code-In-Text--PACKT-">sigmoid</code> activation<a id="_idIndexMarker848"/> function. Input to the output layer comes from <code class="Code-In-Text--PACKT-">layer2</code>, so that is added in bracket at the end:</p>
    <pre class="programlisting code"><code class="hljs-code">output = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)(layer2)
</code></pre>
    <p class="normal">After defining all of the layers, we define the model using a <code class="Code-In-Text--PACKT-">Model</code> class, where we need to specify <code class="Code-In-Text--PACKT-">inputs</code> and <code class="Code-In-Text--PACKT-">outputs</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">model = Model(inputs=input, outputs=output)
</code></pre>
    <p class="normal">The complete code for the functional model is shown here:</p>
    <pre class="programlisting code"><code class="hljs-code">input = Input(shape=(<span class="hljs-number">2</span>,))
layer1 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'relu'</span>)(input)
layer2 = Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'relu'</span>)(layer1)
output = Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)(layer2)
model = Model(inputs=input, outputs=output)
</code></pre>
    <h3 id="_idParaDest-233" class="title">Compiling the model</h3>
    <p class="normal">Now that we have <a id="_idIndexMarker849"/>defined the model, the next step is to compile it. In this phase, we set up how the model should learn. We define three parameters when compiling the model:</p>
    <ul>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">optimizer</code> parameter: This defines the optimization algorithm we want to use; for example, the gradient descent.</li>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">loss</code> parameter: This is the objective function that we are trying to minimize; for example, the mean squared error or cross-entropy loss.</li>
      <li class="bullet">The <code class="Code-In-Text--PACKT-">metrics</code> parameter: This is the metric through which we want to assess the model's performance; for example, <code class="Code-In-Text--PACKT-">accuracy</code>. We can also specify more than one metric.</li>
    </ul>
    <p class="normal">Run the following <a id="_idIndexMarker850"/>code to compile the model:</p>
    <pre class="programlisting code"><code class="hljs-code">model.compile(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer=<span class="hljs-string">'sgd'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <h3 id="_idParaDest-234" class="title">Training the model</h3>
    <p class="normal">We defined and also <a id="_idIndexMarker851"/>compiled the model. Now, we will train the model. Training the model can be done using the <code class="Code-In-Text--PACKT-">fit</code> function. We specify our features, <code class="Code-In-Text--PACKT-">x</code>; labels, <code class="Code-In-Text--PACKT-">y</code>; the number of <code class="Code-In-Text--PACKT-">epochs</code> we want to train the model for; and <code class="Code-In-Text--PACKT-">batch_size</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">model.fit(x=data, y=labels, epochs=<span class="hljs-number">100</span>, batch_size=<span class="hljs-number">10</span>)
</code></pre>
    <h3 id="_idParaDest-235" class="title">Evaluating the model</h3>
    <p class="normal">After training the<a id="_idIndexMarker852"/> model, we will evaluate the model on the test set:</p>
    <pre class="programlisting code"><code class="hljs-code">model.evaluate(x=data_test,y=labels_test)
</code></pre>
    <p class="normal">We can also evaluate the model on the same train set, and that will help us to understand the training accuracy:</p>
    <pre class="programlisting code"><code class="hljs-code">model.evaluate(x=data,y=labels)
</code></pre>
    <p class="normal">That's it. Let's see how to use TensorFlow for the MNIST digit classification task in the next section.</p>
    <h2 id="_idParaDest-236" class="title">MNIST digit classification using TensorFlow 2.0</h2>
    <p class="normal">Now, we will see <a id="_idIndexMarker853"/>how we can perform MNIST <a id="_idIndexMarker854"/>handwritten digit classification, using TensorFlow 2.0. It requires only a few lines of code compared to TensorFlow 1.x. As we have learned, TensorFlow 2.0 uses Keras as its high-level API; we just need to add <code class="Code-In-Text--PACKT-">tf.keras</code> to the Keras code.</p>
    <p class="normal">Let's start by loading the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">mnist = tf.keras.datasets.mnist
</code></pre>
    <p class="normal">Create a training set and a test set with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">(x_train,y_train), (x_test, y_test) = mnist.load_data()
</code></pre>
    <p class="normal">Normalize the train and test sets by dividing the values of <code class="Code-In-Text--PACKT-">x</code> by the maximum value of <code class="Code-In-Text--PACKT-">x</code>; that is, <code class="Code-In-Text--PACKT-">255.0</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">x_train, x_test = tf.cast(x_train/<span class="hljs-number">255.0</span>, tf.float32), tf.cast(x_test/<span class="hljs-number">255.0</span>, tf.float32)
y_train, y_test = tf.cast(y_train,tf.int64),tf.cast(y_test,tf.int64)
</code></pre>
    <p class="normal">Define the sequential model as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">model = tf.keras.models.Sequential()
</code></pre>
    <p class="normal">Now, let's add layers to the model. We use a three-layer network with the ReLU function in the hidden layer and softmax in the final layer:</p>
    <pre class="programlisting code"><code class="hljs-code">model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(<span class="hljs-number">256</span>, activation=<span class="hljs-string">"relu"</span>))
model.add(tf.keras.layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">"relu"</span>))
model.add(tf.keras.layers.Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">"softmax"</span>))
</code></pre>
    <p class="normal">Compile <a id="_idIndexMarker855"/>the model by running the following line<a id="_idIndexMarker856"/> of code:</p>
    <pre class="programlisting code"><code class="hljs-code">model.compile(optimizer=<span class="hljs-string">'sgd'</span>, loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">Train the model:</p>
    <pre class="programlisting code"><code class="hljs-code">model.fit(x_train, y_train, batch_size=<span class="hljs-number">32</span>, epochs=<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">Evaluate the model:</p>
    <pre class="programlisting code"><code class="hljs-code">model.evaluate(x_test, y_test)
</code></pre>
    <p class="normal">That's it! Writing code with the Keras API is that simple.</p>
    <h1 id="_idParaDest-237" class="title">Summary</h1>
    <p class="normal">We started off this chapter by understanding TensorFlow and how it uses computational graphs. We learned that computation in TensorFlow is represented by a computational graph, which consists of several nodes and edges, where nodes are mathematical operations, such as addition and multiplication, and edges are tensors. </p>
    <p class="normal">Next, we learned that variables are containers used to store values, and they are used as input to several other operations in a computational graph. We also learned that placeholders are like variables, where we only define the type and dimension but do not assign the values, and the values for the placeholders are fed at runtime. Moving forward, we learned about TensorBoard, which is TensorFlow's visualization tool and can be used to visualize a computational graph. We also explored eager execution, which is more Pythonic and allows rapid prototyping.</p>
    <p class="normal">We understood that, unlike graph mode, where we need to construct a graph every time to perform any operation, eager execution follows the imperative programming paradigm, where any operation can be performed immediately, without having to create a graph, just like we do in Python.</p>
    <p class="normal">In the next chapter, we begin our <strong class="keyword">deep reinforcement learning</strong> (<strong class="keyword">DRL</strong>) journey by understanding one of the popular DRL algorithms, called the<strong class="keyword"> Deep Q Network</strong> (<strong class="keyword">DQN</strong>).</p>
    <h1 id="_idParaDest-238" class="title">Questions</h1>
    <p class="normal">Let's put our knowledge of TensorFlow to the test by answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">What is a TensorFlow session?</li>
      <li class="numbered">Define a placeholder.</li>
      <li class="numbered">What is TensorBoard?</li>
      <li class="numbered">Why is eager execution mode useful?</li>
      <li class="numbered">What are all the steps involved in building a model using Keras? </li>
      <li class="numbered">How does Keras's functional model differ from its sequential model?</li>
    </ol>
    <h1 id="_idParaDest-239" class="title">Further reading</h1>
    <p class="normal">You can learn more about TensorFlow by checking its official documentation at <a href="https://www.tensorflow.org/tutorials"><span class="url">https://www.tensorflow.org/tutorials</span></a>.</p>
  </div>
</body></html>