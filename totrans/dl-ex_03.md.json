["```\n# reading the train and test sets using pandas\ntrain_data = pd.read_csv('data/train.csv', header=0)\ntest_data = pd.read_csv('data/test.csv', header=0)\n\n# concatenate the train and test set together for doing the overall feature engineering stuff\ndf_titanic_data = pd.concat([train_data, test_data])\n\n# removing duplicate indices due to coming the train and test set by re-indexing the data\ndf_titanic_data.reset_index(inplace=True)\n\n# removing the index column the reset_index() function generates\ndf_titanic_data.drop('index', axis=1, inplace=True)\n\n# index the columns to be 1-based index\ndf_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)\n```", "```\n# replacing the missing value in cabin variable \"U0\"\ndf_titanic_data['Cabin'][df_titanic_data.Cabin.isnull()] = 'U0'\n```", "```\n# handling the missing values by replacing it with the median fare\ndf_titanic_data['Fare'][np.isnan(df_titanic_data['Fare'])] = df_titanic_data['Fare'].median()\n```", "```\n# replacing the missing values with the most common value in the variable\ndf_titanic_data.Embarked[df_titanic_data.Embarked.isnull()] = df_titanic_data.Embarked.dropna().mode().values\n```", "```\nAge feature:\n```", "```\n# Define a helper function that can use RandomForestClassifier for handling the missing values of the age variable\ndef set_missing_ages():\n    global df_titanic_data\n\n    age_data = df_titanic_data[\n        ['Age', 'Embarked', 'Fare', 'Parch', 'SibSp', 'Title_id', 'Pclass', 'Names', 'CabinLetter']]\n    input_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 1::]\n    target_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 0]\n\n    # Creating an object from the random forest regression function of sklearn<use the documentation for more details>\n    regressor = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n\n    # building the model based on the input values and target values above\n    regressor.fit(input_values_RF, target_values_RF)\n\n    # using the trained model to predict the missing values\n    predicted_ages = regressor.predict(age_data.loc[(df_titanic_data.Age.isnull())].values[:, 1::])\n\n```", "```\n    # Filling the predicted ages in the original titanic dataframe\n    age_data.loc[(age_data.Age.isnull()), 'Age'] = predicted_ages\n```", "```\n# constructing binary features\ndef process_embarked():\n    global df_titanic_data\n\n    # replacing the missing values with the most common value in the variable\n    df_titanic_data.Embarked[df.Embarked.isnull()] = df_titanic_data.Embarked.dropna().mode().values\n\n    # converting the values into numbers\n    df_titanic_data['Embarked'] = pd.factorize(df_titanic_data['Embarked'])[0]\n\n    # binarizing the constructed features\n    if keep_binary:\n        df_titanic_data = pd.concat([df_titanic_data, pd.get_dummies(df_titanic_data['Embarked']).rename(\n            columns=lambda x: 'Embarked_' + str(x))], axis=1)\n```", "```\n# the cabin number is a sequence of of alphanumerical digits, so we are going to create some features\n# from the alphabetical part of it\ndf_titanic_data['CabinLetter'] = df_titanic_data['Cabin'].map(lambda l: get_cabin_letter(l))\ndf_titanic_data['CabinLetter'] = pd.factorize(df_titanic_data['CabinLetter'])[0]\n```", "```\ndef get_cabin_letter(cabin_value):\n    # searching for the letters in the cabin alphanumerical value\n    letter_match = re.compile(\"([a-zA-Z]+)\").search(cabin_value)\n\n    if letter_match:\n        return letter_match.group()\n    else:\n        return 'U'\n```", "```\n# scale by subtracting the mean from each value\n```", "```\nscaler_processing = preprocessing.StandardScaler()\n```", "```\ndf_titanic_data['Age_scaled'] = scaler_processing.fit_transform(df_titanic_data['Age'])\n```", "```\n# Binarizing the features by binning them into quantiles\ndf_titanic_data['Fare_bin'] = pd.qcut(df_titanic_data['Fare'], 4)\n\nif keep_binary:\n    df_titanic_data = pd.concat(\n        [df_titanic_data, pd.get_dummies(df_titanic_data['Fare_bin']).rename(columns=lambda x: 'Fare_' + str(x))],\n        axis=1)\n```", "```\n# getting the different names in the names variable\ndf_titanic_data['Names'] = df_titanic_data['Name'].map(lambda y: len(re.split(' ', y)))\n```", "```\n# Getting titles for each person\ndf_titanic_data['Title'] = df_titanic_data['Name'].map(lambda y: re.compile(\", (.*?)\\.\").findall(y)[0])\n\n# handling the low occurring titles\ndf_titanic_data['Title'][df_titanic_data.Title == 'Jonkheer'] = 'Master'\ndf_titanic_data['Title'][df_titanic_data.Title.isin(['Ms', 'Mlle'])] = 'Miss'\ndf_titanic_data['Title'][df_titanic_data.Title == 'Mme'] = 'Mrs'\ndf_titanic_data['Title'][df_titanic_data.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\ndf_titanic_data['Title'][df_titanic_data.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\n\n# binarizing all the features\nif keep_binary:\n    df_titanic_data = pd.concat(\n        [df_titanic_data, pd.get_dummies(df_titanic_data['Title']).rename(columns=lambda x: 'Title_' + str(x))],\n        axis=1)\n```", "```\n# repllacing the missing value in cabin variable \"U0\"\ndf_titanic_data['Cabin'][df_titanic_data.Cabin.isnull()] = 'U0'\n\n# the cabin number is a sequence of of alphanumerical digits, so we are going to create some features\n# from the alphabetical part of it\ndf_titanic_data['CabinLetter'] = df_titanic_data['Cabin'].map(lambda l: get_cabin_letter(l))\ndf_titanic_data['CabinLetter'] = pd.factorize(df_titanic_data['CabinLetter'])[0]\n\n# binarizing the cabin letters features\nif keep_binary:\n    cletters = pd.get_dummies(df_titanic_data['CabinLetter']).rename(columns=lambda x: 'CabinLetter_' + str(x))\n    df_titanic_data = pd.concat([df_titanic_data, cletters], axis=1)\n\n# creating features from the numerical side of the cabin\ndf_titanic_data['CabinNumber'] = df_titanic_data['Cabin'].map(lambda x: get_cabin_num(x)).astype(int) + 1\n\n```", "```\n# Helper function for constructing features from the ticket variable\ndef process_ticket():\n    global df_titanic_data\n\n    df_titanic_data['TicketPrefix'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_prefix(y.upper()))\n    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('[\\.?\\/?]', '', y))\n    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('STON', 'SOTON', y))\n\n    df_titanic_data['TicketPrefixId'] = pd.factorize(df_titanic_data['TicketPrefix'])[0]\n\n    # binarzing features for each ticket layer\n    if keep_binary:\n        prefixes = pd.get_dummies(df_titanic_data['TicketPrefix']).rename(columns=lambda y: 'TicketPrefix_' + str(y))\n        df_titanic_data = pd.concat([df_titanic_data, prefixes], axis=1)\n\n    df_titanic_data.drop(['TicketPrefix'], axis=1, inplace=True)\n\n    df_titanic_data['TicketNumber'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_num(y))\n    df_titanic_data['TicketNumberDigits'] = df_titanic_data['TicketNumber'].map(lambda y: len(y)).astype(np.int)\n    df_titanic_data['TicketNumberStart'] = df_titanic_data['TicketNumber'].map(lambda y: y[0:1]).astype(np.int)\n\n    df_titanic_data['TicketNumber'] = df_titanic_data.TicketNumber.astype(np.int)\n\n    if keep_scaled:\n        scaler_processing = preprocessing.StandardScaler()\n        df_titanic_data['TicketNumber_scaled'] = scaler_processing.fit_transform(\n            df_titanic_data.TicketNumber.reshape(-1, 1))\n\ndef get_ticket_prefix(ticket_value):\n    # searching for the letters in the ticket alphanumerical value\n    match_letter = re.compile(\"([a-zA-Z\\.\\/]+)\").search(ticket_value)\n    if match_letter:\n        return match_letter.group()\n    else:\n        return 'U'\n\ndef get_ticket_num(ticket_value):\n    # searching for the numbers in the ticket alphanumerical value\n    match_number = re.compile(\"([\\d]+$)\").search(ticket_value)\n    if match_number:\n        return match_number.group()\n    else:\n        return '0'\n\n```", "```\n# Constructing features manually based on  the interaction between the individual features\nnumeric_features = df_titanic_data.loc[:,\n                   ['Age_scaled', 'Fare_scaled', 'Pclass_scaled', 'Parch_scaled', 'SibSp_scaled',\n                    'Names_scaled', 'CabinNumber_scaled', 'Age_bin_id_scaled', 'Fare_bin_id_scaled']]\nprint(\"\\nUsing only numeric features for automated feature generation:\\n\", numeric_features.head(10))\n\nnew_fields_count = 0\nfor i in range(0, numeric_features.columns.size - 1):\n    for j in range(0, numeric_features.columns.size - 1):\n        if i <= j:\n            name = str(numeric_features.columns.values[i]) + \"*\" + str(numeric_features.columns.values[j])\n            df_titanic_data = pd.concat(\n                [df_titanic_data, pd.Series(numeric_features.iloc[:, i] * numeric_features.iloc[:, j], name=name)],\n                axis=1)\n            new_fields_count += 1\n        if i < j:\n            name = str(numeric_features.columns.values[i]) + \"+\" + str(numeric_features.columns.values[j])\n            df_titanic_data = pd.concat(\n                [df_titanic_data, pd.Series(numeric_features.iloc[:, i] + numeric_features.iloc[:, j], name=name)],\n                axis=1)\n            new_fields_count += 1\n        if not i == j:\n            name = str(numeric_features.columns.values[i]) + \"/\" + str(numeric_features.columns.values[j])\n            df_titanic_data = pd.concat(\n                [df_titanic_data, pd.Series(numeric_features.iloc[:, i] / numeric_features.iloc[:, j], name=name)],\n                axis=1)\n            name = str(numeric_features.columns.values[i]) + \"-\" + str(numeric_features.columns.values[j])\n            df_titanic_data = pd.concat(\n                [df_titanic_data, pd.Series(numeric_features.iloc[:, i] - numeric_features.iloc[:, j], name=name)],\n                axis=1)\n            new_fields_count += 2\n\nprint(\"\\n\", new_fields_count, \"new features constructed\")\n```", "```\n# using Spearman correlation method to remove the feature that have high correlation\n\n# calculating the correlation matrix\ndf_titanic_data_cor = df_titanic_data.drop(['Survived', 'PassengerId'], axis=1).corr(method='spearman')\n\n# creating a mask that will ignore correlated ones\nmask_ignore = np.ones(df_titanic_data_cor.columns.size) - np.eye(df_titanic_data_cor.columns.size)\ndf_titanic_data_cor = mask_ignore * df_titanic_data_cor\n\nfeatures_to_drop = []\n\n# dropping the correclated features\nfor column in df_titanic_data_cor.columns.values:\n\n    # check if we already decided to drop this variable\n    if np.in1d([column], features_to_drop):\n        continue\n\n    # finding highly correlacted variables\n    corr_vars = df_titanic_data_cor[abs(df_titanic_data_cor[column]) > 0.98].index\n    features_to_drop = np.union1d(features_to_drop, corr_vars)\n\nprint(\"\\nWe are going to drop\", features_to_drop.shape[0], \" which are highly correlated features...\\n\")\ndf_titanic_data.drop(features_to_drop, axis=1, inplace=True)\n```", "```\nIf 0.5*red + 0.3*green + 0.2*blue > 0.6 : return cat;\n```", "```\nelse return dog;\n```", "```\n# minimum variance percentage that should be covered by the reduced number of variables\nvariance_percentage = .99\n\n# creating PCA object\npca_object = PCA(n_components=variance_percentage)\n\n# trasforming the features\ninput_values_transformed = pca_object.fit_transform(input_values, target_values)\n\n# creating a datafram for the transformed variables from PCA\npca_df = pd.DataFrame(input_values_transformed)\n\nprint(pca_df.shape[1], \" reduced components which describe \", str(variance_percentage)[1:], \"% of the variance\")\n```", "```\nimport re\nimport numpy as np\nimport pandas as pd\nimport random as rd\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.decomposition import PCA\n\n# Print options\nnp.set_printoptions(precision=4, threshold=10000, linewidth=160, edgeitems=999, suppress=True)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', 160)\npd.set_option('expand_frame_repr', False)\npd.set_option('precision', 4)\n\n# constructing binary features\ndef process_embarked():\n    global df_titanic_data\n\n    # replacing the missing values with the most common value in the variable\n    df_titanic_data.Embarked[df.Embarked.isnull()] = df_titanic_data.Embarked.dropna().mode().values\n\n    # converting the values into numbers\n    df_titanic_data['Embarked'] = pd.factorize(df_titanic_data['Embarked'])[0]\n\n    # binarizing the constructed features\n    if keep_binary:\n        df_titanic_data = pd.concat([df_titanic_data, pd.get_dummies(df_titanic_data['Embarked']).rename(\n            columns=lambda x: 'Embarked_' + str(x))], axis=1)\n\n# Define a helper function that can use RandomForestClassifier for handling the missing values of the age variable\ndef set_missing_ages():\n    global df_titanic_data\n\n    age_data = df_titanic_data[\n        ['Age', 'Embarked', 'Fare', 'Parch', 'SibSp', 'Title_id', 'Pclass', 'Names', 'CabinLetter']]\n    input_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 1::]\n    target_values_RF = age_data.loc[(df_titanic_data.Age.notnull())].values[:, 0]\n\n    # Creating an object from the random forest regression function of sklearn<use the documentation for more details>\n    regressor = RandomForestRegressor(n_estimators=2000, n_jobs=-1)\n\n    # building the model based on the input values and target values above\n    regressor.fit(input_values_RF, target_values_RF)\n\n    # using the trained model to predict the missing values\n    predicted_ages = regressor.predict(age_data.loc[(df_titanic_data.Age.isnull())].values[:, 1::])\n\n    # Filling the predicted ages in the original titanic dataframe\n    age_data.loc[(age_data.Age.isnull()), 'Age'] = predicted_ages\n\n# Helper function for constructing features from the age variable\ndef process_age():\n    global df_titanic_data\n\n    # calling the set_missing_ages helper function to use random forest regression for predicting missing values of age\n    set_missing_ages()\n\n    # # scale the age variable by centering it around the mean with a unit variance\n    # if keep_scaled:\n    # scaler_preprocessing = preprocessing.StandardScaler()\n    # df_titanic_data['Age_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Age.reshape(-1, 1))\n\n    # construct a feature for children\n    df_titanic_data['isChild'] = np.where(df_titanic_data.Age < 13, 1, 0)\n\n    # bin into quartiles and create binary features\n    df_titanic_data['Age_bin'] = pd.qcut(df_titanic_data['Age'], 4)\n\n    if keep_binary:\n        df_titanic_data = pd.concat(\n            [df_titanic_data, pd.get_dummies(df_titanic_data['Age_bin']).rename(columns=lambda y: 'Age_' + str(y))],\n            axis=1)\n\n    if keep_bins:\n        df_titanic_data['Age_bin_id'] = pd.factorize(df_titanic_data['Age_bin'])[0] + 1\n\n    if keep_bins and keep_scaled:\n        scaler_processing = preprocessing.StandardScaler()\n        df_titanic_data['Age_bin_id_scaled'] = scaler_processing.fit_transform(\n            df_titanic_data.Age_bin_id.reshape(-1, 1))\n\n    if not keep_strings:\n        df_titanic_data.drop('Age_bin', axis=1, inplace=True)\n\n# Helper function for constructing features from the passengers/crew names\ndef process_name():\n    global df_titanic_data\n\n    # getting the different names in the names variable\n    df_titanic_data['Names'] = df_titanic_data['Name'].map(lambda y: len(re.split(' ', y)))\n\n    # Getting titles for each person\n    df_titanic_data['Title'] = df_titanic_data['Name'].map(lambda y: re.compile(\", (.*?)\\.\").findall(y)[0])\n\n    # handling the low occurring titles\n    df_titanic_data['Title'][df_titanic_data.Title == 'Jonkheer'] = 'Master'\n    df_titanic_data['Title'][df_titanic_data.Title.isin(['Ms', 'Mlle'])] = 'Miss'\n    df_titanic_data['Title'][df_titanic_data.Title == 'Mme'] = 'Mrs'\n    df_titanic_data['Title'][df_titanic_data.Title.isin(['Capt', 'Don', 'Major', 'Col', 'Sir'])] = 'Sir'\n    df_titanic_data['Title'][df_titanic_data.Title.isin(['Dona', 'Lady', 'the Countess'])] = 'Lady'\n\n    # binarizing all the features\n    if keep_binary:\n        df_titanic_data = pd.concat(\n            [df_titanic_data, pd.get_dummies(df_titanic_data['Title']).rename(columns=lambda x: 'Title_' + str(x))],\n            axis=1)\n\n    # scaling\n    if keep_scaled:\n        scaler_preprocessing = preprocessing.StandardScaler()\n        df_titanic_data['Names_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Names.reshape(-1, 1))\n\n    # binning\n    if keep_bins:\n        df_titanic_data['Title_id'] = pd.factorize(df_titanic_data['Title'])[0] + 1\n\n    if keep_bins and keep_scaled:\n        scaler = preprocessing.StandardScaler()\n        df_titanic_data['Title_id_scaled'] = scaler.fit_transform(df_titanic_data.Title_id.reshape(-1, 1))\n\n# Generate features from the cabin input variable\ndef process_cabin():\n    # refering to the global variable that contains the titanic examples\n    global df_titanic_data\n\n    # repllacing the missing value in cabin variable \"U0\"\n    df_titanic_data['Cabin'][df_titanic_data.Cabin.isnull()] = 'U0'\n\n    # the cabin number is a sequence of of alphanumerical digits, so we are going to create some features\n    # from the alphabetical part of it\n    df_titanic_data['CabinLetter'] = df_titanic_data['Cabin'].map(lambda l: get_cabin_letter(l))\n    df_titanic_data['CabinLetter'] = pd.factorize(df_titanic_data['CabinLetter'])[0]\n\n    # binarizing the cabin letters features\n    if keep_binary:\n        cletters = pd.get_dummies(df_titanic_data['CabinLetter']).rename(columns=lambda x: 'CabinLetter_' + str(x))\n        df_titanic_data = pd.concat([df_titanic_data, cletters], axis=1)\n\n    # creating features from the numerical side of the cabin\n    df_titanic_data['CabinNumber'] = df_titanic_data['Cabin'].map(lambda x: get_cabin_num(x)).astype(int) + 1\n\n    # scaling the feature\n    if keep_scaled:\n        scaler_processing = preprocessing.StandardScaler() # handling the missing values by replacing it with the median feare\n    df_titanic_data['Fare'][np.isnan(df_titanic_data['Fare'])] = df_titanic_data['Fare'].median()\n    df_titanic_data['CabinNumber_scaled'] = scaler_processing.fit_transform(df_titanic_data.CabinNumber.reshape(-1, 1))\n\ndef get_cabin_letter(cabin_value):\n    # searching for the letters in the cabin alphanumerical value\n    letter_match = re.compile(\"([a-zA-Z]+)\").search(cabin_value)\n\n    if letter_match:\n        return letter_match.group()\n    else:\n        return 'U'\n\ndef get_cabin_num(cabin_value):\n    # searching for the numbers in the cabin alphanumerical value\n    number_match = re.compile(\"([0-9]+)\").search(cabin_value)\n\n    if number_match:\n        return number_match.group()\n    else:\n        return 0\n\n# helper function for constructing features from the ticket fare variable\ndef process_fare():\n    global df_titanic_data\n\n    # handling the missing values by replacing it with the median feare\n    df_titanic_data['Fare'][np.isnan(df_titanic_data['Fare'])] = df_titanic_data['Fare'].median()\n\n    # zeros in the fare will cause some division problems so we are going to set them to 1/10th of the lowest fare\n    df_titanic_data['Fare'][np.where(df_titanic_data['Fare'] == 0)[0]] = df_titanic_data['Fare'][\n                                                                             df_titanic_data['Fare'].nonzero()[\n                                                                                 0]].min() / 10\n\n    # Binarizing the features by binning them into quantiles\n    df_titanic_data['Fare_bin'] = pd.qcut(df_titanic_data['Fare'], 4)\n\n    if keep_binary:\n        df_titanic_data = pd.concat(\n            [df_titanic_data, pd.get_dummies(df_titanic_data['Fare_bin']).rename(columns=lambda x: 'Fare_' + str(x))],\n            axis=1)\n\n    # binning\n    if keep_bins:\n        df_titanic_data['Fare_bin_id'] = pd.factorize(df_titanic_data['Fare_bin'])[0] + 1\n\n    # scaling the value\n    if keep_scaled:\n        scaler_processing = preprocessing.StandardScaler()\n        df_titanic_data['Fare_scaled'] = scaler_processing.fit_transform(df_titanic_data.Fare.reshape(-1, 1))\n\n    if keep_bins and keep_scaled:\n        scaler_processing = preprocessing.StandardScaler()\n        df_titanic_data['Fare_bin_id_scaled'] = scaler_processing.fit_transform(\n            df_titanic_data.Fare_bin_id.reshape(-1, 1))\n\n    if not keep_strings:\n        df_titanic_data.drop('Fare_bin', axis=1, inplace=True)\n\n# Helper function for constructing features from the ticket variable\ndef process_ticket():\n    global df_titanic_data\n\n    df_titanic_data['TicketPrefix'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_prefix(y.upper()))\n    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('[\\.?\\/?]', '', y))\n    df_titanic_data['TicketPrefix'] = df_titanic_data['TicketPrefix'].map(lambda y: re.sub('STON', 'SOTON', y))\n\n    df_titanic_data['TicketPrefixId'] = pd.factorize(df_titanic_data['TicketPrefix'])[0]\n\n    # binarzing features for each ticket layer\n    if keep_binary:\n        prefixes = pd.get_dummies(df_titanic_data['TicketPrefix']).rename(columns=lambda y: 'TicketPrefix_' + str(y))\n        df_titanic_data = pd.concat([df_titanic_data, prefixes], axis=1)\n\n    df_titanic_data.drop(['TicketPrefix'], axis=1, inplace=True)\n\n    df_titanic_data['TicketNumber'] = df_titanic_data['Ticket'].map(lambda y: get_ticket_num(y))\n    df_titanic_data['TicketNumberDigits'] = df_titanic_data['TicketNumber'].map(lambda y: len(y)).astype(np.int)\n    df_titanic_data['TicketNumberStart'] = df_titanic_data['TicketNumber'].map(lambda y: y[0:1]).astype(np.int)\n\n    df_titanic_data['TicketNumber'] = df_titanic_data.TicketNumber.astype(np.int)\n\n    if keep_scaled:\n        scaler_processing = preprocessing.StandardScaler()\n        df_titanic_data['TicketNumber_scaled'] = scaler_processing.fit_transform(\n            df_titanic_data.TicketNumber.reshape(-1, 1))\n\ndef get_ticket_prefix(ticket_value):\n    # searching for the letters in the ticket alphanumerical value\n    match_letter = re.compile(\"([a-zA-Z\\.\\/]+)\").search(ticket_value)\n    if match_letter:\n        return match_letter.group()\n    else:\n        return 'U'\n\ndef get_ticket_num(ticket_value):\n    # searching for the numbers in the ticket alphanumerical value\n    match_number = re.compile(\"([\\d]+$)\").search(ticket_value)\n    if match_number:\n        return match_number.group()\n    else:\n        return '0'\n\n# constructing features from the passenger class variable\ndef process_PClass():\n    global df_titanic_data\n\n    # using the most frequent value(mode) to replace the messing value\n    df_titanic_data.Pclass[df_titanic_data.Pclass.isnull()] = df_titanic_data.Pclass.dropna().mode().values\n\n    # binarizing the features\n    if keep_binary:\n        df_titanic_data = pd.concat(\n            [df_titanic_data, pd.get_dummies(df_titanic_data['Pclass']).rename(columns=lambda y: 'Pclass_' + str(y))],\n            axis=1)\n\n    if keep_scaled:\n        scaler_preprocessing = preprocessing.StandardScaler()\n        df_titanic_data['Pclass_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Pclass.reshape(-1, 1))\n\n# constructing features based on the family variables subh as SibSp and Parch\ndef process_family():\n    global df_titanic_data\n\n    # ensuring that there's no zeros to use interaction variables\n    df_titanic_data['SibSp'] = df_titanic_data['SibSp'] + 1\n    df_titanic_data['Parch'] = df_titanic_data['Parch'] + 1\n\n    # scaling\n    if keep_scaled:\n        scaler_preprocessing = preprocessing.StandardScaler()\n        df_titanic_data['SibSp_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.SibSp.reshape(-1, 1))\n        df_titanic_data['Parch_scaled'] = scaler_preprocessing.fit_transform(df_titanic_data.Parch.reshape(-1, 1))\n\n    # binarizing all the features\n    if keep_binary:\n        sibsps_var = pd.get_dummies(df_titanic_data['SibSp']).rename(columns=lambda y: 'SibSp_' + str(y))\n        parchs_var = pd.get_dummies(df_titanic_data['Parch']).rename(columns=lambda y: 'Parch_' + str(y))\n        df_titanic_data = pd.concat([df_titanic_data, sibsps_var, parchs_var], axis=1)\n\n# binarzing the sex variable\ndef process_sex():\n    global df_titanic_data\n    df_titanic_data['Gender'] = np.where(df_titanic_data['Sex'] == 'male', 1, 0)\n\n# dropping raw original\ndef process_drops():\n    global df_titanic_data\n    drops = ['Name', 'Names', 'Title', 'Sex', 'SibSp', 'Parch', 'Pclass', 'Embarked', \\\n             'Cabin', 'CabinLetter', 'CabinNumber', 'Age', 'Fare', 'Ticket', 'TicketNumber']\n    string_drops = ['Title', 'Name', 'Cabin', 'Ticket', 'Sex', 'Ticket', 'TicketNumber']\n    if not keep_raw:\n        df_titanic_data.drop(drops, axis=1, inplace=True)\n    elif not keep_strings:\n        df_titanic_data.drop(string_drops, axis=1, inplace=True)\n\n# handling all the feature engineering tasks\ndef get_titanic_dataset(binary=False, bins=False, scaled=False, strings=False, raw=True, pca=False, balanced=False):\n    global keep_binary, keep_bins, keep_scaled, keep_raw, keep_strings, df_titanic_data\n    keep_binary = binary\n    keep_bins = bins\n    keep_scaled = scaled\n    keep_raw = raw\n    keep_strings = strings\n\n    # reading the train and test sets using pandas\n    train_data = pd.read_csv('data/train.csv', header=0)\n    test_data = pd.read_csv('data/test.csv', header=0)\n\n    # concatenate the train and test set together for doing the overall feature engineering stuff\n    df_titanic_data = pd.concat([train_data, test_data])\n\n    # removing duplicate indices due to coming the train and test set by re-indexing the data\n    df_titanic_data.reset_index(inplace=True)\n\n    # removing the index column the reset_index() function generates\n    df_titanic_data.drop('index', axis=1, inplace=True)\n\n    # index the columns to be 1-based index\n    df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)\n\n    # processing the titanic raw variables using the helper functions that we defined above\n    process_cabin()\n    process_ticket()\n    process_name()\n    process_fare()\n    process_embarked()\n    process_family()\n    process_sex()\n    process_PClass()\n    process_age()\n    process_drops()\n\n    # move the survived column to be the first\n    columns_list = list(df_titanic_data.columns.values)\n    columns_list.remove('Survived')\n    new_col_list = list(['Survived'])\n    new_col_list.extend(columns_list)\n    df_titanic_data = df_titanic_data.reindex(columns=new_col_list)\n\n    print(\"Starting with\", df_titanic_data.columns.size,\n          \"manually constructing features based on the interaction between them...\\n\", df_titanic_data.columns.values)\n\n    # Constructing features manually based on the interaction between the individual features\n    numeric_features = df_titanic_data.loc[:,\n                       ['Age_scaled', 'Fare_scaled', 'Pclass_scaled', 'Parch_scaled', 'SibSp_scaled',\n                        'Names_scaled', 'CabinNumber_scaled', 'Age_bin_id_scaled', 'Fare_bin_id_scaled']]\n    print(\"\\nUsing only numeric features for automated feature generation:\\n\", numeric_features.head(10))\n\n    new_fields_count = 0\n    for i in range(0, numeric_features.columns.size - 1):\n        for j in range(0, numeric_features.columns.size - 1):\n            if i <= j:\n                name = str(numeric_features.columns.values[i]) + \"*\" + str(numeric_features.columns.values[j])\n                df_titanic_data = pd.concat(\n                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] * numeric_features.iloc[:, j], name=name)],\n                    axis=1)\n                new_fields_count += 1\n            if i < j:\n                name = str(numeric_features.columns.values[i]) + \"+\" + str(numeric_features.columns.values[j])\n                df_titanic_data = pd.concat(\n                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] + numeric_features.iloc[:, j], name=name)],\n                    axis=1)\n                new_fields_count += 1\n            if not i == j:\n                name = str(numeric_features.columns.values[i]) + \"/\" + str(numeric_features.columns.values[j])\n                df_titanic_data = pd.concat(\n                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] / numeric_features.iloc[:, j], name=name)],\n                    axis=1)\n                name = str(numeric_features.columns.values[i]) + \"-\" + str(numeric_features.columns.values[j])\n                df_titanic_data = pd.concat(\n                    [df_titanic_data, pd.Series(numeric_features.iloc[:, i] - numeric_features.iloc[:, j], name=name)],\n                    axis=1)\n                new_fields_count += 2\n\n    print(\"\\n\", new_fields_count, \"new features constructed\")\n\n    # using Spearman correlation method to remove the feature that have high correlation\n\n    # calculating the correlation matrix\n    df_titanic_data_cor = df_titanic_data.drop(['Survived', 'PassengerId'], axis=1).corr(method='spearman')\n\n    # creating a mask that will ignore correlated ones\n    mask_ignore = np.ones(df_titanic_data_cor.columns.size) - np.eye(df_titanic_data_cor.columns.size)\n    df_titanic_data_cor = mask_ignore * df_titanic_data_cor\n\n    features_to_drop = []\n\n    # dropping the correclated features\n    for column in df_titanic_data_cor.columns.values:\n\n        # check if we already decided to drop this variable\n        if np.in1d([column], features_to_drop):\n            continue\n\n        # finding highly correlacted variables\n        corr_vars = df_titanic_data_cor[abs(df_titanic_data_cor[column]) > 0.98].index\n        features_to_drop = np.union1d(features_to_drop, corr_vars)\n\n    print(\"\\nWe are going to drop\", features_to_drop.shape[0], \" which are highly correlated features...\\n\")\n    df_titanic_data.drop(features_to_drop, axis=1, inplace=True)\n\n    # splitting the dataset to train and test and do PCA\n    train_data = df_titanic_data[:train_data.shape[0]]\n    test_data = df_titanic_data[test_data.shape[0]:]\n\n    if pca:\n        print(\"reducing number of variables...\")\n        train_data, test_data = reduce(train_data, test_data)\n    else:\n        # drop the empty 'Survived' column for the test set that was created during set concatenation\n        test_data.drop('Survived', axis=1, inplace=True)\n\n    print(\"\\n\", train_data.columns.size, \"initial features generated...\\n\") # , input_df.columns.values\n\n    return train_data, test_data\n\n# reducing the dimensionality for the training and testing set\ndef reduce(train_data, test_data):\n    # join the full data together\n    df_titanic_data = pd.concat([train_data, test_data])\n    df_titanic_data.reset_index(inplace=True)\n    df_titanic_data.drop('index', axis=1, inplace=True)\n    df_titanic_data = df_titanic_data.reindex_axis(train_data.columns, axis=1)\n\n    # converting the survived column to series\n    survived_series = pd.Series(df_titanic_data['Survived'], name='Survived')\n\n    print(df_titanic_data.head())\n\n    # getting the input and target values\n    input_values = df_titanic_data.values[:, 1::]\n    target_values = df_titanic_data.values[:, 0]\n\n    print(input_values[0:10])\n\n    # minimum variance percentage that should be covered by the reduced number of variables\n    variance_percentage = .99\n\n    # creating PCA object\n    pca_object = PCA(n_components=variance_percentage)\n\n    # trasforming the features\n    input_values_transformed = pca_object.fit_transform(input_values, target_values)\n\n    # creating a datafram for the transformed variables from PCA\n    pca_df = pd.DataFrame(input_values_transformed)\n\n    print(pca_df.shape[1], \" reduced components which describe \", str(variance_percentage)[1:], \"% of the variance\")\n\n    # constructing a new dataframe that contains the newly reduced vars of PCA\n    df_titanic_data = pd.concat([survived_series, pca_df], axis=1)\n\n    # split into separate input and test sets again\n    train_data = df_titanic_data[:train_data.shape[0]]\n    test_data = df_titanic_data[test_data.shape[0]:]\n    test_data.reset_index(inplace=True)\n    test_data.drop('index', axis=1, inplace=True)\n    test_data.drop('Survived', axis=1, inplace=True)\n\n    return train_data, test_data\n\n# Calling the helper functions\nif __name__ == '__main__':\n    train, test = get_titanic_dataset(bins=True, scaled=True, binary=True)\n    initial_drops = ['PassengerId']\n    train.drop(initial_drops, axis=1, inplace=True)\n    test.drop(initial_drops, axis=1, inplace=True)\n\n    train, test = reduce(train, test)\n\n    print(train.columns.values)\n```"]