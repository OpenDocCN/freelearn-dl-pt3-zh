- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Other Useful Deep Learning Libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow from Google is not the only framework available for deep learning
    tasks. There is a good range of libraries and frameworks available, each with
    its special features, capabilities, and use cases. In this chapter, we will explore
    some of the popular deep learning libraries and compare their features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The chapter will include:'
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ONNX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open AI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp16](https://packt.link/dltfchp16).
  prefs: []
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Hugging Face
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hugging Face is not new for us; *Chapter 6*, *Transformers*, introduced us to
    the library. Hugging Face is an NLP-centered startup, founded by Delangue and
    Chaumond in 2016\. It has, in a short time, established itself as one of the best
    tools for all NLP-related tasks. The AutoNLP and accelerated inference API are
    available for a price. However, its core NLP libraries datasets, tokenizers, Accelerate,
    and transformers (*Figure 16.1*) are available for free. It has built a cool community-driven
    open-source platform.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated with medium confidence](img/B18331_16_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: NLP libraries from Hugging Face'
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of the Hugging Face ecosystem is its transformers library. The Tokenizers
    and Datasets libraries support the Transformers library. To use these libraries,
    we need to install them first. Transformers can be installed using a simple `pip
    install` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Some of the out-of-the-box models available with Hugging Face are text summarization,
    question answering, text classification, audio classification, automatic speech
    recognition, feature extraction, image classification, and translation. In *Figure
    16.2*, we can see the result of the out-of-the-box summarization model available
    with Hugging Face.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing graphical user interface  Description automatically
    generated](img/B18331_16_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: Out-of-the-box text summarization using Hugging Face'
  prefs: []
  type: TYPE_NORMAL
- en: Besides these out-of-the-box models, we can use the large number of models and
    datasets available at Hugging Face Hub and can use them with PyTorch, TensorFlow,
    and JAX to build customized models.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI is another well-known name for people working in the field of reinforcement
    learning. Their Gym module is a standard toolkit used by developers across the
    globe for developing and comparing reinforcement learning algorithms. In *Chapter
    11*, *Reinforcement Learning*, we have already covered the Gym module in detail.
    In this chapter, we will explore two more offerings by OpenAI.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI GPT-3 API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “OpenAI GPT3 is a machine learning platform that allows developers to build
    custom algorithms for deep learning. This platform was released in December of
    2017 and has been widely used by businesses and individuals in the field of artificial
    intelligence. One of the primary reasons that GPT3 has been so successful is because
    it is easy to use and has a wide range of features. This platform is able to learn
    from data and can be used for a variety of tasks, including deep learning, natural
    language processing, and image recognition. GPT3 is also popular because it is
    open source and can be used by anyone. This makes it an ideal platform for anyone
    who wants to learn about deep learning and the various ways that it can be used.
    Overall, GPT3 is a powerful and easy-to-use machine learning platform that has
    been widely used by businesses and individuals in the field of artificial intelligence.”
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the text generated by the OpenAI GPT-3 API, when asked to write on
    GPT-3 itself ([https://beta.openai.com/playground](https://beta.openai.com/playground)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18331_16_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: Text generation using OpenAI GPT-3 API'
  prefs: []
  type: TYPE_NORMAL
- en: 'The OpenAI GPT-3 API offers the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Text Completion**: Here, the GPT-3 API is used to generate or manipulate
    text and even code. You can use it to write a tagline, an introduction, or an
    essay, or you can leave a sentence half-written and ask it to complete it. People
    have used it to generate stories and advertisement leads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic Search**: This allows you to do a semantic search over a set of
    documents. For example, you can upload documents using the API; it can handle
    up to 200 documents, where each file can be a maximum of 150 MB in size, and the
    total limited to 1 GB at any given time. The API will take your query and rank
    the documents based on the semantic similarity score (ranges normally between
    0-300).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Question Answering**: This API uses the documents uploaded as the source
    of truth; the API first searches the documents for relevance to the question.
    Then it ranks them based on the semantic relevance and finally, answers the question.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text Classification**: The text classification endpoint of OpenAI GPT-3 takes
    as input a labeled set of examples and then uses the labels in it to label the
    query text. There are a lot of examples where this feature has been used to perform
    sentiment analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initially, the OpenAI GPT-3 was available only after applying for it, but now,
    anyone can use the API; there is no longer a waitlist.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI DALL-E 2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The GPT-3 API by OpenAI deals with all things related to NLP; DALL-E 2 goes
    a step further. DALL-E was originally released by OpenAI in January 2021\. It
    claims to produce photorealistic images based on the textual description provided
    to the model. It can also make realistic edits to existing images; you can use
    it to add or remove objects and elements from the image, and when it does so,
    it considers the effect on shadows, reflections, and texture. *Figure 16.4* shows
    some of the remarkable feats by DALL-E 2\. In the figures on the top row, I gave
    DALL-E 2 a text describing what I want: “Albert Einstein flying on dinosaur over
    the Amazon Forest.” It generated a cartoon-like image. The images in the lower
    row are generated using the image-editor feature of DALL-E 2\. I added the image
    on the left, and it generated four variations. The variations look very realistic
    if you ignore that the faces are blurred:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_16_04_1.png)![](img/B18331_16_04_2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: On top is the image generated by DALL-E 2, and below are the images
    edited by DALL-E 2'
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing this book (August 2022), DALL-E 2 is not available for
    public use. But imagine the possibilities for artists and professionals working
    in creating digital media once the model is available as an API.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI Codex
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a student starts with their first lessons of programming, as a teacher,
    I often recommend that they think of a program as a set of instructions – the
    only important thing to master is writing those instructions as clearly as possible
    in whatever language you know.
  prefs: []
  type: TYPE_NORMAL
- en: Well, Codex makes it happen, you just need to give it the instructions of what
    you want to achieve, and it will generate the respective code for you.
  prefs: []
  type: TYPE_NORMAL
- en: OpenAI launches it as a general-purpose programming model, and it has been trained
    on publicly available GitHub codes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below are a few snippets of the task and corresponding code generated by Codex:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first task, as you can see, is done flawlessly. In the second task, we asked
    it to find the sum of the Fibonacci sequence; instead, it generated the Fibonacci
    sequence, which is a more common problem. This tells us that while it is great
    at doing run-of-the-mill jobs, the need for real programmers is still there.
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like TensorFlow, PyTorch is a full-fledged deep learning framework. In AI-based
    social groups, you will often find die-hard fans of PyTorch and TensorFlow arguing
    that theirs is best. PyTorch, developed by Facebook (Meta now), is an open-source
    deep learning framework. Many researchers prefer it for its flexible and modular
    approach. PyTorch also has stable support for production deployment. Like TF,
    the core of PyTorch is its tensor processing library and its automatic differentiation
    engine. In a C++ runtime environment, it leverages TorchScript for an easy transition
    between graph and eager mode. The major feature that makes PyTorch popular is
    its ability to use dynamic computation, i.e., its ability to dynamically build
    the computational graph – this gives the programmer flexibility to modify and
    inspect the computational graphs anytime.
  prefs: []
  type: TYPE_NORMAL
- en: 'The PyTorch library consists of many modules, which are used as building blocks
    to make complex models. Additionally, PyTorch also provides convenient functions
    to transfer variables and models between different devices viz CPU, GPU, or TPU.
    Of special mention are the following three powerful modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**NN Module**: This is the base class where all layers and functions to build
    a deep learning network are. Below, you can see the code snippet where the NN
    module is used to build a network. The network can then be instantiated using
    the statement `net = My_Net(1,10,5)`; this creates a network with one input channel,
    10 output neurons, and a kernel of size `5x5`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is a summary of the network:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Autograd Module**: This is the heart of PyTorch. The module provides classes
    and functions that are used for implementing automatic differentiation. The module
    creates an acyclic graph called the dynamic computational graph; the leaves of
    this graph are the input tensors, and the root is the output tensors. It calculates
    a gradient by tracing the root to the leaf and multiplying every gradient in the
    path using the chain rule. The following code snippet shows how to use the Autograd
    module for calculating gradients. The `backward()` function computes the gradient
    of the loss with respect to all the tensors whose `requires_grad` is set to `True`.
    So suppose you have a variable `w`, then after the call to `backward(),` the tensor
    `w.grad` will give us the gradient of the loss with respect to `w`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can then use this to update the variable `w` as per the learning rule:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Optim Module**: The Optim module implements various optimization algorithms.
    Some of the optimizer algorithms available in Optim are SGD, AdaDelta, Adam, SparseAdam,
    AdaGrad, and LBFGS. One can also use the Optim module to create complex optimizers.
    To use the Optim module, one just needs to construct an optimizer object that
    will hold the current state and will update the parameters based on gradients.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch is used by many companies for their AI solutions. **Tesla** uses PyTorch
    for **AutoPilot**. The Tesla Autopilot, which uses the footage from eight cameras
    around the vehicle, passes that footage through 48 neural networks for object
    detection, semantic segmentation, and monocular depth estimation. The system provides
    level 2 vehicle automation. They take video from all eight cameras to generate
    road layout, any static infrastructure (e.g., buildings and traffic/electricity
    poles), and 3D objects (other vehicles, persons on the road, and so on). The networks
    are trained iteratively in real time. While a little technical, this 2019 talk
    by Andrej Karpathy, Director of AI at Tesla, gives a bird’s-eye view of Autopilot
    and its capabilities: [https://www.youtube.com/watch?v=oBklltKXtDE&t=670s](https://www.youtube.com/watch?v=oBklltKXtDE&t=670s).
    Uber’s Pyro, a probabilistic deep learning library, and OpenAI are other examples
    of big AI companies using PyTorch for research and development.'
  prefs: []
  type: TYPE_NORMAL
- en: ONNX
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Open Neural Network Exchange** (**ONNX**) provides an open-source format
    for AI models. It supports both deep learning models and traditional machine learning
    models. It is a format designed to represent any type of model, and it achieves
    this by using an intermediate representation of the computational graph created
    by different frameworks. It supports PyTorch, TensorFlow, MATLAB, and many more
    deep learning frameworks. Thus, using ONNX, we can easily convert models from
    one framework to another. This helps in reducing the time from research to deployment.
    For example, you can use ONNX to convert a PyTorch model to ONNX.js form, which
    can then be directly deployed on the web.'
  prefs: []
  type: TYPE_NORMAL
- en: H2O.ai
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O is a fast, scalable machine learning and deep learning framework developed
    by H2O.ai, released under the open-source Apache license. According to the company
    website, as of the time of writing this book, more than 20,000 organizations use
    H2O for their ML/deep learning needs. The company offers many products like H2O
    AI cloud, H2O Driverless AI, H2O wave, and Sparkling Water. In this section, we
    will explore its open-source product, H2O.
  prefs: []
  type: TYPE_NORMAL
- en: It works on big data infrastructure on Hadoop, Spark, or Kubernetes clusters
    and it can also work in standalone mode. It makes use of distributed systems and
    in-memory computing, which allows it to handle a large amount of data in memory,
    even with a small cluster of machines. It has an interface for R, Python, Java,
    Scala, and JavaScript, and even has a built-in web interface.
  prefs: []
  type: TYPE_NORMAL
- en: H2O includes a large number of statistical-based ML algorithms such as generalized
    linear modeling, Naive Bayes, random forest, gradient boosting, and all major
    deep learning algorithms. The best part of H2O is that one can build thousands
    of models, compare the results, and even do hyperparameter tuning with only a
    few lines of code. H2O also has better data pre-processing tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'H2O requires Java, therefore, ensure that Java is installed on your system.
    You can install H2O to work in Python using PyPi, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: H2O AutoML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the most exciting features of H2O is **AutoML**, the automatic ML. It
    is an attempt to develop a user-friendly ML interface that can be used by beginners
    and non-experts. H2O AutoML automates the process of training and tuning a large
    selection of candidate models. Its interface is designed so that users just need
    to specify their dataset, input and output features, and any constraints they
    want on the number of total models trained, or time constraints. The rest of the
    work is done by the AutoML itself, in the specified time constraint; it identifies
    the best performing models and provides a **leaderboard**. It has been observed
    that usually, the Stacked Ensemble model, the ensemble of all the previously trained
    models, occupies the top position on the leaderboard. There is a large number
    of options that advanced users can use; details of these options and their various
    features are available at [http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.xhtml](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about H2O, visit their website: [http://h2o.ai](http://h2o.ai).'
  prefs: []
  type: TYPE_NORMAL
- en: AutoML using H2O
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us try H2O AutoML on a synthetically created dataset. We use the scikit-learn
    `make_circles` method to create the data and save it as a CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we can use H2O, we need to initiate its server, which is done using
    the `init()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following shows the output we will receive after initializing the H2O server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the file containing the synthetic data that we created earlier. Since
    we want to treat the problem as a classification problem, whether the points lie
    in a circle or not, we redefine our label `''y''` as `asfactor()` – this will
    tell the H2O AutoML module to treat the variable `y` as categorical, and thus
    the problem as classification. The dataset is split into training, validation,
    and test datasets in a ratio of 60:20:20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we invoke the AutoML module from H2O and train on our training dataset.
    AutoML will search a maximum of 10 models, but you can change the parameter `max_models`
    to increase or decrease the number of models to test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'For each of the models, it gives a performance summary, for example, in *Figure
    16.5*, you can see the evaluation summary for a binomial GLM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Text  Description automatically generated](img/B18331_16_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.5: Performance summary of one of the models by H2O AutoML'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the performance of all the models evaluated by H2O AutoML on
    a leaderboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the snippet of the leaderboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: H2O model explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'H2O provides a convenient wrapper for a number of explainability methods and
    their visualizations using a single function `explain()` with a dataset and model.
    To get explainability on our test data for the models tested by AutoML, we will
    use `aml.explain()`. Below, we use the `explain` module for the `StackedEnsemble_BestOfFamily`
    model – the topmost in the leaderboard (we are continuing with the same data that
    we created in the previous section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The results are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application  Description automatically generated](img/B18331_16_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.6: A confusion matrix on test dataset generated by H2O explain module'
  prefs: []
  type: TYPE_NORMAL
- en: The ground truth is displayed in rows and the prediction by the model in columns.
    For our data, 0 was predicted correctly 104 times, and 1 was predicted correctly
    88 times.
  prefs: []
  type: TYPE_NORMAL
- en: Partial dependence plots
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Partial Dependence Plots** (**PDP**) provide a graphical depiction of the
    marginal effect of a variable on the response of the model. It can tell us about
    the relationship between the output label and the input feature. *Figure 16.7*
    shows the PDP plots as obtained from the H2O `explain` module on our synthetic
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing chart  Description automatically generated](img/B18331_16_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: PDP for input features x[1] and x[2]'
  prefs: []
  type: TYPE_NORMAL
- en: For building PDP plots for each feature, H2O considers the rest of the features
    as constant. So, in the PDP plot for x[1] (x[2]), the feature x[2] (x[1]) is kept
    constant and the mean response is measured, as x[1] (x[2]) is varied. The graph
    shows that both features play an important role in determining if the point is
    a circle or not, especially for values lying between [-0.5, 0.5].
  prefs: []
  type: TYPE_NORMAL
- en: Variable importance heatmap
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can also check the importance of variables across different models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Icon  Description automatically generated](img/B18331_16_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: Variable importance heatmap for input features x[1] and x[2]'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 16.8* shows how much importance was given to the two input features
    by different algorithms. We can see that the models that gave almost equal importance
    to the two features are doing well on the leaderboard, while **GLM_1**, which
    treated both features quite differently, has only about 41% accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: Model correlation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The prediction between different models is correlated; we can check this correlation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, histogram  Description automatically generated](img/B18331_16_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.9: Model correlation'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 16.9* shows the model correlation; it shows the correlation between
    the predictions on the test dataset for different models. It measures the frequency
    of identical predictions to calculate correlations. Again, we can see that except
    for **GLM_1**, most other models perform almost equally, with accuracy ranging
    from 84-93% on the leaderboard.'
  prefs: []
  type: TYPE_NORMAL
- en: What we have discussed here is just the tip of the iceberg; each of the frameworks
    listed here has entire books on their features and applications. Depending upon
    your use case, you should choose the respective framework. If you are building
    a model for production, TensorFlow is a better choice for both web-based and Edge
    applications. If you are building a model where you need better control of training
    and how the gradients are updated, then PyTorch is better suited. If you need
    to work cross-platform very often, ONNX can be useful. And finally, H2O and platforms
    like OpenAI GPT-3 and DALL-E 2 provide a low-threshold entry into the field of
    artificial intelligence and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we briefly covered the features and capabilities of some other
    popular deep learning frameworks, libraries, and platforms. We started with Hugging
    Face, a popular framework for NLP. Then we explored OpenAI’s GPT-3 and DALL-E
    2, both very powerful frameworks. The GPT-3 API can be used for a variety of NLP-related
    tasks, and DALL-E 2 uses GPT-3 to generate images from textual descriptions. Next,
    we touched on the PyTorch framework. According to many people, PyTorch and TensorFlow
    are equal competitors, and PyTorch indeed has many features comparable to TensorFlow.
    In this chapter, we briefly talked about some important features like the NN module,
    Optim module, and Autograd module of PyTorch. We also discussed ONNX, the open-source
    format for deep learning models, and how we can use it to convert the model from
    one framework to another. Lastly, the chapter introduced H2O and its AutoML and
    `explain` modules.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about graph neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1831217224278819687.png)'
  prefs: []
  type: TYPE_IMG
