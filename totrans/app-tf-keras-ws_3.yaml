- en: '3\. Real-World Deep Learning: Evaluating the Bitcoin Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter focuses on how to evaluate a neural network model. We'll modify
    the network's hyperparameters to improve its performance. However, before altering
    any parameters, we need to measure how the model performs. By the end of this
    chapter, you will be able to evaluate a model using different functions and techniques.
    You will also learn about hypermeter optimization by implementing functions and
    regularization strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous chapter, you trained your model. But how will you check its
    performance and whether it is performing well or not? Let''s find out by evaluating
    a model. In machine learning, it is common to define two distinct terms: **parameter**
    and **hyperparameter**. Parameters are properties that affect how a model makes
    predictions from data, say from a particular dataset. Hyperparameters refer to
    how a model learns from data. Parameters can be learned from the data and modified
    dynamically. Hyperparameters, on the other hand, are higher-level properties defined
    before the training begins and are not typically learned from data. In this chapter,
    you will learn about these factors in detail and understand how to use them with
    different evaluation techniques to improve the performance of a model.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For a more detailed overview of machine learning, refer to *Python Machine Learning*,
    *Sebastian Raschka and Vahid Mirjalili, Packt Publishing, 2017)*.
  prefs: []
  type: TYPE_NORMAL
- en: Problem Categories
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Generally, there are two categories of problems that can be solved by neural
    networks: **classification** and **regression**. Classification problems concern
    the prediction of the right categories from data—for instance, whether the temperature
    is hot or cold. Regression problems are about the prediction of values in a continuous
    scalar—for instance, what the actual temperature value is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The problems in these two categories are characterized by the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '`yes` or `no`. However, they must be clearly assigned to each data element.
    An example of a classification problem would be to assign either the `car` or
    `not a car` labels to an image using a convolutional neural network. The MNIST
    example we explored in *Chapter 1*, *Introduction to Neural Networks and Deep
    Learning*, is another example of a classification problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: These are problems that are characterized by a continuous variable
    (that is, a scalar). They are measured in terms of ranges, and their evaluations
    consider how close to the real values the network is. An example is a time-series
    classification problem in which a recurrent neural network is used to predict
    the future temperature values. The Bitcoin price-prediction problem is another
    example of a regression problem.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While the overall structure of how to evaluate these models is the same for
    both of these problem categories, we employ different techniques to evaluate how
    models perform. In the next section, we'll explore these techniques for either
    classification or regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: Loss Functions, Accuracy, and Error Rates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks utilize functions that measure how the networks perform compared
    to a **validation set**—that is, a part of the data that is kept separate to be
    used as part of the training process. These functions are called **loss functions**.
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions evaluate how wrong a neural network's predictions are. Then,
    they propagate those errors back and make adjustments to the network, modifying
    how individual neurons are activated. Loss functions are key components of neural
    networks, and choosing the right loss function can have a significant impact on
    how the network performs. Errors are propagated through a process called **backpropagation**,
    which is a technique for propagating the errors that are returned by the loss
    function to each neuron in a neural network. Propagated errors affect how neurons
    activate, and ultimately, how they influence the output of that network.
  prefs: []
  type: TYPE_NORMAL
- en: Many neural network packages, including Keras, use this technique by default.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For more information about the mathematics of backpropagation, please refer
    to *Deep Learning* by *Ian Goodfellow et. al., MIT Press, 2016*.
  prefs: []
  type: TYPE_NORMAL
- en: We use different loss functions for regression and classification problems.
    For classification problems, we use accuracy functions (that is, the proportion
    of the number of times the predictions were correct to the number of times predictions
    were made). For example, if you predict a toss of a coin that will result in *m*
    times as heads when you toss it *n* times and your prediction is correct, then
    the accuracy will be calculated as *m/n*. For regression problems, we use error
    rates (that is, how close the predicted values were to the observed ones).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a summary of common loss functions that can be utilized, alongside
    their common applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1: Common loss functions used for classification and regression
    problems'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.1: Common loss functions used for classification and regression problems'
  prefs: []
  type: TYPE_NORMAL
- en: For regression problems, the MSE function is the most common choice, while for
    classification problems, binary cross-entropy (for binary category problems) and
    categorical cross-entropy (for multi-category problems) are common choices. It
    is advised to start with these loss functions, then experiment with other functions
    as you evolve your neural network, aiming to gain performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The network we developed in *Chapter 2*, *Real-World Deep Learning with TensorFlow
    and Keras: Predicting the Price of Bitcoin*, uses MSE as its loss function. In
    the next section, we''ll explore how that function performs as the network trains.'
  prefs: []
  type: TYPE_NORMAL
- en: Different Loss Functions, Same Architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before moving ahead to the next section, let's explore, in practical terms,
    how these problems are different in the context of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The *TensorFlow Playground* ([https://playground.tensorflow.org/](https://playground.tensorflow.org/))
    application has been made available by the TensorFlow team to help us understand
    how neural networks work. Here, we can see a neural network represented with its
    layers: input (on the left), hidden layers (in the middle), and output (on the
    right).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note:'
  prefs: []
  type: TYPE_NORMAL
- en: 'These images can be viewed in the repository on GitHub at: [https://packt.live/2Cl1t0H](https://packt.live/2Cl1t0H).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also choose different sample datasets to experiment with on the far
    left-hand side. And, finally, on the far right-hand side, we can see the output
    of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2: TensorFlow Playground web application'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.2: TensorFlow Playground web application'
  prefs: []
  type: TYPE_NORMAL
- en: 'Take the parameters for a neural network shown in this visualization to gain
    an idea of how each parameter affects the model''s results. This application helps
    us explore the different problem categories we discussed in the previous section.
    When we choose `Regression` (upper right-hand corner), the colors of the dots
    are colored in a range of color values between orange and blue:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3: Regression problem example in TensorFlow Playground'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.3: Regression problem example in TensorFlow Playground'
  prefs: []
  type: TYPE_NORMAL
- en: 'When we choose `Classification` as the `Problem type`, the dots in the dataset
    are colored with only two color values: either blue or orange. When working on
    classification problems, the network evaluates its loss function based on how
    many blues and oranges the network has gotten wrong. It checks how far away to
    the right the color values are for each dot in the network, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4: Details of the TensorFlow Playground application. Different colors
    are assigned to different classes in classification problems'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.4: Details of the TensorFlow Playground application. Different colors
    are assigned to different classes in classification problems'
  prefs: []
  type: TYPE_NORMAL
- en: After clicking on the play button, we notice that the numbers in the `Training
    loss` area keep going down as the network continuously trains. The numbers are
    very similar in each problem category because the loss functions play the same
    role in both neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: However, the actual loss function that's used for each category is different
    and is chosen depending on the problem type.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluating neural networks is where TensorBoard excels. As we explained in *Chapter
    1*, *Introduction to Neural Networks and Deep Learning*, TensorBoard is a suite
    of visualization tools that's shipped with TensorFlow. Among other things, we
    can explore the results of loss function evaluations after each epoch. A great
    feature of TensorBoard is that we can organize the results of each run separately
    and compare the resulting loss function metrics for each run. We can then decide
    on which hyperparameters to tune and have a general sense of how the network is
    performing. The best part is that this is all done in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to use TensorBoard with our model, we will use Keras'' `callback`
    function. We do this by importing the TensorBoard `callback` and passing it to
    our model when calling its `fit()` function. The following code shows an example
    of how this would be implemented in the Bitcoin model we created in the *Chapter
    2*, *Real-World Deep Learning with TensorFlow and Keras: Predicting the Price
    of Bitcoin*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Keras `callback` functions are called at the end of each epoch run. In this
    case, Keras calls the TensorBoard `callback` to store the results from each run
    on the disk. There are many other useful `callback` functions available, and you
    can create custom ones using the Keras API.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to the Keras callback documentation ([https://keras.io/callbacks/](https://keras.io/callbacks/))
    for more information.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing the TensorBoard callback, the loss function metrics are now
    available in the TensorBoard interface. You can now run a TensorBoard process
    (with `tensorboard --logdir=./logs`) and leave it running while you train your
    network with `fit()`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main graphic to evaluate is typically called loss. We can add more metrics
    by passing known metrics to the metrics parameter in the `fit()` function. These
    will then be available for visualization in TensorBoard, but will not be used
    to adjust the network weights. The interactive graphics will continue to update
    in real time, which allows you to understand what is happening on every epoch.
    In the following screenshot, you can see a TensorBoard instance showing loss function
    results, alongside other metrics that have been added to the metrics parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5: Screenshot of a TensorBoard instance showing the loss function
    results'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.5: Screenshot of a TensorBoard instance showing the loss function
    results'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will talk more about how to implement the different
    metrics we discussed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Model Evaluation Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In both regression and classification problems, we split the input dataset
    into three other datasets: train, validation, and test. Both the train and the
    validation sets are used to train the network. The train set is used by the network
    as input, while the validation set is used by the loss function to compare the
    output of the neural network to the real data and compute how wrong the predictions
    are. Finally, the test set is used after the network has been trained to measure
    how the network can perform on data it has never seen before.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: There isn't a clear rule for determining how the train, validation, and test
    datasets must be divided. It is a common approach to divide the original dataset
    into 80 percent train and 20 percent test, then to further divide the train dataset
    into 80 percent train and 20 percent validation. For more information about this
    problem, please refer to *Python Machine Learning*, by *Sebastian Raschka and
    Vahid Mirjalili (Packt Publishing, 2017)*.
  prefs: []
  type: TYPE_NORMAL
- en: In classification problems, you pass both the data and the labels to the neural
    network as related but distinct data. The network then learns how the data is
    related to each label. In regression problems, instead of passing data and labels,
    we pass the variable of interest as one parameter and the variables that are used
    for learning patterns as another. Keras provides an interface for both of those
    use cases with the `fit()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `fit()` method can use either the `validation_split` or the `validation_data`
    parameter, but not both at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following snippet to understand how to use the `validation_split` and
    `validation_data` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`X_train`: features from training set'
  prefs: []
  type: TYPE_NORMAL
- en: '`Y_train`: labels from training set'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size`: the size of one batch'
  prefs: []
  type: TYPE_NORMAL
- en: '`epochs`: the number of iterations'
  prefs: []
  type: TYPE_NORMAL
- en: '`verbose`: the level of output you want'
  prefs: []
  type: TYPE_NORMAL
- en: '`callbacks`: call a function after every epoch'
  prefs: []
  type: TYPE_NORMAL
- en: '`validation_split`: validation percentage split if you have not created it
    explicitly'
  prefs: []
  type: TYPE_NORMAL
- en: '`validation_data`: validation data if you have created it explicitly'
  prefs: []
  type: TYPE_NORMAL
- en: Loss functions evaluate the progress of models and adjust their weights on every
    run. However, loss functions only describe the relationship between training data
    and validation data. In order to evaluate if a model is performing correctly,
    we typically use a third set of data—which is not used to train the network—and
    compare the predictions made by our model to the values available in that set
    of data. That is the role of the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras provides the `model.evaluate()` method, which makes the process of evaluating
    a trained neural network against a test set easy. The following code illustrates
    how to use the `evaluate()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `evaluate()` method returns both the results of the loss function and the
    results of the functions passed to the `metrics` parameter. We will be using that
    function frequently in the Bitcoin problem to test how the model performs on the
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that the Bitcoin model we trained previously looks a bit different
    than this example. That is because we are using an LSTM architecture. LSTMs are
    designed to predict sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Because of that, we do not use a set of variables to predict a different single
    variable—even if it is a regression problem. Instead, we use previous observations
    from a single variable (or set of variables) to predict future observations of
    that same variable (or set). The `y` parameter on `keras.fit()` contains the same
    variable as the `x` parameter, but only the predicted sequences. So, let's have
    a look at how to evaluate the bitcoin model we trained previously.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the Bitcoin Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We created a test set during our activities in *Chapter 1*, *Introduction to
    Neural Networks and Deep Learning*. That test set contains 21 weeks of daily Bitcoin
    price observations, which is equivalent to about 10 percent of the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also trained our neural network using the other 90 percent of data (that
    is, the train set with 187 weeks of data, minus 1 for the validation set) in *Chapter
    2*, *Real-World Deep Learning with TensorFlow and Keras: Predicting the Price
    of Bitcoin*, and stored the trained network on disk (`bitcoin_lstm_v0`). We can
    now use the `evaluate()` method in each of the 21 weeks of data from the test
    set and inspect how that first neural network performs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to do that, though, we have to provide 186 preceding weeks. We have
    to do this because our network has been trained to predict one week of data using
    exactly 186 weeks of continuous data (we will deal with this behavior by retraining
    our network periodically with larger periods in *Chapter 4*, *Productization*,
    when we deploy a neural network as a web application). The following snippet implements
    the `evaluate()` method to evaluate the performance of our model in a test dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we evaluate each week using Keras'' `model.evaluate()`
    method, then store its output in the `evaluated_weeks` variable. We then plot
    the resulting MSE for each week, as shown in the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6: MSE for each week in the test set'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.6: MSE for each week in the test set'
  prefs: []
  type: TYPE_NORMAL
- en: The resulting MSE from our model suggests that our model performs well during
    most weeks, except for weeks 2, 8, 12, and 16, when its value increases from about
    0.005 to 0.02\. Our model seems to be performing well for almost all of the other
    test weeks.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our first trained network (`bitcoin_lstm_v0`) may be suffering from a phenomenon
    known as **overfitting**. Overfitting is when a model is trained to optimize a
    validation set, but it does so at the expense of more generalizable patterns from
    the phenomenon we are interested in predicting. The main issue with overfitting
    is that a model learns how to predict the validation set, but fails to predict
    new data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function we used in our model reaches very low levels at the end of
    our training process. Not only that, but this happens early: the MSE loss function
    that''s used to predict the last week in our data decreases to a stable plateau
    around epoch 30\. This means that our model is predicting the data from week 187
    almost perfectly, using the preceding 186 weeks. Could this be the result of overfitting?'
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at the preceding plot again. We know that our LSTM model reaches
    extremely low values in our validation set (about 2.9 × 10-6), yet it also reaches
    low values in our test set. The key difference, however, is in the scale. The
    MSE for each week in our test set is about 4,000 times bigger (on average) than
    in the test set. This means that the model is performing much worse in our test
    data than in the validation set. This is worth considering.
  prefs: []
  type: TYPE_NORMAL
- en: The scale, though, hides the power of our LSTM model; even performing much worse
    in our test set, the predictions' MSE errors are still very, very low. This suggests
    that our model may be learning patterns from the data.
  prefs: []
  type: TYPE_NORMAL
- en: Model Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It's one thing is to measure our model comparing MSE errors, and another to
    be able to interpret its results intuitively.
  prefs: []
  type: TYPE_NORMAL
- en: Using the same model, let's create a series of predictions for the following
    weeks, using 186 weeks as input. We do that by sliding a window of 186 weeks over
    the complete series (that is, train plus test sets) and making predictions for
    each of those windows.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet makes predictions for all the weeks of the test dataset
    using the `Keras model.predict()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, we make predictions using the `model.predict()` method,
    then store these predictions in the `predicted_weeks` variable. Then, we plot
    the resulting predictions, making the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7: MSE for each week in the test set'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.7: MSE for each week in the test set'
  prefs: []
  type: TYPE_NORMAL
- en: The results of our model suggest that its performance isn't all that bad. By
    observing the pattern from the `Predicted` line (grey), we can see that the network
    has identified a fluctuating pattern happening on a weekly basis, in which the
    normalized prices go up in the middle of the week, then down by the end of it
    but. However, there's still a lot of room for improvement as it is unable to pick
    up higher fluctuations. With the exception of a few weeks—the same as with our
    previous MSE analysis—most weeks fall close to the correct values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s denormalize the predictions so that we can investigate the prediction
    values using the same scale as the original data (that is, US dollars). We can
    do this by implementing a denormalization function that uses the day index from
    the predicted data to identify the equivalent week on the test data. After that
    week has been identified, the function then takes the first value of that week
    and uses that value to denormalize the predicted values by using the same point-relative
    normalization technique but inverted. The following snippet denormalizes data
    using an inverted point-relative normalization technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The `denormalize()` function takes the first closing price from the test's first
    day of an equivalent week.
  prefs: []
  type: TYPE_NORMAL
- en: Our results now compare the predicted values with the test set using US dollars.
    As shown in the following plot, the `bitcoin_lstm_v0` model seems to perform quite
    well in predicting the Bitcoin prices for the following 7 days. But how can we
    measure that performance in interpretable terms?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8: De-normalized predictions per week'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.8: De-normalized predictions per week'
  prefs: []
  type: TYPE_NORMAL
- en: Interpreting Predictions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Our last step is to add interpretability to our predictions. The preceding plot
    seems to show that our model prediction matches the test data somewhat closely,
    but how closely?
  prefs: []
  type: TYPE_NORMAL
- en: Keras' `model.evaluate()` function is useful for understanding how a model is
    performing at each evaluation step. However, given that we are typically using
    normalized datasets to train neural networks, the metrics that are generated by
    the `model.evaluate()` method are also hard to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to solve that problem, we can collect the complete set of predictions
    from our model and compare it with the test set using two other functions from
    *Figure 3.1* that are easier to interpret: MAPE and RMSE, which are implemented
    as `mape()` and `rmse()`, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: These functions are implemented using NumPy. The original implementations come
    from [https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn](https://stats.stackexchange.com/questions/58391/mean-absolute-percentage-error-mape-in-scikit-learn)
    and [https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy](https://stackoverflow.com/questions/16774849/mean-squared-error-in-numpy)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see the implementation of these methods in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After comparing our test set with our predictions using both of those functions,
    we have the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Denormalized RMSE**: $596.6 USD'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Denormalized MAPE**: 4.7 percent'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This indicates that our predictions differ, on average, about $596 from real
    data. This represents a difference of about 4.7 percent from real Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: These results facilitate the understanding of our predictions. We will continue
    to use the `model.evaluate()` method to keep track of how our LSTM model is improving,
    but will also compute both `rmse()` and `mape()` on the complete series on every
    version of our model to interpret how close we are to predicting Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 3.01: Creating an Active Training Environment'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we'll create a training environment for our neural network
    that facilitates both its training and evaluation. This environment is particularly
    important for the next topic, in which we'll search for an optimal combination
    of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will start both a Jupyter Notebook instance and a TensorBoard instance.
    Both of these instances can remain open for the remainder of this exercise. Let''s
    get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your Terminal, navigate to the `Chapter03/Exercise3.01` directory and
    execute the following code to start a Jupyter Notebook instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The server should open in your browser automatically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open the Jupyter Notebook named `Exercise3.01_Creating_an_active_training_environment.ipynb`:![Figure
    3.9: Jupyter Notebook highlighting the section, Evaluate LSTM Model'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_03_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.9: Jupyter Notebook highlighting the section, Evaluate LSTM Model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Also using your Terminal, start a TensorBoard instance by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Ensure the `logs` directory is empty in the repository.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Open the URL that appears on screen and leave that browser tab open, as well.
    Execute the initial cells containing the import statements to ensure that the
    dependencies are loaded.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Execute the two cells under Validation Data to load the train and test datasets
    in the Jupyter Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Don't forget to change the path (highlighted) of the files based on where they
    are saved on your system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Add TensorBoard callback and retrain the model. Execute the cells under Re-Train
    model with TensorBoard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, let's evaluate how our model performed against the test data. Our model
    is trained using 186 weeks to predict a week into the future—that is, the following
    sequence of 7 days. When we built our first model, we divided our original dataset
    between a training and a test set. Now, we will take a combined version of both
    datasets (let's call it a combined set) and move a sliding window of 186 weeks.
    At each window, we execute Keras' `model.evaluate()` method to evaluate how the
    network performed on that specific week.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Execute the cells under the header, `Evaluate LSTM Model`. The key concept
    of these cells is to call the `model.evaluate()` method for each of the weeks
    in the test set. This line is the most important:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each evaluation result is now stored in the `evaluated_weeks` variable. That
    variable is a simple array containing the sequence of MSE predictions for every
    week in the test set. Go ahead and plot the results:![Figure 3.10: MSE results
    from the model.evaluate() method for each week of the test set'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.10: MSE results from the model.evaluate() method for each week of
    the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As we've already discussed, the MSE loss function is difficult to interpret.
    To facilitate our understanding of how our model is performing, we also call the
    `model.predict()` method on each week from the test set and compare its predicted
    results with the set's values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Navigate to the *Interpreting Model Results* section and execute the code cells
    under the `Make Predictions` subheading. Notice that we are calling the `model.predict()`
    method, but with a slightly different combination of parameters. Instead of using
    both the `X` and `Y` values, we only use `X`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At each window, we will issue predictions for the following week and store
    the results. Now, we can plot the normalized results alongside the normalized
    values from the test set, as shown in the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.11: Plotting the normalized values returned from model.predict()
    for each'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: week of the test set](img/B15911_03_11.jpg)
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.11: Plotting the normalized values returned from model.predict() for
    each week of the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will also make the same comparisons but using denormalized values. In order
    to denormalize our data, we must identify the equivalent week between the test
    set and the predictions. Then, we can take the first price value for that week
    and use it to reverse the point-relative normalization equation from *Chapter
    2*, *Real-World Deep Learning with TensorFlow and Keras: Predicting the Price
    of Bitcoin*.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Navigate to the `De-normalized Predictions` header and execute all the cells
    under that header.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this section, we defined the `denormalize()` function, which performs the
    complete denormalization process. In contrast to the other functions, this function
    takes in a pandas DataFrame instead of a NumPy array. We do this to use dates
    as an index. This is the most relevant cell block from that header:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our denormalized results (as seen in the following plot) show that our model
    makes predictions that are close to the real Bitcoin prices. But how close?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.12: Plotting the denormalized values returned from model.predict()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: for each week of the test set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.12: Plotting the denormalized values returned from model.predict()
    for each week of the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The LSTM network uses MSE values as its loss function. However, as we've already
    discussed, MSE values are difficult to interpret. To solve this, we need to implement
    two functions (loaded from the `utilities.py` script) that implement the `rmse()`
    and `mape()` functions. These functions add interpretability to our model by returning
    a measurement on the same scale that our original data used, and by comparing
    the difference in scale as a percentage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Navigate to the `De-normalizing Predictions` header and load two functions
    from the `utilities.py` script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The functions from this script are actually really simple:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Each function is implemented using NumPy's vector-wise operations. They work
    well in vectors of the same length. They are designed to be applied on a complete
    set of results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using the `mape()` function, we can now understand that our model predictions
    are about 4.7 percent away from the prices from the test set. This is equivalent
    to a RSME (calculated using the `rmse()` function) of about $596.6.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Before moving on to the next section, go back into the Notebook and find the
    `Re-train Model with TensorBoard` header. You may have noticed that we created
    a helper function called `train_model()`. This function is a wrapper around our
    model that trains (using `model.fit()`) our model, storing its respective results
    under a new directory. Those results are then used by TensorBoard in order to
    display statistics for different models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Go ahead and modify some of the values for the parameters that were passed
    to the `model.fit()` function (try epochs, for instance). Now, run the cells that
    load the model into memory from disk (this will replace your trained model):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, run the `train_model()` function again, but with different parameters,
    indicating a new run version. When you run this command, you will be able to train
    a newer version of the model and specify the newer version in the version parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZhK4z3](https://packt.live/2ZhK4z3).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Dvd9i3](https://packt.live/2Dvd9i3).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we learned how to evaluate a network using loss functions.
    We learned that loss functions are key elements of neural networks since they
    evaluate the performance of a network at each epoch and are the starting point
    for the propagation of adjustments back into layers and nodes. We also explored
    why some loss functions can be difficult to interpret (for instance, the MSE function)
    and developed a strategy using two other functions—RMSE and MAPE—to interpret
    the predicted results from our LSTM model.
  prefs: []
  type: TYPE_NORMAL
- en: Most importantly, we've concluded this exercise with an active training environment.
    We now have a system that can train a deep learning model and evaluate its results
    continuously. This will be key when we move on to optimizing our network in the
    next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have trained a neural network to predict the next 7 days of Bitcoin
    prices using the preceding 76 weeks of prices. On average, this model issues predictions
    that are about 8.4 percent distant from real Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section describes common strategies for improving the performance of neural
    network models:'
  prefs: []
  type: TYPE_NORMAL
- en: Adding or removing layers and changing the number of nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing or decreasing the number of training epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experimenting with different activation functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using different regularization strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will evaluate each modification using the same active learning environment
    we developed by the end of the *Model Evaluation* section, measuring how each
    one of these strategies may help us develop a more precise model.
  prefs: []
  type: TYPE_NORMAL
- en: Layers and Nodes – Adding More Layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural networks with single hidden layers can perform fairly well on many problems.
    Our first Bitcoin model (`bitcoin_lstm_v0`) is a good example: it can predict
    the next 7 days of Bitcoin prices (from the test set) with error rates of about
    8.4 percent using a single LSTM layer. However, not all problems can be modeled
    with single layers.'
  prefs: []
  type: TYPE_NORMAL
- en: The more complex the function you are working to predict, the higher the likelihood
    that you will need to add more layers. A good way to determine whether adding
    new layers is a good idea is to understand what their role in a neural network
    is.
  prefs: []
  type: TYPE_NORMAL
- en: Each layer creates a model representation of its input data. Earlier layers
    in the chain create lower-level representations, while later layers create higher-level representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'While this description may be difficult to translate into real-world problems,
    its practical intuition is simple: when working with complex functions that have
    different levels of representation, you may want to experiment with adding layers.'
  prefs: []
  type: TYPE_NORMAL
- en: Adding More Nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The number of neurons that your layer requires is related to how both the input
    and output data is structured. For instance, if you are working on a binary classification
    problem to classify a 4 x 4 pixel image, then you can try out the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Have a hidden layer that has 12 neurons (one for each available pixel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have an output layer that has only two neurons (one for each predicted class)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is common to add new neurons alongside the addition of new layers. Then,
    we can add a layer that has either the same number of neurons as the previous
    one, or a multiple of the number of neurons from the previous layer. For instance,
    if your first hidden layer has 12 neurons, you can experiment with adding a second
    layer that has either 12, 6, or 24 neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Adding layers and neurons can have significant performance limitations. Feel
    free to experiment with adding layers and nodes. It is common to start with a
    smaller network (that is, a network with a small number of layers and neurons),
    then grow according to its performance gains.
  prefs: []
  type: TYPE_NORMAL
- en: If this comes across as imprecise, your intuition is right.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To quote *Aurélien Géron*, YouTube's former lead for video classification, "*Finding
    the perfect amount of neurons is still somewhat of a black art*."
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, a word of caution: the more layers you add, the more hyperparameters
    you have to tune—and the longer your network will take to train. If your model
    is performing fairly well and not overfitting your data, experiment with the other
    strategies outlined in this chapter before adding new layers to your network.'
  prefs: []
  type: TYPE_NORMAL
- en: Layers and Nodes – Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will modify our original LSTM model by adding more layers. In LSTM models,
    we typically add LSTM layers in a sequence, making a chain between LSTM layers.
    In our case, the new LSTM layer has the same number of neurons as the original
    layer, so we don't have to configure that parameter.
  prefs: []
  type: TYPE_NORMAL
- en: We will name the modified version of our model `bitcoin_lstm_v1`. It is good
    practice to name each one of the models in terms of which one is attempting different
    hyperparameter configurations. This helps you to keep track of how each different
    architecture performs, and also to easily compare model differences in TensorBoard.
    We will compare all the different modified architectures at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Before adding a new LSTM layer, we need to set the `return_sequences` parameter
    to `True` on the first LSTM layer. We do this because the first layer expects
    a sequence of data with the same input as that of the first layer. When this parameter
    is set to `False`, the LSTM layer outputs the predicted parameters in a different,
    incompatible output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example adds a second LSTM layer to the original `bitcoin_lstm_v0`
    model, making it `bitcoin_lstm_v1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Epochs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Epochs** are the number of times the network adjusts its weights in response
    to the data passing through and its resulting loss function. Running a model for
    more epochs can allow it to learn more from data, but you also run the risk of
    overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: When training a model, prefer to increase the epochs exponentially until the
    loss function starts to plateau. In the case of the `bitcoin_lstm_v0` model, its
    loss function plateaus at about 100 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Our LSTM model uses a small amount of data to train, so increasing the number
    of epochs does not affect its performance in a significant way. For instance,
    if we attempt to train it at 103 epochs, the model barely gains any improvements.
    This will not be the case if the model being trained uses enormous amounts of
    data. In those cases, a large number of epochs is crucial to achieve good performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'I suggest you use the following rule of thumb: *the larger the data used to
    train your model, the more epochs it will need to achieve good performance*.'
  prefs: []
  type: TYPE_NORMAL
- en: Epochs – Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our Bitcoin dataset is rather small, so increasing the epochs that our model
    trains may have only a marginal effect on its performance. In order to have the
    model train for more epochs, we only have to change the `epochs` parameter in
    the `model.fit()` method. In the following snippet, you will see how to change
    the number of epochs that our model trains for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This change bumps our model to `v2`, effectively making it `bitcoin_lstm_v2`.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Activation functions** evaluate how much you need to activate individual
    neurons. They determine the value that each neuron will pass to the next element
    of the network, using both the input from the previous layer and the results from
    the loss function—or if a neuron should pass any values at all.'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Activation functions are a topic of great interest for those in the scientific
    community researching neural networks. For an overview of research currently being
    done on the topic and a more detailed review on how activation functions work,
    please refer to *Deep Learning by Ian Goodfellow et. al., MIT Press, 2017*.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow and Keras provide many activation functions—and new ones are occasionally
    added. As an introduction, three are important to consider; let's explore each
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This section has been greatly inspired by the article *Understanding Activation
    Functions in Neural Networks* by Avinash Sharma V, available at [https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0).
  prefs: []
  type: TYPE_NORMAL
- en: Linear (Identity) Functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Linear functions only activate a neuron based on a constant value. They are
    defined by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13: Formula for linear functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.13: Formula for linear functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *c* is the constant value. When *c = 1*, neurons will pass the values
    as is, without any modification needed by the activation function. The issue with
    using linear functions is that, due to the fact that neurons are activated linearly,
    chained layers now function as a single large layer. In other words, we lose the
    ability to construct networks with many layers, in which the output of one influences
    the other:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14: Illustration of a linear function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.14: Illustration of a linear function'
  prefs: []
  type: TYPE_NORMAL
- en: The use of linear functions is generally considered obsolete for most networks
    because they do not compute complex features and do not induce proper non-linearity
    in neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperbolic Tangent (Tanh) Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tanh is a non-linear function, and is represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15: Formula for hyperbolic tangent function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.15: Formula for hyperbolic tangent function'
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the effect they have on nodes is evaluated continuously. Also,
    because of its non-linearity, we can use this function to change how one layer
    influences the next layer in the chain. When using non-linear functions, layers
    activate neurons in different ways, making it easier to learn different representations
    from data. However, they have a sigmoid-like pattern that penalizes extreme node
    values repeatedly, causing a problem called vanishing gradients. Vanishing gradients
    have negative effects on the ability of a network to learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16: Illustration of a tanh function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.16: Illustration of a tanh function'
  prefs: []
  type: TYPE_NORMAL
- en: Tanhs are popular choices, but due to the fact that they are computationally
    expensive, ReLUs are often used instead.
  prefs: []
  type: TYPE_NORMAL
- en: Rectified Linear Unit Functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**ReLU** stands for **Rectified Linear Unit**. It filters out negative values
    and keeps only the positive values. ReLU functions are often recommended as great
    starting points before trying other functions. They are defined by the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17: Formula for ReLU functions'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.17: Formula for ReLU functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'ReLUs have non-linear properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18: Illustration of a ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.18: Illustration of a ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: ReLUs tend to penalize negative values. So, if the input data (for instance,
    normalized between -1 and 1) contains negative values, those will now be penalized
    by ReLUs. That may not be the intended behavior.
  prefs: []
  type: TYPE_NORMAL
- en: We will not be using ReLU functions in our network because our normalization
    process creates many negative values, yielding a much slower learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Activation Functions – Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The easiest way to implement activation functions in Keras is by instantiating
    the `Activation()` class and adding it to the `Sequential()` model. `Activation()`
    can be instantiated with any activation function available in Keras (for a complete
    list, see [https://keras.io/activations/](https://keras.io/activations/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we will use the `tanh` function. After implementing an activation
    function, we bump the version of our model to `v2`, making it `bitcoin_lstm_v3`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: After executing the `compile` command, your model has been built according to
    the layers specified and is now ready to be trained. There are a number of other
    activation functions worth experimenting with. Both TensorFlow and Keras provide
    a list of implemented functions in their respective official documentations. Before
    implementing your own, start with the ones we've already implemented in both TensorFlow
    and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization Strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Neural networks are particularly prone to overfitting. Overfitting happens when
    a network learns the patterns of the training data but is unable to find generalizable
    patterns that can also be applied to the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Regularization strategies refer to techniques that deal with the problem of
    overfitting by adjusting how the network learns. In the following sections, we''ll
    discuss two common strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: L2 Regularization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: L2 Regularization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**L2 regularization** (or **weight decay**) is a common technique for dealing
    with overfitting models. In some models, certain parameters vary in great magnitudes.
    L2 regularization penalizes such parameters, reducing the effect of these parameters
    on the network.'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularizations use the ![3](img/B15911_03_Formula_01.png) parameter to determine
    how much to penalize a model neuron. We typically set that to a very low value
    (that is, 0.0001); otherwise, we risk eliminating the input from a given neuron
    completely.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Dropout is a regularization technique based on a simple question: *if we randomly
    take away a proportion of the nodes from the layers, how will the other node adapt?*
    It turns out that the remaining neurons adapt, learning to represent patterns
    that were previously handled by those neurons that are missing.'
  prefs: []
  type: TYPE_NORMAL
- en: The dropout strategy is simple to implement and is typically very effective
    at avoiding overfitting. This will be our preferred regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization Strategies – Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to implement the dropout strategy using Keras, we''ll import the `Dropout()`
    method and add it to our network immediately after each LSTM layer. This addition
    effectively makes our network `bitcoin_lstm_v4`. In this snippet, we''re adding
    the `Dropout()` step to our model (`bitcoin_lstm_v3`), making it `bitcoin_lstm_v4`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We could have used L2 regularization instead of dropout. Dropout drops out random
    neurons in each epoch, whereas L2 regularization penalizes neurons that have high
    weight values. In order to apply L2 regularization, simply instantiate the `ActivityRegularization()`
    class with the L2 parameter set to a low value (for instance, 0.0001). Then, place
    it in the place where the `Dropout()` class has been added to the network. Feel
    free to experiment by adding that to the network while keeping both `Dropout()`
    steps, or simply replace all the `Dropout()` instances with `ActivityRegularization()`
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All in all, we have created four versions of our model. Three of these versions,
    that is, `bitcoin_lstm_v1`, `bitcoin_lstm_v2`, and `bitcoin_lstm_v3`, were created
    by applying different optimization techniques that were outlined in this chapter.
    Now, we have to evaluate which model performs best. In order to do that, we will
    use the same metrics we used in our first model: MSE, RMSE, and MAPE. MSE is used
    to compare the error rates of the model on each predicted week. RMSE and MAPE
    are computed to make the model results easier to interpret. The following table
    shows this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19: Model results for all models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.19: Model results for all models'
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, our first model (`bitcoin_lstm_v0`) performed the best in nearly
    all defined metrics. We will be using that model to build our web application
    and continuously predict Bitcoin prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 3.01: Optimizing a Deep Learning Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this activity, we''ll implement different optimization strategies on the
    model we created in *Chapter 2*, *Real-World Deep Learning with TensorFlow and
    Keras: Predicting the Price of Bitcoin* (`bitcoin_lstm_v0`). This model achieves
    a MAPE performance on the complete de-normalization test set of about 8.4 percent.
    We will try to reduce that gap and get more accurate predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start TensorBoard from a Terminal.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a Jupyter Notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the train and test data and split the `lstm` input in the format required
    by the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous exercise, we create a model architecture. Copy that model architecture
    and add a new LSTM layer. Compile and create a model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the number of epochs in *step 4* by creating a new model. Compile and
    create a new model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Change the activation function to `tanh` or `relu` and create a new model. Compile
    and train a new model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a new layer for dropout after the LSTM layer and create a new model. Keep
    values such as `0.2` or `0.3` for dropout. Compile and train a new model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate and compare all the models that were trained in this activity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The solution to this activity can be found on page 141.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to evaluate our model using the MSE, RMSE, and
    MAPE metrics. We computed the latter two metrics in a series of 19-week predictions
    made by our first neural network model. By doing this, we learned that it was
    performing well.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to optimize a model. We looked at optimization techniques,
    which are typically used to increase the performance of neural networks. Also,
    we implemented a number of these techniques and created a few more models to predict
    Bitcoin prices with different error rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will be turning our model into a web application that
    does two things: retrains our model periodically with new data and is able to
    make predictions using an HTTP API interface.'
  prefs: []
  type: TYPE_NORMAL
