["```\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\nimport numpy as np\nimport gym \n```", "```\nenv = gym.make('CartPole-v0') \n```", "```\nstate_shape = env.observation_space.shape[0] \n```", "```\nnum_actions = env.action_space.n \n```", "```\ngamma = 0.95 \n```", "```\ndef discount_and_normalize_rewards(episode_rewards): \n```", "```\n discounted_rewards = np.zeros_like(episode_rewards) \n```", "```\n reward_to_go = 0.0\n    for i in reversed(range(len(episode_rewards))):\n        reward_to_go = reward_to_go * gamma + episode_rewards[i]\n        discounted_rewards[i] = reward_to_go \n```", "```\n discounted_rewards -= np.mean(discounted_rewards)\n    discounted_rewards /= np.std(discounted_rewards)\n\n    return discounted_rewards \n```", "```\nstate_ph = tf.placeholder(tf.float32, [None, state_shape], name=\"state_ph\") \n```", "```\naction_ph = tf.placeholder(tf.int32, [None, num_actions], name=\"action_ph\") \n```", "```\ndiscounted_rewards_ph = tf.placeholder(tf.float32, [None,], name=\"discounted_rewards\") \n```", "```\nlayer1 = tf.layers.dense(state_ph, units=32, activation=tf.nn.relu) \n```", "```\nlayer2 = tf.layers.dense(layer1, units=num_actions) \n```", "```\nprob_dist = tf.nn.softmax(layer2) \n```", "```\nneg_log_policy = tf.nn.softmax_cross_entropy_with_logits_v2(logits = layer2, labels = action_ph) \n```", "```\nloss = tf.reduce_mean(neg_log_policy * discounted_rewards_ph) \n```", "```\ntrain = tf.train.AdamOptimizer(0.01).minimize(loss) \n```", "```\nnum_iterations = 1000 \n```", "```\nwith tf.Session() as sess: \n```", "```\n sess.run(tf.global_variables_initializer()) \n```", "```\n for i in range(num_iterations): \n```", "```\n episode_states, episode_actions, episode_rewards = [],[],[] \n```", "```\n done = False \n```", "```\n Return = 0 \n```", "```\n state = env.reset() \n```", "```\n while not done: \n```", "```\n state = state.reshape([1,4]) \n```", "```\n pi = sess.run(prob_dist, feed_dict={state_ph: state}) \n```", "```\n a = np.random.choice(range(pi.shape[1]), p=pi.ravel()) \n```", "```\n next_state, reward, done, info = env.step(a) \n```", "```\n env.render() \n```", "```\n Return += reward \n```", "```\n action = np.zeros(num_actions)\n            action[a] = 1 \n```", "```\n episode_states.append(state)\n            episode_actions.append(action)\n            episode_rewards.append(reward) \n```", "```\n state=next_state \n```", "```\n discounted_rewards= discount_and_normalize_rewards(episode_rewards) \n```", "```\n feed_dict = {state_ph: np.vstack(np.array(episode_states)),\n                     action_ph: np.vstack(np.array(episode_actions)),\n                     discounted_rewards_ph: discounted_rewards \n                    } \n```", "```\n loss_, _ = sess.run([loss, train], feed_dict=feed_dict) \n```", "```\n if i%10==0:\n            print(\"Iteration:{}, Return: {}\".format(i,Return)) \n```"]