- en: Influential Classification Tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After the deep learning breakthrough in 2012, research toward more refined classification
    systems based on **convolutional neural networks** (**CNNs**) gained momentum.
    Innovation is moving at a frantic pace nowadays, as more and more companies are
    developing smart products. Among the numerous solutions developed over the years
    for object classification, some have became famous for their contributions to
    computer vision. They have been derived and adapted for so many different applications
    that they have achieved must-know status, and so deserve their own chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In parallel with the advanced network architectures introduced by these solutions,
    other methods have been explored to better prepare CNNs for their specific tasks.
    So, in the second part of this chapter, we will look at how the knowledge acquired
    by networks on specific use cases can be transferred to new applications for enhanced
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What instrumental architectures such as VGG, inception, and ResNet have brought
    to computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How these solutions can be reimplemented or directly reused for classification
    tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What transfer learning is, and how to efficiently repurpose trained networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jupyter notebooks illustrating the concepts presented in this chapter can be
    found in the GitHub folder at [github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter04](https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter04).
  prefs: []
  type: TYPE_NORMAL
- en: 'The only new package introduced in this chapter is `tensorflow-hub`. Installation
    instructions can be found at [https://www.tensorflow.org/hub/installation](https://www.tensorflow.org/hub/installation)
    (though it is a single-line command with `pip`: `pip install tensorflow-hub`).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding advanced CNN architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Research in computer vision has been moving forward both through incremental
    contributions and large innovative leaps. Challenges organized by researchers
    and companies, inviting experts to submit new solutions in order to best solve
    a predefined task, have been playing a key role in triggering such instrumental
    contributions. The ImageNet classification contest (**ImageNet Large Scale Visual
    Recognition Challenge** *(***ILSVRC**); see [Chapter 1](3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml),
    *Computer Vision and Neural Networks*) is a perfect example. With its millions
    of images split into 1,000 fine-grained classes, it still represents a great challenge
    for daring researchers, even after the significant and symbolic victory of AlexNet
    in 2012.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will present some of the classic deep learning methods that
    followed AlexNet in tackling ILSVRC, covering the reasons leading to their development
    and the contributions they made.
  prefs: []
  type: TYPE_NORMAL
- en: VGG – a standard CNN architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first network architecture we will present is **VGG** (or *VGGNet*), developed
    by the *Visual Geometry Group* from Oxford University. Though the group only achieved
    second place in the ILSVRC classification task in 2014, their method influenced
    many later architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the VGG architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Looking at the motivation of the VGG authors, and then their contributions,
    we will present how the VGG architecture achieved higher accuracy with fewer parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AlexNet was a game changer, being the first CNN successfully trained for such
    a complex recognition task and making several contributions that are still valid
    nowadays, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The use of a **rectified linear unit** (***ReLU***) as an activation function,
    which prevents the vanishing gradient problem (explained later in this chapter),
    and thus improving training (compared to using sigmoid or tanh)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application of **dropout** to CNNs (with all the benefits covered in [Chapter
    3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern Neural Networks*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The typical CNN architecture combining blocks of convolution and pooling layers,
    with dense layers afterward for the final prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application of random transformations (image translation, horizontal flipping,
    and more) to synthetically augment the dataset (that is, augmenting the number
    of different training images by randomly editing the original samples—see [Chapter
    7](337ec077-c215-4782-b56c-beae4d94d718.xhtml), *Training on Complex and Scarce
    Datasets,* for more details)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Still, even back then, it was clear that this prototype architecture had room
    for improvement. The main motivation of many researchers was to try going deeper
    (that is, building a network composed of a larger number of stacked layers), despite
    the challenges arising from this. Indeed, more layers typically means more parameters
    to train, making the learning process more complex. As we will describe in the
    next paragraph, however, Karen Simonyan and Andrew Zisserman from Oxford's VGG
    group tackled this challenge with success. The method they submitted to ILSVRC
    2014 reached a top-5 error of 7.3%, dividing the 16.4% error of AlexNet by more
    than two!
  prefs: []
  type: TYPE_NORMAL
- en: '**Top-5 accuracy** is one of the main classification metrics of ILSVRC. It
    considers that a method has predicted properly if the correct class is among its
    five first guesses. Indeed, for many applications, it is fine to have a method
    that''s able to reduce a large number of class candidates to a lower number (for
    instance, to leave the final choice between the remaining candidates to an expert
    user). The top-5 metrics are a specific case of the more generic top-k metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In their paper (*Very Deep Convolutional Networks for Large-Scale Image Recognition*,
    *ArXiv, 2014*), Simonyan and Zisserman presented how they developed their network
    to be deeper than most previous ones. They actually introduced six different CNN
    architectures, from 11 to 25 layers deep. Each network is composed of five blocks
    of several consecutive convolutions followed by a max-pooling layer and three
    final dense layers (with dropout for training). All the convolutional and max-pooling
    layers have `SAME` for padding. The convolutions have *s = 1* for stride, and
    are using the *ReLU* function for activation. All in all, a typical VGG network
    is represented in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/444b8da7-80c0-47b2-a276-ff2fee8922f8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: VGG-16 architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The two most performant architectures, still commonly used nowadays, are called
    **VGG-16** and **VGG-19**. The numbers (16 and 19) represent the *depth* of these
    CNN architectures; that is, the number of *trainable* layers stacked together.
    For example, as shown in *Figure 4.1*, VGG-16 contains 13 convolutional layers
    and 3 dense ones, hence a depth of 16 (excluding the non-trainable operations;
    that is, the 5 max-pooling and 2 dropout layers). The same goes for VGG-19, which
    is composed of three additional convolutions. VGG-16 has approximately 138 million
    parameters, and VGG-19 has 144 million. Those numbers are quite high, although,
    as we will demonstrate in the following section, the VGG researchers took a new
    approach to keep these values in check despite the depth of their architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Contributions – standardizing CNN architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following paragraphs, we will summarize the most significant contributions
    introduced by these researchers while further detailing their architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing large convolutions with multiple smaller ones
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The authors began with a simple observation—a stack of two convolutions with
    *3* × *3* kernels has the same receptive field as a convolution with *5* × *5*
    kernels (refer to [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml)*, Modern
    Neural Networks*, for the **effective receptive field** (**ERF**) formula).
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, three consecutive *3* × *3* convolutions result in a *7* × *7* receptive
    field, and five *3* × *3* operations result in an *11* × *11* receptive field.
    Therefore, while AlexNet has large filters (up to *11* × *11*), the VGG network
    contains more numerous but smaller convolutions for a larger ERF. The benefits
    of this change are twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: '**It decreases the number of parameters**: Indeed, the *N* filters of an *11*
    × *11* convolution layer imply *11* × *11* × *D* × *N* = *121D**N* values to train
    just for their kernels (for an input of depth *D*), while five *3* × *3* convolutions
    have a total of *1* × *(3* × *3* × *D* × *N**)* + *4* × *(3* × *3* × *N* × *N**)*
    = *9**D**N* + *36**N²* weights for their kernels. As long as *N* < *3.6**D*, this
    means fewer parameters. For instance, for *N* = *2**D*, the number of parameters
    drops from *242**D²* to *153**D²* (refer to the previous equations). This makes
    the network easier to optimize, as well as much lighter (we invite you to look
    at the decrease for the replacements of the *7* × *7* and *5* × *5* convolutions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**It increases the non-linearity**: Having a larger number of convolution layers—each
    followed by a *non-linear* activation function such as *ReLU*—increases the networks''
    capacity to learn complex features (that is, by combining more non-linear operations).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, replacing larger convolutions with small, consecutive ones allowed
    the VGG authors to effectively go deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the depth of the feature maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Based on another intuition, the VGG authors doubled the depth of the feature
    maps for each block of convolutions (from 64 after the first convolution to 512).
    As each set is followed by a max-pooling layer with a *2* × *2* window size and
    a stride of 2, the depth doubles while the spatial dimensions are halved.
  prefs: []
  type: TYPE_NORMAL
- en: This allows the encoding of spatial information into more and more complex and
    discriminative features for classification.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting data with scale jittering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Simonyan and Zisserman also introduced a **data augmentation** mechanism that
    they named **scale jittering**. At each training iteration, they randomly scale
    the batched images (from 256 pixels to 512 pixels for their smaller side) before
    cropping them to the proper input size (*224* × *224* for their ILSVRC submission).
    With this random transformation, the network will be confronted with samples with
    different scales and will learn to properly classify them despite this scale jittering
    (refer to *Figure 4.2*). The network becomes more robust as a result, as it is
    trained on images covering a larger range of realistic transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation is the procedure of synthetically increasing the size of training
    datasets by applying random transformations to their images in order to create
    different versions. Details and concrete examples are provided in [Chapter 7](337ec077-c215-4782-b56c-beae4d94d718.xhtml),
    *Training on Complex and Scarce Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: The authors also suggested applying random scaling and cropping at test time.
    The idea is to generate several versions of the query image this way and to feed
    them all to the network, with the intuition that it increases the chance of feeding
    content on a scale the network is particularly used to. The final prediction is
    obtained by averaging the results for each version.
  prefs: []
  type: TYPE_NORMAL
- en: 'In their paper, they demonstrate how this process tends to also improve accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8a127497-cc9f-4714-b826-45b6fd79ada8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Example of scale jittering. Notice that it is common to not preserve
    the aspect ratio of the content to further transform the images'
  prefs: []
  type: TYPE_NORMAL
- en: The same principle was previously used by the AlexNet authors. During both training
    and testing, they were generating several versions of each image with different
    combinations of cropping and flipping transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Replacing fully connected layers with convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the classic VGG architecture ends with several **fully connected** (**FC**)
    layers (such as AlexNet), the authors suggest an alternative version. In this
    version, the dense layers are replaced by convolutional ones.
  prefs: []
  type: TYPE_NORMAL
- en: The first set of convolutions with larger kernels (*7* × *7* and *3* × *3*)
    reduces the spatial size of the feature maps to *1* × *1* (with no padding applied
    beforehand) and increases their depth to 4,096\. Finally, a *1* × *1* convolution
    is used with as many filters as classes to predict from (that is, *N* = 1,000
    for ImageNet). The resulting *1* × *1* × *N* vector is normalized with the `softmax`
    function, and then flattened into the final class predictions (with each value
    of the vector representing the predicted class probability).
  prefs: []
  type: TYPE_NORMAL
- en: '*1* × *1* convolutions are commonly used to change the depth of the input volume
    without affecting its spatial structure. For each spatial position, the new values
    are interpolated from all the depth values at that position.'
  prefs: []
  type: TYPE_NORMAL
- en: Such a network without any dense layers is called a **fully convolutional network **(**FCN**).
    As mentioned in [Chapter 3](dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml), *Modern
    Neural Networks*, and as has been highlighted by the VGG authors, FCNs can be
    applied to images of different sizes, with no need for cropping beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly, to achieve the best accuracy for ILSVRC, the authors trained
    and used both versions (normal and FCN), once again averaging their results to
    obtain the final predictions. This technique is named **model averaging** and
    is frequently used in production.
  prefs: []
  type: TYPE_NORMAL
- en: Implementations in TensorFlow and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks to the efforts that the authors put into creating a clear architecture,
    VGG-16 and VGG-19 are among the simplest classifiers to reimplement. Example code
    can be found in the GitHub folder for this chapter, for educational purposes.
    However, in computer vision, as in many domains, it is always preferable not to
    reinvent the wheel and to instead reuse existing tools that are available. The
    following paragraphs present different preimplemented VGG solutions that you can
    directly adapt and reuse.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While TensorFlow itself does not offer any official implementation of the VGG
    architectures, neatly implemented VGG-16 and VGG-19 networks are available in
    the `tensorflow/models` GitHub repository ([https://github.com/tensorflow/models](https://github.com/tensorflow/models)).
    This repository, maintained by TensorFlow contributors, contains numerous well-curated
    state-of-the-art or experimental models. It is often recommended that you should
    search this repository when looking for a specific network.
  prefs: []
  type: TYPE_NORMAL
- en: We invite our readers to have a look at the VGG code there (currently available
    at [https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/slim/python/slim/nets/vgg.py)),
    as it reimplements the FCN version we described earlier.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Keras API has an official implementation of these architectures, accessible
    via its `tf.keras.applications` package (refer to the documentation at [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications)).
    This package contains several other well-known models and provides *pre trained*
    parameters for each (that is, parameters saved from prior training on a specific
    dataset). For instance, you can instantiate a VGG network with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: With these default arguments, Keras instantiates the VGG-16 network and loads
    the persisted parameter values obtained after a complete training cycle on ImageNet.
    With this single command, we have a network ready to classify images into the
    1,000 ImageNet categories. If we would like to retrain the network from scratch
    instead, we should fix `weights=None` and Keras will randomly set the weights.
  prefs: []
  type: TYPE_NORMAL
- en: In Keras terminology, the *top* layers correspond to the final consecutive dense
    layers. Therefore, if we set `include_top=False`, the VGG dense layers will be
    excluded, and the network's outputs will be the feature maps of the last convolution/max-pooling
    block. This can be useful if we want to reuse the pre trained VGG network to extract
    meaningful features (which can be applied to more advanced tasks), and not just
    for classification. The `pooling` function parameter can be used in those cases
    (that is, when `include_top=False`) to specify an optional operation to be applied
    to the feature maps before returning them (`pooling='avg'` or `pooling='max'`
    to apply a global average- or max- pooling).
  prefs: []
  type: TYPE_NORMAL
- en: GoogLeNet and the inception module
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developed by researchers at Google, the architecture we will now present was
    also applied to ILSVRC 2014 and won first place for the classification task ahead
    of VGGNet. **GoogLeNet** (for *Google* and *LeNet*, as an homage to this pioneering
    network) is structurally very different from its linear challenger, introducing
    the notion of *inception blocks* (the network is also commonly called an **inception
    network**).
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the GoogLeNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we will see in the following section, the GoogLeNet authors, Christian Szegedy
    and others, approached the conception of a more efficient CNN from a very different
    angle than the VGG researchers (*Going Deeper with Convolutions*, Proceedings
    of the CVPR IEEE conference, 2014).
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While VGG's authors took AlexNet and worked on standardizing and optimizing
    its structure in order to obtain a clearer and deeper architecture, researchers
    at Google took a different approach. Their first consideration, as mentioned in
    the paper, was the optimization of the CNN computational footprint.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, in spite of careful engineering (refer to VGG), the deeper CNNs are,
    the larger their number of trainable parameters and their number of computations
    per prediction become (it is costly with respect to memory and time). For instance,
    VGG-16 weighs approximately 93 MB (in terms of parameter storage), and the VGG
    submission for ILSVRC took two to three weeks to train on four GPUs. With approximately
    5 million parameters, GoogLeNet is 12 times lighter than AlexNet and 21 times
    lighter than VGG-16, and the network was trained within a week. As a result, GoogLeNet—and
    more recent inception networks—can even run on more modest machines (such as smartphones),
    which contributed to their lasting popularity.
  prefs: []
  type: TYPE_NORMAL
- en: We have to keep in mind that, despite this impressive reduction in the numbers
    of parameters and operations, GoogLeNet did win the classification challenge in
    2014 with a top-5 error of 6.7% (against 7.3% with VGG). This performance is the
    result of the second target of Szegedy and others—the conception of a network
    that was not only deeper but also larger, with blocks of parallel layers for *multiscale
    processing*. While we will detail this solution later in this chapter, the intuition
    behind it is simple. Building a CNN is a complex, iterative task. How do we know
    which layer (such as convolutional or pooling) should be added to the stack in
    order to improve the accuracy? How do we know which kernel size would work best
    for a given layer? After all, kernels of different sizes will not react to features
    of the same scale. How can we avoid such a trade-off? A solution, according to
    the authors, is to use the *inception modules* they developed, composed of several
    different layers working in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As shown in *Figure 4.3*, GoogLeNet architecture is not as straightforward as
    the previous architectures we studied, although it can be analyzed region by region.
    The input images are first processed by a classic series of convolutional and
    max-pooling layers. Then, the information goes through a stack of nine inception
    modules. These modules (often called **subnetworks**; further detailed in *Figure
    4.4*), are blocks of layers stacked vertically and horizontally. For each module,
    the input feature maps are passed to four parallel sub-blocks composed of one
    or two different layers (convolutions with different kernel sizes and max-pooling).
  prefs: []
  type: TYPE_NORMAL
- en: 'The results of these four parallel operations are then concatenated together
    along the depth dimension and into a single feature volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/994f1060-1599-4d5a-96e6-c61b06fc087e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: GoogLeNet architecture. The inception modules are detailed in *Figure
    4.4*'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding figure, all the convolutional and max-pooling layers have `SAME` for
    padding. The convolutions have *s = 1* for stride if unspecified and are using
    the *ReLU* function for activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'This network is composed of several layer blocks sharing a similar structure
    with parallel layers—the inception modules. For instance, the first inception
    module, represented in *Figure 4.3*, receives a feature volume of size *28* ×
    *28* × *192 *for input. Its first parallel sub-block, composed of a single *1*
    × *1* convolution output (*N* = 64 and *s* = 1), thus generates a *28* × *28* ×
    *64* tensor. Similarly, the second sub-module, composed of two convolutions, outputs
    a *28* × *28* × *128* tensor; and the two remaining ones output a *28* × *28* ×
    *32* and a *28* × *28* × *32* feature volume, respectively. Therefore, by stacking
    these four results together along the last dimension, the first inception module
    outputs a *28* × *28* × *256* tensor, which is then passed to the second module,
    and so on. In the following diagram, the naive solution is represented on the
    left, and the module used in GoogLeNet (that is, the inception module v1) is shown
    on the right (note that in GoogLeNet, the number of filters *N* increases the
    deeper the module is):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79d097f6-6abf-4bc6-8e4f-48bf0a4c09c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Inception modules: naive versus actual'
  prefs: []
  type: TYPE_NORMAL
- en: The features of the last module are average, pooled from *7* × *7* × *1,024*
    to *1* × *1* × *1,024*, and are finally densely converted into the prediction
    vector. As shown in *Figure 4.3*, the network is further composed of two auxiliary
    branches, also leading to predictions. Their purpose will be detailed in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: In total, GoogLeNet is a 22-layer deep architecture (counting the trainable
    layers only), with a total of more than 60 convolutional and FC layers. And yet,
    this much larger network has 12 times fewer parameters than AlexNet.
  prefs: []
  type: TYPE_NORMAL
- en: Contributions – popularizing larger blocks and bottlenecks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The low number of parameters, as well as the network's performance, are the
    results of several concepts implemented by the GoogLeNet authors. We will cover
    the main ones in this section.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will present only the key concepts differentiating the inception
    networks from the ones we introduced previously. Note that the GoogLeNet authors
    reapplied several other techniques that we have already covered, such as the prediction
    of multiple crops for each input image and the use of other image transformations
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: Capturing various details with inception modules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduced by Min Lin and others in their influential **Network in Network** (**NIN**)
    paper in 2013, the idea of having a CNN composed of sub-network modules was adapted
    and fully exploited by the Google team. As previously mentioned and shown in *Figure
    4.4*, the basic inception modules they developed are composed of four parallel
    layers—three convolutions with filters of size *1* × *1*, *3* × *3*, and *5* ×
    *5*, respectively, and one max-pooling layer with stride `1`. The advantages of
    this parallel processing, with the results concatenated together after, are numerous.
  prefs: []
  type: TYPE_NORMAL
- en: As explained in the Motivation sub-section, this architecture allows for the
    multiscale processing of the data. The results of each inception module combine
    features of different scales, capturing a wider range of information. We do not
    have to choose which kernel size may be the best (such a choice would require
    several iterations of training and testing cycles), that is, the network learns
    by itself which convolutions to rely on more for each module.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, while we presented how vertically stacking layers with non-linear
    activation functions positively affects a network's performance, this is also
    true for horizontal combinations. The concatenation of features mapped from different
    layers further adds to the non-linearity of the CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Using 1 x 1 convolutions as bottlenecks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Though not a contribution *per se*, Szegedy et al. made the following technique
    notorious by efficiently applying it to their network.
  prefs: []
  type: TYPE_NORMAL
- en: As previously mentioned in the *Replacing fully connected layers with convolutions*
    section, *1* × *1* convolutional layers (with a stride of 1) are often used to
    change the overall depth of input volumes without affecting their spatial structures.
    Such a layer with *N* filters would take an input of shape *H* × *W* × *D* and
    return an interpolated *H* × *W* × *N* tensor. For each pixel in the input image,
    its *D* channel values will be interpolated by the layer (according to its filter
    weights) into *N* channel values.
  prefs: []
  type: TYPE_NORMAL
- en: This property can be applied to reduce the number of parameters required for
    larger convolutions by compressing the features' depth beforehand (using *N* <
    *D*). This technique basically uses *1* × *1* convolutions as **bottlenecks**
    (that is, as intermediary layers reducing the dimensionality and, thus, the number
    of parameters). Since activations in neural networks are often redundant or left
    unused, such bottlenecks usually barely affect the performance (as long as they
    do not drastically reduce the depth). Moreover, GoogLeNet has its parallel layers
    to compensate for the depth reduction. Indeed, in inception networks, bottlenecks
    are present in every module, before all larger convolutions and after max-pooling
    operations, as illustrated in *Figure 4.4*.
  prefs: []
  type: TYPE_NORMAL
- en: Given the *5* × *5* convolution in the first inception module (taking as input
    a *28* × *28* × *192* volume) for example, the tensor containing its filters would
    be of the dimension *5* × *5* × *192* × *32* in the naive version. This represents
    153,600 parameters just for this convolution. In the first version of the inception
    module (that is, with bottlenecks), a *1* × *1* convolution is introduced before
    the 5 × 5 one, with *N* = 16\. As a result, the two convolutions require a total
    of *1* × *1* × *192* × *16* + *5* × *5* × *16* × *32* = *15,872* trainable values
    for their kernels. This is 10 times fewer parameters than the previous version
    (just for this single *5* × *5* layer), for the same output size! Furthermore,
    as mentioned already, the addition of layers with a non-linear activation function
    (*ReLU*) further improves the networks' ability to grasp complex concepts.
  prefs: []
  type: TYPE_NORMAL
- en: We are presenting GoogLeNet as submitted to ILSVRC 2014 in this chapter. More
    commonly named **Inception V1**, this architecture has been refined by its authors
    since then. **Inception V2** and **Inception V3** contain several improvements,
    such as replacing the *5 × 5* and *7 × 7* convolutions by smaller ones (as done
    in VGG), improving the bottlenecks' hyperparameters to reduce the information
    loss, and adding *BatchNorm* layers.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling instead of fully connecting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another solution used by the inception authors to reduce the number of parameters
    was to use an average-pooling layer instead of a fully connected one after the
    last convolutional block. With a *7* × *7* window size and stride of 1, this layer
    reduces the feature volume from *7* × *7* × *1,024* to *1* × *1* × *1,024* without
    any parameter to train. A dense layer would have added (*7* × *7* × *1,024*) ×
    1,024 = *51,380,224* parameters. Though the network loses a bit in expressiveness
    with this replacement, the computational gain is enormous (and the network already
    contains enough non-linear operations to capture the information it needs for
    the final prediction).
  prefs: []
  type: TYPE_NORMAL
- en: The last and only FC layer in GoogLeNet has *1,024* × *1,000* = *1,024,000*
    parameters, a fifth of the total number the network has!
  prefs: []
  type: TYPE_NORMAL
- en: Fighting vanishing gradient with intermediary losses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As briefly mentioned when introducing the architecture, GoogLeNet has two auxiliary
    branches at training time (removed after), also leading to predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Their purpose is to improve the propagation of the loss through the network
    during training. Indeed, deeper CNNs are often plagued with **vanishing gradient**.
    Many CNN operations (for instance, *sigmoid*) have derivatives with small amplitudes
    (below one). Therefore, the higher the number of layers, the smaller the product
    of the derivatives becomes when backpropagating (as more values below one are
    multiplied together, the closer to zero the result will become). Often, the gradient
    simply vanishes/shrinks to zero when reaching the first layers. Since the gradient
    values are directly used to update the parameters, these layers won't effectively
    learn if the gradient is too small.
  prefs: []
  type: TYPE_NORMAL
- en: The opposite phenomenon—the **exploding gradient** problem—can also happen with
    deeper networks. When operations whose derivatives can take on larger magnitudes
    are used, their product during backpropagation can become so big that it makes
    the training unstable (with huge, erratic weight updates) or it can even sometimes
    overflow (`NaN` values).
  prefs: []
  type: TYPE_NORMAL
- en: The down-to-earth, yet effective, solution to this problem implemented here
    is to reduce the distance between the first layers and predictions, by introducing
    additional classification losses at various network depths. If the gradient from
    the final loss cannot flow properly to the first layers, these will still be trained
    to help with classification thanks to the closer intermediary losses. Incidentally,
    this solution also slightly improves the robustness of the layers affected by
    multiple losses, as they must learn to extract discriminative features that are
    not only useful to the main network, but also to the shorter branches.
  prefs: []
  type: TYPE_NORMAL
- en: Implementations in TensorFlow and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the inception architecture may look complex to implement at first glance,
    we already have most of the tools to do so. Moreover, several pretrained versions
    are also made available by TensorFlow and Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Inception module with the Keras Functional API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The networks we have implemented so far were purely sequential, with a single
    path from inputs to predictions. The inception model differs from those, with
    its multiple parallel layers and branches. This gives us the opportunity to demonstrate
    that such operational graphs are not much more difficult to instantiate with the
    available APIs. In the following section, we will write an inception module using
    the Keras Functional API (refer to the documentation at [https://keras.io/getting-started/sequential-model-guide/](https://keras.io/getting-started/sequential-model-guide/)).
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have mostly been using the Keras Sequential API, which is not well-adapted
    for multipath architectures (as its name implies). The Keras Functional API is
    closer to the TensorFlow paradigm, with Python variables for the layers being
    passed as parameters to the next ones to build a graph. The following code presents
    a simplistic model implemented with both APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With the functional API, a layer can easily be passed to multiple others, which
    is what we need for the parallel blocks of the inception modules. Their results
    can then be merged together using a `concatenate` layer (refer to the documentation
    at [https://keras.io/layers/merge/#concatenate_1](https://keras.io/layers/merge/#concatenate_1)).
    Therefore, the naive inception block presented in *Figure 4.4* can be implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: We will leave it to you to adapt this code to implement the proper modules for
    Inception V1 by adding the bottleneck layers.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow model and TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Google offers several scripts and tutorials explaining how to directly use its
    inception networks, or how to retrain them for new applications. The directory
    dedicated to this architecture in the `tensorflow/models` Git repository ([https://github.com/tensorflow/models/tree/master/research/inception](https://github.com/tensorflow/models/tree/master/research/inception))
    is also rich and well-documented. Moreover, a pretrained version of Inception
    V3 is available on **TensorFlow Hub**, which gives us the opportunity to introduce
    this platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow Hub is a repository of pretrained models. In a similar way to how
    Docker allows people to easily share and reuse software packages, removing the
    need to reconfigure distributions, TensorFlow Hub gives access to pretrained models
    so that people do not have to spend time and resources reimplementing and retraining.
    It combines a website ([https://tfhub.dev](https://tfhub.dev)) where people can
    search for specific models (depending, for example, on the target recognition
    task), and a Python package to easily download and start using these models. For
    instance, we can fetch and set up an Inception V3 network as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Though this code is quite succinct, a lot is happening. A preliminary step was
    to browse the [tfhub.dev](https://tfhub.dev) website and decide on a model there.
    On the page presenting the selected model ([https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2](https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/2);
    stored in `model_url`), we can read that the inception model we chose is defined
    as an **image feature vector** that expects *299 *× *299 *× *3* inputs, among
    other details. To use a TensorFlow Hub model, we need to know how to interface
    with it.
  prefs: []
  type: TYPE_NORMAL
- en: The *image feature vector* type tells us that this network returns extracted
    features; that is, the results of the last convolutional block before the dense
    operations. With such a model, it is up to us to add the final layers (for instance,
    so that the output size corresponds to the number of considered classes).
  prefs: []
  type: TYPE_NORMAL
- en: The latest versions of the TensorFlow Hub interface seamlessly with Keras, and
    a complete pretrained TensorFlow Hub model can be fetched and instantiated as
    a Keras layer thanks to `tensorflow_hub.KerasLayer(model_url, trainable, ...)`.
    Like any Keras layer, it can then be used inside larger Keras models or TensorFlow
    estimators.
  prefs: []
  type: TYPE_NORMAL
- en: Though this may not seem as straightforward as using the Keras Applications
    API, TensorFlow Hub has an exotic catalog of models, which is destined to increase
    over time.
  prefs: []
  type: TYPE_NORMAL
- en: One of the Jupyter notebooks available in the Git repository is dedicated to
    TensorFlow Hub and its usage.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with VGG, Keras provides an implementation of Inception V3, optionally, with
    weights pretrained on ImageNet. `tf.keras.applications.InceptionV3()` (refer to
    the documentation at [https://keras.io/applications/#inceptionv3](https://keras.io/applications/#inceptionv3))
    has the same signature as the one presented for VGG.
  prefs: []
  type: TYPE_NORMAL
- en: We have mentioned AlexNet, the winning solution of ILSVRC 2012, as well as VGGNet
    and GoogLeNet, which prevailed during the 2014 edition. You might be wondering
    who won in 2013\. The challenge that year was dominated by the **ZFNet** architecture
    (named after its creators, Matthew Zeiler and Rob Fergus from New York University).
    If ZFNet is not covered in this chapter, it is because its architecture was not
    particularly innovative, and has not really been reused afterward.
  prefs: []
  type: TYPE_NORMAL
- en: However, Zeiler and Fergus' significant contribution lay somewhere else—they
    developed and applied several operations to the visualization of CNNs (such as **unpooling**
    and **transposed convolution**, also known as **deconvolution**, which are both
    detailed in [Chapter 6](c4bb2429-f9f5-424d-8462-e376fd81f5a4.xhtml), *Enhancing
    and Segmenting Images*). Indeed, a common criticism of neural networks was that
    they behave like *black boxes*, and that no one can really grasp why and how they
    work so well. Zeiler and Fergus' work was an important first step toward opening
    up CNNs to reveal their inner processes (such as how they end up reacting to particular
    features and how they learn more abstract concepts as they go deeper.) Visualizing
    how each layer of their network reacted to specific images and contributed to
    the final prediction, the authors were able to optimize its hyperparameters and
    thus improve its performance (*Visualizing and Understanding Convolutional Networks*,
    Springer, 2014).
  prefs: []
  type: TYPE_NORMAL
- en: Research toward understanding neural networks is still ongoing (for instance,
    with a multitude of recent work capturing and analyzing the *attention* of networks
    toward specific elements) and has already greatly helped to improve current systems.
  prefs: []
  type: TYPE_NORMAL
- en: ResNet – the residual network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last architecture we will address in this chapter won the 2015 edition of
    ILSVRC. Composed of a new kind of module, the residual module, **ResNet** (**residual
    network**) provides an efficient approach to creating very deep networks, beating
    larger models such as Inception in terms of performance.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the ResNet architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Developed by Kaiming He et al., researchers at Microsoft, the ResNet architecture
    is an interesting solution to learning problems affecting CNNs. Following the
    structure of previous sections, we will first clarify the author's targets and
    introduce their novel architecture (refer to *Deep Residual Learning for Image
    Recognition*, Proceedings of the CVPR IEEE conference, 2016).
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Inception networks demonstrated that going larger is a valid strategy in image
    classification, as well as other recognition tasks. Nevertheless, experts still
    kept trying to increase networks in order to solve more and more complex tasks.
    However, the question *Is learning better networks as easy as stacking more layers?*,
    asked in the preamble of the paper written by He et al., is justified.
  prefs: []
  type: TYPE_NORMAL
- en: We know already that the deeper a network goes, the harder it becomes to train
    it. But besides the *vanishing/exploding gradient* problems (covered by other
    solutions already), He et al. pointed out another problem that deeper CNNs face—*performance
    degradation*. It all started with a simple observation—the accuracy of CNNs does
    not linearly increase with the addition of new layers. A degradation problem appears
    as the networks' depth increases. Accuracy starts saturating and even degrading.
    Even the training loss starts decreasing when negligently stacking too many layers,
    proving that the problem is not caused by overfitting. For instance, the authors
    compared the accuracy of an 18-layer-deep CNN with a 34-layer one, showing that
    the latter performs worse than the shallower version during and after training.
    In their paper, He et al. proposed a solution to build very deep and performant
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: With *model averaging* (applying ResNet models of various depths) and *prediction
    averaging* (over multiple crops of each input image), the ResNet authors reached
    a historically low 3.6% top-5 error rate for the ILSVRC challenge. This was the
    first time an algorithm beat humans on that dataset. Human performance had been
    measured by the challenge organizers, with the best human candidate reaching a
    5.1% error rate (refer to *ImageNet Large-Scale Visual Recognition Challenge*,
    Springer, 2015). Achieving super-human performance on such a task was a huge milestone
    for deep learning. We should, however, keep in mind that, while algorithms can
    expertly solve a specific task, they still do not have the human ability to extend
    that knowledge to others, or to grasp the context of the data they are to deal
    with.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Like Inception, ResNet has known several iterative improvements to its architecture,
    for instance, with the addition of bottleneck convolutions or the use of smaller
    kernels. Like VGG, ResNet also has several pseudo-standardized versions characterized
    by their depth: ResNet-18, ResNet-50, ResNet-101, ResNet-152, and others. Indeed,
    the winning ResNet network for ILSVRC 2015 vertically stacked 152 trainable layers
    (with a total of 60 million parameters), which was an impressive feat at that
    time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1cdf0cd3-2fb2-4866-bd2c-76b2b0aa52bf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Exemplary ResNet architecture'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, all the convolutional and max-pooling layers have
    `SAME` for padding, and for stride *s = 1* if unspecified. Batch normalization
    is applied after each *3* × *3* convolution (on the residual path, in gray), and
    *1* × *1* convolutions (on the mapping path in black) have no activation function
    (identity).
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 4.5*, the ResNet architecture is slimmer than the Inception
    architecture, though it is similarly composed of layer blocks with parallel operations.
    Unlike Inception, where each parallel layer non-linearly processes the input information,
    ResNet blocks are composed of one non-linear path, and one identity path. The
    former (represented by the thinner gray arrows in *Figure 4.5*) applies a couple
    of convolutions with batch normalization and *ReLU* activation to the input feature
    maps. The latter (represented by the thicker black arrows) simply forward the
    features without applying any transformation.
  prefs: []
  type: TYPE_NORMAL
- en: The last statement is not always true. As shown in *Figure 4.5*, *1* × *1* convolutions
    are applied in order to adapt the depth of the features, when the depth is increased
    in parallel by the non-linear branches. On those occasions, to avoid a large increase
    in the number of parameters, the spatial dimensionality is also reduced on both
    sides using a stride of *s* = 2.
  prefs: []
  type: TYPE_NORMAL
- en: As in inception modules, the feature maps from each branch (that is, the transformed
    features and the original ones) are merged together before being passed to the
    next block. Unlike inception modules, however, this merging is not performed through
    depth concatenation, but through element-wise addition (a simple operation that
    does not require any additional parameters). We will cover, in the following section,
    the benefits of these residual blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in most implementations, the last *3* × *3* convolution of each residual
    block is not followed directly by *ReLU* activation. Instead, the non-linear function
    is applied after merging with the identity branch is done.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the features from the last block are average-pooled and densely converted
    into predictions, as in GoogLeNet.
  prefs: []
  type: TYPE_NORMAL
- en: Contributions – forwarding the information more deeply
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Residual blocks have been a significant contribution to machine learning and
    computer vision. In the following section, we will cover the reasons for this.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating a residual function instead of a mapping
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As the ResNet authors pointed out, the degradation phenomenon would not happen
    if layers could easily learn **identity mapping** (that is, if a set of layers
    could learn weights so that their series of operations finally return the same
    tensors as the input layers).
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the authors argue that, when adding some layers on top of a CNN, we
    should at least obtain the same training/validation errors if these additional
    layers were able to converge to the identity function. They would learn to at
    least pass the result of the original network without degrading it. Since that
    is not the case—as we can often observe a degradation—it means that identity mapping
    is not easy to learn for CNN layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'This led to the idea of introducing residual blocks, with two paths:'
  prefs: []
  type: TYPE_NORMAL
- en: One path further processes the data with some additional convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One path performs the identity mapping (that is, forwarding the data with no
    changes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We may intuitively grasp how this can solve the degradation problem. When adding
    a residual block on top of a CNN, its original performance can at least be preserved
    by setting the weights of the processing branch to zero, leaving only the predefined
    identity mapping. The processing path will only be considered if it benefits loss
    minimization.
  prefs: []
  type: TYPE_NORMAL
- en: The data forwarding path is usually called **skip** or **shortcut**. The processing
    one is commonly called **residual** **path**, since the output of its operations
    is then added to the original input, with the magnitude of the processed tensor
    being much smaller than the input one when the identity mapping is close to optimal
    (hence, the term *residual*). Overall, this residual path only introduces small
    changes to the input data, making it possible to forward patterns to deeper layers.
  prefs: []
  type: TYPE_NORMAL
- en: In their paper, He et al. demonstrate that their architecture not only tackles
    the degradation problem, but their ResNet models achieve better accuracy than
    traditional ones for the same number of layers.
  prefs: []
  type: TYPE_NORMAL
- en: Going ultra-deep
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is also worth noting that residual blocks do not contain more parameters
    than traditional ones, as the skip and addition operations do not require any.
    They can, therefore, be efficiently used as building blocks for *ultra-deep* networks.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the 152-layer network applied to the ImageNet challenge, the authors
    illustrated their contributions by training an impressive 1,202-layer one. They
    reported no difficulty training such a massive CNN (although its validation accuracy
    was slightly lower than for the 152-layer network, allegedly because of overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: More recent works have been exploring the use of residual computations to build
    deeper and more efficient networks, such as **Highway** networks (with a trainable
    switch value to decide which path should be used for each residual block) or **DenseNet**
    models (adding further skip connections between blocks).
  prefs: []
  type: TYPE_NORMAL
- en: Implementations in TensorFlow and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As with previous architectures, we already have the tools needed to reimplement
    ResNet ourselves, while also having direct access to preimplemented/pretrained
    versions.
  prefs: []
  type: TYPE_NORMAL
- en: Residual blocks with the Keras Functional API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As practice, let's implement a basic residual block ourselves. As shown in *Figure
    4.5*, the residual path consists of two convolutional layers, each one followed
    by batch normalization. The *ReLU* activation function is applied directly after
    the first convolution. For the second, the function is only applied after merging
    with the other path. Using the Keras Functional API, the residual path can thus
    be implemented in a matter of five or six lines, as demonstrated in the following
    code.
  prefs: []
  type: TYPE_NORMAL
- en: The shortcut path is even simpler. It contains either no layer at all, or a
    single *1* × *1* convolution to reshape the input tensor when the residual path
    is altering its dimensions (for instance, when a larger stride is used).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the results of the two paths are added together, and the *ReLU* function
    is applied to the sum. All in all, a basic residual block can be implemented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: A more elegant function is presented in one of the Jupyter notebooks. This notebook
    also contains a complete implementation of the ResNet architecture and a brief
    demonstration of a classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow model and TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like the Inception networks, ResNet ones have their own official implementation
    provided in the `tensorflow/models` Git repository, as well as their own pretrained
    TensorFlow Hub modules.
  prefs: []
  type: TYPE_NORMAL
- en: We invite you to check out the official `tensorflow/models` implementation,
    as it offers several types of residual blocks from more recent research efforts.
  prefs: []
  type: TYPE_NORMAL
- en: The Keras model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, Keras once again provides its own ResNet implementations—for instance,
    `tf.keras.applications.ResNet50()` (refer to the documentation at [https://keras.io/applications/#resnet50](https://keras.io/applications/#resnet50))—with
    the option to load parameters pretrained on ImageNet. These methods have the same
    signature as previously covered Keras applications.
  prefs: []
  type: TYPE_NORMAL
- en: The complete code for the usage of this Keras application is also provided in
    the Git repository.
  prefs: []
  type: TYPE_NORMAL
- en: The list of CNN architectures presented in this chapter does not pretend to
    be exhaustive. It has been curated to cover solutions both instrumental to the
    computer vision domain and of pedagogical value.
  prefs: []
  type: TYPE_NORMAL
- en: As research in visual recognition keeps moving forward at a fast pace, more
    advanced architectures are being proposed, building upon previous solutions (as
    Highway and DenseNet methods do for ResNet, for instance), merging them (as with
    the Inception-ResNet solution), or optimizing them for particular use cases (such
    as the lighter MobileNet, which was made to run on smartphones). It is, therefore,
    always a good idea to check what the state of the art has to offer (for example,
    on official repositories or research journals) before trying to reinvent the wheel.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This idea of reusing knowledge provided by others is not only important in computer
    science. The development of human technology over the millennia is the result
    of our ability to transfer knowledge from one generation to another, and from
    one domain to another. Many researchers believe that applying this guidance to
    machine learning could be one of the keys to developing more proficient systems
    that will be able to solve new tasks without having to relearn everything from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, this section will present what **transfer learning** means for artificial
    neural networks, and how it can be applied to our models.
  prefs: []
  type: TYPE_NORMAL
- en: Overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first introduce what transfer learning is and how it is performed in
    deep learning, depending on the use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Definition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first part of this chapter, we presented several well-known CNNs, developed
    for the ImageNet classification challenge. We mentioned that these models are
    commonly repurposed for a broader range of applications. In the following pages,
    we will finally elaborate on the reasons behind this reconditioning and how it
    is performed.
  prefs: []
  type: TYPE_NORMAL
- en: Human inspiration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like many developments in machine learning, transfer learning is inspired by
    our own human way of tackling complex tasks and gathering knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned in the introduction of this section, the first inspiration is our
    ability as a species to transfer knowledge from one individual to another. Experts
    can efficiently transfer the precious knowledge they have gathered over the years
    to a large number of students through oral or written teaching. By harnessing
    the knowledge that has been accumulated and distilled generation after generation,
    human civilizations have been able to continuously refine and extend their technical
    abilities. Phenomena that took millennia for our ancestors to understand— such
    as human biology, the solar system, and more—became common knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, as individuals, we also have the ability to transfer some expertise
    from one task to another. For example, people mastering one foreign language have
    an easier time learning similar ones. Similarly, people who have been driving
    a car for some time already have knowledge of the rules of the road and some related
    reflexes, which are useful if they want to learn how to drive other vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: These abilities to master complex tasks by building upon available knowledge,
    and to repurpose acquired skills to similar activities, are central to human intelligence.
    Researchers in machine learning dream of reproducing them.
  prefs: []
  type: TYPE_NORMAL
- en: Motivation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unlike humans, most machine learning systems have been designed, so far, for
    single, specific tasks. Directly applying a trained model to a different dataset
    would yield poor results, especially if the data samples do not share the same
    semantic content (for instance, MNIST digit images versus ImageNet photographs)
    or the same image quality/distribution (for instance, a dataset of smartphone
    pictures versus a dataset of high-quality pictures). As CNNs are trained to extract
    and interpret specific features, their performance will be compromised if the
    feature distribution changes. Therefore, some transformations are necessary to
    apply networks to new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Solutions have been investigated for decades. In 1998, Sebastian Thrun and
    Lorien Pratt edited *Learning to Learn*, a book compiling the prevalent research
    stands on the topic. More recently, in their *Deep Learning* book ([http://www.deeplearningbook.org/contents/representation.html](http://www.deeplearningbook.org/contents/representation.html) on
    page 534, MIT Press), Ian Goodfellow, Yoshua Bengio, and Aaron Courville defined
    transfer learning as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[...] the situation where what has been learned in one setting (for example,
    distribution p[1]) is exploited to improve generalization in another setting (say,
    distribution p[2]).'
  prefs: []
  type: TYPE_NORMAL
- en: It makes sense for researchers to suppose that, for example, some of the features
    a CNN is extracting to classify hand-written digits could be partially reused
    for the classification of hand-written texts. Similarly, a network that learned
    to detect human faces could be partially repurposed for the evaluation of facial
    expressions. Indeed, even though the inputs (full images for face detection versus
    cropped ones for the new task) and outputs (detection results versus classification
    values) are different, some of the network's layers are already trained to extract
    facial features, which is useful for both tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, a **task** is defined by the inputs provided (for example,
    pictures from smartphones) and the expected outputs (for example, prediction results
    for a specific set of classes). For instance, classification and detection on
    ImageNet are two different tasks with the same input images but different outputs.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, algorithms can target similar tasks (for example, pedestrian
    detection) but have access to different sets of data (for example, CCTV images
    from different locations, or from cameras of different quality). These methods
    are thus trained on different **domains** (that is, data distributions).
  prefs: []
  type: TYPE_NORMAL
- en: It is the goal of transfer learning to apply the knowledge either from one task
    to another or from one domain to another. The latter type of transfer learning
    is called **domain adaptation** and will be more specifically covered in [Chapter
    7](337ec077-c215-4782-b56c-beae4d94d718.xhtml), *Training on Complex and Scarce
    Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is especially interesting when not enough data is available
    to properly learn the new task (that is, there are not enough image samples to
    estimate the distribution). Indeed, deep learning methods are data hungry; they
    require large datasets for their training. Such datasets—especially labeled ones
    for supervised learning—are often tedious, if not impossible, to gather. For example,
    experts building recognition systems to automate industries cannot go to every
    plant to take hundreds of pictures of every new manufactured product and its components.
    They often have to deal with much smaller datasets, which are not large enough
    for the CNNs to satisfactorily converge. Such limitations explain the efforts
    to reuse knowledge acquired on well-documented visual tasks for those other cases.
  prefs: []
  type: TYPE_NORMAL
- en: With their millions of annotated images from a large number of categories, ImageNet—and,
    more recently, COCO—are particularly rich datasets. It is assumed that CNNs trained
    on those have acquired quite an expertise in visual recognition, hence the availability
    in Keras and TensorFlow Hub of standard models (Inception, ResNet-50, and others)
    already trained on these datasets. People looking for models to transfer knowledge
    from commonly use these.
  prefs: []
  type: TYPE_NORMAL
- en: Transferring CNN knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So, how can you transfer some knowledge from one model to another? Artificial
    neural networks have one advantage over human brains that facilitates this operation:
    they can be easily stored and duplicated. The expertise of a CNN is nothing but
    the values taken by its parameters after training—values that can easily be restored
    and transferred to similar networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning for CNNs mostly consists of reusing the complete or partial
    architecture and weights of a performant network trained on a rich dataset to
    instantiate a new model for a different task. From this conditioned instantiation,
    the new model can then be *fine-tuned*; that is, it can be further trained on
    the available data for the new task/domain.
  prefs: []
  type: TYPE_NORMAL
- en: As we highlighted in the previous chapters, the first layers of a network tend
    to extract low-level features (such as lines, edges, or color gradients), whereas
    final convolutional layers react to more complex notions (such as specific shapes
    and patterns). For classification tasks, the final pooling and/or fully connected
    layers then process these high-level feature maps (often called **bottleneck features**)
    to make their class predictions.
  prefs: []
  type: TYPE_NORMAL
- en: This typical setup and related observations led to various transfer learning
    strategies. Pretrained CNNs, with their final prediction layers removed, started
    being used as efficient *feature extractors*. When the new task is similar enough
    to the ones these extractors were trained for, they can directly be used to output
    pertinent features (the *image feature vector* models on TensorFlow Hub are available
    for that exact purpose). These features can then be processed by one or two new
    dense layers, which are trained to output the task-related predictions. To preserve
    the quality of the extracted features, the layers of the feature extractors are
    often *frozen* during this training phase; that is, their parameters are not updated
    during the gradient descent. In other cases, when the tasks/domains are less similar,
    some of the last layers of the feature extractors—or all of them—are *fine-tuned*;
    that is, trained along with the new prediction layers on the task data. These
    different strategies are further explained in the next paragraphs.
  prefs: []
  type: TYPE_NORMAL
- en: Use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, which pretrained model should we reuse? Which layers should be
    frozen or fine-tuned? The answers to these questions depend on the similarity
    between the target task and the tasks that models have already been trained on,
    as well as the abundance of training samples for the new application.
  prefs: []
  type: TYPE_NORMAL
- en: Similar tasks with limited training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer learning is especially useful when you want to solve a particular task
    and do not have enough training samples to properly train a performant model,
    but do have access to a larger and similar training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The model can be pretrained on this larger dataset until convergence (or, if
    available and pertinent, we can fetch an available pretrained model). Then, its
    final layers should be removed (when the target task is different, that is, its
    output differs from the pretraining task) and replaced with layers adapted to
    the target task. For example, imagine that we want to train a model to distinguish
    between pictures of bees and pictures of wasps. ImageNet contains images for these
    two classes, which could be used as a training dataset, but their number is not
    high enough for an efficient CNN to learn without overfitting. However, we could
    first train this network on the full ImageNet dataset to classify from the 1,000
    categories to develop broader expertise. After this pretraining, its final dense
    layers can be removed and replaced by layers configured to output predictions
    for our two target classes.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned earlier, the new model can finally be prepared for its task
    by freezing the pretrained layers and by training only the dense ones on top.
    Indeed, since the target training dataset is too small, the model would end up
    overfitting if we do not freeze its feature extractor component. By fixing these
    parameters, we make sure that the network keeps the expressiveness it developed
    on the richer dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Similar tasks with abundant training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The bigger the training dataset available for the target task, the smaller the
    chances of the network overfitting if we completely retrain it. Therefore, in
    such cases, people commonly unfreeze the latest layers of the feature extractor.
    In other words, the bigger the target dataset is, the more layers there are that
    can be safely fine-tuned. This allows the network to extract features that are
    more relevant to the new task, and thus to better learn how to perform it.
  prefs: []
  type: TYPE_NORMAL
- en: The model has already been through a first training phase on a similar dataset
    and is probably close to convergence already. Therefore, it is common practice
    to use a smaller learning rate for the fine-tuning phase.
  prefs: []
  type: TYPE_NORMAL
- en: Dissimilar tasks with abundant training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we have access to a rich enough training set for our application, does it
    even make sense to use a pretrained model? This question is legitimate if the
    similarity between the original and target tasks is too low. Pretraining a model,
    or even downloading pretrained weights, can be costly. However, researchers demonstrated
    through various experiments that, in most cases, it is better to initialize a
    network with pretrained weights (even from a dissimilar use case) than with random
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning makes sense when the tasks or their domains share at least
    some basic similarities. For instance, images and audio files can both be stored
    as two-dimensional tensors, and CNNs (such as ResNet ones) are commonly applied
    to both. However, the models are relying on completely different features for
    visual and audio recognition. It would typically not benefit a model for visual
    recognition to receive the weights from a network trained for an audio-related
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Dissimilar tasks with limited training data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, what if the target task is so specific that training samples are barely
    available and using pretrained weights does not make much sense? First, it would
    be necessary to reconsider applying or repurposing a deep model. Training such
    a model on a small dataset would lead to overfitting, and a deep pretrained extractor
    would return features that are too irrelevant for the specific task. However,
    we can still benefit from transfer learning if we keep in mind that the first
    layers of CNNs react to low-level features. Instead of only removing the final
    prediction layers of a pretrained model, we can also remove some of the last convolutional
    blocks, which are too task-specific. A shallow classifier can then be added on
    top of the remaining layers, and the new model can finally be fine-tuned.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning with TensorFlow and Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To conclude this chapter, we will briefly cover how transfer learning can be
    performed with TensorFlow and Keras. We invite our readers to go through the related
    Jupyter notebook in parallel, to have transfer learning illustrated on classification
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Model surgery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Indirectly, we have already presented how standard pretrained models provided
    through TensorFlow Hub and Keras applications can be fetched and easily transformed
    into feature extractors for new tasks. However, it is also common to reuse non-standard
    networks; for example, more specific state-of-the-art CNNs provided by experts,
    or custom models already trained for some previous tasks. We will demonstrate
    how any models can be edited for transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Removing layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first task is to remove the final layers of the pretrained model to transform
    it into a feature extractor. As usual, Keras makes this operation quite easy.
    For `Sequential` models, the list of layers is accessible through the `model.layers` attribute.
    This structure has a `pop()` method, which removes the last layer of the model.
    Therefore, if we know the number of final layers we need to remove to transform
    a network into a specific feature extractor (for instance, two layers for a standard
    ResNet model), this can be done as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In pure TensorFlow, editing an operational graph supporting a model is neither
    simple nor recommended. However, we have to keep in mind that unused graph operations
    are not executed at runtime. So, still having the old layers present in the compiled
    graph will not affect the computational performance of the new model, as long
    as they are not called anymore. Therefore, instead of removing layers, we simply
    need to pinpoint the last layer/operation of the previous model we want to keep.
    If we somehow lost track of its corresponding Python object, but know its name
    (for instance, by checking the graph in Tensorboard), its representative tensor
    can be recovered by looping over the layers of the model and checking their names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'However, Keras provides additional methods to simplify this process. Knowing
    the name of the last layer to keep (for instance, after printing the names with
    `model.summary()`), a feature extractor model can be built in a couple of lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Sharing its weights with the original model, this feature-extraction model is
    ready for use.
  prefs: []
  type: TYPE_NORMAL
- en: Grafting layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Adding new prediction layers on top of a feature extractor is straightforward
    (compared with previous examples with TensorFlow Hub), as it is just a matter
    of adding new layers on top of the corresponding model. For example, this can
    be done as follows, using the Keras API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, through Keras, TensorFlow 2 makes it straightforward to shorten,
    extend, or combine models!
  prefs: []
  type: TYPE_NORMAL
- en: Selective training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transfer learning makes the training phase a bit more complex because we should
    first restore the pretrained layers and define which ones should be frozen. Thankfully,
    several tools are available that simplify these operations.
  prefs: []
  type: TYPE_NORMAL
- en: Restoring pretrained parameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow has some utility functions to warm-start estimators; that is, to
    initialize some of their layers with pretrained weights. The following snippet
    tells TensorFlow to use the saved parameters of a pretrained estimator for the
    new one for the layers sharing the same name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The `WarmStartSettings` initializer takes an optional `vars_to_warm_start` parameter,
    which can also be used to provide the names of the specific variables (as a list
    or a regex) that you want to restore from the checkpoint files (refer to the documentation
    for more details at [https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings](https://www.tensorflow.org/api_docs/python/tf/estimator/WarmStartSettings)).
  prefs: []
  type: TYPE_NORMAL
- en: 'With Keras, we can simply restore the pretrained model before its transformation
    for the new task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Although it is not exactly optimal to restore the complete model before removing
    some of its layers, this solution has the advantage of being concise.
  prefs: []
  type: TYPE_NORMAL
- en: Freezing layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In TensorFlow, the most versatile method for freezing layers consists of removing
    their `tf.Variable` attributes from the list of variables passed to the optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In Keras, layers have a `.trainable` attribute, which can simply be set to
    `False` in order to freeze them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Again, for complete transfer learning examples, we invite you to go through
    the Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Classification challenges, such as ILSVRC, are great playgrounds for researchers,
    leading to the development of more advanced deep learning solutions. In their
    own way, each of the architectures we detailed in this chapter became instrumental
    in computer vision and are still applied to increasingly complex applications.
    As we will see in the following chapters, their technical contributions inspired
    other methods for a wide range of visual tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, not only did we learn to reuse state-of-the-art solutions, but we
    also discovered how algorithms themselves can benefit from the knowledge acquired
    from previous tasks. With transfer learning, the performance of CNNs can be greatly
    improved for specific applications. This is especially true for tasks such as
    object detection, which will be the topic of our next chapter. Annotating datasets
    for object detection is more tedious than for image-level recognition, so methods
    usually have access to smaller training datasets. It is, therefore, important
    to keep transfer learning in mind as a solution to obtain efficient models.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Which TensorFlow Hub module can be used to instantiate an inception classifier
    for ImageNet?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can you freeze the first three residual macro-blocks of a ResNet-50 model
    from Keras applications?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When is transfer learning not recommended?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Hands-On Transfer Learning with Python* ([https://www.packtpub.com/big-data-and-business-intelligence/hands-transfer-learning-python](https://www.packtpub.com/big-data-and-business-intelligence/hands-transfer-learning-python)),
    by Dipanjan Sarkar, Raghav Bali, and Tamoghna Ghosh: This book covers transfer
    learning in more detail, while applying deep learning to domains other than computer
    vision.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
