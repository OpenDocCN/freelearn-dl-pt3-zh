["```\n#get deep learning basics\nimport tensorflow as tf \n```", "```\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-large\")\nGPT2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2-large\", pad_token_id=tokenizer.eos_token_id) \n```", "```\n# settings\n#for reproducability\nSEED = 34\ntf.random.set_seed(SEED)\n#maximum number of words in output text\nMAX_LEN = 70 \n```", "```\ninput_sequence = \"There are times when I am really tired of people, but I feel lonely too.\" \n```", "```\n# encode context the generation is conditioned on\ninput_ids = tokenizer.encode(input_sequence, return_tensors='tf')\n# generate text until the output length (which includes the context length) reaches 70\ngreedy_output = GPT2.generate(input_ids, max_length = MAX_LEN)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(greedy_output[0], skip_special_tokens = True))\nOutput:\n----------------------------------------------------------------------------------------------------\nThere are times when I am really tired of people, but I feel lonely too. I feel like I'm alone in the world. I feel like I'm alone in my own body. I feel like I'm alone in my own mind. I feel like I'm alone in my own heart. I feel like I'm alone in my own mind \n```", "```\n# set return_num_sequences > 1\nbeam_outputs = GPT2.generate(\n    input_ids, \n    max_length = MAX_LEN, \n    num_beams = 5, \n    no_repeat_ngram_size = 2, \n    num_return_sequences = 5, \n    early_stopping = True\n)\nprint('')\nprint(\"Output:\\n\" + 100 * '-')\n# now we have 5 output sequences\nfor i, beam_output in enumerate(beam_outputs):\n      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_      tokens=True)))\nOutput:\n----------------------------------------------------------------------------------------------------\n0: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n\"I feel like I can't do anything right now,\" she said. \"I'm so tired.\"\n1: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n\"I feel like I can't do anything right now,\" she says. \"I'm so tired.\"\n2: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing with my life.\"\n3: There are times when I am really tired of people, but I feel lonely too. I don''t know what to do with myself.\"\"\n\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I'm supposed to be doing.\"\n4: There are times when I am really tired of people, but I feel lonely too. I don't know what to do with myself.\"\n\"I feel like I can't do anything right now,\" she says. \"I'm not sure what I should do.\" \n```", "```\n# use temperature to decrease the sensitivity to low probability candidates\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_k = 0, \n                             temperature = 0.2\n)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True))\nOutput:\n----------------------------------------------------------------------------------------------------\nThere are times when I am really tired of people, but I feel lonely too. I feel like I'm alone in my own world. I feel like I'm alone in my own life. I feel like I'm alone in my own mind. I feel like I'm alone in my own heart. I feel like I'm alone in my own \n```", "```\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_k = 0, \n                             temperature = 0.8\n)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True))\nOutput:\n----------------------------------------------------------------------------------------------------\nThere are times when I am really tired of people, but I feel lonely too. I find it strange how the people around me seem to be always so nice. The only time I feel lonely is when I'm on the road. I can't be alone with my thoughts.\nWhat are some of your favourite things to do in the area \n```", "```\n#sample from only top_k most likely words\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_k = 50\n)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')\nOutput:\n----------------------------------------------------------------------------------------------------\nThere are times when I am really tired of people, but I feel lonely too. I go to a place where you can feel comfortable. It's a place where you can relax. But if you're so tired of going along with the rules, maybe I won't go. You know what? Maybe if I don't go, you won''t ... \n```", "```\n#sample only from 80% most likely words\nsample_output = GPT2.generate(\n                             input_ids, \n                             do_sample = True, \n                             max_length = MAX_LEN, \n                             top_p = 0.8, \n                             top_k = 0\n)\nprint(\"Output:\\n\" + 100 * '-')\nprint(tokenizer.decode(sample_output[0], skip_special_tokens = True), '...')\nOutput:\n----------------------------------------------------------------------------------------------------\nThere are times when I am really tired of people, but I feel lonely too. I feel like I should just be standing there, just sitting there. I know I'm not a danger to anybody. I just feel alone.\" ... \n```", "```\n#combine both sampling techniques\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = 2*MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .7,\n                              top_k = 50, \n                              top_p = 0.85, \n                              num_return_sequences = 5\n)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_    special_tokens = True)))\n    print('')\nOutput:\n----------------------------------------------------------------------------------------------------\n0: There are times when I am really tired of people, but I feel lonely too. I don't feel like I am being respected by my own country, which is why I am trying to change the government.\"\nIn a recent video posted to YouTube, Mr. Jaleel, dressed in a suit and tie, talks about his life in Pakistan and his frustration at his treatment by the country's law enforcement agencies. He also describes how he met a young woman from California who helped him organize the protest in Washington.\n\"She was a journalist who worked with a television channel in Pakistan,\" Mr. Jaleel says in the video. \"She came to my home one day,...\n1: There are times when I am really tired of people, but I feel lonely too. It's not that I don't like to be around other people, but it's just something I have to face sometimes.\nWhat is your favorite thing to eat?\nThe most favorite thing I have eaten is chicken and waffles. But I love rice, soups, and even noodles. I also like to eat bread, but I like it a little bit less.\nWhat is your ideal day of eating?\nIt varies every day. Sometimes I want to eat at home, because I'm in a house with my family. But then sometimes I just have to have some sort...\n2: There are times when I am really tired of people, but I feel lonely too. I think that there is something in my heart that is trying to be a better person, but I don't know what that is.\"\nSo what can be done?\n\"I want people to take the time to think about this,\" says Jorja, who lives in a small town outside of Boston.\nShe has been thinking a lot about her depression. She wants to make a documentary about it, and she wants to start a blog about it.\n\"I want to make a video to be a support system for people who are going through the same thing I was going through...\n3: There are times when I am really tired of people, but I feel lonely too.\nI want to be able to take good care of myself. I am going to be a very good person, even if I am lonely.\nSo, if it's lonely, then I will be happy. I will be a person who will be able to have good care of myself.\nI have made this wish.\nWhat is my hope? What is my goal? I want to do my best to be able to meet it, butâ€¦\n\"Yuu, what are you saying, Yuu?\"\n\"Uwa, what is it?\"\nI...\n4: There are times when I am really tired of people, but I feel lonely too. The only person I really love is my family. It's just that I'm not alone.\"\n-Juan, 24, a student\nA study from the European Economic Area, a free trade area between the EU and Iceland, showed that there are 2.3 million EU citizens living in Iceland. Another survey in 2014 showed that 1.3 million people in Iceland were employed.\nThe government is committed to making Iceland a country where everyone can live and work.\n\"We are here to help, not to steal,\" said one of the people who drove up in a Volkswagen.\n... \n```", "```\nMAX_LEN = 500\nprompt1 = 'In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.'\ninput_ids = tokenizer.encode(prompt1, return_tensors='tf')\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85 \n                              #num_return_sequences = 5\n)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_    special_tokens = True)))\n    print('')\nOutput:\n----------------------------------------------------------------------------------------------------\n0: In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.\nThis is the first time a herd of unicorns have been discovered in the Andes Mountains, a vast region stretching from the Himalayas to the Andes River in Bolivia.\nAccording to the BBC, the unicorns were spotted by a small group of researchers on a private expedition, but they were the only ones that came across the bizarre creatures.\nIt was later learned that these were not the wild unicorns that were spotted in the wild in recent years, but rather a domesticated variety of the species.\nAlthough they do not speak English, they do carry their own unique language, according to the researchers, who have named it \"Ungla.\"\nThe herd of unicorns, which was discovered by a small group of researchers, is the first of its kind discovered in the Andes Mountains. It is thought that the herd of wild unicorns were introduced to the area hundreds of years ago by a local rancher who was attempting to make a profit from the animals.\nAlthough they do not speak English, they do carry their own unique language, according to the researchers, who have named it \"Ungla.\"\nThe researchers claim that the unicorns have only been sighted in the Andes Mountains, where they can be seen throughout the mountains of South America.\nWhile the unicorns do not speak English, they do carry their own unique language, according to the researchers, who have named it \"Ungla.\"\nUngla is a highly intelligent, cooperative species with a high level of social and cognitive complexity, and is capable of displaying sophisticated behaviors.\nThey are a particularly unique species, because they are capable of surviving in extreme conditions for long periods of time and without being fed or watered.\nThe team believes that the species was probably domesticated in the Andes Mountains, where it could not survive in its natural habitat.\n\"We can see from the genetics that the animals were probably domesticated in the Andes Mountains where they could not survive in their natural habitat and with water and food sources,\" said Professor David Catt, from the University of Cambridge, who led the study.\n\"So these were animals that would have been... \n```", "```\nprompt2 = 'Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today.'\ninput_ids = tokenizer.encode(prompt2, return_tensors='tf')\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85\n                              #num_return_sequences = 5\n)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_    special_tokens = True)))\n    print('')\nOutput:\n----------------------------------------------------------------------------------------------------\n0: Miley Cyrus was caught shoplifting from Abercrombie and Fitch on Hollywood Boulevard today. In a video captured by one of her friends, the singer is seen grabbing her bag, but then quickly realizing the merchandise she has to leave is too expensive to be worth a $1.99 purchase.\nThe video has already gone viral, and while the celebrity is certainly guilty of breaking the law (even if she can't be accused of stealing for a second time), there's one aspect of the situation that should make the most sense. It's just like the shopping situation in the movie The Fast and the Furious, where Michael Corleone is caught in possession of counterfeit designer clothing.\nThis time around, though, the situation involves Cyrus. It's not a copy, per se. It's actually a replica, a pair of a black and white Nike Air Force 1s, a colorway she wore in her music video.\nIt seems that the actress is caught by a friend who had gotten her a pair of those sneakers when she was in school, so this is no surprise to her. After all, there was a video of her stealing from her own store back in 2012, when she was a freshman at New York University.\nIt's not that there's anything wrong with the product. If the merchandise is in good shape, that's all that matters. But there are a few things that should come to mind when it comes to these shoes.\nFor one, the fabric is incredibly thin. The fabric is so thin that the upper actually makes the shoes look like they're made of leather. There's even a thin layer of plastic between the upper and the shoe.\nSecondly, the material isn't even a shoe. It's just a piece of leather. It's not actually a leather shoe at all, even though it's made of the same material as the other items on the show. It's just a piece of leather. And it's not the kind of leather that would actually hold up in a fight.\nThis is something that should be familiar to anyone who's ever shopped at the store. If you go into the store looking for a pair of new Nike Air Force 1s, and the salesperson is just selling you a piece of leather, you're going to get disappointed. That's the nature of these shoes.\nIn addition to the aforementioned \"stolen\" footwear, Miley Cyrus... \n```", "```\nprompt3 = 'Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry'\ninput_ids = tokenizer.encode(prompt3, return_tensors='tf')\nsample_outputs = GPT2.generate(\n                              input_ids,\n                              do_sample = True, \n                              max_length = MAX_LEN,                              #to test how long we can generate and it be coherent\n                              #temperature = .8,\n                              top_k = 50, \n                              top_p = 0.85 \n                              #num_return_sequences = 5\n)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}...\".format(i, tokenizer.decode(sample_output, skip_    special_tokens = True)))\n    print('')\nOutput:\n----------------------------------------------------------------------------------------------------\n0: Legolas and Gimli advanced on the orcs, raising their weapons with a harrowing war cry, and they roared their battle cries as they charged the orcs with their spears and arrows. They reached the front of the line, where the enemy were gathered, and they fell upon them with a hail of fire and arrows, slaying many orcs and wounding others. The battle raged on for a long time, and eventually the two sides met and fought for a long time more. The orcs fell and the two armies were victorious. The orcs were killed and the two armies were victorious.\nThe two armies fought one last time in battle. Gimli slew many of the orcs and led his men to safety. They went to the city and took it. When they returned, Sauron's servants were waiting to kill them. The two armies fought again, and the battle raged on for a long time more. Gimli slew many of the orcs and led his men to safety. They went to the city and took it. When they returned, Sauron's servants were waiting to kill them. The two armies fought again, and the battle raged on for a long time more. Gimli slew many of the orcs and led his men to safety. They went to the city and took it. When they returned, Sauron's servants were waiting to kill them. The two armies fought again, and the battle raged on for a long time more. Gimli slew many of the orcs and led his men to safety. They went to the city and took it. When they returned, Sauron's servants were waiting to kill them. The two armies fought again, and the battle raged on for a long time more. Gimli slew many of the orcs and led his men to safety. They went to the city and took it. When they returned, Sauron's servants were waiting to kill them. The two armies fought again, and the battle raged on for a long time more. Gimli slew many of the orcs and led his men to safety. They went to the city and took it. When they returned, Sauron's servants were waiting to kill them. The two armies fought again, and the battle raged on for a long time more. Gimli slew many of the orcs and led his men to safety. They went to the city and took it. When they returned, Sauron's servants were waiting to kill them. The two armies fought again, and the battle raged on for a... \n```", "```\nimport pandas as pd\nimport re\nimport numpy as np\nnp.random.seed(0)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport keras\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, LSTM, GRU, Embedding\nfrom keras.layers import Activation, Bidirectional, GlobalMaxPool1D, GlobalMaxPool2D, Dropout\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.callbacks import EarlyStopping\nfrom keras.optimizers import RMSprop, adam\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nimport seaborn as sns\nimport transformers\nfrom transformers import AutoTokenizer\nfrom tokenizers import BertWordPieceTokenizer\nfrom keras.initializers import Constant\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom collections import Counter\nstop=set(stopwords.words('english'))\nimport os \n```", "```\ndef basic_cleaning(text):\n    text=re.sub(r'https?://www\\.\\S+\\.com','',text)\n    text=re.sub(r'[^A-Za-z|\\s]','',text)\n    text=re.sub(r'\\*+','swear',text) #capture swear words that are **** out\n    return text\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndef remove_multiplechars(text):\n    text = re.sub(r'(.)\\1{3,}',r'\\1', text)\n    return text\ndef clean(df):\n    for col in ['text']:#,'selected_text']:\n        df[col]=df[col].astype(str).apply(lambda x:basic_cleaning(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_emoji(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_html(x))\n        df[col]=df[col].astype(str).apply(lambda x:remove_multiplechars(x))\n    return df\ndef fast_encode(texts, tokenizer, chunk_size=256, maxlen=128):    \n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n\n    for i in range(0, len(texts), chunk_size):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n\n    return np.array(all_ids)\ndef preprocess_news(df,stop=stop,n=1,col='text'):\n    '''Function to preprocess and create corpus'''\n    new_corpus=[]\n    stem=PorterStemmer()\n    lem=WordNetLemmatizer()\n    for text in df[col]:\n        words=[w for w in word_tokenize(text) if (w not in stop)]\n\n        words=[lem.lemmatize(w) for w in words if(len(w)>n)]\n\n        new_corpus.append(words)\n\n    new_corpus=[word for l in new_corpus for word in l]\n    return new_corpus \n```", "```\ndf = pd.read_csv('/kaggle/input/tweet-sentiment-extraction/train.csv')\ndf.head() \n```", "```\ndf.dropna(inplace=True)\ndf_clean = clean(df) \n```", "```\ndf_clean_selection = df_clean.sample(frac=1)\nX = df_clean_selection.text.values\ny = pd.get_dummies(df_clean_selection.sentiment)\ntokenizer = text.Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(list(X))\nlist_tokenized_train = tokenizer.texts_to_sequences(X)\nX_t = sequence.pad_sequences(list_tokenized_train, maxlen=128) \n```", "```\ntokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")  ## change it to commit\n# Save the loaded tokenizer locally\nsave_path = '/kaggle/working/distilbert_base_uncased/'\nif not os.path.exists(save_path):\n    os.makedirs(save_path)\ntokenizer.save_pretrained(save_path)\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('distilbert_base_uncased/vocab.txt', lowercase=True)\nfast_tokenizer\nX = fast_encode(df_clean_selection.text.astype(str), fast_tokenizer, maxlen=128)\ntransformer_layer = transformers.TFDistilBertModel.from_pretrained('distilbert-base-uncased')\nembedding_size = 128 input_ = Input(shape=(100,)) \ninp = Input(shape=(128, )) \nembedding_matrix=transformer_layer.weights[0].numpy() \nx = Embedding(embedding_matrix.shape[0], embedding_matrix.shape[1],embeddings_initializer=Constant(embedding_matrix),trainable=False)(inp) \n```", "```\nx = Bidirectional(LSTM(50, return_sequences=True))(x) \nx = Bidirectional(LSTM(25, return_sequences=True))(x) \nx = GlobalMaxPool1D()(x) x = Dropout(0.5)(x) \nx = Dense(50, activation='relu', kernel_regularizer='L1L2')(x) \nx = Dropout(0.5)(x) \nx = Dense(3, activation='softmax')(x) \nmodel_DistilBert = Model(inputs=[inp], outputs=x)\nmodel_DistilBert.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel_DistilBert.summary()\nModel: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         (None, 128)               0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 128, 768)          23440896  \n_________________________________________________________________\nbidirectional_1 (Bidirection (None, 128, 100)          327600    \n_________________________________________________________________\nbidirectional_2 (Bidirection (None, 128, 50)           25200     \n_________________________________________________________________\nglobal_max_pooling1d_1 (Glob (None, 50)                0         \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 50)                2550      \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 50)                0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 3)                 153       \n=================================================================\nTotal params: 23,796,399\nTrainable params: 355,503\nNon-trainable params: 23,440,896\n_________________________________________________________________ \n```", "```\nmodel_DistilBert.fit(X,y,batch_size=32,epochs=10,validation_split=0.1)\nTrain on 24732 samples, validate on 2748 samples\nEpoch 1/10\n24732/24732 [==============================] - 357s 14ms/step - loss: 1.0516 - accuracy: 0.4328 - val_loss: 0.8719 - val_accuracy: 0.5466\nEpoch 2/10\n24732/24732 [==============================] - 355s 14ms/step - loss: 0.7733 - accuracy: 0.6604 - val_loss: 0.7032 - val_accuracy: 0.6776\nEpoch 3/10\n24732/24732 [==============================] - 355s 14ms/step - loss: 0.6668 - accuracy: 0.7299 - val_loss: 0.6407 - val_accuracy: 0.7354\nEpoch 4/10\n24732/24732 [==============================] - 355s 14ms/step - loss: 0.6310 - accuracy: 0.7461 - val_loss: 0.5925 - val_accuracy: 0.7478\nEpoch 5/10\n24732/24732 [==============================] - 347s 14ms/step - loss: 0.6070 - accuracy: 0.7565 - val_loss: 0.5817 - val_accuracy: 0.7529\nEpoch 6/10\n24732/24732 [==============================] - 343s 14ms/step - loss: 0.5922 - accuracy: 0.7635 - val_loss: 0.5817 - val_accuracy: 0.7584\nEpoch 7/10\n24732/24732 [==============================] - 343s 14ms/step - loss: 0.5733 - accuracy: 0.7707 - val_loss: 0.5922 - val_accuracy: 0.7638\nEpoch 8/10\n24732/24732 [==============================] - 343s 14ms/step - loss: 0.5547 - accuracy: 0.7832 - val_loss: 0.5767 - val_accuracy: 0.7627\nEpoch 9/10\n24732/24732 [==============================] - 346s 14ms/step - loss: 0.5350 - accuracy: 0.7870 - val_loss: 0.5767 - val_accuracy: 0.7584\nEpoch 10/10\n24732/24732 [==============================] - 346s 14ms/step - loss: 0.5219 - accuracy: 0.7955 - val_loss: 0.5994 - val_accuracy: 0.7580 \n```", "```\nimport os\nimport zipfile\nimport shutil\nimport urllib.request\nimport logging\nimport lzma\nimport json\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport time\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModelForQuestionAnswering, TFBertForMaskedLM, TFBertForQuestionAnswering \n```", "```\ndef get_pretrained_squad_model(model_name):\n\n    model, tokenizer = None, None\n\n    if model_name == \"distilbertsquad1\":        \n        tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\",use_fast=True)\n        model = TFBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\", from_pt=True)\n\n    elif model_name == \"distilbertsquad2\": \n        tokenizer = AutoTokenizer.from_pretrained(\"twmkn9/distilbert-base-uncased-squad2\",use_fast=True)\n        model = TFAutoModelForQuestionAnswering.from_pretrained(\"twmkn9/distilbert-base-uncased-squad2\", from_pt=True)\n\n    elif model_name == \"bertsquad2\": \n        tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-base-cased-squad2\",use_fast=True)\n        model = TFBertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\", from_pt=True)\n\n    elif model_name == \"bertlargesquad2\": \n        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",use_fast=True)\n        model = TFBertForQuestionAnswering.from_pretrained(\"deepset/bert-large-uncased-whole-word-masking-squad2\", from_pt=True)\n\n    elif model_name == \"albertbasesquad2\": \n        tokenizer = AutoTokenizer.from_pretrained(\"twmkn9/albert-base-v2-squad2\",use_fast=True)\n        model = TFBertForQuestionAnswering.from_pretrained(\"twmkn9/albert-base-v2-squad2\", from_pt=True)\n\n    elif model_name == \"distilrobertasquad2\": \n        tokenizer = AutoTokenizer.from_pretrained(\"twmkn9/distilroberta-base-squad2\",use_fast=True)\n        model = TFBertForQuestionAnswering.from_pretrained(\"twmkn9/\ndistilroberta-base-squad2\", from_pt=True)\n\n    elif model_name == \"robertasquad2\": \n        tokenizer = AutoTokenizer.from_pretrained(\"deepset/roberta-base-squad2\",use_fast=True)\n        model = TFAutoModelForQuestionAnswering.from_pretrained(\"deepset/roberta-base-squad2\", from_pt=True)\n\n    elif model_name == \"bertlm\":\n\n        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\",                                                  use_fast=True)\n        model = TFBertForMaskedLM.from_pretrained(\"bert-base-uncased\",                                                   from_pt=True)\n    return model, tokenizer \n```", "```\ndef get_answer_span(question, context, model, tokenizer): \n    inputs = tokenizer.encode_plus(question, context, return_tensors=\"tf\", add_special_tokens=True, max_length=512) \n    answer_start_scores, answer_end_scores = model(inputs)  \n    answer_start = tf.argmax(answer_start_scores, axis=1).numpy()[0] \n    answer_end = (tf.argmax(answer_end_scores, axis=1) + 1).numpy()[0]  \n    print(tokenizer.convert_tokens_to_string(inputs[\"input_ids\"][0][answer_start:answer_end]))\n    return answer_start, answer_end \n```", "```\ndef clean_tokens(gradients, tokens, token_types):\n\n    \"\"\"\n      Clean the tokens and gradients gradients\n      Remove \"[CLS]\",\"[CLR]\", \"[SEP]\" tokens\n      Reduce (mean) gradients values for tokens that are split ##\n    \"\"\"\n\n    token_holder = []\n    token_type_holder = []    \n    gradient_holder = []     \n    i = 0\n\n    while i < len(tokens):\n        if (tokens[i] not in [\"[CLS]\",\"[CLR]\", \"[SEP]\"]):\n            token = tokens[i]              \n            conn = gradients[i]               \n            token_type = token_types[i]\n\n            if i < len(tokens)-1 :\n                if tokens[i+1][0:2] == \"##\":\n                    token = tokens[i]\n                    conn = gradients[i]  \n                    j = 1\n                    while i < len(tokens)-1 and tokens[i+1][0:2] == \"##\":                        \n                        i +=1 \n                        token += tokens[i][2:]\n                        conn += gradients[i]   \n                        j+=1\n                    conn = conn /j \n            token_holder.append(token)\n            token_type_holder.append(token_type)\n            gradient_holder.append(conn)\n\n        i +=1\n\n    return  gradient_holder,token_holder, token_type_holder\ndef get_best_start_end_position(start_scores, end_scores):\n\n    answer_start = tf.argmax(start_scores, axis=1).numpy()[0] \n    answer_end = (tf.argmax(end_scores, axis=1) + 1).numpy()[0] \n    return answer_start, answer_end\ndef get_correct_span_mask(correct_index, token_size):\n\n    span_mask = np.zeros((1, token_size))\n    span_mask[0, correct_index] = 1\n    span_mask = tf.constant(span_mask, dtype='float32')\n\n    return span_mask \n\ndef get_embedding_matrix(model):\n\n    if \"DistilBert\" in type(model).__name__:\n        return model.distilbert.embeddings.word_embeddings\n    else:\n        return model.bert.embeddings.word_embeddings\ndef get_gradient(question, context, model, tokenizer): \n\n    \"\"\"Return gradient of input (question) wrt to model output span prediction \n      Args:\n          question (str): text of input question\n          context (str): text of question context/passage\n          model (QA model): Hugging Face BERT model for QA transformers.modeling_tf_distilbert.TFDistilBertForQuestionAnswering, transformers.modeling_tf_bert.TFBertForQuestionAnswering\n          tokenizer (tokenizer): transformers.tokenization_bert.BertTokenizerFast \n      Returns:\n            (tuple): (gradients, token_words, token_types, answer_text)\n    \"\"\"\n    embedding_matrix = get_embedding_matrix(model)  \n    encoded_tokens =  tokenizer.encode_plus(question, context, add_special_tokens=True, return_token_type_ids=True, return_tensors=\"tf\")\n    token_ids = list(encoded_tokens[\"input_ids\"].numpy()[0])\n    vocab_size = embedding_matrix.get_shape()[0]\n    # convert token ids to one hot. We can't differentiate wrt to int token ids hence the need for one hot representation\n    token_ids_tensor = tf.constant([token_ids], dtype='int32')\n    token_ids_tensor_one_hot = tf.one_hot(token_ids_tensor, vocab_size) \n\n    with tf.GradientTape(watch_accessed_variables=False) as tape:\n\n        # (i) watch input variable\n        tape.watch(token_ids_tensor_one_hot)\n\n        # multiply input model embedding matrix; allows us do backprop wrt one hot input \n        inputs_embeds = tf.matmul(token_ids_tensor_one_hot,embedding_matrix)  \n        # (ii) get prediction\n        start_scores,end_scores = model({\"inputs_embeds\": inputs_embeds, \"token_type_ids\": encoded_tokens[\"token_type_ids\"], \"attention_mask\": encoded_tokens[\"attention_mask\"] })\n        answer_start, answer_end = get_best_start_end_position(start_scores, end_scores)\n        start_output_mask = get_correct_span_mask(answer_start, len(token_ids))\n        end_output_mask = get_correct_span_mask(answer_end, len(token_ids))\n        # zero out all predictions outside of the correct span positions; we want to get gradients wrt to just these positions\n        predict_correct_start_token = tf.reduce_sum(start_scores *                                                     start_output_mask)\n        predict_correct_end_token = tf.reduce_sum(end_scores *                                                   end_output_mask) \n        # (iii) get gradient of input with respect to both start and end output\n        gradient_non_normalized = tf.norm(\n            tape.gradient([predict_correct_start_token, predict_correct_end_token], token_ids_tensor_one_hot),axis=2)\n        # (iv) normalize gradient scores and return them as \"explanations\"\n        gradient_tensor = (\n            gradient_non_normalized /\n            tf.reduce_max(gradient_non_normalized)\n        )\n        gradients = gradient_tensor[0].numpy().tolist()\n        token_words = tokenizer.convert_ids_to_tokens(token_ids) \n        token_types = list(encoded_tokens[\"token_type_ids\"].numpy()[0])\n        answer_text = tokenizer.decode(token_ids[answer_start:answer_end])\n        return  gradients,  token_words, token_types,answer_text\ndef explain_model(question, context, model, tokenizer, explain_method = \"gradient\"):    \n    if explain_method == \"gradient\":        \n        return get_gradient(question, context, model, tokenizer) \n```", "```\ndef plot_gradients(tokens, token_types, gradients, title): \n\n    \"\"\" Plot  explanations\n    \"\"\"\n    plt.figure(figsize=(21,3)) \n    xvals = [ x + str(i) for i,x in enumerate(tokens)]\n    colors =  [ (0,0,1, c) for c,t in zip(gradients, token_types) ]\n    edgecolors = [ \"black\" if t==0 else (0,0,1, c)  for c,t in zip(gradients, token_types) ]\n    # colors =  [  (\"r\" if t==0 else \"b\")  for c,t in zip(gradients, token_types) ]    \n    plt.tick_params(axis='both', which='minor', labelsize=29)    \n    p = plt.bar(xvals, gradients, color=colors, linewidth=1, edgecolor=edgecolors)    \n    plt.title(title)     \n    p=plt.xticks(ticks=[i for i in range(len(tokens))], labels=tokens, fontsize=12,rotation=90) \n```", "```\nquestions = [\n    { \"question\": \"what is the goal of the fourth amendment?  \", \"context\": \"The Fourth Amendment of the U.S. Constitution provides that '[t]he right of the people to be secure in their persons, houses, papers, and effects, against unreasonable searches and seizures, shall not be violated, and no Warrants shall issue, but upon probable cause, supported by Oath or affirmation, and particularly describing the place to be searched, and the persons or things to be seized.'The ultimate goal of this provision is to protect people's right to privacy and freedom from unreasonable intrusions by the government. However, the Fourth Amendment does not guarantee protection from all searches and seizures, but only those done by the government and deemed unreasonable under the law.\" },\n    { \"question\": \"\"what is the taj mahal made of?\", \"context\": \"The Taj Mahal is an ivory-white marble mausoleum on the southern bank of the river Yamuna in the Indian city of Agra. It was commissioned in 1632 by the Mughal emperor Shah Jahan (reigned from 1628 to 1658) to house the tomb of his favourite wife, Mumtaz Mahal; it also houses the tomb of Shah Jahan himself. The tomb is the centrepiece of a 17-hectare (42-acre) complex, which includes a mosque and a guest house, and is set in formal gardens bounded on three sides by a crenellated wall. Construction of the mausoleum was essentially completed in 1643, but work continued on other phases of the project for another 10 years. The Taj Mahal complex is believed to have been completed in its entirety in 1653 at a cost estimated at the time to be around 32 million rupees, which in 2020 would be approximately 70 billion rupees (about U.S. $916 million). The construction project employed some 20,000 artisans under the guidance of a board of architects led by the court architect to the emperor. The Taj Mahal was designated as a UNESCO World Heritage Site in 1983 for being the jewel of Muslim art in India and one of the universally admired masterpieces of the world's heritage. It is regarded by many as the best example of Mughal architecture and a symbol of India's rich history. The Taj Mahal attracts 7â€“8 million visitors a year and in 2007, it was declared a winner of the New 7 Wonders of the World (2000â€“2007) initiative.\" },\n    { \"question\": \"Who ruled macedonia \", \"context\": \"Macedonia was an ancient kingdom on the periphery of Archaic and Classical Greece, and later the dominant state of Hellenistic Greece. The kingdom was founded and initially ruled by the Argead dynasty, followed by the Antipatrid and Antigonid dynasties. Home to the ancient Macedonians, it originated on the northeastern part of the Greek peninsula. Before the 4th century BC, it was a small kingdom outside of the area dominated by the city-states of Athens, Sparta and Thebes, and briefly subordinate to Achaemenid Persia\" },\n    { \"question\": \"what are the symptoms of COVID-19\", \"context\": \"COVID-19 is the infectious disease caused by the most recently discovered coronavirus. This new virus and disease were unknown before the outbreak began in Wuhan, China, in December 2019\\. The most common symptoms of COVID-19 are fever, tiredness, and dry cough. Some patients may have aches and pains, nasal congestion, runny nose, sore throat or diarrhea. These symptoms are usually mild and begin gradually. Some people become infected but don't develop any symptoms and don't feel unwell. Most people (about 80%) recover from the disease without needing special treatment. Around 1 out of every 6 people who gets COVID-19 becomes seriously ill and develops difficulty breathing. Older people, and those with underlying medical problems like high blood pressure, heart problems or diabetes, are more likely to develop serious illness. People with fever, cough and difficulty breathing should seek medical attention.\" },\n]\nmodel_names = [\"distilbertsquad1\",\"distilbertsquad2\",\"bertsquad2\",\"bertlargesquad2\"]\nresult_holder = []\nfor model_name in model_names:\n    bqa_model, bqa_tokenizer = get_pretrained_squad_model(model_name)\n\n    for row in questions:\n\n        start_time = time.time() \n        question, context = row[\"question\"], row[\"context\"] \n        gradients, tokens, token_types, answer  = explain_model(question, context, bqa_model, bqa_tokenizer) \n        elapsed_time = time.time() - start_time\n        result_holder.append({\"question\": question,  \"context\":context, \"answer\": answer, \"model\": model_name, \"runtime\": elapsed_time})\nresult_df = pd.DataFrame(result_holder) \n```", "```\nquestion_df = result_df[result_df[\"model\"] == \"bertsquad2\"].reset_index()[[\"question\"]]\ndf_list = [question_df]\nfor model_name in model_names:\n\n    sub_df = result_df[result_df[\"model\"] == model_name].reset_index()[[\"answer\", \"runtime\"]]\n    sub_df.columns = [ (col_name + \"_\" + model_name)  for col_name in                                                             sub_df.columns]\n    df_list.append(sub_df)\n\njdf = pd.concat(df_list, axis=1)\nanswer_cols = [\"question\"] + [col for col in jdf.columns if 'answer' in col]\njdf[answer_cols] \n```", "```\nruntime_cols = [col for col in jdf.columns if 'runtime' in col] \nmean_runtime = jdf[runtime_cols].mean()\nprint(\"Mean runtime per model across 4 question/context pairs\")\nprint(mean_runtime)\nMean runtime per model across 4 question/context pairs\nruntime_distilbertsquad1    0.202405\nruntime_distilbertsquad2    0.100577\nruntime_bertsquad2          0.266057\nruntime_bertlargesquad2     0.386156\ndtype: float64 \n```"]