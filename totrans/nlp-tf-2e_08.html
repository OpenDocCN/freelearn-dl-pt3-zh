<html><head></head><body>
  <div id="_idContainer416" class="Basic-Text-Frame">
    <h1 class="chapterNumber">8</h1>
    <h1 id="_idParaDest-185" class="chapterTitle">Applications of LSTM – Generating Text</h1>
    <p class="normal">Now that we have a good understanding of the underlying mechanisms of LSTMs, such as how they solve the problem of the vanishing gradient and update rules, we can look at how to use them in NLP tasks. LSTMs are employed for tasks such as text generation and image caption generation. For example, language modeling is at the core of any NLP task, as the ability to model language effectively leads to effective language understanding. Therefore, this is typically used for pretraining downstream decision support NLP models. By itself, language modeling can be used to generate songs (<a href="https://towardsdatascience.com/generating-drake-rap-lyrics-using-language-models-and-lstms-8725d71b1b12"><span class="url">https://towardsdatascience.com/generating-drake-rap-lyrics-using-language-models-and-lstms-8725d71b1b12</span></a>), movie scripts (<a href="https://builtin.com/media-gaming/ai-movie-script"><span class="url">https://builtin.com/media-gaming/ai-movie-script</span></a>), etc.</p>
    <p class="normal">The application that we will cover in this chapter is building an LSTM that can write new folk stories. For this task, we will download translations of some folk stories by the Grimm brothers. We will use these stories to train an LSTM and then ask it to output a fresh new story. We will process the text by breaking it into character-level bigrams (n-grams where <em class="italic">n=2</em>) and make a vocabulary out of the unique bigrams. Note that representing bigrams as one-hot-encoded vectors is very ineffective for machine learning models, as it forces the model to treat each bigram as an independent unit of text that is entirely different from other bigrams. But bigrams do share semantics, where certain bigrams co-occur where certain ones would not. One-hot encoding will ignore this important property, which is undesirable. To leverage this property in our modeling, we will use an embedding layer and jointly train it with the model. </p>
    <p class="normal">We will also explore ways to implement previously described techniques such as greedy sampling or beam search for improving the quality of predictions. Afterward, we will see how we can implement time-series models other than standard LSTMs, such as GRUs.</p>
    <p class="normal">Specifically, this chapter will cover the following main topics:</p>
    <ul>
      <li class="bulletList">Our data</li>
      <li class="bulletList">Implementing the language model</li>
      <li class="bulletList">Comparing LSTMs to LSTMs with peephole connections and GRUs</li>
      <li class="bulletList">Improving sequential models – beam search</li>
      <li class="bulletList">Improving LSTMs – generating text with words instead of n-grams</li>
    </ul>
    <h1 id="_idParaDest-186" class="heading-1">Our data</h1>
    <p class="normal">First, we will discuss<a id="_idIndexMarker740"/> the data we will use for text generation and various preprocessing steps employed to clean the data.</p>
    <h2 id="_idParaDest-187" class="heading-2">About the dataset</h2>
    <p class="normal">First, we will <a id="_idIndexMarker741"/>understand what the dataset looks like so that when we see the generated text, we can assess whether it makes sense, given the training data. We will download the first 100 books from the website <a href="https://www.cs.cmu.edu/~spok/grimmtmp/"><span class="url">https://www.cs.cmu.edu/~spok/grimmtmp/</span></a>. These are translations of a set of books (from German to English) by the Grimm brothers.</p>
    <p class="normal">Initially, we will download all 209 books from the website with an automated script, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">url = <span class="hljs-string">'https://www.cs.cmu.edu/~spok/grimmtmp/'</span>
dir_name = <span class="hljs-string">'data'</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">download_data</span>(<span class="hljs-params">url, filename, download_dir</span>):
    <span class="hljs-string">"""Download a file if not present, and make sure it's the right </span>
<span class="hljs-string">    size."""</span>
      
    <span class="hljs-comment"># Create directories if doesn't exist</span>
    os.makedirs(download_dir, exist_ok=<span class="hljs-literal">True</span>)
    
    <span class="hljs-comment"># If file doesn't exist download</span>
    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> os.path.exists(os.path.join(download_dir,filename)):
        filepath, _ = urlretrieve(url + filename, 
        os.path.join(download_dir,filename))
    <span class="hljs-keyword">else</span>:
        filepath = os.path.join(download_dir, filename)
        
    <span class="hljs-keyword">return</span> filepath
<span class="hljs-comment"># Number of files and their names to download</span>
num_files = <span class="hljs-number">209</span>
filenames = [<span class="hljs-built_in">format</span>(i, <span class="hljs-string">'03d'</span>)+<span class="hljs-string">'.txt'</span> <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">1</span>,num_files+<span class="hljs-number">1</span>)]
<span class="hljs-comment"># Download each file</span>
<span class="hljs-keyword">for</span> fn <span class="hljs-keyword">in</span> filenames:
    download_data(url, fn, dir_name)
<span class="hljs-comment"># Check if all files are downloaded</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(filenames)):
    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))
    <span class="hljs-keyword">assert</span> file_exists
<span class="hljs-built_in">print</span>(<span class="hljs-string">'{} files found.'</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(filenames)))
</code></pre>
    <p class="normal">We will now <a id="_idIndexMarker742"/>show example text snippets extracted from two randomly picked stories. The following is the first snippet:</p>
<blockquote class="packt_quote">
    <p class="quote">Then she said, my dearest benjamin, your father has had these coffins made for you and for your eleven brothers, for if I bring a little girl into the world, you are all to be killed and buried in them. And as she wept while she was saying this, the son comforted her and said, weep not, dear mother, we will save ourselves, and go hence. But she said, go forth into the forest with your eleven brothers, and let one sit constantly on the highest tree which can be found, and keep watch, looking towards the tower here in the castle. If I give birth to a little son, I will put up a white flag, and then you may venture to come back. But if I bear a daughter, I will hoist a red flag, and then fly hence as quickly as you are able, and may the good God protect you.</p>
</blockquote>
    <p class="normal">The second text snippet is as follows:</p>
<blockquote class="packt_quote">
    <p class="quote">Red-cap did not know what a wicked creature he was, and was not at all afraid of him.</p>
    <p class="quote">“Good-day, little red-cap,” said he.</p>
    <p class="quote">“Thank you kindly, wolf.”</p>
    <p class="quote">“Whither away so early, little red-cap?”</p>
    <p class="quote">“To my grandmother’s.”</p>
    <p class="quote">“What have you got in your apron?”</p>
    <p class="quote">“Cake and wine. Yesterday was baking-day, so poor sick grandmother is to have something good, to make her stronger.”</p>
    <p class="quote">“Where does your grandmother live, little red-cap?”</p>
    <p class="quote">“A good quarter of a league farther on in the wood. Her house stands under the three large oak-trees, the nut-trees are just below. You surely must know it,” replied little red-cap.</p>
    <p class="quote">The wolf thought to himself, what a tender young creature. What a nice plump mouthful, she will be better to eat than the old woman.</p>
</blockquote>
    <p class="normal">We now<a id="_idIndexMarker743"/> understand what our data looks like. With that understanding, let us move on to processing our data further.</p>
    <h2 id="_idParaDest-188" class="heading-2">Generating training, validation, and test sets</h2>
    <p class="normal">We will<a id="_idIndexMarker744"/> segregate<a id="_idIndexMarker745"/> the stories we downloaded into<a id="_idIndexMarker746"/> three sets: training, validation, and test files. We will use the content in each set of files as the training, validation, and test data. We will use scikit-learn’s <code class="inlineCode">train_test_split()</code> function to do so.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-comment"># Fix the random seed so we get the same output everytime</span>
random_state = <span class="hljs-number">54321</span>
filenames = [os.path.join(dir_name, f) <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> os.listdir(dir_name)]
<span class="hljs-comment"># First separate train and valid+test data</span>
train_filenames, test_and_valid_filenames = train_test_split(filenames, test_size=<span class="hljs-number">0.2</span>, random_state=random_state)
<span class="hljs-comment"># Separate valid+test data to validation and test data</span>
valid_filenames, test_filenames = train_test_split(test_and_valid_filenames, test_size=<span class="hljs-number">0.5</span>, random_state=random_state) 
<span class="hljs-comment"># Print out the sizes and some sample filenames</span>
<span class="hljs-keyword">for</span> subset_id, subset <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>((<span class="hljs-string">'train'</span>, <span class="hljs-string">'valid'</span>, <span class="hljs-string">'test'</span>), (train_filenames, valid_filenames, test_filenames)):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">"Got {} files in the {} dataset (e.g. </span>
<span class="hljs-built_in">    </span><span class="hljs-string">{})"</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(subset), subset_id, subset[:<span class="hljs-number">3</span>]))
</code></pre>
    <p class="normal">The <code class="inlineCode">train_test_split()</code> function takes an <code class="inlineCode">iterable</code> (e.g. list, tuple, array, etc.) as an input and splits it into two sets based on a defined split ratio. In this case, the input is a list of filenames and we first make a split of 80%-20% training and [validation + test] data. Then we further split the <code class="inlineCode">test_and_valid_filenames</code> 50%-50% to generate test and validation sets. Note how we also pass a random seed to the <code class="inlineCode">train_test_split</code> function to make sure we get the same split over multiple runs. </p>
    <p class="normal">This code will output the following text:</p>
    <pre class="programlisting con"><code class="hljs-con">Got 167 files in the train dataset (e.g. ['data\\117.txt', 'data\\133.txt', 'data\\069.txt'])
Got 21 files in the valid dataset (e.g. ['data\\023.txt', 'data\\078.txt', 'data\\176.txt'])
Got 21 files in the test dataset (e.g. ['data\\129.txt', 'data\\207.txt', 'data\\170.txt'])
</code></pre>
    <p class="normal">We can see<a id="_idIndexMarker747"/> that <a id="_idIndexMarker748"/>from our 209 files, we have<a id="_idIndexMarker749"/> roughly 80% of files allocated as training data, 10% as validation data, and the final 10% as testing data.</p>
    <h2 id="_idParaDest-189" class="heading-2">Analyzing the vocabulary size</h2>
    <p class="normal">We will be <a id="_idIndexMarker750"/>using bigrams (i.e. n-grams with <em class="italic">n=2</em>) to train our <a id="_idIndexMarker751"/>language model. That is, we will split the story into units of two characters. Furthermore, we will convert all characters to lowercase to reduce the input dimensionality. Using character-level bigrams helps us to language model with a reduced vocabulary, leading to faster model training. For example:</p>
    <p class="normal"><em class="italic">The king was hunting in the forest.</em></p>
    <p class="normal">would break down to a sequence of bigrams as follows:</p>
    <p class="normal"><em class="italic">[‘th’, ‘e ‘, ‘ki’, ‘ng’, ‘ w’, ‘as’, …]</em></p>
    <p class="normal">Let’s find out how large the vocabulary is. For that, we first define a <code class="inlineCode">set</code> object. Next, we go through each training file, read the content, and store that as a string in the variable document. </p>
    <p class="normal">Finally, we update the <code class="inlineCode">set</code> object with all the bigrams in the string containing each story. We get the bigrams by traversing the string two characters at a time:</p>
    <pre class="programlisting code"><code class="hljs-code">bigram_set = <span class="hljs-built_in">set</span>()
<span class="hljs-comment"># Go through each file in the training set</span>
<span class="hljs-keyword">for</span> fname <span class="hljs-keyword">in</span> train_filenames:
    document = [] <span class="hljs-comment"># This will hold all the text</span>
    <span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(fname, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> f:
        <span class="hljs-keyword">for</span> row <span class="hljs-keyword">in</span> f:
            <span class="hljs-comment"># Convert text to lower case to reduce input dimensionality</span>
            document.append(row.lower())
        <span class="hljs-comment"># From the list of text we have, generate one long string </span>
<span class="hljs-comment">        # (containing all training stories)</span>
        document = <span class="hljs-string">" "</span>.join(document)
        <span class="hljs-comment"># Update the set with all bigrams found</span>
        bigram_set.update([document[i:i+<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, 
        <span class="hljs-built_in">len</span>(document), <span class="hljs-number">2</span>)])
<span class="hljs-comment"># Assign to a variable and print </span>
n_vocab = <span class="hljs-built_in">len</span>(bigram_set)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Found {} unique bigrams"</span>.<span class="hljs-built_in">format</span>(n_vocab))
</code></pre>
    <p class="normal">This <a id="_idIndexMarker752"/>would <a id="_idIndexMarker753"/>print:</p>
    <pre class="programlisting con"><code class="hljs-con">Found 705 unique bigrams
</code></pre>
    <p class="normal">We have a vocabulary of 705 bigrams. It would have been a lot more if we decided to treat each word as a unit, as opposed to character-level bigrams.</p>
    <h2 id="_idParaDest-190" class="heading-2">Defining the tf.data pipeline</h2>
    <p class="normal">We will now<a id="_idIndexMarker754"/> define <a id="_idIndexMarker755"/>a fully fledged data pipeline that is capable of reading the files from the disk and transforming the content into a format or structure that can be used to train the model. The <code class="inlineCode">tf.data</code> API in TensorFlow allows you to define data pipelines that can manipulate data in specific ways to suite machine learning models. For that we will define a function called <code class="inlineCode">generate_tf_dataset()</code> that takes:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">filenames</code> – A list of filenames containing the text to be used for the model</li>
      <li class="bulletList"><code class="inlineCode">ngram_width</code> – Width of the n-grams to be extracted</li>
      <li class="bulletList"><code class="inlineCode">window_size</code> – Length of the sequence of n-grams to be used to generate a single data point for the model</li>
      <li class="bulletList"><code class="inlineCode">batch_size</code> – Size of the batch</li>
      <li class="bulletList"><code class="inlineCode">shuffle</code> – (defaults to <code class="inlineCode">False</code>) Whether to shuffle the data or not</li>
    </ul>
    <p class="normal">For example assume an <code class="inlineCode">ngram_width</code> of 2, <code class="inlineCode">batch size</code> of 1, and <code class="inlineCode">window_size</code> of 5. This function would take the string “<em class="italic">the king was hunting in the forest</em>” and output:</p>
    <pre class="programlisting con"><code class="hljs-con">Batch 1: ["th", "e ", "ki", " ng", " w"] -&gt; ["e ", "ki", "ng", " w", "as"]
Batch 2: ["as", " h", "un", "ti", "ng"] -&gt; [" h", "un", "ti", "ng", " i"]
…
</code></pre>
    <p class="normal">The left list in each batch represents the input sequence, and the right list represents the target sequence. Note how the right list is simply the left one shifted one to the right. Also note how there’s no overlap between the inputs in the two records. But in the actual <a id="_idIndexMarker756"/>function, we <a id="_idIndexMarker757"/>will maintain a small overlap between records. <em class="italic">Figure 8.1</em> illustrates the high-level process:</p>
    <figure class="mediaobject"><img src="../Images/B14070_08_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.1: The high-level steps of the data transformation we will be implementing with the tf.data API</p>
    <p class="normal">Let’s discuss the specifics of how the pipeline is implemented using TensorFlow’s <code class="inlineCode">tf.data</code> API. We define the code to generate the data pipeline as a reusable function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">generate_tf_dataset</span>(<span class="hljs-params">filenames, ngram_width, window_size, batch_size, shuffle=</span><span class="hljs-literal">False</span>):
    <span class="hljs-string">""" Generate batched data from a list of files speficied """</span>
    <span class="hljs-comment"># Read the data found in the documents</span>
    documents = []
    <span class="hljs-keyword">for</span> f <span class="hljs-keyword">in</span> filenames:
        doc = tf.io.read_file(f)
        doc = tf.strings.ngrams(    <span class="hljs-comment"># Generate ngrams from the string</span>
            tf.strings.bytes_split(
            <span class="hljs-comment"># Create a list of chars from a string</span>
                tf.strings.regex_replace(
                <span class="hljs-comment"># Replace new lines with space</span>
                    tf.strings.lower(    <span class="hljs-comment"># Convert string to lower case</span>
                        doc
                    ), <span class="hljs-string">"\n"</span>, <span class="hljs-string">" "</span>
                )
            ),
            ngram_width, separator=<span class="hljs-string">''</span>
        )
        documents.append(doc.numpy().tolist())
    
    <span class="hljs-comment"># documents is a list of list of strings, where each string is a story</span>
    <span class="hljs-comment"># From that we generate a ragged tensor</span>
    documents = tf.ragged.constant(documents)
    <span class="hljs-comment"># Create a dataset where each row in the ragged tensor would be a </span>
<span class="hljs-comment">    # sample</span>
    doc_dataset = tf.data.Dataset.from_tensor_slices(documents)
    <span class="hljs-comment"># We need to perform a quick transformation - tf.strings.ngrams </span>
<span class="hljs-comment">    # would generate</span> <span class="hljs-comment">all the ngrams (e.g. abcd -&gt; ab, bc, cd) with</span>
<span class="hljs-comment">    # overlap, however for our data</span> <span class="hljs-comment">we do not need the overlap, so we need</span>
<span class="hljs-comment">    # to skip the overlapping ngrams</span>
    <span class="hljs-comment"># The following line does that</span>
    doc_dataset = doc_dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x[::ngram_width])
    <span class="hljs-comment"># Here we are using a window function to generate windows from text</span>
    <span class="hljs-comment"># For a text sequence with window_size 3 and shift 1 you get</span>
    <span class="hljs-comment"># e.g. ab, cd, ef, gh, ij, ... -&gt; [ab, cd, ef], [cd, ef, gh], [ef, </span>
<span class="hljs-comment">    # gh, ij], ...</span>
    <span class="hljs-comment"># each of these windows is a single training sequence for our model</span>
    doc_dataset = doc_dataset.flat_map(
        <span class="hljs-keyword">lambda</span> x: tf.data.Dataset.from_tensor_slices(
            x
        ).window(
            size=window_size+<span class="hljs-number">1</span>, shift=<span class="hljs-built_in">int</span>(window_size * <span class="hljs-number">0.75</span>)
        ).flat_map(
            <span class="hljs-keyword">lambda</span> window: window.batch(window_size+<span class="hljs-number">1</span>, 
<span class="hljs-keyword">            </span>drop_remainder=<span class="hljs-literal">True</span>)
        )
    )
    
    <span class="hljs-comment"># From each windowed sequence we generate input and target tuple</span>
    <span class="hljs-comment"># e.g. [ab, cd, ef] -&gt; ([ab, cd], [cd, ef])</span>
    doc_dataset = doc_dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: (x[:-<span class="hljs-number">1</span>], x[<span class="hljs-number">1</span>:]))
    <span class="hljs-comment"># Batch the data</span>
    doc_dataset = doc_dataset.batch(batch_size=batch_size)
    <span class="hljs-comment"># Shuffle the data if required</span>
    doc_dataset = doc_dataset.shuffle(buffer_size=batch_size*<span class="hljs-number">10</span>) <span class="hljs-keyword">if</span> 
    shuffle <span class="hljs-keyword">else</span> doc_dataset
    
    <span class="hljs-comment"># Return the data</span>
    <span class="hljs-keyword">return</span> doc_dataset
</code></pre>
    <p class="normal">Let’s now <a id="_idIndexMarker758"/>discuss the <a id="_idIndexMarker759"/>above code in more detail. First we go through each file in the <code class="inlineCode">filenames</code> variable and read the content in each with:</p>
    <pre class="programlisting code"><code class="hljs-code">doc = tf.io.read_file(f)
</code></pre>
    <p class="normal">After the content is read, we generate n-grams from that using the <code class="inlineCode">tf.strings.ngrams()</code> function. However, this function excepts a list of chars as opposed to a string. </p>
    <p class="normal">Therefore, we convert the string into a list of chars with the <code class="inlineCode">tf.strings.bytes_split()</code> function. Additionally, we perform several preprocessing steps, such as:</p>
    <ul>
      <li class="bulletList">Converting text to lowercase with <code class="inlineCode">tf.strings.lower()</code></li>
      <li class="bulletList">Replacing new-line characters (<code class="inlineCode">\n</code>) with space to have a continuous stream of words</li>
    </ul>
    <p class="normal">Each of these stories is stored in a list object <code class="inlineCode">(documents)</code>. It is important to note that, <code class="inlineCode">tf.strings.ngrams()</code> produces all possible n-grams for a given n-gram length. In other words, consecutive n-grams would overlap. For example, the sequence “<em class="italic">The king was hunting</em>” with an n-gram length of 2 would produce <code class="inlineCode">["Th", "he", "e ", " k", …]</code>. Therefore, we will need an extra processing step later to remove the overlapping n-grams from the sequence. After all of them are read and processed, we create a <code class="inlineCode">RaggedTensor</code> object from the documents:</p>
    <pre class="programlisting code"><code class="hljs-code">documents = tf.ragged.constant(documents)
</code></pre>
    <p class="normal">A <code class="inlineCode">RaggedTensor</code> is a special type of tensor that can have dimensions that accept arbitrarily sized inputs. For example, it is almost impossible that all the stories would have the same number of n-grams in each as they vary from each other a lot. In this case, we <a id="_idIndexMarker760"/>will have <a id="_idIndexMarker761"/>arbitrarily long sequences of n-grams representing our stories. Therefore, we can use a <code class="inlineCode">RaggedTensor</code> to store these arbitrarily sized sequences.</p>
    <div class="note">
      <p class="normal"><code class="inlineCode">tf.RaggedTensor</code> objects are a special type of tensor that can have variable-sized dimensions. You can read more about ragged tensors at <a href="https://www.tensorflow.org/api_docs/python/tf/RaggedTensor"><span class="url">https://www.tensorflow.org/api_docs/python/tf/RaggedTensor</span></a>. There are many ways to define a ragged tensor.</p>
      <p class="normal">We can define a ragged tensor by passing a nested list containing values to the <code class="inlineCode">tf.ragged.constant()</code> function:</p>
      <pre class="programlisting code"><code class="hljs-code">a = tf.ragged.constant([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>], [<span class="hljs-number">1</span>,<span class="hljs-number">2</span>], [<span class="hljs-number">1</span>]])
</code></pre>
      <p class="normal">We can also define a flat sequence of values and define where to split the rows:</p>
      <pre class="programlisting code"><code class="hljs-code">b = tf.RaggedTensor.from_row_splits([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">7</span>],
row_splits=[<span class="hljs-number">0</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>])
</code></pre>
      <p class="normal">Here, each value in the <code class="inlineCode">row_splits</code> argument defines where the subsequent row in the resulting tensor ends. For example, the first row will contain elements from index 0 to 3 (i.e. 0, 1, 2). This will output:</p>
      <pre class="programlisting con"><code class="hljs-con">&lt;tf.RaggedTensor [[1, 2, 3], [], [4, 5, 6], [7]]&gt;
</code></pre>
      <p class="normal">You can get the shape of the tensor using <code class="inlineCode">b.shape</code>, which will return:</p>
      <pre class="programlisting con"><code class="hljs-con">[4, None]
</code></pre>
    </div>
    <p class="normal">Next, we create a <code class="inlineCode">tf.data.Dataset</code> from the tensor with the <code class="inlineCode">tf.data.Dataset.from_tensor_slices()</code> function. </p>
    <p class="normal">This function simply produces a dataset, where a single item in the dataset would be a row of the provided tensor. For example, if you provide a standard tensor of shape <code class="inlineCode">[10, 8, 6]</code>, it will produce 10 samples of shape <code class="inlineCode">[8, 6]</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">doc_dataset = tf.data.Dataset.from_tensor_slices(documents)
</code></pre>
    <p class="normal">Here, we simply get rid of the overlapping n-grams by taking only every <em class="italic">n</em><sup class="superscript">th</sup> n-gram in the sequence:</p>
    <pre class="programlisting code"><code class="hljs-code">doc_dataset = doc_dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: x[::ngram_width])
</code></pre>
    <p class="normal">We will then use the <code class="inlineCode">tf.data.Dataset.window()</code> function to create shorter, fixed-length windowed <a id="_idIndexMarker762"/>sequences<a id="_idIndexMarker763"/> from each story:</p>
    <pre class="programlisting code"><code class="hljs-code">doc_dataset = doc_dataset.flat_map(
    <span class="hljs-keyword">lambda</span> x: tf.data.Dataset.from_tensor_slices(
        x
    ).window(
        size=window_size+<span class="hljs-number">1</span>, shift=<span class="hljs-built_in">int</span>(window_size * <span class="hljs-number">0.75</span>)
    ).flat_map(
        <span class="hljs-keyword">lambda</span> window: window.batch(window_size+<span class="hljs-number">1</span>, 
<span class="hljs-keyword">        </span>drop_remainder=<span class="hljs-literal">True</span>)
    )
)
</code></pre>
    <p class="normal">From each window, we generate input and target pairs, as follows. We take all the n-grams except the last as inputs and all the n-grams except the first as targets. This way, at each time step, the model will be predicting the next n-gram given all the previous n-grams. The shift determines how much we shift the window at each iteration. Having some overlap between records make sure the model doesn’t treat the story as independent windows, which may lead to poor performance. We will maintain around 25% overlap between two consecutive sequences:</p>
    <pre class="programlisting code"><code class="hljs-code">doc_dataset = doc_dataset.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x: (x[:-<span class="hljs-number">1</span>], x[<span class="hljs-number">1</span>:]))
</code></pre>
    <p class="normal">We shuffle the data using <code class="inlineCode">tf.data.Dataset.shuffle()</code> and batch the data with a predefined batch size. Note that we have to specify a <code class="inlineCode">buffer_size</code> for the <code class="inlineCode">shuffle()</code> function. <code class="inlineCode">buffer_size</code> determines how much data is retrieved before shuffling. The more data you buffer, the better the shuffling would be, but also the worse the memory consumption would be:</p>
    <pre class="programlisting code"><code class="hljs-code">doc_dataset = doc_dataset.shuffle(buffer_size=batch_size*<span class="hljs-number">10</span>) <span class="hljs-keyword">if</span> shuffle <span class="hljs-keyword">else</span> doc_dataset
doc_dataset = doc_dataset.batch(batch_size=batch_size)
</code></pre>
    <p class="normal">Finally, we specify the necessary hyperparameters and generate three datasets: training, validation, and testing:</p>
    <pre class="programlisting code"><code class="hljs-code">ngram_length = <span class="hljs-number">2</span>
batch_size = <span class="hljs-number">256</span>
window_size = <span class="hljs-number">128</span>
train_ds = generate_tf_dataset(train_filenames, ngram_length, window_size, batch_size, shuffle=<span class="hljs-literal">True</span>)
valid_ds = generate_tf_dataset(valid_filenames, ngram_length, window_size, batch_size)
test_ds = generate_tf_dataset(test_filenames, ngram_length, window_size, batch_size)
</code></pre>
    <p class="normal">Let’s <a id="_idIndexMarker764"/>generate some<a id="_idIndexMarker765"/> data and look at the data generated by this function:</p>
    <pre class="programlisting code"><code class="hljs-code">ds = generate_tf_dataset(train_filenames, <span class="hljs-number">2</span>, window_size=<span class="hljs-number">10</span>, batch_size=<span class="hljs-number">1</span>).take(<span class="hljs-number">5</span>)
<span class="hljs-keyword">for</span> record <span class="hljs-keyword">in</span> ds:
        <span class="hljs-built_in">print</span>(record[<span class="hljs-number">0</span>].numpy(), <span class="hljs-string">'-&gt;'</span>, record[<span class="hljs-number">1</span>].numpy())
</code></pre>
    <p class="normal">This returns:</p>
    <pre class="programlisting con"><code class="hljs-con">[[b'th' b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ']] -&gt; [[b'er' b'e ' b'wa' b's ' b'on' b'ce' b' u' b'po' b'n ' b'a ']]
[[b' u' b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph']] -&gt; [[b'po' b'n ' b'a ' b'ti' b'me' b' a' b' s' b'he' b'ph' b'er']]
[[b' s' b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ']] -&gt; [[b'he' b'ph' b'er' b'd ' b'bo' b'y ' b'wh' b'os' b'e ' b'fa']]
…
</code></pre>
    <p class="normal">Here, you can see that the target sequence is just the input sequence shifted one to the right. The <code class="inlineCode">b</code> in front of the characters denotes that the characters are stored as bytes. Next, we will look at how we can implement the model. </p>
    <h1 id="_idParaDest-191" class="heading-1">Implementing the language model</h1>
    <p class="normal">Here, we will <a id="_idIndexMarker766"/>discuss the details of the LSTM implementation. </p>
    <p class="normal">First, we will discuss the hyperparameters that are used for the LSTM and their effects. </p>
    <p class="normal">Thereafter, we will discuss the parameters (weights and biases) required to implement the LSTM. We will then discuss how these parameters are used to write the operations taking place within the LSTM. This will be followed by understanding how we will sequentially feed data to the LSTM. Next, we will discuss how to train the model. Finally, we will investigate how we can use the learned model to output predictions, which are essentially bigrams that will eventually add up to a meaningful story.</p>
    <h2 id="_idParaDest-192" class="heading-2">Defining the TextVectorization layer</h2>
    <p class="normal">We<a id="_idIndexMarker767"/> discussed the <code class="inlineCode">TextVectorization</code> layer<a id="_idIndexMarker768"/> and used it in <em class="chapterRef">Chapter 6, R</em><em class="italic">ecurrent Neural Networks</em>. We’ll be using the same text vectorization mechanism to tokenize text. In summary, the <code class="inlineCode">TextVectorization</code> layer provides you with a convenient way to integrate text tokenization (i.e. converting strings into a list of tokens that are represented by integer IDs) into the model as a layer.</p>
    <p class="normal">Here, we will define a <code class="inlineCode">TextVectorization</code> layer to convert the sequences of n-grams to sequences of integer IDs:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow.keras.layers <span class="hljs-keyword">as</span> layers
<span class="hljs-keyword">import</span> tensorflow.keras.models <span class="hljs-keyword">as</span> models
<span class="hljs-comment"># The vectorization layer that will convert string bigrams to IDs</span>
text_vectorizer = tf.keras.layers.TextVectorization(
    max_tokens=n_vocab, standardize=<span class="hljs-literal">None</span>,
    split=<span class="hljs-literal">None</span>, input_shape=(window_size,)
)
</code></pre>
    <p class="normal">Note that we are defining several important arguments, such as the <code class="inlineCode">max_tokens</code> (size of the vocabulary), the <code class="inlineCode">standardize</code> argument to not perform any text preprocessing, the <code class="inlineCode">split</code> argument to not perform any splitting, and finally, the <code class="inlineCode">input_shape</code> argument to inform the layer that the input will be a batch of sequences of n-grams. With that, we have to train the text vectorization layer to recognize the available n-grams and map them to unique IDs. We can simply pass our training <code class="inlineCode">tf.data</code> pipeline to this layer to learn the n-grams.</p>
    <pre class="programlisting code"><code class="hljs-code">text_vectorizer.adapt(train_ds)
</code></pre>
    <p class="normal">Next, let’s print the words in the vocabulary to see what this layer has learned:</p>
    <pre class="programlisting code"><code class="hljs-code">text_vectorizer.get_vocabulary()[:<span class="hljs-number">10</span>]
</code></pre>
    <p class="normal">This will output:</p>
    <pre class="programlisting con"><code class="hljs-con">['', '[UNK]', 'e ', 'he', ' t', 'th', 'd ', ' a', ', ', ' h']
</code></pre>
    <p class="normal">Once the <code class="inlineCode">TextVectorization</code> layer is trained, we have to modify our training, validation, and testing data pipelines slightly. Remember that our data pipelines output sequences of n-gram strings as inputs and targets. We need to convert the target sequences to sequences of n-gram IDs so that a loss can be computed. For that we will simply pass the targets in the datasets through the <code class="inlineCode">text_vectorizer</code> layer using the <code class="inlineCode">tf.data.Dataset.map()</code> functionality:</p>
    <pre class="programlisting code"><code class="hljs-code">train_ds = train_ds.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x, y: (x, text_vectorizer(y)))
valid_ds = valid_ds.<span class="hljs-built_in">map</span>(<span class="hljs-keyword">lambda</span> x, y: (x, text_vectorizer(y)))
</code></pre>
    <p class="normal">Next, we <a id="_idIndexMarker769"/>will look at the LSTM-based model we’ll <a id="_idIndexMarker770"/>be using. We’ll go through various components of the model such as the embedding layer, LSTM layers, and the final prediction layer.</p>
    <h2 id="_idParaDest-193" class="heading-2">Defining the LSTM model</h2>
    <p class="normal">We will <a id="_idIndexMarker771"/>define a<a id="_idIndexMarker772"/> simple LSTM-based model. Our model will have:</p>
    <ul>
      <li class="bulletList">The previously trained <code class="inlineCode">TextVectorization</code> layer</li>
      <li class="bulletList">An embedding layer randomly initialized and jointly trained with the model</li>
      <li class="bulletList">Two LSTM layers each with 512 and 256 nodes respectively</li>
      <li class="bulletList">A fully-connected hidden layer with 1024 nodes and ReLU activation</li>
      <li class="bulletList">The final prediction layer with <code class="inlineCode">n_vocab</code> nodes and <code class="inlineCode">softmax</code> activation</li>
    </ul>
    <p class="normal">Since the model is quite straightforward with the layers defined sequentially, we will use the Sequential API to define this model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow.keras.backend <span class="hljs-keyword">as</span> K
K.clear_session()
lm_model = models.Sequential([
    text_vectorizer,
    layers.Embedding(n_vocab+<span class="hljs-number">2</span>, <span class="hljs-number">96</span>),
    layers.LSTM(<span class="hljs-number">512</span>, return_state=<span class="hljs-literal">False</span>, return_sequences=<span class="hljs-literal">True</span>),
    layers.LSTM(<span class="hljs-number">256</span>, return_state=<span class="hljs-literal">False</span>, return_sequences=<span class="hljs-literal">True</span>),
    layers.Dense(<span class="hljs-number">1024</span>, activation=<span class="hljs-string">'relu'</span>),
    layers.Dropout(<span class="hljs-number">0.5</span>),
    layers.Dense(n_vocab, activation=<span class="hljs-string">'softmax'</span>)
])
</code></pre>
    <p class="normal">We start by calling <code class="inlineCode">K.clear_session()</code>, which is a function that clears the current TensorFlow session (e.g. layers and variables defined and their states). Otherwise, if you run multiple times in a notebook, it will create an unnecessary number of layers and variables. Additionally, let’s<a id="_idIndexMarker773"/> look at the parameters<a id="_idIndexMarker774"/> of the LSTM layer in more detail: </p>
    <ul>
      <li class="bulletList"><code class="inlineCode">return_state</code> – Setting this to<a id="_idIndexMarker775"/> <code class="inlineCode">False</code> means that the layer outputs only the final output, whereas if set to <code class="inlineCode">True</code>, it will return state vectors along with the final output of the layer. For example, for an LSTM layer, setting <code class="inlineCode">return_state=True</code> means you’ll get three outputs: the final output, cell state, and hidden state. Note that the final output and the hidden state will be identical in this case.</li>
      <li class="bulletList"><code class="inlineCode">return_sequences</code> – Setting this to true will cause the layer to output the full output sequences, as opposed to just the last output. For example, setting this to false will give you a [<em class="italic">b, n</em>]-sized output where <em class="italic">b</em> is the batch size and <em class="italic">n</em> is the number of nodes in the layer. If true, it will output a [<em class="italic">b, t, n</em>]-sized output, where <em class="italic">t</em> is the number of time steps.</li>
    </ul>
    <p class="normal">You can see a summary of this model by executing:</p>
    <pre class="programlisting code"><code class="hljs-code">lm_model.summary()
</code></pre>
    <p class="normal">which returns:</p>
    <pre class="programlisting con"><code class="hljs-con">Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 text_vectorization (TextVec  multiple                 0         
 torization)                                                     
                                                                 
 embedding (Embedding)       (None, 128, 96)           67872     
                                                                 
 lstm (LSTM)                 (None, 128, 512)          1247232   
                                                                 
 lstm_1 (LSTM)               (None, 128, 256)          787456    
                                                                 
 dense (Dense)               (None, 128, 1024)         263168    
                                                                 
 dropout (Dropout)           (None, 128, 1024)         0         
                                                                 
 dense_1 (Dense)             (None, 128, 705)          722625    
                                                                 
=================================================================
Total params: 3,088,353
Trainable params: 3,088,353
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">Next, let’s <a id="_idIndexMarker776"/>look at the metrics we can use to track <a id="_idIndexMarker777"/>model performance and finally compile the model with appropriate loss, optimizer, and metrics.</p>
    <h2 id="_idParaDest-194" class="heading-2">Defining metrics and compiling the model</h2>
    <p class="normal">For our language<a id="_idIndexMarker778"/> model, we <a id="_idIndexMarker779"/>have to define a performance metric that we can use to demonstrate how good the model is. We have typically seen accuracy being used widely as a general-purpose evaluation metric across different ML tasks. However, accuracy might not be cut out for this task, mainly because it relies on the model choosing the exact word/bigram for a given time step as in the dataset. However, languages are complex and there can be many different choices to generate the next word/bigram given a text. Therefore, NLP practitioners rely on a metric <a id="_idIndexMarker780"/>known as <strong class="keyWord">perplexity</strong>, which measures how “perplexed” or “surprised” the model was to see a <em class="italic">t</em>+1 bigram given 1:<em class="italic">t </em>bigrams.</p>
    <p class="normal">Perplexity computation is simple. It’s simply the entropy to the power of two. Entropy is a measure of the uncertainty or randomness of an event. The more uncertain the outcome of the event, the higher the entropy (to learn more about entropy visit <a href="https://machinelearningmastery.com/what-is-information-entropy/"><span class="url">https://machinelearningmastery.com/what-is-information-entropy/</span></a>). Entropy is computed as:</p>
    <p class="center"><img src="../Images/B14070_08_001.png" alt="" style="height: 2.70em !important;"/></p>
    <p class="normal">In machine learning, to optimize ML models, we measure the difference between the predicted probability distribution versus the target probability distribution for a given sample. For that, we use cross-entropy, an extension of entropy for two distributions: </p>
    <p class="center"><img src="../Images/B14070_08_002.png" alt="" style="height: 3.33em !important;"/></p>
    <p class="normal">Finally, we define perplexity as:</p>
    <p class="center"><img src="../Images/B14070_08_003.png" alt="" style="height: 1.25em !important;"/></p>
    <p class="normal">To learn more about the relationship between cross entropy and perplexity visit <a href="https://thegradient.pub/understanding-evaluation-metrics-for-language-models/"><span class="url">https://thegradient.pub/understanding-evaluation-metrics-for-language-models/</span></a>.</p>
    <p class="normal">In TensorFlow, we define a custom <code class="inlineCode">tf.keras.metrics.Metric</code> object to compute perplexity. We are going to use <code class="inlineCode">tf.keras.metrics.Mean</code> as our super-class as it already knows <a id="_idIndexMarker781"/>how to compute and track the mean value of<a id="_idIndexMarker782"/> a given metric:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">PerplexityMetric</span>(tf.keras.metrics.Mean):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, name=</span><span class="hljs-string">'perplexity'</span><span class="hljs-params">, **kwargs</span>):
        <span class="hljs-built_in">super</span>().__init__(name=name, **kwargs)
        self.cross_entropy = 
        tf.keras.losses.SparseCategoricalCrossentropy(
        from_logits=<span class="hljs-literal">False</span>, reduction=<span class="hljs-string">'none'</span>)
    <span class="hljs-keyword">def</span> <span class="hljs-title">_calculate_perplexity</span>(<span class="hljs-params">self, real, pred</span>):
        
        <span class="hljs-comment"># The next 4 lines zero-out the padding from loss </span>
<span class="hljs-comment">        # calculations, this follows the logic from: </span>
<span class="hljs-comment">        # https://www.tensorflow.org/beta/tutorials/text/transformer#loss_</span>
<span class="hljs-comment">        # and_metrics </span>
        loss_ = self.cross_entropy(real, pred)
        <span class="hljs-comment"># Calculating the perplexity steps: </span>
        step1 = K.mean(loss_, axis=-<span class="hljs-number">1</span>)
        perplexity = K.exp(step1)
        <span class="hljs-keyword">return</span> perplexity 
    <span class="hljs-keyword">def</span> <span class="hljs-title">update_state</span>(<span class="hljs-params">self, y_true, y_pred, sample_weight=</span><span class="hljs-literal">None</span>):
        perplexity = self._calculate_perplexity(y_true, y_pred)
        <span class="hljs-built_in">super</span>().update_state(perplexity)
</code></pre>
    <p class="normal">Here we are simply computing the cross-entropy loss for a given batch of predictions and targets using the built-in <code class="inlineCode">SparseCategoricalCrossentropy</code> loss object. Then we raise it to the power of exponential to get the perplexity. We will now compile our model using:</p>
    <ul>
      <li class="bulletList">Sparse categorical cross-entropy as our loss function</li>
      <li class="bulletList">Adam as our optimizer</li>
      <li class="bulletList">Accuracy and perplexity as our metrics</li>
    </ul>
    <pre class="programlisting code"><code class="hljs-code">lm_model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'sparse_categorical_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>, metrics=[<span class="hljs-string">'accuracy'</span>, PerplexityMetric()])
</code></pre>
    <p class="normal">Here, the<a id="_idIndexMarker783"/> perplexity metric will be tracked during model<a id="_idIndexMarker784"/> training and validation and be printed out, similar to the accuracy metric.</p>
    <h2 id="_idParaDest-195" class="heading-2">Training the model</h2>
    <p class="normal">It’s time to train <a id="_idIndexMarker785"/>our model. Since we have done all the heavy-lifting required (e.g. reading files, preprocessing and transforming text, and compiling the model), all we have to do is call our model with the <code class="inlineCode">fit()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">lm_model.fit(train_ds, validation_data=valid_ds, epochs=<span class="hljs-number">60</span>)
</code></pre>
    <p class="normal">Here we are passing <code class="inlineCode">train_ds</code> (training data pipeline) as the first argument and <code class="inlineCode">valid_ds</code> (validation data pipeline) for the <code class="inlineCode">validation_data</code> argument, and setting the training to run for 60 epochs. Once the model is trained, let us evaluate it on the test dataset by simply calling:</p>
    <pre class="programlisting code"><code class="hljs-code">lm_model.evaluate(test_ds)
</code></pre>
    <p class="normal">This gives the following output:</p>
    <pre class="programlisting con"><code class="hljs-con">5/5 [==============================] - 0s 45ms/step - loss: 2.4742 - accuracy: 0.3968 - perplexity: 12.3155
</code></pre>
    <p class="normal">You might have slight variations in the metrics you see, but it should roughly converge to the same value.</p>
    <h2 id="_idParaDest-196" class="heading-2">Defining the inference model</h2>
    <p class="normal">During training, we<a id="_idIndexMarker786"/> trained our model and evaluated it on sequences of bigrams. This works for us because during training and evaluation, we have the full text available to us. However, when we need to generate new text, we do not have anything available to us. Therefore, we have to make adjustments to our trained model so that it can generate text from scratch.</p>
    <p class="normal">The way we do this is by defining a recursive model that takes the current time step’s output of the model as the input to the next time step. This way we can keep predicting words/bigrams for an infinite number of steps. We provide the initial seed as a random word/bigram picked from the corpus (or even a sequence of bigrams). </p>
    <p class="normal"><em class="italic">Figure 8.2</em> illustrates how the inference model works.</p>
    <figure class="mediaobject"><img src="../Images/B14070_08_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.2: The operational view of the inference model we’ll be building from our trained model</p>
    <p class="normal">Our inference model<a id="_idIndexMarker787"/> is going to be comparatively more sophisticated, as we need to design an iterative process to generate text using previous predictions as inputs. Therefore, we will be using Keras’s Functional API to implement the model:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define inputs to the model</span>
inp = tf.keras.layers.Input(dtype=tf.string, shape=(<span class="hljs-number">1</span>,))
inp_state_c_lstm = tf.keras.layers.Input(shape=(<span class="hljs-number">512</span>,))
inp_state_h_lstm = tf.keras.layers.Input(shape=(<span class="hljs-number">512</span>,))
inp_state_c_lstm_1 = tf.keras.layers.Input(shape=(<span class="hljs-number">256</span>,))
inp_state_h_lstm_1 = tf.keras.layers.Input(shape=(<span class="hljs-number">256</span>,))
text_vectorized_out = lm_model.get_layer(<span class="hljs-string">'text_vectorization'</span>)(inp)
<span class="hljs-comment"># Define embedding layer and output</span>
emb_layer = lm_model.get_layer(<span class="hljs-string">'embedding'</span>)
emb_out = emb_layer(text_vectorized_out)
<span class="hljs-comment"># Defining a LSTM layers and output</span>
lstm_layer = tf.keras.layers.LSTM(<span class="hljs-number">512</span>, return_state=<span class="hljs-literal">True</span>, return_sequences=<span class="hljs-literal">True</span>)
lstm_out, lstm_state_c, lstm_state_h = lstm_layer(emb_out, initial_state=[inp_state_c_lstm, inp_state_h_lstm])
lstm_1_layer = tf.keras.layers.LSTM(<span class="hljs-number">256</span>, return_state=<span class="hljs-literal">True</span>, return_sequences=<span class="hljs-literal">True</span>)
lstm_1_out, lstm_1_state_c, lstm_1_state_h = lstm_1_layer(lstm_out, initial_state=[inp_state_c_lstm_1, inp_state_h_lstm_1])
<span class="hljs-comment"># Defining a Dense layer and output</span>
dense_out = lm_model.get_layer(<span class="hljs-string">'dense'</span>)(lstm_1_out)
<span class="hljs-comment"># Defining the final Dense layer and output</span>
final_out = lm_model.get_layer(<span class="hljs-string">'dense_1'</span>)(dense_out)
<span class="hljs-comment"># Copy the weights from the original model</span>
lstm_layer.set_weights(lm_model.get_layer(<span class="hljs-string">'lstm'</span>).get_weights())
lstm_1_layer.set_weights(lm_model.get_layer(<span class="hljs-string">'lstm_1'</span>).get_weights())
<span class="hljs-comment"># Define final model</span>
infer_model = tf.keras.models.Model(
    inputs=[inp, inp_state_c_lstm, inp_state_h_lstm, 
    inp_state_c_lstm_1, inp_state_h_lstm_1], 
    outputs=[final_out, lstm_state_c, lstm_state_h, lstm_1_state_c, 
    lstm_1_state_h])
</code></pre>
    <p class="normal">We start by defining <a id="_idIndexMarker788"/>an input layer that takes an input having one time step. </p>
    <p class="normal">Note that we are defining the <code class="inlineCode">shape</code> argument. This means it can accept an arbitrarily sized batch of data (as long as it has one time step). We also define several other inputs to maintain the states of the LSTM layers we have. This is because we have to maintain state vectors of LSTM layers explicitly as we are recursively generating outputs from the model:</p>
    <pre class="programlisting code"><code class="hljs-code">inp = tf.keras.layers.Input(dtype=tf.string, shape=(<span class="hljs-number">1</span>,))
inp_state_c_lstm = tf.keras.layers.Input(shape=(<span class="hljs-number">512</span>,))
inp_state_h_lstm = tf.keras.layers.Input(shape=(<span class="hljs-number">512</span>,))
inp_state_c_lstm_1 = tf.keras.layers.Input(shape=(<span class="hljs-number">256</span>,))
inp_state_h_lstm_1 = tf.keras.layers.Input(shape=(<span class="hljs-number">256</span>,))
</code></pre>
    <p class="normal">Next we retrieve the trained model’s <code class="inlineCode">text_vectorization</code> layer and transform the text to integer IDs using it:</p>
    <pre class="programlisting code"><code class="hljs-code">text_vectorized_out = lm_model.get_layer(<span class="hljs-string">'text_vectorization'</span>)(inp)
</code></pre>
    <p class="normal">Then we obtain the embeddings layer of the train model and use it to generate the embedding output:</p>
    <pre class="programlisting code"><code class="hljs-code">emb_layer = lm_model.get_layer(<span class="hljs-string">'embedding'</span>)
emb_out = emb_layer(text_vectorized_out)
</code></pre>
    <p class="normal">We will create a fresh new LSTM layer to represent the first LSTM layer in the trained model. This is because the inference LSTM layers will have slight differences to the trained LSTM layers. Therefore, we will define new layers and copy the trained weights over later. We set the <code class="inlineCode">return_state</code> argument to <code class="inlineCode">True</code>. By setting this to true we get three outputs <a id="_idIndexMarker789"/>when we call the layer with an input: the final output, the cell state, and the final state vector. Note how we are also passing another argument called <code class="inlineCode">initial_state</code>. The <code class="inlineCode">initial_state</code> needs to be a list of tensors: the cell state and the final state vector, in that order. We are passing the input layers as those states and will populate them accordingly during runtime:</p>
    <pre class="programlisting code"><code class="hljs-code">lstm_layer = tf.keras.layers.LSTM(<span class="hljs-number">512</span>, return_state=<span class="hljs-literal">True</span>, return_sequences=<span class="hljs-literal">True</span>)
lstm_out, lstm_state_c, lstm_state_h = lstm_layer(emb_out, initial_state=[inp_state_c_lstm, inp_state_h_lstm])
</code></pre>
    <p class="normal">Similarly, the second LSTM layer will be defined. We get the dense layers and replicate the fully connected layers found in the trained model. Note that we don’t use <code class="inlineCode">softmax</code> in the last layer. </p>
    <p class="normal">This is because at inference time <code class="inlineCode">softmax</code> is only an overhead, as we only need the output class with the highest output score (i.e. it doesn’t need to be a probability distribution):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Defining a Dense layer and output</span>
dense_out = lm_model.get_layer(<span class="hljs-string">'dense'</span>)(lstm_1_out)
<span class="hljs-comment"># Defining the final Dense layer and output</span>
final_out = lm_model.get_layer(<span class="hljs-string">'dense_1'</span>)(dense_out)
</code></pre>
    <p class="normal">Don’t forget to copy the weights of the trained LSTM layers to our newly created LSTM layers:</p>
    <pre class="programlisting code"><code class="hljs-code">lstm_layer.set_weights(lm_model.get_layer(<span class="hljs-string">'lstm'</span>).get_weights())
lstm_1_layer.set_weights(lm_model.get_layer(<span class="hljs-string">'lstm_1'</span>).get_weights())
</code></pre>
    <p class="normal">Finally, we define the model:</p>
    <pre class="programlisting code"><code class="hljs-code">infer_model = tf.keras.models.Model(
    inputs=[inp, inp_state_c_lstm, inp_state_h_lstm, 
    inp_state_c_lstm_1, inp_state_h_lstm_1], 
    outputs=[final_out, lstm_state_c, lstm_state_h, lstm_1_state_c, 
    lstm_1_state_h])
</code></pre>
    <p class="normal">Our model takes a sequence of 1 bigram as the input, along with state vectors of both LSTM layers, and <a id="_idIndexMarker790"/>outputs the final prediction probabilities and the new state vectors of both LSTM layers. Let us now generate new text from the model.</p>
    <h2 id="_idParaDest-197" class="heading-2">Generating new text with the model</h2>
    <p class="normal">We’ll use our <a id="_idIndexMarker791"/>new <a id="_idIndexMarker792"/>inference model to generate a story. We will define an initial seed that we will use to generate a story. Here, we take the first phrase from one of the test files. Then we use it to generate text recursively, by using the predicted bigram at time <em class="italic">t</em> as the input at time <em class="italic">t</em>+1. We will run this for 500 steps:</p>
    <pre class="programlisting code"><code class="hljs-code">text = [<span class="hljs-string">"When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren ground"</span>]
seq = [text[<span class="hljs-number">0</span>][i:i+<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>, <span class="hljs-built_in">len</span>(text[<span class="hljs-number">0</span>]), <span class="hljs-number">2</span>)]
<span class="hljs-comment"># build up model state using the given string</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Making predictions from a {} element long input"</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(seq)))
vocabulary = infer_model.get_layer(<span class="hljs-string">"text_vectorization"</span>).get_vocabulary()
index_word = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(vocabulary)), vocabulary))
<span class="hljs-comment"># Reset the state of the model initially</span>
infer_model.reset_states()
<span class="hljs-comment"># Defining the initial state as all zeros</span>
state_c = np.zeros(shape=(<span class="hljs-number">1</span>,<span class="hljs-number">512</span>))
state_h = np.zeros(shape=(<span class="hljs-number">1</span>,<span class="hljs-number">512</span>))
state_c_1 = np.zeros(shape=(<span class="hljs-number">1</span>,<span class="hljs-number">256</span>))
state_h_1 = np.zeros(shape=(<span class="hljs-number">1</span>,<span class="hljs-number">256</span>))
<span class="hljs-comment"># Recursively update the model by assigning new state to state</span>
<span class="hljs-keyword">for</span> c <span class="hljs-keyword">in</span> seq:    
    <span class="hljs-comment">#print(c)</span>
    out, state_c, state_h, state_c_1, state_h_1 = infer_model.predict(
        [np.array([[c]]), state_c, state_h, state_c_1, state_h_1]
)
<span class="hljs-comment"># Get final prediction after feeding the input string</span>
wid = <span class="hljs-built_in">int</span>(np.argmax(out[<span class="hljs-number">0</span>],axis=-<span class="hljs-number">1</span>).ravel())
word = index_word[wid]
text.append(word)
<span class="hljs-comment"># Define first input to generate text recursively from</span>
x = np.array([[word]])
<span class="hljs-comment"># Code listing 10.7</span>
<span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">500</span>):
    
    <span class="hljs-comment"># Get the next output and state</span>
    out, state_c, state_h, state_c_1, state_h_1  = 
    infer_model.predict([x, state_c, state_h, state_c_1, state_h_1 ])
    
    <span class="hljs-comment"># Get the word id and the word from out</span>
    out_argsort = np.argsort(out[<span class="hljs-number">0</span>], axis=-<span class="hljs-number">1</span>).ravel()
    wid = <span class="hljs-built_in">int</span>(out_argsort[-<span class="hljs-number">1</span>])
    word = index_word[wid]
    
    <span class="hljs-comment"># If the word ends with space, we introduce a bit of randomness</span>
    <span class="hljs-comment"># Essentially pick one of the top 3 outputs for that timestep </span>
<span class="hljs-comment">    # depending on their likelihood</span>
    <span class="hljs-keyword">if</span> word.endswith(<span class="hljs-string">' '</span>):
        <span class="hljs-keyword">if</span> np.random.normal()&gt;<span class="hljs-number">0.5</span>:
            width = <span class="hljs-number">5</span>
            i = np.random.choice(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(-width,<span class="hljs-number">0</span>)), 
            p=out_argsort[-width:]/out_argsort[-width:].<span class="hljs-built_in">sum</span>())
            wid = <span class="hljs-built_in">int</span>(out_argsort[i])    
            word = index_word[wid]
            
    <span class="hljs-comment"># Append the prediction</span>
    text.append(word)
    
    <span class="hljs-comment"># Recursively make the current prediction the next input</span>
    x = np.array([[word]])
    
<span class="hljs-comment"># Print the final output    </span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">'\n'</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">'='</span>*<span class="hljs-number">60</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Final text: "</span>)
<span class="hljs-built_in">print</span>(<span class="hljs-string">''</span>.join(text))
</code></pre>
    <p class="normal">Notice how we are recursively using the variables <code class="inlineCode">x</code>, <code class="inlineCode">state_c</code>, <code class="inlineCode">state_h</code>, <code class="inlineCode">state_c_1</code>, and <code class="inlineCode">state_h_1</code> to generate and assign new values.</p>
    <pre class="programlisting code"><code class="hljs-code">    out, state_c, state_h, state_c_1, state_h_1  = 
    infer_model.predict([x, state_c, state_h, state_c_1, state_h_1 ])
</code></pre>
    <p class="normal">Moreover, we <a id="_idIndexMarker793"/>will use a simple condition to diversify the <a id="_idIndexMarker794"/>inputs we are generating:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> word.endswith(<span class="hljs-string">' '</span>):
        <span class="hljs-keyword">if</span> np.random.normal()&gt;<span class="hljs-number">0.5</span>:
            width = <span class="hljs-number">5</span>
            i = np.random.choice(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(-width,<span class="hljs-number">0</span>)), 
            p=out_argsort[-width:]/out_argsort[-width:].<span class="hljs-built_in">sum</span>())
            wid = <span class="hljs-built_in">int</span>(out_argsort[i])    
            word = index_word[wid]
</code></pre>
    <p class="normal">Essentially, if the predicted bigram ends with the <code class="inlineCode">' '</code> character, we will choose the next bigram randomly, from the top five bigrams. Each bigram will be chosen according to its predicted likelihood. Let’s see what the output text looks like:</p>
    <pre class="programlisting con"><code class="hljs-con">When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren groundy the king's daughter and said, i will so the king's daughter angry this they were and said, "i will so the king's daughter.  the king's daughter.' they were to the forest of the stork.  then the king's daughters, and they were to the forest of the stork, and, and then they were to the forest.  ...
</code></pre>
    <p class="normal">It seems our model is able to generate actual words and phrases that make sense. Next we will investigate how the text generated from standard LSTMs compares to other models, such<a id="_idIndexMarker795"/> as<a id="_idIndexMarker796"/> LSTMs with peepholes and GRUs.</p>
    <h1 id="_idParaDest-198" class="heading-1">Comparing LSTMs to LSTMs with peephole connections and GRUs</h1>
    <p class="normal">Now we will compare LSTMs to LSTMs with peepholes and GRUs in the text generation task. This will help us to compare how well different models (LSTMs with peepholes and GRUs) perform in terms of perplexity. Remember that we prefer perplexity over accuracy, as accuracy assumes there’s only one correct token given a previous input sequence. However, as we have learned, language is complex and there can be many different correct ways to generate text given previous inputs. This is available as an exercise in <code class="inlineCode">ch08_lstms_for_text_generation.ipynb</code> located in the <code class="inlineCode">Ch08-Language-Modelling-with-LSTMs</code> folder.</p>
    <h2 id="_idParaDest-199" class="heading-2">Standard LSTM</h2>
    <p class="normal">First, we will reiterate <a id="_idIndexMarker797"/>the components of a standard LSTM. We will not repeat the code for standard LSTMs as it is identical to what we discussed previously. Finally, we will see some text generated by an LSTM.</p>
    <h3 id="_idParaDest-200" class="heading-3">Review</h3>
    <p class="normal">Here, we will revisit what a <a id="_idIndexMarker798"/>standard LSTM looks like. As we already mentioned, an LSTM consists of the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Input gate</strong> – This decides how much of the current input is written to the cell state</li>
      <li class="bulletList"><strong class="keyWord">Forget gate</strong> – This decides how much of the previous cell state is written to the current cell state</li>
      <li class="bulletList"><strong class="keyWord">Output gate</strong> – This decides how much information from the cell state is exposed to output into the external hidden state</li>
    </ul>
    <p class="normal">In <em class="italic">Figure 8.3</em>, we illustrate how each of these gates, inputs, cell states, and the external hidden states are connected:</p>
    <figure class="mediaobject"><img src="../Images/B14070_08_03.png" alt="Review"/></figure>
    <p class="packt_figref">Figure 8.3: An LSTM cell</p>
    <h2 id="_idParaDest-201" class="heading-2">Gated Recurrent Units (GRUs)</h2>
    <p class="normal">Here we will first <a id="_idIndexMarker799"/>briefly delineate what a GRU is composed of, followed by showing the code for implementing a GRU cell. Finally, we look at some code generated by a GRU cell.</p>
    <h3 id="_idParaDest-202" class="heading-3">Review</h3>
    <p class="normal">Let’s briefly revisit<a id="_idIndexMarker800"/> what a GRU is. A GRU is an elegant simplification of the operations of an LSTM. A GRU introduces two different modifications to an LSTM (see <em class="italic">Figure 8.4</em>):</p>
    <ul>
      <li class="bulletList">It connects the internal cell state and the external hidden state into a single state</li>
      <li class="bulletList">Then it combines the input gate and the forget gate into one update gate</li>
    </ul>
    <figure class="mediaobject"><img src="../Images/B14070_08_04.png" alt="Review"/></figure>
    <p class="packt_figref">Figure 8.4: A GRU cell</p>
    <p class="normal">The GRU model uses a simpler gating mechanism than the LSTM. However, it still manages to <a id="_idIndexMarker801"/>capture important capabilities such as memory updates, forgets, etc.</p>
    <h3 id="_idParaDest-203" class="heading-3">The model</h3>
    <p class="normal">Here we<a id="_idIndexMarker802"/> will define a GRU-based language model:</p>
    <pre class="programlisting code"><code class="hljs-code">text_vectorizer = tf.keras.layers.TextVectorization(
    max_tokens=n_vocab, standardize=<span class="hljs-literal">None</span>,
    split=<span class="hljs-literal">None</span>, input_shape=(window_size,)
)
<span class="hljs-comment"># Train the model on existing data</span>
text_vectorizer.adapt(train_ds)
lm_gru_model = models.Sequential([
    text_vectorizer,
    layers.Embedding(n_vocab+<span class="hljs-number">2</span>, <span class="hljs-number">96</span>),
    layers.GRU(<span class="hljs-number">512</span>, return_sequences=<span class="hljs-literal">True</span>),
    layers.GRU(<span class="hljs-number">256</span>, return_sequences=<span class="hljs-literal">True</span>),
    layers.Dense(<span class="hljs-number">1024</span>, activation=<span class="hljs-string">'relu'</span>),
    layers.Dropout(<span class="hljs-number">0.5</span>),
    layers.Dense(n_vocab, activation=<span class="hljs-string">'softmax'</span>)
])
</code></pre>
    <p class="normal">The training code is identical to how we trained the LSTM-based model. Therefore, we won’t duplicate our discussion here. Next we’ll look at a slightly different variant of LSTM models.</p>
    <h2 id="_idParaDest-204" class="heading-2">LSTMs with peepholes</h2>
    <p class="normal">Here we will<a id="_idIndexMarker803"/> discuss LSTMs with peepholes and how they are different from a standard LSTM. After that, we will discuss their implementation.</p>
    <h3 id="_idParaDest-205" class="heading-3">Review</h3>
    <p class="normal">Now, let’s briefly <a id="_idIndexMarker804"/>look at LSTMs with peepholes. Peepholes are essentially a way for the gates (input, forget, and output) to directly see the cell <a id="_idIndexMarker805"/>state, instead of waiting for the external hidden state (see <em class="italic">Figure 8.5</em>):</p>
    <figure class="mediaobject"><img src="../Images/B14070_08_05.png" alt="Review"/></figure>
    <p class="packt_figref">Figure 8.5: An LSTM with peepholes</p>
    <h3 id="_idParaDest-206" class="heading-3">The code</h3>
    <p class="normal">Note that we’re <a id="_idIndexMarker806"/>using an implementation of the peephole connections that are diagonal. We found that nondiagonal peephole connections (proposed by Gers and Schmidhuber in their paper <em class="italic">Recurrent Nets that Time and Count</em>, <em class="italic">Neural Networks</em>, <em class="italic">2000</em>) hurt performance more than they help, for this language modeling task. Therefore, we’re using a different variation that uses diagonal peephole connections, as used by Sak, Senior, and Beaufays in their paper <em class="italic">Long Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling</em>, <em class="italic">Proceedings of the Annual Conference of the International Speech Communication Association</em>.</p>
    <p class="normal">Fortunately, we have this technique implemented as an <code class="inlineCode">RNNCell</code> object in <code class="inlineCode">tensorflow_addons</code>. Therefore, all we need to do is wrap this <code class="inlineCode">PeepholeLSTMCell</code> object in a <code class="inlineCode">layers.RNN</code> object<a id="_idIndexMarker807"/> to produce the desired layer. The following is the code implementation:</p>
    <pre class="programlisting code"><code class="hljs-code">text_vectorizer = tf.keras.layers.TextVectorization(
    max_tokens=n_vocab, standardize=<span class="hljs-literal">None</span>,
    split=<span class="hljs-literal">None</span>, input_shape=(window_size,)
)
<span class="hljs-comment"># Train the model on existing data</span>
text_vectorizer.adapt(train_ds)
lm_peephole_model = models.Sequential([
    text_vectorizer,
    layers.Embedding(n_vocab+<span class="hljs-number">2</span>, <span class="hljs-number">96</span>),
    layers.RNN(
        tfa.rnn.PeepholeLSTMCell(<span class="hljs-number">512</span>),
        return_sequences=<span class="hljs-literal">True</span>
    ),
    layers.RNN(
        tfa.rnn.PeepholeLSTMCell(<span class="hljs-number">256</span>),
        return_sequences=<span class="hljs-literal">True</span>
    ),
    layers.Dense(<span class="hljs-number">1024</span>, activation=<span class="hljs-string">'relu'</span>),
    layers.Dropout(<span class="hljs-number">0.5</span>),
    layers.Dense(n_vocab, activation=<span class="hljs-string">'softmax'</span>)
])
</code></pre>
    <p class="normal">Now let’s look at the training and validation perplexities of different models and how they change over time.</p>
    <h2 id="_idParaDest-207" class="heading-2">Training and validation perplexities over time</h2>
    <p class="normal">In <em class="italic">Figure 8.6</em>, we have <a id="_idIndexMarker808"/>plotted the behavior of perplexity over time for LSTMs, LSTMs with peepholes, and GRUs. We can see that GRUs are a clear-cut winner in terms of performance. This can be attributed to the innovative simplification of LSTM cells found in GRU cells. But it looks like GRU model does overfit quite heavily. Therefore, it’s important to use techniques such as early stopping to prevent such behavior. We can see that LSTMs with peepholes haven’t given us much advantage in terms of performance. But it is important to keep in mind that we are using a relatively small dataset. </p>
    <p class="normal">For larger, more complex datasets, the performance might vary. We will leave experimenting with GRU cells for the reader and continue with the LSTM model:</p>
    <figure class="mediaobject"><img src="../Images/B14070_08_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.6: Perplexity change for training data over time (LSTMs, LSTM (peephole), and GRUs)</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">The current literature suggests that among LSTMs and GRUs, there is no clear winner and a lot depends on the task (refer to the paper <em class="italic">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</em>, <em class="italic">Chung and others</em>, <em class="italic">NIPS 2014 Workshop on Deep Learning</em>, <em class="italic">December 2014 </em>at<em class="italic"> </em><a href="https://arxiv.org/abs/1412.3555"><span class="url">https://arxiv.org/abs/1412.3555</span></a>).</p>
    </div>
    <p class="normal">In this section, we discussed three different models: standard LSTMs, GRUs, and LSTMs with peepholes. </p>
    <p class="normal">The results clearly indicate that, for this dataset, GRUs outperform other variants. In the next section, we will discuss techniques that can enhance the predictive <a id="_idIndexMarker809"/>power of sequential models.</p>
    <h1 id="_idParaDest-208" class="heading-1">Improving sequential models – beam search</h1>
    <p class="normal">As we saw <a id="_idIndexMarker810"/>earlier, the<a id="_idIndexMarker811"/> generated text can be improved. Now let’s see if beam search, which we discussed in <em class="chapterRef">Chapter 7, Understanding Long Short-Term Memory Networks</em>, might help to improve the performance. The standard way to predict from a language model is by predicting one step at a time and using the prediction from the previous time step as the new input. In beam search, we predict several steps ahead before picking an input. </p>
    <p class="normal">This enables us to pick output sequences that may not look as attractive if taken individually, but are better when considered as a sequence. The way beam search works is by, at a given time, predicting <em class="italic">m</em><sup class="superscript">n</sup> output sequences or beams. <em class="italic">m</em> is known as the beam width and <em class="italic">n</em> is the beam depth. Each output sequence (or a beam) is <em class="italic">n</em> bigrams predicted into the future. We compute the joint probability of each beam by multiplying individual prediction probabilities of the items in that beam. We then pick the beam with the highest joint probability as our output sequence for that given time step. Note that this is a greedy search, meaning that we will calculate the best candidates at each depth of the tree iteratively, as the tree grows. It should be noted that this search will not result in the globally best beam. <em class="italic">Figure 8.7</em> shows an example. We will indicate the best beam candidates (and their probabilities) with bold font and arrows:</p>
    <figure class="mediaobject"><img src="../Images/B14070_08_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 8.7: A beam search illustrating the requirement for updating beam states at each step. Each number underneath the word represents the probability of that word being chosen. For the words not in bold, you can assume the probabilities are negligible</p>
    <p class="normal">We can see that in the first step, the word “<em class="italic">hunting</em>” has the highest probability. However, if we perform a beam search with a beam depth of 3, we get the sequence [<em class="italic">“king”, “was”, “hunting”</em>] with a joint probability of <em class="italic">0.3 * 0.5 * 0.4 = 0.06</em> as the best beam. </p>
    <p class="normal">This is higher than a beam that would start from the word “<em class="italic">hunting</em>” (which has a joint <a id="_idIndexMarker812"/>probability<a id="_idIndexMarker813"/> of <em class="italic">0.5 * 0.1 * 0.3 = 0.015</em>).</p>
    <h2 id="_idParaDest-209" class="heading-2">Implementing beam search</h2>
    <p class="normal">We implement <a id="_idIndexMarker814"/>beam search as a recursive function. But first we will implement a function that performs a single step of our recursive function called <code class="inlineCode">beam_one_step()</code>. This function simply takes a model, an input, and states (from the LSTM) and produces the output and new states.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">beam_one_step</span>(<span class="hljs-params">model, input_, states</span>): 
    <span class="hljs-string">""" Perform the model update and output for one step"""</span>
    out = model.predict([input_, *states])
    output, new_states = out[<span class="hljs-number">0</span>], out[<span class="hljs-number">1</span>:]
    <span class="hljs-keyword">return</span> output, new_states
</code></pre>
    <p class="normal">Next, we write the main recursive function that performs beam search. This function takes the following arguments:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">model</code> – An inference-based language model</li>
      <li class="bulletList"><code class="inlineCode">input_</code> – The initial input</li>
      <li class="bulletList"><code class="inlineCode">states</code> – The initial state vectors</li>
      <li class="bulletList"><code class="inlineCode">beam_depth</code> – The search depth of the beam</li>
      <li class="bulletList"><code class="inlineCode">beam_width</code> – The search width of the beam (i.e. number of candidates considered at a given depth)</li>
    </ul>
    <p class="normal">Let’s now discuss the function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">beam_search</span>(<span class="hljs-params">model, input_, states, beam_depth=</span><span class="hljs-number">5</span><span class="hljs-params">, beam_width=</span><span class="hljs-number">3</span>):
    <span class="hljs-string">""" Defines an outer wrapper for the computational function of </span>
<span class="hljs-string">    beam search """</span>
    vocabulary = 
    infer_model.get_layer(<span class="hljs-string">"text_vectorization"</span>).get_vocabulary()
    index_word = <span class="hljs-built_in">dict</span>(<span class="hljs-built_in">zip</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(vocabulary)), vocabulary))
    <span class="hljs-keyword">def</span> <span class="hljs-title">recursive_fn</span>(<span class="hljs-params">input_, states, sequence, log_prob, i</span>):
        <span class="hljs-string">""" This function performs actual recursive computation of the </span>
<span class="hljs-string">        long string"""</span>
        
        <span class="hljs-keyword">if</span> i == beam_depth:
            <span class="hljs-string">""" Base case: Terminate the beam search """</span>
            results.append((<span class="hljs-built_in">list</span>(sequence), states, np.exp(log_prob)))
            <span class="hljs-keyword">return</span> sequence, log_prob, states
        <span class="hljs-keyword">else</span>:
            <span class="hljs-string">""" Recursive case: Keep computing the output using the </span>
<span class="hljs-string">            previous outputs"""</span>
            output, new_states = beam_one_step(model, input_, states)
            
            <span class="hljs-comment"># Get the top beam_width candidates for the given depth</span>
            top_probs, top_ids = tf.nn.top_k(output, k=beam_width)
            top_probs, top_ids = top_probs.numpy().ravel(), 
            top_ids.numpy().ravel()
            <span class="hljs-comment"># For each candidate compute the next prediction</span>
            <span class="hljs-keyword">for</span> p, wid <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(top_probs, top_ids):
                new_log_prob = log_prob + np.log(p)
                
                <span class="hljs-comment"># we are going to penalize joint probability whenever </span>
<span class="hljs-comment">                # the same symbol is repeating</span>
                <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(sequence)&gt;<span class="hljs-number">0</span> <span class="hljs-keyword">and</span> wid == sequence[-<span class="hljs-number">1</span>]:
                    new_log_prob = new_log_prob + np.log(<span class="hljs-number">1e-1</span>)
                    
                sequence.append(wid)
                _ = recursive_fn(np.array([[index_word[wid]]]), 
                new_states, sequence, new_log_prob, i+<span class="hljs-number">1</span>)
                sequence.pop()
    results = []
    sequence = []
    log_prob = <span class="hljs-number">0.0</span>
    recursive_fn(input_, states, sequence, log_prob, <span class="hljs-number">0</span>)
    results = <span class="hljs-built_in">sorted</span>(results, key=<span class="hljs-keyword">lambda</span> x: x[<span class="hljs-number">2</span>], reverse=<span class="hljs-literal">True</span>)
    <span class="hljs-keyword">return</span> results
</code></pre>
    <p class="normal">The <code class="inlineCode">beam_search()</code> function<a id="_idIndexMarker815"/> in fact defines a nested recursive function (<code class="inlineCode">recursive_fn</code>) that accumulates the outputs as it is called and stores the results in a list called results. The <code class="inlineCode">recursive_fn()</code> does the following. If the function has been called a number of times equal to the <code class="inlineCode">beam_depth</code>, then it returns the current result. If the number of function calls hasn’t reached the predefined depth, for a given depth index, then the <code class="inlineCode">recursive_fn()</code>:</p>
    <ul>
      <li class="bulletList">Computes the new output and states using the <code class="inlineCode">beam_one_step()</code> function</li>
      <li class="bulletList">Gets the IDs and probabilities of the top bigram candidates</li>
      <li class="bulletList">Computes the joint probability of each beam in the log space (in log space we get better numerical stability for smaller probability values)</li>
      <li class="bulletList">Finally, we call the same function with the new inputs, new state, and the next depth index</li>
    </ul>
    <p class="normal">With that <a id="_idIndexMarker816"/>you can simply call the <code class="inlineCode">beam_search() </code>function to get beams of predictions from the inference model. Let’s look at how we can do that next.</p>
    <h2 id="_idParaDest-210" class="heading-2">Generating text with beam search</h2>
    <p class="normal">Here we<a id="_idIndexMarker817"/> will <a id="_idIndexMarker818"/>only show the part where we iteratively call <code class="inlineCode">beam_search()</code> to generate new text. For the full code refer to <code class="inlineCode">ch08_lstms_for_text_generation.ipynb</code>.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-number">50</span>):
    <span class="hljs-built_in">print</span>(<span class="hljs-string">'.'</span>, end=<span class="hljs-string">''</span>)
    <span class="hljs-comment"># Get the results from beam search</span>
    result = beam_search(infer_model, x, states, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>)
    
    <span class="hljs-comment"># Get one of the top 10 results based on their likelihood</span>
    n_probs = np.array([p <span class="hljs-keyword">for</span> _,_,p <span class="hljs-keyword">in</span> result[:<span class="hljs-number">10</span>]])
    p_j = np.random.choice(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(n_probs.size)), 
    p=n_probs/n_probs.<span class="hljs-built_in">sum</span>())                    
    best_beam_ids, states, _ = result[p_j]
    x = np.array([[index_word[best_beam_ids[-<span class="hljs-number">1</span>]]]])
    text.extend([index_word[w] <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> best_beam_ids])
</code></pre>
    <p class="normal">We simply call the function <code class="inlineCode">beam_search()</code> with <code class="inlineCode">infer_model</code>, current input <code class="inlineCode">x</code>, current states <code class="inlineCode">states</code>, <code class="inlineCode">beam depth</code>, and <code class="inlineCode">beam width</code>, and update <code class="inlineCode">x</code> and <code class="inlineCode">states</code> to reflect the winning beam. Then the model will iteratively use the winning beam to produce the next beam.</p>
    <p class="normal">Let’s see how our LSTM performs with beam search:</p>
    <pre class="programlisting con"><code class="hljs-con">When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren groundr, said the king's daughter went out of the king's son to the king's daughter, and then the king's daughter went into the world, and asked the hedgehog's daughter that the king was about to the forest, and there was on the window, and said, "if you will give her that you have been and said, i will give him the king's daughter, but when she went to the king's sister, and when she was still before the window, and said to himself, and when he said to her father, and that he had nothing and said to hi
</code></pre>
    <p class="normal">Here’s what the standard LSTM with greedy sampling (i.e. predicting one word at a time) outputs:</p>
    <pre class="programlisting con"><code class="hljs-con">When adam and eve were driven out of paradise, they were compelled to build a house for themselves on barren groundr, and then this they were all the third began to be able to the forests, and they were.  the king's daughter was no one was about to the king's daughter to the forest of them to the stone.  then the king's daughter was, and then the king's daughter was nothing-eyes, and the king's daughter was still, and then that had there was about through the third, and the king's daughters was seems to the king's daughter to the forest of them to the stone for them to the forests, and that it was not been to be ables, and the king's daughter wanted to be and said, ...
</code></pre>
    <p class="normal">Compared to the text produced by the LSTM, this text seems to have more variation in it while keeping the text grammatically consistent as well. So, in fact, beam search helps to produce <a id="_idIndexMarker819"/>quality<a id="_idIndexMarker820"/> predictions compared to predicting one word at a time. But still, there are instances where words together don’t make much sense. Let’s see how we can improve our LSTM further.</p>
    <h1 id="_idParaDest-211" class="heading-1">Improving LSTMs – generating text with words instead of n-grams</h1>
    <p class="normal">Here we will <a id="_idIndexMarker821"/>discuss ways to<a id="_idIndexMarker822"/> improve LSTMs. We have so far used bigrams as our basic unit of text. But you would get better results by incorporating words, as opposed to bigrams. This is because using words reduces the overhead of the model by alleviating the need to learn to form words from bigrams. We will discuss how we can employ word vectors in the code to generate better-quality text compared to using bigrams. </p>
    <h2 id="_idParaDest-212" class="heading-2">The curse of dimensionality</h2>
    <p class="normal">One major <a id="_idIndexMarker823"/>limitation stopping us from using words instead of n-grams as the input to our LSTM is that this will drastically increase the number of parameters in our model. Let’s understand this through an example. Consider that we have an input of size <em class="italic">500</em> and a cell state of size <em class="italic">100</em>. This would result in a total of approximately <em class="italic">240K</em> parameters (excluding the softmax layer), as shown here: </p>
    <p class="center"><img src="../Images/B14070_08_004.png" alt="" style="height: 1.15em !important;"/></p>
    <p class="normal">Let’s now increase the size of the input to <em class="italic">1000</em>. Now the total number of parameters would be approximately <em class="italic">440K</em>, as shown here: </p>
    <p class="center"><img src="../Images/B14070_08_005.png" alt="" style="height: 1.15em !important;"/></p>
    <p class="normal">As you can see, for an increase of 500 units of the input dimensionality, the number of parameters has grown by 200,000. This not only increases the computational complexity but also increases the risk of overfitting due to the large number of parameters. So, we need ways of restricting the dimensionality of the input.</p>
    <h2 id="_idParaDest-213" class="heading-2">Word2vec to the rescue</h2>
    <p class="normal">As you will<a id="_idIndexMarker824"/> remember, not only can Word2vec give a lower-dimensional feature representation of words compared to one-hot encoding, but it also gives semantically sound features. To understand this, let’s consider three words: <em class="italic">cat</em>, <em class="italic">dog</em>, and <em class="italic">volcano</em>. If we one-hot encode just these words and calculate the Euclidean distance between them, it would be the following:</p>
    <p class="normal"><em class="italic">distance(cat,volcano) = distance(cat,dog)</em></p>
    <p class="normal">However, if we learn word embeddings, it would be the following:</p>
    <p class="normal"><em class="italic">distance(cat,volcano) &gt; distance(cat,dog)</em></p>
    <p class="normal">We would like our features to represent the latter, where similar things have a lower distance than dissimilar things. Consequently, the model will be able to generate better-quality text.</p>
    <h2 id="_idParaDest-214" class="heading-2">Generating text with Word2vec</h2>
    <p class="normal">The structure <a id="_idIndexMarker825"/>of the model remains<a id="_idIndexMarker826"/> more<a id="_idIndexMarker827"/> or less the same as what we have discussed. It is only the units of text we would consider that changes. </p>
    <p class="normal"><em class="italic">Figure 8.8 </em>depicts the overall architecture of LSTM-Word2vec: </p>
    <figure class="mediaobject"><img src="../Images/B14070_08_08.png" alt="Generating text with Word2vec"/></figure>
    <p class="packt_figref">Figure 8.8: The structure of a language modeling LSTM using word vectors</p>
    <p class="normal">You have a few options when it comes to using word vectors. You can either:</p>
    <ul>
      <li class="bulletList">Randomly initialize the vectors and jointly learn them during the task</li>
      <li class="bulletList">Train the embeddings using a word vector algorithm (e.g. Word2vec, GloVe, etc.) beforehand</li>
      <li class="bulletList">Use pretrained word vectors freely available to download, to initialize the embedding layer</li>
    </ul>
    <div class="note">
      <p class="normal"><strong class="keyWord">Note</strong></p>
      <p class="normal">Below we list a few freely available pretrained word vectors. Word vectors found<a id="_idIndexMarker828"/> by <a id="_idIndexMarker829"/>learning<a id="_idIndexMarker830"/> from a text corpus with billions of words are freely available to be downloaded and used:</p>
      <ul>
        <li class="bulletList"><strong class="keyWord">Word2vec</strong>: <a href="https://code.google.com/archive/p/word2vec/"><span class="url">https://code.google.com/archive/p/word2vec/</span></a></li>
        <li class="bulletList"><strong class="keyWord">Pretrained GloVe word vectors</strong>: <a href="https://nlp.stanford.edu/projects/glove/"><span class="url">https://nlp.stanford.edu/projects/glove/</span></a></li>
        <li class="bulletList"><strong class="keyWord">fastText word vectors</strong>: <a href="https://github.com/facebookresearch/fastText"><span class="url">https://github.com/facebookresearch/fastText</span></a></li>
      </ul>
    </div>
    <p class="normal">We <a id="_idIndexMarker831"/>end <a id="_idIndexMarker832"/>our <a id="_idIndexMarker833"/>discussion on language modeling here.</p>
    <h1 id="_idParaDest-215" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we looked at the implementations of the LSTM algorithm and other various important aspects to improve LSTMs beyond standard performance. As an exercise, we trained our LSTM on the text of stories by the Grimm brothers and asked the LSTM to output a fresh new story. We discussed how to implement an LSTM model with code examples extracted from exercises.</p>
    <p class="normal">Next, we had a technical discussion about how to implement LSTMs with peepholes and GRUs. Then we did a performance comparison between a standard LSTM and its variants. We saw that the GRUs performed the best compared to LSTMs with peepholes and LSTMs. </p>
    <p class="normal">Then we discussed some of the various improvements possible for enhancing the quality of outputs generated by an LSTM. The first improvement was beam search. We looked at an implementation of beam search and covered how to implement it step by step. Then we looked at how we can use word embeddings to teach our LSTM to output better text.</p>
    <p class="normal">In conclusion, LSTMs are very powerful machine learning models that can capture both long-term and short-term dependencies. </p>
    <p class="normal">Moreover, beam search in fact helps to produce more realistic-looking textual phrases compared to predicting one at a time. </p>
    <p class="normal">In the next chapter, we will look at how sequential models can be used to solve a more complex type of problem known as sequence-to-sequence problems. Specifically, we will look at how we can perform machine translation by formulating it as a sequence-to-sequence problem.</p>
    <p class="center">To access the code files for this book, visit our GitHub page at: <a href="https://packt.link/nlpgithub"><span class="url">https://packt.link/nlpgithub</span></a></p>
    <p class="center">Join our Discord community to meet like-minded people and learn alongside more than 1000 members at: <a href="https://packt.link/nlp"><span class="url">https://packt.link/nlp</span></a></p>
    <figure class="mediaobject"> <img src="../Images/QR_Code5143653472357468031.png" alt=""/></figure>
  </div>
</body></html>