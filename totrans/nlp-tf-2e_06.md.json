["```\n-DOCSTART- -X- -X- O\nEU NNP B-NP B-ORG\nrejects VBZ B-VP O\nGerman JJ B-NP B-MISC\ncall NN I-NP O\nto TO B-VP O\nboycott VB I-VP O\nBritish JJ B-NP B-MISC\nlamb NN I-NP O\n. . O O\nThe DT B-NP O\nEuropean NNP I-NP B-ORG\nCommission NNP I-NP I-ORG\nsaid VBD B-VP O\n...\nto TO B-PP O\nsheep NN B-NP O\n. . O O \n```", "```\ndef read_data(filename):\n    '''\n    Read data from a file with given filename\n    Returns a list of sentences (each sentence a string), \n    and list of ner labels for each string\n    '''\n    print(\"Reading data ...\")\n    # master lists - Holds sentences (list of tokens),\n    # ner_labels (for each token an NER label)\n    sentences, ner_labels = [], [] \n\n    # Open the file\n    with open(filename,'r',encoding='latin-1') as f:        \n        # Read each line\n        is_sos = True \n        # We record at each line if we are seeing the beginning of a \n        # sentence\n\n        # Tokens and labels of a single sentence, flushed when encountered\n        # a new one\n        sentence_tokens = []\n        sentence_labels = []\n        i = 0\n        for row in f:\n        # If we are seeing an empty line or -DOCSTART- that's a new line\n            if len(row.strip()) == 0 or row.split(' ')[0] == '-\n            DOCSTART-':\n                is_sos = False\n            # Otherwise keep capturing tokens and labels\n            else:\n                is_sos = True\n                token, _, _, ner_label = row.split(' ')\n                sentence_tokens.append(token)\n                sentence_labels.append(ner_label.strip())\n\n            # When we reach the end / or reach the beginning of next\n            # add the data to the master lists, flush the temporary one\n            if not is_sos and len(sentence_tokens)>0:\n                sentences.append(' '.join(sentence_tokens))\n                ner_labels.append(sentence_labels)\n                sentence_tokens, sentence_labels = [], []\n\n    print('\\tDone')\n    return sentences, ner_labels \n```", "```\n# Train data\ntrain_sentences, train_labels = read_data(train_filepath) \n# Validation data\nvalid_sentences, valid_labels = read_data(dev_filepath) \n# Test data\ntest_sentences, test_labels = read_data(test_filepath) \n```", "```\n# Print some data\nprint('\\nSample data\\n')\nfor v_sent, v_labels in zip(valid_sentences[:5], valid_labels[:5]):\n    print(\"Sentence: {}\".format(v_sent))\n    print(\"Labels: {}\".format(v_labels))\n    print('\\n') \n```", "```\nSentence: West Indian all-rounder Phil Simmons took four for 38 on Friday as Leicestershire beat Somerset by an innings and 39 runs in two days to take over at the head of the county championship .\nLabels: ['B-MISC', 'I-MISC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\nSentence: Their stay on top , though , may be short-lived as title rivals Essex , Derbyshire and Surrey all closed in on victory while Kent made up for lost time in their rain-affected match against Nottinghamshire .\nLabels: ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-ORG', 'O']\nSentence: After bowling Somerset out for 83 on the opening morning at Grace Road , Leicestershire extended their first innings by 94 runs before being bowled out for 296 with England discard Andy Caddick taking three for 83 .\nLabels: ['O', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'O', 'B-ORG', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'O', 'B-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O'] \n```", "```\nfrom itertools import chain\n# Print the value count for each label\nprint(\"Training data label counts\")\nprint(pd.Series(chain(*train_labels)).value_counts()) \n```", "```\nTraining data label counts\nO         169578\nB-LOC       7140\nB-PER       6600\nB-ORG       6321\nI-PER       4528\nI-ORG       3704\nB-MISC      3438\nI-LOC       1157\nI-MISC      1155\ndtype: int64 \n```", "```\npd.Series(train_sentences).str.split().str.len().describe(percentiles=[0.05, 0.95]) \n```", "```\ncount    14041.000000\nmean        14.501887\nstd         11.602756\nmin          1.000000\n5%           2.000000\n50%         10.000000\n95%         37.000000\nmax        113.000000\ndtype: float64 \n```", "```\ndef get_label_id_map(train_labels):\n    # Get the unique list of labels\n    unique_train_labels = pd.Series(chain(*train_labels)).unique()\n    # Create a class label -> class ID mapping\n    labels_map = dict(\n        zip(unique_train_labels, \n    np.arange(unique_train_labels.shape[0])))\n    print(\"labels_map: {}\".format(labels_map))\n    return labels_map \n```", "```\nlabels_map = get_label_id_map(train_labels) \n```", "```\nlabels_map: {'B-ORG': 0, 'O': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8} \n```", "```\ndef get_padded_int_labels(labels, labels_map, max_seq_length,\nreturn_mask=True):\n    # Convert string labels to integers \n    int_labels = [[labels_map[x] for x in one_seq] for one_seq in \n    labels]\n\n    # Pad sequences\n    if return_mask:\n        # If we return mask, we first pad with a special value (-1) and \n        # use that to create the mask and later replace -1 with 'O'\n        padded_labels = np.array(\n            tf.keras.preprocessing.sequence.pad_sequences(\n                int_labels, maxlen=max_seq_length, padding='post', \n                truncating='post', value=-1\n            )\n        )\n\n        # mask filter\n        mask_filter = (padded_labels != -1)\n        # replace -1 with 'O' s ID\n        padded_labels[~mask_filter] = labels_map['O']        \n        return padded_labels, mask_filter.astype('int')\n\n    else:\n        padded_labels = np.array(ner_pad_sequence_func(int_labels, \n        value=labels_map['O']))\n        return padded_labels \n```", "```\nmax_seq_length = 40 \n```", "```\n# Convert string labels to integers for all train/validation/test data\n# Pad train/validation/test data\npadded_train_labels, train_mask = get_padded_int_labels(\n    train_labels, labels_map, max_seq_length, return_mask=True\n)\npadded_valid_labels, valid_mask = get_padded_int_labels(\n    valid_labels, labels_map, max_seq_length, return_mask=True\n)\npadded_test_labels, test_mask  = get_padded_int_labels(\n    test_labels, labels_map, max_seq_length, return_mask=True\n) \n```", "```\n# Print some labels IDs\nprint(padded_train_labels[:2])\nprint(train_mask[:2]) \n```", "```\n[[0 1 2 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1]\n [3 4 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n  1 1 1 1]]\n[[1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0]\n [1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n  0 0 0 0]] \n```", "```\n# The maximum length of sequences\nmax_seq_length = 40\n# Size of token embeddings\nembedding_size = 64\n# Number of hidden units in the RNN layer\nrnn_hidden_size = 64\n# Number of output nodes in the last layer\nn_classes = 9\n# Number of samples in a batch\nbatch_size = 64\n# Number of epochs to train\nepochs = 3 \n```", "```\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n```", "```\ntoy_corpus = [\"I went to the market on Sunday\", \"The Market was empty.\"] \n```", "```\ntoy_vectorization_layer = TextVectorization() \n```", "```\n# Fit it on a corpus of data\ntoy_vectorization_layer.adapt(toy_corpus) \n```", "```\ntoy_vectorized_output = toy_vectorization_layer(toy_corpus) \n```", "```\n[[ 9  4  6  2  3  8  7]\n [ 2  3  5 10  0  0  0]] \n```", "```\nVocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty'] \n```", "```\ntoy_vectorization_layer = TextVectorization(max_tokens=5)\ntoy_vectorization_layer.adapt(toy_corpus)\ntoy_vectorized_output = toy_vectorization_layer(toy_corpus) \n```", "```\n[[1 4 1 2 3 1 1]\n [2 3 1 1 0 0 0]] \n```", "```\nVocabulary: ['', '[UNK]', 'the', 'market', 'went'] \n```", "```\ntoy_vectorization_layer = TextVectorization(standardize=None)\ntoy_vectorization_layer.adapt(toy_corpus)\ntoy_vectorized_output = toy_vectorization_layer(toy_corpus) \n```", "```\n[[12  2  4  5  7  6 10]\n [ 9 11  3  8  0  0  0]] \n```", "```\nVocabulary: ['', '[UNK]', 'went', 'was', 'to', 'the', 'on', 'market', 'empty.', 'The', 'Sunday', 'Market', 'I'] \n```", "```\ntoy_vectorization_layer = TextVectorization(output_sequence_length=4)\ntoy_vectorization_layer.adapt(toy_corpus)\ntoy_vectorized_output = toy_vectorization_layer(toy_corpus) \n```", "```\n[[ 9  4  6  2]\n [ 2  3  5 10]] \n```", "```\nVocabulary: ['', '[UNK]', 'the', 'market', 'went', 'was', 'to', 'sunday', 'on', 'i', 'empty'] \n```", "```\nimport tensorflow.keras.layers as layers\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization \n```", "```\n# Input layer\nword_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string) \n```", "```\ndef get_fitted_token_vectorization_layer(corpus, max_seq_length, vocabulary_size=None):\n    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n\n    # Define a text vectorization layer\n    vectorization_layer = TextVectorization(\n        max_tokens=vocabulary_size, standardize=None,        \n        output_sequence_length=max_seq_length, \n    )\n    # Fit it on a corpus of data\n    vectorization_layer.adapt(corpus)\n\n    # Get the vocabulary size\n    n_vocab = len(vectorization_layer.get_vocabulary())\n    return vectorization_layer, n_vocab \n```", "```\n# Text vectorization layer\nvectorize_layer, n_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length) \n```", "```\n# Vectorized output (each word mapped to an int ID)\nvectorized_out = vectorize_layer(word_input) \n```", "```\n# Look up embeddings for the returned IDs\nembedding_layer = layers.Embedding(\n    input_dim=n_vocab,\noutput_dim=embedding_size,\nmask_zero=True\n)(vectorized_out) \n```", "```\n# Define a simple RNN layer, it returns an output at each position\nrnn_layer = layers.SimpleRNN(\n    units=rnn_hidden_size, return_sequences=True\n)\nrnn_out = rnn_layer(embedding_layer) \n```", "```\ndense_layer = layers.Dense(n_classes, activation='softmax')\ndense_out = dense_layer(rnn_out) \n```", "```\nmodel = tf.keras.Model(inputs=word_input, outputs=dense_out) \n```", "```\ndef macro_accuracy(y_true, y_pred):\n\n    # [batch size, time] => [batch size * time]\n    y_true = tf.cast(tf.reshape(y_true, [-1]), 'int32')\n    # [batch size, sequence length, n_classes] => [batch size * time]\n    y_pred = tf.cast(tf.reshape(tf.argmax(y_pred, axis=-1), [-1]), \n    'int32')\n\n    sorted_y_true = tf.sort(y_true)\n    sorted_inds = tf.argsort(y_true)\n\n    sorted_y_pred = tf.gather(y_pred, sorted_inds)\n\n    sorted_correct = tf.cast(tf.math.equal(sorted_y_true, \n    sorted_y_pred), 'int32')\n\n    # We are adding one to make sure there are no division by zero\n    correct_for_each_label = \n    tf.cast(tf.math.segment_sum(sorted_correct, sorted_y_true), \n    'float32') + 1\n    all_for_each_label = \n    tf.cast(tf.math.segment_sum(tf.ones_like(sorted_y_true), \n    sorted_y_true), 'float32') + 1\n\n    mean_accuracy = \n    tf.reduce_mean(correct_for_each_label/all_for_each_label)\n\n    return mean_accuracy \n```", "```\nmean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name='macro_accuracy') \n```", "```\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[mean_accuracy_metric]) \n```", "```\ndef get_class_weights(train_labels):\n\n    label_count_ser = pd.Series(chain(*train_labels)).value_counts()\n    label_count_ser = label_count_ser.min()/label_count_ser\n\n    label_id_map = get_label_id_map(train_labels)\n    label_count_ser.index = label_count_ser.index.map(label_id_map)\n    return label_count_ser.to_dict() \n```", "```\ndef get_sample_weights_from_class_weights(labels, class_weights):\n    \"\"\" From the class weights generate sample weights \"\"\"\n    return np.vectorize(class_weights.get)(labels) \n```", "```\ntrain_class_weights = get_class_weights(train_labels)\nprint(\"Class weights: {}\".format(train_class_weights))\n# Get sample weights (we cannot use class_weight with TextVectorization\n# layer)\ntrain_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights) \n```", "```\nlabels_map: {\n    'B-ORG': 0, \n    'O': 1, \n    'B-MISC': 2, \n    'B-PER': 3, \n    'I-PER': 4, \n    'B-LOC': 5, \n    'I-ORG': 6, \n    'I-MISC': 7, \n    'I-LOC': 8\n}\nClass weights: {\n    1: 0.006811025015037328, \n    5: 0.16176470588235295, \n    3: 0.17500000000000002, \n    0: 0.18272425249169436, \n    4: 0.25507950530035334, \n    6: 0.31182505399568033, \n    2: 0.33595113438045376, \n    8: 0.9982713915298186, \n    7: 1.0\n} \n```", "```\n# Make train_sequences an array\ntrain_sentences = np.array(train_sentences)\n# Training the model\nmodel.fit(\n        train_sentences, padded_train_labels, \n        sample_weight=train_sample_weights,\n        batch_size=batch_size,\n        epochs=epochs, \n        validation_data=(np.array(valid_sentences), \n        padded_valid_labels)\n) \n```", "```\nmodel.evaluate(np.array(test_sentences), padded_test_labels) \n```", "```\nn_samples = 5\nvisual_test_sentences = test_sentences[:n_samples]\nvisual_test_labels = padded_test_labels[:n_samples] \n```", "```\nvisual_test_predictions = model.predict(np.array(visual_test_sentences))\nvisual_test_pred_labels = np.argmax(visual_test_predictions, axis=-1) \n```", "```\nrev_labels_map = dict(zip(labels_map.values(), labels_map.keys())) \n```", "```\nfor i, (sentence, sent_labels, sent_preds) in enumerate(zip(visual_test_sentences, visual_test_labels, visual_test_pred_labels)):    \n    n_tokens = len(sentence.split())\n    print(\"Sample:\\t\",\"\\t\".join(sentence.split()))\n    print(\"True:\\t\",\"\\t\".join([rev_labels_map[i] for i in \n    sent_labels[:n_tokens]]))\n    print(\"Pred:\\t\",\"\\t\".join([rev_labels_map[i] for i in \n    sent_preds[:n_tokens]]))\n    print(\"\\n\") \n```", "```\nSample:     SOCCER    -    JAPAN    GET    LUCKY    WIN    ,    CHINA    IN    SURPRISE    DEFEAT    .\nTrue:         O    O    B-LOC    O    O    O    O    B-LOC    O    O    O    O\nPred:         O    O    B-MISC    O    O    O    O    B-PER    O    B-LOC    O    O\nSample:     Nadim    Ladki\nTrue:         B-PER    I-PER\nPred:         B-LOC    O\nSample:     AL-AIN    ,    United    Arab    Emirates    1996-12-06\nTrue:         B-LOC    O    B-LOC    I-LOC    I-LOC    O\nPred:         B-LOC    O    B-LOC    I-LOC    I-LOC    I-ORG\nSample:     Japan    began    the    defence    of    their    Asian    Cup    title    with    a    lucky    2-1    win    against    Syria    in    a    Group    C    championship    match    on    Friday    .\nTrue:         B-LOC    O    O    O    O    O    B-MISC    I-MISC    O    O    O    O    O    O    O    B-LOC    O    O    O    O    O    O    O    O    O\nPred:         B-LOC    I-LOC    O    O    O    O    B-MISC    I-MISC    I-MISC    O    O    O    O    O    O    B-LOC    O    O    O    O    O    O    O    O    O \n```", "```\nvocab_ser = pd.Series(\n    pd.Series(train_sentences).str.split().explode().unique()\n)\nvocab_ser.str.len().describe(percentiles=[0.05, 0.95]) \n```", "```\ncount    23623.000000\nmean         6.832705\nstd          2.749288\nmin          1.000000\n5%           3.000000\n50%          7.000000\n95%         12.000000\nmax         61.000000\ndtype: float64 \n```", "```\ndef prepare_corpus_for_char_embeddings(tokenized_sentences, max_seq_length):\n    \"\"\" Pads each sequence to a maximum length \"\"\"\n    proc_sentences = []\n    for tokens in tokenized_sentences:\n        if len(tokens) >= max_seq_length:\n            proc_sentences.append([[t] for t in \n            tokens[:max_seq_length]])\n        else:\n            proc_sentences.append([[t] for t in \n            tokens+['']*(max_seq_length-len(tokens))])\n\n    return proc_sentences \n```", "```\n# Define sample data\ndata = ['aaaa bb c', 'd eee']\n# Pad sequences\ntokenized_sentences = prepare_corpus_for_char_embeddings([d.split() for d in data], 3) \n```", "```\nPadded sequence: [[['aaaa'], ['bb'], ['c']], [['d'], ['eee'], ['']]] \n```", "```\ndef get_fitted_char_vectorization_layer(corpus, max_seq_length, max_token_length, vocabulary_size=None):\n    \"\"\" Fit a TextVectorization layer on given data \"\"\"\n    def _split_char(token):\n        return tf.strings.bytes_split(token)\n    # Define a text vectorization layer\n    vectorization_layer = TextVectorization(\n        standardize=None,      \n        split=_split_char,\n        output_sequence_length=max_token_length, \n    )\n    tokenized_sentences = [sent.split() for sent in corpus]\n    padded_tokenized_sentences = \n    prepare_corpus_for_char_embeddings(tokenized_sentences, \n    max_seq_length)\n\n    # Fit it on a corpus of data\n    vectorization_layer.adapt(padded_tokenized_sentences)\n\n    # Get the vocabulary size\n    n_vocab = len(vectorization_layer.get_vocabulary())\n    return vectorization_layer, n_vocab \n```", "```\nmax_seq_length = 40\nmax_token_length = 12 \n```", "```\n# Input layer (tokens)\nword_input = tf.keras.layers.Input(shape=(1,), dtype=tf.string) \n```", "```\n# Text vectorize layer (token)\ntoken_vectorize_layer, n_token_vocab = get_fitted_token_vectorization_layer(train_sentences, max_seq_length)\n# Vectorized output (each word mapped to an int ID)\ntoken_vectorized_out = token_vectorize_layer(word_input) \n```", "```\n# Text vectorize layer (char)\nchar_vectorize_layer, n_char_vocab = get_fitted_char_vectorization_layer(train_sentences, max_seq_length, max_token_length) \n```", "```\ntokenized_word_input = layers.Lambda(\n    lambda x: tf.strings.split(x).to_tensor(default_value='', \n    shape=[None, max_seq_length, 1])\n)(word_input)\nchar_vectorized_out = char_vectorize_layer(tokenized_word_input) \n```", "```\n# Produces a [batch size, seq length, token_length, emb size]\nchar_embedding_layer = layers.Embedding(input_dim=n_char_vocab, output_dim=32, mask_zero=True)(char_vectorized_out) \n```", "```\n# A 1D convolutional layer that will generate token embeddings by shifting # a convolutional kernel over the sequence of chars in each token (padded)\nchar_token_output = layers.Conv1D(filters=1, kernel_size=5, strides=1, padding='same', activation='relu')(char_embedding_layer) \n```", "```\n# There is an additional dimension of size 1 (out channel dimension) that\n# we need to remove\nchar_token_output = layers.Lambda(lambda x: x[:, :, :, 0])(char_token_output) \n```", "```\n# Concatenate the token and char embeddings\nconcat_embedding_out = layers.Concatenate()([token_embedding_out, char_token_output]) \n```", "```\n# Define a simple bidirectional RNN layer, it returns an output at each\n# position\nrnn_layer_1 = layers.SimpleRNN(\n    units=64, activation='tanh', use_bias=True, return_sequences=True\n)\nrnn_out_1 = rnn_layer_1(concat_embedding_out) \n```", "```\n# Defines the final prediction layer\ndense_layer = layers.Dense(n_classes, activation='softmax')\ndense_out = dense_layer(rnn_out_1) \n```", "```\n# Defines the model\nchar_token_embedding_rnn = tf.keras.Model(inputs=word_input, outputs=dense_out)\n\n# Define a macro accuracy measure\nmean_accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=macro_accuracy, name='macro_accuracy')\n# Compile the model with a loss optimizer and metrics\nchar_token_embedding_rnn.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=[mean_accuracy_metric]) \n```", "```\n# Make train_sequences an array\ntrain_sentences = np.array(train_sentences)\n# Get sample weights (we cannot use class_weight with TextVectorization\n# layer)\ntrain_sample_weights = get_sample_weights_from_class_weights(padded_train_labels, train_class_weights)\n# Training the model\nchar_token_embedding_rnn.fit(\n    train_sentences, padded_train_labels,\n    sample_weight=train_sample_weights,\n    batch_size=64,\n    epochs=3, \n    validation_data=(np.array(valid_sentences), padded_valid_labels)\n) \n```", "```\n    rnn_layer_1 = layers.SimpleRNN(\n        units=64, activation='tanh', use_bias=True, return_sequences=True\n    )\n    rnn_out_1 = rnn_layer_1(concat_embedding_out)\n    rnn_layer_2 = layers.SimpleRNN(\n        units=32, activation='tanh', use_bias=True, return_sequences=True\n    )\n    rnn_out_1 = rnn_layer_1(rnn_out_1) \n    ```", "```\n    rnn_layer_1 = layers.Bidreictional(layers.SimpleRNN(\n        units=64, activation='tanh', use_bias=True, return_sequences=True\n    )) \n    ```"]