- en: '*Chapter 2*: Performing Image Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer vision is a vast field that takes inspiration from many places. Of
    course, this means that its applications are wide and varied. However, the biggest
    breakthroughs over the past decade, especially in the context of deep learning
    applied to visual tasks, have occurred in a particular domain known as **image
    classification**.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, image classification consists of the process of discerning
    what's in an image based on its visual content. Is there a dog or a cat in this
    image? What number is in this picture? Is the person in this photo smiling or
    not?
  prefs: []
  type: TYPE_NORMAL
- en: Because image classification is such an important and pervasive task in deep
    learning applied to computer vision, the recipes in this chapter will focus on
    the ins and outs of classifying images using TensorFlow 2.x.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a binary classifier to detect smiles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a multi-class classifier to play Rock Paper Scissors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a multi-label classifier to label watches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing ResNet from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying images with a pre-trained network using the Keras API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying images with a pre-trained network using TensorFlow Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using data augmentation to improve performance with the Keras API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using data augmentation to improve performance with the tf.data and tf.image
    APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Besides a working installation of TensorFlow 2.x, it''s highly recommended
    to have access to a GPU, given that some of the recipes are very resource-intensive,
    making the use of a CPU an inviable option. In each recipe, you''ll find the steps
    and dependencies needed to complete it in the *Getting ready* section. Finally,
    the code shown in this chapter is available in full here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3bOjqnU](https://bit.ly/3bOjqnU)'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a binary classifier to detect smiles
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In its most basic form, image classification consists of discerning between
    two classes, or signaling the presence or absence of some trait. In this recipe,
    we'll implement a binary classifier that tells us whether a person in a photo
    is smiling.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You''ll need to install `Pillow`, which is very easy with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use the `SMILEs` dataset, located here: [https://github.com/hromi/SMILEsmileD](https://github.com/hromi/SMILEsmileD).
    Clone or download a zipped version of the repository to a location of your preference.
    In this recipe, we assume the data is inside the `~/.keras/datasets` directory,
    under the name `SMILEsmileD-master`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Positive (left) and negative (right) examples](img/B14768_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Positive (left) and negative (right) examples
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to train a smile classifier from scratch on the `SMILEs`
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to load the images and labels from a list of file paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we are loading the images in grayscale, and we're encoding the labels
    by checking whether the word *positive* is in the file path of the image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function to build the neural network. This model''s structure is based
    on **LeNet** (you can find a link to LeNet''s paper in the *See also* section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Because this is a binary classification problem, a single Sigmoid-activated
    neuron is enough in the output layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the image paths into a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the `load_images_and_labels()` function defined previously to load the
    dataset into memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the images and compute the number of positive, negative, and total
    examples in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create train, test, and validation subsets of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the model and compile it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model. Because the dataset is unbalanced, we are assigning weights
    to each class proportional to the number of positive and negative images in the
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After 20 epochs, the network should get around 90% accuracy on the test set.
    In the following section, we'll explain the previous steps.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We just trained a network to determine whether a person is smiling or not in
    a picture. Our first big task was to take the images in the dataset and load them
    into a format suitable for our neural network. Specifically, the `load_image_and_labels()`
    function is in charge of loading an image in grayscale, resizing it to 32x32x1,
    and then converting it into a `numpy` array. To extract the label, we looked at
    the containing folder of each image: if it contained the word positive, we encoded
    the label as 1; otherwise, we encoded it as 0 (a trick we used here was casting
    a Boolean as a float, like this: `float(label)`).'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we built the neural network, which is inspired by the LeNet architecture.
    The biggest takeaway here is that because this is a binary classification problem,
    we can use a single Sigmoid-activated neuron to discern between the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: We then took 20% of the images to comprise our test set, and from the remaining
    80% we took an additional 20% to create our validation set. With these three subsets
    in place, we proceeded to train the network over 20 epochs, using `binary_crossentropy`
    as our loss function and `rmsprop` as the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: To account for the imbalance in the dataset (out of the 13,165 images, only
    3,690 contain smiling people, while the remaining 9,475 do not), we passed a `class_weight`
    dictionary where we assigned a weight conversely proportional to the number of
    instances of each class in the dataset, effectively forcing the model to pay more
    attention to the 1.0 class, which corresponds to *smile*.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we achieved around 90.5% accuracy on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information on the `SMILEs` dataset, you can visit the official GitHub
    repository here: [https://github.com/hromi/SMILEsmileD](https://github.com/hromi/SMILEsmileD).
    You can read the LeNet paper here (it''s pretty long, though): [http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a multi-class classifier to play rock paper scissors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More often than not, we are interested in categorizing an image into more than
    two classes. As we'll see in this recipe, implementing a neural network to differentiate
    between many categories is fairly straightforward, and what better way to demonstrate
    this than by training a model that can play the widely known Rock Paper Scissors
    game?
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready? Let's dive in!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll use the `Rock-Paper-Scissors Images` dataset, which is hosted on Kaggle
    at the following location: [https://www.kaggle.com/drgfreeman/rockpaperscissors](https://www.kaggle.com/drgfreeman/rockpaperscissors).
    To download it, you''ll need a Kaggle account, so sign in or sign up accordingly.
    Then, unzip the dataset in a location of your preference. In this recipe, we assume
    the unzipped folder is inside the `~/.keras/datasets` directory, under the name
    `rockpaperscissors`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sample images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Example images of rock (left), paper (center), and scissors
    (right)](img/B14768_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Example images of rock (left), paper (center), and scissors (right)
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin implementing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps explain how to train a multi-class **Convolutional Neural
    Network** (**CNN**) to distinguish between the three classes of the Rock Paper
    Scissors game:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a list with the three classes, and also an alias to `tf.data.experimental.AUTOTUNE`,
    which we''ll use later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The values in `CLASSES` match the names of the directories that contain the
    images for each class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function to load an image and its label, given its file path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we are one-hot encoding by comparing the name of the folder that
    contains the image (extracted from `image_path`) with the `CLASSES` list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define a function to build the network architecture. In this case, it''s a
    very simple and shallow one, which is enough for the problem we are solving:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to, given a path to a dataset, return a `tf.data.Dataset`
    instance of images and labels, in batches and optionally shuffled:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the image paths into a list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create train, test, and validation subsets of image paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the training, test, and validation datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate and compile the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model for `250` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After 250 epochs, our network achieves around 93.5% accuracy on the test set.
    Let's understand what we just did.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We started by defining the `CLASSES` list, which allowed us to quickly one-hot
    encode the labels of each image, based on the name of the directory where they
    were contained, as we observed in the body of the `load_image_and_label()` function.
    In this same function, we read an image from disk, decoded it from its JPEG format,
    converted it to grayscale (color information is not necessary in this problem),
    and then resized it to more manageable dimensions of 32x32x1.
  prefs: []
  type: TYPE_NORMAL
- en: '`build_network()` creates a very simple and shallow CNN, comprising a single
    convolutional layer, activated with `ReLU()`, followed by an output, a fully connected
    layer of three neurons, corresponding to the number of categories in the dataset.
    Because this is a multi-class classification task, we use `Softmax()` to activate
    the outputs.'
  prefs: []
  type: TYPE_NORMAL
- en: '`prepare_dataset()` leverages the `load_image_and_label()` function defined
    previously to convert file paths into batches of image tensors and one-hot encoded
    labels.'
  prefs: []
  type: TYPE_NORMAL
- en: Using the three functions explained here, we prepared three subsets of data,
    with the purpose of training, validating, and testing the neural network. We trained
    the model for 250 epochs, using the `adam` optimizer and `CategoricalCrossentropy(from_logits=True)`
    as our loss function (`from_logits=True` produces a bit more numerical stability).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we got around 93.5% accuracy on the test set. Based on these results,
    you could use this network as a component of a Rock Paper Scissors game to recognize
    the hand gestures of a player and react accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information on the `Rock-Paper-Scissors Images` dataset, refer to
    the official Kaggle page where it''s hosted: [https://www.kaggle.com/drgfreeman/rockpaperscissors](https://www.kaggle.com/drgfreeman/rockpaperscissors).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a multi-label classifier to label watches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neural network is not limited to modeling the distribution of a single variable.
    In fact, it can easily handle instances where each image has multiple labels associated
    with it. In this recipe, we'll implement a CNN to classify the gender and style/usage
    of watches.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we must install `Pillow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll use the `Fashion Product Images (Small)` dataset hosted in Kaggle,
    which, after signing in, you can download here: [https://www.kaggle.com/paramaggarwal/fashion-product-images-small](https://www.kaggle.com/paramaggarwal/fashion-product-images-small).
    In this recipe, we assume the data is inside the `~/.keras/datasets` directory,
    under the name `fashion-product-images-small`. We''ll only use a subset of the
    data, focused on watches, which we''ll construct programmatically in the *How
    to do it…* section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sample images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Example images](img/B14768_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Example images
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s review the steps to complete the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to build the network architecture. First, implement the convolutional
    blocks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, add the fully convolutional layers:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to load all images and labels (gender and usage), given a
    list of image paths and a dictionary of metadata associated with each of them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the random seed to guarantee reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the paths to the images and the `styles.csv` metadata file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Keep only the `Watches` images for `Casual`, `Smart Casual`, and `Formal` usage,
    suited to `Men` and `Women`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the images and labels, resizing the images into a 64x64x3 shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the images and multi-hot encode the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the train, validation, and test splits:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build and compile the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model for `20` epochs, in batches of `64` images at a time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This block prints as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the model to make predictions on a test image, displaying the probability
    of each label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'That prints this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compare the ground truth labels with the network''s prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's see how it all works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We implemented a smaller version of a `gender` and `usage` metadata associated
    with each watch. In other words, we modeled two binary classification problems
    at the same time: one for `gender`, and one for `usage`. This is the reason we
    activated the outputs of the network with Sigmoid, instead of Softmax, and also
    why the loss function used is `binary_crossentropy` and not `categorical_crossentropy`.'
  prefs: []
  type: TYPE_NORMAL
- en: We trained the aforementioned network over 20 epochs, on batches of 64 images
    at a time, obtaining a respectable 90% accuracy on the test set. Finally, we made
    a prediction on an unseen image from the test set and verified that the labels
    produced with great certainty by the network (100% certainty for `Casual`, and
    99.16% for `Women`) correspond to the ground truth categories `Casual` and `Women`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information on the `Fashion Product Images (Small)` dataset, refer
    to the official Kaggle page where it is hosted: [https://www.kaggle.com/paramaggarwal/fashion-product-images-small](https://www.kaggle.com/paramaggarwal/fashion-product-images-small).
    I recommend you read the paper where the seminal **VGG** architecture was introduced:
    [https://arxiv.org/abs/1409.1556](https://arxiv.org/abs/1409.1556).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ResNet from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Residual Network**, or **ResNet** for short, constitutes one of the most
    groundbreaking advancements in deep learning. This architecture relies on a component
    called the residual module, which allows us to ensemble networks with depths that
    were unthinkable a couple of years ago. There are variants of **ResNet** that
    have more than 100 layers, without any loss of performance!'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll implement `CIFAR-10`, `CINIC-10`.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We won''t explain **ResNet** in depth, so it is a good idea to familiarize
    yourself with the architecture if you are interested in the details. You can read
    the original paper here: [https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to implement **ResNet** from the ground up:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all necessary modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an alias to the `tf.data.expertimental.AUTOTUNE` option, which we''ll
    use later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to create a residual module in the `reduce=True`, we apply
    a 1x1 convolution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we combine the shortcut and the third block into a single layer and
    return that as our output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to build a custom **ResNet** network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to load an image and its one-hot encoded labels, based on
    its file path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to create a `tf.data.Dataset` instance of images and labels
    from a glob-like pattern that refers to the folder where the images are:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the mean RGB values of the `CINIC-10` dataset, which is used in the
    `load_image_and_label()` function to mean normalize the images (this information
    is available on the official `CINIC-10` site):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the classes of the `CINIC-10` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download and extract the `CINIC-10` dataset to the `~/.keras/datasets` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the glob-like patterns to the train, test, and validation subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build, compile, and train a `ModelCheckpoint()` callback:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the best model (in this case, `model.38-0.72.hdf5`) and evaluate it on
    the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's learn how it all works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to `residual_module()` function receives the input data (`data`), the
    number of filters (`filters`), the stride (`stride`) of the convolutional blocks,
    a `reduce` flag to indicate whether we want to reduce the spatial size of the
    shortcut branch by applying a 1x1 convolution (a technique used to reduce the
    dimensionality of the output volumes of the filters), and parameters to adjust
    the amount of regularization (`reg`) and batch normalization applied to the different
    layers (`bn_eps` and `bn_momentum`).
  prefs: []
  type: TYPE_NORMAL
- en: 'A residual module comprises two branches: the first one is the skip connection,
    also known as the shortcut branch, which is basically the same as the input. The
    second or main branch is composed of three convolution blocks: a 1x1 with a quarter
    of the filters, a 3x3 one, also with a quarter of the filters, and finally another
    1x1, which uses all the filters. The shortcut and main branches are concatenated
    in the end using the `Add()` layer.'
  prefs: []
  type: TYPE_NORMAL
- en: '`build_network()` allows us to specify the number of stages to use, and also
    the number of filters per stage. We start by applying a 3x3 convolution to the
    input (after being batch normalized). Then we proceed to create the stages. A
    stage is a series of residual modules connected to each other. The length of the
    `stages` list controls the number of stages to create, and each element in this
    list controls the number of layers in that particular stage. The `filters` parameter
    contains the number of filters to use in each residual block within a stage. Finally,
    we built a fully connected network, Softmax-activated, on top of the stages with
    as many units as there are classes in the dataset (in this case, 10).'
  prefs: []
  type: TYPE_NORMAL
- en: Because `CINIC-10` is not an easy dataset and that we did not apply any data
    augmentation or transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information on the `CINIC-10` dataset, visit this link: [https://datashare.is.ed.ac.uk/handle/10283/3192](https://datashare.is.ed.ac.uk/handle/10283/3192).'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images with a pre-trained network using the Keras API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We do not always need to train a classifier from scratch, especially when the
    images we want to categorize resemble ones that another network trained on. In
    these instances, we can simply reuse the model, saving ourselves lots of time.
    In this recipe, we'll use a pre-trained network on ImageNet to classify a custom
    image.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will need `Pillow`. We can install it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'You''re free to use your own images in the recipe. Alternatively, you can download
    the one at this link: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2/recipe5/dog.jpg](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2/recipe5/dog.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the image we''ll pass to the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Image passed to the pre-trained classifier'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_02_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Image passed to the pre-trained classifier
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we'll see in this section, re-using a pre-trained classifier is very easy!
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages. These include the pre-trained network used for
    classification, as well as some helper functions to pre process the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate an `InceptionV3` network pre-trained on ImageNet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the image to classify. `InceptionV3` takes a 299x299x3 image, so we must
    resize it accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the image to a `numpy` array, and wrap it into a singleton batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pre process the image the same way `InceptionV3` does:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the model to make predictions on the image, and then decode the predictions
    to a matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Examine the top `5` predictions along with their probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the original image with its most probable label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This block generates the following image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.5 – Correctly classified image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_02_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – Correctly classified image
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how it all works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As evidenced here, in order to classify images effortlessly, using a pre-trained
    network on ImageNet, we just need to instantiate the proper model with the right
    weights, like this: `InceptionV3(weights=''imagenet'')`. This will download the
    architecture and the weights if it is the first time we are using them; otherwise,
    a version of these files will be cached in our system.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, we loaded the image we wanted to classify, resized it to dimensions compatible
    with `InceptionV3` (299x299x3), converted it into a singleton batch with `np.expand_dims(image,
    axis=0)`, and pre processed it the same way `InceptionV3` did when it was trained,
    with `preprocess_input(image)`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we got the predictions from the model, which we need to transform to a
    prediction matrix with the help of `imagenet_utils.decode_predictions(predictions)`.
    This matrix contains the label and probabilities in the 0th row, which we inspected
    to get the five most probable classes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about Keras pre-trained models here: [https://www.tensorflow.org/api_docs/python/tf/keras/applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications).'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images with a pre-trained network using TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow Hub** (**TFHub**) is a repository of hundreds of machine learning
    models contributed to by the big and rich community that surrounds TensorFlow.
    Here we can find models for a myriad of different tasks, not only for computer
    vision but for applications in many different domains, such as **Natural Language
    Processing** (**NLP**) and reinforcement learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll use a model trained on ImageNet, hosted on TFHub, to make
    predictions on a custom image. Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll need the `tensorflow-hub` and `Pillow` packages, which can be easily
    installed using `pip`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to use the same image we use in this recipe, you can download it
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2/recipe6/beetle.jpg](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch2/recipe6/beetle.jpg).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the image we''ll classify:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – Image to be classified'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_02_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – Image to be classified
  prefs: []
  type: TYPE_NORMAL
- en: Let's head to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s proceed with the recipe steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the URL of the pre-trained `ResNetV2152` classifier in **TFHub**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download and instantiate the classifier hosted on TFHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the image we''ll classify, convert it to a `numpy` array, normalize it,
    and wrap it into a singleton batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the pre-trained model to classify the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the index of the most probable prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download the ImageNet labels into a file named `ImageNetLabels.txt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the labels into a `numpy` array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the name of the class corresponding to the index of the most probable
    prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the original image with its most probable label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.7 – Correctly classified image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_02_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – Correctly classified image
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how it all works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After importing the relevant packages, we proceeded to define the URL of the
    model we wanted to use to classify our input image. To download and convert such
    a network into a Keras model, we used the convenient `hub.KerasLayer` class in
    *Step 3*. Then, in *Step 4*, we loaded the image we wanted to classify into memory,
    making sure its dimensions match the ones the network expects: 224x224x3.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Steps 5* and *6* perform the classification and extract the most probable
    category, respectively. However, to make this prediction human-readable, we downloaded
    a plain text file with all ImageNet labels in *Step 7*, which we then parsed using
    `numpy`, allowing us to use the index of the most probable category to obtain
    the corresponding label, finally displayed in *Step 10* along with the input image.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about the pre-trained model we used here: [https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4](https://tfhub.dev/google/imagenet/resnet_v2_152/classification/4).'
  prefs: []
  type: TYPE_NORMAL
- en: Using data augmentation to improve performance with the Keras API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More often than not, we can benefit from providing more data to our model. But
    data is expensive and scarce. Is there a way to circumvent this limitation? Yes,
    there is! We can synthesize new training examples by performing little modifications
    on the ones we already have, such as random rotations, random cropping, and horizontal
    flipping, among others. In this recipe, we'll learn how to use data augmentation
    with the Keras API to improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We must install `Pillow` and `tensorflow_docs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'In this recipe, we''ll use the `Caltech 101` dataset, which is available here:
    [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Download and decompress `101_ObjectCategories.tar.gz` to your preferred location.
    From now on, we assume the data is inside the `~/.keras/datasets` directory, under
    the name `101_ObjectCategories`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are sample images from `Caltech 101`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Caltech 101 sample images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_02_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – Caltech 101 sample images
  prefs: []
  type: TYPE_NORMAL
- en: Let's implement!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The steps listed here are necessary to complete the recipe. Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to load all images in the dataset, along with their labels,
    based on their file paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to build a smaller version of **VGG**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to plot and save a model''s training curve:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the paths to all images in the dataset, excepting the ones of the `BACKGROUND_Google`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the set of classes in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset into memory, normalizing the images and one-hot encoding the
    labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the training and testing subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build, compile, train, and evaluate a neural network without data augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy on the test set is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here''s the accuracy curve:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Training and validation accuracy for a network without data
    augmentation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B14768_02_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.9 – Training and validation accuracy for a network without data augmentation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build, compile, train, and evaluate the same network, this time with data augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy on the test set when we use data augmentation is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the accuracy curve looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.10 – Training and validation accuracy for a network with data augmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_02_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 – Training and validation accuracy for a network with data augmentation
  prefs: []
  type: TYPE_NORMAL
- en: Comparing *Steps 10* and *11*, we observe a noticeable gain in performance by
    using data augmentation. Let's understand better what we did in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a scaled-down version of `Caltech 101` dataset.
    First, we trained a network only on the original data, and then using data augmentation.
    The first network (see *Step 10*) obtained an accuracy level on the test set of
    61.3% and clearly shows signs of overfitting, because the gap that separates the
    training and validation accuracy curves is very wide. On the other hand, by applying
    a series of random perturbations, through `ImageDataGenerator()`, such as horizontal
    flips, rotations, width, and height shifting, among others (see *Step 11*), we
    increased the accuracy on the test set to 65.2%. Also, the gap between the training
    and validation accuracy curves is much smaller this time, which suggests a regularization
    effect resulting from the application of data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about `Caltech 101` here: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).'
  prefs: []
  type: TYPE_NORMAL
- en: Using data augmentation to improve performance with the tf.data and tf.image
    APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data augmentation is a powerful technique we can apply to artificially increment
    the size of our dataset, by creating slightly modified copies of the images at
    our disposal. In this recipe, we'll leverage the `tf.data` and `tf.image` APIs
    to increase the performance of a CNN trained on the challenging `Caltech 101`
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We must install `tensorflow_docs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'In this recipe, we''ll use the `Caltech 101` dataset, which is available here:
    [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).
    Download and decompress `101_ObjectCategories.tar.gz` to your preferred location.
    From now on, we assume the data is inside the `~/.keras/datasets` directory, in
    a folder named `101_ObjectCategories`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sample images from `Caltech 101`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – Caltech 101 sample images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_02_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – Caltech 101 sample images
  prefs: []
  type: TYPE_NORMAL
- en: Let's go to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's go over the steps required to complete this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an alias for the `tf.data.experimental.AUTOTUNE` flag, which we''ll
    use later on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to create a smaller version of **VGG**. Start by creating
    the input layer and the first block of two convolutions with 32 filters each:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Continue with the second block of two convolutions, this time each with 64
    kernels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the last part of the architecture, which consists of a series of fully
    connected layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to plot and save the training curves of a model, given its
    training history:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to load an image and one-hot encode its label, based on the
    image''s file path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to augment an image by performing random transformations
    on it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to prepare a `tf.data.Dataset` of images, based on a glob-like
    pattern that refers to the folder where they live:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the paths to all images in the dataset, excepting the ones of the `BACKGROUND_Google`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the unique categories in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the image paths into training and testing subsets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the training and testing datasets, without augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate, compile, train and evaluate the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy on the test set is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And here''s the accuracy curve:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.12 – Training and validation accuracy for a network without data
    augmentation'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B14768_02_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 2.12 – Training and validation accuracy for a network without data augmentation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prepare the training and testing sets, this time applying data augmentation
    to the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate, compile, train, and evaluate the network on the augmented data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy on the test set when we use data augmentation is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And the accuracy curve looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Training and validation accuracy for a network with data augmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_02_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 – Training and validation accuracy for a network with data augmentation
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand what we just did in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We just implemented a trimmed down version of the famous `Caltech 101` dataset.
    To better understand the advantages of data augmentation, we fitted a first version
    on the original data, without any modification, obtaining an accuracy level of
    65.32% on the test set. This first model displays signs of overfitting, because
    the gap that separates the training and validation accuracy curves widens early
    in the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we trained the same network on an augmented dataset (see *Step 15*), using
    the `augment()` function defined earlier. This greatly improved the model's performance,
    reaching a respectable accuracy of 74.19% on the test set. Also, the gap between
    the training and validation accuracy curves is noticeably smaller, which suggests
    a regularization effect coming out from the application of data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about `Caltech 101` here: [http://www.vision.caltech.edu/Image_Datasets/Caltech101/](http://www.vision.caltech.edu/Image_Datasets/Caltech101/).'
  prefs: []
  type: TYPE_NORMAL
