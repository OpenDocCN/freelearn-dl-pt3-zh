<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Credit Card Fraud Detection using Autoencoders</h1>
                </header>
            
            <article>
                
<p>The digital world is growing rapidly. We are used to performing many of our daily tasks online, such as booking cabs, shopping on e-commerce websites, and even recharging our phones. For the majority of these tasks, we are used to paying with credit cards. However, it is a known fact that a credit card can be compromised, which could result in a fraudulent transaction. The Nilson report estimates that for every $100 spent, seven cents are stolen. It estimates the total credit card fraud market to be around $30 billion.</p>
<p class="mce-root"><span>Detecting whether a transaction is fraudulent or not is a very impactful data science problem. Every bank that issues credit cards invests in technology to detect fraud and take the appropriate actions immediately. There are lot of standard supervised learning techniques such as logistic regression, from random forest to classifying fraud.</span></p>
<p>In this chapter, we will take a closer look at an unsupervised approach to detecting credit card fraud using auto-encoders by exploring the following topics:</p>
<ul>
<li>Understanding auto-encoders</li>
<li>Defining and training a fraud detection model</li>
<li>Testing a fraud detection model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding auto-encoders</h1>
                </header>
            
            <article>
                
<p>Auto-encoders are a type of artificial neural network whose job is to learn a low-dimensional representation of input data using unsupervised learning. They are quite popular when it comes to dimensionality reduction of input and in generative models.</p>
<p>Essentially, an auto-encoder learns to compress data into a low-dimensional representation and then reconstructs that representation into something that matches the original data. This way, the low-dimensional representation ignores the noise, which is not helpful in reconstructing the original data.</p>
<p>As mentioned previously, they are also useful in generating models, particularly images. For example, if we feed the representation of <em>dog</em> and <em>flying</em>, it may attempt to generate an image of a <em>flying cat</em>, which it has not seen before.</p>
<p>Structurally, auto-encoders consist of two parts, the encoder and the decoder. The encoder generates the low-dimensional representation of inputs, and the decoder helps regenerate the input from the representation. Generally, the encoder and the decoder are feedforward neural networks with one or multiple hidden layers.</p>
<p>The following diagram illustrates the configuration of a typical auto-encoder:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-500 image-border" src="assets/6cf79baa-d1b7-46d2-8b33-5bf772eeb6bd.png" style="width:26.58em;height:18.75em;"/></p>
<p>To define the encoder (<img class="fm-editor-equation" src="assets/3edd31b1-5963-4738-b6fa-6c48d1fed1d8.png" style="width:0.92em;height:1.67em;"/>) and the decoder (<img class="fm-editor-equation" src="assets/50571af6-621f-44c8-bb24-8f8c7aca02ce.png" style="width:1.17em;height:1.33em;"/>), an auto-encoder can be mathematically represented as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7192ce52-f955-4587-8bdb-6bcb988d11ae.png" style="width:17.17em;height:6.08em;"/></p>
<p>As mentioned in the equation, the parameters of the encoder and the decoder are optimized in a way that minimizes a special kind of error, which is known as <strong>reconstruction error.</strong> Reconstruction error is the error between the reconstructed input and the original input.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a fraud detection model</h1>
                </header>
            
            <article>
                
<p>For this project, we are going to use the credit card dataset from Kaggle (<a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">https://www.kaggle.com/mlg-ulb/creditcardfraud</a>), <span>Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015</span>. It consists of credit card transaction data from two days, from European cardholders. The dataset is highly imbalanced and contains approximately 284,000 pieces of transaction data with 492 instances of fraud (0.172% of the total).</p>
<p>There are 31 numerical columns in the dataset. Two of them are time and amount. <strong>Time</strong> denotes the amount of time elapsed (in seconds) between each transaction and the first transaction in the dataset. <strong>Amount</strong> is the total amount regarding the transaction. For our model, we will eliminate the time column as it doesn't help with the accuracy of the model. The rest of the features (V1, V2 ... V28) are obtained from Principal Component Analysis (<a href="https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/">https://ocw.mit.edu/courses/mathematics/18-650-statistics-for-applications-fall-2016/lecture-videos/lecture-19-video/</a>) of original features for confidential reasons. <strong>Class</strong> is the target variable, which indicates whether the transaction was fraudulent or not.</p>
<p>To pre-process the data, <span>there is not much that needs to be done.</span> This is mainly because a lot of the data is already cleaned up. </p>
<p>Usually, in a classical machine learning model such as logistic regression, we feed the data points of both negative and positive classes into the algorithm. However, since we are using auto-encoders, we will model it differently.</p>
<p>Essentially, our training set will consist of only non-fraudulent transaction data. The idea is that whenever we pass a fraudulent transaction through our trained model, it should detect it as an anomaly. We are framing this problem as anomaly detection rather than classification. </p>
<p><span>The model will consist of two fully connected encoder layers with 14 and seven neurons, respectively. There will be two decoder layers with seven and 29 neurons, respectively. Additionally, we will use L1 regularization during training.</span></p>
<p>Lastly, to define the model, we will use <span>Keras with Tensorflow at the backend for training auto-encoders.</span></p>
<div class="packt_infobox">Regularization is a technique in machine learning that's used to reduce overfitting. Overfitting happens when the model learns a signal as well as noise in the training data and can't generalize well to unseen dataset. While there are many ways to avoid overfitting such as cross validation, sampling, and so on, regularization specifically adds a penalty to weights of the model so that we don't learn an overly complex model. L1 regularization adds an L1 norm penalty on all the weights of the model. This way, any weight that doesn't contribute to the accuracy is shrunk to zero.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining and training a fraud detection model </h1>
                </header>
            
            <article>
                
<p>The following are the steps for defining and training the model:</p>
<ol>
<li>Transform <kbd>'Amount'</kbd> by removing the mean and scaling it to the unit's variance:</li>
</ol>
<pre style="padding-left: 60px">def preprocess_data(data):<br/>data = data.drop(['Time'], axis=1)<br/>data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))<br/>return<br/> data</pre>
<div class="packt_infobox">Note that we use the <kbd>StandardScaler</kbd> utility from scikit-learn for this purpose.</div>
<ol start="2">
<li>To model our dataset, split it into train and test data, with train consisting of only non-fraudulent transaction and test consisting of both fraudulent and non-fraudulent transactions:</li>
</ol>
<pre style="padding-left: 60px">def get_train_and_test_data(processed_data):<br/>X_train, X_test = train_test_split(processed_data, test_size=0.25, random_state=RANDOM_SEED)<br/>X_train = X_train[X_train.Class == 0]<br/>X_train = X_train.drop(['Class'], axis=1)<br/>y_test = X_test['Class']<br/>X_test = X_test.drop(['Class'], axis=1)<br/>X_train = X_train.values<br/>X_test = X_test.values<br/>return X_train, X_test,y_test</pre>
<ol start="3">
<li>Define the model by using the following code:</li>
</ol>
<pre style="padding-left: 60px">def define_model(self):<br/>dim_input = self.train_data.shape[1]<br/>layer_input = Input(shape=(dim_input,))<br/>layer_encoder = Dense(DIM_ENCODER, activation="tanh",<br/>activity_regularizer=regularizers.l1(10e-5))(layer_input)<br/>layer_encoder = Dense(int(DIM_ENCODER / 2), activation="relu")(layer_encoder)<br/>layer_decoder = Dense(int(DIM_ENCODER / 2), activation='tanh')(layer_encoder)<br/>layer_decoder = Dense(dim_input, activation='relu')(layer_decoder)<br/>autoencoder = Model(inputs=layer_input, outputs=layer_decoder)<br/>return autoencoder</pre>
<ol start="4">
<li>Once the model is defined, train the model using Keras:</li>
</ol>
<pre style="padding-left: 60px">def train_model(self):<br/>self.model.compile(optimizer=OPTIMIZER,<br/>loss=LOSS,<br/>metrics=[EVAL_METRIC])<br/>checkpoint = ModelCheckpoint(filepath=os.path.join(MODEL_SAVE_DIR, "trained_model.h5"),<br/>verbose=0,<br/>save_best_only=True)<br/>log_tensorboard = TensorBoard(log_dir='./logs',<br/>histogram_freq=0,<br/>write_graph=True,<br/>write_images=True)<br/>history = self.model.fit(self.train_data, self.train_data,<br/>epochs=EPOCHS,<br/>batch_size=BATCH_SIZE,<br/>shuffle=True,<br/>validation_data=(self.test_data, self.test_data),<br/>verbose=1,<br/>callbacks=[checkpoint, log_tensorboard]).history<br/>self.history = history<br/>print("Training Done. Plotting Loss Curves")<br/>self.plot_loss_curves()</pre>
<p class="mce-root"/>
<ol start="5">
<li>Use the following parameters to find the output of the model:</li>
</ol>
<ul>
<li style="list-style-type: none">
<ul>
<li>EPOCHS = 100.</li>
<li>BATCH_SIZE = 32.</li>
<li>OPTIMIZER = 'Adam'.</li>
<li>LOSS = Mean squared error between reconstructed and original input.</li>
<li>EVAL_METRIC = 'Accuracy'. This is the usual binary classification accuracy.</li>
</ul>
</li>
</ul>
<ol start="6">
<li>Store a <kbd>TensorBoard</kbd> file to visua<span>lize the graph or other variables on it. Also, store the best-performing model through the checkpoints provided by Keras. Generate the loss curves by epoch for the training and testing data: </span></li>
</ol>
<pre style="padding-left: 60px">def plot_loss_curves(self):<br/>fig = plt.figure(num="Loss Curves")<br/>fig.set_size_inches(12, 6)<br/>plt.plot(self.history['loss'])<br/>plt.plot(self.history['val_loss'])<br/>plt.title('Loss By Epoch')<br/>plt.ylabel('Loss')<br/>plt.xlabel('Epoch Num')<br/>plt.legend(['Train_Data', 'Test_Data'], loc='upper right');<br/>plt.grid(True, alpha=.25)<br/>plt.tight_layout()<br/>image_name = 'Loss_Curves.png'<br/>fig.savefig(os.path.join(PLOTS_DIR,image_name), dpi=fig.dpi)<br/>plt.clf()</pre>
<ol start="7">
<li>The following diagram illustrates the loss curves that are generated when the model is trained for 100 epochs:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-501 image-border" src="assets/996d8452-9c97-496b-a9cf-c5e3ff1ecbe8.png" style="width:100.00em;height:50.00em;"/></p>
<p>We can observe that for the training set, the loss or reconstruction error decreases at the start of the training and saturates toward the end. This saturation implies that the model has finished learning the weights.</p>
<div class="packt_infobox">Save the model with the lowest loss in the testing set.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing a fraud detection model</h1>
                </header>
            
            <article>
                
<p>Once the training process is complete, break down the reconstruction error in the testing set by fraudulent and non-fraudulent (normal) transactions. Generate the <span>reconstruction error by different classes of transactions:</span></p>
<pre>def plot_reconstruction_error_by_class(self):<br/>self.get_test_predictions()<br/>mse = np.mean(np.power(self.test_data - self.test_predictions, 2), axis=1)<br/>self.recon_error = pd.DataFrame({'recon_error': mse,<br/>'true_class': self.y_test})<br/>## Plotting the errors by class<br/># Normal Transactions<br/>fig = plt.figure(num = "Recon Error with Normal Transactions")<br/>fig.set_size_inches(12, 6)<br/>ax = fig.add_subplot(111)<br/>normal_error_df = self.recon_error[(self.recon_error['true_class'] == 0) &amp; (self.recon_error['recon_error'] &lt; 50)]<br/>_ = ax.hist(normal_error_df.recon_error.values, bins=20)<br/>plt.xlabel("Recon Error Bins")<br/>plt.ylabel("Num Samples")<br/>plt.title("Recon Error with Normal Transactions")<br/>plt.tight_layout()<br/>image_name = "Recon_Error_with_Normal_Transactions.png"<br/>fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)<br/>plt.clf()</pre>
<p>The following diagrams illustrate the reconstruction error distribution of fraudulent and normal transactions in the testing set:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-502 image-border" src="assets/3ca17733-d635-496e-ba21-ec6d1717ded3.png" style="width:100.00em;height:50.00em;"/></p>
<p>The next diagram is for fraud transactions:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-503 image-border" src="assets/c9facdbf-16d4-451d-a201-2015bc4ffe6f.png" style="width:100.00em;height:50.00em;"/></p>
<p>As we can see, the reconstruction error for normal transactions is very close to zero for most of the transactions. However, the reconstruction error with fraudulent transactions has a wide distribution, with the majority still being close to zero.</p>
<p>This suggests that a threshold on the reconstruction error can serve as a classification threshold on normal versus fraudulent transactions.</p>
<p>For evaluating the model, we will use the metrics Precision and Recall which were defined in <a href="5de5c0e0-5fcc-4352-af84-4b813138ea90.xhtml">Chapter 4</a>, <em>Digit Classification Using TensorFlow Lite</em>, of the book. Firstly, let's look at the precision and recall at various thresholds of reconstruction error:</p>
<pre>def get_precision_recall_curves(self):<br/>precision, recall, threshold = precision_recall_curve(self.recon_error.true_class, self.recon_error.recon_error)<br/># Plotting the precision curve<br/>fig = plt.figure(num ="Precision Curve")<br/>fig.set_size_inches(12, 6)<br/>plt.plot(threshold, precision[1:], 'g', label='Precision curve')<br/>plt.title('Precision By Recon Error Threshold Values')<br/>plt.xlabel('Threshold')<br/>plt.ylabel('Precision')<br/>plt.tight_layout()<br/>image_name = 'Precision_Threshold_Curve.png'<br/>fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)<br/>plt.clf()<br/>plt.plot(threshold, recall[1:], 'g', label='Recall curve')<br/>plt.title('Recall By Recon Error Threshold Values')<br/>plt.xlabel('Threshold')<br/>plt.ylabel('Recall')<br/>plt.tight_layout()<br/>image_name = 'Recall_Threshold_Curve.png'<br/>fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)<br/>plt.clf()</pre>
<p>The reconstruction error <span>threshold </span>for precision and recall are shown in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-837 image-border" src="assets/7f72f802-1e2c-47bc-a9dd-8aaa6acf9b9b.png" style="width:57.25em;height:28.50em;"/></p>
<p>The diagram represents the error threshold for recall:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-511 image-border" src="assets/eb99eb0a-81d4-4ee5-803e-4e5566976672.png" style="width:59.92em;height:29.92em;"/></p>
<p>As we can see, recall decreases when there is an increase in reconstruction error, and vice versa for precision. There are a few dips due to the dataset.</p>
<p>There is one other thing that we need to keep in mind. As mentioned previously, <span>there is always a trade-off between high precision and high recall in machine learning. We need to choose any one for our particular model. </span></p>
<p>Generally, businesses prefer a model with high precision or high recall. For fraud detection, we would like to have a model with high recall. This is essential as we can classify the majority of fraudulent transactions as fraud. One method to counter the loss of precision is to do a manual verification of transactions classified as fraud to determine whether they are actually fraudulent. This will help in ensuring a good experience for the end user.</p>
<p>Here is the code to generate a confusion matrix with <kbd>min_recall</kbd> = 80%:</p>
<pre>def get_confusion_matrix(self, min_recall = 0.8):<br/># Get the confusion matrix with min desired recall on the testing dataset used.<br/>precision, recall, threshold = precision_recall_curve(self.recon_error.true_class, self.recon_error.recon_error)<br/>idx = filter(lambda x: x[1] &gt; min_recall, enumerate(recall[1:]))[-1][0]<br/>th = threshold[idx]<br/>print ("Min recall is : %f, Threshold for recon error is: %f " %(recall[idx+1], th))<br/># Get the confusion matrix<br/>predicted_class = [1 if e &gt; th else 0 for e in self.recon_error.recon_error.values]<br/>cnf_matrix = confusion_matrix(self.recon_error.true_class, predicted_class)<br/>classes = ['Normal','Fraud']<br/>fig = plt.figure(figsize=(12, 12))<br/>plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.cm.Blues)<br/>plt.title("Confusion Matrix")<br/>plt.colorbar()<br/>tick_marks = np.arange(len(classes))<br/>plt.xticks(tick_marks, classes, rotation=45)<br/>plt.yticks(tick_marks, classes)<br/>fmt = 'd'<br/>thresh = cnf_matrix.max() / 2.<br/>for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):<br/>plt.text(j, i, format(cnf_matrix[i, j], fmt),<br/>horizontalalignment="center",<br/>color="white" if cnf_matrix[i, j] &gt; thresh else "black")<br/>plt.ylabel('True label')<br/>plt.xlabel('Predicted label')<br/>plt.tight_layout()<br/>image_name = 'Confusion_Matrix_with_threshold_{}.png'.format(th)<br/>fig.savefig(os.path.join(PLOTS_DIR, image_name), dpi=fig.dpi)<br/>plt.clf()</pre>
<p>The confusion matrix that's obtained from the preceding code is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-509 image-border" src="assets/7bd4f84c-ac36-4477-abb8-f39711b06812.png" style="width:47.67em;height:50.92em;"/></p>
<p>We can observe that out of 120 fraudulent transactions, 97 of them have been classified correctly. However, we have also classified 1,082 normal transaction as being fraudulent, which will have to go through a manual verification process to ensure a good experience for the end user.</p>
<p><span>As a note of caution, we should not assume that auto-encoders are helpful in all binary classification tasks and can achieve better performance than state-of-the-art classification models. The idea behind this project was to illustrate a different approach of using auto-encoders to perform classification tasks.</span></p>
<div class="packt_infobox">Note that in this chapter, we have used the same validation and test set for illustrative purposes. Ideally, once we have defined the threshold on the reconstruction error, we should test the model on some unseen dataset to evaluate its performance in a better manner.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Credit card fraud are ubiquitous in nature. Every company in today's world is employing machine learning to combat payment fraud on their platform. In this chapter, we looked at the problem of classifying fraud using the credit card dataset from Kaggle.</p>
<p>We learned about auto-encoders as a dimensionality reduction technique. We understood that the auto-encoder architecture consists of two components: an encoder and a decoder. We model the parameters of a fully connected network using reconstruction loss.</p>
<p>Thereafter, we looked at the fraud classification problem through the lens of an anomaly detection problem. We trained the auto-encoder model using normal transactions. We then looked at the reconstruction error of the auto-encoder for both normal and fraudulent transactions, and observed that the reconstruction error has a wide distribution for fraudulent transactions. We then defined a threshold on reconstruction to classify the model and generated the confusion matrix.</p>
<p>In the next chapter, <span>we will explore the concept of Bayesian neural networks, which combines the concepts of deep learning and Bayesian learning to model uncertainty in the prediction of deep neural networks.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<p>The following are the questions:</p>
<ul>
<li class="mce-root">What is an auto-encoder?</li>
<li class="mce-root">What are different components of an auto-encoder?</li>
<li class="mce-root">What is the reconstruction loss?</li>
<li class="mce-root">What is the precision and recall?</li>
</ul>


            </article>

            
        </section>
    </body></html>