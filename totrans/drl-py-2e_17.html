<html><head></head><body>
  <div id="_idContainer2970">
    <h1 class="chapterNumber">17</h1>
    <h1 id="_idParaDest-452" class="chapterTitle">Reinforcement Learning Frontiers</h1>
    <p class="normal">Congratulations! You have made it to the final chapter. We have come a long way. We started off with the fundamentals of reinforcement learning and gradually we learned about the state-of-the-art deep reinforcement learning algorithms. In this chapter, we will look at some exciting and promising research trends in reinforcement learning. We will start the chapter by learning what meta learning is and how it differs from other learning paradigms. Then, we will learn about one of the most used meta-learning algorithms, called <strong class="keyword">Model-Agnostic Meta Learning</strong> (<strong class="keyword">MAML</strong>).</p>
    <p class="normal">We will understand MAML in detail, and then we will see how to apply it in a reinforcement learning setting. Following this, we will learn about hierarchical reinforcement learning, and we look into a popular hierarchical reinforcement learning algorithm called MAXQ value function decomposition.</p>
    <p class="normal">At the end of the chapter, we will look at an interesting algorithm called <strong class="keyword">Imagination Augmented Agents</strong> (<strong class="keyword">I2As</strong>), which makes use of both model-based and model-free learning.</p>
    <p class="normal">In this chapter, we will learn about the following topics:</p>
    <ul>
      <li class="bullet">Meta reinforcement learning</li>
      <li class="bullet">Model-agnostic meta learning</li>
      <li class="bullet">Hierarchical reinforcement learning</li>
      <li class="bullet">MAXQ value function decomposition</li>
      <li class="bullet">Imagination Augmented Agents</li>
    </ul>
    <p class="normal">Let's begin the chapter by understanding meta reinforcement learning.</p>
    <h1 id="_idParaDest-453" class="title">Meta reinforcement learning</h1>
    <p class="normal">In order to <a id="_idIndexMarker1505"/>understand how meta reinforcement learning works, first let's understand meta learning. </p>
    <p class="normal">Meta learning is one of the most promising and trending research areas in the field of artificial <a id="_idIndexMarker1506"/>intelligence. It is believed to be a stepping stone for attaining <strong class="keyword">Artificial General Intelligence</strong> (<strong class="keyword">AGI</strong>). What is meta learning? And why do we need meta learning? To answer these questions, let's revisit how deep learning works.</p>
    <p class="normal">We know that in deep learning, we train a deep neural network to perform a task. But the problem with deep neural networks is that we need to have a large training dataset to train our network, as it will fail to learn when we have only a few data points. </p>
    <p class="normal">Let's say we trained a deep learning model to perform task <strong class="keyword">A</strong>. Suppose we have a new task <strong class="keyword">B</strong>, which is closely related to task <strong class="keyword">A</strong>. Although task <strong class="keyword">B</strong> is closely related to task <strong class="keyword">A</strong>, we can't use the model we trained for task <strong class="keyword">A</strong> to perform task <strong class="keyword">B</strong>. We need to train a new model from scratch for task <strong class="keyword">B</strong>. So, for each task, we need to train a new model from scratch although they might be related. But is this really true AI? Not really. How do we humans learn? We generalize our learning to multiple concepts and learn from there. But current learning algorithms master only one task. So, here is where meta learning comes in.</p>
    <p class="normal">Meta learning produces a versatile AI model that can learn to perform various tasks without having to be trained from scratch. We train our meta-learning model on various related tasks with few data points, so for a new related task, it can make use of the learning achieved in previous tasks. Many researchers and scientists believe that meta learning can get us closer to achieving AGI. Learning to learn is the key focus of meta learning. We will understand how exactly meta learning works by looking at a popular meta learning algorithm called MAML in the next section.</p>
    <h2 id="_idParaDest-454" class="title">Model-agnostic meta learning</h2>
    <p class="normal"><strong class="keyword">Model-Agnostic Meta Learning</strong> (<strong class="keyword">MAML</strong>) is one of the most popular meta-learning algorithms <a id="_idIndexMarker1507"/>and it has been a major breakthrough in meta-learning research. The basic idea of MAML is to find a better initial model parameter so that with a good initial parameter, a model can learn quickly on new tasks with fewer gradient steps.</p>
    <p class="normal">So, what do we mean by that? Let's say we are performing a classification task using a neural network. How do we train the network? We start off by initializing random weights and train the network by minimizing the loss. How do we minimize the loss? We minimize the loss using gradient descent. Okay, but how do we use gradient descent to minimize the loss? We use gradient descent to find the optimal weights that will give us the minimal loss. We take multiple gradient steps to find the optimal weights so that we can reach convergence.</p>
    <p class="normal">In MAML, we try to find these optimal weights by learning from the distribution of similar tasks. So, for a new task, we don't have to start with randomly initialized weights; instead, we can start with optimal weights, which will take fewer gradient steps to reach convergence and doesn't require more data points for training.</p>
    <p class="normal">Let's understand how MAML works in simple terms. Let's suppose we have three related tasks: <em class="italic">T</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">2</sub>, and <em class="italic">T</em><sub class="Subscript--PACKT-">3</sub>.</p>
    <p class="normal">First, we <a id="_idIndexMarker1508"/>randomly initialize our model parameter (weight), <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/>. We train our network on task <em class="italic">T</em><sub class="Subscript--PACKT-">1</sub>. Then, we try to minimize the loss <em class="italic">L</em> by gradient descent. We minimize the loss by finding the optimal parameter. Let <img src="../Images/B15558_17_002.png" alt="" style="height: 1.2em;"/> be the optimal parameter for the task <em class="italic">T</em><sub class="Subscript--PACKT-">1</sub>. Similarly, for tasks <em class="italic">T</em><sub class="Subscript--PACKT-">2</sub> and <em class="italic">T</em><sub class="Subscript--PACKT-">3</sub>, we will start off with a randomly initialized model parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/> and minimize the loss by finding the optimal parameters by gradient descent. Let <img src="../Images/B15558_12_208.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_17_005.png" alt="" style="height: 1.2em;"/> be the optimal parameters for tasks <em class="italic">T</em><sub class="Subscript--PACKT-">2</sub> and <em class="italic">T</em><sub class="Subscript--PACKT-">3</sub>, respectively.</p>
    <p class="normal">As we can see in the following figure, we start off each task with the randomly initialized parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> and minimize the loss by finding the optimal parameters <img src="../Images/B15558_17_007.png" alt="" style="height: 1.2em;"/>, <img src="../Images/B15558_17_008.png" alt="" style="height: 1.2em;"/>, and <img src="../Images/B15558_17_009.png" alt="" style="height: 1.2em;"/> for the tasks <em class="italic">T</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">2</sub>, and <em class="italic">T</em><sub class="Subscript--PACKT-">3</sub> respectively:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.1: <img src="../Images/B15558_17_010.png" alt="" style="height: 1.11em;"/> is initialized at a random position</p>
    <p class="normal">However, instead <a id="_idIndexMarker1509"/>of initializing <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> in a random position, that is, with random values, if we initialize <img src="../Images/B15558_09_123.png" alt="" style="height: 1.11em;"/> in a position that is common to all three tasks, then we don't need to take many gradient steps and it will take us less time to train. MAML tries to do exactly this. MAML tries to find this optimal parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/> that is common to many of the related tasks, so we can train a new task relatively quick with few data points without having to take many gradient steps.</p>
    <p class="normal">As <em class="italic">Figure 17.2</em> shows, we shift <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> to a position that is common to all different optimal <img src="../Images/B15558_09_107.png" alt="" style="height: 1.2em;"/> values:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.2: <img src="../Images/B15558_17_016.png" alt="" style="height: 1.11em;"/> is initialized at the optimal position</p>
    <p class="normal">So, for a <a id="_idIndexMarker1510"/>new related task, say, <em class="italic">T</em><sub class="" style="font-style: italic;">4</sub>, we don't have to start with a randomly initialized parameter, <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>. Instead, we can start with the optimal <img src="../Images/B15558_09_123.png" alt="" style="height: 1.11em;"/> value (shifted <img src="../Images/B15558_10_037.png" alt="" style="height: 1.11em;"/>) so that it will take fewer gradient steps to attain convergence.</p>
    <p class="normal">Thus, in MAML, we try to find this optimal <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> value that is common to related tasks to help us learn from fewer data points and minimize our training time. MAML is model-agnostic, meaning that we can apply MAML to any models that are trainable with gradient descent. But how exactly does MAML work? How do we shift the model parameters to an optimal position? Now that we have a basic understanding of MAML, we will address all these questions in the next section.</p>
    <h3 id="_idParaDest-455" class="title">Understanding MAML</h3>
    <p class="normal">Suppose <a id="_idIndexMarker1511"/>we have a model <em class="italic">f</em> parameterized by <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/>, that is, <img src="../Images/B15558_17_022.png" alt="" style="height: 1.11em;"/>, and we have a distribution over tasks, <em class="italic">p(T)</em>. First, we initialize our parameter <img src="../Images/B15558_09_056.png" alt="" style="height: 1.11em;"/> with some random values. Next, we sample a batch of tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> from a distribution over tasks―that is, <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub><em class="italic"> ~ p(T)</em>. Let's say we have sampled five tasks: <em class="italic">T</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">3</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">4</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">5</sub>.</p>
    <p class="normal">Now, for each task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>, we sample <em class="italic">k</em> number of data points and train the model <em class="italic">f</em> parameterized by <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>, that is, <img src="../Images/B15558_17_025.png" alt="" style="height: 1.11em;"/>. We train the model by computing the loss <img src="../Images/B15558_17_026.png" alt="" style="height: 1.2em;"/> and we minimize the loss using gradient descent and find the optimal parameter <img src="../Images/B15558_17_027.png" alt="" style="height: 1.2em;"/>. The parameter update rule using gradient descent is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_028.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">In the preceding equation, the following applies:</p>
    <ul>
      <li class="bullet"><img src="../Images/B15558_17_027.png" alt="" style="height: 1.2em;"/> is the optimal parameter for a task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub></li>
      <li class="bullet"><img src="../Images/B15558_17_030.png" alt="" style="height: 1.11em;"/> is the initial parameter</li>
      <li class="bullet"><img src="../Images/B15558_07_025.png" alt="" style="height: 0.93em;"/> is the learning rate</li>
      <li class="bullet"><img src="../Images/B15558_17_032.png" alt="" style="height: 1.2em;"/> is the gradient of loss for a task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> with the model parameterized as <img src="../Images/B15558_17_033.png" alt="" style="height: 1.11em;"/></li>
    </ul>
    <p class="normal">So, after the preceding parameter update using gradient descent, we will have optimal parameters for all five tasks that we have sampled. That is, for the tasks <em class="italic">T</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">3</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">4</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">5</sub>, we will have the optimal parameters <img src="../Images/B15558_17_034.png" alt="" style="height: 1.2em;"/>, respectively.</p>
    <p class="normal">Now, before <a id="_idIndexMarker1512"/>the next iteration, we perform a meta update or meta optimization. That is, in the previous step, we found the optimal parameter <img src="../Images/B15558_17_035.png" alt="" style="height: 1.2em;"/> by training on each of the tasks, <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>. Now we take some new set of tasks and for each of these new tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>, we don't have to start from the random position <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>; instead, we can start from the optimal position <img src="../Images/B15558_17_037.png" alt="" style="height: 1.2em;"/> to train the model.</p>
    <p class="normal">That is, for each of the new tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>, instead of using the randomly initialized parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>, we use the optimal parameter <img src="../Images/B15558_17_039.png" alt="" style="height: 1.2em;"/>. This implies that we train the model <em class="italic">f</em> parameterized by <img src="../Images/B15558_17_040.png" alt="" style="height: 1.2em;"/>, that is, <img src="../Images/B15558_17_041.png" alt="" style="height: 1.29em;"/> instead of using <img src="../Images/B15558_17_025.png" alt="" style="height: 1.11em;"/>. Then, we calculate the loss <img src="../Images/B15558_17_043.png" alt="" style="height: 1.29em;"/>, compute the gradients, and update the parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>. This makes our randomly initialized parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/> move to an optimal position where we don't have to take many gradient steps. This step is called a meta update, meta optimization, or meta training. It can be expressed as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_046.png" alt="" style="height: 2.78em;"/></figure>
    <p class="normal">In equation (2), the following applies:</p>
    <ul>
      <li class="bullet"><img src="../Images/B15558_14_004.png" alt="" style="height: 1.11em;"/> is the initial parameter</li>
      <li class="bullet"><img src="../Images/B15558_09_152.png" alt="" style="height: 1.11em;"/> is the learning rate</li>
      <li class="bullet"><img src="../Images/B15558_17_049.png" alt="" style="height: 2.78em;"/> is the gradient of loss for each of the new tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>, with the model parameterized as <img src="../Images/B15558_17_041.png" alt="" style="height: 1.29em;"/></li>
    </ul>
    <p class="normal">If you <a id="_idIndexMarker1513"/>look at our previous meta update equation (2) closely, we can see that we are updating our model parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/> by merely taking an average of gradients of each new task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> with the model <em class="italic">f</em> parameterized by <img src="../Images/B15558_17_037.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Figure 17.3 helps us to understand the MAML algorithm better. As we can observe, our MAML <a id="_idIndexMarker1514"/>algorithm has two loops—an <strong class="keyword">inner loop</strong> where we find the optimal parameter <img src="../Images/B15558_17_053.png" alt="" style="height: 1.2em;"/> for each of the tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> using the model <em class="italic">f</em> parameterized <a id="_idIndexMarker1515"/>by initial parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>, that is, <img src="../Images/B15558_17_055.png" alt="" style="height: 1.11em;"/>, and an <strong class="keyword">outer loop </strong>where we use the model <em class="italic">f</em> parameterized by the optimal parameter <img src="../Images/B15558_17_056.png" alt="" style="height: 1.2em;"/> obtained in the previous step, that is <img src="../Images/B15558_17_057.png" alt="" style="height: 1.29em;"/>, and train the model on the new set of tasks, calculate the loss, compute the gradient of the loss, and update the randomly initialized model parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.3: The MAML algorithm</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Note that we should not use the same set of tasks we used to find the optimal parameter <img src="../Images/B15558_17_059.png" alt="" style="height: 1.2em;"/> when updating the model parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> in the outer loop.</p>
    </div>
    <p class="normal">In a nutshell, in MAML, we sample a batch of tasks and for each task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> in the batch, we minimize <a id="_idIndexMarker1516"/>the loss using gradient descent and get the optimal parameter <img src="../Images/B15558_17_027.png" alt="" style="height: 1.2em;"/>. Then, we update our randomly initialized model parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> by calculating gradients for each new task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> with the model parameterized as <img src="../Images/B15558_17_041.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">Still not clear how exactly MAML works? Worry not! Let's look in even more detail at the steps and understand how MAML works in a supervised learning setting in the next section.</p>
    <h3 id="_idParaDest-456" class="title">MAML in a supervised learning setting</h3>
    <p class="normal">As <a id="_idIndexMarker1517"/>we learned, MAML is model-agnostic and so we can apply MAML to any model that can be trained with gradient descent. In this section, let's learn how to apply the MAML algorithm in a supervised learning setting. Before going ahead, let's define our loss function.</p>
    <p class="normal">If we are performing regression, then we can use mean squared error as our loss function:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_064.png" alt="" style="height: 2.96em;"/></figure>
    <p class="normal">If we are performing classification, then we can use cross-entropy loss as our loss function:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_065.png" alt="" style="height: 2.96em;"/></figure>
    <p class="normal">Now let's see step by step how exactly MAML is used in supervised learning.</p>
    <p class="normal">Let's say we have a model <em class="italic">f</em> parameterized by a parameter <img src="../Images/B15558_17_066.png" alt="" style="height: 1.11em;"/>, and we have a distribution over tasks <em class="italic">p(T)</em>. First, we randomly initialize the model parameter <img src="../Images/B15558_09_008.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Next, we sample a batch of tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> from a distribution of tasks, that is, <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub><em class="italic"> ~ p(T)</em>. Let's say we <a id="_idIndexMarker1518"/>have sampled three tasks; then, we have <em class="italic">T</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">3</sub>.</p>
    <p class="normal"><strong class="keyword">Inner loop:</strong> For each task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>, we sample <em class="italic">k</em> data points and prepare our training and test datasets:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_068.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Wait! What are the training and test datasets? We use the training dataset in the inner loop for finding the optimal parameter <img src="../Images/B15558_17_069.png" alt="" style="height: 1.2em;"/> and the test set in the outer loop for finding the optimal parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>. The test dataset does not mean that we are checking the model's performance. It basically acts as a training set in the outer loop. We can also call our test set a meta-training set.</p>
    <p class="normal">Now, we train the model <img src="../Images/B15558_17_055.png" alt="" style="height: 1.11em;"/> on the training dataset <img src="../Images/B15558_17_072.png" alt="" style="height: 1.29em;"/>, calculate the loss, minimize the loss using gradient descent, and get the optimal parameter <img src="../Images/B15558_17_059.png" alt="" style="height: 1.2em;"/> as <img src="../Images/B15558_17_074.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">That is, for each of the tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>, we sample <em class="italic">k</em> data points and prepare <img src="../Images/B15558_17_075.png" alt="" style="height: 1.29em;"/> and <img src="../Images/B15558_17_076.png" alt="" style="height: 1.29em;"/>. Next, we minimize the loss on the training dataset <img src="../Images/B15558_17_077.png" alt="" style="height: 1.29em;"/> and get the optimal parameter <img src="../Images/B15558_17_078.png" alt="" style="height: 1.2em;"/>. As we sampled three tasks, we will have three optimal parameters, <img src="../Images/B15558_17_079.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal"><strong class="keyword">Outer loop</strong>: Now, we perform meta optimization on the test set (meta-training set); that is, we <a id="_idIndexMarker1519"/>try to minimize the loss in the test set <img src="../Images/B15558_17_080.png" alt="" style="height: 1.29em;"/>. Here, we parameterize our model <em class="italic">f</em> by the optimal parameter <img src="../Images/B15558_17_069.png" alt="" style="height: 1.2em;"/> calculated in the previous step. So, we compute the loss of the model <img src="../Images/B15558_17_082.png" alt="" style="height: 1.29em;"/> and the gradients of the loss and update our randomly initialized parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/> using our test dataset (meta-training dataset) as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_084.png" alt="" style="height: 2.78em;"/></figure>
    <p class="normal">We repeat the preceding steps for several iterations to find the optimal parameter. For a clear understanding of how MAML works in supervised learning, let's look into the algorithm in the next section.</p>
    <h4 class="title">Algorithm – MAML in supervised learning</h4>
    <p class="normal">The <a id="_idIndexMarker1520"/>algorithm of MAML in a supervised learning setting is given as follows:</p>
    <ol>
      <li class="numbered">Say that we have a model <em class="italic">f</em> parameterized by a parameter <img src="../Images/B15558_09_056.png" alt="" style="height: 1.11em;"/> and we have a distribution over tasks <em class="italic">p(T)</em>. First, we randomly initialize the model parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>.</li>
      <li class="numbered">Sample a batch of tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> from a distribution of tasks, that is, <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub><em class="italic"> ~ p(T)</em>.</li>
      <li class="numbered">For each task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>:<ol>
          <li class="numbered-l2"> Sample <em class="italic">k</em> data points and prepare our training and test datasets: <figure class="mediaobject"><img src="../Images/B15558_17_068.png" alt="" style="height: 2.69em;"/></figure>
          </li>
          <li class="numbered-l2">Train the model <img src="../Images/B15558_17_088.png" alt="" style="height: 1.11em;"/> on the training dataset <img src="../Images/B15558_17_089.png" alt="" style="height: 1.29em;"/> and compute the loss.</li>
          <li class="numbered-l2">Minimize the loss using gradient descent and get the optimal parameter <img src="../Images/B15558_17_053.png" alt="" style="height: 1.2em;"/> as <img src="../Images/B15558_17_091.png" alt="" style="height: 1.29em;"/>.</li>
        </ol>
      </li>
      <li class="numbered"> Now, minimize the loss on the test set <img src="../Images/B15558_17_092.png" alt="" style="height: 1.29em;"/>. Parameterize the model <em class="italic">f</em> with the optimal parameter <img src="../Images/B15558_17_069.png" alt="" style="height: 1.2em;"/> calculated in the previous step, compute loss <img src="../Images/B15558_17_094.png" alt="" style="height: 1.29em;"/>. Calculate gradients of the loss and update our randomly initialized parameter <img src="../Images/B15558_17_066.png" alt="" style="height: 1.11em;"/> using our test (meta-training) dataset as: <figure class="mediaobject"><img src="../Images/B15558_17_084.png" alt="" style="height: 2.78em;"/>.</figure>
      </li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to <em class="italic">4</em> for several iterations.</li>
    </ol>
    <p class="normal">The following figure gives us an overview of how the MAML algorithm works in supervised learning:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.4: Overview of MAML</p>
    <p class="normal">Now <a id="_idIndexMarker1521"/>that we have learned how to use MAML in a supervised learning setting, in the next section, we will see how to use MAML in a reinforcement learning setting.</p>
    <h3 id="_idParaDest-457" class="title">MAML in a reinforcement learning setting</h3>
    <p class="normal">Now, let's learn how to apply the MAML algorithm in a reinforcement learning setting. We <a id="_idIndexMarker1522"/>know that the objective of reinforcement learning is to find the optimal policy, that is, the policy that gives the maximum return. We've learned about several reinforcement learning algorithms for finding the optimal policy, and we've also learned about several deep reinforcement learning algorithms for finding the optimal policy, where we used the neural network parameterized by <img src="../Images/B15558_10_095.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">We can apply MAML to any algorithm that can be trained with gradient descent. For instance, let's take the policy gradient method. In the policy gradient method, we use a neural network parameterized by <img src="../Images/B15558_09_106.png" alt="" style="height: 1.11em;"/> to find the optimal policy and we train our network using gradient descent. So, we can apply the MAML algorithm to the policy gradient method.</p>
    <p class="normal">Let's understand how MAML works in reinforcement learning step by step.</p>
    <p class="normal">Let's say we have a model (policy network) <em class="italic">f</em> parameterized by a parameter <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/>. The model (policy network) <em class="italic">f</em> tries to find the optimal policy by learning the optimal parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>. Suppose, we have a distribution over tasks <em class="italic">p(T)</em>. First, we randomly initialize the model parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Next, we sample a batch of tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> from a distribution of tasks, that is, <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub><em class="italic"> ~ p(T)</em>. Let's say we <a id="_idIndexMarker1523"/>have sampled three tasks; then, we have <em class="italic">T</em><sub class="Subscript--PACKT-">1</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">2</sub>, <em class="italic">T</em><sub class="Subscript--PACKT-">3</sub>.</p>
    <p class="normal"><strong class="keyword">Inner loop</strong>: For each task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>, we prepare our train dataset <img src="../Images/B15558_17_089.png" alt="" style="height: 1.29em;"/>. Okay, how can we create the training dataset in a reinforcement learning setting?</p>
    <p class="normal">We have the model (policy network) <img src="../Images/B15558_17_025.png" alt="" style="height: 1.11em;"/>. So, we generate <em class="italic">k</em> number of trajectories using our model <img src="../Images/B15558_17_025.png" alt="" style="height: 1.11em;"/>. We know that the trajectories consist of a sequence of state-action pairs. So, we have:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_105.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Now, we compute the loss and minimize it using gradient descent and get the optimal parameter <img src="../Images/B15558_17_053.png" alt="" style="height: 1.2em;"/> as <img src="../Images/B15558_17_074.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">That is, for each of the tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>, we sample <em class="italic">k</em> trajectories and prepare the training dataset <img src="../Images/B15558_17_108.png" alt="" style="height: 1.29em;"/>. Next, we minimize the loss on the training dataset <img src="../Images/B15558_17_109.png" alt="" style="height: 1.29em;"/> and get the optimal parameter <img src="../Images/B15558_17_053.png" alt="" style="height: 1.2em;"/>. As we sampled three tasks, we will have three optimal parameters, <img src="../Images/B15558_17_111.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">We <a id="_idIndexMarker1524"/>also need the test dataset <img src="../Images/B15558_17_112.png" alt="" style="height: 1.29em;"/>, which we use in the outer loop. How do we prepare our test dataset? Now, we use our model <em class="italic">f</em> parameterized by the optimal parameter <img src="../Images/B15558_17_056.png" alt="" style="height: 1.2em;"/>; that is, we use <img src="../Images/B15558_17_057.png" alt="" style="height: 1.29em;"/> and generate <em class="italic">k</em> number of trajectories. So, we have:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_115.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Remember that <img src="../Images/B15558_17_116.png" alt="" style="height: 1.29em;"/> is created by <img src="../Images/B15558_17_117.png" alt="" style="height: 1.11em;"/> and the test (meta-training) dataset <img src="../Images/B15558_17_118.png" alt="" style="height: 1.29em;"/> is created by <img src="../Images/B15558_17_119.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal"><strong class="keyword">Outer loop</strong>: Now, we perform meta optimization on the test (meta-training) dataset; that is, we try to minimize the loss in the test dataset <img src="../Images/B15558_17_120.png" alt="" style="height: 1.29em;"/>. Here, we parameterize our model <em class="italic">f</em> by the optimal parameter <img src="../Images/B15558_17_053.png" alt="" style="height: 1.2em;"/> calculated in the previous step. So, we compute the loss of the model <img src="../Images/B15558_17_043.png" alt="" style="height: 1.29em;"/> and the gradients of the loss and update our randomly initialized parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> using our test (meta-training) dataset as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_084.png" alt="" style="height: 2.78em;"/></figure>
    <p class="normal">We <a id="_idIndexMarker1525"/>repeat the preceding step for several iterations to find the optimal parameter. For a clear understanding of how MAML works in reinforcement learning, let's look into the algorithm in the next section.</p>
    <h4 class="title">Algorithm – MAML in reinforcement learning</h4>
    <p class="normal">The <a id="_idIndexMarker1526"/>algorithm of MAML in a reinforcement learning setting is given as follows:</p>
    <ol>
      <li class="numbered" value="1">Say, we have a model <em class="italic">f</em> parameterized by a parameter <img src="../Images/B15558_10_066.png" alt="" style="height: 1.11em;"/> and we have a distribution over tasks <em class="italic">p(T)</em>. First, we randomly initialize the model parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>.</li>
      <li class="numbered">Sample a batch of tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> from a distribution of tasks, that is, <em class="italic">T</em><sub class="" style="font-style: italic;">i </sub><em class="italic">~ p(T).</em></li>
      <li class="numbered">For each task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub>:<ol>
          <li class="numbered-l2"> Sample <em class="italic">k</em> trajectories using <img src="../Images/B15558_17_025.png" alt="" style="height: 1.11em;"/> and prepare the training dataset: <img src="../Images/B15558_17_128.png" alt="" style="height: 1.29em;"/>.</li>
          <li class="numbered-l2">Train the model <img src="../Images/B15558_17_129.png" alt="" style="height: 1.11em;"/> on the training dataset <img src="../Images/B15558_17_130.png" alt="" style="height: 1.29em;"/> and compute the loss.</li>
          <li class="numbered-l2">Minimize the loss using gradient descent and get the optimal parameter <img src="../Images/B15558_17_131.png" alt="" style="height: 1.2em;"/> as <img src="../Images/B15558_17_132.png" alt="" style="height: 1.29em;"/>.</li>
          <li class="numbered-l2">Sample <em class="italic">k</em> trajectories using <img src="../Images/B15558_17_133.png" alt="" style="height: 1.29em;"/> and prepare the test dataset: <img src="../Images/B15558_17_115.png" alt="" style="height: 1.29em;"/>.</li>
        </ol>
      </li>
      <li class="numbered">Now, we minimize the loss on the test dataset <img src="../Images/B15558_17_135.png" alt="" style="height: 1.29em;"/>. Parameterize the model <em class="italic">f</em> with <a id="_idIndexMarker1527"/>the optimal parameter <img src="../Images/B15558_17_136.png" alt="" style="height: 1.2em;"/> calculated in the previous step and compute the loss <img src="../Images/B15558_17_137.png" alt="" style="height: 1.29em;"/>. Calculate the gradients of the loss and update our randomly initialized parameter <img src="../Images/B15558_17_066.png" alt="" style="height: 1.11em;"/> using our test (meta-training) dataset as: <figure class="mediaobject"><img src="../Images/B15558_17_139.png" alt="" style="height: 2.78em;"/></figure>
      </li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to<em class="italic"> 4</em> for several iterations.</li>
    </ol>
    <p class="normal">That's it! Meta learning is a growing field of research. Now that we have a basic idea of meta learning, you can explore more about meta learning and see how meta learning is used in reinforcement learning. In the next section, we will learn about hierarchical reinforcement learning.</p>
    <h1 id="_idParaDest-458" class="title">Hierarchical reinforcement learning</h1>
    <p class="normal">The <a id="_idIndexMarker1528"/>problem with reinforcement learning is that it cannot scale well with a large number of state spaces and actions, which ultimately leads to the problem called curse of dimensionality. <strong class="keyword">Hierarchical reinforcement learning</strong> (<strong class="keyword">HRL</strong>) is proposed to solve the curse of dimensionality, where we decompose large problems into small subproblems in a hierarchy. Let's suppose the goal of our agent is to reach home from school. Now, our goal is split into a set of subgoals, such as going out of the school gate, booking a cab, and so on.</p>
    <p class="normal">There are different methods used in HRL, such as state-space decomposition, state abstraction, and temporal abstraction. In state-space decomposition, we decompose the state space into different subspaces and try to solve the problem in a smaller subspace. Breaking down the state space also allows faster exploration, as the agent does <a id="_idIndexMarker1529"/>not want to explore the entire state space. In state abstraction, the agent ignores the variables that are irrelevant to achieving the current subtasks in the current state space. In temporal abstraction, the action sequence and action sets are grouped, which divides the single step into multiple steps.</p>
    <p class="normal">We will now look into one of the most commonly used algorithms in HRL, called MAXQ value function decomposition.</p>
    <h2 id="_idParaDest-459" class="title">MAXQ value function Decomposition</h2>
    <p class="normal">MAXQ <a id="_idIndexMarker1530"/>value function decomposition is one of the most frequently used algorithms in HRL. In this section, let's get a basic idea and overview of how MAXQ value function decomposition works. Let's understand how MAXQ value function decomposition works with an example. Let's take a taxi environment as shown in <em class="italic">Figure 17.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.5: Taxi environment</p>
    <p class="normal">Let's suppose our agent is driving a taxi. As <em class="italic">Figure 17.5</em> shows, the tiny, yellow-colored rectangle is the taxi driven by our agent. The letters (<strong class="keyword">R</strong>, <strong class="keyword">G</strong>, <strong class="keyword">Y</strong>, <strong class="keyword">B</strong>) represent the different locations. Thus we have four locations in total, and the agent has to pick up a passenger at one location and drop them off at another location. The agent will receive +20 points as a reward for a successful drop-off and -1 point for every time step it takes. The agent will also lose -10 points for illegal pickups and drop-offs.</p>
    <p class="normal">So the goal of our agent is to learn to pick up and drop off passengers at the correct location in a short time without adding illegal passengers.</p>
    <p class="normal">Now we break the goal of our agent into four subtasks as follows:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Navigate</strong>: In the Navigate subtask, the goal of our agent is to drive the taxi from the current location to one of the target locations. The Navigate(t) subtask will use the four primitive actions: <em class="italic">north</em>, <em class="italic">south</em>, <em class="italic">east</em>, and <em class="italic">west</em>.</li>
      <li class="bullet"><strong class="keyword">Get</strong>:<strong class="keyword"> </strong>In the Get subtask, the goal of our agent is to drive the taxi from its current location to the passenger's location and pick up the passenger.</li>
      <li class="bullet"><strong class="keyword">Put</strong>:<strong class="keyword"> </strong>In the Put subtask, the goal of our agent is to drive the taxi from its current location to the passenger's destination and drop off the passenger.</li>
      <li class="bullet"><strong class="keyword">Root</strong>:<strong class="keyword"> </strong>Root is the whole task.</li>
    </ul>
    <p class="normal">We <a id="_idIndexMarker1531"/>can represent all these subtasks in a directed acyclic graph called a task graph, as <em class="italic">Figure 17.6 </em>shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.6: Task graph</p>
    <p class="normal">As we can observe from the preceding figure, all the subtasks are arranged hierarchically. Each node represents a subtask or primitive action and each edge connects in such a way that a subtask can call its child subtask. As shown, the <strong class="keyword">Navigate(t)</strong> subtask has four primitive actions: <strong class="keyword">East</strong>, <strong class="keyword">West</strong>, <strong class="keyword">North</strong>, and <strong class="keyword">South</strong>. The <strong class="keyword">Get</strong> subtask has a <strong class="keyword">Pickup</strong> primitive action and a <strong class="keyword">Navigate(t)</strong> subtask. Similarly, the <strong class="keyword">Put</strong> subtask has a <strong class="keyword">Putdown</strong> (drop) primitive action and <strong class="keyword">Navigate(t)</strong> subtask.</p>
    <p class="normal">In MAXQ value function decomposition, we decompose the value function into a set of value functions for each of the subtasks. For the efficient designing and debugging of MAXQ <a id="_idIndexMarker1532"/>decompositions, we can redesign our task graphs as <em class="italic">Figure 17.7 </em>shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.7: Task graph</p>
    <p class="normal">As we can observe from <em class="italic">Figure 17.7</em>, our redesigned graph contains two special types of nodes: max nodes and Q nodes. The max nodes define the subtasks in the task decomposition and the Q nodes define the actions that are available for each subtask.</p>
    <p class="normal">Thus, in this section, we got a basic idea of MaxQ value function decomposition. In the next <a id="_idIndexMarker1533"/>section, we will learn about I2A.</p>
    <h1 id="_idParaDest-460" class="title">Imagination augmented agents</h1>
    <p class="normal">Are you a fan of chess? If I asked you to play chess, how would you play it? Before moving any <a id="_idIndexMarker1534"/>chess piece on the chessboard, you might imagine the consequences of moving a chess piece and move the chess piece that you think would help you to win the game. So, basically, before taking any action, we imagine the consequence and, if it is favorable, we proceed with that action, else we refrain from performing that action.</p>
    <p class="normal">Similarly, <strong class="keyword">Imagination Augmented Agents</strong> (<strong class="keyword">I2As</strong>) are augmented with imagination. Before taking any action in an environment, the agent imagines the consequences of taking the action and if they think the action will provide a good reward, they will perform the action. The I2A takes advantage of both model-based and model-free learning. <em class="italic">Figure 17.8</em> shows the architecture of I2As:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.8: I2A architecture</p>
    <p class="normal">As we <a id="_idIndexMarker1535"/>can observe from <em class="italic">Figure 17.8</em>, I2A architecture has both model-based and model-free paths. Thus, the action the agent takes is the result of both the model-based and model-free paths. In the model-based path, we have rollout encoders. </p>
    <p class="normal">These rollout encoders are where the agent performs imagination tasks, so let's take a closer look at the rollout encoders. <em class="italic">Figure 17.9</em> shows a single rollout encoder:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.9: Single imagination rollout</p>
    <p class="normal">From <em class="italic">Figure 17.9</em>, we can observe that the rollout encoders have two layers: the imagine <a id="_idIndexMarker1536"/>future layer and the encoder layer. The imagine future layer is where the imagination happens. The imagine future layer consists of the imagination core.</p>
    <p class="normal">When we feed the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> to the imagination core, we get the next state <img src="../Images/B15558_17_140.png" alt="" style="height: 1.11em;"/> and the reward <img src="../Images/B15558_17_141.png" alt="" style="height: 1.11em;"/>, and when we feed this next state <img src="../Images/B15558_17_140.png" alt="" style="height: 1.11em;"/> to the next imagination core, we get the next state <img src="../Images/B15558_17_143.png" alt="" style="height: 1.11em;"/> and reward <img src="../Images/B15558_17_144.png" alt="" style="height: 1.11em;"/>. If we repeat these for <em class="italic">n</em> steps, we get a rollout, which is <a id="_idIndexMarker1537"/>basically a pair of states and rewards, and then we use encoders such as <strong class="keyword">Long Short-Term Memory</strong> (<strong class="keyword">LSTM</strong>) to encode this rollout. As a result, we get rollout encodings. These rollout encodings are actually the embeddings describing the future imagined path. We will have multiple rollout encoders for different future imagined paths, and we use an aggregator to aggregate this rollout encoder.</p>
    <p class="normal">Okay, but <a id="_idIndexMarker1538"/>how exactly does the imagination happen in the imagination core? What is actually in the imagination core? <em class="italic">Figure 17.10</em> shows a single imagination core:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.10: The imagination core</p>
    <p class="normal">As we can observe from <em class="italic">Figure 17.10</em>, the imagination core consists of a policy network and an environment model. The environment model learns from all the actions that the agent has performed so far. It takes information about the state <img src="../Images/B15558_17_145.png" alt="" style="height: 1.11em;"/>, imagines all the possible futures considering the experience, and chooses the action <img src="../Images/B15558_17_146.png" alt="" style="height: 1.11em;"/> that gives a high reward.</p>
    <p class="normal"><em class="italic">Figure 17.11</em> shows <a id="_idIndexMarker1539"/>the complete architecture of I2As with all components expanded:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.11: Full I2A architecture</p>
    <p class="normal">Have you played Sokoban before? Sokoban is a classic puzzle game where the player has to push boxes to a target location. The rules of the game are very simple: boxes can only be pushed and cannot be pulled. If we push a box in the wrong direction then the puzzle becomes unsolvable:</p>
    <figure class="mediaobject"><img src="../Images/B15558_17_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 17.12: Sokoban environment</p>
    <p class="normal">The I2A <a id="_idIndexMarker1540"/>architecture provides good results in these kinds of environments, where the agent has to plan in advance before taking an action. The authors of the paper tested I2A performance on Sokoban and achieved great results.</p>
    <p class="normal">There are various exciting research advancements happening around deep reinforcement learning. Now that you have finished reading the book, you can start exploring such advancements and experiment with various projects. Learn and reinforce!</p>
    <h1 id="_idParaDest-461" class="title">Summary</h1>
    <p class="normal">We started this chapter by understanding what meta learning is. We learned that with meta learning, we train our model on various related tasks with a few data points, such that for a new related task, our model can make use of the learning obtained from the previous tasks.</p>
    <p class="normal">Next, we learned about a popular meta-learning algorithm called MAML. In MAML, we sample a batch of tasks and for each task <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> in the batch, we minimize the loss using gradient descent and get the optimal parameter <img src="../Images/B15558_17_053.png" alt="" style="height: 1.2em;"/>. Then, we update our randomly initialized model parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> by calculating the gradients for each of the new tasks <em class="italic">T</em><sub class="" style="font-style: italic;">i</sub> with the model parameterized as <img src="../Images/B15558_17_041.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">Moving on, we learned about HRL, where we decompose large problems into small subproblems in a hierarchy. We also looked into the different methods used in HRL, such as state-space decomposition, state abstraction, and temporal abstraction. Next, we got an overview of MAXQ value function decomposition, where we decompose the value function into a set of value functions for each of the subtasks.</p>
    <p class="normal">At the end of the chapter, we learned about I2As, which are augmented with imagination. Before taking any action in an environment, the agent imagines the consequences of taking the action, and if they think the action will provide a good reward, they will perform the action.</p>
    <p class="normal">Deep reinforcement learning is evolving every day with interesting advancements. Now that you have learned about the various state-of-the-art deep reinforcement learning algorithms, you can start building interesting projects and also contribute to deep reinforcement learning research.</p>
    <h1 id="_idParaDest-462" class="title">Questions</h1>
    <p class="normal">Let's test the knowledge you gained in this chapter; try answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">Why do we need meta learning?</li>
      <li class="numbered">What is MAML?</li>
      <li class="numbered">What is the meta objective?</li>
      <li class="numbered">What is the meta training set?</li>
      <li class="numbered">Define HRL.</li>
      <li class="numbered">How does an I2A work?</li>
    </ol>
    <h1 id="_idParaDest-463" class="title">Further reading</h1>
    <p class="normal">For more information, we can refer to the following papers:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</strong> by <em class="italic">Chelsea Finn</em>, <em class="italic">Pieter Abbeel, Sergey Levine</em>, <a href="https://arxiv.org/pdf/1703.03400.pdf"><span class="url">https://arxiv.org/pdf/1703.03400.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition</strong> by <em class="italic">Thomas G. Dietterich</em>, <a href="https://arxiv.org/pdf/cs/9905014.pdf"><span class="url">https://arxiv.org/pdf/cs/9905014.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Imagination-Augmented Agents for Deep Reinforcement Learning</strong> by <em class="italic">Théophane Weber</em>, <em class="italic">et al</em>.,<em class="italic"> </em><a href="https://arxiv.org/pdf/1707.06203.pdf"><span class="url">https://arxiv.org/pdf/1707.06203.pdf</span></a></li>
    </ul>
  </div>
</body></html>