- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time Series, Sequences, and Prediction with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to the final chapter in our journey with TensorFlow. In the last chapter,
    we closed on a high by applying neural networks such as DNNs to forecast time
    series data effectively. In this chapter, we will be exploring an array of advanced
    ideas, such as integrating learning rate schedulers into our workflow to dynamically
    adapt our learning rate and accelerate the process of model training. In previous
    chapters, we emphasized the need for and importance of finding the optimal learning
    rate. When building models with learning rate schedulers, we can achieve this
    in a dynamic way either using inbuilt learning rate schedulers in TensorFlow or
    by crafting our own custom-made learning rate scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss Lambda layers and how these arbitrary layers can be applied
    in our model architecture to enhance quick experimentation, enabling us to embed
    custom functions seamlessly into our model’s architecture, especially when working
    with LSTMs and RNNs. We will switch over from building time series models with
    DNNs to more complex architectures such as CNNs, RNNs, LSTMs, and CNN-LSTM networks.
    We will apply these networks to our sales dataset case study. To conclude this
    chapter, we will extract Apple stock closing price data from Yahoo Finance and
    apply these models to build a forecasting model to predict future stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will be covering the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and applying learning rate schedulers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing Lambda layers in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Employing RNNs, LSTMs, and CNNs for time series forecasting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apple stock price prediction using neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a deeper understanding of time series
    forecasting with TensorFlow, along with hands-on experience in applying different
    techniques in building time series forecasting models for real-world projects.
    Let’s get started.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding and applying learning rate schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291), *Introduction to Time
    Series, Sequences,* and Predictions. we built a DNN that achieved a `LearningRateScheduler`
    callback from TensorFlow, we can dynamically adjust the learning rate during training
    using some built-in techniques. Let us examine some built-in learning rate schedulers:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ExponentialDecay`: This starts with a specified learning rate and decreases
    exponentially after a certain number of steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PiecewiseConstantDecay`: This provides a piecewise constant learning rate,
    where you specify boundaries and learning rates to divide the training process
    into several stages with different learning rates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PolynomialDecay`: This learning rate is a function of the iteration number
    in this schedule. It starts with the initial learning rate and decreases it to
    the end learning rate as per the polynomial function specified.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s add a learning rate scheduler to the feedforward network we used in [*Chapter
    12*](B18118_12.xhtml#_idTextAnchor291)*, Introduction to Time Series, Sequences,
    and Predictions*. We are using the same sales data, but this time we will be applying
    different learning rate schedulers to improve our model’s performance. Let’s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin by importing the libraries for this project:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let us load our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We load the sales data from the GitHub repo for this book and put the CSV data
    into a DataFrame.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we convert the `Date` column into datetime and make it the index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first line of code converts the date column into a datetime format. We do
    this to easily perform time series operations. Next, we change the date column
    by setting it as the index of our DataFrame. This makes it easier to slice and
    dice our data using dates.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s extract the sales values from the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are extracting the sales values from our sales DataFrame and converting
    them into a NumPy array. We will use this NumPy array to create our sliding window
    data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll create a sliding window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Just as we did in [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291), *Introduction
    to Time Series, Sequences, and Predictions,* we are using the sliding window technique
    to convert our time series into a supervised learning problem made up of features
    and labels. Here, the window size of 20 serves as our `X` feature, which contains
    20 consecutive sales values, and our `y` is the next immediate value after those
    first 20 sales values. Here, we use the first 20 values to predict the next value.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s split our data into training and validation sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We convert our data into NumPy array and split our data into training and validation
    sets. We use 80 percent of our data for training and 20 percent of our data for
    the validation set that we will use to evaluate our model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Our next goal is to build out a TensorFlow dataset, which is a more efficient
    format for training models in TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We apply the `from_tensor_slices()` method to make a dataset from the NumPy
    arrays. After this, we use the `cache()` method to speed up training by caching
    our dataset in memory. We apply the `shuffle(buffer_size)` method to randomly
    shuffle our training data to prevent issues such as sequential bias. Then we use
    the `batch(batch_size)` method to split our data into batches of a specified size;
    in this case, batches of 128 are fed into our model during training. Next, we
    use the `prefetch` method to ensure our GPU/CPU will always have data ready for
    processing, reducing the waiting time between the processing of one batch and
    the next. We pass in the `tf.data.experimental.AUTOTUNE` argument to tell TensorFlow
    to automatically determine the optimal number of batches to prefetch. This makes
    our training process smoother and faster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our data is now ready for modeling. Let us explore using this data with in-built
    learning rate schedulers from TensorFlow, after which we will look at how to find
    the optimal learning rate with a custom learning rate scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: In-built learning rate schedulers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will be using the same model as we did in [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291)*,
    Introduction to Time Series, Sequences, and Predictions*. Let’s define our model
    and explore the inbuilt learning rate schedulers:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with the model definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are using three dense layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll use the exponential decay learning rate scheduler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The exponential decay learning rate scheduler sets up a learning rate that decays
    exponentially over time. In this experiment, the initial learning rate is set
    to `0.1`. This learning rate will undergo an exponential decay at a rate of 0.96
    for every 100 steps, as defined by the `decay_steps` parameter. Next, we assign
    our exponential learning rate to our optimizer and compile the model. After this,
    we fit the model for 100 epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll evaluate the performance of the model using the MAE and **mean
    squared error** (**MSE**) and plot the validation forecast against the true values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.1 – True forecast versus the validation forecast using exponential
    decay (zoomed in)](img/B18118_13_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – True forecast versus the validation forecast using exponential
    decay (zoomed in)
  prefs: []
  type: TYPE_NORMAL
- en: When we run the code block, we get an MAE of around 5.31 and an MSE of 43.18,
    and from the zoomed-in plot in *Figure 13**.1*, we see our model is following
    the actual sales validation data closely. However, the result is no better than
    what we achieved in [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291)*, Introduction
    to Time Series, Sequences, and Predictions*. Next, let us experiment with `PiecewiseConstantDecay`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s use the `PiecewiseConstantDecay` learning rate scheduler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `PiecewiseConstantDecay` learning rate scheduler allows us the flexibility
    to define specific learning rates for different periods during training. In our
    case, we specified 30 and 60 steps as our boundaries; this means that for the
    first 30 steps, we apply a learning rate of `0.1`, from 30 to 60, we apply a learning
    rate of `0.01`, and from 61 to the end of training, we apply a learning rate of
    `0.001`. To `PiecewiseConstantDecay`, the number of learning rates should be one
    more than the number of boundaries applied. For example, in our case, we have
    two boundaries (`[30, 60]`) and three learning rates (`[0.1, 0.01, 0.001]`). Once
    we set up the scheduler, we use the same optimizer and compile and fit the model
    as we did with the exponential decay learning rate scheduler. Then, we evaluate
    the performance of the model and generate the following validation plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.2 – True forecast versus the validation forecast using exponential
    decay](img/B18118_13_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – True forecast versus the validation forecast using exponential
    decay
  prefs: []
  type: TYPE_NORMAL
- en: In this experiment, we achieved an MAE of 4.87 and an MSE of 36.97\. This is
    an improved performance. Again, the forecast in *Figure 13**.2* nicely follows
    the true values. Let us zoom in for clarity’s sake.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – Zoomed-in plots for our first two experiments](img/B18118_13_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – Zoomed-in plots for our first two experiments
  prefs: []
  type: TYPE_NORMAL
- en: You can see from *Figure 13**.3*, in which we zoomed into the first 200 days,
    when we applied polynomial decay, the forecasted plot maps better to the true
    values in comparison to when we used exponential decay for our learning rate scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now apply `PolynomialDecay`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this experiment, we set `initial_learning_rate` to `0.1`; this serves as
    our starting learning rate. We set the `decay_steps` parameter to `100`, indicating
    that the learning rate will decay over these 100 steps. Next, we set our `end_learning_rate`
    to `0.01`; this means that by the conclusion of our `decay_steps`, the learning
    rate will have reduced to this value. The `power` parameter controls the exponent
    to which the step decay is raised. In this experiment, we set the `power` value
    to `1.0`, resulting in linear decay.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When we evaluate the model’s performance, we see that we achieved our best result
    so far with an MAE of 4.72 and an MSE of 34.49\. From *Figure 13**.4*, we can
    see that it also closely follows the data, even more accurately than when we used
    `PiecewiseConstantDecay`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.4 – True forecast versus the validation forecast using PolynomialDecay
    (zoomed in)](img/B18118_13_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4 – True forecast versus the validation forecast using PolynomialDecay
    (zoomed in)
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have a good idea of how to apply these learning rate schedulers,
    it is a good idea to tweak the values and see whether you can achieve a much lower
    MAE and MSE. When you are done, let’s look at a custom learning rate scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: By simply tweaking the learning rate, we can see our `PiecewiseConstantDecay`
    learning rate scheduler won this battle, not only with the other learning rates
    but also, it outperforms the simple DNN model with the same architecture that
    we used in [*Chapter 12*](B18118_12.xhtml#_idTextAnchor291)*, Introduction to
    Time Series, Sequences, and Predictions*. You can read more about the learning
    rate scheduler from the document [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler)
    or using this excellent Medium article [https://rmoklesur.medium.com/learning-rate-scheduler-in-keras-cc83d2f022a6](https://rmoklesur.medium.com/learning-rate-scheduler-in-keras-cc83d2f022a6)
    by Moklesur Rahman.
  prefs: []
  type: TYPE_NORMAL
- en: Custom learning rate scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Beyond using built-in learning rate schedulers, TensorFlow provides us with
    an easy way to build a custom learning rate scheduler to help us find the optimal
    learning rate. Let’s do this next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by defining the custom learning rate scheduler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we start with a small learning rate (1×10−7) and we increase this learning
    rate exponentially with each passing epoch. We use `10**(epoch / 10)` to determine
    the rate at which the learning rate increases.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We define the optimizer with the initial learning rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we used an SGD with a starting learning rate of 1×10−7 and a momentum
    of `0.9`. Momentum helps accelerate the optimizer in the right direction and also
    dampens oscillations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we compile the model with the defined optimizer and set our loss as MSE:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We train the model for 200 epochs and then pass the learning rate scheduler
    as a callback. This way, the learning rate is adjusted based on the customization
    when defining our custom learning rate scheduler. We also set `verbose=0` so we
    don’t print the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Calculate the learning rates for each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use this code to calculate the learning rate per epoch and it gives us an
    array of learning rates.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We plot the model loss against the learning rate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This plot is an effective way of selecting the optimal learning rate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.5 – The learning rate loss curve](img/B18118_13_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5 – The learning rate loss curve
  prefs: []
  type: TYPE_NORMAL
- en: To find the optimal learning rate, we are on the lookout for where the loss
    is decreasing most rapidly before it begins to increase again. From the plot,
    we can see the learning rate falls and settles at around 3x10-5, after which it
    begins to rise again. So, we will pick this value as our ideal learning rate for
    this experiment. Now we will retrain our model using this new learning rate as
    our fixed learning rate. When we run the code, we get an MAE of 5.96 and an MSE
    of 55.08.
  prefs: []
  type: TYPE_NORMAL
- en: We have now seen how to use both in-built learning rate schedulers and custom
    schedulers. Let us now switch our attention to using CNNs for time series forecasting.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs for time series forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs have recorded remarkable success in image classification tasks due to their
    ability to detect localized patterns within grid-like data structures. This idea
    can also be applied to time series forecasting. By viewing a time series as a
    sequence of temporal intervals, CNNs can extract and recognize patterns that are
    predictive of future trends. Another important strength of CNNs is their translation-invariant
    nature. This means once they learn a pattern in one segment, the network is well
    equipped to recognize it everywhere else it occurs within the series. This comes
    in handy in detecting reoccurring patterns across time steps.
  prefs: []
  type: TYPE_NORMAL
- en: The setup of a CNN also helps to automatically reduce the dimensionality of
    our input data with the aid of the pooling layers. Hence, the convolution and
    pooling operations in a CNN transform the input series into a streamlined form
    that captures the core features while ensuring computational efficiency. Unlike
    with images, here we use a 1D convolutional filter because of the nature of time
    series data (singular dimension). This filter slides across the time dimension,
    observing localized windows of values as input. It detects informative patterns
    within these intervals through repeated element-wise multiplications and summations
    between its weights and the input windows.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple filters are learned to extract diverse predictive signals – trends,
    seasonal fluctuations, cycles, and more. Similar to patterns within images, CNNs
    can recognize translated versions of these temporal motifs throughout the series.
    When we apply successive convolutional and pooling layers, the network composes
    these low-level features into higher-level representations, progressively condensing
    the series into its most salient components. Fully connected layers ultimately
    use these learned features to make forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: Let us return to our notebooks and apply a 1D CNN in modeling our sales data.
    We already have our training and test data. Now, to model our data with a CNN,
    we need to carry out an extra step, which involves reshaping our data to meet
    the expected input shape a CNN expects. In [*Chapter 7*](B18118_07.xhtml#_idTextAnchor146)*,*
    *Image Classification with Convolutional Neural Networks*, we saw how CNNs required
    3D data in comparison to the 2D data we use when modeling with a DNN; the same
    applies here.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN requires a batch size, a window size, and the number of features. The
    batch size is the first dimension of our input shape; it refers to the number
    of sequences we feed into the CNN. We set the window size value to `20`, and the
    number of features here refers to the number of distinct features at each time
    step. For a univariate time series, this value will be `1`; for a multivariate
    time series, the value will be `2` or more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are dealing with a univariate time series in our case study, our input
    shape needs to look like (`128,` `20, 1`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s prepare our data for modeling with a CNN with the right shape:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Most of the code in this code block is the same. The key step here is the `reshape`
    step, which we use to achieve the input shape required for CNN modeling.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s build our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In our model, we apply 1D convolutional layers due to the single-dimensional
    nature of time series data, unlike the 2D CNNs we employed for image classification,
    due to the 2D structure of images. Here, our model is made up of two 1D convolutional
    layers, each followed by a max pooling layer. In our first convolutional layer,
    we have 64 filters to learn various data patterns with a filter size of `3`, which
    allows it to recognize patterns spanning three time steps. We use a stride of
    `1`; this means that our filter traverses the data one step at a time, and to
    ensure nonlinearity, we use ReLU as our activation function. Notice that we are
    using a new type of padding, called causal padding. This choice is strategic as
    causal padding ensures that the model’s output for a particular time step is influenced
    only by that time step and its predecessors, never by future data. By adding padding
    to the start of the sequence, causal padding respects the natural temporal sequence
    of our data. This is essential to prevent our model from inadvertently “looking
    ahead,” ensuring forecasts rely solely on past and current information.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We earlier outlined that we need 3D input-shaped data to be fed into our CNN
    model made up of the batch size, window size, and number of features. Here, we
    used `input_shape=(window_size, 1)`. We did not state the batch size in the input
    shape definition. This means the model can take batches of different sizes since
    we did not hardcode any batch size. Also, we only have one feature since we are
    dealing with a univariate time series, and that’s why we have specified `1` in
    the input shape along with the window size. The max pooling layer reduces the
    dimensionality of our data. Next, we reach the second convolutional layer; this
    time we use 32 filters, again with a kernel size of `3`, causal padding, and ReLU
    as the activation function. Next, the max pooling layer samples the data again.
    After this, the data is flattened and fed into the fully connected layers to make
    predictions based on the patterns learned from our sales data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s compile and fit the model for 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s evaluate the performance of our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We evaluate the model on the validation set by generating the MAE and MSE. When
    we run the code, we achieve an MAE of 5.37 and an MSE of 44.28\. Here, you have
    the opportunity to see whether you can achieve a much lower MAE by tweaking the
    number of filters, sizes of filters, and so on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, let us see how we can use the RNN family in forecasting time series data.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs in time series forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Time series forecasting poses a unique challenge in the world of machine learning,
    involving the prediction of future values based on previously observed sequential
    data. An intuitive way of thinking about this is to consider a sequence of past
    data points. The question then becomes, given this sequence, how can we predict
    the next data point or sequence of data points? This is where RNNs demonstrate
    their efficacy. RNNs are a specific type of neural network developed to process
    sequential data. They maintain an internal state or “memory” that holds information
    about the elements of the sequence observed thus far. This internal state is updated
    at each step of the sequence, amalgamating information from the new input and
    the previous state. As an example, while predicting sales, an RNN may retain data
    regarding the sales trends from the previous months, the overall trend across
    the past year, and the seasonality effects, among others.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, standard RNNs exhibit a significant limitation: the problem of “vanishing
    gradients.” This problem results in difficulty in maintaining and utilizing information
    from earlier steps in the sequence, especially as the sequence length increases.
    To overcome this hurdle, the deep learning community introduced advanced architectures.
    LSTMs and GRUs are specialized types of RNNs designed explicitly to counteract
    the vanishing gradient issue. These types of RNNs are capable of learning long-term
    dependencies due to their in-built gating mechanisms, which control the flow of
    information in and out of the memory state.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, RNNs, LSTMs, and GRUs can be potent tools for time series forecasting
    because they inherently incorporate the temporal dynamics of the problem. For
    instance, while predicting sales, these models can factor in seasonal patterns,
    holidays, weekends, and more by maintaining information about previous sales periods,
    which could lead to more accurate forecasts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s put a simple RNN into action here and see how it will perform on our
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start by preparing our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, you will observe we are preparing our data in the same way as we did when
    using the DNN and we do not reshape it as we just did with our CNN model. There
    is a simple trick ahead that will help you do this in our model architecture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s define our model architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When modeling with RNNs, just like we saw with CNNs, we need to reshape our
    data as our model expects 3D-shaped input data as well. However, in scenarios
    where you would like to keep your original input shape intact for various experiments
    with different models, we can resort to a simple yet effective solution – the
    Lambda layer. This layer is a powerful tool in our toolbox that lets us perform
    simple, arbitrary functions on the input data, making it an excellent instrument
    for quick experimentation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With Lambda layers, we can execute element-wise mathematical operations, such
    as normalization, linear scaling, and simple arithmetic operations. For instance,
    in our case, we utilize a Lambda layer to expand the dimension of our 2D input
    data to fit the 3D input requirement (`batch_size`, `time_steps`, and `features`)
    of RNNs. In TensorFlow, you can leverage the Keras API’s `tf.keras.layers.Lambda`
    to create a Lambda layer. A Lambda layer serves as an adapter, allowing us to
    make minor tweaks to the data, ensuring it’s in the right format for our model,
    while keeping the original data intact for other uses. Next, we come across two
    simple RNN layers of 40 units each. It is important to note that in the first
    RNN, we included `return_sequence =True`. We use this in RNNs and LSTMs when the
    output of one RNN or LSTM layer is fed into another RNN or LSTM layer. We set
    this parameter to ensure the first RNN layer will return an output for each input
    in the sequence. The output is then fed into the second RNN layer; this layer
    will only return the output of the final step, which is then fed into the dense
    layers, which outputs the predicted value for each sequence. Then, we come across
    another Lambda layer that multiplies the output by 100\. We use this to expand
    the output values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s compile and fit our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we achieve an MAE of 4.84 and an MSE of 35.65 on the validation set. This
    result is slightly worse than the result we achieved when we used the `PolynomialDecay`
    learning rate scheduler. Perhaps here, you have an opportunity to try our different
    learning rate schedulers to achieve a lower MAE.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let us explore LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs in time series forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the NLP sections, we discussed the capabilities of LSTMs and their improvements
    over RNNs by mitigating issues such as the vanishing gradient problem, enabling
    the model to learn longer sequences. In the context of time series forecasting,
    LSTM networks can be quite powerful. Let’s see how we can apply LSTMs to our sales
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s begin by preparing our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that we use the `reshape` step as we do not use the Lambda layers, to
    avoid repeating this code block. Note that we will be using it for this experiment
    and the next experiment using the CNN-LSTM architecture.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let’s define our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The architecture we use here is an RNN structure using LSTM cells – the first
    layer is an LSTM layer of 50 neurons, and it has the `return_sequence` parameter
    set to `True` to ensure the output returned is the complete sequence, which is
    fed into the final LSTM layer. Here, we also use an input shape of `[None, 1]`.
    The next layer also has 50 neurons, and it outputs a single value since we did
    not set the `return_sequence` parameter to `True` here and this is fed into the
    dense layer for predictions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The next step is to compile the model. We compile and fit our model as before,
    then we evaluate it. Here we achieved an MAE of 4.56 and an MSE of 32.42, our
    lowest so far.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.6 – True forecast versus the validation forecast using an LSTM
    (zoomed in)](img/B18118_13_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6 – True forecast versus the validation forecast using an LSTM (zoomed
    in)
  prefs: []
  type: TYPE_NORMAL
- en: From the plot, we see that the predicted values and the true values are much
    more in sync than any other experiment we have carried out so far. Let’s see whether
    we can improve this result using a CNN-LSTM architecture for our next experiment.
  prefs: []
  type: TYPE_NORMAL
- en: CNN-LSTM architecture for time series forecasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has offered compelling solutions for time series forecasting,
    and one of the notable architectures in this space is the CNN-LSTM model. This
    model leverages the strengths of CNNs and LSTM networks, providing an effective
    framework for handling the unique characteristics of time series data. CNNs are
    renowned for their performance in image processing tasks due to their ability
    to learn spatial patterns in images, while in sequential data, they can learn
    local patterns. The convolutional layers within the network apply a series of
    filters to the data, learning and extracting significant local and global temporal
    patterns and trends. These features act as a compressed representation of the
    original data, retaining essential information while reducing dimensionality.
    The reduction in dimensionality leads to a more efficient representation that
    captures relevant patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once significant features have been extracted through the convolutional layers,
    these features become inputs to the LSTM layer(s) of the network. CNN-LSTM models
    have an advantage in their capacity for end-to-end learning. In this architecture,
    CNN and LSTM have complementary roles. The CNN layer captures local patterns and
    the LSTM layers learn temporal relationships from these patterns. This joint optimization
    is central to the performance gains of the CNN-LSTM model in comparison to independent
    architectures. Let us see how to apply this architecture to our sales data. Here
    we are going straight to the model architecture as we have already looked at the
    data preparation steps several times before:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build our model using convolution layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use our 1D convolutional layer to detect patterns from the sequence of values
    as we did in our CNN forecasting experiment. Remember to set `padding` to `causal`
    to ensure the output size remains the same as the input. We set the input shape
    to `[window_size, 1]`. Here, `window_size` represents the number of time steps
    in each input sample. `1` means we are working with a univariate time series.
    For example, if we set `window_size` to `7`, this will mean we are using a week’s
    worth of data for forecasting.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, our data reaches the LSTM layers, which are made up of 2 LSTM layers,
    each made up of 64 neurons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we have the dense layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The LSTM layer adds temporal context to the features extracted by the CNN layers,
    and the dense layers generate the final predictions. Here, we use three fully
    connected layers and output the final forecasted values. With this architecture,
    we achieve an MAE of 4.98 and an MSE of 40.71\. Not bad, but worse than the LSTM
    standalone model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Tuning the hyperparameters to optimize our model’s performance is a good idea
    here. By adjusting parameters such as the learning rate, batch size, or optimizers,
    we may be able to improve the model’s capabilities. We will not be going into
    this here as you are already well equipped to do this. Let us move on to the Apple
    stock price data and use all we have learned to create a series of experiments
    to forecast the future prices of Apple stocks and see which architecture will
    come out on top.
  prefs: []
  type: TYPE_NORMAL
- en: Forecasting Apple stock price data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have now covered everything we need to know about time series for the TensorFlow
    Developer Certificate exam. Let us round off this chapter and the book with a
    real-world use case on time series. For this exercise, we will be working with
    a real-world dataset (Apple closing day stock price). Let’s see how we can do
    this next. The Jupyter notebook for this exercise can be found here: [https://github.com/PacktPublishing/TensorFlow-Developer-Certificate-Guide](https://github.com/PacktPublishing/TensorFlow-Developer-Certificate-Guide).
    Let’s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we are using a new library called `yfinance`. This lets us access the
    Apple stock data for our case study.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You may want to run `pip install yfinance` to get it working if the import fails.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We start by creating a DataFrame using the `AAPL` ticker symbol, which represents
    Apple (the company) in the stock market. To do this, we use the `yf.Ticker` function
    from the `yfinance` library to access Apple’s historical data from Yahoo Finance.
    We apply the `history` method to our `Ticker` object to access the historical
    market data for Apple. Here, we set the period to `1d`, which means daily data
    should be accessed. We also set the `start` and `end` parameters to define the
    date range we want to access; in this case, we are collecting 10 years of data
    from the first day of January 2013 to the last day of January 2023.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we use `df.head()` to get a snapshot of our DataFrame. We can see the
    dataset is made up of seven columns (`Open`, `High`, `Low`, `Close`, `Volume`,
    `Dividends`, and `Stock Splits`), as shown in the following screenshot.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.7 – A snapshot of the Apple stock data](img/B18118_13_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7 – A snapshot of the Apple stock data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s understand what these columns mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Open` stands for the opening price for the trading day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`High` is the highest price at which stocks were traded during the day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Low` is the lowest price at which stocks were traded during the day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Close` stands for the closing price for the trading day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Volume` signifies the number of shares that changed hands during the course
    of the trading day. This can serve as an indicator of the market strength.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Dividends` represents how the company’s earnings are shared among shareholders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Stock Splits` can be viewed as an act of the corporation that increased the
    number of the company’s outstanding shares by splitting each share.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s plot the daily closing price:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we run the code, we get the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.8 – A plot showing the Apple stock closing price between January
    2013 and January 2023](img/B18118_13_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8 – A plot showing the Apple stock closing price between January 2013
    and January 2023
  prefs: []
  type: TYPE_NORMAL
- en: From the plot in *Figure 13**.8*, we can see that the stock has a positive upward
    trend with occasional dips.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the data into a NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: For this exercise, we will be forecasting the daily stock closing price. Hence,
    we take the closing price column and convert it into a NumPy array. This way,
    we create a univariate time series for our experimentation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prepare the windowed dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE230]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now familiar with this code block, which we use to prepare our data
    for modeling. Next, we will use the same set of architectures to carry out our
    experiment with our Apple stock dataset. The results are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '| **Model** | **MAE** |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | 4.56 |'
  prefs: []
  type: TYPE_TB
- en: '| RNN | 2.24 |'
  prefs: []
  type: TYPE_TB
- en: '| LSTM | 3.02 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN-LSTM | 18.75 |'
  prefs: []
  type: TYPE_TB
- en: Figure 13.9 – A table showing the MAE across various models
  prefs: []
  type: TYPE_NORMAL
- en: From our results. we see that we achieved the best-performing model with our
    RNN architecture, having an MAE of 2.24\. You can now save your best model, which
    can be used to predict future stock values or applied to other forecasting problems.
    You can also tweak the hyperparameters further to see whether you can achieve
    a lower MAE.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This model is an illustration of what is possible with forecasting with neural
    networks. However, we must be aware of its limitations. Please refrain from making
    financial decisions using this model as real-world stock market predictions capture
    complex relationships such as economic indicators, market sentiment, and other
    interdependencies, which our basic model does not take into consideration.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have come to the end of this chapter and the book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs: []
  type: TYPE_NORMAL
- en: In this final chapter, we explored some advanced concepts for working with time
    series forecasting with TensorFlow. We saw both how to use in-built learning rate
    schedulers as well as designing custom-made schedulers tailored to our needs.
    Then, we used more specialized models, such as RNNs, LSTM networks, and the combination
    of CNNs and LSTM. We also saw how we could apply Lambda layers to implement custom
    operations and add flexibility to our network architecture.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude the chapter, we worked on forecasting the Apple stock closing price.
    By the end of this chapter, you should have a good understanding of applying concepts
    such as learning rate schedulers and Lambda layers and effectively building time
    series forecasting models using various architectures in readiness for your exam.
    Good luck!
  prefs: []
  type: TYPE_NORMAL
- en: Note from the author
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It gives me great pleasure to see you move from the very fundamentals of machine
    learning to building various projects using TensorFlow. You have now explored
    building models with different neural network architectures. You now have a solid
    foundation upon which you can, and should, build an incredible career as a certified
    TensorFlow developer. You can only become a top developer by building solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Everything we have covered in this book will serve you well as you finalize
    your preparation for the TensorFlow Developer Certificate exam and beyond. I want
    to congratulate you for not giving up and working through all the concepts and
    projects in this book and the exercises. I would like to encourage you to continue
    learning, experimenting, and keeping tabs on the latest developments in the field
    of machine learning. I wish you success in your exam and your career ahead.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load the Google stock data from Yahoo Finance for 01-01-2015 to 01-01-2020.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create training, forecasting, and plotting functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare the data for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build DNN, CNN, LSTM, and CNN-LSTM models to model the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate the models using MAE and MSE.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shi, X., Chen, Z., Wang, H., Yeung, D. Y., Wong, W. K., & Woo, W. C. (2015).
    *Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting*.
    In Advances in Neural Information Processing Systems (pp. 802–810) [https://papers.nips.cc/paper/2015/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.xhtml](https://papers.nips.cc/paper/2015/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.xhtml%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karim, F., Majumdar, S., Darabi, H., & Harford, S. (2019). *LSTM fully convolutional
    networks for time series classification*. IEEE Access, 7, 1662-1669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Siami-Namini, S., Tavakoli, N., & Siami Namin, A. (2019). *The Performance of
    LSTM and BiLSTM in Forecasting Time Series*. In 2019 IEEE International Conference
    on Big Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow learning rate scheduler: [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Lambda* *layers*: [https://keras.io/api/layers/core_layers/lambda/](https://keras.io/api/layers/core_layers/lambda/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Time series* *forecasting*: [https://www.tensorflow.org/tutorials/structured_data/time_series](https://www.tensorflow.org/tutorials/structured_data/time_series%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*tf.data: Build TensorFlow input* *pipelines*. [https://www.tensorflow.org/guide/data](https://www.tensorflow.org/guide/data%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Windowed datasets for time* *series*: [https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing](https://www.tensorflow.org/tutorials/structured_data/time_series#data_windowing%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
