- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Essentials of NLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Language has been a part of human evolution. The development of language allowed
    better communication between people and tribes. The evolution of written language,
    initially as cave paintings and later as characters, allowed information to be
    distilled, stored, and passed on from generation to generation. Some would even
    say that the hockey stick curve of advancement is because of the ever-accumulating
    cache of stored information. As this stored information trove becomes larger and
    larger, the need for computational methods to process and distill the data becomes
    more acute. In the past decade, a lot of advances were made in the areas of image
    and speech recognition. Advances in **Natural Language Processing** (**NLP**)
    are more recent, though computational methods for NLP have been an area of research
    for decades. Processing textual data requires many different building blocks upon
    which advanced models can be built. Some of these building blocks themselves can
    be quite challenging and advanced. This chapter and the next focus on these building
    blocks and the problems that can be solved with them through simple models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will focus on the basics of pre-processing text and build
    a simple spam detector. Specifically, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The typical text processing workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection and labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text normalization, including case normalization, text tokenization, stemming,
    and lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling datasets that have been text normalized
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Vectorizing text
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Modeling datasets with vectorized text
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by getting to grips with the text processing workflow most NLP models use.
  prefs: []
  type: TYPE_NORMAL
- en: A typical text processing workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand how to process text, it is important to understand the general
    workflow for NLP. The following diagram illustrates the basic steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Typical stages of a text processing workflow'
  prefs: []
  type: TYPE_NORMAL
- en: The first two steps of the process in the preceding diagram involve collecting
    labeled data. A supervised model or even a semi-supervised model needs data to
    operate. The next step is usually normalizing and featurizing the data. Models
    have a hard time processing text data as is. There is a lot of hidden structure
    in a given text that needs to be processed and exposed. These two steps focus
    on that. The last step is building a model with the processed inputs. While NLP
    has some unique models, this chapter will use only a simple deep neural network
    and focus more on the normalization and vectorization/featurization. Often, the
    last three stages operate in a cycle, even though the diagram may give the impression
    of linearity. In industry, additional features require more effort to develop
    and more resources to keep running. Hence, it is important that features add value.
    Taking this approach, we will use a simple model to validate different normalization/vectorization/featurization
    steps. Now, let's look at each of these stages in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection and labeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step of any **Machine Learning** (**ML**) project is to obtain a dataset.
    Fortunately, in the text domain, there is plenty of data to be found. A common
    approach is to use libraries such as `scrapy` or Beautiful Soup to scrape data
    from the web. However, data is usually unlabeled, and as such can't be used in
    supervised models directly. This data is quite useful though. Through the use
    of transfer learning, a language model can be trained using unsupervised or semi-supervised
    methods and can be further used with a small training dataset specific to the
    task at hand. We will cover transfer learning in more depth in *Chapter 3*, *Named
    Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding*, when we look
    at transfer learning using BERT embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: In the labeling step, textual data sourced in the data collection step is labeled
    with the right classes. Let's take some examples. If the task is to build a spam
    classifier for emails, then the previous step would involve collecting lots of
    emails. This labeling step would be to attach a *spam* or *not spam* label to
    each email. Another example could be sentiment detection on tweets. The data collection
    step would involve gathering a number of tweets. This step would label each tweet
    with a label that acts as a ground truth. A more involved example would involve
    collecting news articles, where the labels would be summaries of the articles.
    Yet another example of such a case would be an email auto-reply functionality.
    Like the spam case, a number of emails with their replies would need to be collected.
    The labels in this case would be short pieces of text that would approximate replies.
    If you are working on a specific domain without much public data, you may have
    to do these steps yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Given that text data is generally available (outside of specific domains like
    health), labeling is usually the biggest challenge. It can be quite time consuming
    or resource intensive to label data. There has been a lot of recent focus on using
    semi-supervised approaches to labeling data. We will cover some methods for labeling
    data at scale using semi-supervised methods and the **snorkel** library in *Chapter
    7*, *Multi-modal Networks and Image Captioning with ResNets and Transformer*,
    when we look at weakly supervised learning for classification using Snorkel.
  prefs: []
  type: TYPE_NORMAL
- en: There is a number of commonly used datasets that are available on the web for
    use in training models. Using transfer learning, these generic datasets can be
    used to prime ML models and then you can use a small amount of domain-specific
    data to fine-tune the model. Using these publicly available datasets gives us
    a few advantages. First, all the data collection has been already performed. Second,
    labeling has already been done. Lastly, using such a dataset allows the comparison
    of results with the state of the art; most papers use specific datasets in their
    area of research and publish benchmarks. For example, the **Stanford Question
    Answering Dataset** (or **SQuAD** for short) is often used as a benchmark for
    question-answering models. It is a good source to train on as well.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting labeled data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this book, we will rely on publicly available datasets. The appropriate datasets
    will be called out in their respective chapters along with instructions on downloading
    them. To build a spam detection system on an email dataset, we will be using the
    SMS Spam Collection dataset made available by University of California, Irvine.
    This dataset can be downloaded using instructions available in the tip box below.
    Each SMS is tagged as "SPAM" or "HAM," with the latter indicating it is not a
    spam message.
  prefs: []
  type: TYPE_NORMAL
- en: University of California, Irvine, is a great source of machine learning datasets.
    You can see all the datasets they provide by visiting [http://archive.ics.uci.edu/ml/datasets.php](http://archive.ics.uci.edu/ml/datasets.php).
    Specifically for NLP, you can see some publicly available datasets on [https://github.com/niderhoff/nlp-datasets](https://github.com/niderhoff/nlp-datasets).
  prefs: []
  type: TYPE_NORMAL
- en: Before we start working with the data, the development environment needs to
    be set up. Let's take a quick moment to set up the development environment.
  prefs: []
  type: TYPE_NORMAL
- en: Development environment setup
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using Google Colaboratory, or Colab for short,
    to write code. You can use your Google account, or register a new account. Google
    Colab is free to use, requires no configuration, and also provides access to GPUs.
    The user interface is very similar to a Jupyter notebook, so it should seem familiar.
    To get started, please navigate to [colab.research.google.com](http://colab.research.google.com)
    using a supported web browser. A web page similar to the screenshot below should
    appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Google Colab website'
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to create a new notebook. There are a couple of options. The
    first option is to create a new notebook in Colab and type in the code as you
    go along in the chapter. The second option is to upload a notebook from the local
    drive into Colab. It is also possible to pull in notebooks from GitHub into Colab,
    the process for which is detailed on the Colab website. For the purposes of this
    chapter, a complete notebook named `SMS_Spam_Detection.ipynb` is available in
    the GitHub repository of the book in the `chapter1-nlp-essentials` folder. Please
    upload this notebook into Google Colab by clicking **File | Upload Notebook**.
    Specific sections of this notebook will be referred to at the appropriate points
    in the chapter in tip boxes. The instructions for creating the notebook from scratch
    are in the main description.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **File** menu option at the top left and click on **New Notebook**.
    A new notebook will open in a new browser tab. Click on the notebook name at the
    top left, just above the **File** menu option, and edit it to read `SMS_Spam_Detection`.
    Now the development environment is set up. It is time to begin loading in data.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us edit the first line of the notebook and import TensorFlow 2\.
    Enter the following code in the first cell and execute it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of running this cell should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This confirms that version 2.4.0 of the TensorFlow library was loaded. The highlighted
    line in the preceding code block is a magic command for Google Colab, instructing
    it to use TensorFlow version 2+. The next step is to download the data file and
    unzip to a location in the Colab notebook on the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: The code for loading the data is in the *Download Data* section of the notebook.
    Also note that as of writing, the release version of TensorFlow was 2.4.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be done with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output confirms that the data was downloaded and extracted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Reading the data file is trivial:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The last line of code shows a sample line of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This example is labeled as not spam. The next step is to split each line into
    two columns – one with the text of the message and the other as the label. While
    we are separating these labels, we will also convert the labels to numeric values.
    Since we are interested in predicting spam messages, we can assign a value of
    `1` to the spam messages. A value of `0` will be assigned to legitimate messages.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this part is in the *Pre-Process Data* section of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that the following code is verbose for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now the dataset is ready for further processing in the pipeline. However, let's
    take a short detour to see how to configure GPU access in Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling GPUs on Google Colab
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the advantages of using Google Colab is access to free GPUs for small
    tasks. GPUs make a big difference in the training time of NLP models, especially
    ones that use **Recurrent Neural Networks** (**RNNs**). The first step in enabling
    GPU access is to start a runtime, which can be done by executing a command in
    the notebook. Then, click on the **Runtime** menu option and select the **Change
    Runtime** option, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: Colab runtime settings menu option'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, a dialog box will show up, as shown in the following screenshot. Expand
    the **Hardware Accelerator** option and select **GPU**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: Enabling GPUs on Colab'
  prefs: []
  type: TYPE_NORMAL
- en: Now you should have access to a GPU in your Colab notebook! In NLP models, especially
    when using RNNs, GPUs can shave a lot of minutes or hours off the training time.
  prefs: []
  type: TYPE_NORMAL
- en: For now, let's turn our attention back to the data that has been loaded and
    is ready to be processed further for use in models.
  prefs: []
  type: TYPE_NORMAL
- en: Text normalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text normalization is a pre-processing step aimed at improving the quality of
    the text and making it suitable for machines to process. Four main steps in text normalization
    are case normalization, tokenization and stop word removal, **Parts-of-Speech**
    (**POS**) tagging, and stemming.
  prefs: []
  type: TYPE_NORMAL
- en: Case normalization applies to languages that use uppercase and lowercase letters.
    All languages based on the Latin alphabet or the Cyrillic alphabet (Russian, Mongolian,
    and so on) use upper- and lowercase letters. Other languages that sometimes use
    this are Greek, Armenian, Cherokee, and Coptic. In case normalization, all letters
    are converted to the same case. It is quite helpful in semantic use cases. However,
    in other cases, this may hinder performance. In the spam example, spam messages
    may have more words in all-caps compared to regular messages.
  prefs: []
  type: TYPE_NORMAL
- en: Another common normalization step removes punctuation in the text. Again, this
    may or may not be useful given the problem at hand. In most cases, this should
    give good results. However, in some cases, such as spam or grammar models, it
    may hinder performance. It is more likely for spam messages to use more exclamation
    marks or other punctuation for emphasis.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this part is in the *Data Normalization* section of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s build a baseline model with three simple features:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of characters in the message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of capital letters in the message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of punctuation symbols in the message
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To do so, first, we will convert the data into a `pandas` DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s build some simple functions that can count the length of the message,
    and the numbers of capital letters and punctuation symbols. Python''s regular
    expression package, `re`, will be used to implement these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the `num_capitals()` function, substitutions are performed for the capital
    letters in English. The `count` of these substitutions provides the count of capital
    letters. The same technique is used to count the number of punctuation symbols.
    Please note that the method used to count capital letters is specific to English.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional feature columns will be added to the DataFrame, and then the set
    will be split into test and train sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This should generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: Base dataset for initial spam model'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can be used to split the dataset into training and test
    sets, with 80% of the records in the training set and the rest in the test set.
    Further more, labels will be removed from both the training and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Now we are ready to build a simple classifier to use this data.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling normalized data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that modeling was the last part of the text processing pipeline described
    earlier. In this chapter, we will use a very simple model, as the objective is
    to show different basic NLP data processing techniques more than modeling. Here,
    we want to see if three simple features can aid in the classification of spam.
    As more features are added, passing them through the same model will help in seeing
    if the featurization aids or hampers the accuracy of the classification.
  prefs: []
  type: TYPE_NORMAL
- en: The *Model Building* section of the workbook has the code shown in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'A function is defined that allows the construction of models with different
    numbers of inputs and hidden units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This model uses binary cross-entropy for computing loss and the Adam optimizer
    for training. The key metric, given that this is a binary classification problem,
    is accuracy. The default parameters passed to the function are sufficient as only
    three features are being passed in.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can train our simple baseline model with only three features like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is not bad as our three simple features help us get to 93% accuracy. A
    quick check shows that there are 592 spam messages in the test set, out of a total
    of 4,459\. So, this model is doing better than a very simple model that guesses
    everything as not spam. That model would have an accuracy of 87%. This number
    may be surprising but is fairly common in classification problems where there
    is a severe class imbalance in the data. Evaluating it on the training set gives
    an accuracy of around 93.4%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Please note that the actual performance you see may be slightly different due
    to the data splits and computational vagaries. A quick verification can be performed
    by plotting the confusion matrix to see the performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '|  | Predicted Not Spam | Predicted Spam |'
  prefs: []
  type: TYPE_TB
- en: '| Actual Not Spam | 3,771 | 96 |'
  prefs: []
  type: TYPE_TB
- en: '| Actual Spam | 186 | 406 |'
  prefs: []
  type: TYPE_TB
- en: This shows that 3,771 out of 3,867 regular messages were classified correctly,
    while 406 out of 592 spam messages were classified correctly. Again, you may get
    a slightly different result.
  prefs: []
  type: TYPE_NORMAL
- en: To test the value of the features, try re-running the model by removing one
    of the features, such as punctuation or a number of capital letters, to get a
    sense of their contribution to the model. This is left as an exercise for the
    reader.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This step takes a piece of text and converts it into a list of tokens. If the
    input is a sentence, then separating the words would be an example of tokenization.
    Depending on the model, different granularities can be chosen. At the lowest level,
    each character could become a token. In some cases, entire sentences of paragraphs
    can be considered as a token:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a logo  Description automatically generated](img/B16252_01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: Tokenizing a sentence'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram shows two ways a sentence can be tokenized. One way to
    tokenize is to chop a sentence into words. Another way is to chop into individual
    characters. However, this can be a complex proposition in some languages such
    as Japanese and Mandarin.
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation in Japanese
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many languages use a word separator, a space, to separate words. This makes
    the task of tokenizing on words trivial. However, there are other languages that
    do not use any markers or separators between words. Some examples of such languages
    are Japanese and Chinese. In such languages, the task is referred to as *segmentation*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, in Japanese, there are mainly three different types of characters
    that are used: *Hiragana*, *Kanji*, and *Katakana*. Kanji is adapted from Chinese
    characters, and similar to Chinese, there are thousands of characters. Hiragana
    is used for grammatical elements and native Japanese words. Katakana is mostly
    used for foreign words and names. Depending on the preceding characters, a character
    may be part of an existing word or the start of a new word. This makes Japanese
    one of the most complicated writing systems in the world. Compound words are especially
    hard. Consider the following compound word that reads *Election Administration
    Committee*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_01_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be tokenized in two different ways, outside of the entire phrase being
    considered one word. Here are two examples of tokenizing (from the Sudachi library):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_01_004.png) (Election / Administration / Committee)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B16252_01_005.png) (Election / Administration / Committee / Meeting)'
  prefs: []
  type: TYPE_IMG
- en: Common libraries that are used specifically for Japanese segmentation or tokenization
    are MeCab, Juman, Sudachi, and Kuromoji. MeCab is used in Hugging Face, spaCy,
    and other libraries.
  prefs: []
  type: TYPE_NORMAL
- en: The code shown in this section is in the *Tokenization and Stop Word Removal*
    section of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately, most languages are not as complex as Japanese and use spaces to
    separate words. In Python, splitting by spaces is trivial. Let''s take an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding split operation results in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The two highlighted lines in the preceding output show that the naïve approach
    in Python will result in punctuation being included in the words, among other
    issues. Consequently, this step is done through a library like StanfordNLP. Using
    `pip`, let''s install this package in our Colab notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The StanfordNLP package uses PyTorch under the hood as well as a number of
    other packages. These and other dependencies will be installed. By default, the
    package does not install language files. These have to be downloaded. This is
    shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The English file is approximately 235 MB. A prompt will be displayed to confirm
    the download and the location to store it in:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: Prompt for downloading English models'
  prefs: []
  type: TYPE_NORMAL
- en: Google Colab recycles the runtimes upon inactivity. This means that if you perform
    commands in the book at different times, you may have to re-execute every command
    again from the start, including downloading and processing the dataset, downloading
    the StanfordNLP English files, and so on. A local notebook server would usually
    maintain the state of the runtime but may have limited processing power. For simpler
    examples as in this chapter, Google Colab is a decent solution. For the more advanced
    examples later in the book, where training may run for hours or days, a local
    runtime or one running on a cloud **Virtual Machine** (**VM**) would be preferred.
  prefs: []
  type: TYPE_NORMAL
- en: 'This package provides capabilities for tokenization, POS tagging, and lemmatization
    out of the box. To start with tokenization, we instantiate a pipeline and tokenize
    a sample text to see how this works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The `lang` parameter is used to indicate that an English pipeline is desired.
    The second parameter, `processors`, indicates the type of processing that is desired
    in the pipeline. This library can also perform the following processing steps
    in the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pos` labels each token with a POS token. The next section provides more details
    on POS tags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lemma`, which can convert different forms of verbs, for example, to the base
    form. This will be covered in detail in the *Stemming and lemmatization* section
    later in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`depparse` performs dependency parsing between words in a sentence. Consider
    the following example sentence, "Hari went to school." *Hari* is interpreted as
    a noun by the POS tagger, and becomes the governor of the word *went*. The word
    *school* is dependent on *went* as it describes the object of the verb.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For now, only tokenization of text is desired, so only the tokenizer is used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows that the tokenizer correctly divided the text into two sentences.
    To investigate what words were removed, the following code can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note the highlighted words in the preceding output. Punctuation marks were separated
    out into their own words. Text was split into multiple sentences. This is an improvement
    over only using spaces to split. In some applications, removal of punctuation
    may be required. This will be covered in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the preceding example of Japanese. To see the performance of StanfordNLP
    on Japanese tokenization, the following piece of code can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the first step, which involves downloading the Japanese language model,
    similar to the English model that was downloaded and installed previously. Next,
    a Japanese pipeline will be instantiated and the words will be processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You may recall that the Japanese text reads Election Administration Committee.
    Correct tokenization should produce three words, where first two should be two
    characters each, and the last word is three characters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This matches the expected output. StanfordNLP supports 53 languages, so the
    same code can be used for tokenizing any language that is supported.
  prefs: []
  type: TYPE_NORMAL
- en: Coming back to the spam detection example, a new feature can be implemented
    that counts the number of words in the message using this tokenization functionality.
  prefs: []
  type: TYPE_NORMAL
- en: This word count feature is implemented in the *Adding Word Count Feature* section
    of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible that spam messages have different numbers of words than regular
    messages. The first step is to define a method to compute the number of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, using the train and test splits, add a column for the word count feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The last line in the preceding code block creates a new model with four input
    features.
  prefs: []
  type: TYPE_NORMAL
- en: '**PyTorch warning**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you execute functions in the StanfordNLP library, you may see a warning
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Internally, StanfordNLP uses the PyTorch library. This warning is due to StanfordNLP
    using an older version of a function that is now deprecated. For all intents and
    purposes, this warning can be ignored. It is expected that maintainers of StanfordNLP
    will update their code.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling tokenized data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This model can be trained like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'There is only a marginal improvement in accuracy. One hypothesis is that the
    number of words is not useful. It would be useful if the average number of words
    in spam messages were smaller or larger than regular messages. Using pandas, this
    can be quickly verified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: Statistics for spam message features'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compare the preceding results to the statistics for regular messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: Statistics for regular message features'
  prefs: []
  type: TYPE_NORMAL
- en: Some interesting patterns can quickly be seen. Spam messages usually have much
    less deviation from the mean. Focus on the **Capitals** feature column. It shows
    that regular messages use far fewer capitals than spam messages. At the 75^(th)
    percentile, there are 3 capitals in a regular message versus 21 for spam messages.
    On average, regular messages have 4 capital letters while spam messages have 15\.
    This variation is much less pronounced in the number of words category. Regular
    messages have 17 words on average, while spam has 29\. At the 75^(th) percentile,
    regular messages have 22 words while spam messages have 35\. This quick check
    yields an indication as to why adding the word features wasn't that useful. However,
    there are a couple of things to consider still. First, the tokenization model
    split out punctuation marks as words. Ideally, these words should be removed from
    the word counts as the punctuation feature is showing that spam messages use a
    lot more punctuation characters. This will be covered in the *Parts-of-speech
    tagging* section. Secondly, languages have some common words that are usually
    excluded. This is called stop word removal and is the focus of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Stop word removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stop word removal involves removing common words such as articles (the, an)
    and conjunctions (and, but), among others. In the context of information retrieval
    or search, these words would not be helpful in identifying documents or web pages
    that would match the query. As an example, consider the query "Where is Google
    based?". In this query, *is* is a stop word. The query would produce similar results
    irrespective of the inclusion of *is*. To determine the stop words, a simple approach
    is to use grammar clues.
  prefs: []
  type: TYPE_NORMAL
- en: In English, articles and conjunctions are examples of classes of words that
    can usually be removed. A more robust way is to consider the frequency of occurrence
    of words in a corpus, set of documents, or text. The most frequent terms can be
    selected as candidates for the stop word list. It is recommended that this list
    be reviewed manually. There can be cases where words may be frequent in a collection
    of documents but are still meaningful. This can happen if all the documents in
    the collection are from a specific domain or on a specific topic. Consider a set
    of documents from the Federal Reserve. The word *economy* may appear quite frequently
    in this case; however, it is unlikely to be a candidate for removal as a stop
    word.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, stop words may actually contain information. This may be applicable
    to phrases. Consider the fragment "flights to Paris." In this case, *to* provides
    valuable information, and its removal may change the meaning of the fragment.
  prefs: []
  type: TYPE_NORMAL
- en: Recall the stages of the text processing workflow. The step after text normalization
    is vectorization. This step is discussed in detail later in the *Vectorizing text*
    section of this chapter, but the key step in vectorization is to build a vocabulary
    or dictionary of all the tokens. The size of this vocabulary can be reduced by
    removing stop words. While training and evaluating models, removing stop words
    reduces the number of computation steps that need to be performed. Hence, the
    removal of stop words can yield benefits in terms of computation speed and storage
    space. Modern advances in NLP see smaller and smaller stop words lists as more
    efficient encoding schemes and computation methods evolve. Let's try and see the
    impact of stop words on the spam problem to develop some intuition about its usefulness.
  prefs: []
  type: TYPE_NORMAL
- en: Many NLP packages provide lists of stop words. These can be removed from the
    text after tokenization. Tokenization was done through the StanfordNLP library
    previously. However, this library does not come with a list of stop words. NLTK
    and spaCy supply stop words for a set of languages. For this example, we will
    use an open source package called `stopwordsiso`.
  prefs: []
  type: TYPE_NORMAL
- en: The *Stop Word Removal* section of the notebook contains the code for this section.
  prefs: []
  type: TYPE_NORMAL
- en: This Python package takes the list of stop words from the stopwords-iso GitHub
    project at [https://github.com/stopwords-iso/stopwords-iso](https://github.com/stopwords-iso/stopwords-iso).
    This package provides stop words in 57 languages. The first step is to install
    the Python package that provides access to the stop words lists.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command will install the package through the notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Supported languages can be checked with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'English language stop words can be checked as well to get an idea of some of
    the words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Given that tokenization was already implemented in the preceding `word_counts()`
    method, the implementation of that method can be updated to include removing stop
    words. However, all the stop words are in lowercase. Case normalization was discussed
    earlier, and capital letters were a useful feature for spam detection. In this
    case, tokens need to be converted to lowercase to effectively remove them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'A consequence of using stop words is that a message such as "When are you going
    to ride your bike?" counts as only 3 words. When we see if this has had any effect
    on the statistics for word length, the following picture emerges:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: Word counts for spam messages after removing stop words'
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the word counts prior to stop word removal, the average number of
    words has been reduced from 29 to 18, almost a 30% decrease. The 25^(th) percentile
    changed from 26 to 14\. The maximum has also reduced from 49 to 33\.
  prefs: []
  type: TYPE_NORMAL
- en: 'The impact on regular messages is even more dramatic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.11: Word counts for regular messages after removing stop words'
  prefs: []
  type: TYPE_NORMAL
- en: Comparing these statistics to those from before stop word removal, the average
    number of words has more than halved to almost 8\. The maximum number of words
    has also reduced from 209 to 147\. The standard deviation of regular messages
    is about the same as its mean, indicating that there is a lot of variation in
    the number of words in regular messages. Now, let's see if this helps us train
    a model and improve its accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling data with stop words removed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that the feature without stop words is computed, it can be added to the
    model to see its impact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This accuracy reflects a slight improvement over the previous model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: In NLP, stop word removal used to be standard practice. In more modern applications,
    stop words may actually end up hindering performance in some use cases, rather
    than helping. It is becoming more common not to exclude stop words. Depending
    on the problem you are solving, stop word removal may or may not help.
  prefs: []
  type: TYPE_NORMAL
- en: Note that StanfordNLP will separate words like *can't* into *ca* and *n't*.
    This represents the expansion of the short form into its constituents, *can* and
    *not*. These contractions may or may not appear in the stop word list. Implementing
    a more robust stop word detector is left to the reader as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: StanfordNLP uses a supervised RNN with **Bi-directional Long Short-Term Memory**
    (**BiLSTM**) units. This architecture uses a vocabulary to generate embeddings
    through the vectorization of the vocabulary. The vectorization and generation
    of embeddings is covered later in the chapter, in the *Vectorizing text* section.
    This architecture of BiLSTMs with embeddings is often a common starting point
    in NLP tasks. This will be covered and used in successive chapters in detail.
    This particular architecture for tokenization is considered the state of the art
    as of the time of writing this book. Prior to this, **Hidden Markov Model** (**HMM**)-based
    models were popular.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the languages in question, regular expression-based tokenization
    is also another approach. The NLTK library provides the Penn Treebank tokenizer
    based on regular expressions in a `sed` script. In future chapters, other tokenization
    or segmentation schemes such as **Byte Pair Encoding** (**BPE**) and WordPiece
    will be explained.
  prefs: []
  type: TYPE_NORMAL
- en: The next task in text normalization is to understand the structure of a text
    through POS tagging.
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Languages have a grammatical structure. In most languages, words can be categorized
    primarily into verbs, adverbs, nouns, and adjectives. The objective of this part
    of the processing step is to take a piece of text and tag each word token with
    a POS identifier. Note that this makes sense only in the case of word-level tokens.
    Commonly, the Penn Treebank POS tagger is used by libraries including StanfordNLP
    to tag words. By convention, POS tags are added by using a code after the word,
    separated by a slash. As an example, `NNS` is the tag for a plural noun. If the
    words `goats` was encountered, it would be represented as `goats/NNS`. In the
    StandfordNLP library, **Universal POS** (**UPOS**) tags are used. The following
    tags are part of the UPOS tag set. More details on mapping of standard POS tags
    to UPOS tags can be seen at [https://universaldependencies.org/docs/tagset-conversion/en-penn-uposf.html](https://universaldependencies.org/docs/tagset-conversion/en-penn-uposf.html).
    The following is a table of the most common tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Tag | Class | Examples |'
  prefs: []
  type: TYPE_TB
- en: '| ADJ | **Adjective**: Usually describes a noun. Separate tags are used for
    comparatives and superlatives. | Great, pretty |'
  prefs: []
  type: TYPE_TB
- en: '| ADP | **Adposition**: Used to modify an object such as a noun, pronoun, or
    phrase; for example, "Walk **up** the stairs." Some languages like English use
    prepositions while others such as Hindi and Japanese use postpositions. | Up,
    inside |'
  prefs: []
  type: TYPE_TB
- en: '| ADV | **Adverb**: A word or phrase that modifies or qualifies an adjective,
    verb, or another adverb. | Loudly, often |'
  prefs: []
  type: TYPE_TB
- en: '| AUX | **Auxiliary verb**: Used in forming mood, voice, or tenses of other
    verbs. | Will, can, may |'
  prefs: []
  type: TYPE_TB
- en: '| CCONJ | **Co-ordinating conjunction**: Joins two phrases, clauses, or sentences.
    | And, but, that |'
  prefs: []
  type: TYPE_TB
- en: '| INTJ | **Interjection**: An exclamation, interruption, or sudden remark.
    | Oh, uh, lol |'
  prefs: []
  type: TYPE_TB
- en: '| NOUN | **Noun**: Identifies people, places, or things. | Office, book |'
  prefs: []
  type: TYPE_TB
- en: '| NUM | **Numeral**: Represents a quantity. | Six, nine |'
  prefs: []
  type: TYPE_TB
- en: '| DET | **Determiner**: Identifies a specific noun, usually as a singular.
    | A, an, the |'
  prefs: []
  type: TYPE_TB
- en: '| PART | **Particle**: Parts of speech outside of the main types. | To, n''t
    |'
  prefs: []
  type: TYPE_TB
- en: '| PRON | **Pronoun**: Substitutes for other nouns, especially proper nouns.
    | She, her |'
  prefs: []
  type: TYPE_TB
- en: '| PROPN | **Proper noun**: A name for a specific person, place, or thing. |
    Gandhi, US |'
  prefs: []
  type: TYPE_TB
- en: '| PUNCT | Different punctuation symbols. | , ? / |'
  prefs: []
  type: TYPE_TB
- en: '| SCONJ | **Subordinating conjunction**: Connects independent clause to a dependent
    clause. | Because, while |'
  prefs: []
  type: TYPE_TB
- en: '| SYM | Symbols including currency signs, emojis, and so on. | $, #, % :) |'
  prefs: []
  type: TYPE_TB
- en: '| VERB | **Verb**: Denotes action or occurrence. | Go, do |'
  prefs: []
  type: TYPE_TB
- en: '| X | **Other**: That which cannot be classified elsewhere. | Etc, 4\. (a numbered
    list bullet) |'
  prefs: []
  type: TYPE_TB
- en: 'The best way to understand how POS tagging works is to try it out:'
  prefs: []
  type: TYPE_NORMAL
- en: The code for this section is in the *POS Based Features* section of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code instantiates an English pipeline and processes a sample
    piece of text. The next piece of code is a reusable function to print back the
    sentence tokens with the POS tags:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This method can be used to investigate the tagging for the preceding example
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Most of these tags would make sense, though there may be some inaccuracies.
    For example, the word *lookin* is miscategorized as a noun. Neither StanfordNLP,
    nor a model from another package, will be perfect. This is something that we have
    to account for in building models using such features. There are a couple of different
    features that can be built using these POS. First, we can update the `word_counts()`
    method to exclude the punctuation from the count of words. The current method
    is unaware of the punctuation when it counts the words. Additional features can
    be created that look at the proportion of different types of grammatical elements
    in the messages. Note that so far, all features are based on the structure of
    the text, and not on the content itself. Working with content features will be
    covered in more detail as this book continues.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a next step, let''s update the `word_counts()` method and add a feature
    to show the percentages of symbols and punctuation in a message – with the hypothesis
    that maybe spam messages use more punctuation and symbols. Other features around
    types of different grammatical elements can also be built. These are left to you
    to implement. Our `word_counts()` method is updated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'This function is a little different compared to the previous one. Since there
    are multiple computations that need to be performed on the message in each row,
    these operations are combined and a `Series` object with column labels is returned.
    This can be merged with the main DataFrame like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'A similar process can be performed on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'A quick check of the statistics for spam and non-spam messages in the training
    set shows the following, first for non-spam messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.12: Statistics for regular messages after using POS tags'
  prefs: []
  type: TYPE_NORMAL
- en: 'And then for spam messages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.13: Statistics for spam messages after using POS tags'
  prefs: []
  type: TYPE_NORMAL
- en: In general, word counts have been reduced even further after stop word removal.
    Further more, the new `Punct` feature computes the ratio of punctuation tokens
    in a message relative to the total tokens. Now we can build a model with this
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling data with POS tagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Plugging these features into the model, the following results are obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The accuracy shows a slight increase and is now up to 94.66%. Upon testing,
    it seems to hold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: The final part of text normalization is stemming and lemmatization. Though we
    will not be building any features for the spam model using this, it can be quite
    useful in other cases.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and lemmatization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In certain languages, the same word can take a slightly different form depending
    on its usage. Consider the word *depend* itself. The following are all valid forms
    of the word *depend*: *depends*, *depending*, *depended*, *dependent*. Often,
    these variations are due to tenses. In some languages like Hindi, verbs may have
    different forms for different genders. Another case is derivatives of the same
    word such as *sympathy*, *sympathetic*, *sympathize*, and *sympathizer*. These
    variations can take different forms in other languages. In Russian, proper nouns
    take different forms based on usage. Suppose there is a document talking about
    London (Лондон). The phrase *in London* (в Лондоне) spells *London* differently
    than *from London* (из Лондона). These variations in the spelling of *London*
    can cause issues when matching some input to sections or words in a document.'
  prefs: []
  type: TYPE_NORMAL
- en: When processing and tokenizing text to construct a vocabulary of words appearing
    in the corpora, the ability to identify the root word can reduce the size of the
    vocabulary while expanding the accuracy of matches. In the preceding Russian example,
    any form of the word London can be matched to any other form if all the forms
    are normalized to a common representation post-tokenization. This process of normalization
    is called stemming or lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: Stemming and lemmatization differ in their approach and sophistication but serve
    the same objective. Stemming is a simpler, heuristic rule-based approach that
    chops off the affixes of words. The most famous stemmer is called the Porter stemmer,
    published by Martin Porter in 1980\. The official website is [https://tartarus.org/martin/PorterStemmer/](https://tartarus.org/martin/PorterStemmer/),
    where various versions of the algorithm implemented in various languages are linked.
  prefs: []
  type: TYPE_NORMAL
- en: 'This stemmer only works for English and has rules including removing *s* at
    the end of the words for plurals, and removing endings such as *-ed* or *-ing*.
    Consider the following sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Stemming is aimed at reducing vocabulary and aid understanding of morphological
    processes. This helps people understand the morphology of words and reduce size
    of corpus."'
  prefs: []
  type: TYPE_NORMAL
- en: 'After stemming using Porter''s algorithm, this sentence will be reduced to
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Stem is aim at reduce vocabulari and aid understand of morpholog process .
    Thi help peopl understand the morpholog of word and reduc size of corpu ."'
  prefs: []
  type: TYPE_NORMAL
- en: Note how different forms of *morphology*, *understand*, and *reduce* are all
    tokenized to the same form.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization approaches this task in a more sophisticated manner, using vocabularies
    and morphological analysis of words. In the study of linguistics, a morpheme is
    a unit smaller than or equal to a word. When a morpheme is a word in itself, it
    is called a root or a free morpheme. Conversely, every word can be decomposed
    into one or more morphemes. The study of morphemes is called morphology. Using
    this morphological information, a word's root form can be returned post-tokenization.
    This base or dictionary form of the word is called a *lemma*, hence the process
    is called lemmatization. StanfordNLP includes lemmatization as part of processing.
  prefs: []
  type: TYPE_NORMAL
- en: The *Lemmatization* section of the notebook has the code shown here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple piece of code to take the preceding sentences and parse them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: After processing, we can iterate through the tokens to get the lemma of each
    word. This is shown in the following code fragment. The lemma of a word is exposed
    as the `.lemma` property of each word inside a token. For the sake of brevity
    of code, a simplifying assumption is made here that each token has only one word.
  prefs: []
  type: TYPE_NORMAL
- en: 'The POS for each word is also printed out to help us understand how the process
    was performed. Some key words in the following output are highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Compare this output to the output of the Porter stemmer earlier. One immediate
    thing to notice is that lemmas are actual words as opposed to fragments, as was
    the case with the Porter stemmer. In the case of *reduce*, the usage in both sentences
    is in the form of a verb, so the choice of lemma is consistent. Focus on the words
    *understand* and *understanding* in the preceding output. As the POS tag shows,
    it is used in two different forms. Consequently, it is not reduced to the same
    lemma. This is different from the Porter stemmer. The same behavior can be observed
    for *morphology* and *morphological*. This is a quite sophisticated behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Now that text normalization is completed, we can begin the vectorization of
    text.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorizing text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While building models for the SMS message spam detection thus far, only aggregate
    features based on counts or distributions of lexical or grammatical features have
    been considered. The actual words in the messages have not been used thus far.
    There are a couple of challenges in using the text content of messages. The first
    is that text can be of arbitrary lengths. Comparing this to image data, we know
    that each image has a fixed width and height. Even if the corpus of images has
    a mixture of sizes, images can be resized to a common size with minimal loss of
    information by using a variety of compression mechanisms. In NLP, this is a bigger
    problem compared to computer vision. A common approach to handle this is to truncate
    the text. We will see various ways to handle variable-length texts in various
    examples throughout the book.
  prefs: []
  type: TYPE_NORMAL
- en: The second issue is that of the representation of words with a numerical quantity
    or feature. In computer vision, the smallest unit is a pixel. Each pixel has a
    set of numerical values indicating color or intensity. In a text, the smallest
    unit could be a word. Aggregating the Unicode values of the characters does not
    convey or embody the meaning of the word. In fact, these character codes embody
    no information at all about the character, such as its prevalence, whether it
    is a consonant or a vowel, and so on. However, averaging the pixels in a section
    of an image could be a reasonable approximation of that region of the image. It
    may represent how that region would look if seen from a large distance. A core
    problem then is to construct a numerical representation of words. Vectorization
    is the process of converting a word to a vector of numbers that embodies the information
    contained in the word. Depending on the vectorization technique, this vector may
    have additional properties that may allow comparison with other words, as will
    be shown in the *Word vectors* section later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest approach for vectorizing is to use counts of words. The second
    approach is more sophisticated, with its origins in information retrieval, and
    is called TF-IDF. The third approach is relatively new, having been published
    in 2013, and uses RNNs to generate embeddings or word vectors. This method is
    called Word2Vec. The newest method in this area as of the time of writing was
    BERT, which came out in the last quarter of 2018\. The first three methods will
    be discussed in this chapter. BERT will be discussed in detail in *Chapter 3*,
    *Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi Decoding.*
  prefs: []
  type: TYPE_NORMAL
- en: Count-based vectorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea behind count-based vectorization is really simple. Each unique word
    appearing in the corpus is assigned a column in the vocabulary. Each document,
    which would correspond to individual messages in the spam example, is assigned
    a row. The counts of the words appearing in that document are entered in the relevant
    cell corresponding to the document and the word. With `n` unique documents containing
    `m` unique words, this results in a matrix of `n` rows by `m` columns. Consider
    a corpus like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: There are three documents in this corpus of text. The `scikit-learn` (`sklearn`)
    library provides methods for undertaking count-based vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling after count-based vectorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Google Colab, this library should already be installed. If it is not installed
    in your Python environment, it can be installed via the notebook like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'The `CountVectorizer` class provides a built-in tokenizer that separates the
    tokens of two or more characters in length. This class takes a variety of options
    including a custom tokenizer, a stop word list, the option to convert characters
    to lowercase prior to tokenization, and a binary mode that converts every positive
    count to 1\. The defaults provide a reasonable choice for an English language
    corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, a model is fit to the corpus. The last line prints out
    the tokens that are used as columns. The full matrix can be seen as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'This process has now converted a sentence such as "I like fruits. Fruits like
    bananas" into a vector (`0, 0, 0, 1, 0, 0, 0, 2, 0, 2, 0, 0`). This is an example
    of **context-free** vectorization. Context-free refers to the fact that the order
    of the words in the document did not make any difference in the generation of
    the vector. This is merely counting the instances of the words in a document.
    Consequently, words with multiple meanings may be grouped into one, for example,
    *bank*. This may refer to a place near the river or a place to keep money. However,
    it does provide a method to compare documents and derive similarity. The cosine
    similarity or distance can be computed between two documents, to see which documents
    are similar to which other documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows that the first sentence and the second sentence have a 0.136 similarity
    score (on a scale of 0 to 1). The first and third sentence have nothing in common.
    The second and third sentence have a similarity score of 0.308 – the highest in
    this set. Another use case of this technique is to check the similarity of the
    documents with given keywords. Let''s say that the query is *apple and bananas*.
    This first step is to compute the vector of this query, and then compute the cosine
    similarity scores against the documents in the corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This shows that this query matches the second sentence in the corpus the best.
    The third sentence would rank second, and the first sentence would rank lowest.
    In a few lines, a basic search engine has been implemented, along with logic to
    serve queries! At scale, this is a very difficult problem, as the number of words
    or columns in a web crawler would top 3 billion. Every web page would be represented
    as a row, so that would also require billions of rows. Computing a cosine similarity
    in milliseconds to serve an online query and keeping the content of this matrix
    updated is a massive undertaking.
  prefs: []
  type: TYPE_NORMAL
- en: The next step from this rather simple vectorization scheme is to consider the
    information content of each word in constructing this matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Term Frequency-Inverse Document Frequency (TF-IDF)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In creating a vector representation of the document, only the presence of words
    was included – it does not factor in the importance of a word. If the corpus of
    documents being processed is about a set of recipes with fruits, then one may
    expect words like *apples*, *raspberries*, and *washing* to appear frequently.
    **Term Frequency** (**TF**) represents how often a word or token occurs in a given
    document. This is exactly what we did in the previous section. In a set of documents
    about fruits and cooking, a word like *apple* may not be terribly specific to
    help identify a recipe. However, a word like *tuile* may be uncommon in that context.
    Therefore, it may help to narrow the search for recipes much faster than a word
    like *raspberry*. On a side note, feel free to search the web for raspberry tuile
    recipes. If a word is rare, we want to give it a higher weight, as it may contain
    more information than a common word. A term can be upweighted by the inverse of
    the number of documents it appears in. Consequently, words that occur in a lot
    of documents will get a smaller score compared to terms that appear in fewer documents.
    This is called the **Inverse Document Frequency** (**IDF**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, the score of each term in a document can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_01_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *t* represents the word or term, and *d* represents a specific document.
  prefs: []
  type: TYPE_NORMAL
- en: It is common to normalize the TF of a term in a document by the total number
    of tokens in that document.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IDF is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_01_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *N* represents the total number of documents in the corpus, and *n*[t]
    represents the number of documents where the term is present. The addition of
    1 in the denominator avoids the divide-by-zero error. Fortunately, `sklearn` provides
    methods to compute TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: The *TF-IDF Vectorization* section of the notebook contains the code for this
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s convert the counts from the previous section into their TF-IDF equivalents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_01_14.png)'
  prefs: []
  type: TYPE_IMG
- en: This should give some intuition on how TF-IDF is computed. Even with three toy
    sentences and a very limited vocabulary, many of the columns in each row are 0\.
    This vectorization produces **sparse representations**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, this can be applied to the problem of detecting spam messages. Thus far,
    the features for each message have been computed based on some aggregate statistics
    and added to the `pandas` DataFrame. Now, the content of the message will be tokenized
    and converted into a set of columns. The TF-IDF score for each word or token will
    be computed for each message in the array. This is surprisingly easy to do with
    `sklearn`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: The second parameter shows that 7,741 tokens were uniquely identified. These
    are the columns of features that will be used in the model later. Note that the
    vectorizer was created with the binary flag. This implies that even if a token
    appears multiple times in a message, it is counted as one. The next line trains
    the TF-IDF model on the training dataset. Then, it converts the words in the test
    set according to the TF-IDF scores learned from the training set. Let's train
    a model on just these TF-IDF features.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling using TF-IDF features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With these TF-IDF features, let''s train a model and see how it does:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Whoa – we are able to classify every one correctly! In all honesty, the model
    is probably overfitting, so some regularization should be applied. The test set
    gives this result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'An accuracy rate of 98.39% is by far the best we have gotten in any model so
    far. Checking the confusion matrix, it is evident that this model is indeed doing
    very well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Only 2 regular messages were classified as spam, while only 16 spam messages
    were classified as being not spam. This is indeed a very good model. Note that
    this dataset has Indonesian (or Bahasa) words as well as English words in it.
    Bahasa uses the Latin alphabet. This model, without using a lot of pretraining
    and knowledge of language, vocabulary, and grammar, was able to do a very reasonable
    job with the task at hand.
  prefs: []
  type: TYPE_NORMAL
- en: However, this model ignores the relationships between words completely. It treats
    the words in a document as unordered items in a set. There are better models that
    vectorize the tokens in a way that preserves some of the relationships between
    the tokens. This is explored in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Word vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous example, a row vector was used to represent a document. This
    was used as a feature for the classification model to predict spam labels. However,
    no information can be gleaned reliably from the relationships between words. In
    NLP, a lot of research has been focused on learning the words or representations
    in an unsupervised way. This is called representation learning. The output of
    this approach is a representation of a word in some vector space, and the word
    can be considered **embedded** in that space. Consequently, these word vectors
    are also called embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The core hypothesis behind word vector algorithms is that words that occur near
    each other are related to each other. To see the intuition behind this, consider
    two words, *bake* and *oven*. Given a sentence fragment of five words, where one
    of these words is present, what would be the probability of the other being present
    as well? You would be right in guessing that the probability is likely quite high.
    Suppose now that words are being mapped into some two-dimensional space. In that
    space, these two words should be closer to each other, and probably further away
    from words like *astronomy* and *tractor*.
  prefs: []
  type: TYPE_NORMAL
- en: The task of learning these embeddings for the words can be then thought of as
    adjusting words in a giant multidimensional space where similar words are closer
    to each other and dissimilar words are further apart from each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'A revolutionary approach to do this is called Word2Vec. This algorithm was
    published by Tomas Mikolov and collaborators from Google in 2013\. This approach
    produces dense vectors of the order of 50-300 dimensions generally (though larger
    are known), where most of the values are non-zero. In contrast, in our previous
    trivial spam example, the TF-IDF model had 7,741 dimensions. The original paper
    had two algorithms proposed in it: **continuous bag-of-words** and **continuous
    skip-gram**. On semantic tasks and overall, the performance of skip-gram was state
    of the art at the time of its publication. Consequently, the continuous skip-gram
    model with negative sampling has become synonymous with Word2Vec. The intuition
    behind this model is fairly straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider this sentence fragment from a recipe: "Bake until the cookie is golden
    brown all over." Under the assumption that a word is related to the words that
    appear near it, a word from this fragment can be picked and a classifier can be
    trained to predict the words around it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a logo  Description automatically generated](img/B16252_01_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.14: A window of 5 centered on cookie'
  prefs: []
  type: TYPE_NORMAL
- en: Taking an example of a window of five words, the word in the center is used
    to predict two words before and two words after it. In the preceding figure, the
    fragment is *until the cookie is golden*, with the focus on the word *cookie*.
    Assuming that there are 10,000 words in the vocabulary, a network can be trained
    to predict binary decisions given a pair of words. The training objective is that
    the network predicts `true` for pairs like (*cookie*, *golden*) while predicting
    `false` for (*cookie*, *kangaroo*). This particular approach is called **Skip-Gram
    Negative Sampling** (**SGNS**) and it considerably reduces the training time required
    for large vocabularies. Very similar to the single-layer neural model in the previous
    section, a model can be trained with a one-to-many as the output layer. The sigmoid
    activation would be changed to a `softmax` function. If the hidden layer has 300
    units, then its dimensions would be 10,000 x 300, that is, for each of the words,
    there will be a set of weights. The objective of the training is to learn these
    weights. In fact, these weights become the embedding for that word once training
    is complete.
  prefs: []
  type: TYPE_NORMAL
- en: The choice of units in the hidden layer is a hyperparameter that can be adapted
    for specific applications. 300 is commonly found as it is available through pretrained
    embeddings on the Google News dataset. Finally, the error is computed as the sum
    of the categorical cross-entropy of all the word pairs in negative and positive
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of this model is that it does not require any supervised training
    data. Running sentences can be used to provide positive examples. For the model
    to learn effectively, it is important to provide negative samples as well. Words
    are randomly sampled using their probability of occurrence in the training corpus
    and fed as negative examples.
  prefs: []
  type: TYPE_NORMAL
- en: To understand how the Word2Vec embeddings work, let's download a set of pretrained
    embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: The code shown in the following section can be found in the *Word Vectors* section
    of the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Pretrained models using Word2Vec embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since we are only interested in experimenting with a pretrained model, we can
    use the Gensim library and its pretrained embeddings. Gensim should already be
    installed in Google Colab. It can be installed like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'After the requisite imports, pretrained embeddings can be downloaded and loaded.
    Note that these particular embeddings are approximately 1.6 GB in size, so may
    take a very long time to load (you may encounter some memory issues as well):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Another issue that you may run into is the Colab session expiring if left alone
    for too long while waiting for the download to finish. This may be a good time
    to switch to a local notebook, which will also be helpful in future chapters.
    Now, we are ready to inspect the similar words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'This is pretty good. Let''s see how this model does at a word analogy task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'The model is able to guess that compared to the other words, which are all
    countries, Tokyo is the odd one out, as it is a city. Now, let''s try a very famous
    example of mathematics on these word vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: Given that *King* was provided as an input to the equation, it is simple to
    filter the inputs from the outputs and *Queen* would be the top result. SMS spam
    classification could be attempted using these embeddings. However, future chapters
    will cover the use of GloVe embeddings and BERT embeddings for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: A pretrained model like the preceding can be used to vectorize a document. Using
    these embeddings, models can be trained for specific purposes. In later chapters,
    newer methods of generating contextual embeddings, such as BERT, will be discussed
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we worked through the basics of NLP, including collecting and
    labeling training data, tokenization, stop word removal, case normalization, POS
    tagging, stemming, and lemmatization. Some vagaries of these in languages such
    as Japanese and Russian were also covered. Using a variety of features derived
    from these approaches, we trained a model to classify spam messages, where the
    messages had a combination of English and Bahasa Indonesian words. This got us
    to a model with 94% accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: However, the major challenge in using the content of the messages was in defining
    a way to represent words as vectors such that computations could be performed
    on them. We started with a simple count-based vectorization scheme and then graduated
    to a more sophisticated TF-IDF approach, both of which produced sparse vectors.
    This TF-IDF approach gave a model with 98%+ accuracy in the spam detection task.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we saw a contemporary method of generating dense word embeddings, called
    Word2Vec. This method, though a few years old, is still very relevant in many
    production applications. Once the word embeddings are generated, they can be cached
    for inference and that makes an ML model using these embeddings run with relatively
    low latency.
  prefs: []
  type: TYPE_NORMAL
- en: We used a very basic deep learning model for solving the SMS spam classification
    task. Like how **Convolutional Neural Networks** (**CNNs**) are the predominant
    architecture in computer vision, **Recurrent Neural Networks** (**RNNs**), especially
    those based on **Long Short-Term Memory** (**LSTM**) and **Bi-directional LSTMs**
    (**BiLSTMs**), are most commonly used to build NLP models. In the next chapter,
    we cover the structure of LSTMs and build a sentiment analysis model using BiLSTMs.
    These models will be used extensively in creative ways to solve different NLP
    problems in future chapters.
  prefs: []
  type: TYPE_NORMAL
