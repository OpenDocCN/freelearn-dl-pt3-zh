<html><head></head><body>
		<div id="_idContainer053">
			<h1 id="_idParaDest-122"><em class="italic"><a id="_idTextAnchor140"/>Chapter 4</em>: Enhancing and Styling Images with DeepDream, Neural Style Transfer, and Image Super-Resolution</h1>
			<p>Although deep neural networks excel in traditional computer vision tasks for purely practical applications, they have a fun side too! As we'll discover in this chapter, we can unlock the artistic side of deep learning with the help of a little bit of cleverness and math, of course!</p>
			<p>We'll start this chapter by covering <strong class="bold">DeepDream</strong>, an algorithm used to make neural networks produce dream-like images. Next, we'll seize the power of transfer learning to apply the style of famous paintings to our own images (this is known as <strong class="bold">Neural Style Transfer</strong>). Finally, we'll close with <strong class="bold">Image Super-Resolution</strong>, a deep learning approach that's used to improve the quality of an image.</p>
			<p>In this chapter, we will cover the following recipes:</p>
			<ul>
				<li>Implementing DeepDream</li>
				<li><a id="_idTextAnchor141"/><a id="_idTextAnchor142"/>Generating your own dreamy images</li>
				<li>Implementing Neural Style Transfer</li>
				<li>Applying style transfer to custom images</li>
				<li>Applying style transfer with TFHub</li>
				<li>Improving image resolution with deep learning</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor143"/>Technical requirements</h1>
			<p>The usual advice whenever we are working with deep learning applies here: if possible, access a GPU since it greatly improves efficiency and lowers the computing time. In each recipe, you'll find specific preparation instructions in the <em class="italic">Getting ready</em> section, if needed. You can find all the code for this chapter here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3bDns2A">https://bit.ly/3bDns2A</a>.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor144"/><a id="_idTextAnchor145"/>Implementing DeepDream</h1>
			<p><strong class="bold">DeepDream</strong> is <a id="_idIndexMarker286"/>the result of an experiment that aimed to visualize the internal patterns that are learned by a neural network. In order to achieve this goal, we can pass an image through the network, compute its gradient with respect to the activations of a specific layer, and then modify the image to increase the magnitude of such activations to, in turn, magnify the patterns. The result? Psychedelic, surreal photos!</p>
			<p>Although this recipe is a bit complex due to the nature of <strong class="bold">DeepDream</strong>, we will take it one step at a time, so don't worry.</p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor146"/>Getting ready</h2>
			<p>We don't need to install anything extra for this recipe. However, we won't dive deep into the details<a id="_idIndexMarker287"/> of <strong class="bold">DeepDream</strong>, but if you're interested in the topic, you can read the original blog post by Google here: <a href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a>.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor147"/>How to do it…</h2>
			<p>Follow these steps and you'll have your own deep dreamer in no time:</p>
			<ol>
				<li>Import all the necessary packages:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.applications.inception_v3 import *</p></li>
				<li>Define<a id="_idIndexMarker288"/> the <strong class="source-inline">DeepDreamer</strong> class and its constructor:<p class="source-code">class DeepDreamer(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 octave_scale=1.30,</p><p class="source-code">                 octave_power_factors=None,</p><p class="source-code">                 layers=None):</p></li>
				<li>The constructor parameters specify the scale by which we'll increase the size of an image (<strong class="source-inline">octave_scale</strong>), as well as the factor that will applied to the scale (<strong class="source-inline">octave_power_factors</strong>). <strong class="source-inline">layers</strong> contains the target layers that will be used to generate the dreams. Next, let's store the parameters as object members:<p class="source-code">        self.octave_scale = octave_scale</p><p class="source-code">        if octave_power_factors is None:</p><p class="source-code">            self.octave_power_factors = [*range(-2, 3)]</p><p class="source-code">        else:</p><p class="source-code">            self.octave_power_factors = </p><p class="source-code">                       octave_power_factors</p><p class="source-code">        if layers is None:</p><p class="source-code">            self.layers = ['mixed3', 'mixed5']</p><p class="source-code">        else:</p><p class="source-code">            self.layers = layers</p></li>
				<li>If some of <a id="_idIndexMarker289"/>the inputs are <strong class="source-inline">None</strong>, we use defaults. If not, we use the inputs. Finally, create the dreamer model by extracting our <strong class="source-inline">layers</strong> from a pre-trained <strong class="source-inline">InceptionV3</strong> network:<p class="source-code">        self.base_model = InceptionV3(weights='imagenet',</p><p class="source-code">                                     include_top=False)</p><p class="source-code">        outputs = [self.base_model.get_layer(name).output</p><p class="source-code">                  for name in self.layers]</p><p class="source-code">        self.dreamer_model = Model(self.base_model.input,</p><p class="source-code">                                   outputs)</p></li>
				<li>Define a private method that will compute the loss:<p class="source-code">    def _calculate_loss(self, image):</p><p class="source-code">        image_batch = tf.expand_dims(image, axis=0)</p><p class="source-code">        activations = self.dreamer_model(image_batch)</p><p class="source-code">        if len(activations) == 1:</p><p class="source-code">            activations = [activations]</p><p class="source-code">        losses = []</p><p class="source-code">        for activation in activations:</p><p class="source-code">            loss = tf.math.reduce_mean(activation)</p><p class="source-code">            losses.append(loss)</p><p class="source-code">        total_loss = tf.reduce_sum(losses)</p><p class="source-code">        return total_loss</p></li>
				<li>Define <a id="_idIndexMarker290"/>a private method that will perform gradient ascent (remember, we want to magnify the patterns of the image). To increase performance, we can wrap this function in <strong class="source-inline">tf.function</strong>:<p class="source-code">    @tf.function</p><p class="source-code">    def _gradient_ascent(self, image, steps, step_size):</p><p class="source-code">        loss = tf.constant(0.0)</p><p class="source-code">        for _ in range(steps):</p><p class="source-code">            with tf.GradientTape() as tape:</p><p class="source-code">                tape.watch(image)</p><p class="source-code">                loss = self._calculate_loss(image)</p><p class="source-code">            gradients = tape.gradient(loss, image)</p><p class="source-code">            gradients /= tf.math.reduce_std(gradients) </p><p class="source-code">                                          + 1e-8</p><p class="source-code">            image = image + gradients * step_size</p><p class="source-code">            image = tf.clip_by_value(image, -1, 1)</p><p class="source-code">        return loss, image</p></li>
				<li>Define a private method that will convert the image tensor generated by the dreamer back into a <strong class="source-inline">NumPy</strong> array:<p class="source-code">    def _deprocess(self, image):</p><p class="source-code">        image = 255 * (image + 1.0) / 2.0</p><p class="source-code">        image = tf.cast(image, tf.uint8)</p><p class="source-code">        image = np.array(image)</p><p class="source-code">        return image</p></li>
				<li>Define a<a id="_idIndexMarker291"/> private method that will generate a dreamy image by performing<strong class="source-inline"> _gradient_ascent()</strong> for a specific number of steps:<p class="source-code">    def _dream(self, image, steps, step_size):</p><p class="source-code">        image = preprocess_input(image)</p><p class="source-code">        image = tf.convert_to_tensor(image)</p><p class="source-code">        step_size = tf.convert_to_tensor(step_size)</p><p class="source-code">        step_size = tf.constant(step_size)</p><p class="source-code">        steps_remaining = steps</p><p class="source-code">        current_step = 0</p><p class="source-code">        while steps_remaining &gt; 0:</p><p class="source-code">            if steps_remaining &gt; 100:</p><p class="source-code">                run_steps = tf.constant(100)</p><p class="source-code">            else:</p><p class="source-code">                run_steps = </p><p class="source-code">                     tf.constant(steps_remaining)</p><p class="source-code">            steps_remaining -= run_steps</p><p class="source-code">            current_step += run_steps</p><p class="source-code">            loss, image = self._gradient_ascent(image,</p><p class="source-code">                                           run_steps,</p><p class="source-code">                                           step_size)</p><p class="source-code">        result = self._deprocess(image)</p><p class="source-code">        return result</p></li>
				<li>Define a<a id="_idIndexMarker292"/> public method that will generate dreamy images. The main difference between this and <strong class="source-inline">_dream()</strong> (defined in <em class="italic">Step 6</em> and used internally here) is that we'll use different image sizes (called <strong class="bold">octaves</strong>), as determined <a id="_idIndexMarker293"/>by the original image shape multiplied by a factor, which is the product of powering <strong class="source-inline">self.octave_scale</strong> to each power in <strong class="source-inline">self.octave_power_factors</strong>:<p class="source-code">    def dream(self, image, steps=100, step_size=0.01):</p><p class="source-code">        image = tf.constant(np.array(image))</p><p class="source-code">        base_shape = tf.shape(image)[:-1]</p><p class="source-code">        base_shape = tf.cast(base_shape, tf.float32)</p><p class="source-code">        for factor in self.octave_power_factors:</p><p class="source-code">            new_shape = tf.cast(</p><p class="source-code">                base_shape * (self.octave_scale ** </p><p class="source-code">                               factor),</p><p class="source-code">                               tf.int32)</p><p class="source-code">            image = tf.image.resize(image, </p><p class="source-code">                                   new_shape).numpy()</p><p class="source-code">            image = self._dream(image, steps=steps,</p><p class="source-code">                                step_size=step_size)</p><p class="source-code">        base_shape = tf.cast(base_shape, tf.int32)</p><p class="source-code">        image = tf.image.resize(image, base_shape)</p><p class="source-code">        image = tf.image.convert_image_dtype(image / </p><p class="source-code">                                              255.0,</p><p class="source-code">                                       dtype=tf.uint8)</p><p class="source-code">        image = np.array(image)</p><p class="source-code">        return np.array(image)</p></li>
			</ol>
			<p>The <strong class="source-inline">DeepDreamer()</strong> class <a id="_idIndexMarker294"/>can be reused to produce dream-like versions of any image we supply to it. We'll see how this works in the next section.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor148"/>How it works…</h2>
			<p>We just implemented a utility class to easily apply <strong class="bold">DeepDream</strong>. The algorithm works by calculating the gradient with respect to the activations of a set of layers, then using such gradients to enhance the patterns seen by the network. </p>
			<p>In our <strong class="source-inline">DeepDreamer()</strong> class, the previously described process is implemented in the <strong class="source-inline">_gradient_ascent()</strong> method (defined in <em class="italic">Step 4</em>), where we calculated the gradients and added them to the original image over a series of steps. The result was an activation map where, in each subsequent step, the <strong class="bold">excitement</strong> of certain neurons in the target layers was magnified.</p>
			<p>Generating a dream consists of applying gradient ascent many times, which we basically did in the <strong class="source-inline">_dream()</strong> method (<em class="italic">Step 6</em>).</p>
			<p>One of the problems of applying gradient ascent at the same scale is that the result looks noisy, with low resolution. Also, the patterns seem to happen at the same granularity level, which produces a uniformity in the result that decreases the dream-like effect we want. To resolve all these issues, the main method, <strong class="source-inline">dream()</strong>, applies gradient ascent at different <a id="_idIndexMarker295"/>scales (called <strong class="bold">octaves</strong>), where the dreamy output of one octave is the input of the next iteration, at a higher scale. </p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor149"/>See also</h2>
			<p>To see the dream-like results of passing different combinations of parameters to <strong class="source-inline">DeepDreamer()</strong>, please see the next recipe, <em class="italic">Generating your own dreamy images</em>.</p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor150"/>Generating your own dreamy images</h1>
			<p>Deep learning<a id="_idIndexMarker296"/> has an entertaining side. <strong class="bold">DeepDream</strong> is one application that aims to understand the inner workings of deep neural networks by exciting certain activations on selected layers. However, beyond the investigative intent of the experiment, it also produces psychedelic, dream-like fun images.</p>
			<p>In this recipe, we'll experiment with several configurations of <strong class="bold">DeepDream</strong> on a test image and see how they affect the results.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor151"/>Getting ready</h2>
			<p>We'll use the <strong class="source-inline">DeepDreamer()</strong> implementation from the first recipe of this chapter (<em class="italic">Implementing DeepDream</em>). Although I encourage you to try this out with your own images, if you want to follow this recipe as closely as possible, you can download the sample image here: https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe2/road.jpg.</p>
			<p>Let's take a look at the sample image:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/Image86668.jpg" alt="Figure 4.1 – Sample image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – Sample image</p>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor152"/>How to do it…</h2>
			<p>Follow these steps to<a id="_idIndexMarker297"/> cook up your own dreamy photos:</p>
			<ol>
				<li value="1">Let's start by importing the required packages. Notice that we are importing <strong class="source-inline">DeepDreamer()</strong> from the previous recipe, <em class="italic">Implementing DeepDream</em>:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p><p class="source-code">from ch4.recipe1.deepdream import DeepDreamer</p></li>
				<li>Define the <strong class="source-inline">load_image()</strong> function that will load images from disk into memory as <strong class="source-inline">NumPy</strong> arrays:<p class="source-code">def load_image(image_path):</p><p class="source-code">    image = load_img(image_path)</p><p class="source-code">    image = img_to_array(image)</p><p class="source-code">    return image</p></li>
				<li>Define a function that will display an image (represented as a <strong class="source-inline">NumPy</strong> array) using <strong class="source-inline">matplotlib</strong>:<p class="source-code">def show_image(image):</p><p class="source-code">    plt.imshow(image)</p><p class="source-code">    plt.show()</p></li>
				<li>Load the original image and display it:<p class="source-code">original_image = load_image('road.jpg')</p><p class="source-code">show_image(original_image / 255.0)</p><p>Here, we can <a id="_idIndexMarker298"/>see the displayed original image:</p><div id="_idContainer037" class="IMG---Figure"><img src="image/B14768_04_002.jpg" alt="Figure 4.2 – Original image that we’ll modify shortly&#13;&#10;"/></div><p class="figure-caption">Figure 4.2 – Original image that we'll modify shortly</p><p>As we can see, it is just a road that cuts through a forest.</p></li>
				<li>Generate a dreamy version of the image using the default parameters and display the result:<p class="source-code">dreamy_image = DeepDreamer().dream(original_image)</p><p class="source-code">show_image(dreamy_image)</p><p>Here's the<a id="_idIndexMarker299"/> result:</p><div id="_idContainer038" class="IMG---Figure"><img src="image/B14768_04_003.jpg" alt="Figure 4.3 – Result of using DeepDream with the default parameters&#13;&#10;"/></div><p class="figure-caption">Figure 4.3 – Result of using DeepDream with the default parameters</p><p>The result preserves the overall theme of the original photo but adds lots of distortion on top of it in the form of circles, curves, and other basic patterns. Cool – and a bit creepy!</p></li>
				<li>Use three layers. Layers near the top (for instance, <strong class="source-inline">'mixed7'</strong>) encode higher-level patterns:<p class="source-code">dreamy_image = (DeepDreamer(layers=['mixed2',</p><p class="source-code">                                    'mixed5',</p><p class="source-code">                                    'mixed7'])</p><p class="source-code">                .dream(original_image))</p><p class="source-code">show_image(dreamy_image)</p><p>Here's the<a id="_idIndexMarker300"/> result of using three layers:</p><div id="_idContainer039" class="IMG---Figure"><img src="image/B14768_04_004.jpg" alt="Figure 4.4 – Result of using DeepDream with more, higher-level layers&#13;&#10;"/></div><p class="figure-caption">Figure 4.4 – Result of using DeepDream with more, higher-level layers</p><p>The addition of more layers softened the produced dream. We can see that the patterns are smoother than before, which is likely due to the fact that the <strong class="source-inline">'mixed7'</strong> layer encodes more abstract information because it is farther down the architecture. Let's remember that the first layers in a network learn basic patterns, such as lines and shapes, while the layers closer to the output combine these basic patterns to learn more complex, abstract information.</p></li>
				<li>Finally, let's <a id="_idIndexMarker301"/>use more <strong class="bold">octaves</strong>. The result we expect is an image with less noise and more heterogeneous patterns:<p class="source-code">dreamy_image = (DeepDreamer(octave_power_factors=[-3, -1,</p><p class="source-code">                                                  0, 3])</p><p class="source-code">                .dream(original_image))</p><p class="source-code">show_image(dreamy_image)                </p><p>Here's the resulting image after using more octaves:</p></li>
			</ol>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B14768_04_005.jpg" alt="Figure 4.5 – Result of using DeepDream with more octaves&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Result of using DeepDream with more octaves</p>
			<p>This generated dream contains a satisfying mixture of both high- and low-level patterns, as well as a better color distribution than the one produced in <em class="italic">Step 4</em>.</p>
			<p>Let's go to the next section to understand what we've just done.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor153"/>How it works…</h2>
			<p>In this recipe, we<a id="_idIndexMarker302"/> leveraged the hard work we did in the <em class="italic">Implementing DeepDream</em> recipe in order to produce several dreamy versions of our input image of a road in a forest. By combining different parameters, we discovered that the results could vary widely. Using higher layers, which encode more abstract information, we obtained pictures with less noise and more nuanced patterns.</p>
			<p>If we choose to use more octaves, this translates into more images, at different scales, being processed by the network. This approach generates less saturated images, while keeping the more raw, basic patterns typical of the first few layers in a convolutional neural network.</p>
			<p>In the end, with just an image and a little creativity, we can obtain pretty interesting results!</p>
			<p>An even more entertaining application of deep learning is Neural Style Transfer, which we will cover in the next recipe.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor154"/>Implementing Neural Style Transfer</h1>
			<p>Creativity and artistic<a id="_idIndexMarker303"/> expression are not traits that we tend to associate with deep neural networks and AI in general. However, did you know that with the right tweaks, we can turn pre-trained networks into impressive artists, capable of applying the distinctive style of famous painters such as Monet, Picasso, and Van Gogh to our mundane pictures?</p>
			<p>This is exactly what Neural Style Transfer does. By the end of this recipe, we'll have the artistic prowess of any painter at our disposal! </p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor155"/>Getting ready</h2>
			<p>We don't need to install any libraries or bring in extra resources to implement Neural Style Transfer. However, because this is a hands-on recipe, we won't detail the inner workings of our solution extensively. If you're interested in the ins and outs of Neural Style Transfer, I recommend that you read the original paper here: <a href="https://arxiv.org/abs/1508.06576">https://arxiv.org/abs/1508.06576</a>.</p>
			<p>I hope you're ready because we are about to begin!</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor156"/>How to do it…</h2>
			<p>Follow these steps to <a id="_idIndexMarker304"/>implement your own, reusable, neural style transferrer:</p>
			<ol>
				<li value="1">Import the necessary packages (notice that we're using a pre-trained <strong class="bold">VGG19</strong> network in our implementation):<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.applications.vgg19 import *</p></li>
				<li>Define the <strong class="source-inline">StyleTransferrer()</strong> class and its constructor:<p class="source-code">class StyleTransferrer(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 content_layers=None,</p><p class="source-code">                 style_layers=None):</p></li>
				<li>The only relevant parameters are two optional lists of layers for the content and style generation, respectively. If they are <strong class="source-inline">None</strong>, we'll use defaults internally (as we'll see shortly). Next, load the pre-trained <strong class="source-inline">VGG19</strong> and freeze it:<p class="source-code">        self.model = VGG19(weights='imagenet',</p><p class="source-code">                           include_top=False)</p><p class="source-code">        self.model.trainable = False</p></li>
				<li>Set the weight (importance) of the style and content losses (we'll use these parameters later). Also, store<a id="_idIndexMarker305"/> the content and style layers (or use the defaults if necessary):<p class="source-code">        self.style_weight = 1e-2</p><p class="source-code">        self.content_weight = 1e4</p><p class="source-code">        if content_layers is None:</p><p class="source-code">            self.content_layers = ['block5_conv2']</p><p class="source-code">        else:</p><p class="source-code">            self.content_layers = content_layers</p><p class="source-code">        if style_layers is None:</p><p class="source-code">            self.style_layers = ['block1_conv1',</p><p class="source-code">                                 'block2_conv1',</p><p class="source-code">                                 'block3_conv1',</p><p class="source-code">                                 'block4_conv1',</p><p class="source-code">                                 'block5_conv1']</p><p class="source-code">        else:</p><p class="source-code">            self.style_layers = style_layers</p></li>
				<li>Define and store the style transferrer model, which takes the <strong class="bold">VGG19</strong> input layer as input and outputs all the content and style layers (please take into account that we can use any model, but the best results are usually achieved using either VGG19 or InceptionV3):<p class="source-code">        outputs = [self.model.get_layer(name).output</p><p class="source-code">                   for name in</p><p class="source-code">                   (self.style_layers + </p><p class="source-code">                   self.content_layers)]</p><p class="source-code">        self.style_model = Model([self.model.input], </p><p class="source-code">                                outputs)</p></li>
				<li>Define a<a id="_idIndexMarker306"/> private method that will calculate the <strong class="bold">Gram Matrix</strong>, which is <a id="_idIndexMarker307"/>used to calculate the style of an image. This is represented by a matrix that contains the means and correlations across different feature maps in the input tensor (for instance, the weights in a particular layer), known as a <strong class="bold">Gram Matrix</strong>. For more information on the <strong class="bold">Gram Matrix</strong>, please refer to the <em class="italic">See also</em> section of this recipe:<p class="source-code">    def _gram_matrix(self, input_tensor):</p><p class="source-code">        result = tf.linalg.einsum('bijc,bijd-&gt;bcd',</p><p class="source-code">                                  input_tensor,</p><p class="source-code">                                  input_tensor)</p><p class="source-code">        input_shape = tf.shape(input_tensor)</p><p class="source-code">        num_locations = np.prod(input_shape[1:3])</p><p class="source-code">        num_locations = tf.cast(num_locations,tf.float32)</p><p class="source-code">        result = result / num_locations</p><p class="source-code">        return result</p></li>
				<li>Next, define a private method that will calculate the outputs (content and style). What this private method does is pass the inputs to the model and then compute the <strong class="bold">Gram Matrix</strong> of all the style layers, as well as the identity of the content layers, returning dicts that map each layer name to the processed values:<p class="source-code">    def _calc_outputs(self, inputs):</p><p class="source-code">        inputs = inputs * 255</p><p class="source-code">        preprocessed_input = preprocess_input(inputs)</p><p class="source-code">        outputs = self.style_model(preprocessed_input)</p><p class="source-code">        style_outputs = outputs[:len(self.style_layers)]</p><p class="source-code">        content_outputs = </p><p class="source-code">                    outputs[len(self.style_layers):]</p><p class="source-code">        style_outputs = </p><p class="source-code">       [self._gram_matrix(style_output)</p><p class="source-code">                         for style_output in </p><p class="source-code">                            style_outputs]</p><p class="source-code">        content_dict = {content_name: value</p><p class="source-code">                        for (content_name, value)</p><p class="source-code">                        in zip(self.content_layers,</p><p class="source-code">                               content_outputs)}</p><p class="source-code">        style_dict = {style_name: value</p><p class="source-code">                      for (style_name, value)</p><p class="source-code">                      in zip(self.style_layers,</p><p class="source-code">                             style_outputs)}</p><p class="source-code">        return {'content': content_dict,</p><p class="source-code">                'style': style_dict}</p></li>
				<li>Define a <a id="_idIndexMarker308"/>static helper private method that will clip values between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>:<p class="source-code">    @staticmethod</p><p class="source-code">    def _clip_0_1(image):</p><p class="source-code">        return tf.clip_by_value(image,</p><p class="source-code">                                clip_value_min=0.0,</p><p class="source-code">                                clip_value_max=1.0)</p></li>
				<li>Define a static helper private method that will compute the loss between a pair of outputs and targets:<p class="source-code">    @staticmethod</p><p class="source-code">    def _compute_loss(outputs, targets):</p><p class="source-code">        return tf.add_n([</p><p class="source-code">            tf.reduce_mean((outputs[key] - </p><p class="source-code">                           targets[key]) ** 2)</p><p class="source-code">            for key in outputs.keys()</p><p class="source-code">        ])</p></li>
				<li>Define a<a id="_idIndexMarker309"/> private method that will compute the total loss, which is the result of computing the style and content loss individually, by multiplying them by their respective weight distributed across the corresponding layer and then adding them up: <p class="source-code">    def _calc_total_loss(self,</p><p class="source-code">                         outputs,</p><p class="source-code">                         style_targets,</p><p class="source-code">                         content_targets):</p><p class="source-code">        style_outputs = outputs['style']</p><p class="source-code">        content_outputs = outputs['content']</p><p class="source-code">        n_style_layers = len(self.style_layers)</p><p class="source-code">        s_loss = self._compute_loss(style_outputs,</p><p class="source-code">                                    style_targets)</p><p class="source-code">        s_loss *= self.style_weight / n_style_layers</p><p class="source-code">        n_content_layers = len(self.content_layers)</p><p class="source-code">        c_loss = self._compute_loss(content_outputs,</p><p class="source-code">                                    content_targets)</p><p class="source-code">        c_loss *= self.content_weight / n_content_layers</p><p class="source-code">        return s_loss + c_loss</p></li>
				<li>Next, define <a id="_idIndexMarker310"/>a private method that will train the model. During a set number of epochs, and for a given number of steps per epoch, we'll calculate the outputs (style and content), compute the total loss, and obtain and apply the gradient to the generated image while using <strong class="source-inline">Adam</strong> as an optimizer:<p class="source-code">    @tf.function()</p><p class="source-code">    def _train(self,</p><p class="source-code">               image,</p><p class="source-code">               s_targets,</p><p class="source-code">               c_targets,</p><p class="source-code">               epochs,</p><p class="source-code">               steps_per_epoch):</p><p class="source-code">        optimizer = </p><p class="source-code">              tf.optimizers.Adam(learning_rate=2e-2,</p><p class="source-code">                                       beta_1=0.99,</p><p class="source-code">                                       epsilon=0.1)</p><p class="source-code">        for _ in range(epochs):</p><p class="source-code">            for _ in range(steps_per_epoch):</p><p class="source-code">                with tf.GradientTape() as tape:</p><p class="source-code">                    outputs = </p><p class="source-code">                         self._calc_outputs(image)</p><p class="source-code">                    loss = </p><p class="source-code">                      self._calc_total_loss(outputs,</p><p class="source-code">                                           s_targets,</p><p class="source-code">                                          c_targets)</p><p class="source-code">                gradient = tape.gradient(loss, image)</p><p class="source-code">                optimizer.apply_gradients([(gradient, </p><p class="source-code">                                            image)])</p><p class="source-code">                image.assign(self._clip_0_1(image))</p><p class="source-code">        return image </p></li>
				<li>Define a <a id="_idIndexMarker311"/>static helper private method that will convert a tensor into a <strong class="source-inline">NumPy</strong> image:<p class="source-code">    @staticmethod</p><p class="source-code">    def _tensor_to_image(tensor):</p><p class="source-code">        tensor = tensor * 255</p><p class="source-code">        tensor = np.array(tensor, dtype=np.uint8)</p><p class="source-code">        if np.ndim(tensor) &gt; 3:</p><p class="source-code">            tensor = tensor[0]</p><p class="source-code">        return tensor</p></li>
				<li>Finally, define a public <strong class="source-inline">transfer()</strong> method that will take a style image and a content image and <a id="_idIndexMarker312"/>generate a new image. This should preserve the content as much as possible while still applying the style of the style image: <p class="source-code">    def transfer(self, s_image, c_image, epochs=10,</p><p class="source-code">                 steps_per_epoch=100):</p><p class="source-code">        s_targets = self._calc_outputs(s_image)['style']</p><p class="source-code">        c_targets = </p><p class="source-code">          self._calc_outputs(c_image)['content']</p><p class="source-code">        image = tf.Variable(c_image)</p><p class="source-code">        image = self._train(image,</p><p class="source-code">                            s_targets,</p><p class="source-code">                            c_targets,</p><p class="source-code">                            epochs,</p><p class="source-code">                            steps_per_epoch)</p><p class="source-code">        return self._tensor_to_image(image)</p></li>
			</ol>
			<p>That was a lot of work! We'll go a bit deeper in the next section.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor157"/>How it works…</h2>
			<p>In this recipe, we learned that Neural Style Transfer works by optimizing two losses instead of one. On one hand, we want to preserve the content as much as possible, but on the other hand, we want to make this content look like it was produced using the style of the style image. </p>
			<p>Quantifying content is <a id="_idIndexMarker313"/>achieved by using the content layers, as we would normally do in image classification. How do we quantify style, though? Here's where the <strong class="bold">Gram Matrix</strong> plays a crucial role, since it computes the correlations across the feature maps (more precisely, the outputs) of the style layers. </p>
			<p>How do we inform the network that the content is more important than the style? By using weights when computing the combined loss. By default, the content weight is <em class="italic">10,000</em>, while the style weight is just <em class="italic">0.01</em>. This tells the network that most of its effort should be on reproducing the content, but also optimizing it a bit for style.</p>
			<p>In the end, we obtained an image that preserves the coherence of the original one, but with the visual appeal of the style reference image, which is the result of optimizing the output so that it matches the statistics of both input images.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor158"/>See also</h2>
			<p>If you want to learn more the math behind <a id="_idIndexMarker314"/>the <strong class="bold">Gram Matrix</strong>, go to <a href="https://encyclopediaofmath.org/wiki/Gram_matrix">https://encyclopediaofmath.org/wiki/Gram_matrix</a>. To see <strong class="source-inline">StyleTransferrer()</strong> in action, see the next recipe, <em class="italic">Applying style transfer to custom images</em>.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor159"/>Applying style transfer to custom images</h1>
			<p>Have you ever <a id="_idIndexMarker315"/>wondered how a picture of your puppy Fluffy<a id="_idIndexMarker316"/> would look if your favorite artist painted it? What if a photo of your car was the product of merging it with the magic of your most beloved painting? Well, you don't have to wonder anymore! With Neural Style Transfer, we can make our favorite images look like wonderful pieces of art effortlessly! </p>
			<p>In this recipe, we'll use the <strong class="source-inline">StyleTransferrer()</strong> class we implemented in the <em class="italic">Implementing Neural Style Transfer</em> recipe to stylize our own images.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor160"/>Getting ready</h2>
			<p>In this recipe, we'll be using the <strong class="source-inline">StyleTransferrer()</strong> implementation from the previous recipe. In <a id="_idIndexMarker317"/>order to maximize the fun you'll get out of<a id="_idIndexMarker318"/> this recipe, you can find the sample image, along with many different paintings (which you can use as the style reference), here: </p>
			<p><a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe4">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe4</a>.</p>
			<p>The following is the sample image we'll be using:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B14768_04_006.jpg" alt="Figure 4.6 – Sample content image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – Sample content image</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor161"/>How to do it…</h2>
			<p>The following steps will teach you how to transfer the style of famous paintings to your own images:</p>
			<ol>
				<li value="1">Import the necessary packages:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from chapter4.recipe3.styletransfer import StyleTransferrer</p><p>Notice we're importing <strong class="source-inline">StyleTransferrer()</strong>, which we implemented in the <em class="italic">Implementing Neural Style Transfer</em> recipe.</p></li>
				<li>Tell <a id="_idIndexMarker319"/>TensorFlow that we want to run in eager <a id="_idIndexMarker320"/>mode because otherwise, it will try to run the <strong class="source-inline">tf.function</strong> decorator functions in <strong class="source-inline">StyleTransferrer()</strong> in graph mode, which will prevent it from working properly:<p class="source-code">tf.config.experimental_run_functions_eagerly(True)</p></li>
				<li>Define a function that will load an image as a TensorFlow tensor. Notice that we're rescaling it to a sensible size. We are doing this because Neural Style Transfer is a<a id="_idIndexMarker321"/> resource-intensive process, so working on large images can take a long time:<p class="source-code">def load_image(image_path):</p><p class="source-code">    dimension = 512</p><p class="source-code">    image = tf.io.read_file(image_path)</p><p class="source-code">    image = tf.image.decode_jpeg(image, channels=3)</p><p class="source-code">    image = tf.image.convert_image_dtype(image, </p><p class="source-code">                                         tf.float32)</p><p class="source-code">    shape = tf.cast(tf.shape(image)[:-1], tf.float32)</p><p class="source-code">    longest_dimension = max(shape)</p><p class="source-code">    scale = dimension / longest_dimension</p><p class="source-code">    new_shape = tf.cast(shape * scale, tf.int32)</p><p class="source-code">    image = tf.image.resize(image, new_shape)</p><p class="source-code">    return image[tf.newaxis, :]</p></li>
				<li>Define a function that will display an image using <strong class="source-inline">matplotlib</strong>:<p class="source-code">def show_image(image):</p><p class="source-code">    if len(image.shape) &gt; 3:</p><p class="source-code">        image = tf.squeeze(image, axis=0)</p><p class="source-code">    plt.imshow(image)</p><p class="source-code">    plt.show()</p></li>
				<li>Load <a id="_idIndexMarker322"/>the content<a id="_idIndexMarker323"/> image and display it:<p class="source-code">content = load_image('bmw.jpg')</p><p class="source-code">show_image(content)</p><p>Here's the content image:</p><div id="_idContainer042" class="IMG---Figure"><img src="image/B14768_04_007.jpg" alt="Figure 4.7 – Content image of a car&#13;&#10;"/></div><p class="figure-caption">Figure 4.7 – Content image of a car</p><p>We'll apply the style of a painting to this image.</p></li>
				<li>Load and<a id="_idIndexMarker324"/> display the style image:<p class="source-code">style = load_image(art.jpg')</p><p class="source-code">show_image(style)</p><p>Here's<a id="_idIndexMarker325"/> the style image:</p><div id="_idContainer043" class="IMG---Figure"><img src="image/B14768_04_008.jpg" alt="Figure 4.8 – Style image&#13;&#10;"/></div><p class="figure-caption">Figure 4.8 – Style image</p><p>Can you imagine how <a id="_idIndexMarker326"/>our car would look if the artist of this painting painted it?</p></li>
				<li>Use <strong class="source-inline">StyleTransferrer()</strong> to apply the style of the painting to our image of a BMW. Then, display <a id="_idIndexMarker327"/>the result:<p class="source-code">stylized_image = StyleTransferrer().transfer(style, </p><p class="source-code">                                             content)</p><p class="source-code">show_image(stylized_image)</p><p>Here's the result:</p><div id="_idContainer044" class="IMG---Figure"><img src="image/B14768_04_009.jpg" alt="Figure 4.9 – Result of applying the style of the painting to the content image&#13;&#10;"/></div><p class="figure-caption">Figure 4.9 – Result of applying the style of the painting to the content image</p><p>Impressive, isn't it?</p></li>
				<li>Repeat<a id="_idIndexMarker328"/> this process, this time for 100 <a id="_idIndexMarker329"/>epochs:<p class="source-code">stylized_image = StyleTransferrer().transfer(style, </p><p class="source-code">                                             content,</p><p class="source-code">                                           epochs=100)</p><p class="source-code">show_image(stylized_image)</p><p>Here's the result:</p></li>
			</ol>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B14768_04_010.jpg" alt="Figure 4.10 – Result of applying the style of the painting to the content image for 100 epochs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Result of applying the style of the painting to the content image for 100 epochs</p>
			<p>This time, the result is sharper. However, we had to wait a while for the process to complete. There's a<a id="_idIndexMarker330"/> clear trade-off between time and quality.</p>
			<p>Let's move on<a id="_idIndexMarker331"/> to the next section.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor162"/>How it works…</h2>
			<p>In this recipe, we leveraged the hard work we did in the <em class="italic">Implementing Neural Style Transfer</em> recipe. We took an image of a car and applied the style of a cool and captivating piece of art to it. The result, as we saw, is fascinating.</p>
			<p>However, we must be aware of how taxing this process is since it takes a long time to complete on a CPU – even on a GPU. Therefore, there's a trade-off to be accounted for between the number of epochs or iterations used to refine the result and the overa<a id="_idTextAnchor163"/><a id="_idTextAnchor164"/>ll quality of the output. </p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor165"/>See also</h2>
			<p>I encourage you to try this recipe with your own pictures and styles. As a starting point, you can use the images in the following repository to hit the ground running: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe4">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe4</a>. There, you'll find famous artworks from Warhol, Matisse, and Monet, among others.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor166"/><a id="_idTextAnchor167"/>Applying style transfer with TFHub</h1>
			<p>Implementing Neural Style Transfer from scratch is a demanding task. Fortunately, we can use out-of-the-box solutions that live in <strong class="bold">TensorFlow Hub</strong> (<strong class="bold">TFHub</strong>).</p>
			<p>In this recipe, we'll<a id="_idIndexMarker332"/> style our own images in just <a id="_idIndexMarker333"/>a few lines of code by harnessing the utility and convenience that TFHub provides.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor168"/>Getting ready</h2>
			<p>We must install <strong class="source-inline">tensorflow-hub</strong>. We can do this with just a simple <strong class="source-inline">pip</strong> command:</p>
			<p class="source-code">$&gt; pip install tensorflow-hub</p>
			<p>If you want to access different sample content and style images, please visit this link: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe5">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe5</a>. </p>
			<p>Let's take a look at the sample image:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B14768_04_011.jpg" alt="Figure 4.11 – Content image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Content image</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor169"/>How to do it…</h2>
			<p>Neural Style Transfer with TFHub is a<a id="_idIndexMarker334"/> breeze! Follow<a id="_idIndexMarker335"/> these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the necessary dependencies:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow_hub import load</p></li>
				<li>Define a function that will load an image as a TensorFlow tensor. We need to rescale the image in order to save time and resources, given that Neural Style Transfer is a taxing process, so working on large images can take a long time:<p class="source-code">def load_image(image_path):</p><p class="source-code">    dimension = 512</p><p class="source-code">    image = tf.io.read_file(image_path)</p><p class="source-code">    image = tf.image.decode_jpeg(image, channels=3)</p><p class="source-code">    image = tf.image.convert_image_dtype(image, </p><p class="source-code">                                         tf.float32)</p><p class="source-code">    shape = tf.cast(tf.shape(image)[:-1], tf.float32)</p><p class="source-code">    longest_dimension = max(shape)</p><p class="source-code">    scale = dimension / longest_dimension</p><p class="source-code">    new_shape = tf.cast(shape * scale, tf.int32)</p><p class="source-code">    image = tf.image.resize(image, new_shape)</p><p class="source-code">    return image[tf.newaxis, :]</p></li>
				<li>Define <a id="_idIndexMarker336"/>a function that will <a id="_idIndexMarker337"/>convert a tensor into an image:<p class="source-code">def tensor_to_image(tensor):</p><p class="source-code">    tensor = tensor * 255</p><p class="source-code">    tensor = np.array(tensor, dtype=np.uint8)</p><p class="source-code">    if np.ndim(tensor) &gt; 3:</p><p class="source-code">        tensor = tensor[0]</p><p class="source-code">    return tensor</p></li>
				<li>Define a function that will display an image using <strong class="source-inline">matplotlib</strong>:<p class="source-code">def show_image(image):</p><p class="source-code">    if len(image.shape) &gt; 3:</p><p class="source-code">        image = tf.squeeze(image, axis=0)</p><p class="source-code">    plt.imshow(image)</p><p class="source-code">    plt.show()</p></li>
				<li>Define the path to the style transfer implementation in TFHub and load the model: <p class="source-code">module_url = ('https://tfhub.dev/google/magenta/'</p><p class="source-code">              'arbitrary-image-stylization-v1-256/2')</p><p class="source-code">hub_module = load(module_url)</p></li>
				<li>Load<a id="_idIndexMarker338"/> the content<a id="_idIndexMarker339"/> image. Then, display it:<p class="source-code">image = load_image('bmw.jpg')</p><p class="source-code">show_image(image)</p><p>Here it is:</p><div id="_idContainer047" class="IMG---Figure"><img src="image/B14768_04_012.jpg" alt="Figure 4.12 – Content image of a car&#13;&#10;"/></div><p class="figure-caption">Figure 4.12 – Content image of a car</p><p>We'll apply style transfer to this photo in the next step.</p></li>
				<li>Load<a id="_idIndexMarker340"/> and display the<a id="_idIndexMarker341"/> style image:<p class="source-code">style_image = load_image('art4.jpg')</p><p class="source-code">show_image(style_image)</p><p>Here, you can see the style image:</p><div id="_idContainer048" class="IMG---Figure"><img src="image/B14768_04_013.jpg" alt="Figure 4.13 – This is our style image of choice&#13;&#10;"/></div><p class="figure-caption">Figure 4.13 – This is our style image of choice</p><p>We'll pass this and the content image to the TFHub module we recently created and wait for the result.</p></li>
				<li>Apply Neural Style Transfer using the model we downloaded from TFHub and display the result:<p class="source-code">results = hub_module(tf.constant(image),</p><p class="source-code">                     tf.constant(style_image))</p><p class="source-code">stylized_image = tensor_to_image(results[0])</p><p class="source-code">show_image(stylized_image)</p><p>Here's the result of applying Neural Style Transfer with TFHub:</p></li>
			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B14768_04_014.jpg" alt="Figure 4.14 – Result of applying style transfer using TFHub&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14 – Result of applying style transfer using TFHub</p>
			<p><em class="italic">Voilà!</em> The<a id="_idIndexMarker342"/> result looks pretty good, don't you<a id="_idIndexMarker343"/> think? We'll dive a bit deeper in the next section.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor170"/>How it works…</h2>
			<p>In this recipe, we learned that using TFHub to stylize images is substantially easier than implementing the algorithm from scratch. However, it gives us less control since it acts as a black box. </p>
			<p>Either way, the result is quite satisfactory because it preserves the coherence and meaning of the original scene, while adding the artistic traits of the style image on top. </p>
			<p>The most important part is downloading the correct module from TFHub, and then loading it using the <strong class="source-inline">load()</strong> function. </p>
			<p>For the pre-packaged module to work, we must pass both the content and style images as <strong class="source-inline">tf.constant</strong> constants.</p>
			<p>Finally, because<a id="_idIndexMarker344"/> we received a tensor, in order<a id="_idIndexMarker345"/> to properly display the result on-screen, we used our custom function, <strong class="source-inline">tensor_to_image()</strong>, to turn it into a <strong class="source-inline">NumPy</strong> array that can easily be plotted using <strong class="source-inline">matplotlib</strong>.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor171"/>See also</h2>
			<p>You can read more about the TFHub module<a id="_idIndexMarker346"/> we used here at https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2.</p>
			<p>Also, why don't you play around with your own images and other styles? You can use the assets here as a starting point: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe5">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch4/recipe5</a>.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor172"/>Improving image resolution with deep learning</h1>
			<p><strong class="bold">Convolutional Neural Networks (CNNs)</strong> can <a id="_idIndexMarker347"/>also be used to improve the resolution of low-quality<a id="_idIndexMarker348"/> images. Historically, we can achieve this by using interpolation <a id="_idIndexMarker349"/>techniques, example-based approaches, or low- to high-resolution mappings that must be learned. </p>
			<p>As we'll see in this recipe, we can obtain better results faster by using an end-to-end deep learning-based approach. </p>
			<p>Sound interesting? Let's get to it!</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor173"/>Getting ready</h2>
			<p>We will need <strong class="source-inline">Pillow</strong> in this recipe, which you can install with the following command:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>In this recipe, we are using the <strong class="source-inline">Dog and Cat Detection</strong> dataset, which is hosted on Kaggle: <a href="https://www.kaggle.com/andrewmvd/dog-and-cat-detection">https://www.kaggle.com/andrewmvd/dog-and-cat-detection</a>. In order to download it, you'll need to sign in on the website or sign up. Once you're logged in, save it in a place of your preference as <strong class="source-inline">dogscats.zip</strong>. Finally, decompress it in a folder named <strong class="source-inline">dogscats</strong>. From now on, we'll assume the data is in <strong class="source-inline">~/.keras/datasets/dogscats</strong>. </p>
			<p>The<a id="_idIndexMarker350"/> following is a sample from the two<a id="_idIndexMarker351"/> classes in the dataset:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B14768_04_015.jpg" alt="Figure 4.15 – Example images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15 – Example images</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor174"/>How to do it…</h2>
			<p>Follow these steps to implement a fully convolutional network in order to perform image super-resolution:</p>
			<ol>
				<li value="1">Import all the necessary modules:<p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">from PIL import Image</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define a <a id="_idIndexMarker352"/>function that will build the <a id="_idIndexMarker353"/>network architecture. Notice that this is a fully convolutional network, which means only convolutional layers (besides the activations) comprise it, including the output:<p class="source-code">def build_srcnn(height, width, depth):</p><p class="source-code">    input = Input(shape=(height, width, depth))</p><p class="source-code">    x = Conv2D(filters=64, kernel_size=(9, 9),</p><p class="source-code">               kernel_initializer='he_normal')(input)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = Conv2D(filters=32, kernel_size=(1, 1),</p><p class="source-code">               kernel_initializer='he_normal')(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    output = Conv2D(filters=depth, kernel_size=(5, 5),</p><p class="source-code">                    kernel_initializer='he_normal')(x)</p><p class="source-code">    return Model(input, output)</p></li>
				<li>Define a function that will resize an image based on a scale factor. Take into consideration that it receives an image represented as a <strong class="source-inline">NumPy</strong> array:<p class="source-code">def resize_image(image_array, factor):</p><p class="source-code">    original_image = Image.fromarray(image_array)</p><p class="source-code">    new_size = np.array(original_image.size) * factor</p><p class="source-code">    new_size = new_size.astype(np.int32)</p><p class="source-code">    new_size = tuple(new_size)</p><p class="source-code">    resized = original_image.resize(new_size)</p><p class="source-code">    resized = img_to_array(resized)</p><p class="source-code">    resized = resized.astype(np.uint8)</p><p class="source-code">    return resized</p></li>
				<li>Define a<a id="_idIndexMarker354"/> function that will tightly<a id="_idIndexMarker355"/> crop an image. We are doing this because we want the image to fit nicely when we apply a sliding window to extract patches later. <strong class="source-inline">SCALE</strong> is the factor we want the network to learn how to enlarge images by:<p class="source-code">def tight_crop_image(image):</p><p class="source-code">    height, width = image.shape[:2]</p><p class="source-code">    width -= int(width % SCALE)</p><p class="source-code">    height -= int(height % SCALE)</p><p class="source-code">    return image[:height, :width]</p></li>
				<li>Define a function that will purposely reduce the resolution of an image by downsizing it and then upsizing it:<p class="source-code">def downsize_upsize_image(image):</p><p class="source-code">    scaled = resize_image(image, 1.0 / SCALE)</p><p class="source-code">    scaled = resize_image(scaled, SCALE / 1.0)</p><p class="source-code">    return scaled</p></li>
				<li>Define a function that will crop patches from input images. <strong class="source-inline">INPUT_DIM</strong> is the height and width of the images we will feed into the network:<p class="source-code">def crop_input(image, x, y):</p><p class="source-code">    y_slice = slice(y, y + INPUT_DIM)</p><p class="source-code">    x_slice = slice(x, x + INPUT_DIM)</p><p class="source-code">    return image[y_slice, x_slice]</p></li>
				<li>Define a function that will crop patches of output images. <strong class="source-inline">LABEL_SIZE</strong> is the height and width of the images outputted by the network. On the other hand, <strong class="source-inline">PAD</strong> is the <a id="_idIndexMarker356"/>number of pixels that will<a id="_idIndexMarker357"/> be used as padding to ensure we are cropping the region of interest properly:<p class="source-code">def crop_output(image, x, y):</p><p class="source-code">    y_slice = slice(y + PAD, y + PAD + LABEL_SIZE)</p><p class="source-code">    x_slice = slice(x + PAD, x + PAD + LABEL_SIZE)</p><p class="source-code">    return image[y_slice, x_slice]</p></li>
				<li>Set the random seed:<p class="source-code">SEED = 999</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Load the paths to all the images in the dataset:<p class="source-code">file_patten = (pathlib.Path.home() / '.keras' / </p><p class="source-code">               'datasets' /</p><p class="source-code">               'dogscats' / 'images' / '*.png')</p><p class="source-code">file_pattern = str(file_patten)</p><p class="source-code">dataset_paths = [*glob(file_pattern)]</p></li>
				<li>Because the dataset is huge and we don't need all the images in it to achieve our goal, let's randomly pick 1,500 of them:<p class="source-code">SUBSET_SIZE = 1500</p><p class="source-code">dataset_paths = np.random.choice(dataset_paths, </p><p class="source-code">                                 SUBSET_SIZE)</p></li>
				<li>Define the<a id="_idIndexMarker358"/> parameters that will be<a id="_idIndexMarker359"/> used to create our dataset of low-resolution patches as input and high-resolution patches (the labels) as output. All of these parameters were defined in previous steps, except for <strong class="source-inline">STRIDE</strong>, which is the number of pixels we'll slide both in the horizontal and vertical axes to extract patches:<p class="source-code">SCALE = 2.0</p><p class="source-code">INPUT_DIM = 33</p><p class="source-code">LABEL_SIZE = 21</p><p class="source-code">PAD = int((INPUT_DIM - LABEL_SIZE) / 2.0)</p><p class="source-code">STRIDE = 14</p></li>
				<li>Build the dataset. The inputs will be low-resolution patches that have been extracted from the images after being downsized and upsized. The labels will be patches from the unaltered image:<p class="source-code">data = []</p><p class="source-code">labels = []</p><p class="source-code">for image_path in dataset_paths:</p><p class="source-code">    image = load_img(image_path)</p><p class="source-code">    image = img_to_array(image)</p><p class="source-code">    image = image.astype(np.uint8)</p><p class="source-code">    image = tight_crop_image(image)</p><p class="source-code">    scaled = downsize_upsize_image(image)</p><p class="source-code">    height, width = image.shape[:2]</p><p class="source-code">    for y in range(0, height - INPUT_DIM + 1, STRIDE):</p><p class="source-code">        for x in range(0, width - INPUT_DIM + 1, STRIDE):</p><p class="source-code">            crop = crop_input(scaled, x, y)</p><p class="source-code">            target = crop_output(image, x, y)</p><p class="source-code">            data.append(crop)</p><p class="source-code">            labels.append(target)</p><p class="source-code">data = np.array(data)</p><p class="source-code">labels = np.array(labels)</p></li>
				<li>Instantiate the<a id="_idIndexMarker360"/> network, which we'll train<a id="_idIndexMarker361"/> for 12 epochs while using <strong class="source-inline">Adam()</strong> as our optimizer with learning rate decay. The loss function is <strong class="source-inline">'mse'</strong>. Why? Because our goal is not to achieve great accuracy, but to learn a set of filters that correctly map patches from low to high resolution:<p class="source-code">EPOCHS = 12</p><p class="source-code">optimizer = Adam(lr=1e-3, decay=1e-3 / EPOCHS)</p><p class="source-code">model = build_srcnn(INPUT_DIM, INPUT_DIM, 3)</p><p class="source-code">model.compile(loss='mse', optimizer=optimizer)</p></li>
				<li>Train the network:<p class="source-code">BATCH_SIZE = 64</p><p class="source-code">model.fit(data, labels, batch_size=BATCH_SIZE, </p><p class="source-code">          epochs=EPOCHS)</p></li>
				<li>Now, to evaluate our solution, we'll load a test image, convert it into a <strong class="source-inline">NumPy</strong> array, and reduce its resolution:<p class="source-code">image = load_img('dogs.jpg')</p><p class="source-code">image = img_to_array(image)</p><p class="source-code">image = image.astype(np.uint8)</p><p class="source-code">image = tight_crop_image(image)</p><p class="source-code">scaled = downsize_upsize_image(image)</p></li>
				<li>Display<a id="_idIndexMarker362"/> the low-resolution <a id="_idIndexMarker363"/>image:<p class="source-code">plt.title('Low resolution image (Downsize + Upsize)')</p><p class="source-code">plt.imshow(scaled)</p><p class="source-code">plt.show()</p><p>Let's see the result:</p><div id="_idContainer051" class="IMG---Figure"><img src="image/B14768_04_016.jpg" alt="Figure 4.16 – Low-resolution test image&#13;&#10;"/></div><p class="figure-caption">Figure 4.16 – Low-resolution test image</p><p>Now, we<a id="_idIndexMarker364"/> want to create a sharper<a id="_idIndexMarker365"/> version of this photo.</p></li>
				<li>Create a canvas with the same dimensions of the input image. This is where we'll store the high-resolution patches generated by the network:<p class="source-code">output = np.zeros(scaled.shape)</p><p class="source-code">height, width = output.shape[:2]</p></li>
				<li>Extract low-resolution patches, pass them through the network to obtain their high-resolution counterparts, and place them in their proper location in the output canvas:<p class="source-code">for y in range(0, height - INPUT_DIM + 1, LABEL_SIZE):</p><p class="source-code">    for x in range(0, width - INPUT_DIM + 1, LABEL_SIZE):</p><p class="source-code">        crop = crop_input(scaled, x, y)</p><p class="source-code">        image_batch = np.expand_dims(crop, axis=0)</p><p class="source-code">        prediction = model.predict(image_batch)</p><p class="source-code">        new_shape = (LABEL_SIZE, LABEL_SIZE, 3)</p><p class="source-code">        prediction = prediction.reshape(new_shape)</p><p class="source-code">        output_y_slice = slice(y + PAD, y + PAD + </p><p class="source-code">                               LABEL_SIZE)</p><p class="source-code">        output_x_slice = slice(x + PAD, x + PAD + </p><p class="source-code">                              LABEL_SIZE)</p><p class="source-code">        output[output_y_slice, output_x_slice] = </p><p class="source-code">                                 prediction</p></li>
				<li>Finally, display<a id="_idIndexMarker366"/> the high-resolution<a id="_idIndexMarker367"/> result:<p class="source-code">plt.title('Super resolution result (SRCNN output)')</p><p class="source-code">plt.imshow(output / 255)</p><p class="source-code">plt.show()</p><p>Here's the super-resolution output:</p></li>
			</ol>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B14768_04_017.jpg" alt="Figure 4.17 – High-resolution test image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.17 – High-resolution test image</p>
			<p>Compared <a id="_idIndexMarker368"/>to the low-resolution image, this<a id="_idIndexMarker369"/> photo does a better job of detailing the dogs and the overall scene, don't you think?</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">I recommend that you open both the low- and high-resolution images in a PDF or photo viewer. This will help you closely examine the differences between them and convince yourself that the network did its job well. It can be hard to judge the distinction in the print version of this book.</p>
			<h2 id="_idParaDest-151"><a id="_idTextAnchor175"/>How it works…</h2>
			<p>In this recipe, we <a id="_idIndexMarker370"/>created a model capable of improving the resolution of a blurry or low resolution image. The biggest takeaway of this <a id="_idIndexMarker371"/>implementation is that it is powered by a <strong class="bold">fully convolutional neural network</strong>, meaning <a id="_idIndexMarker372"/>that it comprises only convolutional layers and their activations. </p>
			<p>This is a regression problem, where each pixel in the output is a feature we want to learn.</p>
			<p>However, our goal is not to optimize for accuracy, but to train the model so the feature maps encode the necessary information to produce high-resolution patches from low-resolution ones.</p>
			<p>Now, we must ask ourselves: why patches? We don't want to <em class="italic">learn</em> what's in the image. Instead, again, we want our network to figure out how to go from low to high resolution. Patches are good enough for this purpose as they enclose localized patterns that are easier to grasp.</p>
			<p>You might have noticed that we didn't train for many epochs (only 12). This is by design because it's been shown that training for too long can actually hurt the network's performance.</p>
			<p>Finally, it must be noted that because this network was trained on images of dogs and cats, its expertise lies in upscaling photos of these animals. Nonetheless, by switching the dataset, we can easily create a super-resolution network that specializes in other kind of data.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor176"/>See also</h2>
			<p>Our implementation is based on the great work of Dong et al., whose paper on the subject can be read here: <a href="https://arxiv.org/abs/1501.00092">https://arxiv.org/abs/1501.00092</a>.</p>
		</div>
	</body></html>