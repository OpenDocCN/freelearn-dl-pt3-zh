- en: 6\. Regularization and Hyperparameter Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to hyperparameter tuning. You will get
    hands-on experience in using TensorFlow to perform regularization on deep learning
    models to reduce overfitting. You will explore concepts such as L1, L2, and dropout
    regularization. Finally, you will look at the Keras Tuner package for performing
    automatic hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will be able to apply regularization and tune
    hyperparameters in order to reduce the risk of overfitting your model and improve
    its performance.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned how classification models can solve problems
    when the response variable is discrete. You also saw different metrics used to
    assess the performance of such classifiers. You got hands-on experience in building
    and training binary, multi-class, and multi-label classifiers with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'When evaluating a model, you will face three different situations: model overfitting,
    model underfitting, and model performing. The last one is the ideal scenario,
    in which a model is accurately predicting the right outcome and is generalizing
    to unseen data well.'
  prefs: []
  type: TYPE_NORMAL
- en: If a model is underfitting, it means it is neither achieving satisfactory performance
    nor accurately predicting the target variable. In this case, a data scientist
    can try tuning different hyperparameters and finding the best combination that
    will boost the accuracy of the model. Another possibility is to improve the input
    dataset by handling issues such as the cleanliness of the data or feature engineering.
  prefs: []
  type: TYPE_NORMAL
- en: A model is overfitting when it can only achieve high performance on the training
    set and performs poorly on the test set. In this case, the model has only learned
    patterns from the data relevant to the data used for training. Regularization
    helps to lower the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main goal of a data scientist is to train a model that achieves high performance
    and generalizes to unseen data well. The model should be able to predict the right
    outcome on both data used during the training process and new data. This is the
    reason why a model is always assessed on the test set. This set of data serves
    as a proxy to evaluate the ability of the model to output correct results while
    in production.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Model not overfitting or underfitting'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_06_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.1: Model not overfitting or underfitting'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6.1*, the linear model (line) seems to predict relatively accurate
    results for both the training (circles) and test (triangles) sets.
  prefs: []
  type: TYPE_NORMAL
- en: But sometimes a model fails to generalize well and will overfit the training
    set. In this case, the performance of the model will be very different between
    the training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2: Model overfitting'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2: Model overfitting'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.2* shows the model (line) has only learned to predict accurately
    for the training set (circles) and is performing badly on the test set (triangles).
    This model is clearly overfitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are **regularization techniques** that a data scientist can
    use to reduce and prevent overfitting, defined in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: L1 Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For deep learning models, overfitting happens when some of the features have
    higher weights than they should. The model puts too much emphasis on these features
    as it believes they are extremely important for predicting the training set. Unfortunately,
    these features are less relevant for the test set or any new unseen data. Regularization
    techniques try to penalize such weights and reduce their importance to the model
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to perform regularization. One of them is to add a
    regularization component to the cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Adding a regularization component to the cost function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3: Adding a regularization component to the cost function'
  prefs: []
  type: TYPE_NORMAL
- en: The addition of this regularization component will lead the weights of the model
    to be smaller as neural networks try to reduce the cost function while performing
    forward and backward propagations.
  prefs: []
  type: TYPE_NORMAL
- en: 'One very popular regularization component is L1\. Its formula is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: L1 regularization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.4: L1 regularization'
  prefs: []
  type: TYPE_NORMAL
- en: '![Formula](img/B16341_06_04a.png) is a hyperparameter that defines the level
    of penalization of the L1 regularization. `W` is the weight of the model. With
    L1 regularization, you add the sum of the absolute value of the weights to the
    model loss.'
  prefs: []
  type: TYPE_NORMAL
- en: L1 regularization is sometimes referred to as `0`. Therefore, only the relevant
    features are used for making predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow, you can define L1 regularization with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `l` parameter corresponds to the ![Formula 2](img/B16341_06_04b.png) hyperparameter.
    The instantiated L1 regularization can then be added to any layer from TensorFlow
    Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, you added the L1 regularizer that you defined earlier
    to a fully connected layer of `10` units.
  prefs: []
  type: TYPE_NORMAL
- en: L2 Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*L2* regularization is similar to *L1* in that it adds a regularization component
    to the cost function, but its formula is different:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: L2 regularization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_06_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.5: L2 regularization'
  prefs: []
  type: TYPE_NORMAL
- en: L2 regularization tends to decrease the weights of the non-relevant features.
    They will be close to `0`, but not exactly `0`. So, it reduces the impact of these
    features but does not disable them as L1 does.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow, you can define L2 regularization as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, you defined an L2 regularizer and added it to a fully
    connected layer of `20` units.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow provides another regularizer class that combines both L1 and L2
    regularizers. You can instantiate it with the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, you instantiated L1 and L2 regularizers and specified
    the factors for L1 and L2 as `0.01` and `0.001`, respectively. You can observe
    that more weights are put on the L1 regularization compared to L2\. These values
    are hyperparameters that can be fine-tuned depending on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, you will put this into practice as you apply L2 regularization
    to a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.01: Predicting a Connect-4 Game Outcome Using the L2 Regularizer'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, you will build and train two multi-class models in TensorFlow
    that will predict the class outcome for player one in the game Connect-4\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each observation of this dataset contains different situations of the game
    with different positions. For each of these situations, the model tries to predict
    the outcome for the first player: win, loss, or draw. The first model will not
    have any regularization, while the second will have L2 regularization:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be accessed here: [https://packt.link/xysRc](https://packt.link/xysRc).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original dataset can be found here: [http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the pandas library and use `pd` as the alias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `file_url` that contains the URL to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset into a DataFrame called `data` using the `read_csv()` function
    and provide the URL to the CSV file. Print the first five rows using the `head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6: First five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.6: First five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding figure shows the first five rows of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the target variable (the `class` column) using the `pop()` method and
    save it in a variable named `target`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the TensorFlow library and use `tf` as the alias. Then, import the `Dense`
    class from `tensorflow.keras.layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed as `8` to get reproducible results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a sequential model using `tf.keras.Sequential()` and store it in
    a variable called `model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a fully connected layer of `512` units with `Dense()` and specify ReLu
    as the activation function and the input shape as `(42,)`, which corresponds to
    the number of features from the dataset. Save it in a variable called `fc1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create three fully connected layers of `512`, `128`, and `128` units with `Dense()`
    and specify ReLu as the activation function. Save them in three variables, called
    `fc2`, `fc3`, and `fc4`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a fully connected layer of three units (corresponding to the number
    of classes) with `Dense()` and specify softmax as the activation function. Save
    it in a variable called `fc5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sequentially add all five fully connected layers to the model using the `add()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the model using the `summary()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7: Summary of the model architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.7: Summary of the model architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Instantiate a `SparseCategoricalCrossentropy()` function from `tf.keras.losses`
    and save it in a variable called `loss`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `Adam()` from `tf.keras.optimizers` with `0.001` as the learning
    rate and save it in a variable called `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model using the `compile()` method, and specify the optimizer and
    loss you created in *steps 14* and *15* and `accuracy` as the metric to be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the model training process using the `fit()` method for five epochs and
    split the data into a validation set with 20% of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.8: Logs of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.8: Logs of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding output reveals that the model is overfitting. It achieved an accuracy
    score of `0.85` on the training set and only `0.58` on the validation set. Now,
    train another model with L2 regularization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create five fully connected layers similar to the previous model''s and specify
    the L2 regularizer for the `kernel_regularizer` parameters. Use the value `0.001`
    for the regularizer factor. Save the layers in five variables, called `reg_fc1`,
    `reg_fc2`, `reg_fc3`, `reg_fc4`, and `reg_fc5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a sequential model using `tf.keras.Sequential()`, store it in a
    variable called `model2`, and add sequentially all five fully connected layers
    to the model using the `add()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9: Summary of the model architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.9: Summary of the model architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Compile the model using the `compile()` method, and specify the optimizer and
    loss you created in *steps 14* and *15* and `accuracy` as the metric to be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the model training process using the `fit()` method for five epochs and
    split the data into a validation set with 20% of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10: Logs of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.10: Logs of the training process'
  prefs: []
  type: TYPE_NORMAL
- en: With the addition of L2 regularization, the model now has similar accuracy scores
    between the training (`0.68`) and test (`0.58`) sets. The model is not overfitting
    as much as before, but its performance is not great.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to apply L1 and L2 regularization to neural networks,
    the next section will introduce another regularization technique, called **dropout**.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout Regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unlike L1 and L2 regularization, dropout is a regularization technique specific
    to neural networks. The logic behind it is very simple: the networks will randomly
    change the weights of some features to `0`. This will force the model to rely
    on other features that would have been ignored and, therefore, bump up their weights.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11: Dropout of neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_06_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.11: Dropout of neural networks'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding example shows an architecture with a dropout of 50%. This means
    that 50% of the units of the model are turned off at each iteration. The following
    code snippet shows you how to create a dropout layer of 50% in TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In the next exercise, you will extend the previous model by applying dropout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.02: Predicting a Connect-4 Game Outcome Using Dropout'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will be using the same dataset as for *Exercise 6.01*,
    *Predicting a Connect-4 Game Outcome Using the L2 Regularizer*. You will build
    and train a multi-class model in TensorFlow that will predict the class outcome
    for player 1 in the game Connect-4 using the dropout technique as a regularizer:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be accessed here: [https://packt.link/0Bo1B](https://packt.link/0Bo1B).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original dataset can be found here: [http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the pandas library and use `pd` as the alias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable, `file_url`, to store the URL of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset into a DataFrame, `data`, using the `read_csv()` function
    and provide the URL of the CSV file. Print the first five rows using the `head()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.12: First five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.12: First five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the target variable (the column called `class`) using the `pop()` method,
    and save it in a variable called `target`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the TensorFlow library and use `tf` as the alias. Then, import the `Dense`
    class from `tensorflow.keras.layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed as `8` to get reproducible results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a sequential model using `tf.keras.Sequential()` and store it in
    a variable called `model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a fully connected layer of `512` units with `Dense()` and specify ReLu
    as the activation function and the input shape as `(42,)`, which corresponds to
    the number of features from the dataset. Save it in a variable called `fc1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create three fully connected layers of `512`, `128`, and `128` units with `Dense()`
    and specify ReLu as the activation function. Save them in three variables, called
    `fc2`, `fc3`, and `fc4`, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a fully connected layer of three units (corresponding to the number
    of classes) with `Dense()` and specify softmax as the activation function. Save
    it in a variable called `fc5`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sequentially add all five fully connected layers to the model with a dropout
    layer of `0.75` in between each of them using the `add()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the summary of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.13: Summary of the model architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.13: Summary of the model architecture'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Instantiate a `SparseCategoricalCrossentropy()` function from `tf.keras.losses`
    and save it in a variable called `loss`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `Adam()` from `tf.keras.optimizers` with `0.001` as the learning
    rate and save it in a variable called `optimizer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model using the `compile()` method, specify the optimizer and loss,
    and set `accuracy` as the metric to be displayed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Start the model training process using the `fit()` method for five epochs and
    split the data into a validation set with 20% of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.14: Logs of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.14: Logs of the training process'
  prefs: []
  type: TYPE_NORMAL
- en: With the addition of dropout, the model now has similar accuracy scores between
    the training (`0.69`) and test (`0.59`) sets. The model is not overfitting as
    much as before, but its performance is still less than ideal.
  prefs: []
  type: TYPE_NORMAL
- en: You have now seen how to apply L1, L2, or dropout as regularizers for a model.
    In deep learning, there is another very simple technique that you can apply to
    avoid overfitting—that is, early stopping.
  prefs: []
  type: TYPE_NORMAL
- en: Early Stopping
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Another reason why neural networks overfit is due to the training process. The
    more you train the model, the more it will try to improve its performance. By
    training the model for a longer duration (more epochs), it will at some point
    start finding patterns that are only relevant to the training set. In such a case,
    the difference between the scores of the training and test (or validation) sets
    will start increasing after a certain number of epochs.
  prefs: []
  type: TYPE_NORMAL
- en: To prevent this situation, you can stop the model training when the difference
    between the two sets starts to increase. This technique is called **early stopping**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15: Early stopping to prevent overfitting'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_06_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.15: Early stopping to prevent overfitting'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding graph shows the loss value of a model on the training and test
    (or validation) sets according to the number of epochs. In early epochs, the loss
    value is quite different between the two sets. As the training goes on, the models
    start learning the relevant patterns for making predictions and both losses converge.
    But after a while, they start diverging. The loss of the training set keeps decreasing
    while the one for the test (or validation) set is increasing. You can observe
    that the model is overfitting and is optimizing only for the training set. Stopping
    the training at the point when the difference between the two losses starts to
    increase prevents the model from overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow, you can achieve this by setting up callbacks that analyze the
    performance of the models at each epoch and compare its score between the training
    and test sets. To define an early stopping callback, you will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows you how to instantiate an `EarlyStopping` class that
    will monitor the accuracy score of the validation set and wait for five successive
    epochs with no improvement before stopping the training process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next activity, you will practice applying both L1 and L2 regularization
    to a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.01: Predicting Income with L1 and L2 Regularizers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `census-income-train.csv` dataset contains weighted census data extracted
    from the 1994 and 1995 current population surveys conducted by the US Census Bureau.
    The dataset is the subset of the original dataset shared by the US Census Bureau.
    In this activity, you are tasked with building and training a regressor to predict
    the income of a person based on their census data. The dataset can be accessed
    here: [https://packt.link/G8xFd](https://packt.link/G8xFd).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the required libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a list called `usecols` containing the column names `AAGE`, `ADTIND`,
    `ADTOCC`, `SEOTR`, `WKSWORK`, and `PTOTVAL`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the data using the `read_csv()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training (the first 15,000 rows) and test (the last 5,000
    rows) sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the multi-class classifier with five fully connected layers of, respectively,
    `512`, `512`, `128`, `128`, and `26` units.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model on the training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.16: Logs of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.16: Logs of the training process'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor269).
  prefs: []
  type: TYPE_NORMAL
- en: In the section ahead, you will see how to tune hyperparameters to achieve better results.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Previously, you saw how to deal with a model that is overfitting by using different
    regularization techniques. These techniques help the model to better generalize
    to unseen data but, as you have seen, they can also lead to inferior performance
    and make the model underfit.
  prefs: []
  type: TYPE_NORMAL
- en: With neural networks, data scientists have access to different hyperparameters
    they can tune to improve the performance of a model. For example, you can try
    different learning rates and see whether one leads to better results, you can
    try different numbers of units for each hidden layer of a network, or you can
    test to see whether different ratios of dropout can achieve a better trade-off
    between overfitting and underfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the choice of one hyperparameter can impact the effect of another
    one. So, as the number of hyperparameters and values you want to tune grows, the
    number of combinations to be tested will increase exponentially. It will also
    take a lot of time to train models for all these combinations—especially if you
    have to do it manually. There are some packages that can automatically scan the
    hyperparameter search space you defined and find the best combination overall
    for you. In the section ahead, you will see how to use one of them: Keras Tuner.'
  prefs: []
  type: TYPE_NORMAL
- en: Keras Tuner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unfortunately, this package is not included in TensorFlow. You will need to
    install it manually by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This package is very simple to use. There are two concepts to understand: **hyperparameters**
    and **tuners**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Hyperparameters are the classes used to define a parameter that will be assessed
    by the tuner. You can use different types of hyperparameters. The main ones are
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`hp.Boolean`: A choice between `True` and `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hp.Int`: A choice with a range of integers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hp.Float`: A choice with a range of decimals'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hp.Choice`: A choice within a list of possible values'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet shows you how to define a hyperparameter called
    `learning_rate` that can only take one of four values—`0.1`, `0.01`, `0.001`,
    or `0.0001`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'A tuner in the Keras Tuner package is an algorithm that will look at the hyperparameter
    search space, test some combinations, and find the one that gives the best result.
    The Keras Tuner package provides different tuners, and in the section ahead, you
    will look at three of them: **random search**, **Hyperband**, and **Bayesian optimization**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once defined with the algorithm of your choice, you can call the `search()`
    method to start the hyperparameter tuning process on the training and test sets,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the search is complete, you can access the best combination with `get_best_hyperparameters()`
    and then look specifically at one of the hyperparameters you defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the `hypermodel.build()` method will instantiate a TensorFlow Keras
    model with the best hyperparameters found:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: It's as simple as that. Now, let's have a look at the random search tuner.
  prefs: []
  type: TYPE_NORMAL
- en: Random Search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Random search is one of the available algorithms in this package. As its name
    implies, it randomly defines the combinations to be tested by sampling through
    the search space. Even though this algorithm doesn't test every single possible
    combination, random search provides very good results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm that tests every single combination of the search space is called
    grid search.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17: Comparison between grid search and random search'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_06_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.17: Comparison between grid search and random search'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding figure shows an example of the difference between grid search
    and random search. You can see that grid search splits the search space into a
    grid and tests each of the combinations, but some may lead to the same loss value,
    which makes it less efficient. On the other side, random search covers the search
    space more efficiently and helps find the optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras Tuner, before instantiating a tuner, you need to define a model-building
    function that will define the architecture of the TensorFlow Keras model to be
    trained with the hyperparameters you want to test. Here is an example of such
    a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code snippet, you created a model composed of three fully connected
    layers of `512`, `128`, and `10` units that will be trained with a categorical cross-entropy
    loss function and the Adam optimizer. You defined the `learning_rate` hyperparameter
    that will be assessed by Keras Tuner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model-building function is defined, you can instantiate a random search
    tuner like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, you instantiated a `RandomSearch` tuner that will look
    at the model and hyperparameters defined in the `model_builder` function using
    the validation accuracy as the `objective` metric and will run for a maximum of
    `10` trials.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, you will use random search to find the best set of hyperparameters
    for a model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.03: Predicting a Connect-4 Game Outcome Using Random Search from
    Keras Tuner'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will be using the same dataset as for *Exercise 6.01*,
    *Predicting a Connect-4 Game Outcome Using the L2 Regularizer*. You will build
    and train a multi-class model in TensorFlow that will predict the class outcome
    for player 1 in the game Connect-4 using the Keras Tuner package to find the best
    regularization factor for L2 regularization through random search:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be accessed here: [https://packt.link/aTSbC](https://packt.link/aTSbC).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original dataset can be found here: [http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the pandas library and use `pd` as the alias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `file_url` that contains the URL to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset into a DataFrame called `data` using the `read_csv()` method
    and provide the URL to the CSV file. Print the first five rows using the `head()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.18: First five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.18: First five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the target variable (the column called `class`) using the `pop()` method
    and save it in a variable called `target`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `train_test_split` from `sklearn.model_selection`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets using `train_test_split()`, with
    20% of the data for testing and `42` for `random_state`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the `kerastuner` package and then import it and assign it the `kt`
    alias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the TensorFlow library and use `tf` as the alias. Then, import the `Dense`
    class from `tensorflow.keras.layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed as `8` using `tf.random.set_seed()` to get reproducible results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function called `model_builder` that will create a sequential model
    with the same architecture as *Exercise 6.02*, *Predicting a Connect-4 Game Outcome
    Using Dropout*, with L2 regularization, but this time, provide an `hp.Choice`
    hyperparameter for the regularization factor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `RandomSearch` tuner and assign `val_accuracy` to `objective`
    and `10` to `max_trials`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Launch the hyperparameter search with the `search()` method on the training
    and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the best hyperparameter combination (index `0`) with `get_best_hyperparameters()`
    and save it in a variable called `best_hps`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the best value for the `l2` regularization hyperparameter, save it
    in a variable called `best_l2`, and print its value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following result:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The best value for the `l2` hyperparameter found by random search is `0.0001`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Start the model training process using the `fit()` method for five epochs and
    use the test set for `validation_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.19: Logs of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.19: Logs of the training process'
  prefs: []
  type: TYPE_NORMAL
- en: Using a random search tuner, you found the best value for L2 regularization
    (`0.0001`), which helped the model to achieve an accuracy of `0.83` on the training
    set and `0.81` on the test set. These scores are quite an improvement on those
    from *Exercise 6.01*, *Predicting a Connect-4 Game Outcome Using the L2 Regularizer*
    (`0.69` for the training set and `0.59` for the test set).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will use another Keras tuner, called Hyperband.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperband
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Hyperband is another tuner available in the Keras Tuner package. Like random
    search, it randomly picks candidates from the search space, but more efficiently.
    The idea behind it is to test a set of combinations for just one or two iterations,
    keeping only the best performers and training them for longer. So, the algorithm
    doesn''t waste time in training non-performing combinations as with random search.
    Instead, it simply discards them from the next run. Only the ones that achieve
    higher performance are kept for longer training. To instantiate a Hyperband tuner,
    execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: This tuner takes a model-building function and an objective metric as input
    parameters, as for random search. But it requires an additional one, `max_epochs`,
    corresponding to the maximum number of epochs a model is allowed to train for
    during the hyperparameter search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 6.04: Predicting a Connect-4 Game Outcome Using Hyperband from Keras
    Tuner'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, you will be using the same dataset as for *Exercise 6.01*,
    *Predicting a Connect-4 Game Outcome Using the L2 Regularizer*. You will build
    and train a multi-class model in TensorFlow that will predict the class outcome
    for player 1 in the game Connect-4 using the Keras Tuner package to find the best
    learning rate and the number of units for the input layer through Hyperband:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be accessed here: [https://packt.link/WLgen](https://packt.link/WLgen).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The original dataset can be found here: [http://archive.ics.uci.edu/ml/datasets/Connect-4](http://archive.ics.uci.edu/ml/datasets/Connect-4).'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the pandas library and use `pd` as the alias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a variable called `file_url` that contains the URL to the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset into a DataFrame called `data` using the `read_csv()` method
    and provide the URL to the CSV file. Print the first five rows using the `head()`
     method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.20: First five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.20: First five rows of the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the target variable (`class`) using the `pop()` method, and save it
    in a variable called `target`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import `train_test_split` from `sklearn.model_selection`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets using `train_test_split()`, with
    20% of the data for testing and `42` for `random_state`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the `keras-tuner` package, and then import it and assign it the `kt`
    alias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the TensorFlow library and use `tf` as the alias, and then import the
    `Dense` class from `tensorflow.keras.layers`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the seed as `8` using `tf.random.set_seed()` to get reproducible results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function called `model_builder` to create a sequential model with
    the same architecture as *Exercise 6.02*, *Predicting a Connect-4 Game Outcome
    Using Dropout*, with L2 regularization and a `0.0001` regularization factor. But,
    this time, provide a hyperparameter, `hp.Choice`, for the learning rate (`0.01`,
    `0.001`, or `0.0001`) and an `hp.Int` function for the number of units (between
    `128` and `512` with a step of `64`) for the input fully connected layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a Hyperband tuner, and assign `val_accuracy` to the `objective`
    metric and `5` to `max_epochs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Launch the hyperparameter search with `search()` on the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the best hyperparameter combination (index `0`) with `get_best_hyperparameters()`
    and save it in a variable called `best_hps`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the best value for the number of units for the input layer, save it
    in a variable called `best_units`, and print its value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The best value for the number of units of the input layer found by Hyperband
    is `192`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Extract the best value for the learning rate, save it in a variable called
    `best_lr`, and print its value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The best value for the learning rate hyperparameter found by Hyperband is `0.001`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Start the model training process using the `fit()` method for five epochs and
    use the test set for `validation_data`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.21: Logs of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.21: Logs of the training process'
  prefs: []
  type: TYPE_NORMAL
- en: Using Hyperband as the tuner, you found the best number of units for the input
    layer (`192`) and learning rate (`0.001`). With these hyperparameters, the final
    model achieved an accuracy of `0.81` on both the training and test sets. It is
    not overfitting much and achieved a satisfactory accuracy score.
  prefs: []
  type: TYPE_NORMAL
- en: Another very popular tuner is Bayesian optimization, which you will learn about
    in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Optimization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Bayesian optimization is another very popular algorithm used for automatic
    hyperparameter tuning. It uses probabilities to determine the best combination
    of hyperparameters. The objective is to iteratively build a probability model
    that optimizes the objective function from a set of hyperparameters. At each iteration,
    the probability model is updated from the results obtained. Therefore, unlike
    random search and Hyperband, Bayesian optimization takes past results into account
    to improve new ones. The following code snippet will show you how to instantiate
    a Bayesian optimizer in Keras Tuner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: The expected parameters are similar to random search, including the model-building
    function, the `objective` metric, and the maximum number of trials.
  prefs: []
  type: TYPE_NORMAL
- en: In the following activity, you will use Bayesian optimization to predict the
    income of a person.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 6.02: Predicting Income with Bayesian Optimization from Keras Tuner'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this activity, you will use the same dataset as used in *Activity 6.01*,
    *Predicting Income with L1 and L2 Regularizers*. You are tasked with building
    and training a regressor to predict the income of a person based on their census
    data. You will perform automatic hyperparameter tuning with Keras Tuner and find
    the best combination of hyperparameters for the learning rate, the number of units
    for the input layer, and L2 regularization with Bayesian optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps will help you to complete the activity:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the data with `read_csv()` from pandas.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the target variable with the `pop()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split the data into training (the first 15,000 rows) and test (the last 5,000
    rows) sets.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create the model-building function multi-class classifier with five fully connected
    layers of `512`, `512`, `128`, `128`, and `26` units and the three different hyperparameters
    to be tuned: the learning rate (between `0.01` and `0.001`), the number of units
    for the input layer (between `128` and `512` and a step of `64`), and L2 regularization
    (between `0.1`, `0.01`, and `0.001`).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find the best combination of hyperparameters with Bayesian optimization.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model on the training set with the best hyperparameters found.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The expected output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.22: Logs of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_06_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 6.22: Logs of the training process'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor270).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You started your journey in this chapter with an introduction to the different
    scenarios of training a model. A model is overfitting when its performance is
    much better on the training set than the test set. An underfitting model is one
    that can achieve good results only after training. Finally, a good model achieves
    good performance on both the training and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you encountered several regularization techniques that can help prevent
    a model from overfitting. You first looked at the L1 and L2 regularizations, which
    add a penalty component to the cost function. This additional penalty helps to
    simplify the model by reducing the weights of some features. Then, you went through
    two different techniques specific to neural networks: dropout and early stopping.
    Dropout randomly drops some units in the model architecture and forces it to consider
    other features to make predictions. Early stopping is a mechanism that automatically
    stops the training of a model once the performance of the test set starts to deteriorate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, you learned how to use the Keras Tuner package for automatic hyperparameter
    tuning. You considered three specific types of tuners: random search, Hyperband,
    and Bayesian optimization. You saw how to instantiate them, perform a hyperparameter
    search, and extract the best values and model. This process helped you to achieve
    better performance on the models trained for the exercises and activities.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn more about **Convolutional Neural Networks**
    (**CNNs**). Such architecture has led to groundbreaking results in computer vision
    in the past few years. The following chapter will show you how to use this architecture
    to recognize objects in images.
  prefs: []
  type: TYPE_NORMAL
