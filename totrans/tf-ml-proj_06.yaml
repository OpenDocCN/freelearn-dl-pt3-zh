- en: Predicting Stock Prices using Gaussian Process Regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about a new model for forecasting known as **Gaussian
    processes**, popularly abbreviated as **GPs**, this is extremely popular in forecasting
    applications where we want to model non-linear functions with a few data points
    and also to quantify uncertainty in predictions.
  prefs: []
  type: TYPE_NORMAL
- en: We will use Gaussian processes to predict the stock prices of three major stocks,
    namely, Google, Netflix, and the **General Electric** (GE) company.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this chapter is divided into the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Bayes' rule
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayesian inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Gaussian processes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the stock market dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying Gaussian processes to predict stock market prices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Bayes' rule
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let us begin by reviewing the Bayes' rule and it's associated terminology, before
    we start with our project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayes'' rule is used to describe the probability of an event, based on prior
    knowledge of conditions that might be related to the event. For example, let''s
    say we want to predict the probability a person having diabetes. If we know the
    preliminary medical test results, we can hope to get a more accurate prediction
    than when we don''t know results of the test. Let''s put some numbers around this
    to understand mathematically:'
  prefs: []
  type: TYPE_NORMAL
- en: 1% of population has diabetes ( and therefore 99% do not)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preliminary tests detect diabetes 80% of the time when it is there ( therefore
    20% of time we require advanced tests)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10% of time preliminary test detect diabetes even when it is not there (therefore
    90% of time they give the correct result):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|  | **Diabetes (1%)** | **No diabetes (99%)** |'
  prefs: []
  type: TYPE_TB
- en: '| **Test Positive** | 80% | 10% |'
  prefs: []
  type: TYPE_TB
- en: '| **Test Negative** | 20% | 90% |'
  prefs: []
  type: TYPE_TB
- en: So, if a person has diabetes, we will be looking at first column and he has
    80% chance of being detected. And if a person doesn't have diabetes, we will be
    looking at second column and he has 10% chance of testing positive for diabetes
    from preliminary tests.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's say a person was detected positively for diabetes from preliminary
    test. What are the chances that he actually has diabetes?
  prefs: []
  type: TYPE_NORMAL
- en: 'As it turns out, a renowned scientist Thomas Bayes'' ( 171-1761) provided a
    mathematical framework to compute probability in the cases like above. His mathematical
    formula can be given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/951a9bc4-f393-4cca-b9ec-ec069aa20acc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9b5a1e8a-8850-47f6-af12-bdfefc76b742.png) denotes the probability of
    diabetes in a randomly selected person which is 1% in this case.![](img/3ed237e2-b687-4dce-9b02-5cfc221d8105.png) is
    also known as **prior** in Bayesian terminology which denotes our belief for an
    event without any additional information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a8e2c941-82d0-4bce-8131-7003c7d02c84.png) denotes the probability of
    a person having diabetes given that he was detected positive by preliminary search
    results. In Bayesian terminology, this is also known as **posterior **which denotes
    the updated probability of an event after having obtained additional information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/2da3eb1b-c253-48d6-a05c-5bceeaef1d24.png) denotes the probability of
    getting a positive result in preliminary test given a person has diabetes. It
    is 80% in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/57855b2f-e2c4-4b39-980f-f4b984330f3c.png) denotes the probability that
    a random person will be tested positive in preliminary test. This can also be
    written down as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/651f55bb-405e-4212-8f20-32b7567eff2e.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/29bff235-bef5-4999-977b-54d6c0121efa.png)'
  prefs: []
  type: TYPE_IMG
- en: Bayes' rule is used heavily to quantify uncertainty in predictions of machine
    learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Bayesian inference
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know about the basics of Bayes' rule, let's try to understand the
    concept of Bayesian inference or modeling.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, real-world environments are always dynamic, noisy, observation costly,
    and time-sensitive. When business decisions are based on forecasting in these
    environments, we want to not only produce better forecasts, but also quantify
    the uncertainty in these forecasts. For this reason, the theory of Bayesian inferences
    is extremely handy as it provides a principled approach to such problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a typical time series model, we effectively carry out curve fitting based
    on *y* when given the *x* variable. This helps to fit a curve based on past observations.
    Let''s try to understand its limitations. Consider the following example of temperature
    in a city:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Day** | **Temperature** |'
  prefs: []
  type: TYPE_TB
- en: '| May 1 10 AM | 10.5 degrees Celsius |'
  prefs: []
  type: TYPE_TB
- en: '| May 15 10 AM | 17.5 degrees Celsius |'
  prefs: []
  type: TYPE_TB
- en: '| May 30 10 AM | 25 degrees Celsius |'
  prefs: []
  type: TYPE_TB
- en: 'Using curve fitting, we obtain the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ad60f2e-4859-40f2-9249-754813b4c20e.png)'
  prefs: []
  type: TYPE_IMG
- en: However, this will imply that the temperature function is linear, and, on tenth
    day, we expect the temperature to be 15 degrees Celsius. It is common knowledge
    that the temperature of a city fluctuates a lot during a day and it depends on
    when we take the readings. Curve fitting defines one of the functions over a given
    set of readings.
  prefs: []
  type: TYPE_NORMAL
- en: 'This example leads us to the conclusion that there is a family of curves that
    can model the given observations. The idea of the distribution of curves that
    model the given observation is central to Bayesian inference or modeling. The
    question now is: what should be the process of choosing one function over this
    family of functions? Or whether we should, in fact, choose one?'
  prefs: []
  type: TYPE_NORMAL
- en: One way to narrow down this family of functions is to short list a subset of
    them based on our prior knowledge about the problem. For example, we know that
    in May we don't expect temperatures to go below zero degrees Celsius. We can use
    this knowledge and discard all the functions which have points below zero degrees.
    Another popular way to think about this problem is to define a distribution over
    a function space based on our prior knowledge. Furthermore, the job of modeling,
    in this case, is to refine the distribution over possible functions based on the
    observed data points. Since these models don't have any defined parameters, they
    are popularly known as **Bayesian non-parametric models**.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Gaussian processes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Gaussian process (GP) can be thought of as an alternative Bayesian approach
    to regression problems. They are also referred to as infinite dimensional Gaussian
    distributions. GP defines a priori over functions that can be converted into a
    posteriori once we have observed a few data points. Although it doesn’t seem possible
    to define distributions over functions, it turns out that we only need to define
    distributions over a function's values at observed data points.
  prefs: []
  type: TYPE_NORMAL
- en: Formally, let's say that we observed a function, ![](img/a278cd36-da50-4984-b8a2-a5a31a0f41d5.png),
    at n values ![](img/8038a339-08c0-4bd9-bec2-d847bcdd5923.png) as ![](img/2de559fe-a00c-4832-acc5-2500e03fe6a6.png).
    The function is a GP if all of the values, ![](img/272f6482-2994-476c-95d1-50943eb6a3ea.png), are
    jointly Gaussian, with a mean of ![](img/694a4dd4-0161-4ce4-94bb-13b0eb5de4cd.png) and
    a covariance of ![](img/688c125d-6b48-472b-b56c-8840ca089f5b.png)  given by ![](img/0efeffbe-fa9f-424b-9683-1e937eb3cdec.png).
    Here, the ![](img/1c3d2b1a-ad87-4bf6-89ff-553cd3009dcf.png) function defines how
    two variables are related to each other. We will discuss different kinds of kernels
    later in this section. The joint Gaussian distribution of many Gaussian variables
    is also known as Multivariate Gaussian.
  prefs: []
  type: TYPE_NORMAL
- en: From the previous temperature example, we can imagine that various functions
    can be fit to the given observations on temperature. Some functions are smoother
    than others. One way to capture smoothness is by using the covariance matrix.
    The covariance matrix ensures that two values (![](img/2c19505a-d914-41fb-b347-50ac27de6216.png))
    close in the input space produce closer values in the output space (![](img/dff74ee8-c4bb-4b6e-b92b-df324c62a94c.png)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, the problem we are trying to solve through GP is as follows: given
    a set of inputs, ![](img/22a0a040-b0f1-4ee4-a7df-4d05f59026ab.png), and its values, ![](img/b78eee49-4717-466d-a15d-e549943f25c3.png),
    we are trying to estimate the distribution over outputs, ![](img/1a8e2103-1df5-43e0-b5b9-a0110238c6ed.png), for
    a new set of inputs, ![](img/1e052223-2b40-49aa-8eec-84799a90fb52.png). Mathematically,
    the quantity we are trying to estimate can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb393d3a-f09d-488f-8ad2-609f363feadb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'To obtain this, we model ![](img/f0780df4-977d-42b6-ad5b-cbcdbd14ea16.png) as
    a GP so that we know that both ![](img/8e69496e-df16-4a8c-8aff-03c2682fc80f.png) and
    ![](img/90734fc7-ecae-44f7-8d7a-a1414ba3a81f.png) are coming from a multivariate
    Gaussian with the following mean and covariance function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4effe6bc-fa12-4378-89ce-144622aec16e.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/1b140f99-0195-4a9c-a7f9-fadd2dc131fa.png) and ![](img/7c45a81a-67c0-4dae-971b-009496dafd32.png) represent
    the prior mean of the distribution of ![](img/f0780df4-977d-42b6-ad5b-cbcdbd14ea16.png) and ![](img/1a8e2103-1df5-43e0-b5b9-a0110238c6ed.png) over
    observed and unobserved function values, respectively, ![](img/26a39e62-f73f-45b1-8f00-383feb03aeca.png) represents
    the matrix obtained after applying the kernel function to each of the observed
    values, ![](img/22a0a040-b0f1-4ee4-a7df-4d05f59026ab.png).
  prefs: []
  type: TYPE_NORMAL
- en: Kernel function tries to map the similarity between two data points in the input
    space to the output space. Let's assume, there are two data points ![](img/52170400-b12c-4c42-bb24-a55a896d7743.png) and ![](img/8f98f264-1213-4dc6-902c-e0a1c367f07f.png) with
    corresponding function values as ![](img/d44d4d04-8614-4401-8287-44f8ac6a7d78.png) and ![](img/5fa623e9-d9e4-4274-aaf2-f1cd53fe6392.png).
    The Kernel function measures how the closeness between two points ![](img/52170400-b12c-4c42-bb24-a55a896d7743.png) and ![](img/8f98f264-1213-4dc6-902c-e0a1c367f07f.png) in
    the input space maps to the similarity or correlation between their function values ![](img/b7e763af-d663-46c2-ac24-12eaf5ad80d7.png) and ![](img/12d419c0-ad2c-4b7b-a0b6-63af38e88879.png).
  prefs: []
  type: TYPE_NORMAL
- en: We apply this kernel function to all the pairs of observations in the dataset,
    thereby, creating a matrix of similarities known as Kernel/Covariance matrix (*K*). Assuming,
    there are 10 input data points, the kernel function will be applied to each pair
    of data points leading to a 10x10 Kernel Matrix (*K*). If the function values
    at two data points ![](img/52170400-b12c-4c42-bb24-a55a896d7743.png) and ![](img/8f98f264-1213-4dc6-902c-e0a1c367f07f.png)
    is expected to be similar, kernel is expected to have high value at *(i,j)* in
    the matrix. We do a detailed discussion on different kernels in GPs in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: In the equation, ![](img/db2e0ad6-026c-43cc-86be-aeb8a27e9bef.png) represents
    the matrix obtained by applying the same kernel function to values in the training
    and testing dataset, and ![](img/8061a3b0-c2f5-4795-b4a8-d3d2a3c94b18.png) is
    the matrix obtained by measuring the similarity between the input values in the
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we will assume that there is some linear algebra magic that
    can help us to achieve the conditional distribution of ![](img/fb393d3a-f09d-488f-8ad2-609f363feadb.png) from
    the joint distribution and obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2639840-4335-48e2-840c-4ffe2865b2d2.png)'
  prefs: []
  type: TYPE_IMG
- en: We are going to skip the derivations, but if you would like to know more, you
    can visit Rasmussen and Williams ([http://www.gaussianprocess.org/gpml/chapters/RW.pdf](http://www.gaussianprocess.org/gpml/chapters/RW.pdf)).
  prefs: []
  type: TYPE_NORMAL
- en: With this analytical result, we have access to the entire distribution of function
    values over the testing dataset. Modeling the predictions as distributions also
    helps in quantifying the uncertainty surrounding the predictions, which is quite
    important in many time series applications.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing kernels in GPs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In many applications, we find that the prior mean is always set to zero as it
    is simple, convenient and works well for many applications. However, choosing
    the appropriate kernels for the task is not always straightforward. As mentioned
    in the previous section, kernels effectively try to map the similarity between
    two input data points in the input space to the output (function) space. The only
    requirement for the kernel function (![](img/66ba8791-225f-4f4e-93a7-079e2199a612.png))
    is that it should map any two input values ![](img/8f12aaf0-d55b-4194-b4d7-89dbc34c2dae.png) and ![](img/bfad4856-888b-4329-84c0-ba9970c3aadc.png) to
    a scalar such that Kernel Matrix (![](img/83a5b785-4c39-456a-8f54-52ba57395735.png))
    is a positive/semi-definite for it to be a valid covariance function.
  prefs: []
  type: TYPE_NORMAL
- en: For sake of brevity, we exclude explanation of fundamental concepts of Covariance
    matrices and how they are always positive semi-definite. We encourage the readers
    to refer to lecture notes from [MIT](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2008/lecture-notes/MIT6_436JF08_lec15.pdf)
    (https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-436j-fundamentals-of-probability-fall-2008/lecture-notes/MIT6_436JF08_lec15.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'While a complete discussion of all of the types of kernels is beyond the scope
    of this chapter, we will discuss the two kernels used to build this project:'
  prefs: []
  type: TYPE_NORMAL
- en: '**White noise kernel: **As the name suggests, the white noise kernel adds a
    white noise (of variance) to the existing covariance matrix. Mathematically, it
    is given as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/dba08700-b379-4d43-bdca-a513b15a2d9b.png)'
  prefs: []
  type: TYPE_IMG
- en: If there are many settings, the data points are not accurate and are corrupted
    by some random noise. Noise in the input data can be modeled by adding a white
    noise kernel to the covariance matrix.
  prefs: []
  type: TYPE_NORMAL
- en: '**Squared exponential (SE) kernel:** Given two scalars, ![](img/7d0effde-cd01-4263-a54c-7c95219de40a.png) and
    ![](img/2d724806-82a9-4682-85eb-865b58604c98.png), the squared exponential kernel
    is given by the following equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/360c9c14-b89e-4808-9583-3aa0d293ffe5.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/3c68e226-e471-4656-9a57-905bb111b4ff.png) is a scaling factor
    and ![](img/240d0539-3f80-4c0f-9079-a7efc4089974.png) which is the smoothness
    parameter determines the smoothness of kernel function. It is quite a popular
    kernel because the functions drawn from the GP through this kernel are infinitely
    differentiable, which makes it suitable for many applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some samples that have been drawn from a GP with an SE kernel that
    has ![](img/3c68e226-e471-4656-9a57-905bb111b4ff.png) fixed to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eefbb334-ce35-454c-b87d-1b8ac9cdacc6.png)'
  prefs: []
  type: TYPE_IMG
- en: We can observe that the functions become smoother as ![](img/c616a4ff-2e8f-4c08-9ce5-816548dadc00.png) increases.
    For more information on different kinds of kernels, refer to *The Kernel Cookbook*
    ([https://www.cs.toronto.edu/~duvenaud/cookbook/](https://www.cs.toronto.edu/~duvenaud/cookbook/)https://www.cs.toronto.edu/~duvenaud/cookbook/)
  prefs: []
  type: TYPE_NORMAL
- en: Choosing the hyper parameters of a kernel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have defined kernels with different parameters. For example, in a
    squared exponential kernel, we have the parameters ![](img/e2c9b8c5-d9d5-4682-ad41-55184f6e0f33.png) and ![](img/a76bfe27-56de-458c-afc8-d2f256a4a6ee.png).
    Let's denote the parameter set of any kernel as ![](img/0c20142e-ae5a-453a-83f3-4da976648679.png).
    The question now is, how do we estimate ![](img/4f6c6973-0029-47aa-b77a-23a0aab277e6.png)?
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned previously, we model the distribution of the output of ![](img/93fd2135-8859-4e8d-80d0-ecbbcb8d9237.png) function to
    be a random sample from a multivariate Gaussian distribution. In this manner,
    the marginal likelihood of observed data points is a multivariate Gaussian that's
    been conditioned on the input points ![](img/4b1002f7-07c2-4b86-bc5e-a630fc9b13d1.png) and
    the parameter ![](img/4f6c6973-0029-47aa-b77a-23a0aab277e6.png). Thus, we can
    choose ![](img/4f6c6973-0029-47aa-b77a-23a0aab277e6.png) by maximizing the likelihood
    of the observed data points over this assumption.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how GPs make predictions, let's see how we can make
    predictions on the stock market using GPs and potentially make some money.
  prefs: []
  type: TYPE_NORMAL
- en: Applying GPs to stock market prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this project, we will try to predict the prices of three major stocks in
    the market. The dataset for this exercise can be downloaded from Yahoo Finance
    ([https://finance.yahoo.com](https://finance.yahoo.com)). We downloaded the entire
    stock history for three companies:'
  prefs: []
  type: TYPE_NORMAL
- en: Google ([https://finance.yahoo.com/quote/GOOG](https://finance.yahoo.com/quote/GOOG))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netflix ([https://finance.yahoo.com/quote/NFLX](https://finance.yahoo.com/quote/NFLX))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General Electric company ([https://finance.yahoo.com/quote/GE](https://finance.yahoo.com/quote/GE))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We choose three datasets to compare GP performance across different stocks.
    Feel free to try this for more stocks.
  prefs: []
  type: TYPE_NORMAL
- en: All of these datasets are present in the GitHub repository. Thus, there is no
    need to download them again to run the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The CSV files in the dataset have multiple columns. They are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Date**: Calendar date when the price of the stock was measured.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Open**: The opening price of the day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High:** The highest price of the day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Low:** The lowest price of the day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Close:** The closing price of the day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adj Close:** The adjusted closing price is the closing price of the stock
    that has been amended to include any dividends or other corporate actions before
    the following day''s opening. This is our target variable or Y in the dataset.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Volume:** The volume denotes the number of shares traded during a day.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To begin our project, we will consider two forecasting problems each, for all
    three stock datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: In the first problem, we will train on prices from the years 2008-2016 and predict
    for the entire year of 2017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the second problem, we will train on prices from the years 2008-2018 (up
    to the third quarter) and predict the fourth quarter of 2018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For predicting stock prices, we don't need to model the entire time series of
    a stock as a single time series, as in many classical methods (an example of this
    is regression). For GP, each time series is divided into several time series (one
    for each year) for every stock. Intuitively, this makes sense, as each stock follows
    a yearly cycle.
  prefs: []
  type: TYPE_NORMAL
- en: 'The time series of each year of a stock is an input to the model as a separate
    time series. Therefore, the forecasting problem becomes as follows: predict future
    prices of the stock given multiple yearly time series (one for each historical
    year) as the input. As GP models are distributions over functions, we want to
    predict the mean and uncertainty at each data point in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before modeling, we need to normalize the prices to be zero mean and unit standard
    deviation. This is a requirement in Gaussian processes because of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We assume the prior on the output distribution to be zero mean, so normalization
    is required to match our assumption.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many kernels for the covariance matrix have scale parameters in them. Normalizing
    the input helps us to get better estimates of kernel parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For obtaining the posterior distribution in Gaussian processes, we have to invert
    the covariance matrix. Normalization helps to avoid any kind of numerical issues
    with this procedure. Note that we haven't discussed the linear algebra of obtaining
    the posterior in detail in this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the data has been normalized, we can train our model and predict prices
    using Gaussian processes. For modeling, we use the plug, and play functions from
    the GPflow ([https://github.com/GPflow/GPflow](https://github.com/GPflow/GPflow))
    library, which is a wrapper on top of TensorFlow for Gaussian processes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The independent variable (X) in the prediction problem is comprised of two
    factors:'
  prefs: []
  type: TYPE_NORMAL
- en: The year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The day of the year
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dependent variable (Y) in the problem is the normalized adjusted closing
    price for each day in a year as mentioned before.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before training the model, we need to define the prior and kernel function
    for Gaussian Processes. For this problem, we use the standard zero mean prior.
    We use a kernel function for the covariance matrix that is generated as the sum
    of two kernels which are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Squared exponential (or RBF, as mentioned in the GPflow package) kernel with
    `lengthscale` = `1` and `variance` = 63.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White noise with initial `variance` to be very low, such as *1e-10.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The idea behind choosing squared exponential is that it is infinitely differentiable
    and it's the easiest one to understand. White noise is used to account for any
    systemic noise we might observe in our target variables. While it may not be the
    best choice of kernels, it is good for understanding purposes. Feel free to experiment
    with other kernels and see whether they work well.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a stock price prediction model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will begin our project by processing the data present in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a dataframe with yearly time series for each stock. Represent each year's
    stock price by an individual column in that dataframe. Restrict number of rows
    in the dataframe to 252 which is roughly the number of trading days in a year.
    Also add the fiscal quarter associated with each row of data as a separate column.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that there are almost 252 trading days in a year, as stock markets are
    closed on weekends.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if there are more trading days in a particular year (like leap year), limit data
    to 252 days to ensure consistency across the years. In case the number of trading
    days is less than 252 in a particular year, extrapolate the data to 252 days by
    imputing the mean price of the year for missing days. Implement the following
    code to achieve this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For each year, normalize the prices to transform the yearly series to zero mean
    and unit standard deviation. Also, subtract the first day price from all of the
    data points in that year. This basically forces a yearly time series to start
    from zero, thereby avoiding any influence of previous year's prices on it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Please make sure to install this library as mentioned in the **README** file
    in the repository for this chapter, before executing the code.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the previous section, generate the covariance matrix as a sum
    of two kernels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We use the SciPy optimizer in GPflow package to optimize hyper parameters using
    maximum likelihood estimation. Scipy is a standard optimizer from Python library
    Scipy. If you are not familiar with Scipy optimizer, please refer to the official
    page ([https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Implement the final wrapper function `make_gp_predictions` to train a GP model
    and make future price predictions. Following are the steps that the function implements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Takes the input of training data, start and end of training period and prediction
    year and quarter.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Constructs 2 separate series using data from the start year of the training
    period, one for the independent variables (X) and one for the target (Y). Each
    element in series (X) represents each day of the year and consists of two independent
    variables, year and day of the year. For example, for start year 2008, X looks
    like [[2008,1], [2008,2],[2008,3].......[2008,252]].
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Appends the independent and target variables for each subsequent year to list
    X and Y respectively.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If input `pred_quarters` is not None, predicts for the quarters specified instead
    of the entire year. For example, if `pred_quarters` is [4] and `pred_year `is
    2018, the function will predict for Quarter 4 of 2018 using all the data till
    Quarter 3 of 2018.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Defines the kernel function as mentioned before and trains the GP model using
    Scipy optimizer.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Predicts the stock prices for the prediction period.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Understanding the results obtained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try to understand how good our predictions are for each of the stocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Netflix (NFLX)**: The following diagram illustrates the prices of the Netflix
    stock from **2002** through **2018**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/cb7fcc4b-bb57-4b83-ae7f-7270e5ed0165.png)'
  prefs: []
  type: TYPE_IMG
- en: The price for the year **2018** is defined using the two vertical lines. It
    shows the growth in the price of the stock throughout the entire year.
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the first problem case, we consider the period from **2008-2016** for
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd1a998d-3f38-4faa-a166-0525b69a5fe4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Normalizing the prices by each year for modeling gives us the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f737d653-8cb7-4ae2-9ac8-75e5ac1c1db7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Predicting the prices of the stock for the whole year of **2017** with a **95%
    confidence interval** gives us the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7811b0b1-6932-4e9c-ada7-77b8e03e74bd.png)'
  prefs: []
  type: TYPE_IMG
- en: Comparing of the generated values with the actual values, it is clear that the
    model falters by predicting the value to be less than the actual value. However,
    the reason for this could be the highs and lows of the prices of Netflix in the
    year **2016**. These are not captured through the basic-level kernel used in this
    project.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the second problem case, we consider the train period from **2008-2018**,
    including the first three quarters. The plot for the Netflix stock price during
    this period is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d5f521d-56ef-49e0-bf1b-bce74fb8e187.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the normalized values, we achieve the following plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db9c27fd-f922-4581-8a9d-d366987faffe.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the trend is very well captured in this prediction, along with
    uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: '**General Electric company** (**GE**): To understand the difference between
    the actual and the predicted prices, it is necessary to plot the graph that has
    the actual values. Here is the plot that illustrates the historical prices of
    GE stock:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/98fb46c8-2454-4644-aa7a-8b94f92a6d24.png)'
  prefs: []
  type: TYPE_IMG
- en: As mentioned previously, the dotted vertical lines represent the prices in the
    year **2018**.
  prefs: []
  type: TYPE_NORMAL
- en: 'As per our first problem case, we consider the period from **2008-2016** for
    training. The chart for that period is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c0b1610-d4b0-4b19-a6ae-fa7136effc43.png)'
  prefs: []
  type: TYPE_IMG
- en: There was a huge dip in **2009**, but since then, the stock has been growing
    steadily.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we used normalized prices for every year for modeling purposes, let''s
    look at the input data for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5192d44-9ff1-42c2-96d7-19cb2d86e7d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The predictions with normalized prices for the whole of **2017** with a **95%
    confidence interval** are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a7e5592-8b81-429c-bca4-702e1235d8e7.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, the model has captured an accurate trend of the stock.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the second prediction, we consider the train period from **2008-2018**,
    including the first three quarters of **2018**. The plot for the GE stock price
    during that period is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85e5b5bc-822e-4d47-82fc-7b68fadb8ce4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The predicted prices for that period with a **95% confidence interval** are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd225648-98fe-4d6c-8db8-06b1229778a6.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Google** (**GOOG**): The following chart illustrates the historical prices
    of Google stock:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**![](img/59c4504b-3445-44a0-af60-f9b932723bdb.png)**'
  prefs: []
  type: TYPE_NORMAL
- en: As specified previously, the dotted vertical lines represent the prices for **2018**.
  prefs: []
  type: TYPE_NORMAL
- en: 'As per the first problem case, the period from **2008-2016** is essential for
    training, so let''s look at the chart for this period:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39bbfb2c-a830-425f-97c6-7fbf49be392a.png)'
  prefs: []
  type: TYPE_IMG
- en: There was a huge dip in **2009** and since then, the stock has been growing
    steadily, except during **2015**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we used normalized prices by year for modeling, let''s look at the input
    data for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efd040ee-dcf9-409a-9b3b-ab13c4378d12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Predictions (normalized prices) for a complete year, **2017**, with a **95%
    confidence interval**, are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36c9abde-18a1-4d39-bed8-808fca53dcb5.png)'
  prefs: []
  type: TYPE_IMG
- en: We were able to capture the overall upward trend of the price, but with very
    wide confidence bands.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the next prediction, we considered the train period from **2008-2018**,
    including the first three quarters. A plot of the Google stock price during that
    period is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57e40b7a-b603-4f75-aafd-1a67782d40c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The predicted prices for that period with a **95% confidence interval** are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd5abf5d-fa5a-4e41-9983-ad3ff9162666.png)'
  prefs: []
  type: TYPE_IMG
- en: In 2018, the trend has been better captured overall.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about a very popular Bayesian forecasting model
    known as the Gaussian process and used it to predict stock prices.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part of this chapter, we looked at the forecasting problem by sampling
    an appropriate function from a multivariate Gaussian rather than use predicting
    point forecasts. We looked at a special kind of non-parametric Bayesian model
    named Gaussian processes.
  prefs: []
  type: TYPE_NORMAL
- en: Thereafter, we used GP to predict the prices of three stocks, namely Google,
    Netflix, and GE, for 2017 and Q4 2018\. We observed that our predictions were
    mostly within a 95% confidence interval, but far from perfect.
  prefs: []
  type: TYPE_NORMAL
- en: Gaussian processes are used widely in applications where we need to model non-linear
    functions with uncertainty with very few data points. However, they sometimes
    fail to scale to very high dimensional problems in which other deep learning algorithms,
    such as LSTM, would perform better.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will take a closer look at an unsupervised approach
    to detecting credit card frauds using auto-encoders.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are Gaussian processes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you improve the predictions by trying out different kernels?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can you apply GP model to other stocks in S&P 500 and compare its performance
    with the ones mentioned here?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
