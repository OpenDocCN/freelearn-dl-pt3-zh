- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning with TensorFlow and TF-Agents
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TF-Agents is a library for **reinforcement learning** (**RL**) in **TensorFlow**
    (**TF**). It makes the design and implementation of various algorithms easier
    by providing a number of modular components corresponding to the core parts of
    an RL problem:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: An agent operates in an **environment** and learns by processing signals received
    every time it chooses an action. In TF-Agents, an environment is typically implemented
    in Python and wrapped in a TF wrapper to enable efficient parallelization.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **policy** maps an observation from the environment into a distribution over
    actions.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **driver** executes a policy in an environment for a specified number of steps
    (also called **episodes**).
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **replay buffer** is used to store experience (agent trajectories in action
    space, along with associated rewards) of executing a policy in an environment;
    the buffer content is queried for a subset of trajectories during training.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic idea is to cast each of the problems we discuss as a RL problem,
    and then map the components into TF-Agents counterparts. In this chapter, we will
    show how TF-Agents can be used to solve some simple RL problems:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
- en: The GridWorld problem
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenAI Gym environment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-armed bandits for content personalization
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best way to start our demonstration of RL capabilities in TF-Agents is
    with a toy problem: GridWorld is a good choice due to its intuitive geometry and
    easy-to-interpret action but, despite this simplicity, it constitutes a proper
    objective, where we can investigate the optimal paths an agent takes to achieve
    the goal.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: GridWorld
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in this section is adapted from [https://github.com/sachag678](https://github.com/sachag678).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: We begin by demonstrating the basic TF-Agents functionality in the GridWorld
    environment. RL problems are best studied in the context of either games (where
    we have a clearly defined set of rules and fully observable context), or toy problems
    such as GridWorld. Once the basic concepts are clearly defined in a simplified
    but non-straightforward environment, we can move to progressively more challenging
    situations.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to define a GridWorld environment: this is a 6x6 square board,
    where the agent starts at (0,0), the finish is at (5,5), and the goal of the agent
    is to find the path from the start to the finish. Possible actions are moves up/down/left/right.
    If the agent lands on the finish, it receives a reward of 100, and the game terminates
    after 100 steps if the end was not reached by the agent. An example of the GridWorld
    "map" is provided here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, box and whisker chart  Description automatically generated](img/B16254_11_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: The GridWorld "map"'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Now we understand what we're working with, let's build a model to find its way
    around the GridWorld from **(0,0)** to **(5,5)**.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we begin by loading the necessary libraries:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: TF-Agents is a library under active development, so, despite our best efforts
    to keep the code up to date, certain imports might need to be modified by the
    time you are running this code.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: 'A crucial step is defining the environment that our agent will be operating
    in. Inheriting from the `PyEnvironment` class, we specify the `init` method (action
    and observation definitions), conditions for resetting/terminating the state,
    and the mechanics for moving:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have the following preliminary setup:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We begin by creating the environments and wrapping them to ensure that they
    terminate after 100 steps:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'For this recipe, we will be using a **Deep Q-Network** (**DQN**) agent. This
    means that we need to define the network and the associated optimizer first:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As indicated above, the TF-Agents library is under active development. The
    current version works with TF > 2.3, but it was originally written for TensorFlow
    1.x. The code used in this adaptation was developed using a previous version,
    so for the sake of backward compatibility, we require a less-than-elegant workaround,
    such as the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define the agent:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As a next step, we create the replay buffer and replay observer. The former
    is used for storing the (action, observation) pairs for training:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then create a dataset from our buffer so that it can be iterated over:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The final bit of preparation involves creating a driver that will simulate
    the agent in the game and store the (state, action, reward) tuples in the replay
    buffer, along with storing a number of metrics:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Having finished the preparatory groundwork, we can run the driver, draw experience
    from the dataset, and use it to train the agent. For monitoring/logging purposes,
    we print the loss and average return at specific intervals:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once the code executes successfully, you should observe output similar to the
    following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'While detailed, the output of the training routine is not that well suited
    for reading by a human. However, we can visualize the progress of our agent instead:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Which will deliver us the following graph:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_11_02.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: Average episode length over number of episodes'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph demonstrates the progress in our model: after the first 4,000 episodes,
    there is a massive drop in the average episode length, indicating that it takes
    our agent less and less time to reach the ultimate objective.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Documentation for customized environments can be found at [https://www.tensorflow.org/agents/tutorials/2_environments_tutorial](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'RL is a huge field and even a basic introduction is beyond the scope of this
    book, but for those interested in learning more, the best recommendation is the
    classic Sutton and Barto book: [http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: CartPole
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will make use of Open AI Gym, a set of environments containing
    non-trivial elementary problems that can be solved using RL approaches. We''ll
    use the CartPole environment. The objective of the agent is to learn how to keep
    a pole balanced on a moving cart, with possible actions including a movement to
    the left or to the right:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_11_03.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: The CartPole environment, with the black cart balancing a long
    pole'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Now we understand our environment, let's build a model to balance a pole.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by installing some prerequisites and importing the necessary libraries.
    The installation part is mostly required to ensure that we can generate visualizations
    of the trained agent''s performance:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As before, there are some hyperparameters of our toy problem that we define:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we proceed with function definitions for our problem. Start by computing
    the average return for a policy in our environment over a fixed period (measured
    by the number of episodes):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Boilerplate code for collecting a single step and the associated data aggregation
    are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'If a picture is worth a thousand words, then surely a video must be even better.
    In order to visualize the performance of our agent, we need a function that renders
    the actual animation:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With the preliminaries out of the way, we can now proceed to actually setting
    up our environment:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the CartPole environment, the following applies:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'An observation is an array of four floats:'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The position and velocity of the cart
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The angular position and velocity of the pole
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward is a scalar float value
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An action is a scalar integer with only two possible values:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0 — "move left"
  id: totrans-78
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 — "move right"
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As before, split the training and evaluation environments and apply the wrappers:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Define the network forming the backbone of the learning algorithm in our agent:
    a neural network predicting the expected returns of all actions (commonly referred
    to as Q-values in RL literature) given an observation of the environment as input:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With this, we can instantiate a DQN agent:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Set up the policies – the main one used for evaluation and deployment, and
    the secondary one that is utilized for data collection:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In order to have an admittedly not very sophisticated comparison, we will also
    create a random policy (as the name suggests, it acts randomly). This demonstrates
    an important point, however: a policy can be created independently of an agent:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To get an action from a policy, we call the `policy.action(time_step)` method.
    The `time_step` contains the observation from the environment. This method returns
    a policy step, which is a named tuple with three components:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '**Action**: the action to be taken (move left or move right)'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State**: used for stateful (RNN-based) policies'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Info**: auxiliary data, such as the log probabilities of actions:'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The replay buffer tracks the data collected from the environment, which is
    used for training:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: For most agents, `collect_data_spec` is a named tuple called **Trajectory**,
    containing the specs for observations, actions, rewards, and other items.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: 'We now make use of our random policy to explore the environment:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The replay buffer can now be accessed by an agent by means of a pipeline. Since
    our DQN agent needs both the current and the next observation to calculate the
    loss, the pipeline samples two adjacent rows at a time (`num_steps = 2`):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'During the training part, we switch between two steps, collecting data from
    the environment and using it to train the DQN:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'A (partial) output of the code block is given here. By way of a quick reminder,
    `step` is the iteration in the training process, `loss` is the value of the loss
    function in the deep network driving the logic behind our agent, and `Average
    Return` is the reward at the end of the current run:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Each iteration consists of 200 time steps and keeping the pole up gives a reward
    of 1, so our maximum reward per episode is 200:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_11_04.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Average return over number of iterations'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding graph, the agent takes about 10 thousand iterations
    to discover a successful policy (with some hits and misses, as the U-shaped pattern
    of reward in that part demonstrates). After that, the reward stabilizes and the
    algorithm is able to successfully complete the task each time.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also observe the performance of our agents in a video. As regards the
    random policy, you can try the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And as regards the trained one, you can try the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: See also
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open AI Gym environment documentation can be found at [https://gym.openai.com/](https://gym.openai.com/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: MAB
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In probability theory, a **multi-armed bandit** (**MAB**) problem refers to
    a situation where a limited set of resources must be allocated between competing
    choices in such a manner that some form of long-term objective is maximized. The
    name originated from the analogy that was used to formulate the first version
    of the model. Imagine we have a gambler facing a row of slot machines who has
    to decide which ones to play, how many times, and in what order. In RL, we formulate
    it as an agent that wants to balance exploration (acquisition of new knowledge)
    and exploitation (optimizing decisions based on experience already acquired).
    The objective of this balancing is the maximization of a total reward over a period
    of time.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'An MAB is a simplified RL problem: an action taken by the agent does not influence
    the subsequent state of the environment. This means that there is no need to model
    state transitions, credit rewards to past actions, or plan ahead to get to rewarding
    states. The goal of an MAB agent is to determine a policy that maximizes the cumulative
    reward over time.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'The main challenge is an efficient approach to the exploration-exploitation
    dilemma: if we always try to exploit the action with the highest expected rewards,
    there is a risk we miss out on better actions that could have been uncovered with
    more exploration.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The setup used in this example is adapted from the Vowpal Wabbit tutorial at
    [https://vowpalwabbit.org/tutorials/cb_simulation.html](https://vowpalwabbit.org/tutorials/cb_simulation.html).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will simulate the problem of personalizing online content:
    Tom and Anna go to a website at different times of the day and are shown an article.
    Tom likes politics in the morning and music in the afternoon, while Anna prefers
    sport or politics in the morning and politics in the afternoon. Casting the problem
    in MAB terms, this means the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: The context is a pair {user, time of day}
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible actions are news topics {politics, sport, music, food}
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward is 1 if a user is shown content they find interesting at this time,
    and 0 otherwise
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The objective is to maximize the reward measured through the **clickthrough
    rate** (**CTR**) of the users.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we begin by loading the necessary packages:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We then define some hyperparameters that will be used later:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The first function we need is a context sampler to generate observations coming
    from the environment. Since we have two users and two parts of the day, it comes
    down to generating two-element binary vectors:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we define a generic function for calculating the reward per arm:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can use the function to define the rewards per arm. They reflect the set
    of preferences described at the beginning of this recipe:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The final part of our function''s setup involves a calculation of the optimal
    rewards for a given context:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'For the sake of this example, we assume that the environment is stationary;
    in other words, the preferences do not change over time (which does not need to
    be the case in a practical scenario, depending on your time horizon of interest):'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We are now ready to instantiate an agent implementing a bandit algorithm. We
    use a predefined `LinUCB` class; as usual, we define the observation (two elements
    representing the user and the time of day), time step, and action specification
    (one of four possible types of content):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A crucial component of the MAB setup is regret, which is defined as the difference
    between an actual reward collected by the agent and the expected reward of an
    **oracle policy**:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can now commence the training of our agent. We run the trainer loop for
    `num_iterations` and execute `steps_per_loop` in each step. Finding the appropriate
    values for those parameters is usually about striking a balance between the recent
    nature of updates and training efficiency:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can visualize the results of our experiment by plotting the regret (negative
    reward) over subsequent iterations of the algorithm:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Which will plot the following graph for us:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_11_05.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Performance of a trained UCB agent over time'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5：训练过的UCB代理随时间的表现
- en: As the preceding graph demonstrates, after an initial period of learning (indicating
    a spike in regret around iteration 30), the agent keeps getting better at serving
    the desired content. There is a lot of variation going on, which shows that even
    in a simplified setting – two users – efficient personalization remains a challenge.
    Possible avenues of improvement could involve longer training or adapting a DQN
    agent so that more sophisticated logic can be employed for prediction.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，在初始学习阶段（在第30次迭代附近的遗憾出现峰值），代理会不断改进，提供期望的内容。过程中有很多变化，表明即便在一个简化的环境下——两个用户——高效个性化仍然是一个挑战。改进的可能方向包括更长时间的训练，或者调整DQN代理，使其能够采用更复杂的逻辑来进行预测。
- en: See also
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 另见
- en: 'An extensive collection of bandits and related environments can be found in
    the *TF-Agents documentation repository*: [https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的强盗算法及其环境的广泛集合可以在*TF-Agents文档库*中找到：[https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2)。
- en: 'Readers interested in contextual multi-armed bandits are encouraged to follow
    the relevant chapters from the book by *Sutton and Barto*: [https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对上下文多臂强盗问题感兴趣的读者可以参考*Sutton和Barto*的书中的相关章节：[https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)。
