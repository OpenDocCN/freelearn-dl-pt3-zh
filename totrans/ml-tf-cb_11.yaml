- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning with TensorFlow and TF-Agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TF-Agents is a library for **reinforcement learning** (**RL**) in **TensorFlow**
    (**TF**). It makes the design and implementation of various algorithms easier
    by providing a number of modular components corresponding to the core parts of
    an RL problem:'
  prefs: []
  type: TYPE_NORMAL
- en: An agent operates in an **environment** and learns by processing signals received
    every time it chooses an action. In TF-Agents, an environment is typically implemented
    in Python and wrapped in a TF wrapper to enable efficient parallelization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **policy** maps an observation from the environment into a distribution over
    actions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **driver** executes a policy in an environment for a specified number of steps
    (also called **episodes**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **replay buffer** is used to store experience (agent trajectories in action
    space, along with associated rewards) of executing a policy in an environment;
    the buffer content is queried for a subset of trajectories during training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The basic idea is to cast each of the problems we discuss as a RL problem,
    and then map the components into TF-Agents counterparts. In this chapter, we will
    show how TF-Agents can be used to solve some simple RL problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The GridWorld problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The OpenAI Gym environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-armed bandits for content personalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The best way to start our demonstration of RL capabilities in TF-Agents is
    with a toy problem: GridWorld is a good choice due to its intuitive geometry and
    easy-to-interpret action but, despite this simplicity, it constitutes a proper
    objective, where we can investigate the optimal paths an agent takes to achieve
    the goal.'
  prefs: []
  type: TYPE_NORMAL
- en: GridWorld
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in this section is adapted from [https://github.com/sachag678](https://github.com/sachag678).
  prefs: []
  type: TYPE_NORMAL
- en: We begin by demonstrating the basic TF-Agents functionality in the GridWorld
    environment. RL problems are best studied in the context of either games (where
    we have a clearly defined set of rules and fully observable context), or toy problems
    such as GridWorld. Once the basic concepts are clearly defined in a simplified
    but non-straightforward environment, we can move to progressively more challenging
    situations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to define a GridWorld environment: this is a 6x6 square board,
    where the agent starts at (0,0), the finish is at (5,5), and the goal of the agent
    is to find the path from the start to the finish. Possible actions are moves up/down/left/right.
    If the agent lands on the finish, it receives a reward of 100, and the game terminates
    after 100 steps if the end was notÂ reached by the agent. An example of the GridWorld
    "map" is provided here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, box and whisker chart  Description automatically generated](img/B16254_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: The GridWorld "map"'
  prefs: []
  type: TYPE_NORMAL
- en: Now we understand what we're working with, let's build a model to find its way
    around the GridWorld from **(0,0)** to **(5,5)**.
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we begin by loading the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: TF-Agents is a library under active development, so, despite our best efforts
    to keep the code up to date, certain imports might need to be modified by the
    time you are running this code.
  prefs: []
  type: TYPE_NORMAL
- en: 'A crucial step is defining the environment that our agent will be operating
    in. Inheriting from the `PyEnvironment` class, we specify the `init` method (action
    and observation definitions), conditions for resetting/terminating the state,
    and the mechanics for moving:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We have the following preliminary setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by creating the environments and wrapping them to ensure that they
    terminate after 100 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'For this recipe, we will be using a **Deep Q-Network** (**DQN**) agent. This
    means that we need to define the network and the associated optimizer first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As indicated above, the TF-Agents library is under active development. The
    current version works with TF > 2.3, but it was originally written for TensorFlow
    1.x. The code used in this adaptation was developed using a previous version,
    so for the sake of backward compatibility, we require a less-than-elegant workaround,
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'As a next step, we create the replay buffer and replay observer. The former
    is used for storing the (action, observation) pairs for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We then create a dataset from our buffer so that it can be iterated over:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The final bit of preparation involves creating a driver that will simulate
    the agent in the game and store the (state, action, reward) tuples in the replay
    buffer, along with storing a number of metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Having finished the preparatory groundwork, we can run the driver, draw experience
    from the dataset, and use it to train the agent. For monitoring/logging purposes,
    we print the loss and average return at specific intervals:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the code executes successfully, you should observe output similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'While detailed, the output of the training routine is not that well suited
    for reading by a human. However, we can visualize the progress of our agent instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Which will deliver us the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: Average episode length over number of episodes'
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph demonstrates the progress in our model: after the first 4,000 episodes,
    there is a massive drop in the average episode length, indicating that it takes
    our agent less and less time to reach the ultimate objective.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Documentation for customized environments can be found at [https://www.tensorflow.org/agents/tutorials/2_environments_tutorial](https://www.tensorflow.org/agents/tutorials/2_environments_tutorial).
  prefs: []
  type: TYPE_NORMAL
- en: 'RL is a huge field and even a basic introduction is beyond the scope of this
    book, but for those interested in learning more, the best recommendation is the
    classic Sutton and Barto book: [http://incompleteideas.net/book/the-book.html](http://incompleteideas.net/book/the-book.html)'
  prefs: []
  type: TYPE_NORMAL
- en: CartPole
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will make use of Open AI Gym, a set of environments containing
    non-trivial elementary problems that can be solved using RL approaches. We''ll
    use the CartPole environment. The objective of the agent is to learn how to keep
    a pole balanced on a moving cart, with possible actions including a movement to
    the left or to the right:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: The CartPole environment, with the black cart balancing a long
    pole'
  prefs: []
  type: TYPE_NORMAL
- en: Now we understand our environment, let's build a model to balance a pole.
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by installing some prerequisites and importing the necessary libraries.
    The installation part is mostly required to ensure that we can generate visualizations
    of the trained agent''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, there are some hyperparameters of our toy problem that we define:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we proceed with function definitions for our problem. Start by computing
    the average return for a policy in our environment over a fixed period (measured
    by the number of episodes):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Boilerplate code for collecting a single step and the associated data aggregation
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'If a picture is worth a thousand words, then surely a video must be even better.
    In order to visualize the performance of our agent, we need a function that renders
    the actual animation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preliminaries out of the way, we can now proceed to actually setting
    up our environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the CartPole environment, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: 'An observationÂ is an array of four floats:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The position and velocity of the cart
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The angular position and velocity of the pole
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The rewardÂ is a scalar float value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An actionÂ is a scalar integer with only two possible values:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 0Â â "move left"
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 1Â â "move right"
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As before, split the training and evaluation environments and apply the wrappers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the network forming the backbone of the learning algorithm in our agent:
    a neural network predicting the expected returns of all actions (commonly referred
    to as Q-values in RLÂ literature) given an observation of the environment as input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'With this, we can instantiate a DQN agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the policies â the main one used for evaluation and deployment, and
    the secondary one that is utilized for data collection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to have an admittedly not very sophisticated comparison, we will also
    create a random policy (as the name suggests, it acts randomly). This demonstrates
    an important point, however: a policy can be created independently of an agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To get an action from a policy, we call theÂ `policy.action(time_step)`Â method.
    TheÂ `time_step`Â contains the observation from the environment. This method returns
    aÂ policy step, which is a named tuple with three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Action**: the action to be taken (move left or move right)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State**: used for stateful (RNN-based) policies'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Info**: auxiliary data, such as the log probabilities of actions:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The replay buffer tracks the data collected from the environment, which is
    used for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: For most agents,Â `collect_data_spec`Â is a named tuple calledÂ **Trajectory**,
    containing the specs for observations, actions, rewards, and other items.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now make use of our random policy to explore the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The replay buffer can now be accessed by an agent by means of a pipeline. Since
    our DQN agent needs both the current and the next observation to calculate the
    loss, the pipeline samples two adjacent rows at a time (`num_steps = 2`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'During the training part, we switch between two steps, collecting data from
    the environment and using it to train the DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'A (partial) output of the code block is given here. By way of a quick reminder,
    `step` is the iteration in the training process, `loss` is the value of the loss
    function in the deep network driving the logic behind our agent, and `Average
    Return` is the reward at the end of the current run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Each iteration consists of 200 time steps and keeping the pole up gives a reward
    of 1, so our maximum reward per episode is 200:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Average return over number of iterations'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding graph, the agent takes about 10 thousand iterations
    to discover a successful policy (with some hits and misses, as the U-shaped pattern
    of reward in that part demonstrates). After that, the reward stabilizes and the
    algorithm is able to successfully complete the task each time.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also observe the performance of our agents in a video. As regards the
    random policy, you can try the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'And as regards the trained one, you can try the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Open AI Gym environment documentation can be found at [https://gym.openai.com/](https://gym.openai.com/).
  prefs: []
  type: TYPE_NORMAL
- en: MAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In probability theory, a **multi-armed bandit** (**MAB**) problem refers to
    a situation where a limited set of resources must be allocated between competing
    choices in such a manner that some form of long-term objective is maximized. The
    name originated from the analogy that was used to formulate the first version
    of the model. Imagine we have a gambler facing a row of slot machines who has
    to decide which ones to play, how many times, and in what order. In RL, we formulate
    it as an agent that wants to balance exploration (acquisition of new knowledge)
    and exploitation (optimizing decisions based on experience already acquired).
    The objective of this balancing is the maximization of a total reward over a period
    of time.
  prefs: []
  type: TYPE_NORMAL
- en: 'An MAB is a simplified RL problem: an action taken by the agent does not influence
    the subsequent state of the environment. This means that there is no need to model
    state transitions, credit rewards to past actions, or plan ahead to get to rewarding
    states. The goal of an MAB agent is to determine a policy that maximizes the cumulative
    reward over time.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The main challenge is an efficient approach to the exploration-exploitation
    dilemma: if we always try to exploit the action with the highest expected rewards,
    there is a risk we miss out on better actions that could have been uncovered with
    more exploration.'
  prefs: []
  type: TYPE_NORMAL
- en: The setup used in this example is adapted from the Vowpal Wabbit tutorial at
    [https://vowpalwabbit.org/tutorials/cb_simulation.html](https://vowpalwabbit.org/tutorials/cb_simulation.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will simulate the problem of personalizing online content:
    Tom and Anna go to a website at different times of the day and are shown an article.
    Tom likes politics in the morning and music in the afternoon, while Anna prefers
    sport or politics in the morning and politics in the afternoon. Casting the problem
    in MAB terms, this means the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The context is a pair {user, time of day}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Possible actions are news topics {politics, sport, music, food}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reward is 1 if a user is shown content they find interesting at this time,
    and 0Â otherwise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The objective is to maximize the reward measured through the **clickthrough
    rate** (**CTR**) of the users.
  prefs: []
  type: TYPE_NORMAL
- en: How do we go about it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we begin by loading the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We then define some hyperparameters that will be used later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The first function we need is a context sampler to generate observations coming
    from the environment. Since we have two users and two parts of the day, it comes
    down to generating two-element binary vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define a generic function for calculating the reward per arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the function to define the rewards per arm. They reflect the set
    of preferences described at the beginning of this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The final part of our function''s setup involves a calculation of the optimal
    rewards for a given context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of this example, we assume that the environment is stationary;
    in other words, the preferences do not change over time (which does not need to
    be the case in a practical scenario, depending on your time horizon of interest):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We are now ready to instantiate an agent implementing a bandit algorithm. We
    use a predefined `LinUCB` class; as usual, we define the observation (two elements
    representing the user and the time of day), time step, and action specification
    (one of four possible types of content):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'A crucial component of the MAB setup is regret, which is defined as the difference
    between an actual reward collected by the agent and the expected reward of an
    **oracle policy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now commence the training of our agent. We run the trainer loop for
    `num_iterations` and execute `steps_per_loop` in each step. Finding the appropriate
    values for those parameters is usually about striking a balance between the recent
    nature of updates and training efficiency:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize the results of our experiment by plotting the regret (negative
    reward) over subsequent iterations of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Which will plot the following graph for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Performance of a trained UCB agent over time'
  prefs: []
  type: TYPE_NORMAL
- en: As the preceding graph demonstrates, after an initial period of learning (indicating
    a spike in regret around iteration 30), the agent keeps getting better at serving
    the desired content. There is a lot of variation going on, which shows that even
    in a simplified setting â two users â efficient personalization remains a challenge.
    Possible avenues of improvement could involve longer training or adapting a DQN
    agent so that more sophisticated logic can be employed for prediction.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An extensive collection of bandits and related environments can be found in
    the *TF-Agents documentation repository*: [https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2](https://github.com/tensorflow/agents/tree/master/tf_agents/bandits/agents/examples/v2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Readers interested in contextual multi-armed bandits are encouraged to follow
    the relevant chapters from the book by *Sutton and Barto*: [https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf).'
  prefs: []
  type: TYPE_NORMAL
