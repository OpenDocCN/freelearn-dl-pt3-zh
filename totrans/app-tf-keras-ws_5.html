<html><head></head><body>
		<div id="_idContainer093" class="Content">
			<h1 id="_idParaDest-113"><a id="_idTextAnchor117"/>Appendix</h1>
		</div>
		<div id="_idContainer113" class="Content">
			<h1 id="_idParaDest-114"><a id="_idTextAnchor118"/>Chapter 01: Introduction to Neural Networks and Deep Learning</h1>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor119"/>Activity 1.01: Training a Neural Network with Different Hyperparameters</h2>
			<p>Solution:</p>
			<ol>
				<li>Using your Terminal, navigate to the directory cloned from <a href="https://packt.live/2ZVyf0C">https://packt.live/2ZVyf0C</a> and execute the following command to start TensorBoard:<p class="source-code">$ tensorboard --logdir logs/fit</p><p>The output is as follows:</p><div id="_idContainer094" class="IMG---Figure"><img src="image/B15911_01_15.jpg" alt="Figure 1.15: A screenshot of a Terminal after starting a TensorBoard instance&#13;&#10;"/></div><p class="figure-caption">Figure 1.15: A screenshot of a Terminal after starting a TensorBoard instance</p></li>
				<li>Now, open the URL provided by TensorBoard in your browser. You should be able to see the TensorBoard <strong class="source-inline">SCALARS</strong> page:<div id="_idContainer095" class="IMG---Figure"><img src="image/B15911_01_16.jpg" alt="Figure 1.16: A screenshot of the TensorBoard SCALARS page&#13;&#10;"/></div><p class="figure-caption">Figure 1.16: A screenshot of the TensorBoard SCALARS page</p></li>
				<li>On the TensorBoard page, click on the <strong class="source-inline">SCALARS</strong> page and enlarge the <strong class="source-inline">epoch_accuracy</strong> graph. Now, move the smoothing slider to <strong class="source-inline">0.6</strong>.<p>The accuracy graph measures how accurately the network was able to guess the labels of a test set. At first, the network guesses those labels completely incorrectly. This happens because we have initialized the weights and biases of our network with random values, so its first attempts are a guess. The network will then change the weights and biases of its layers on a second run; the network will continue to invest in the nodes that give positive results by altering their weights and biases and will penalize those that don't by gradually reducing their impact on the network (eventually reaching <strong class="source-inline">0</strong>). As you can see, this is an efficient technique that quickly yields great results.</p></li>
				<li>To train another model by changing various hyperparameters, open a Terminal in <strong class="source-inline">Chapter01/Activity1.01</strong>. Activate the environment. Change the following lines in <strong class="source-inline">mnist.py</strong>:<p class="source-code">learning_rate = 0.0001 (at line number 47)</p><p class="source-code">epochs = 10 (at line number 56)</p><p>The <strong class="source-inline">mnist.py</strong> file will look as follows:</p><div id="_idContainer096" class="IMG---Figure"><img src="image/B15911_01_17.jpg" alt="Figure 1.17: A screenshot of the mnist.py file and the hyperparameters to change&#13;&#10;"/></div><p class="figure-caption">Figure 1.17: A screenshot of the mnist.py file and the hyperparameters to change</p></li>
				<li>Now repeat <em class="italic">steps 1-3</em> for this newly trained model. Start TensorFlow, open the <strong class="source-inline">Scalar</strong> page with the URL seen on TensorBoard, and view the <strong class="source-inline">epoch_accuracy</strong> graph on the <strong class="source-inline">Scalar</strong> page. You will see the difference compared to the earlier graphs:<div id="_idContainer097" class="IMG---Figure"><img src="image/B15911_01_18.jpg" alt="Figure 1.18: A screenshot from TensorBoard showing the parameters specified in step 4&#13;&#10;"/></div><p class="figure-caption">Figure 1.18: A screenshot from TensorBoard showing the parameters specified in step 4</p></li>
				<li>Now repeat <em class="italic">step 4</em>. Open a Terminal in <strong class="source-inline">Chapter01/Activity1.01</strong>. Activate the environment. Change the following lines in <strong class="source-inline">mnist.py</strong> to the following values:<p class="source-code">learning_rate = 0.01 (at line number 47)</p><p class="source-code">epochs = 5 (at line number 56)</p><p>Visualize the results. You will get graphs like these:</p><div id="_idContainer098" class="IMG---Figure"><img src="image/B15911_01_19.jpg" alt="Figure 1.19: A screenshot of the TensorBoard graphs&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.19: A screenshot of the TensorBoard graphs</p>
			<p>Now try running the model with any of your custom values and see how the graph changes.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Use the <strong class="source-inline">mnist.py</strong> file for your reference at <a href="https://packt.live/2ZVyf0C">https://packt.live/2ZVyf0C</a>.</p>
			<p class="callout">There are many other parameters that you can modify in your neural network. For now, experiment with the epochs and the learning rate of your network. You will notice that those two on their own can greatly change the output of your network—but only by so much. Experiment to see if you can train this network faster with the current architecture just by altering those two parameters.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3eiFdC3">https://packt.live/3eiFdC3</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
			<p>Verify how your network is training using TensorBoard. Alter those parameters a few more times by multiplying the starting values by 10 until you notice that the network is improving. This process of tuning the network and finding improved accuracy is essentially what is used in industry applications today to improve existing neural network models.</p>
			<h1 id="_idParaDest-116"><a id="_idTextAnchor120"/>Chapter 02: Real-World Deep Learning: Predicting the Price of Bitcoin</h1>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor121"/>Activity 2.01: Assembling a Deep Learning System</h2>
			<p>Solution:</p>
			<p>We will continue to use Jupyter Notebooks and the data prepared in previous exercises of chapter 2 (<strong class="source-inline">data/train_dataset.csv</strong>), as well as the model that we stored locally (<strong class="source-inline">bitcoin_ lstm_v0.h5</strong>):</p>
			<ol>
				<li value="1">Import the libraries required to load and train the deep learning model: <p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">from tensorflow.keras.models import load_model</p><p class="source-code">plt.style.use('seaborn-white')</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">close_point_relative_normalization</strong> variable will be used to train our LSTM model.</p><p>We will start by loading the dataset we prepared during our previous activities. We'll use pandas to load that dataset into memory.</p></li>
				<li>Load the training dataset into memory using pandas, as follows:<p class="source-code">train = pd.read_csv('data/train_dataset.csv')</p></li>
				<li>Now, quickly inspect the dataset by executing the following command:<p class="source-code">train.head()</p><p>As explained in this chapter, LSTM networks require tensors with three dimensions. These dimensions are period length, the number of periods, and the number of features. </p><p>Now, proceed to create weekly groups, and then rearrange the resulting array to match those dimensions.</p></li>
				<li>Feel free to use the <strong class="source-inline">create_groups()</strong> function provided to perform this operation:<p class="source-code"><strong class="source-inline">create_groups(data=train, group_size=7)</strong></p><p>The default values for that function are <strong class="source-inline">7</strong> days.</p><p>Now, make sure you split the data into two sets: training and validation. We do this by assigning the last week from the Bitcoin prices dataset to the evaluation set. We then train the network to evaluate that last week. Separate the last week of the training data and reshape it using <strong class="source-inline">numpy.reshape()</strong>. Reshaping it is important, as the LSTM model only accepts data organized in this way:</p><p class="source-code">X_train = data[:-1,:].reshape(1, 186, 7)</p><p class="source-code">Y_validation = data[-1].reshape(1, 7)</p><p>Our data is now ready to be used in training. Now, we load our previously saved model and train it with a given number of epochs.</p></li>
				<li>Navigate to the <strong class="source-inline">Load Our Model</strong>  header and load our previously trained model:<p class="source-code">model = load_model('bitcoin_lstm_v0.h5')</p></li>
				<li>And now, train that model with our training data, <strong class="source-inline">X_train</strong> and <strong class="source-inline">Y_validation</strong>:<p class="source-code">%%time</p><p class="source-code">history = model.fit( x=X_train, y=Y_validation, epochs=100)</p><p>Notice that we store the logs of the model in a variable called <strong class="source-inline">history</strong>. The model logs are useful for exploring specific variations in its training accuracy and observing how well the loss function is performing:</p><div id="_idContainer099" class="IMG---Figure"><img src="image/B15911_02_27.jpg" alt="Figure 2.27: Section of the Jupyter notebook where we load our earlier model &#13;&#10;and train it with new data&#13;&#10;"/></div><p class="figure-caption">Figure 2.27: Section of the Jupyter Notebook where we load our earlier model and train it with new data</p></li>
				<li>Finally, let's make a prediction with our trained model. Using the same data, <strong class="source-inline">X_train</strong>, call the following method:<p class="source-code">model.predict(x=X_train)</p></li>
				<li>The model immediately returns a list of normalized values with the prediction for the next 7 days. Use the <strong class="source-inline">denormalize()</strong> function to turn the data into US dollar values. Use the latest values available as a reference for scaling the predicted results:<p class="source-code">denormalized_prediction = denormalize(predictions, \</p><p class="source-code">                                      last_weeks_value)</p><p>The output is as follows:</p><div id="_idContainer100" class="IMG---Figure"><img src="image/B15911_02_28.jpg" alt="Figure 2.28: Projection of Bitcoin prices for 7 days in the future &#13;&#10;using the LSTM model we just built&#13;&#10;"/></div><p class="figure-caption">Figure 2.28: Projection of Bitcoin prices for 7 days in the future using the LSTM model we just built</p><p class="callout-heading">Note</p><p class="callout">We combine both time series in this graph: the real data (before the vertical line) and the predicted data (after the vertical line). The model shows a variance similar to the patterns seen before and it suggests a price increase during the following 7-day period.</p></li>
				<li>When you are done experimenting, save your model with the following command:<p class="source-code">model.save('bitcoin_lstm_v0_trained.h5')</p><p>We will save this trained network for future reference and compare its performance with other models.</p></li>
			</ol>
			<p>The network may have learned patterns from our data, but how can it do that with such a simple architecture and so little data? LSTMs are powerful tools for learning patterns from data. However, we will learn in our next sessions that they can also suffer from overfitting, a phenomenon common in neural networks, in which they learn patterns from the training data that are useless when predicting real-world patterns. We will learn how to deal with that and how to improve our network to make useful predictions.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZWfqub">https://packt.live/2ZWfqub</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3gIhhcT">https://packt.live/3gIhhcT</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor122"/>Chapter 3: Real-World Deep Learning: Evaluating the Bitcoin Model</h1>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor123"/>Activity 3.01: Optimizing a Deep Learning Model</h2>
			<p>Solution:</p>
			<ol>
				<li value="1">Using your Terminal, start a TensorBoard instance by executing the following command:<p class="source-code">$ cd ./Chapter03/Activity3.01/</p><p class="source-code">$ tensorboard --logdir=logs/</p><p>You will see the <strong class="source-inline">SCALARS</strong> page once TensorBoard opens in the browser:</p><div id="_idContainer101" class="IMG---Figure"><img src="image/B15911_03_20.jpg" alt="Figure 3.20: Screenshot of a TensorBoard showing SCALARS page&#13;&#10;"/></div><p class="figure-caption">Figure 3.20: Screenshot of a TensorBoard showing SCALARS page</p></li>
				<li>Open the URL that appears on screen and leave that browser tab open as well. Also, start a Jupyter Notebook instance with the following command:<p class="source-code">$ jupyter-lab</p><p>Here's the screenshot showing the Jupyter Notebook:</p><div id="_idContainer102" class="IMG---Figure"><img src="image/B15911_03_21.jpg" alt="Figure 3.21: Jupyter Notebook&#13;&#10;"/></div><p class="figure-caption">Figure 3.21: Jupyter Notebook</p><p>Open the URL that appears in a different browser window.</p></li>
				<li>Now, open the Jupyter Notebook called <strong class="source-inline">Activity3.01_Optimizing_a_deep_learning_model.ipynb</strong> and navigate to the title of the Notebook. Run the cell to, import all the required libraries.</li>
				<li>Set the seed to avoid randomness:<p class="source-code">np.random.seed(0)</p><p>We will load the train and test data like we did in the previous activities. We will also split it into train and test groups using the <strong class="source-inline">split_lstm_input()</strong> utility function:</p><div id="_idContainer103" class="IMG---Figure"><img src="image/B15911_03_22.jpg" alt="Figure 3.22: Screenshot showing results of loading datasets&#13;&#10;"/></div><p class="figure-caption">Figure 3.22: Screenshot showing results of loading datasets</p><p>In each section of this Notebook, we will implement new optimization techniques in our model. Each time we do so, we'll train a fresh model and store its trained instance in a variable that describes the model version. For instance, our first model, <strong class="source-inline">bitcoin_lstm_v0</strong>, is called <strong class="source-inline">model_v0</strong> in this Notebook. At the very end of the Notebook, we'll evaluate all the models using MSE, RMSE, and MAPE.</p></li>
				<li>To get these models up and running, execute the cells under the <strong class="bold">Reference Model</strong> section.</li>
				<li>Now, in the open Jupyter Notebook, navigate to the <strong class="source-inline">Adding Layers and Nodes</strong> header. You will recognize our first model in the next cell. This is the basic LSTM network that we built in <em class="italic">Chapter 2</em>, <em class="italic">Real-World Deep Learning with TensorFlow and Keras: Predicting the Price of Bitcoin</em>. Now, we have to add a new LSTM layer to this network:<div id="_idContainer104" class="IMG---Figure"><img src="image/B15911_03_23.jpg" alt="Figure 3.23: Jupyter Notebook with code for adding new LSTM layer&#13;&#10;"/></div><p class="figure-caption">Figure 3.23: Jupyter Notebook with code for adding new LSTM layer</p><p>Using your knowledge from this chapter, go ahead and add a new LSTM layer and then compile and train the model.</p><p>While training your models, remember to frequently visit the running TensorBoard instance. You will be able to see each model run and compare the results of their loss functions there:</p><div id="_idContainer105" class="IMG---Figure"><img src="image/B15911_03_24.jpg" alt="Figure 3.24: Output of the loss function for different models&#13;&#10;"/></div><p class="figure-caption">Figure 3.24: Output of the loss function for different models</p><p>The TensorBoard instance displays many different model runs. TensorBoard is really useful for tracking model training in real time.</p></li>
				<li>In this section, we are interested in exploring different magnitudes of epochs. Use the <strong class="source-inline">train_model()</strong> utility function to name different model versions and runs:<p class="source-code">train_model(model=model_v0, X=X_train, \</p><p class="source-code">            Y=Y_validate, epochs=100, \</p><p class="source-code">            version=0, run_number=0)</p><p>Train the model with a few different epoch parameters.</p><p>At this point, you are interested in making sure the model doesn't overfit the training data. You want to avoid this, because if it does, it will not be able to predict patterns that are represented in the training data but have different representations in the test data.</p><p>After you are done experimenting with epochs, move to the next optimization technique: activation functions.</p></li>
				<li>Now, navigate to the <strong class="source-inline">Activation Functions</strong> header in the Notebook. In this section, you only need to change the following variable:<p class="source-code">activation_function = "relu”</p><p>We have used the <strong class="source-inline">tanh</strong> function in this section, but feel free to try other activation functions. Review the list available at <a href="https://keras.io/activations/">https://keras.io/activations/</a> and try other possibilities.</p><p>Our final option is to try different regularization strategies. This is notably more complex and may take a few iterations for us to notice any gains—especially with so little data. Also, adding regularization strategies typically increases the training time of your network.</p></li>
				<li>Now, navigate to the <strong class="source-inline">Regularization Strategies</strong> header in the Notebook. In this section, you need to implement the <strong class="source-inline">Dropout()</strong> regularization strategy. Find the right place to put that step and implement it in our model:<div id="_idContainer106" class="IMG---Figure"><img src="image/B15911_03_25.jpg" alt="Figure 3.25: Jupyter Notebook showing code for regularization strategies&#13;&#10;"/></div><p class="figure-caption">Figure 3.25: Jupyter Notebook showing code for regularization strategies</p></li>
				<li>You can also try L2 regularization here (or combine both). Do the same as you did with <strong class="source-inline">Dropout()</strong>, but now using <strong class="source-inline">ActivityRegularization(l2=0.0001)</strong>.<p>Finally, let's evaluate our models using RMSE and MAPE.</p></li>
				<li>Now, navigate to the <strong class="source-inline">Evaluate Models</strong> header in the Notebook. In this section, we will evaluate the model predictions for the next 19 weeks of data in the test set. Then, we will compute the RMSE and MAPE of the predicted series versus the test series.<p>First plot looks as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B15911_03_26.jpg" alt="Figure 3.26: Prediction series versus test series #1&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.26: Prediction series versus test series #1</p>
			<p>Second plot looks as follows:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B15911_03_27.jpg" alt="Figure 3.27: Prediction series versus test series #2&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.27: Prediction series versus test series #2</p>
			<p>Third plot looks as follows:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B15911_03_28.jpg" alt="Figure 3.28: Prediction series versus test series #3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.28: Prediction series versus test series #3</p>
			<p>Fourth plot looks as follows:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B15911_03_29.jpg" alt="Figure 3.29: Prediction series versus test series #4&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.29: Prediction series versus test series #4</p>
			<p>Fifth plot will look as follows:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B15911_03_30.jpg" alt="Figure 3.30: Prediction series versus test series #5&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.30: Prediction series versus test series #5</p>
			<p>We have implemented the same evaluation techniques from <em class="italic">Exercise 2.01,</em> <em class="italic">Exploring Bitcoin Dataset</em>, <a href="https://packt.live/3ehbgCi">https://packt.live/3ehbgCi</a>, all wrapped in utility functions. Simply run all the cells from this section until the end of the Notebook to see the results.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZgAo87">https://packt.live/2ZgAo87</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3ft5Wgk">https://packt.live/3ft5Wgk</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this activity, we used different evaluation techniques to get more accurate results. We tried to train for more epochs, changed the activation function, added regularization, and compared results in different scenarios. Take this opportunity to tweak the values for the preceding optimization techniques and attempt to beat the performance of that model.</p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor124"/>Chapter 4: Productization</h1>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor125"/>Activity 4.01: Deploying a Deep Learning Application</h2>
			<p>Solution:</p>
			<p>In this activity, we deploy our model as a web application locally. This allows us to connect to the web application using a browser or to use another application through the application's HTTP API. You can find the code for this activity at <a href="https://packt.live/2Zdor2S">https://packt.live/2Zdor2S</a>.</p>
			<ol>
				<li value="1">Using your Terminal, navigate to the <strong class="source-inline">cryptonic</strong> directory and build the Docker images for all the required components:<p class="source-code">$ docker build --tag cryptonic:latest .</p><p class="source-code">$ docker build --tag cryptonic-cache:latest       cryptonic-cache/</p><p>Those two commands build the two images that we will use in this application: <strong class="source-inline">cryptonic</strong> (containing the Flask application) and <strong class="source-inline">cryptonic-cache</strong> (containing the Redis cache).</p></li>
				<li>After building the images, identify the <strong class="source-inline">docker-compose.yml</strong> file and open it in a text editor. Change the <strong class="source-inline">BITCOIN_START_DATE</strong> parameter to a date other than <strong class="source-inline">2017-01-01</strong>:<p class="source-code">BITCOIN_START_DATE = # Use other date here</p></li>
				<li>As a final step, deploy your web application locally using <strong class="source-inline">docker-compose up</strong>, as follows:<p class="source-code">docker-compose up</p><p>You should see a log of activity on your Terminal, including the training epochs completed by your model.</p></li>
				<li>After the model has been trained, you can visit your application at <strong class="source-inline">http:// localhost:5000</strong> and make predictions at <strong class="source-inline">http://localhost:5000/predict</strong>:<div id="_idContainer112" class="IMG---Figure"><img src="image/B15911_04_07.jpg" alt="Figure 4.7: Screenshot of the Cryptonic application deployed locally&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 4.7: Scree<a id="_idTextAnchor126"/>nshot of the Cryptonic application deployed locally</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Zg0wjd">https://packt.live/2Zg0wjd</a>.</p>
			<p class="callout">This section does not currently have an online interactive example, and will need to be run locally.</p>
		</div>
		<div>
			<div id="_idContainer114" class="Basic-Text-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer115" class="Content">
			</div>
		</div>
	</body></html>