["```\nIf length(fish)> length* then label(fish) = Tuna\nOtherwise label(fish) = Opah \n```", "```\nIf length(fish)> 7 then label(fish) = Tuna\nOtherwise label(fish) = Opah\n```", "```\nimport numpy as np\nnp.random.seed(2018)\n```", "```\nimport os\nimport glob\nimport cv2\nimport datetime\nimport pandas as pd\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n```", "```\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\nfrom keras.optimizers import SGD\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom sklearn.metrics import log_loss\nfrom keras import __version__ as keras_version\n```", "```\n# Parameters\n# ----------\n# img_path : path\n#    path of the image to be resized\ndef rezize_image(img_path):\n   #reading image file\n   img = cv2.imread(img_path)\n   #Resize the image to to be 32 by 32\n   img_resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)\nreturn img_resized\n```", "```\n# Loading the training samples and their corresponding labels\ndef load_training_samples():\n    #Variables to hold the training input and output variables\n    train_input_variables = []\n    train_input_variables_id = []\n    train_label = []\n    # Scanning all images in each folder of a fish type\n    print('Start Reading Train Images')\n    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n    for fld in folders:\n       folder_index = folders.index(fld)\n       print('Load folder {} (Index: {})'.format(fld, folder_index))\n       imgs_path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n       files = glob.glob(imgs_path)\n       for file in files:\n           file_base = os.path.basename(file)\n           # Resize the image\n           resized_img = rezize_image(file)\n           # Appending the processed image to the input/output variables of the classifier\n           train_input_variables.append(resized_img)\n           train_input_variables_id.append(file_base)\n           train_label.append(folder_index)\n     return train_input_variables, train_input_variables_id, train_label\n```", "```\ndef load_testing_samples():\n    # Scanning images from the test folder\n    imgs_path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n    files = sorted(glob.glob(imgs_path))\n    # Variables to hold the testing samples\n    testing_samples = []\n    testing_samples_id = []\n    #Processing the images and appending them to the array that we have\n    for file in files:\n       file_base = os.path.basename(file)\n       # Image resizing\n       resized_img = rezize_image(file)\n       testing_samples.append(resized_img)\n       testing_samples_id.append(file_base)\n    return testing_samples, testing_samples_id\n```", "```\ndef load_normalize_training_samples():\n    # Calling the load function in order to load and resize the training samples\n    training_samples, training_label, training_samples_id = load_training_samples()\n    # Converting the loaded and resized data into Numpy format\n    training_samples = np.array(training_samples, dtype=np.uint8)\n    training_label = np.array(training_label, dtype=np.uint8)\n    # Reshaping the training samples\n    training_samples = training_samples.transpose((0, 3, 1, 2))\n    # Converting the training samples and training labels into float format\n    training_samples = training_samples.astype('float32')\n    training_samples = training_samples / 255\n    training_label = np_utils.to_categorical(training_label, 8)\n    return training_samples, training_label, training_samples_id\n```", "```\ndef load_normalize_testing_samples():\n    # Calling the load function in order to load and resize the testing samples\n    testing_samples, testing_samples_id = load_testing_samples()\n    # Converting the loaded and resized data into Numpy format\n    testing_samples = np.array(testing_samples, dtype=np.uint8)\n    # Reshaping the testing samples\n    testing_samples = testing_samples.transpose((0, 3, 1, 2))\n    # Converting the testing samples into float format\n    testing_samples = testing_samples.astype('float32')\n    testing_samples = testing_samples / 255\n    return testing_samples, testing_samples_id\n```", "```\ndef create_cnn_model_arch():\n    pool_size = 2 # we will use 2x2 pooling throughout\n    conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n    conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n    kernel_size = 3 # we will use 3x3 kernels throughout\n    drop_prob = 0.5 # dropout in the FC layer with probability 0.5\n    hidden_size = 32 # the FC layer will have 512 neurons\n    num_classes = 8 # there are 8 fish types\n    # Conv [32] -> Conv [32] -> Pool\n    cnn_model = Sequential()\n    cnn_model.add(ZeroPadding2D((1, 1), input_shape=(3, 32, 32), dim_ordering='th'))\n    cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu', \n      dim_ordering='th'))\n    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n    cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu',\n      dim_ordering='th'))\n    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2),\n      dim_ordering='th'))\n    # Conv [64] -> Conv [64] -> Pool\n    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu',\n      dim_ordering='th'))\n    cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n    cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu',\n      dim_ordering='th'))\n    cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2),\n     dim_ordering='th'))\n    # Now flatten to 1D, apply FC then ReLU (with dropout) and finally softmax(output layer)\n    cnn_model.add(Flatten())\n    cnn_model.add(Dense(hidden_size, activation='relu'))\n    cnn_model.add(Dropout(drop_prob))\n    cnn_model.add(Dense(hidden_size, activation='relu'))\n    cnn_model.add(Dropout(drop_prob))\n    cnn_model.add(Dense(num_classes, activation='softmax'))\n    # initiating the stochastic gradient descent optimiser\n    stochastic_gradient_descent = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)    cnn_model.compile(optimizer=stochastic_gradient_descent,  # using the stochastic gradient descent optimiser\n                  loss='categorical_crossentropy')  # using the cross-entropy loss function\n    return cnn_model\n```", "```\ndef create_model_with_kfold_cross_validation(nfolds=10):\n    batch_size = 16 # in each iteration, we consider 32 training examples at once\n    num_epochs = 30 # we iterate 200 times over the entire training set\n    random_state = 51 # control the randomness for reproducibility of the results on the same platform\n    # Loading and normalizing the training samples prior to feeding it to the created CNN model\n    training_samples, training_samples_target, training_samples_id = \n      load_normalize_training_samples()\n    yfull_train = dict()\n    # Providing Training/Testing indices to split data in the training samples\n    # which is splitting data into 10 consecutive folds with shuffling\n    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)\n    fold_number = 0 # Initial value for fold number\n    sum_score = 0 # overall score (will be incremented at each iteration)\n    trained_models = [] # storing the modeling of each iteration over the folds\n    # Getting the training/testing samples based on the generated training/testing indices by \n      Kfold\n    for train_index, test_index in kf:\n       cnn_model = create_cnn_model_arch()\n       training_samples_X = training_samples[train_index] # Getting the training input variables\n       training_samples_Y = training_samples_target[train_index] # Getting the training output/label variable\n       validation_samples_X = training_samples[test_index] # Getting the validation input variables\n       validation_samples_Y = training_samples_target[test_index] # Getting the validation output/label variable\n       fold_number += 1\n       print('Fold number {} from {}'.format(fold_number, nfolds))\n       callbacks = [\n           EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n       ]\n       # Fitting the CNN model giving the defined settings\n       cnn_model.fit(training_samples_X, training_samples_Y, batch_size=batch_size,\n         nb_epoch=num_epochs,\n             shuffle=True, verbose=2, validation_data=(validation_samples_X,\n               validation_samples_Y),\n             callbacks=callbacks)\n       # measuring the generalization ability of the trained model based on the validation set\n       predictions_of_validation_samples = \n         cnn_model.predict(validation_samples_X.astype('float32'), \n         batch_size=batch_size, verbose=2)\n       current_model_score = log_loss(Y_valid, predictions_of_validation_samples)\n       print('Current model score log_loss: ', current_model_score)\n       sum_score += current_model_score*len(test_index)\n       # Store valid predictions\n       for i in range(len(test_index)):\n           yfull_train[test_index[i]] = predictions_of_validation_samples[i]\n       # Store the trained model\n       trained_models.append(cnn_model)\n    # incrementing the sum_score value by the current model calculated score\n    overall_score = sum_score/len(training_samples)\n    print(\"Log_loss train independent avg: \", overall_score)\n    #Reporting the model loss at this stage\n    overall_settings_output_string = 'loss_' + str(overall_score) + '_folds_' + str(nfolds) + \n      '_ep_' + str(num_epochs)\n    return overall_settings_output_string, trained_models\n```", "```\ndef test_generality_crossValidation_over_test_set( overall_settings_output_string, cnn_models):\n    batch_size = 16 # in each iteration, we consider 32 training examples at once\n    fold_number = 0 # fold iterator\n    number_of_folds = len(cnn_models) # Creating number of folds based on the value used in the training step\n    yfull_test = [] # variable to hold overall predictions for the test set\n    #executing the actual cross validation test process over the test set\n    for j in range(number_of_folds):\n       model = cnn_models[j]\n       fold_number += 1\n       print('Fold number {} out of {}'.format(fold_number, number_of_folds))\n       #Loading and normalizing testing samples\n       testing_samples, testing_samples_id = load_normalize_testing_samples()\n       #Calling the current model over the current test fold\n       test_prediction = model.predict(testing_samples, batch_size=batch_size, verbose=2)\n       yfull_test.append(test_prediction)\n    test_result = merge_several_folds_mean(yfull_test, number_of_folds)\n    overall_settings_output_string = 'loss_' + overall_settings_output_string \\ + '_folds_' +\n      str(number_of_folds)\n    format_results_for_types(test_result, testing_samples_id, overall_settings_output_string)\n```", "```\nif __name__ == '__main__':\n  info_string, models = create_model_with_kfold_cross_validation()\n  test_generality_crossValidation_over_test_set(info_string, models)\n```"]