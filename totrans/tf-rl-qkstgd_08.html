<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep RL Applied to Autonomous Driving</h1>
                </header>
            
            <article>
                
<p>Autonomous driving is one of the hottest technological revolutions in development as of the time of writing this. It will dramatically alter how humanity looks at transportation in general, and will drastically reduce travel costs as well as increase safety. Several state-of-the-art algorithms are used by the autonomous vehicle development community to this end. These include, but are not limited to, perception, localization, path planning, and control. Perception deals with the identification of the environment around an autonomous vehicle—pedestrians, cars, bicycles, and so on. Localization involves the identification of the exact location—or pose to be more precise—of the vehicle in a precomputed map of the environment. Path planning, as the name implies, is the process of planning the path of the autonomous vehicle, both in the long term (say, from point <em>A</em> to point <em>B</em>) as well as the shorter term (say, the next 5 seconds). Control is the actual execution of the desired path, including evasive maneuvers. In particular, <strong>reinforcement learning</strong> (<strong>RL</strong>) is widely used in the path planning and control of the autonomous vehicle, both for urban as well as highway <span>driving</span><span>.</span></p>
<p>In this chapter, we will use the <strong><span>The Open Racing Car Simulator</span></strong> (<strong>TORCS</strong>) simulator to train an RL agent to learn to successfully drive on a racetrack. While the CARLA simulator is more robust and has realistic rendering, TORCS is easier to use and so is a good first option. The interested reader is encouraged to try out training RL agents on the CARLA simulator after completing this book.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Learning to use TORCS</li>
<li>Training a <strong>Deep Deterministic Policy Gradient</strong> (<strong>DDPG</strong>) agent to learn to drive</li>
<li>Training a <strong>Proximal Policy Optimization</strong> (<strong>PPO</strong>) agent</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p class="mce-root">To complete this chapter, we will need the following:</p>
<ul>
<li>Python (version 2 or 3)</li>
<li>NumPy</li>
<li>Matplotlib</li>
<li>TensorFlow (version 1.4 or higher)</li>
<li>TORCS racing car simulator</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Car driving simulators</h1>
                </header>
            
            <article>
                
<p>Applying RL in autonomous driving necessitates the use of robust car-driving simulators, as the RL agent cannot be trained on the road directly. To this end, several open source car-driving simulators have been developed by the research community, with each having its own pros and cons. Some of the open source car driving simulators are:</p>
<ul>
<li>CARLA
<ul>
<li><a href="http://vladlen.info/papers/carla.pdf" target="_blank">http://vladlen.info/papers/carla.pdf</a></li>
<li>Developed at Intel labs</li>
<li>Suited to urban driving</li>
</ul>
</li>
<li>TORCS<br/>
<ul>
<li><a href="http://torcs.sourceforge.net/" target="_blank">http://torcs.sourceforge.net/</a></li>
<li>Racing car</li>
</ul>
</li>
<li>DeepTraffic
<ul>
<li><a href="https://selfdrivingcars.mit.edu/deeptraffic/" target="_blank">https://selfdrivingcars.mit.edu/deeptraffic/</a></li>
<li>Developed at MIT</li>
<li>Suited to highway driving</li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning to use TORCS</h1>
                </header>
            
            <article>
                
<p>We will first learn how to use the TORCS racing car simulator, which is an open source simulator. You can obtain the download instructions from <a href="http://torcs.sourceforge.net/index.php?name=Sections&amp;op=viewarticle&amp;artid=3">http://torcs.sourceforge.net/index.php?name=Sections&amp;op=viewarticle&amp;artid=3</a> but the salient steps are summarized as follows for Linux:</p>
<ol>
<li>Download the <kbd>torcs-1.3.7.tar.bz2</kbd> file from <a href="https://sourceforge.net/projects/torcs/files/all-in-one/1.3.7/torcs-1.3.7.tar.bz2/download" target="_blank">https://sourceforge.net/projects/torcs/files/all-in-one/1.3.7/torcs-1.3.7.tar.bz2/download</a></li>
<li>Unpack the package with <kbd>tar xfvj torcs-1.3.7.tar.bz2</kbd></li>
<li>Run the following commands:
<ul>
<li><kbd>cd torcs-1.3.7</kbd></li>
<li><kbd>./configure</kbd></li>
<li><kbd>make</kbd></li>
<li><kbd>make install</kbd></li>
<li><kbd>make datainstall</kbd></li>
</ul>
</li>
<li>
<p>The default installation directories are:</p>
<ul>
<li><kbd>/usr/local/bin</kbd>: TORCS command (directory should be in your <kbd>PATH</kbd>)</li>
<li><kbd>/usr/local/lib/torcs</kbd>: TORCS dynamic <kbd>libs</kbd> (directory MUST be in your <kbd>LD_LIBRARY_PATH</kbd> if you don't use the TORCS shell)</li>
<li><kbd>/usr/local/share/games/torcs</kbd>: TORCS data files</li>
</ul>
</li>
</ol>
<p>By running the <kbd>torcs</kbd> command (the default location is <kbd>/usr/local/bin/torcs</kbd>), you can now see the TORCS simulator open. The desired settings can then be chosen, including the choice of the car, racetrack, and so on. The simulator can also be played as a video game, but we are interested in using it to train an RL agent.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">State space</h1>
                </header>
            
            <article>
                
<p class="mce-root">We will next define the state space for TORCS. <em>Table 2: Description of the available sensors (part II). Ranges are reported with their unit of measure (where defined)</em> of the <em>Simulated Car Racing Championship: Competition Software Manual</em> document at <a href="https://arxiv.org/pdf/1304.1672.pdf">https://arxiv.org/pdf/1304.1672.pdf</a> provides a summary of the state parameters that are available for the simulator. We will use the following entries as our state space; the number in brackets identifies the size of the entry:</p>
<ul>
<li><kbd>angle</kbd>: Angle between the car direction and the track (1)</li>
<li><kbd>track</kbd>: This will give us the end of the track measured every 10 degrees from -90 to +90 degrees; it has 19 real values, counting the end values (19)</li>
<li><kbd>trackPos</kbd>: Distance between the car and the track axis (1)</li>
<li><kbd>speedX</kbd>: Speed of the car in the longitudinal direction (1)</li>
<li><kbd>speedY</kbd>: Speed of the car in the transverse direction (1)</li>
<li><kbd>speedZ</kbd>: Speed of the car in the <em>Z</em>-direction; we don't need this actually, but we retain it for now (1)</li>
<li><kbd>wheelSpinVel</kbd>: The rotational speed of the four wheels of the car (4)</li>
<li><kbd>rpm</kbd>: The car engine's rpm (1)</li>
</ul>
<p>See the previously mentioned document for a better understanding of the preceding variables, including their permissible ranges. Summing up the number of real valued entries, we note that our state space is a real valued vector of size 1+19+1+1+1+1+4+1 = 29. Our action space is of size <em>3</em>: the steering, acceleration, and brake. Steering is in the range [<em>-1,1</em>], and acceleration is in the range [<em>0,1</em>], as is the brake.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Support files</h1>
                </header>
            
            <article>
                
<p>The open source community has also developed two Python files to interface TORCS with Python so that we can call TORCS from Python commands. In addition, to automatically start TORCS, we need another <kbd>sh</kbd> file. These three files are summarized as follows:</p>
<ul>
<li><kbd>gym_torcs.py</kbd></li>
<li><kbd>snakeoil3_gym.py</kbd></li>
<li><kbd>autostart.sh</kbd></li>
</ul>
<p>These files are included in the code files for the chapter (<a href="https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide" target="_blank">https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide</a>), but can also be obtained from a Google search. In lines ~130-160 of <kbd>gym_torcs.py</kbd>, the reward function is set. You can see the following lines, which convert the raw simulator states to NumPy arrays:</p>
<pre># Reward setting Here #######################################<br/># direction-dependent positive reward<br/>track = np.array(obs['track'])<br/>trackPos = np.array(obs['trackPos'])<br/>sp = np.array(obs['speedX'])<br/>damage = np.array(obs['damage'])<br/>rpm = np.array(obs['rpm'])</pre>
<p>The reward function is then set as follows. Note that we give rewards for higher longitudinal speed along the track (the cosine of the angle term), and penalize lateral speed (the sine of the angle term). Track position is also penalized. Ideally, if this were zero, we would be at the center of the track, and values of <em>+1</em> or <em>-1</em> imply that we are at the edges of the track, which is not desired and hence penalized:</p>
<pre>progress = sp*np.cos(obs['angle']) - np.abs(sp*np.sin(obs['angle'])) - sp * np.abs(obs['trackPos'])<br/>reward = progress</pre>
<p>We terminate the episode if the car is out of the track and/or the progress of the agent is stuck using the following code:</p>
<pre>if (abs(track.any()) &gt; 1 or abs(trackPos) &gt; 1): # Episode is terminated if the car is out of track<br/>    print("Out of track ")<br/>    reward = -100 #-200<br/>    episode_terminate = True<br/>    client.R.d['meta'] = True<br/><br/>if self.terminal_judge_start &lt; self.time_step: # Episode terminates if the progress of agent is small<br/>    if progress &lt; self.termination_limit_progress:<br/>         print("No progress", progress)<br/>         reward = -100 # KAUSHIK ADDED THIS<br/>         episode_terminate = True<br/>         client.R.d['meta'] = True</pre>
<p>We are now ready to train an RL agent to drive a car in TORCS successfully. We will use a DDPG agent first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a DDPG agent to learn to drive</h1>
                </header>
            
            <article>
                
<p>Most of the DDPG code is the same as we saw earlier in <a href="c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml" target="_blank">Chapter 5</a>, <em>Deep Deterministic Policy Gradients (DDPG)</em>; only the differences will be summarized here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding ddpg.py</h1>
                </header>
            
            <article>
                
<p>Our state dimension for TORCS is <kbd>29</kbd> and the action dimension is <kbd>3</kbd>; these are set in <kbd>ddpg.py</kbd> as follows:</p>
<pre>state_dim = 29<br/>action_dim = 3<br/>action_bound = 1.0</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding AandC.py</h1>
                </header>
            
            <article>
                
<p>The actor and critic file, <kbd>AandC.py</kbd>, also needs to be modified. In particular, the <kbd>create_actor_network</kbd> in the <kbd>ActorNetwork</kbd> class is edited to have two hidden layers with <kbd>400</kbd> and <kbd>300</kbd> neurons, respectively. Also, the output consists of three actions: <kbd>steering</kbd>, <kbd>acceleration</kbd>, and <kbd>brake</kbd>. Since steering is in the [<em>-1,1</em>] range, the <kbd>tanh</kbd> activation function is used; <kbd>acceleration</kbd> and <kbd>brake</kbd> are in the [<em>0,1</em>] range, and so the <kbd>sigmoid</kbd> activation function is used. We then <kbd>concat</kbd> them along axis dimension <kbd>1</kbd>, and this is the output of our actor's policy:</p>
<pre>   def create_actor_network(self, scope):<br/>       with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):<br/>          state = tf.placeholder(name='a_states', dtype=tf.float32, shape=[None, self.s_dim])<br/>  <br/>          net = tf.layers.dense(inputs=state, units=400, activation=None, kernel_initializer=winit, bias_initializer=binit, name='anet1') <br/>          net = tf.nn.relu(net)<br/><br/>          net = tf.layers.dense(inputs=net, units=300, activation=None, kernel_initializer=winit, bias_initializer=binit, name='anet2')<br/>          net = tf.nn.relu(net)<br/>          <br/>          steering = tf.layers.dense(inputs=net, units=1, activation=tf.nn.tanh, kernel_initializer=rand_unif, bias_initializer=binit, name='steer') <br/>          acceleration = tf.layers.dense(inputs=net, units=1, activation=tf.nn.sigmoid, kernel_initializer=rand_unif, bias_initializer=binit, name='acc') <br/>          brake = tf.layers.dense(inputs=net, units=1, activation=tf.nn.sigmoid, kernel_initializer=rand_unif, bias_initializer=binit, name='brake') <br/>     <br/>          out = tf.concat([steering, acceleration, brake], axis=1) <br/><br/>          return state, out</pre>
<p>Likewise, the <kbd>CriticNetwork</kbd> class <kbd>create_critic_network()</kbd> function is edited to have two hidden layers for the neural network, with <kbd>400</kbd> and <kbd>300</kbd> neurons, respectively. This is shown in the following code:</p>
<pre>    def create_critic_network(self, scope):<br/>        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):<br/>           state = tf.placeholder(name='c_states', dtype=tf.float32, shape=[None, self.s_dim]) <br/>           action = tf.placeholder(name='c_action', dtype=tf.float32, shape=[None, self.a_dim]) <br/><br/>           net = tf.concat([state, action],1) <br/><br/>           net = tf.layers.dense(inputs=net, units=400, activation=None, kernel_initializer=winit, bias_initializer=binit, name='cnet1') <br/>           net = tf.nn.relu(net)<br/><br/>           net = tf.layers.dense(inputs=net, units=300, activation=None, kernel_initializer=winit, bias_initializer=binit, name='cnet2') <br/>           net = tf.nn.relu(net)<br/><br/>           out = tf.layers.dense(inputs=net, units=1, activation=None, kernel_initializer=rand_unif, bias_initializer=binit, name='cnet_out') <br/>           return state, action, out</pre>
<p>The other changes to be made are in <kbd>TrainOrTest.py</kbd>, which we will look into next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Coding TrainOrTest.py</h1>
                </header>
            
            <article>
                
<p>Import the TORCS environment from <kbd>gym_torcs</kbd> so that we can train the RL agent on it:</p>
<ol>
<li><strong>Import TORCS</strong>: Import the TORCS environment from <kbd>gym_torcs</kbd> as follows:</li>
</ol>
<pre style="padding-left: 60px">from gym_torcs import TorcsEnv</pre>
<ol start="2">
<li><strong>The</strong> <kbd>env</kbd> <strong>variable</strong>: Create a TORCS environment variable using the following command:</li>
</ol>
<pre style="padding-left: 30px">    # Generate a Torcs environment<br/>    env = TorcsEnv(vision=False, throttle=True, gear_change=False)</pre>
<ol start="3">
<li class="mce-root"><strong>Relaunch TORCS</strong>: Since TORCS is known to have a memory leak error, reset the environment every <kbd>100</kbd> episodes using <kbd>relaunch=True</kbd>; otherwise reset without any arguments as follows:</li>
</ol>
<pre style="padding-left: 60px">if np.mod(i, 100) == 0:<br/>    ob = env.reset(relaunch=True) #relaunch TORCS every N episodes  due to a memory leak error<br/>else:<br/>    ob = env.reset()</pre>
<ol start="4">
<li><strong>Stack up state space</strong>: Use the following command to stack up the 29-dimension space:</li>
</ol>
<pre style="padding-left: 60px">s = np.hstack((ob.angle, ob.track, ob.trackPos, ob.speedX, ob.speedY, ob.speedZ, ob.wheelSpinVel/100.0, ob.rpm))</pre>
<ol start="5">
<li><strong>Number of time steps per episode</strong>: Choose the number of time steps <kbd>msteps</kbd> to run per episode. For the first <kbd>100</kbd> episodes<span>, the agent has not learned much, and so you can choose <kbd>100</kbd> time steps per episode; we gradually increase this linearly for later episodes up to the upper limit of <kbd>max_steps</kbd>. </span></li>
</ol>
<div class="packt_tip"><span>This step is not critical and the agent's learning is not dependent on the number of steps we choose per episode. Feel free to experiment with how </span><kbd>msteps</kbd><span> is set.</span></div>
<p class="mce-root" style="padding-left: 60px">    Choose the number of time steps as follows:</p>
<pre style="padding-left: 60px">msteps = max_steps<br/>if (i &lt; 100):<br/>    msteps = 100<br/>elif (i &gt;=100 and i &lt; 200):<br/>    msteps = 100 + (i-100)*9<br/>else: <br/>    msteps = 1000 + (i-200)*5<br/>    msteps = min(msteps, max_steps)</pre>
<ol start="6">
<li><strong>Full throttle</strong>: For the first <kbd>10</kbd> episodes, we apply full throttle to warm up the neural network parameters. Only after that, do we start using the actor's policy. Note that TORCS typically learns in about ~1,500–2,000 episodes, so the first <kbd>10</kbd> episodes will not really have much influence later on in the learning. A<span>pply full throttle to warm up the neural network parameters as follows</span>:</li>
</ol>
<pre style="padding-left: 60px"># first few episodes step on gas! <br/>if (i &lt; 10):<br/>    a[0][0] = 0.0<br/>    a[0][1] = 1.0<br/>    a[0][2] = 0.0</pre>
<p>That's it for the changes that need to be made to the code for the DDPG to play TORCS. The rest of the code is the same as that covered in <a href="c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml" target="_blank">Chapter 5</a>, <em>Deep Deterministic Policy Gradients (DDPG)</em>. We can train the agent using the following command:</p>
<pre><strong>python ddpg.py</strong></pre>
<p> </p>
<p>Enter <kbd>1</kbd> for training; <kbd>0</kbd> is for testing a pretrained agent. Training can take about 2–5 days depending on the speed of the computer used. But this is a fun problem and is worth the effort. The number of steps experienced per episode, as well as the rewards, are stored in <kbd>analysis_file.txt</kbd>, which we can plot. The number of time steps per episode is plotted as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-523 image-border" src="assets/3a0210b4-3bb8-4c79-9db0-22b783f78002.png" style="width:28.92em;height:21.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 1: Number of time steps per episode of TORCS (training mode)</div>
<p>We can see that the car has learned to drive reasonably well after ~600 episodes, with more efficient driving after ~1,500 episodes. Approximately ~300 time steps correspond to one lap of the racetrack. Thus, the agent is able to drive more than seven to eight laps without terminating toward the end of the training. For a cool video of the DDPG agent driving, see the following YouTube link: <a href="https://www.youtube.com/watch?v=ajomz08hSIE">https://www.youtube.com/watch?v=ajomz08hSIE</a>.<a href="https://www.youtube.com/watch?v=ajomz08hSIE"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a PPO agent</h1>
                </header>
            
            <article>
                
<p>We saw previously how to train a DDPG agent to drive a car on TORCS. How to use a PPO agent is left as an exercise for the interested reader. This is a nice challenge to complete. The PPO code from <a href="7f55a061-06a5-4f69-ab05-4eff75c2dacd.xhtml" target="_blank"/><a href="7f55a061-06a5-4f69-ab05-4eff75c2dacd.xhtml" target="_blank">Chapter 7</a>, <em>Trust Region Policy Optimization and Proximal Policy Optimization</em>, can be reused, with the necessary changes made to the TORCS environment. The PPO code for TORCS is also supplied in the code repository (<a href="https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide" target="_blank">https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide</a>), and the interested reader can peruse it. A cool video of a PPO agent driving a car in TORCS is in the following YouTube video at: <a href="https://youtu.be/uE8QaJQ7zDI" target="_blank">https://youtu.be/uE8QaJQ7zDI</a></p>
<p>Another challenge for the interested reader is to use <strong>Trust Region Policy Optimization</strong> (<strong>TRPO</strong>) for the TORCS racing car problem. Try this too, if interested! This is one way to master RL algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we saw how to apply RL algorithms to train an agent to learn to drive a car autonomously. We installed the TORCS racing-car simulator and also learned how to interface it with Python, so that we can train RL agents. We also did a deep dive into the state space for TORCS and the meaning of each of these terms. The DDPG algorithm was then used to train an agent to learn to drive successfully in TORCS. The video rendering in TORCS is really cool! The trained agent was able to drive more than seven to eight laps around the racetrack successfully. Finally, the use of PPO for the same problem of driving a car autonomously was also explored and left as an exercise for the interested reader; code for this is supplied in the book's repository.</p>
<p>This concludes this chapter as well as the book. Feel free to read upon more material online on the application of RL for autonomous driving and robotics. This is now a very hot area of both academic and industry research, and is well funded, with several job openings in these areas. Wishing you the best!</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Why can you not use DQN for the TORCS problem?</li>
<li>We used the Xavier weights initializer for the neural network weights. What other weight initializers are you aware of, and how well will the trained agent perform with them?</li>
<li>Why is the <kbd>abs()</kbd> function used in the reward function, and why is it used for the last two terms but not for the first term?</li>
<li>How can you ensure smoother driving than what was observed in the video?</li>
<li>Why is a replay buffer used in DDPG but not in PPO?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><span><em>Continuous control with deep reinforcement learning</em>, Timothy P. Lillicrap, Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, Daan Wierstra, arXiv</span>:1509.02971<span> (DDPG paper): <a href="https://arxiv.org/abs/1509.02971" target="_blank">https://arxiv.org/abs/1509.02971</a><br/></span></li>
<li><em>Proximal Policy Optimization Algorithms</em>, John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov, arXiv:1707.06347 (PPO paper): <a href="https://arxiv.org/abs/1707.06347" target="_blank">https://arxiv.org/abs/1707.06347</a></li>
<li>TORCS: <a href="http://torcs.sourceforge.net/" target="_blank">http://torcs.sourceforge.net/</a></li>
<li><em>Deep Reinforcement Learning Hands-On</em>, by <em>Maxim Lapan</em>, <em>Packt Publishing</em>: <a href="https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands" target="_blank">https://www.packtpub.com/big-data-and-business-intelligence/deep-reinforcement-learning-hands</a></li>
</ul>


            </article>

            
        </section>
    </body></html>