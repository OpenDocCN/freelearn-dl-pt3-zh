["```\n#Loading the required libraries along with the deep learning platform Keras with TensorFlow as backend\nimport numpy as np\nnp.random.seed(2017)\nimport os\nimport glob\nimport cv2\nimport pandas as pd\nimport time\nimport warnings\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\nfrom keras.optimizers import SGD\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom sklearn.metrics import log_loss\nfrom keras import __version__ as keras_version\n# Parameters\n# ----------\n# x : type\n#    Description of parameter `x`.\ndef rezize_image(img_path):\n  img = cv2.imread(img_path)\n  img_resized = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)\n  return img_resized\n#Loading the training samples from their corresponding folder names, where we have a folder for each type\ndef load_training_samples():\n  #Variables to hold the training input and output variables\n  train_input_variables = []\n  train_input_variables_id = []\n  train_label = []\n  # Scanning all images in each folder of a fish type\n  print('Start Reading Train Images')\n  folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n  for fld in folders:\n      folder_index = folders.index(fld)\n      print('Load folder {} (Index: {})'.format(fld, folder_index))\n      imgs_path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n      files = glob.glob(imgs_path)\n      for file in files:\n          file_base = os.path.basename(file)\n          # Resize the image\n          resized_img = rezize_image(file)\n          # Appending the processed image to the input/output variables of the classifier\n          train_input_variables.append(resized_img)\n          train_input_variables_id.append(file_base)\n          train_label.append(folder_index)\n  return train_input_variables, train_input_variables_id, train_label\n#Loading the testing samples which will be used to testing how well the model was trained\ndef load_testing_samples():\n  # Scanning images from the test folder\n  imgs_path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n  files = sorted(glob.glob(imgs_path))\n  # Variables to hold the testing samples\n  testing_samples = []\n  testing_samples_id = []\n  #Processing the images and appending them to the array that we have\n  for file in files:\n      file_base = os.path.basename(file)\n      # Image resizing\n      resized_img = rezize_image(file)\n      testing_samples.append(resized_img)\n      testing_samples_id.append(file_base)\n  return testing_samples, testing_samples_id\n# formatting the images to fit our model\ndef format_results_for_types(predictions, test_id, info):\n  model_results = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER',\n    'SHARK', 'YFT'])\n  model_results.loc[:, 'image'] = pd.Series(test_id, index=model_results.index)\n  sub_file = 'testOutput_' + info + '.csv'\n  model_results.to_csv(sub_file, index=False)\ndef load_normalize_training_samples():\n  # Calling the load function in order to load and resize the training samples\n  training_samples, training_label, training_samples_id = load_training_samples()\n  # Converting the loaded and resized data into Numpy format\n  training_samples = np.array(training_samples, dtype=np.uint8)\n  training_label = np.array(training_label, dtype=np.uint8)\n  # Reshaping the training samples\n  training_samples = training_samples.transpose((0, 3, 1, 2))\n  # Converting the training samples and training labels into float format\n  training_samples = training_samples.astype('float32')\n  training_samples = training_samples / 255\n  training_label = np_utils.to_categorical(training_label, 8)\n  return training_samples, training_label, training_samples_id\n#Loading and normalizing the testing sample to fit into our model\ndef load_normalize_testing_samples():\n  # Calling the load function in order to load and resize the testing samples\n  testing_samples, testing_samples_id = load_testing_samples()\n  # Converting the loaded and resized data into Numpy format\n  testing_samples = np.array(testing_samples, dtype=np.uint8)\n  # Reshaping the testing samples\n  testing_samples = testing_samples.transpose((0, 3, 1, 2))\n  # Converting the testing samples into float format\n  testing_samples = testing_samples.astype('float32')\n  testing_samples = testing_samples / 255\n  return testing_samples, testing_samples_id\ndef merge_several_folds_mean(data, num_folds):\n  a = np.array(data[0])\n  for i in range(1, num_folds):\n      a += np.array(data[i])\n  a /= num_folds\n  return a.tolist()\n# Create CNN model architecture\ndef create_cnn_model_arch():\n  pool_size = 2 # we will use 2x2 pooling throughout\n  conv_depth_1 = 32 # we will initially have 32 kernels per conv. layer...\n  conv_depth_2 = 64 # ...switching to 64 after the first pooling layer\n  kernel_size = 3 # we will use 3x3 kernels throughout\n  drop_prob = 0.5 # dropout in the FC layer with probability 0.5\n  hidden_size = 32 # the FC layer will have 512 neurons\n  num_classes = 8 # there are 8 fish types\n  # Conv [32] -> Conv [32] -> Pool\n  cnn_model = Sequential()\n  cnn_model.add(ZeroPadding2D((1, 1), input_shape=(3, 32, 32), dim_ordering='th'))\n  cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu', dim_ordering='th'))\n  cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th')\n  cnn_model.add(Convolution2D(conv_depth_1, kernel_size, kernel_size, activation='relu', dim_ordering='th'))\n  cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2), dim_ordering='th'))\n  # Conv [64] -> Conv [64] -> Pool\n  cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n  cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu', dim_ordering='th'))\n  cnn_model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n  cnn_model.add(Convolution2D(conv_depth_2, kernel_size, kernel_size, activation='relu', dim_ordering='th'))\n  cnn_model.add(MaxPooling2D(pool_size=(pool_size, pool_size), strides=(2, 2), dim_ordering='th'))\n  # Now flatten to 1D, apply FC then ReLU (with dropout) and finally softmax(output layer)\n  cnn_model.add(Flatten())\n  cnn_model.add(Dense(hidden_size, activation='relu'))\n  cnn_model.add(Dropout(drop_prob))\n  cnn_model.add(Dense(hidden_size, activation='relu'))\n  cnn_model.add(Dropout(drop_prob))\n  cnn_model.add(Dense(num_classes, activation='softmax'))\n  # initiating the stochastic gradient descent optimiser\n  stochastic_gradient_descent = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n  cnn_model.compile(optimizer=stochastic_gradient_descent,  # using the stochastic gradient descent optimiser\n                    loss='categorical_crossentropy')  # using the cross-entropy loss function\n  return cnn_model\n#Model using with kfold cross validation as a validation method\ndef create_model_with_kfold_cross_validation(nfolds=10):\n  batch_size = 16 # in each iteration, we consider 32 training examples at once\n  num_epochs = 30 # we iterate 200 times over the entire training set\n  random_state = 51 # control the randomness for reproducibility of the results on the same platform\n  # Loading and normalizing the training samples prior to feeding it to the created CNN model\n  training_samples, training_samples_target, training_samples_id = load_normalize_training_samples()\n  yfull_train = dict()\n  # Providing Training/Testing indices to split data in the training samples\n  # which is splitting data into 10 consecutive folds with shuffling\n  kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)\n  fold_number = 0 # Initial value for fold number\n  sum_score = 0 # overall score (will be incremented at each iteration)\n  trained_models = [] # storing the modeling of each iteration over the folds\n  # Getting the training/testing samples based on the generated training/testing indices by Kfold\n  for train_index, test_index in kf:\n      cnn_model = create_cnn_model_arch()\n      training_samples_X = training_samples[train_index] # Getting the training input variables\n      training_samples_Y = training_samples_target[train_index] # Getting the training output/label variable\n      validation_samples_X = training_samples[test_index] # Getting the validation input variables\n      validation_samples_Y = training_samples_target[test_index] # Getting the validation output/label variabl\n      fold_number += 1\n      print('Fold number {} out of {}'.format(fold_number, nfolds))\n      callbacks = [\n          EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n      ]\n      # Fitting the CNN model giving the defined settings\n      cnn_model.fit(training_samples_X, training_samples_Y, batch_size=batch_size,\n        nb_epoch=num_epochs,\n            shuffle=True, verbose=2, validation_data=(validation_samples_X,\n              validation_samples_Y),\n            callbacks=callbacks)\n      # measuring the generalization ability of the trained model based on the validation set\n      predictions_of_validation_samples = \n        cnn_model.predict(validation_samples_X.astype('float32'), batch_size=batch_size, \n          verbose=2)\n      current_model_score = log_loss(Y_valid, predictions_of_validation_samples)\n      print('Current model score log_loss: ', current_model_score)\n      sum_score += current_model_score*len(test_index)\n      # Store valid predictions\n      for i in range(len(test_index)):\n          yfull_train[test_index[i]] = predictions_of_validation_samples[i]\n      # Store the trained model\n      trained_models.append(cnn_model)\n  # incrementing the sum_score value by the current model calculated score\n  overall_score = sum_score/len(training_samples)\n  print(\"Log_loss train independent avg: \", overall_score)\n  #Reporting the model loss at this stage\n  overall_settings_output_string = 'loss_' + str(overall_score) + '_folds_' + str(nfolds) + '_ep_' + str(num_epochs)\n  return overall_settings_output_string, trained_models\n#Testing how well the model is trained\ndef test_generality_crossValidation_over_test_set(overall_settings_output_string, cnn_models):\n  batch_size = 16 # in each iteration, we consider 32 training examples at once\n  fold_number = 0 # fold iterator\n  number_of_folds = len(cnn_models) # Creating number of folds based on the value used in the training step\n  yfull_test = [] # variable to hold overall predictions for the test set\n  #executing the actual cross validation test process over the test set\n  for j in range(number_of_folds):\n      model = cnn_models[j]\n      fold_number += 1\n      print('Fold number {} out of {}'.format(fold_number, number_of_folds))\n      #Loading and normalizing testing samples\n      testing_samples, testing_samples_id = load_normalize_testing_samples()\n      #Calling the current model over the current test fold\n      test_prediction = model.predict(testing_samples, batch_size=batch_size, verbose=2)\n      yfull_test.append(test_prediction)\n  test_result = merge_several_folds_mean(yfull_test, number_of_folds)\n  overall_settings_output_string = 'loss_' + overall_settings_output_string \\\n              + '_folds_' + str(number_of_folds)\n  format_results_for_types(test_result, testing_samples_id, overall_settings_output_string)\n# Start the model training and testing\nif __name__ == '__main__':\n  info_string, models = create_model_with_kfold_cross_validation()\n  test_generality_crossValidation_over_test_set(info_string, models)\n```"]