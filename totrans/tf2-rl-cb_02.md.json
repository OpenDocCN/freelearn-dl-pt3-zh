["```\nimport gym\nimport numpy as np\n```", "```\n    class MazeEnv(gym.Env):\n        def __init__(self, stochastic=True):\n            \"\"\"Stochastic Maze environment with coins,\\\n               obstacles/walls and a goal state.  \n            \"\"\"\n            self.map = np.asarray([\"SWFWG\", \"OOOOO\", \"WOOOW\",\n                                   \"FOWFW\"])\n    ```", "```\n            self.dim = (4, 5)\n            self.img_map = np.ones(self.dim)\n            self.obstacles = [(0, 1), (0, 3), (2, 0), \n                              (2, 4), (3, 2), (3, 4)]\n            for x in self.obstacles:\n                self.img_map[x[0]][x[1]] = 0\n    ```", "```\n    self.slip_action_map = {\n                0: 3,\n                1: 2,\n                2: 0,\n                3: 1,\n            }\n    ```", "```\n    self.index_to_coordinate_map = {\n                0: (0, 0),\n                1: (1, 0),\n                2: (3, 0),\n                3: (1, 1),\n                4: (2, 1),\n                5: (3, 1),\n                6: (0, 2),\n                7: (1, 2),\n                8: (2, 2),\n                9: (1, 3),\n                10: (2, 3),\n                11: (3, 3),\n                12: (0, 4),\n                13: (1, 4),\n            }\n    ```", "```\n    \tself.coordinate_to_index_map = dict((val, key) for \\\n            key, val in self.index_to_coordinate_map.items())\n    ```", "```\n    \tdef num2coin(self, n: int):\n            coinlist = [\n                (0, 0, 0),\n                (1, 0, 0),\n                (0, 1, 0),\n                (0, 0, 1),\n                (1, 1, 0),\n                (1, 0, 1),\n                (0, 1, 1),\n                (1, 1, 1),\n            ]\n            return list(coinlist[n])\n    ```", "```\n    \t   def coin2num(self, v: List):\n            if sum(v) < 2:\n                return np.inner(v, [1, 2, 3])\n            else:\n                return np.inner(v, [1, 2, 3]) + 1\n    ```", "```\n    def set_state(self, state: int) -> None:\n            \"\"\"Set the current state of the environment. \n               Useful for value iteration\n            Args:\n                state (int): A valid state in the Maze env \\\n                int: [0, 112]\n            \"\"\"\n            self.state = state\n    ```", "```\n    def step(self, action, slip=True):\n            \"\"\"Run one step into the Maze env\n            Args:\n                state (Any): Current index state of the maze\n                action (int): Discrete action for up, down,\\\n                left, right\n                slip (bool, optional): Stochasticity in the \\\n                env. Defaults to True.\n            Raises:\n                ValueError: If invalid action is provided as \n                input\n            Returns:\n                Tuple : Next state, reward, done, _\n            \"\"\"\n            self.slip = slip\n            if self.slip:\n                if np.random.rand() < self.slip_probability:\n                    action = self.slip_action_map[action]\n    ```", "```\n    cell = self.index_to_coordinate_map[int(self.state / 8)]\n            if action == 0:\n                c_next = cell[1]\n                r_next = max(0, cell[0] - 1)\n            elif action == 1:\n                c_next = cell[1]\n                r_next = min(self.dim[0] - 1, cell[0] + 1)\n            elif action == 2:\n                c_next = max(0, cell[1] - 1)\n                r_next = cell[0]\n            elif action == 3:\n                c_next = min(self.dim[1] - 1, cell[1] + 1)\n                r_next = cell[0]\n            else:\n                raise ValueError(f\"Invalid action:{action}\")\n    ```", "```\n    if (r_next == self.goal_pos[0]) and (\n                c_next == self.goal_pos[1]\n            ):  # Check if goal reached\n                v_coin = self.num2coin(self.state % 8)\n                self.state = 8 * self.coordinate_to_index_\\\n                    map[(r_next, c_next)] + self.state % 8\n                return (\n                    self.state,\n                    float(sum(v_coin)),\n                    True,\n                )\n    ```", "```\n     else:\n        if (r_next, c_next) in self.obstacles:  # obstacle \n        # tuple list\n                    return self.state, 0.0, False\n    ```", "```\n    else:  # Coin locations\n                    v_coin = self.num2coin(self.state % 8)\n                    if (r_next, c_next) == (0, 2):\n                        v_coin[0] = 1\n                    elif (r_next, c_next) == (3, 0):\n                        v_coin[1] = 1\n                    elif (r_next, c_next) == (3, 3):\n                        v_coin[2] = 1\n                    self.state = 8 * self.coordinate_to_index_map[(r_next, c_next)] + self.coin2num(v_coin)\n                    return (\n                        self.state,\n                        0.0,\n                        False,\n                    )\n    ```", "```\n    def render(self):\n            cell = self.index_to_coordinate_map[int(\n                                             self.state / 8)]\n            desc = self.map.tolist()\n            desc[cell[0]] = (\n                desc[cell[0]][: cell[1]]\n                + \"\\x1b[1;34m\"\n                + desc[cell[0]][cell[1]]\n                + \"\\x1b[0m\"\n                + desc[cell[0]][cell[1] + 1 :]\n            )\n            print(\"\\n\".join(\"\".join(row) for row in desc))\n    ```", "```\n    if __name__ == \"__main__\":\n        env = MazeEnv()\n        obs = env.reset()\n        env.render()\n        done = False\n        step_num = 1\n        action_list = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n        # Run one episode\n        while not done:\n            # Sample a random action from the action space\n            action = env.action_space.sample()\n            next_obs, reward, done = env.step(action)\n            print(\n                f\"step#:{step_num} action:\\\n                {action_list[action]} reward:{reward} \\\n                 done:{done}\"\n            )\n            step_num += 1\n            env.render()\n        env.close()\n    ```", "```\nimport numpy as np\n```", "```\n    from envs.maze import MazeEnv\n    ```", "```\n    env = MazeEnv()\n    print(f\"Observation space: {env.observation_space}\")\n    print(f\"Action space: {env.action_space}\")\n    ```", "```\n    state_dim = env.distinct_states\n    state_values = np.zeros(state_dim)\n    q_values = np.zeros((state_dim, env.action_space.n))\n    policy = np.zeros(state_dim)\n    ```", "```\n    def calculate_values(state, action):\n        \"\"\"Evaluate Value function for given state and action\n        Args:\n            state (int): Valid (discrete) state in discrete \\\n            `env.observation_space`\n            action (int): Valid (discrete) action in \\\n            `env.action_space`\n        Returns:\n            v_sum: value for given state, action\n        \"\"\"\n    ```", "```\n        slip_action = env.slip_action_map[action]\n    ```", "```\n        env.set_state(state)\n        slip_next_state, slip_reward, _ = \\\n                            env.step(slip_action, slip=False)\n    ```", "```\n        transitions = []    transitions.append((slip_reward, slip_next_state,\n                            env.slip))\n    ```", "```\n        env.set_state(state)\n        next_state, reward, _ = env.step(action, slip=False)\n        transitions.append((reward, next_state,\n                            1 - env.slip))\n    ```", "```\n        for reward, next_state, pi in transitions:\n            v_sum += pi * (reward + discount * \\\n                           state_values[next_state])\n        return v_sum\n    ```", "```\n    # Define the maximum number of iterations per learning \n    # step\n    max_iteration = 1000\n    ```", "```\n    for i in range(iters):\n        v_s = np.zeros(state_dim)\n        for state in range(state_dim):\n            if env.index_to_coordinate_map[int(state / 8)]==\\\n            env.goal_pos:\n                continue\n            v_max = float(\"-inf\")\n            for action in range(env.action_space.n):\n                v_sum = calculate_values(state, action)\n                v_max = max(v_max, v_sum)\n            v_s[state] = v_max\n        state_values = np.copy(v_s)\n    ```", "```\n    for state in range(state_dim):\n        for action in range(env.action_space.n):\n            q_values[state, action] = calculate_values(state,\n                                                       action)\n    ```", "```\n    for state in range(state_dim):\n        policy[state] = np.argmax(q_values[state, :])\n    ```", "```\n    print(f\"Q-values: {q_values}\")\n    print(\"Action mapping:[0 - UP; 1 - DOWN; 2 - LEFT; \\\n           3 - RIGHT\")\n    print(f\"optimal_policy: {policy}\")\n    ```", "```\n    from value_function_utils import viusalize_maze_values\n    viusalize_maze_values(q_values, env)\n    ```", "```\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\n```", "```\n    class GridworldV2Env(gym.Env):\n        def __init__(self, step_cost=-0.2, max_ep_length=500,\n        explore_start=False):\n            self.index_to_coordinate_map = {\n                \"0\": [0, 0],\n                \"1\": [0, 1],\n                \"2\": [0, 2],\n                \"3\": [0, 3],\n                \"4\": [1, 0],\n                \"5\": [1, 1],\n                \"6\": [1, 2],\n                \"7\": [1, 3],\n                \"8\": [2, 0],\n                \"9\": [2, 1],\n                \"10\": [2, 2],\n                \"11\": [2, 3],\n            }\n            self.coordinate_to_index_map = {\n                str(val): int(key) for key, val in self.index_to_coordinate_map.items()\n            }\n    ```", "```\n    self.map = np.zeros((3, 4))\n            self.observation_space = gym.spaces.Discrete(1)\n            self.distinct_states = [str(i) for i in \\\n                                     range(12)]\n            self.goal_coordinate = [0, 3]\n            self.bomb_coordinate = [1, 3]\n            self.wall_coordinate = [1, 1]\n            self.goal_state = self.coordinate_to_index_map[\n                              str(self.goal_coordinate)]  # 3\n            self.bomb_state = self.coordinate_to_index_map[\n                              str(self.bomb_coordinate)]  # 7\n            self.map[self.goal_coordinate[0]]\\\n                    [self.goal_coordinate[1]] = 1\n            self.map[self.bomb_coordinate[0]]\\\n                    [self.bomb_coordinate[1]] = -1\n            self.map[self.wall_coordinate[0]]\\\n                    [self.wall_coordinate[1]] = 2\n            self.exploring_starts = explore_start\n            self.state = 8\n            self.done = False\n            self.max_ep_length = max_ep_length\n            self.steps = 0\n            self.step_cost = step_cost\n            self.action_space = gym.spaces.Discrete(4)\n            self.action_map = {\"UP\": 0, \"RIGHT\": 1, \n                               \"DOWN\": 2, \"LEFT\": 3}\n            self.possible_actions = \\\n                               list(self.action_map.values())\n    ```", "```\n    def reset(self):\n            self.done = False\n            self.steps = 0\n            self.map = np.zeros((3, 4))\n            self.map[self.goal_coordinate[0]]\\\n                    [self.goal_coordinate[1]] = 1\n            self.map[self.bomb_coordinate[0]]\\\n                     [self.bomb_coordinate[1]] = -1\n            self.map[self.wall_coordinate[0]]\\\n                    [self.wall_coordinate[1]] = 2\n            if self.exploring_starts:\n                self.state = np.random.choice([0, 1, 2, 4, 6,\n                                               8, 9, 10, 11])\n            else:\n                self.state = 8\n            return self.state\n    ```", "```\n    def get_next_state(self, current_position, action):\n            next_state = self.index_to_coordinate_map[\n                                str(current_position)].copy()\n            if action == 0 and next_state[0] != 0 and \\\n            next_state != [2, 1]:\n                # Move up\n                next_state[0] -= 1\n            elif action == 1 and next_state[1] != 3 and \\\n            next_state != [1, 0]:\n                # Move right\n                next_state[1] += 1\n            elif action == 2 and next_state[0] != 2 and \\\n            next_state != [0, 1]:\n                # Move down\n                next_state[0] += 1\n            elif action == 3 and next_state[1] != 0 and \\\n            next_state != [1, 2]:\n                # Move left\n                next_state[1] -= 1\n            else:\n                pass\n            return self.coordinate_to_index_map[str(\n                                                 next_state)]\n    ```", "```\n    def step(self, action):\n            assert action in self.possible_actions, \\\n            f\"Invalid action:{action}\"\n            current_position = self.state\n            next_state = self.get_next_state(\n                                   current_position, action)\n            self.steps += 1\n            if next_state == self.goal_state:\n                reward = 1\n                self.done = True\n            elif next_state == self.bomb_state:\n                reward = -1\n                self.done = True\n            else:\n                reward = self.step_cost\n            if self.steps == self.max_ep_length:\n                self.done = True\n            self.state = next_state\n            return next_state, reward, self.done\n    ```", "```\n    def temporal_difference_learning(env, max_episodes):\n        grid_state_values = np.zeros((len(\n                                   env.distinct_states), 1))\n        grid_state_values[env.goal_state] = 1\n        grid_state_values[env.bomb_state] = -1\n    ```", "```\n        # v: state-value function\n        v = grid_state_values\n        gamma = 0.99  # Discount factor\n        alpha = 0.01  # learning rate\n        done = False\n    ```", "```\n    for episode in range(max_episodes):\n            state = env.reset()\n    ```", "```\n    while not done:\n                action = env.action_space.sample()  \n                  # random policy\n                next_state, reward, done = env.step(action)\n                # State-value function updates using TD(0)\n                v[state] += alpha * (reward + gamma * \\\n                                    v[next_state] - v[state])\n                state = next_state\n    ```", "```\n    visualize_grid_state_values(grid_state_values.reshape((3, 4)))\n    ```", "```\n    if __name__ == \"__main__\":\n        max_episodes = 4000\n        env = GridworldV2Env(step_cost=-0.1, \n                             max_ep_length=30)\n        temporal_difference_learning(env, max_episodes)\n    ```", "```\nimport numpy as np\n```", "```\n    import numpy as np\n    from envs.gridworldv2 import GridworldV2Env\n    from value_function_utils import (\n        visualize_grid_action_values,\n        visualize_grid_state_values,\n    )\n    ```", "```\n    def monte_carlo_prediction(env, max_episodes):\n        returns = {state: [] for state in \\\n                   env.distinct_states}\n        grid_state_values = np.zeros(len(\n                                    env.distinct_states))\n        grid_state_values[env.goal_state] = 1\n        grid_state_values[env.bomb_state] = -1\n        gamma = 0.99  # Discount factor\n    ```", "```\n    for episode in range(max_episodes):\n            g_t = 0\n            state = env.reset()\n            done = False\n            trajectory = []\n    ```", "```\n            while not done:\n                action = env.action_space.sample()  \n                    # random policy\n                next_state, reward, done = env.step(action)\n                trajectory.append((state, reward))\n                state = next_state\n    ```", "```\n    for idx, (state, reward) in enumerate(trajectory[::-1]):\n                g_t = gamma * g_t + reward\n                # first visit Monte-Carlo prediction\n                if state not in np.array(trajectory[::-1])\\\n                [:, 0][idx + 1 :]:\n                    returns[str(state)].append(g_t)\n                    grid_state_values[state] = np.mean(returns[str(state)])\n    Let's visualize the learned state value function using the visualize_grid_state_values helper function from the value_function_utils script:\n    visualize_grid_state_values(grid_state_values.reshape((3, 4)))\n    ```", "```\n    if __name__ == \"__main__\":\n        max_episodes = 4000\n        env = GridworldV2Env(step_cost=-0.1, \n                             max_ep_length=30)\n        print(f\"===Monte Carlo Prediction===\")\n        monte_carlo_prediction(env, max_episodes)\n    ```", "```\n    def epsilon_greedy_policy(action_logits, epsilon=0.2):\n        idx = np.argmax(action_logits)\n        probs = []\n        epsilon_decay_factor = np.sqrt(sum([a ** 2 for a in \\\n                                            action_logits]))\n        if epsilon_decay_factor == 0:\n            epsilon_decay_factor = 1.0\n        for i, a in enumerate(action_logits):\n            if i == idx:\n                probs.append(round(1 - epsilon + (\n                        epsilon / epsilon_decay_factor), 3))\n            else:\n                probs.append(round(\n                        epsilon / epsilon_decay_factor, 3))\n        residual_err = sum(probs) - 1\n        residual = residual_err / len(action_logits)\n        return np.array(probs) - residual\n    ```", "```\n    def monte_carlo_control(env, max_episodes):\n        grid_state_action_values = np.zeros((12, 4))\n        grid_state_action_values[3] = 1\n        grid_state_action_values[7] = -1\n    ```", "```\n        possible_states = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\"]\n        possible_actions = [\"0\", \"1\", \"2\", \"3\"]\n        returns = {}\n        for state in possible_states:\n            for action in possible_actions:\n                returns[state + \", \" + action] = []\n    ```", "```\n    gamma = 0.99\n        for episode in range(max_episodes):\n            g_t = 0\n            state = env.reset()\n            trajectory = []\n            while True:\n                action_values = \\\n                     grid_state_action_values[state]\n                probs = epsilon_greedy_policy(action_values)\n                action = np.random.choice(np.arange(4), \\\n                                    p=probs)  # random policy\n                next_state, reward, done = env.step(action)\n                trajectory.append((state, action, reward))\n                state = next_state\n                if done:\n                    break\n    ```", "```\n            for step in reversed(trajectory):\n                g_t = gamma * g_t + step[2]\n                Returns[str(step[0]) + \", \" + \\\n                        str(step[1])].append(g_t)\n                grid_state_action_values[step[0]][step[1]]= \\\n                np.mean(\n                    Returns[str(step[0]) + \", \" + \\\n                            str(step[1])]\n                )\n    ```", "```\n    visualize_grid_action_values(grid_state_action_values\n    ```", "```\n    if __name__ == \"__main__\":\n        max_episodes = 4000\n        env = GridworldV2Env(step_cost=-0.1, \\\n                             max_ep_length=30)\n        print(f\"===Monte Carlo Control===\")\n        monte_carlo_control(env, max_episodes)\n    ```", "```\nimport numpy as np\nimport random\n```", "```\n    def sarsa(env, max_episodes):\n        grid_action_values = np.zeros((len(\n                   env.distinct_states), env.action_space.n))\n    ```", "```\n        grid_action_values[env.goal_state] = 1\n        grid_action_values[env.bomb_state] = -1\n    ```", "```\n        gamma = 0.99  # discounting factor\n        alpha = 0.01  # learning rate\n        # q: state-action-value function\n        q = grid_action_values\n    ```", "```\n    for episode in range(max_episodes):\n            step_num = 1\n            done = False\n            state = env.reset()\n            action = greedy_policy(q[state], 1)\n    ```", "```\n    while not done:\n                next_state, reward, done = env.step(action)\n                step_num += 1\n                decayed_epsilon = gamma ** step_num  \n                # Doesn't have to be gamma\n                next_action = greedy_policy(q[next_state], \\\n                                            decayed_epsilon)\n                q[state][action] += alpha * (\n                    reward + gamma * q[next_state] \\\n                        [next_action] - q[state][action]\n                )\n                state = next_state\n                action = next_action\n    ```", "```\n    visualize_grid_action_values(grid_action_values)\n    ```", "```\n    def greedy_policy(q_values, epsilon):\n        \"\"\"Epsilon-greedy policy \"\"\"\n        if random.random() >= epsilon:\n            return np.argmax(q_values)\n        else:\n            return random.randint(0, 3)\n    ```", "```\n    if __name__ == \"__main__\":\n        max_episodes = 4000\n        env = GridworldV2Env(step_cost=-0.1, \\\n                             max_ep_length=30)\n        sarsa(env, max_episodes)\n    ```", "```\nimport numpy as np\nimport random\n```", "```\n    def q_learning(env, max_episodes):\n        grid_action_values = np.zeros((len(\\\n            env.distinct_states), env.action_space.n))\n    ```", "```\n        grid_action_values[env.goal_state] = 1\n        grid_action_values[env.bomb_state] = -1\n    ```", "```\n        gamma = 0.99  # discounting factor\n        alpha = 0.01  # learning rate\n        # q: state-action-value function\n        q = grid_action_values\n    ```", "```\n    for episode in range(max_episodes):\n            step_num = 1\n            done = False\n            state = env.reset()\n    ```", "```\n            while not done:\n                decayed_epsilon = 1 * gamma ** step_num  \n                # Doesn't have to be gamma\n                action = greedy_policy(q[state], \\\n                         decayed_epsilon)\n                next_state, reward, done = env.step(action)\n                # Q-Learning update\n                grid_action_values[state][action] += alpha *(\n                    reward + gamma * max(q[next_state]) - \\\n                    q[state][action]\n                )\n                step_num += 1\n                state = next_state\n    ```", "```\n    visualize_grid_action_values(grid_action_values)\n    ```", "```\n    def greedy_policy(q_values, epsilon):\n        \"\"\"Epsilon-greedy policy \"\"\"\n        if random.random() >= epsilon:\n            return np.argmax(q_values)\n        else:\n            return random.randint(0, 3)\n    ```", "```\n    if __name__ == \"__main__\":\n        max_episodes = 4000\n        env = GridworldV2Env(step_cost=-0.1, \n                             max_ep_length=30)\n        q_learning(env, max_episodes)\n    ```", "```\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport gym\n```", "```\n    class PolicyNet(keras.Model):\n        def __init__(self, action_dim=1):\n            super(PolicyNet, self).__init__()\n            self.fc1 = layers.Dense(24, activation=\"relu\")\n            self.fc2 = layers.Dense(36, activation=\"relu\")\n            self.fc3 = layers.Dense(action_dim,\n                                    activation=\"softmax\")\n    ```", "```\n        def call(self, x):\n            x = self.fc1(x)\n            x = self.fc2(x)\n            x = self.fc3(x)\n            return x\n    ```", "```\n    def process(self, observations):\n            # Process batch observations using `call(x)`\n            # behind-the-scenes\n            action_probabilities = \\\n                self.predict_on_batch(observations)\n            return action_probabilities\n    ```", "```\n    class Agent(object):\n        def __init__(self, action_dim=1):\n            \"\"\"Agent with a neural-network brain powered \n               policy\n            Args:\n                action_dim (int): Action dimension\n            \"\"\"\n            self.policy_net = PolicyNet(\n                                      action_dim=action_dim)\n            self.optimizer = tf.keras.optimizers.Adam(\n                                         learning_rate=1e-3)\n            self.gamma = 0.99\n    ```", "```\n        def policy(self, observation):\n            observation = observation.reshape(1, -1)\n            observation = tf.convert_to_tensor(observation,\n                                            dtype=tf.float32)\n            action_logits = self.policy_net(observation)\n            action = tf.random.categorical(\n                   tf.math.log(action_logits), num_samples=1)\n            return action\n    ```", "```\n        def get_action(self, observation):\n            action = self.policy(observation).numpy()\n            return action.squeeze()\n    ```", "```\n        def learn(self, states, rewards, actions):\n            discounted_reward = 0\n            discounted_rewards = []\n            rewards.reverse()\n    ```", "```\n            for r in rewards:\n                discounted_reward = r + self.gamma * \\\n                                        discounted_reward\n                discounted_rewards.append(discounted_reward)\n                discounted_rewards.reverse()\n    ```", "```\n            for state, reward, action in zip(states, \n            discounted_rewards, actions):\n                with tf.GradientTape() as tape:\n                    action_probabilities = \\\n                        self.policy_net(np.array([state]),\\\n                                        training=True)\n                    loss = self.loss(action_probabilities, \\\n                                     action, reward)\n                grads = tape.gradient(loss, \n                         self.policy_net.trainable_variables)\n                self.optimizer.apply_gradients(\n                    zip(grads, \n                        self.policy_net.trainable_variables)\n                )\n    ```", "```\n        def loss(self, action_probabilities, action, reward):\n            dist = tfp.distributions.Categorical(\n                probs=action_probabilities, dtype=tf.float32\n            )\n            log_prob = dist.log_prob(action)\n            loss = -log_prob * reward\n            return loss\n    ```", "```\n    def train(agent: Agent, env: gym.Env, episodes: int, render=True):\n        \"\"\"Train `agent` in `env` for `episodes`\n        Args:\n            agent (Agent): Agent to train\n            env (gym.Env): Environment to train the agent\n            episodes (int): Number of episodes to train\n            render (bool): True=Enable/False=Disable \\\n                            rendering; Default=True\n        \"\"\"\n    ```", "```\n    for episode in range(episodes):\n            done = False\n            state = env.reset()\n            total_reward = 0\n            rewards = []\n            states = []\n            actions = []\n    ```", "```\n            while not done:\n                action = agent.get_action(state)\n                next_state, reward, done, _ = \\\n                                       env.step(action)\n                rewards.append(reward)\n                states.append(state)\n                actions.append(action)\n                state = next_state\n                total_reward += reward\n                if render:\n                    env.render()\n                if done:\n                    agent.learn(states, rewards, actions)\n                    print(\"\\n\")\n                print(f\"Episode#:{episode} \\\n                ep_reward:{total_reward}\", end=\"\\r\")\n    ```", "```\n    if __name__ == \"__main__\":\n        agent = Agent()\n        episodes = 5000\n        env = gym.make(\"MountainCar-v0\")\n        train(agent, env, episodes)\n        env.close()\n    ```", "```\nimport numpy as np\nimport tensorflow as tf\nimport gym\nimport tensorflow_probability as tfp\n```", "```\n    class ActorCritic(tf.keras.Model):\n        def __init__(self, action_dim):\n            super().__init__()\n            self.fc1 = tf.keras.layers.Dense(512, \\\n                                            activation=\"relu\")\n            self.fc2 = tf.keras.layers.Dense(128, \\\n                                            activation=\"relu\")\n            self.critic = tf.keras.layers.Dense(1, \\\n                                              activation=None)\n            self.actor = tf.keras.layers.Dense(action_dim, \\\n                                             activation=None)\n    ```", "```\n        def call(self, input_data):\n            x = self.fc1(input_data)\n            x1 = self.fc2(x)\n            actor = self.actor(x1)\n            critic = self.critic(x1)\n            return critic, actor\n    ```", "```\n    class Agent:\n        def __init__(self, action_dim=4, gamma=0.99):\n            \"\"\"Agent with a neural-network brain powered \n               policy\n            Args:\n                action_dim (int): Action dimension\n                gamma (float) : Discount factor. Default=0.99\n            \"\"\"\n            self.gamma = gamma\n            self.opt = tf.keras.optimizers.Adam(\n                                          learning_rate=1e-4)\n            self.actor_critic = ActorCritic(action_dim)\n    ```", "```\n        def get_action(self, state):\n            _, action_probabilities = \\\n                         self.actor_critic(np.array([state]))\n            action_probabilities = tf.nn.softmax(\n                                        action_probabilities)\n            action_probabilities = \\\n                                 action_probabilities.numpy()\n            dist = tfp.distributions.Categorical(\n                probs=action_probabilities, dtype=tf.float32\n            )\n            action = dist.sample()\n            return int(action.numpy()[0])\n    ```", "```\n        def actor_loss(self, prob, action, td):\n            prob = tf.nn.softmax(prob)\n            dist = tfp.distributions.Categorical(probs=prob,\n                                           dtype=tf.float32)\n            log_prob = dist.log_prob(action)\n            loss = -log_prob * td\n            return loss\n    ```", "```\n    def learn(self, state, action, reward, next_state, done):\n            state = np.array([state])\n            next_state = np.array([next_state])\n            with tf.GradientTape() as tape:\n                value, action_probabilities = \\\n                    self.actor_critic(state, training=True)\n                value_next_st, _ = self.actor_critic(\n                                   next_state, training=True)\n                td = reward + self.gamma * value_next_st * \\\n                      (1 - int(done)) - value\n                actor_loss = self.actor_loss(\n                            action_probabilities, action, td)\n                critic_loss = td ** 2\n                total_loss = actor_loss + critic_loss\n            grads = tape.gradient(total_loss, \n                       self.actor_critic.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \n                      self.actor_critic.trainable_variables))\n            return total_loss\n    ```", "```\n    def train(agent, env, episodes, render=True):\n        \"\"\"Train `agent` in `env` for `episodes`\n        Args:\n            agent (Agent): Agent to train\n            env (gym.Env): Environment to train the agent\n            episodes (int): Number of episodes to train\n            render (bool): True=Enable/False=Disable \\\n                            rendering; Default=True\n        \"\"\"\n        for episode in range(episodes):\n            done = False\n            state = env.reset()\n            total_reward = 0\n            all_loss = []\n            while not done:\n                action = agent.get_action(state)\n                next_state, reward, done, _ = \\\n                                          env.step(action)\n                loss = agent.learn(state, action, reward, \n                                   next_state, done)\n                all_loss.append(loss)\n                state = next_state\n                total_reward += reward\n                if render:\n                    env.render()\n                if done:\n                    print(\"\\n\")\n                print(f\"Episode#:{episode} \n                        ep_reward:{total_reward}\", \n                        end=\"\\r\")\n    ```", "```\n    if __name__ == \"__main__\":\n        env = gym.make(\"CartPole-v0\")\n        agent = Agent(env.action_space.n)\n        num_episodes = 20000\n        train(agent, env, num_episodes)\n    ```"]