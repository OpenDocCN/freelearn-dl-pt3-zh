["```\n[(\"Transformers\", 1), (\"took\", 2), (\"NLP\", 3), (\"by\", 4), (\"storm\", 5)] \n```", "```\n    !pip install tensorflow_datasets\n    !pip install -U 'tensorflow-text==2.8.*'\n    import logging\n    import time\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import tensorflow_text\n    import tensorflow_datasets as tfds\n    import tensorflow as tf\n    logging.getLogger('tensorflow').setLevel(logging.ERROR)  # suppress warnings \n    ```", "```\n    examples, metadata = tfds.load('ted_hrlr_translate/pt_to_en', with_info=True,\n                                   as_supervised=True)\n    train_examples, val_examples = examples['train'], examples['validation'] \n    ```", "```\n    model_name = 'ted_hrlr_translate_pt_en_converter'\n    tf.keras.utils.get_file(\n        f'{model_name}.zip',\n        f'https://storage.googleapis.com/download.tensorflow.org/models/{model_name}.zip',\n        cache_dir='.', cache_subdir='', extract=True\n    )\n    tokenizers = tf.saved_model.load(model_name) \n    ```", "```\n    for pt_examples, en_examples in train_examples.batch(3).take(1):\n      print('> Examples in Portuguese:')\n    for en in en_examples.numpy():\n      print(en.decode('utf-8')) \n    ```", "```\n    and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n    but what if it were active ?\n    but they did n't test for curiosity . \n    ```", "```\n    encoded = tokenizers.en.tokenize(en_examples)\n    for row in encoded.to_list():\n      print(row) \n    ```", "```\n    [2, 72, 117, 79, 1259, 1491, 2362, 13, 79, 150, 184, 311, 71, 103, 2308, 74, 2679, 13, 148, 80, 55, 4840, 1434, 2423, 540, 15, 3]\n    [2, 87, 90, 107, 76, 129, 1852, 30, 3]\n    [2, 87, 83, 149, 50, 9, 56, 664, 85, 2512, 15, 3] \n    ```", "```\n    round_trip = tokenizers.en.detokenize(encoded)\n    for line in round_trip.numpy():\n      print(line.decode('utf-8')) \n    ```", "```\n    and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n    but what if it were active ?\n    but they did n ' t test for curiosity . \n    ```", "```\n    MAX_TOKENS=128\n    def filter_max_tokens(pt, en):\n      num_tokens = tf.maximum(tf.shape(pt)[1],tf.shape(en)[1])\n      return num_tokens < MAX_TOKENS\n    def tokenize_pairs(pt, en):\n        pt = tokenizers.pt.tokenize(pt)\n        # Convert from ragged to dense, padding with zeros.\n        pt = pt.to_tensor()\n        en = tokenizers.en.tokenize(en)\n        # Convert from ragged to dense, padding with zeros.\n        en = en.to_tensor()\n        return pt, en\n    BUFFER_SIZE = 20000\n    BATCH_SIZE = 64\n    def make_batches(ds):\n      return (\n          ds\n          .cache()\n          .shuffle(BUFFER_SIZE)\n          .batch(BATCH_SIZE)\n          .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n          .filter(filter_max_tokens)\n          .prefetch(tf.data.AUTOTUNE))\n    train_batches = make_batches(train_examples)\n    val_batches = make_batches(val_examples) \n    ```", "```\n    def get_angles(pos, i, d_model):\n      angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n      return pos * angle_rates\n    def positional_encoding(position, d_model):\n      angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                              np.arange(d_model)[np.newaxis, :],\n                              d_model)\n      # apply sin to even indices in the array; 2i\n      angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n      # apply cos to odd indices in the array; 2i+1\n      angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n      pos_encoding = angle_rads[np.newaxis, ...]\n      return tf.cast(pos_encoding, dtype=tf.float32) \n    ```", "```\n    def create_padding_mask(seq):\n      seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n      # add extra dimensions to add the padding\n      # to the attention logits.\n      return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n    def create_look_ahead_mask(size):\n      mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n      return mask  # (seq_len, seq_len) \n    ```", "```\n    def scaled_dot_product_attention(q, k, v, mask):\n      \"\"\"Calculate the attention weights.\n      q, k, v must have matching leading dimensions.\n      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n      The mask has different shapes depending on its type(padding or look ahead)\n      but it must be broadcastable for addition.\n      Args:\n        q: query shape == (..., seq_len_q, depth)\n        k: key shape == (..., seq_len_k, depth)\n        v: value shape == (..., seq_len_v, depth_v)\n        mask: Float tensor with shape broadcastable\n              to (..., seq_len_q, seq_len_k). Defaults to None.\n      Returns:\n        output, attention_weights\n      \"\"\"\n      matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n      # scale matmul_qk\n      dk = tf.cast(tf.shape(k)[-1], tf.float32)\n      scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n      # add the mask to the scaled tensor.\n      if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n      # softmax is normalized on the last axis (seq_len_k) so that the scores\n      # add up to 1.\n      attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n      output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n      return output, attention_weights \n    ```", "```\n    class MultiHeadAttention(tf.keras.layers.Layer):\n      def __init__(self,*, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        assert d_model % self.num_heads == 0\n        self.depth = d_model // self.num_heads\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model)\n      def split_heads(self, x, batch_size):\n        \"\"\"Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3])\n      def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(\n            q, k, v, mask)\n        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n        concat_attention = tf.reshape(scaled_attention,\n                                      (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n        return output, attention_weights \n    ```", "```\n    def point_wise_feed_forward_network(d_model, dff):\n     return tf.keras.Sequential([\n         tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n         tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n     ]) \n    ```", "```\n    class EncoderLayer(tf.keras.layers.Layer):\n      def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n      def call(self, x, training, mask):\n        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n        return out2 \n    ```", "```\n    class DecoderLayer(tf.keras.layers.Layer):\n      def __init__(self,*, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n        self.mha1 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.mha2 = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate)\n      def call(self, x, enc_output, training,\n               look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x)\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n        ffn_output = self.dropout3(ffn_output, training=training)\n        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n        return out3, attn_weights_block1, attn_weights_block2 \n    ```", "```\n    class Encoder(tf.keras.layers.Layer):\n      def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n                   rate=0.1):\n        super(Encoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(MAX_TOKENS, self.d_model)\n        self.enc_layers = [\n            EncoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n            for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n      def call(self, x, training, mask):\n        seq_len = tf.shape(x)[1]\n        # adding embedding and position encoding.\n        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n          x = self.enc_layers[i](x, training, mask)\n        return x  # (batch_size, input_seq_len, d_model) \n    ```", "```\n    class Decoder(tf.keras.layers.Layer):\n      def __init__(self,*, num_layers, d_model, num_heads, dff, target_vocab_size,\n                   rate=0.1):\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(MAX_TOKENS, d_model)\n        self.dec_layers = [\n            DecoderLayer(d_model=d_model, num_heads=num_heads, dff=dff, rate=rate)\n            for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate)\n      def call(self, x, enc_output, training,\n               look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n          x, block1, block2 = self.dec_layers[i](x, enc_output, training,\n                                                 look_ahead_mask, padding_mask)\n          attention_weights[f'decoder_layer{i+1}_block1'] = block1\n          attention_weights[f'decoder_layer{i+1}_block2'] = block2\n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights \n    ```", "```\n    class Transformer(tf.keras.Model):\n      def __init__(self,*, num_layers, d_model, num_heads, dff, input_vocab_size,\n                   target_vocab_size, rate=0.1):\n        super().__init__()\n        self.encoder = Encoder(num_layers=num_layers, d_model=d_model,\n                               num_heads=num_heads, dff=dff,\n                               input_vocab_size=input_vocab_size, rate=rate)\n        self.decoder = Decoder(num_layers=num_layers, d_model=d_model,\n                               num_heads=num_heads, dff=dff,\n                               target_vocab_size=target_vocab_size, rate=rate)\n        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n      def call(self, inputs, training):\n        # Keras models prefer if you pass all your inputs in the first argument\n        inp, tar = inputs\n        enc_padding_mask, look_ahead_mask, dec_padding_mask = self.create_masks(inp, tar)\n        enc_output = self.encoder(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n            tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n        return final_output, attention_weights\n      def create_masks(self, inp, tar):\n        # Encoder padding mask\n        enc_padding_mask = create_padding_mask(inp)\n        # Used in the 2nd attention block in the decoder.\n        # This padding mask is used to mask the encoder outputs.\n        dec_padding_mask = create_padding_mask(inp)\n        # Used in the 1st attention block in the decoder.\n        # It is used to pad and mask future tokens in the input received by\n        # the decoder.\n        look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n        dec_target_padding_mask = create_padding_mask(tar)\n        look_ahead_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n        return enc_padding_mask, look_ahead_mask, dec_padding_mask \n    ```", "```\n    num_layers = 4\n    d_model = 128\n    dff = 512\n    num_heads = 8\n    dropout_rate = 0.1\n    class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n      def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n      def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n    learning_rate = CustomSchedule(d_model)\n    optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n                                         epsilon=1e-9)\n    def loss_function(real, pred):\n      mask = tf.math.logical_not(tf.math.equal(real, 0))\n      loss_ = loss_object(real, pred)\n      mask = tf.cast(mask, dtype=loss_.dtype)\n      loss_ *= mask\n      return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n    def accuracy_function(real, pred):\n      accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n      mask = tf.math.logical_not(tf.math.equal(real, 0))\n      accuracies = tf.math.logical_and(mask, accuracies)\n      accuracies = tf.cast(accuracies, dtype=tf.float32)\n      mask = tf.cast(mask, dtype=tf.float32)\n      return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n    train_loss = tf.keras.metrics.Mean(name='train_loss')\n    train_accuracy = tf.keras.metrics.Mean(name='train_accuracy') \n    ```", "```\n    transformer = Transformer(\n        num_layers=num_layers,\n        d_model=d_model,\n        num_heads=num_heads,\n        dff=dff,\n        input_vocab_size=tokenizers.pt.get_vocab_size().numpy(),\n        target_vocab_size=tokenizers.en.get_vocab_size().numpy(),\n        rate=dropout_rate) \n    ```", "```\n    checkpoint_path = './checkpoints/train'\n    ckpt = tf.train.Checkpoint(transformer=transformer,\n                               optimizer=optimizer)\n    ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n    # if a checkpoint exists, restore the latest checkpoint.\n    if ckpt_manager.latest_checkpoint:\n      ckpt.restore(ckpt_manager.latest_checkpoint)\n      print('Latest checkpoint restored!!') \n    ```", "```\n    train_step_signature = [\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n        tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n    ]\n    @tf.function(input_signature=train_step_signature)\n    def train_step(inp, tar):\n      tar_inp = tar[:, :-1]\n      tar_real = tar[:, 1:]\n      with tf.GradientTape() as tape:\n        predictions, _ = transformer([inp, tar_inp],\n                                     training = True)\n        loss = loss_function(tar_real, predictions)\n      gradients = tape.gradient(loss, transformer.trainable_variables)\n      optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n      train_loss(loss)\n      train_accuracy(accuracy_function(tar_real, predictions))\n    EPOCHS = 20\n    for epoch in range(EPOCHS):\n      start = time.time()\n      train_loss.reset_states()\n      train_accuracy.reset_states()\n      # inp -> portuguese, tar -> english\n      for (batch, (inp, tar)) in enumerate(train_batches):\n        train_step(inp, tar)\n        if batch % 50 == 0:\n          print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n      if (epoch + 1) % 5 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print(f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n      print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n      print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n') \n    ```", "```\n    Epoch 20 Loss 1.5030 Accuracy 0.6720\n    Time taken for 1 epoch: 169.01 secs \n    ```", "```\n    class Translator(tf.Module):\n      def __init__(self, tokenizers, transformer):\n        self.tokenizers = tokenizers\n        self.transformer = transformer\n      def __call__(self, sentence, max_length=MAX_TOKENS):\n        # input sentence is portuguese, hence adding the start and end token\n        assert isinstance(sentence, tf.Tensor)\n        if len(sentence.shape) == 0:\n          sentence = sentence[tf.newaxis]\n        sentence = self.tokenizers.pt.tokenize(sentence).to_tensor()\n        encoder_input = sentence\n        # As the output language is english, initialize the output with the\n        # english start token.\n        start_end = self.tokenizers.en.tokenize([''])[0]\n        start = start_end[0][tf.newaxis]\n        end = start_end[1][tf.newaxis]\n        # 'tf.TensorArray' is required here (instead of a python list) so that the\n        # dynamic-loop can be traced by 'tf.function'.\n        output_array = tf.TensorArray(dtype=tf.int64, size=0, dynamic_size=True)\n        output_array = output_array.write(0, start)\n        for i in tf.range(max_length):\n          output = tf.transpose(output_array.stack())\n          predictions, _ = self.transformer([encoder_input, output], training=False)\n          # select the last token from the seq_len dimension\n          predictions = predictions[:, -1:, :]  # (batch_size, 1, vocab_size)\n          predicted_id = tf.argmax(predictions, axis=-1)\n          # concatentate the predicted_id to the output which is given to the decoder\n          # as its input.\n          output_array = output_array.write(i+1, predicted_id[0])\n          if predicted_id == end:\n            break\n        output = tf.transpose(output_array.stack())\n        # output.shape (1, tokens)\n        text = tokenizers.en.detokenize(output)[0]  # shape: ()\n        tokens = tokenizers.en.lookup(output)[0]\n        # 'tf.function' prevents us from using the attention_weights that were\n        # calculated on the last iteration of the loop. So recalculate them outside\n        # the loop.\n        _, attention_weights = self.transformer([encoder_input, output[:,:-1]], training=False)\n        return text, tokens, attention_weights \n    ```", "```\n    translator = Translator(tokenizers, transformer)\n    def print_translation(sentence, tokens, ground_truth):\n      print(f'{\"Input:\":15s}: {sentence}')\n      print(f'{\"Prediction\":15s}: {tokens.numpy().decode(\"utf-8\")}')\n      print(f'{\"Ground truth\":15s}: {ground_truth}')\n    sentence = 'os meus vizinhos ouviram sobre esta ideia.'\n    ground_truth = 'and my neighboring homes heard about this idea .'\n    translated_text, translated_tokens, attention_weights = translator(\n        tf.constant(sentence))\n    print_translation(sentence, translated_text, ground_truth) \n    ```", "```\n    Input:         : os meus vizinhos ouviram sobre esta ideia.\n    Prediction     : my neighbors have heard about this idea .\n    Ground truth   : and my neighboring homes heard about this idea . \n    ```", "```\n    python -m venv .env\n    source .env/bin/activate\n    pip install transformers[tf-cpu] \n    ```", "```\n    python -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('we love you'))\" \n    ```", "```\n    [{'label': 'POSITIVE', 'score': 0.9998704791069031}] \n    ```", "```\n    from transformers import pipeline\n    generator = pipeline(task=\"text-generation\") \n    ```", "```\n    No model was supplied, defaulted to gpt2 (https://huggingface.co/gpt2)\n    Downloading: 100%|██████████████████████████████| 665/665 [00:00<00:00, 167kB/s]\n    Downloading: 100%|███████████████████████████| 475M/475M [03:24<00:00, 2.44MB/s \n    ```", "```\n    generator(\"Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone\") \n    ```", "```\n    Setting 'pad_token_id' to 50256 (first 'eos_token_id') to generate sequence\n    [{'generated_text': 'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone and Eight for the Dwarves in their halls of rock! Three new Rings of the Elven-kings under the sky, Seven for'}] \n    ```", "```\n    generator (\"The original theory of relativity is based upon the premise that all coordinate systems in relative uniform translatory motion to each other are equally valid and equivalent \") \n    ```", "```\n    Setting 'pad_token_id' to 50256 (first 'eos_token_id') to generate sequence\n    [{'generated_text': 'The original theory of relativity is based upon the premise that all coordinate systems in relative uniform translatory motion to each other are equally valid and equivalent \\xa0to one another. In other words, they can all converge, and therefore all the laws are valid'}] \n    ```", "```\n    generator (\"It takes a great deal of bravery to stand up to our enemies\") \n    ```", "```\n    Setting 'pad_token_id' to 50256 (first 'eos_token_id') to generate sequence\n    [{'generated_text': 'It takes a great deal of bravery to stand up to our enemies that day. She still has a lot to learn from it, or it could take decades to do.\\n\\nWhile some braver men struggle, many are not as lucky'}] \n    ```", "```\n    from transformers import TFAutoModelForSequenceClassification\n    model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\") \n    ```", "```\n    Downloading: 100%|█████████████████████████████| 483/483 [00:00<00:00, 68.9kB/s]\n    Downloading: 100%|███████████████████████████| 347M/347M [01:05<00:00, 5.59MB/s]\n    … \n    ```", "```\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n    sequence = \"The original theory of relativity is based upon the premise that all coordinate systems\"\n    print(tokenizer(sequence)) \n    ```", "```\n    {'input_ids': [101, 1996, 2434, 3399, 1997, 20805, 2003, 2241, 2588, 1996, 18458, 2008, 2035, 13530, 3001, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} \n    ```", "```\n    from transformers import pipeline\n    ner_pipe = pipeline(\"ner\")\n    sequence = \"\"\"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much.\"\"\"\n    for entity in ner_pipe(sequence):\n        print(entity) \n    ```", "```\n    {'entity': 'I-PER', 'score': 0.99908304, 'index': 6, 'word': 'Du', 'start': 13, 'end': 15}\n    {'entity': 'I-PER', 'score': 0.9869529, 'index': 7, 'word': '##rs', 'start': 15, 'end': 17}\n    {'entity': 'I-PER', 'score': 0.9784202, 'index': 8, 'word': '##ley', 'start': 17, 'end': 20}\n    {'entity': 'I-ORG', 'score': 0.6860208, 'index': 14, 'word': 'P', 'start': 38, 'end': 39}\n    {'entity': 'I-ORG', 'score': 0.7713562, 'index': 15, 'word': '##rive', 'start': 39, 'end': 43}\n    {'entity': 'I-ORG', 'score': 0.76567733, 'index': 16, 'word': '##t', 'start': 43, 'end': 44}\n    {'entity': 'I-ORG', 'score': 0.8087192, 'index': 17, 'word': 'Drive', 'start': 45, 'end': 50} \n    ```", "```\n    from transformers import pipeline\n    summarizer = pipeline(\"summarization\")\n    ARTICLE = \"\"\"\n     Mr.\n     and Mrs.\n     Dursley, of number four, Privet Drive, were proud to say\n     that they were perfectly normal, thank you very much.\n     They were the last\n     people you'd expect to be involved in anything strange or mysterious,\n    because they just didn't hold with such nonsense.\n     Mr.\n     Dursley was the director of a firm called Grunnings, which made\n     drills.\n     He was a big, beefy man with hardly any neck, although he did\n     have a very large mustache.\n     Mrs.\n     Dursley was thin and blonde and had\n     nearly twice the usual amount of neck, which came in very useful as she\n     spent so much of her time craning over garden fences, spying on the\n     neighbors.\n     The Dursleys had a small son called Dudley and in their\n     opinion there was no finer boy anywhere\"\"\"\n    print(summarizer(ARTICLE, max_length=130, min_length=30, do_sample=False)) \n    ```", "```\n    No model was supplied, defaulted to t5-small (https://huggingface.co/t5-small)\n    Downloading: 100%|██████████████████████████| 1.17k/1.17k [00:00<00:00, 300kB/s]\n    Downloading: 100%|███████████████████████████| 231M/231M [01:29<00:00, 2.71MB/s]\n    [{'summary_text': \"Mr. and Mrs. Dursley, of number four, were the last people you'd expect to be involved in anything strange or mysterious . the Dursleys had a small son called Dudley and in their opinion there was no finer boy anywhere .\"}] \n    ```", "```\n    summarizer = pipeline(\"summarization\", model='t5-base') \n    ```", "```\n    Downloading: 100%|████████████████████████████████████████████████████████████| 773k/773k [00:00<00:00, 1.28MB/s]\n    Downloading: 100%|██████████████████████████████████████████████████████████| 1.32M/1.32M [00:00<00:00, 1.93MB/s]\n    [{'summary_text': \"bob greene says he and his wife were perfectly normal . he says they were the last people you'd expect to be involved in anything strange or mysterious . greene: they were a big, beefy man with hardly any neck, but had a very large mustache .\"}] \n    ```", "```\n    from datasets import load_dataset\n    dataset = load_dataset(\"yelp_review_full\")\n    from transformers import AutoTokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n    def tokenize_function(examples):\n        return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n    tokenized_datasets = dataset.map(tokenize_function, batched=True)\n    small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n    small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000)) \n    ```", "```\n    from transformers import DefaultDataCollator\n    data_collator = DefaultDataCollator(return_tensors=\"tf\")\n    # convert the tokenized datasets to TensorFlow datasets\n    tf_train_dataset = small_train_dataset.to_tf_dataset(\n        columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n        label_cols=[\"labels\"],\n        shuffle=True,\n        collate_fn=data_collator,\n        batch_size=8,\n    )\n    tf_validation_dataset = small_eval_dataset.to_tf_dataset(\n        columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n        label_cols=[\"labels\"],\n        shuffle=False,\n        collate_fn=data_collator,\n        batch_size=8,\n    ) \n    ```", "```\n    import tensorflow as tf\n    from transformers import TFAutoModelForSequenceClassification\n    model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5) \n    ```", "```\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=tf.metrics.SparseCategoricalAccuracy(),\n    )\n    model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3) \n    ```", "```\n!pip install --upgrade tensorflow_hub\nimport tensorflow_hub as hub\nmodel = hub.KerasLayer(\"https://tfhub.dev/google/nnlm-en-dim128/2\")\nembeddings = model([\"The rain in Spain.\", \"falls\",\n                    \"mainly\", \"In the plain!\"])\nprint(embeddings.shape)  #(4,128) \n```", "```\n    !pip install seaborn\n    !pip install sklearn\n    !pip install tensorflow_hub\n    !pip install tensorflow_text\n    import seaborn as sns\n    from sklearn.metrics import pairwise\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import tensorflow_text as text  # Imports TF ops for preprocessing. \n    ```", "```\n    sentences = [\n        \"Do not pity the dead, Harry. Pity the living, and, above all those who live without love.\",\n        \"It is impossible to manufacture or imitate love\",\n        \"Differences of habit and language are nothing at all if our aims are identical and our hearts are open.\",\n        \"What do I care how he looks? I am good-looking enough for both of us, I theenk! All these scars show is zat my husband is brave!\",\n        \"Love as powerful as your mother's for you leaves it's own mark. To have been loved so deeply, even though the person who loved us is gone, will give us some protection forever.\",\n        \"Family…Whatever yeh say, blood's important. . . .\",\n        \"I cared more for your happiness than your knowing the truth, more for your peace of mind than my plan, more for your life than the lives that might be lost if the plan failed.\"\n    ] \n    ```", "```\n    #@title Configure the model { run: \"auto\" }\n    BERT_MODEL = \"https://tfhub.dev/google/experts/bert/wiki_books/2\" # @param {type: \"string\"} [\"https://tfhub.dev/google/experts/bert/wiki_books/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/mnli/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/qnli/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/qqp/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/squad2/2\", \"https://tfhub.dev/google/experts/bert/wiki_books/sst2/2\",  \"https://tfhub.dev/google/experts/bert/pubmed/2\", \"https://tfhub.dev/google/experts/bert/pubmed/squad2/2\"]\n    # Preprocessing must match the model, but all the above use the same.\n    PREPROCESS_MODEL = \"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\"\n    preprocess = hub.load(PREPROCESS_MODEL)\n    bert = hub.load(BERT_MODEL)\n    inputs = preprocess(sentences)\n    outputs = bert(inputs) \n    ```", "```\n    def plot_similarity(features, labels):\n      \"\"\"Plot a similarity matrix of the embeddings.\"\"\"\n      cos_sim = pairwise.cosine_similarity(features)\n      sns.set(font_scale=1.2)\n      cbar_kws=dict(use_gridspec=False, location=\"left\")\n      g = sns.heatmap(\n          cos_sim, xticklabels=labels, yticklabels=labels,\n          vmin=0, vmax=1, cmap=\"Blues\", cbar_kws=cbar_kws)\n      g.tick_params(labelright=True, labelleft=False)\n      g.set_yticklabels(labels, rotation=0)\n      g.set_title(\"Semantic Textual Similarity\")\n    plot_similarity(outputs[\"pooled_output\"], sentences) \n    ```", "```\npython distillation.py --teacher t5-small --data_dir cnn_dm \\\n--student_decoder_layers 3 --student_encoder_layers 6 --tokenizer_name t5-small \\\n--learning_rate=3e-4 --freeze_encoder --no_teacher --freeze_embeds \\\n--do_train --train_batch_size 32 \\\n--do_predict \\\n--model_name_or_path t5-small --eval_beams 2 --eval_max_gen_length 142 \\\n--val_check_interval 0.25 --n_val 1000 \\\n--output_dir distilt5 --gpus 1 --logger_name wandb \n```"]