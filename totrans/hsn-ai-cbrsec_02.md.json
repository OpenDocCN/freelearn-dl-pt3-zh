["```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom sklearn.linear_model import LinearRegression\n\npool = np.random.RandomState(10)\nx = 5 * pool.rand(30)\ny = 3 * x - 2 + pool.randn(30)\n# y = 3x - 2;\n\nlregr = LinearRegression(fit_intercept=False)\nX = x[:, np.newaxis]\nlregr.fit(X, y)\nlspace = np.linspace(0, 5)\nX_regr = lspace[:, np.newaxis]\ny_regr = lregr.predict(X_regr)\nplt.scatter(x, y);\nplt.plot(X_regr, y_regr);\n\n```", "```\nimport pandas as pd\nimport seaborn as sns\n\ndata_df = pd.read_csv(\"../datasets/clustering.csv\")\ndata_df.describe()\nX_data = data_df.drop('class_1', axis=1)\ny_data = data_df['class_1']\n\nfrom sklearn.decomposition import PCA   \n\npca = PCA(n_components=2)               \npca.fit(X_data)                         \nX_2D = pca.transform(X_data)            \ndata_df['PCA1'] = X_2D[:, 0]\ndata_df['PCA2'] = X_2D[:, 1]\n\nfrom sklearn.mixture import GaussianMixture         \n\ngm = GaussianMixture(n_components=3, covariance_type='full')     \ngm.fit(X_data)                         \ny_gm = gm.predict(X_data)              \ndata_df['cluster'] = y_gm\nsns.lmplot(\"PCA1\", \"PCA2\", data=data_df, col='cluster', fit_reg=False)\n```", "```\nfrom matplotlib.colors import ListedColormap \n# Thanks to Sebastian Raschka for 'plot_decision_regions' function \ndef plot_decision_regions(X, y, classifier, resolution=0.02): \n # setup marker generator and color map \n markers = ('s', 'x', 'o', '^', 'v')\n colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n cmap = ListedColormap(colors[:len(np.unique(y))]) \n # plot the decision surface \n x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution), \n np.arange(x2_min, x2_max, resolution))\n Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n Z = Z.reshape(xx1.shape)\n plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n plt.xlim(xx1.min(), xx1.max())\n plt.ylim(xx2.min(), xx2.max()) \n # plot class samples \n for idx, cl in enumerate(np.unique(y)):\n plt.scatter(x=X[y == cl, 0], y=X[y == cl, 1], \n alpha=0.8, c=cmap(idx), \n marker=markers[idx], label=cl) \nfrom sklearn.linear_model import perceptron\nfrom sklearn.datasets import make_classification \nX, y = make_classification(30, 2, 2, 0, weights=[.3, .3], random_state=300) \nplt.scatter(X[:,0], X[:,1], s=50)\npct = perceptron.Perceptron(max_iter=100, verbose=0, random_state=300, \nfit_intercept=True, eta0=0.002)\npct.fit(X, y)\nplot_decision_regions(X, y, classifier=pct)\nplt.title('Perceptron')\nplt.xlabel('X')\nplt.ylabel('Y')\nplt.show()\n```"]