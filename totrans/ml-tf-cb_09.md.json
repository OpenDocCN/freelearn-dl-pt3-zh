["```\nimport tensorflow as tf\nfrom tensorflow import keras \n# keras module for building LSTM \nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n```", "```\nimport pandas as pd\nimport string, os \nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning) \n```", "```\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Embedding, LSTM, Dense\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nimport keras.utils as ku \n```", "```\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action='ignore', category=FutureWarning) \n```", "```\ndef clean_text(txt):\n    txt = \"\".join(v for v in txt if v not in string.punctuation).lower()\n    txt = txt.encode(\"utf8\").decode(\"ascii\",'ignore')\n    return txt \n```", "```\ndef get_sequence_of_tokens(corpus):\n    ## tokenization\n    tokenizer.fit_on_texts(corpus)\n    total_words = len(tokenizer.word_index) + 1\n\n    ## convert data to sequence of tokens \n    input_sequences = []\n    for line in corpus:\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words \n```", "```\ndef create_model(max_sequence_len, total_words):\n    input_len = max_sequence_len - 1\n    model = Sequential()\n\n    model.add(Embedding(total_words, 10, input_length=input_len))\n    model.add(LSTM(100))\n    model.add(Dense(total_words, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n    return model \n```", "```\ndef generate_padded_sequences(input_sequences):\n    max_sequence_len = max([len(x) for x in input_sequences])\n    input_sequences = np.array(pad_sequences(input_sequences,                       maxlen=max_sequence_len, padding='pre'))\n\n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    label = ku.to_categorical(label, num_classes=total_words)\n    return predictors, label, max_sequence_len \n```", "```\ndef generate_text(seed_text, next_words, model, max_sequence_len):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list],                      maxlen=max_sequence_len-1, padding='pre')\n        predicted = model.predict_classes(token_list, verbose=0)\n\n        output_word = \"\"\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    return seed_text.title() \n```", "```\ncurr_dir = '../input/'\nall_headlines = []\nfor filename in os.listdir(curr_dir):\n    if 'Articles' in filename:\n        article_df = pd.read_csv(curr_dir + filename)\n        all_headlines.extend(list(article_df.headline.values))\n        break\nall_headlines[:10] \n```", "```\n['The Opioid Crisis Foretold',\n 'The Business Deals That Could Imperil Trump',\n 'Adapting to American Decline',\n 'The Republicans' Big Senate Mess',\n 'States Are Doing What Scott Pruitt Won't',\n 'Fake Pearls, Real Heart',\n 'Fear Beyond Starbucks',\n 'Variety: Puns and Anagrams',\n 'E.P.A. Chief's Ethics Woes Have Echoes in His Past',\n 'Where Facebook Rumors Fuel Thirst for Revenge'] \n```", "```\ncorpus = [clean_text(x) for x in all_headlines] \n```", "```\ncorpus[:10]\n['the opioid crisis foretold',\n 'the business deals that could imperil trump',\n 'adapting to american decline',\n 'the republicans big senate mess',\n 'states are doing what scott pruitt wont',\n 'fake pearls real heart',\n 'fear beyond starbucks',\n 'variety puns and anagrams',\n 'epa chiefs ethics woes have echoes in his past',\n 'where facebook rumors fuel thirst for revenge'] \n```", "```\ntokenizer = Tokenizer()\ninp_sequences, total_words = get_sequence_of_tokens(corpus)\ninp_sequences[:10]\n[[1, 708],\n [1, 708, 251],\n [1, 708, 251, 369],\n [1, 370],\n [1, 370, 709],\n [1, 370, 709, 29],\n [1, 370, 709, 29, 136],\n [1, 370, 709, 29, 136, 710],\n [1, 370, 709, 29, 136, 710, 10],\n [711, 5]] \n```", "```\npredictors, label, max_sequence_len =                              generate_padded_sequences(inp_sequences) \n```", "```\n    model = create_model(max_sequence_len, total_words)\n    model.summary()\n    _________________________________________________________________\n    Layer (type)                 Output Shape              Param #   \n    =================================================================\n    embedding_1 (Embedding)      (None, 23, 10)            31340     \n    _________________________________________________________________\n    lstm_1 (LSTM)                (None, 100)               44400     \n    _________________________________________________________________\n    dense_1 (Dense)              (None, 3134)              316534    \n    =================================================================\n    Total params: 392,274\n    Trainable params: 392,274\n    Non-trainable params: 0\n    _________________________________________________________________ \n    ```", "```\nmodel.fit(predictors, label, epochs=100, verbose=2) \n```", "```\nprint (generate_text(\"united states\", 5, model, max_sequence_len))\nUnited States Shouldnt Sit Still An Atlantic\nprint (generate_text(\"president trump\", 5, model, max_sequence_len))\nPresident Trump Vs Congress Bird Moving One\nprint (generate_text(\"joe biden\", 8, model, max_sequence_len))\nJoe Biden Infuses The Constitution Invaded Canada Unique Memorial Award\nprint (generate_text(\"india and china\", 8, model, max_sequence_len))\nIndia And China Deal And The Young Think Again To It\nprint (generate_text(\"european union\", 4, model, max_sequence_len))\nEuropean Union Infuses The Constitution Invaded \n```", "```\niconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv \n```", "```\nimport json\nimport tensorflow as tf\nimport csv\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras import regularizers \n```", "```\nembedding_dim = 100\nmax_length = 16\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size=160000\ntest_portion=.1\nnum_epochs = 50\ndropout_val = 0.2\nnof_units = 64 \n```", "```\ndef create_model(dropout_val, nof_units):\n\n    model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n    tf.keras.layers.Dropout(dropout_val),\n    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n    tf.keras.layers.MaxPooling1D(pool_size=4),\n    tf.keras.layers.LSTM(nof_units),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    model.compile(loss='binary_crossentropy',optimizer='adam',                  metrics=['accuracy'])\n    return model \n```", "```\nnum_sentences = 0\nwith open(\"../input/twitter-sentiment-clean-dataset/training_cleaned.csv\") as csvfile:\n    reader = csv.reader(csvfile, delimiter=',')\n    for row in reader:\n        list_item=[]\n        list_item.append(row[5])\n        this_label=row[0]\n        if this_label=='0':\n            list_item.append(0)\n        else:\n            list_item.append(1)\n        num_sentences = num_sentences + 1\n        corpus.append(list_item) \n```", "```\nsentences=[]\nlabels=[]\nrandom.shuffle(corpus)\nfor x in range(training_size):\n    sentences.append(corpus[x][0])\n    labels.append(corpus[x][1])\n    Tokenize the sentences:\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\nsequences = tokenizer.texts_to_sequences(sentences) \n```", "```\npadded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type) \n```", "```\nsplit = int(test_portion * training_size)\ntest_sequences = padded[0:split]\ntraining_sequences = padded[split:training_size]\ntest_labels = labels[0:split]\ntraining_labels = labels[split:training_size] \n```", "```\nembeddings_index = {};\nwith open('../input/glove6b/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector; \n```", "```\nmodel = create_model(dropout_val, nof_units)\nmodel.summary()\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding (Embedding)        (None, 16, 100)           13877100  \n_________________________________________________________________\ndropout (Dropout)            (None, 16, 100)           0         \n_________________________________________________________________\nconv1d (Conv1D)              (None, 12, 64)            32064     \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 3, 64)             0         \n_________________________________________________________________\nlstm (LSTM)                  (None, 64)                33024     \n_________________________________________________________________\ndense (Dense)                (None, 1)                 65        \n=================================================================\nTotal params: 13,942,253\nTrainable params: 65,153\nNon-trainable params: 13,877,100\n_________________________________________________________________ \n```", "```\nnum_epochs = 50\nhistory = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=2)\nTrain on 144000 samples, validate on 16000 samples\nEpoch 1/50\n144000/144000 - 47s - loss: 0.5685 - acc: 0.6981 - val_loss: 0.5454 - val_acc: 0.7142\nEpoch 2/50\n144000/144000 - 44s - loss: 0.5296 - acc: 0.7289 - val_loss: 0.5101 - val_acc: 0.7419\nEpoch 3/50\n144000/144000 - 42s - loss: 0.5130 - acc: 0.7419 - val_loss: 0.5044 - val_acc: 0.7481\nEpoch 4/50\n144000/144000 - 42s - loss: 0.5017 - acc: 0.7503 - val_loss: 0.5134 - val_acc: 0.7421\nEpoch 5/50\n144000/144000 - 42s - loss: 0.4921 - acc: 0.7563 - val_loss: 0.5025 - val_acc: 0.7518\nEpoch 6/50\n144000/144000 - 42s - loss: 0.4856 - acc: 0.7603 - val_loss: 0.5003 - val_acc: 0.7509 \n```", "```\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(acc))\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show() \n```", "```\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler \n```", "```\nprediction_days = 30\nnof_units =4 \n```", "```\ndef create_model(nunits):\n    # Initialising the RNN\n    regressor = Sequential()\n    # Adding the input layer and the LSTM layer\n    regressor.add(LSTM(units = nunits, activation = 'sigmoid', input_shape = (None, 1)))\n    # Adding the output layer\n    regressor.add(Dense(units = 1))\n    # Compiling the RNN\n    regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n    return regressor \n```", "```\n# Import the dataset and encode the date\ndf = pd.read_csv(\"../input/bitcoin-historical-data/bitstampUSD_1-min_data_2012-01-01_to_2020-09-14.csv\")\ndf['date'] = pd.to_datetime(df['Timestamp'],unit='s').dt.date\ngroup = df.groupby('date')\nReal_Price = group['Weighted_Price'].mean() \n```", "```\ndf_train= Real_Price[:len(Real_Price)-prediction_days]\ndf_test= Real_Price[len(Real_Price)-prediction_days:] \n```", "```\ntraining_set = df_train.values\ntraining_set = np.reshape(training_set, (len(training_set), 1))\nsc = MinMaxScaler()\ntraining_set = sc.fit_transform(training_set)\nX_train = training_set[0:len(training_set)-1]\ny_train = training_set[1:len(training_set)]\nX_train = np.reshape(X_train, (len(X_train), 1, 1)) \n```", "```\nregressor = create_model(nunits = nof_unit)\nregressor.fit(X_train, y_train, batch_size = 5, epochs = 100)\nEpoch 1/100\n3147/3147 [==============================] - 6s 2ms/step - loss: 0.0319\nEpoch 2/100\n3147/3147 [==============================] - 3s 928us/step - loss: 0.0198\nEpoch 3/100\n3147/3147 [==============================] - 3s 985us/step - loss: 0.0089\nEpoch 4/100\n3147/3147 [==============================] - 3s 1ms/step - loss: 0.0023\nEpoch 5/100\n3147/3147 [==============================] - 3s 886us/step - loss: 3.3583e-04\nEpoch 6/100\n3147/3147 [==============================] - 3s 957us/step - loss: 1.0990e-04\nEpoch 7/100\n3147/3147 [==============================] - 3s 830us/step - loss: 1.0374e-04\nEpoch 8/100 \n```", "```\ntest_set = df_test.values\ninputs = np.reshape(test_set, (len(test_set), 1))\ninputs = sc.transform(inputs)\ninputs = np.reshape(inputs, (len(inputs), 1, 1))\npredicted_BTC_price = regressor.predict(inputs)\npredicted_BTC_price = sc.inverse_transform(predicted_BTC_price) \n```", "```\nplt.figure(figsize=(25,15), dpi=80, facecolor='w', edgecolor='k')\nax = plt.gca()  \nplt.plot(test_set, color = 'red', label = 'Real BTC Price')\nplt.plot(predicted_BTC_price, color = 'blue', label = 'Predicted BTC Price')\nplt.title('BTC Price Prediction', fontsize=40)\ndf_test = df_test.reset_index()\nx=df_test.index\nlabels = df_test['date']\nplt.xticks(x, labels, rotation = 'vertical')\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label1.set_fontsize(18)\nfor tick in ax.yaxis.get_major_ticks():\n    tick.label1.set_fontsize(18)\nplt.xlabel('Time', fontsize=40)\nplt.ylabel('BTC Price(USD)', fontsize=40)\nplt.legend(loc=2, prop={'size': 25})\nplt.show() \n```", "```\nimport os\nimport json\nimport gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate, Masking\nfrom tensorflow.keras.layers import LSTM, Bidirectional, GlobalMaxPooling1D, Dropout\nfrom tensorflow.keras.preprocessing import text, sequence\nfrom tqdm import tqdm_notebook as tqdm\nimport fasttext\nfrom tensorflow.keras.models import load_model \n```", "```\nembedding_path = '/kaggle/input/fasttext-crawl-300d-2m-with-subword/crawl-300d-2m-subword/crawl-300d-2M-subword.bin' \n```", "```\ndef build_train(train_path, n_rows=200000, sampling_rate=15):\n    with open(train_path) as f:\n        processed_rows = []\n        for i in tqdm(range(n_rows)):\n            line = f.readline()\n            if not line:\n                break\n            line = json.loads(line)\n            text = line['document_text'].split(' ')\n            question = line['question_text']\n            annotations = line['annotations'][0]\n            for i, candidate in enumerate(line['long_answer_candidates']):\n                label = i == annotations['long_answer']['candidate_index']\n                start = candidate['start_token']\n                end = candidate['end_token']\n                if label or (i % sampling_rate == 0):\n                    processed_rows.append({\n                        'text': \" \".join(text[start:end]),\n                        'is_long_answer': label,\n                        'question': question,\n                        'annotation_id': annotations['annotation_id']\n                    })\n        train = pd.DataFrame(processed_rows)\n\n        return train\ndef build_test(test_path):\n    with open(test_path) as f:\n        processed_rows = []\n        for line in tqdm(f):\n            line = json.loads(line)\n            text = line['document_text'].split(' ')\n            question = line['question_text']\n            example_id = line['example_id']\n            for candidate in line['long_answer_candidates']:\n                start = candidate['start_token']\n                end = candidate['end_token']\n                processed_rows.append({\n                    'text': \" \".join(text[start:end]),\n                    'question': question,\n                    'example_id': example_id,\n                    'sequence': f'{start}:{end}'\n                })\n        test = pd.DataFrame(processed_rows)\n\n    return test \n```", "```\ndef compute_text_and_questions(train, test, tokenizer):\n    train_text = tokenizer.texts_to_sequences(train.text.values)\n    train_questions = tokenizer.texts_to_sequences(train.question.values)\n    test_text = tokenizer.texts_to_sequences(test.text.values)\n    test_questions = tokenizer.texts_to_sequences(test.question.values)\n\n    train_text = sequence.pad_sequences(train_text, maxlen=300)\n    train_questions = sequence.pad_sequences(train_questions)\n    test_text = sequence.pad_sequences(test_text, maxlen=300)\n    test_questions = sequence.pad_sequences(test_questions)\n\n    return train_text, train_questions, test_text, test_questions \n```", "```\ndef build_embedding_matrix(tokenizer, path):\n    embedding_matrix = np.zeros((tokenizer.num_words + 1, 300))\n    ft_model = fasttext.load_model(path)\n    for word, i in tokenizer.word_index.items():\n        if i >= tokenizer.num_words - 1:\n            break\n        embedding_matrix[i] = ft_model.get_word_vector(word)\n\n    return embedding_matrix \n```", "```\ndef build_model(embedding_matrix):\n    embedding = Embedding(\n        *embedding_matrix.shape, \n        weights=[embedding_matrix], \n        trainable=False, \n        mask_zero=True\n    )\n\n    q_in = Input(shape=(None,))\n    q = embedding(q_in)\n    q = SpatialDropout1D(0.2)(q)\n    q = Bidirectional(LSTM(100, return_sequences=True))(q)\n    q = GlobalMaxPooling1D()(q)\n\n    t_in = Input(shape=(None,))\n    t = embedding(t_in)\n    t = SpatialDropout1D(0.2)(t)\n    t = Bidirectional(LSTM(150, return_sequences=True))(t)\n    t = GlobalMaxPooling1D()(t)\n\n    hidden = concatenate([q, t])\n    hidden = Dense(300, activation='relu')(hidden)\n    hidden = Dropout(0.5)(hidden)\n    hidden = Dense(300, activation='relu')(hidden)\n    hidden = Dropout(0.5)(hidden)\n\n    out1 = Dense(1, activation='sigmoid')(hidden)\n\n    model = Model(inputs=[t_in, q_in], outputs=out1)\n    model.compile(loss='binary_crossentropy', optimizer='adam')\n    return model \n```", "```\ndirectory = '../input/tensorflow2-question-answering/'\ntrain_path = directory + 'simplified-nq-train.jsonl'\ntest_path = directory + 'simplified-nq-test.jsonl'\ntrain = build_train(train_path)\ntest = build_test(test_path) \n```", "```\ntrain.head() \n```", "```\ntokenizer = text.Tokenizer(lower=False, num_words=80000)\nfor text in tqdm([train.text, test.text, train.question, test.question]):\n    tokenizer.fit_on_texts(text.values)\ntrain_target = train.is_long_answer.astype(int).values\ntrain_text, train_questions, test_text, test_questions = compute_text_and_questions(train, test, tokenizer)\ndel train \n```", "```\nembedding_matrix = build_embedding_matrix(tokenizer, embedding_path)\nmodel = build_model(embedding_matrix)\nmodel.summary()\nModel: \"functional_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 300)    24000300    input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nspatial_dropout1d (SpatialDropo (None, None, 300)    0           embedding[0][0]                  \n__________________________________________________________________________________________________\nspatial_dropout1d_1 (SpatialDro (None, None, 300)    0           embedding[1][0]                  \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, None, 200)    320800      spatial_dropout1d[0][0]          \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, None, 300)    541200      spatial_dropout1d_1[0][0]        \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 200)          0           bidirectional[0][0]              \n__________________________________________________________________________________________________\nglobal_max_pooling1d_1 (GlobalM (None, 300)          0           bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 500)          0           global_max_pooling1d[0][0]       \n                                                                 global_max_pooling1d_1[0][0]     \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 300)          150300      concatenate[0][0]                \n__________________________________________________________________________________________________\ndropout (Dropout)               (None, 300)          0           dense[0][0]                      \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 300)          90300       dropout[0][0]                    \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 300)          0           dense_1[0][0]                    \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 1)            301         dropout_1[0][0]                  \n==================================================================================================\nTotal params: 25,103,201\nTrainable params: 1,102,901\nNon-trainable params: 24,000,300\n__________________________________________________________________________________________________ \n```", "```\ntrain_history = model.fit(\n    [train_text, train_questions], \n    train_target,\n    epochs=2,\n    validation_split=0.2,\n    batch_size=1024\n) \n```", "```\ndirectory = '/kaggle/input/tensorflow2-question-answering/'\ntest_path = directory + 'simplified-nq-test.jsonl'\ntest = build_test(test_path)\nsubmission = pd.read_csv(\"../input/tensorflow2-question-answering/sample_submission.csv\")\ntest_text, test_questions = compute_text_and_questions(test, tokenizer) \n```", "```\ntest_target = model.predict([test_text, test_questions], batch_size=512)\ntest['target'] = test_target\nresult = (\n    test.query('target > 0.3')\n    .groupby('example_id')\n    .max()\n    .reset_index()\n    .loc[:, ['example_id', 'PredictionString']]\n)\nresult.head() \n```"]