<html><head></head><body>
		<div>
			<div id="_idContainer256" class="Content">
			</div>
		</div>
		<div id="_idContainer257" class="Content">
			<h1 id="_idParaDest-132"><a id="_idTextAnchor153"/>7. Convolutional Neural Networks</h1>
		</div>
		<div id="_idContainer299" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will learn how <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) process image data. You will also learn how to correctly use a CNN on image data. </p>
			<p class="callout">By the end of the chapter, you will be able to create your own CNN for classification and object identification on any image dataset using TensorFlow.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor154"/>Introduction</h1>
			<p>This chapter covers CNNs. CNNs use convolutional layers that are well-suited to extracting features from images. They use learning filters that correlate with the task at hand. Simply put, they are very good at finding patterns in images.</p>
			<p>In the previous chapter, you explored regularization and hyperparameter tuning. You used L1 and L2 regularization and added dropout to a classification model to prevent overfitting on the <strong class="source-inline">connect-4</strong> dataset.</p>
			<p>You will now be shifting gears quite a bit as you dive into deep learning with CNNs. In this chapter, you will learn the fundamentals of how CNNs process image data and how to apply those concepts to your own image classification problem. This is truly where TensorFlow shines. </p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor155"/>CNNs</h1>
			<p>CNNs share many common components with the ANNs you have built so far. The key difference is the inclusion of one or more convolutional layers within the network. Convolutional layers apply convolutions of input data with filters, also known as kernels. Think of a <strong class="bold">convolution</strong> as an <strong class="bold">image transformer</strong>. You have an input image, which goes through the CNN and gives you an output label. Each layer has a unique function or special ability to detect patterns such as curves or edges in an image. CNNs combine the power of deep neural networks and kernel convolutions to transform images and make these image edges or curves easy for the model to see. There are three key components in a CNN:</p>
			<ul>
				<li><strong class="bold">Input image</strong>: The raw image data</li>
				<li><strong class="bold">Filter/kernel</strong>: The image transformation mechanism</li>
				<li><strong class="bold">Output label</strong>: The image classification</li>
			</ul>
			<p>The following figure is an example of a CNN in which the image is input into the network on the left-hand side and the output is generated on the right-hand side. The image components are identified throughout the hidden layers with more basic components, such as edges, identified in earlier hidden layers. Image components combine in the hidden layers to form recognizable features from the dataset. For example, in a CNN to classify images into planes or cars, the recognizable features may be filters that resemble a wheel or propellor. Combinations of these features will be instrumental in determining whether the image is a plane or a car. </p>
			<p>Finally, the output layer is a dense layer used to determine the specific output of the model. For a binary classification model, this may be a dense layer with one unit with a sigmoid activation function. For a more complex multi-class classification, it may be a dense layer with many units, determined by the number of classes, and a softmax activation function to determine one output label for each image presented to the model.</p>
			<div>
				<div id="_idContainer258" class="IMG---Figure">
					<img src="image/B16341_07_01.jpg" alt="Figure 7.1: CNN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1: CNN</p>
			<p>A common CNN configuration includes a convolutional layer followed by a pooling layer. These layers are often used together in this order, as pairs (convolution and pooling). We'll get into the reason for this later in the chapter, but for now, think of these pooling layers as decreasing the size of input images by summarizing the filter results.</p>
			<p>Before you move deeper into convolutional layers, you first need to understand what the data looks like from the computer's perspective.</p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor156"/>Image Representation</h1>
			<p>First, consider how a computer processes an image. To a computer, images are numbers. To be able to work with images for classification or object identification, you need to understand how a model transforms an image input into data. A <strong class="bold">pixel</strong> in an image file is just a piece of data. </p>
			<p>In the following figure, you can see an example of pixel values for a grayscale image of the number eight. For the <strong class="source-inline">28x28</strong>-pixel image, there are a total of <strong class="source-inline">784</strong> pixels. Each pixel has a value between <strong class="source-inline">0</strong> and <strong class="source-inline">255</strong> identifying how light or dark the pixel is. On the right side, there is one large column vector with each pixel value listed. This is used by the model to identify the image.</p>
			<div>
				<div id="_idContainer259" class="IMG---Figure">
					<img src="image/B16341_07_02.jpg" alt="Figure 7.2: Pixel values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2: Pixel values</p>
			<p>Now that you know what the input data looks like, it's time to get a closer look at the convolutional process—more specifically, the convolutional layer.</p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor157"/>The Convolutional Layer</h1>
			<p>Think of a convolution as nothing more than an image transformer with three key elements. First, there is an input image, then a filter, and finally, a feature map.</p>
			<p>This section will cover each of these in turn to give you a solid idea of how images are filtered in a convolutional layer. The convolution is the process of passing a filter window over the input data, which will result in a map of activations known as a <strong class="bold">feature map</strong>. The input data may be the input image to the model or the output of a prior, intermediary layer of the model. The filter is generally a much smaller array, such as <strong class="source-inline">3x3</strong> for two-dimensional data, in which the specific values of the filter are learned during the training process. The filter passes across the input data with a window size equal to the size of the filter, then, the scalar product of the filter and section of the input data is applied, producing what's known as an <strong class="bold">activation</strong>. As this process continues across the entire input data using the same filter, the map of activations is produced, also known as the <strong class="bold">feature map</strong>.</p>
			<p>This concept is illustrated in the following figure, which has two convolutional layers, producing two sets of feature maps. After the feature maps are produced from the first convolutional layer, they are passed into the second convolutional layer. The feature map of the second convolutional layer is passed into a classifier:</p>
			<div>
				<div id="_idContainer260" class="IMG---Figure">
					<img src="image/B16341_07_03.jpg" alt="Figure 7.3: Convolution for classification&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3: Convolution for classification</p>
			<p>The distance, or number of steps, the filter moves with each operation is known as the <strong class="bold">stride</strong>. If the filter goes off the edge, you can do what's called <strong class="bold">padding with zeros</strong>. This way, the output map size is the same as the input map size. This is called <strong class="bold">same padding</strong>. However, if the filter cannot take its required stride without leaning over the edge somewhat, it will count any value over the edge as <strong class="source-inline">0</strong>. This is known as <strong class="bold">valid padding</strong>.</p>
			<p>Let's recap some keywords. There's a <strong class="bold">kernel</strong>, which is a small matrix that is used to apply an effect, and what you saw in the example was a <strong class="source-inline">2x2</strong> kernel. There's <strong class="bold">stride</strong>, which is the number of pixels that you move the kernel by. Lastly, there's <strong class="bold">padding with zeros</strong> around the image, whether or not you add pixels. This ensures that the output is the same size as the input.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor158"/>Creating the Model</h2>
			<p>From the very first chapter, you encountered different types of dimensional tensors. One important thing to note is that you will only be working with <strong class="source-inline">Conv2D</strong>. The layer name <strong class="source-inline">Conv2D</strong> refers only to the movement of a <strong class="bold">filter</strong> or <strong class="bold">kernel</strong>. So, if you recall the description of what the convolutional process is doing, it's simply sliding a kernel across a 2D space. So, for a flat, square image, the kernel only slides in two dimensions.</p>
			<p>When you implement <strong class="source-inline">Conv2D</strong>, you need to pass in certain parameters:</p>
			<ol>
				<li>The first parameter is <strong class="source-inline">filter</strong>. The filters are the dimensionality of the output space. </li>
				<li>Specify <strong class="source-inline">strides</strong>, which is how many pixels will move the kernel across.</li>
				<li>Then, specify <strong class="source-inline">padding</strong>, which is usually <strong class="source-inline">valid</strong> or <strong class="source-inline">same</strong> depending on whether you want an output that is of the same dimension as the input.</li>
				<li>Finally, you can also have <strong class="source-inline">activation</strong>. Here, you will specify what sort of activation you would like to apply to the outputs. If you don't specify an activation, it's simply a linear activation.</li>
			</ol>
			<p>Before you continue, recall from <em class="italic">Chapter 4</em>, <em class="italic">Regression and Classification Models</em>, that a dense layer is one in which every neuron is connected to every neuron in the previous layer. As you can see in the following code, you can easily add a dense layer with <strong class="source-inline">model.add(Dense(32))</strong>. <strong class="source-inline">32</strong> is the number of neurons, followed by the input shape. <strong class="bold">AlexNet</strong> is an example of a CNN with multiple convolution kernels that extracts interesting information from an image.</p>
			<div>
				<div id="_idContainer261" class="IMG---Figure">
					<img src="image/B16341_07_04.jpg" alt="Figure 7.4: AlexNet consists of five convolution layers and three connected layers &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4: AlexNet consists of five convolution layers and three connected layers </p>
			<p class="callout-heading">Note</p>
			<p class="callout">AlexNet is the name of a CNN designed by Alex Krizhevsky.</p>
			<p>A sequential model can be used to build a CNN. Different methods can be used to add a layer; here, we will use the framework of sequentially adding layers to the model using the model's <strong class="source-inline">add</strong> method or passing in a list of all layers when the model is instantiated:</p>
			<p class="source-code">model = models.Sequential()</p>
			<p class="source-code">model.add(Dense(32, input_shape=(250,)))</p>
			<p>The following is a code block showing the code that you'll be using later in the chapter:</p>
			<p class="source-code">our_cnn_model = models.Sequential([layers.Conv2D\</p>
			<p class="source-code">                                   (filters = 32, \</p>
			<p class="source-code">                                    kernel_size = (3,3),</p>
			<p class="source-code">                                    input_shape=(28, 28, 1)), \</p>
			<p class="source-code">                                   layers.Activation('relu'), \</p>
			<p class="source-code">                                   layers.MaxPool2D\</p>
			<p class="source-code">                                   (pool_size = (2, 2)), \</p>
			<p class="source-code">                                   layers.Conv2D\</p>
			<p class="source-code">                                   (filters = 64, \</p>
			<p class="source-code">                                    kernel_size = (3,3)), \</p>
			<p class="source-code">                                   layers.Activation('relu'), \</p>
			<p class="source-code">                                   layers.MaxPool2D\</p>
			<p class="source-code">                                   (pool_size = (2,2)), \</p>
			<p class="source-code">                                   layers.Conv2D\</p>
			<p class="source-code">                                   (filters = 64, \</p>
			<p class="source-code">                                    kernel_size = (3,3)), \</p>
			<p class="source-code">                                    layers.Activation('relu')])</p>
			<p>Use the <strong class="source-inline">Conv2D</strong> layer when working with data that you want to convolve in two dimensions, such as images. For parameters, set the number of filters to <strong class="source-inline">32</strong>, followed by the kernel size of <strong class="source-inline">3x3</strong> pixels (<strong class="source-inline">(3, 3)</strong> in the example). In the first layer, you will always need to specify the <strong class="source-inline">input_shape</strong> dimensions, the height, width, and depth. <strong class="source-inline">input_shape</strong> is the size of the images you will be using. You can also select the activation function to be applied at the end of the layer.</p>
			<p>Now that you have learned how to build a CNN layer in your model, you will practice doing so in your first exercise. In this exercise, you will build the first constructs of a CNN, initialize the model, and add a single convolutional layer to the model.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor159"/>Exercise 7.01: Creating the First Layer to Build a CNN</h2>
			<p>As a TensorFlow freelancer, you've been asked to show your potential employer a few lines of code that demonstrate how you might build the first layer in a CNN. They ask that you keep it simple but provide the first few steps to create a CNN layer. In this exercise, you will complete the first step in creating a CNN—that is, adding the first convolutional layer.</p>
			<p>Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook.</li>
				<li>Import the TensorFlow library and the <strong class="source-inline">models</strong> and <strong class="source-inline">layers</strong> classes from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import models, layers</p></li>
				<li>Check the TensorFlow version:<p class="source-code">print(tf.__version__)</p><p>You should get the following output:</p><p class="source-code">2.6.0</p></li>
				<li>Now, use <strong class="source-inline">models.Sequential</strong> to create your model. The first layer (<strong class="source-inline">Conv2D</strong>) will require the number of nodes (<strong class="source-inline">filters</strong>), the filter size (<strong class="source-inline">3,3</strong>), and the shape of the input. <strong class="source-inline">input_shape</strong> for your first layer will determine the shape of your input images. Add a ReLU activation layer:<p class="source-code">image_shape = (300, 300, 3)</p><p class="source-code">our_first_layer = models.Sequential([layers.Conv2D\</p><p class="source-code">                                    (filters = 16, \</p><p class="source-code">                                    kernel_size = (3,3), \</p><p class="source-code">                                    input_shape = image_shape), \</p><p class="source-code">                                    layers.Activation('relu')])</p><p>Simple enough. You have just taken the first steps in creating your first CNN.</p></li>
			</ol>
			<p>You will now move on to the type of layer that usually follows a convolutional layer—the pooling layer.</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor160"/>Pooling Layer</h1>
			<p>Pooling is an operation that is commonly added to a CNN to reduce the dimensionality of an image by reducing the number of pixels in the output from the convolutional layer it follows. <strong class="bold">Pooling layers</strong> shrink the input image to increase computational efficiency and reduce the number of parameters to limit the risk of <strong class="bold">overfitting</strong>.</p>
			<p>A <strong class="bold">pooling layer</strong> immediately follows a convolution layer and is considered another important part of the CNN structure. This section will focus on two types of pooling:</p>
			<ul>
				<li>Max pooling</li>
				<li>Average pooling</li>
			</ul>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor161"/>Max Pooling</h2>
			<p>With max pooling, a filter or kernel only retains the largest pixel value from an input matrix. To get a clearer idea of what is happening, consider the following example. Say you have a <strong class="source-inline">4x4</strong> input. This first step in max pooling would be to divide the <strong class="source-inline">4x4</strong> matrix into four quadrants. Each quadrant will be of the size <strong class="source-inline">2x2</strong>. Apply a filter of size <strong class="source-inline">2</strong>. This means that your filter will look exactly like a <strong class="source-inline">2x2</strong> matrix.</p>
			<p>Begin by placing the filter on top of your input. For max pooling, this filter will look at all values within the <strong class="source-inline">2x2</strong> area that it covers. It will find the largest value, send that value to your output, and store it there in the upper-left corner of the feature map.</p>
			<div>
				<div id="_idContainer262" class="IMG---Figure">
					<img src="image/B16341_07_05.jpg" alt="Figure 7.5: Max pooling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5: Max pooling</p>
			<p>Then, the filter will move over to the right and repeat the same process, storing the value in the upper-right corner of the <strong class="source-inline">2x2</strong> matrix. Once this operation is complete, the filter will slide down and start at the far left, again repeating the same process, looking for the largest (or maximum) value, and then storing it in the correct place on the <strong class="source-inline">2x2</strong> matrix.</p>
			<p>Recall that the sliding movement is referred to as <strong class="bold">stride</strong>. So, the filter was moving over two places. This would mean it has a stride value of <strong class="source-inline">2</strong>. This process is repeated until the maximum values in each of the four quadrants are <strong class="source-inline">8</strong>, <strong class="source-inline">5</strong>, <strong class="source-inline">7</strong>, and <strong class="source-inline">5</strong>, respectively. Again, to get these numbers, you used a filter of <strong class="source-inline">2x2</strong> and filtered for the largest number within that <strong class="source-inline">2x2</strong> matrix.</p>
			<p>So, in this case, you had a stride of two because you moved two pixels. These are the <strong class="bold">hyperparameters</strong> for max pooling. The values of <strong class="source-inline">filter</strong> and <strong class="source-inline">stride</strong> are <strong class="source-inline">2</strong>. <em class="italic">Figure 7.6</em> shows what an implementation of max pooling might look like with a filter size of 3 x 3 and a <strong class="source-inline">stride</strong> of <strong class="source-inline">1</strong>. </p>
			<p>There are two steps shown in <em class="italic">Figure 7.6</em>. Start at the upper left of the feature map. With the <strong class="source-inline">3x3</strong> filter, you would look at the following numbers, <strong class="source-inline">2</strong>, <strong class="source-inline">8</strong>, <strong class="source-inline">2</strong>, <strong class="source-inline">5</strong>, <strong class="source-inline">4</strong>, <strong class="source-inline">9</strong>, <strong class="source-inline">8</strong>, <strong class="source-inline">4</strong>, and <strong class="source-inline">6</strong>, and choose the largest value, <strong class="source-inline">9</strong>. The <strong class="source-inline">9</strong> would be placed in the upper-left box of our pooled feature map. With a stride of <strong class="source-inline">1</strong>, you would slide the filter one place to the right, as shown in gray.</p>
			<p>Now, look for the largest values from <strong class="source-inline">8</strong>, <strong class="source-inline">2</strong>, <strong class="source-inline">1</strong>, <strong class="source-inline">4</strong>, <strong class="source-inline">9</strong>, <strong class="source-inline">6</strong>, <strong class="source-inline">4</strong>, <strong class="source-inline">6</strong>, and <strong class="source-inline">4</strong>. Again, <strong class="source-inline">9</strong> is the largest value, so add a <strong class="source-inline">9</strong> to the middle place in the top row of the pooled feature map (shown in gray).</p>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/B16341_07_06.jpg" alt="Figure 7.6: Pooled feature map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6: Pooled feature map</p>
			<p>The preceding pool size is <strong class="source-inline">(2, 2)</strong>. It specifies factors that you will downscale with. Here's a more detailed look at what you could do to implement <strong class="source-inline">MaxPool2D</strong>:</p>
			<p class="source-code">layers.MaxPool2D(pool_size=(2, 2), strides=None, \</p>
			<p class="source-code">                 padding='valid')</p>
			<p><strong class="bold">MaxPool2D</strong>: The preceding code snippet introduces a <strong class="source-inline">MaxPool2D</strong> instance. The code snippet initializes a max pooling layer with a pool size of <strong class="source-inline">2x2</strong> and the <strong class="source-inline">stride</strong> value is not specified, so it will default to the pool size value. The <strong class="source-inline">padding</strong> parameter is set to <strong class="source-inline">valid</strong>, meaning there is no padding added. The following code snippet demonstrates its use within a CNN:</p>
			<p class="source-code">image_shape = (300, 300, 3)</p>
			<p class="source-code">our_first_model = models.Sequential([</p>
			<p class="source-code">    layers.Conv2D(filters = 16, kernel_size = (3,3), \</p>
			<p class="source-code">                  input_shape = image_shape), \</p>
			<p class="source-code">    layers.Activation('relu'), \</p>
			<p class="source-code">    layers.MaxPool2D(pool_size = (2, 2)), \</p>
			<p class="source-code">    layers.Conv2D(filters = 32, kernel_size = (3,3)), \</p>
			<p class="source-code">    layers.Activation('relu')])</p>
			<p>In the preceding example, a sequential model is created with two convolutional layers, after each layer is a ReLU activation function, and after the activation function of the first convolutional layer is a max pooling layer.</p>
			<p>Now that you have explored max pooling, let's look at the other type of pooling: average pooling.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor162"/>Average Pooling</h2>
			<p><strong class="bold">Average pooling</strong> operates in a similar way to max pooling, but instead of extracting the largest weight value within the filter, it calculates the average. It then passes along that value to the feature map. <em class="italic">Figure 7.7</em> highlights the difference between max pooling and average pooling.</p>
			<p>In <em class="italic">Figure 7.7</em>, consider the <strong class="source-inline">4x4</strong> matrix on the left. The average of the numbers in the upper-left quadrant is <strong class="source-inline">13</strong>. This would be the average pooling value. The same upper-left quadrant would output <strong class="source-inline">20</strong> to its feature map if it were max pooled because <strong class="source-inline">20</strong> is the largest value within the filter frame. This is a comparison between max pooling and average pooling with hyperparameters, with the <strong class="source-inline">filter</strong> and <strong class="source-inline">stride</strong> parameters both set to <strong class="source-inline">2</strong>:</p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/B16341_07_07.jpg" alt="Figure 7.7: Max versus average pooling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7: Max versus average pooling</p>
			<p>For average pooling, you would use <strong class="source-inline">AveragePooling2D</strong> in place of <strong class="source-inline">MaxPool2D</strong>.</p>
			<p>To implement the average pooling code, you could use the following:</p>
			<p class="source-code">layers.AveragePooling2D(pool_size=(2, 2), strides=None, \</p>
			<p class="source-code">                        padding='valid')</p>
			<p><strong class="bold">AveragePooling2D</strong>: The preceding code snippet demonstrates how to invoke an <strong class="source-inline">AveragePooling2D</strong> layer. In a similar manner to max pooling, the <strong class="source-inline">pool_size</strong>, <strong class="source-inline">strides</strong>, and <strong class="source-inline">padding</strong> parameters can be modified. The following code snippet demonstrates its use within a CNN:</p>
			<p class="source-code">image_shape = (300, 300, 3)</p>
			<p class="source-code">our_first_model = models.Sequential([</p>
			<p class="source-code">    layers.Conv2D(filters = 16, kernel_size = (3,3), \</p>
			<p class="source-code">                  input_shape = image_shape), \</p>
			<p class="source-code">    layers.Activation('relu'), \</p>
			<p class="source-code">    layers.AveragePooling2D(pool_size = (2, 2)), \</p>
			<p class="source-code">    layers.Conv2D(filters = 32, kernel_size = (3,3)), \</p>
			<p class="source-code">    layers.Activation('relu')])</p>
			<p>It's a good idea to keep in mind the benefits of using pooling layers. One of these benefits is that if you down-sample the image, the <em class="italic">image shrinks</em>. This means that you have <em class="italic">less data to process</em> and fewer multiplications to do, which, of course, speeds things up.</p>
			<p>Up to this point, you've created your first CNN layer and learned how to use pooling layers. Now you'll use what you've learned so far to build a pooling layer for the CNN in the following exercise.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor163"/>Exercise 7.02: Creating a Pooling Layer for a CNN</h2>
			<p>You receive an email from your potential employer for the TensorFlow freelancing job that you applied for in <em class="italic">Exercise 7.01</em>, <em class="italic">Creating the First Layer to Build a CNN</em>. The email asks whether you can show how you would code a pooling layer for a CNN. In this exercise, you will build your base model by adding a pooling layer, as requested by your potential employer:</p>
			<ol>
				<li value="1">Open a new Jupyter notebook and import the TensorFlow library:<p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import models, layers</p></li>
				<li>Create your model using <strong class="source-inline">models.Sequential</strong>. The first layer, <strong class="source-inline">Conv2D</strong>, will require the number of nodes, the filter size, and the shape of the tensor, as in the previous exercise. It will be followed by an activation layer, a node at the end of the neural network:<p class="source-code">image_shape = (300, 300, 3)</p><p class="source-code">our_first_model = models.Sequential([</p><p class="source-code">    layers.Conv2D(filters = 16, kernel_size = (3,3), \</p><p class="source-code">                  input_shape = image_shape), \</p><p class="source-code">    layers.Activation('relu')])</p></li>
				<li>Now, add a <strong class="source-inline">MaxPool2D</strong> layer by using the model's <strong class="source-inline">add</strong> method:<p class="source-code">our_first_model.add(layers.MaxPool2D(pool_size = (2, 2))</p><p>In this model, you have created a CNN with a convolutional layer, followed by a ReLU activation function then a max pooling layer. The models take images of size <strong class="source-inline">300x300</strong> with three color channels.</p></li>
			</ol>
			<p>Now that you have successfully added a <strong class="source-inline">MaxPool2D</strong> layer to your CNN, the next step is to add a <strong class="bold">flattening layer</strong> so that your model can use all the data.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor164"/>Flattening Layer</h2>
			<p>Adding a flattening layer is an important step as you will need to provide the neural network with data in a form that it can process. Remember that after you perform the convolution operation, it will still be multi-dimensional. So, to change your data back into one-dimensional form, you will use a flattening layer. To achieve this, you take the pooled feature map and flatten it into a column, as shown in the following figure. In <em class="italic">Figure 7.8</em>, you can see that you start with the input matrix on the left side of the diagram, use a final pooled feature map, and stretch it out into a single column vector:</p>
			<div>
				<div id="_idContainer265" class="IMG---Figure">
					<img src="image/B16341_07_08.jpg" alt="Figure 7.8: Flattening layer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8: Flattening layer</p>
			<p>The following is an implemented flattening layer:</p>
			<p class="source-code">image_shape = (300, 300, 3)</p>
			<p class="source-code">our_first_model = models.Sequential([</p>
			<p class="source-code">    layers.Conv2D(filters = 16, kernel_size = (3,3), \</p>
			<p class="source-code">                  input_shape = image_shape), \</p>
			<p class="source-code">    layers.Activation('relu'), \</p>
			<p class="source-code">    layers.MaxPool2D(pool_size = (2, 2)), \</p>
			<p class="source-code">    layers.Conv2D(filters = 32, kernel_size = (3,3)), \</p>
			<p class="source-code">    layers.Activation('relu'), \</p>
			<p class="source-code">    layers.MaxPool2D(pool_size = (2, 2)), \</p>
			<p class="source-code">    layers.Flatten()])</p>
			<p>Here, a flatten layer is added as the final layer to this model. Now that you've created your first CNN and pooling layers, you will put all the pieces together and build a CNN in the upcoming exercise.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor165"/>Exercise 7.03: Building a CNN</h2>
			<p>You were hired as a freelancer from your work in <em class="italic">Exercise 7.01</em>, <em class="italic">Creating the First Layer to Build a CNN</em>, and <em class="italic">Exercise 7.02</em>, <em class="italic">Creating a Pooling Layer for a CNN</em>. Now that you've got the job, your first assignment is to help your start-up company build its prototype product to show to investors and raise capital. The company is trying to develop a horse or human classifier app, and they want you to get started right away. They tell you that they just need the classifier to work for now and that there will be room for improvements on it soon.</p>
			<p>In this exercise, you will build a convolutional base layer for your model using the <strong class="source-inline">horses_or_humans</strong> dataset. In this dataset, the images aren't centered. The target images are displayed at all angles and at different positions in the frame. You will continue to build on this foundation throughout the chapter, adding to it piece by piece.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be downloaded using the <strong class="source-inline">tensorflow_datasets</strong> package.</p>
			<ol>
				<li value="1">Import all the necessary libraries:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import matplotlib.image as mpimg</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">from tensorflow.keras import models, layers</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p><p class="source-code">from keras_preprocessing import image as kimage</p><p>First, you need to import the TensorFlow library. You will use <strong class="source-inline">tensorflow_datasets</strong> to load your dataset, <strong class="source-inline">tensorflow.keras.models</strong> to build a sequential TensorFlow model, <strong class="source-inline">tensorflow.keras.layers</strong> to add layers to your CNN model, <strong class="source-inline">RMSprop</strong> as your optimizer, and <strong class="source-inline">matplotlib.pyplot</strong> and <strong class="source-inline">matplotlib.image</strong> for some quick visualizations.</p></li>
				<li>Load your dataset from the <strong class="source-inline">tensorflow_datasets</strong> package:<p class="source-code">(our_train_dataset, our_test_dataset), \</p><p class="source-code">dataset_info = tfds.load('horses_or_humans',\</p><p class="source-code">                         split = ['train', 'test'],\</p><p class="source-code">                         data_dir = 'content/',\</p><p class="source-code">                         shuffle_files = True,\</p><p class="source-code">                         with_info = True)</p><p class="source-code">assert isinstance(our_train_dataset, tf.data.Dataset)</p><p>Here, you used the <strong class="source-inline">tensorflow_datasets</strong> package imported as <strong class="source-inline">tfds</strong>. You used the <strong class="source-inline">tfds.load()</strong> function to load the <strong class="source-inline">horses_or_humans</strong> dataset. It is a binary image classification dataset with two classes: horses and humans.</p><p class="callout-heading">Note</p><p class="callout">More information on the dataset can be found at <a href="https://laurencemoroney.com/datasets.html">https://laurencemoroney.com/datasets.html</a>.</p><p class="callout">More information on the <strong class="source-inline">tensorflow_datasets</strong> package can be found at <a href="https://www.tensorflow.org/datasets">https://www.tensorflow.org/datasets</a>. </p><p>The <strong class="source-inline">split = ['train', 'test']</strong> argument specifies which split of the data you want to load. In this example, you are loading the train and test splits into <strong class="source-inline">our_train_dataset</strong> and <strong class="source-inline">our_test_dataset</strong>, respectively. Specify <strong class="source-inline">with_info = True</strong> to load the metadata about the dataset into the <strong class="source-inline">dataset_info</strong> variable. After loading, use <strong class="source-inline">assert</strong> to make sure that the loaded dataset is an instance of the <strong class="source-inline">tf.data.Dataset</strong> object class.</p></li>
				<li>View information about the dataset using the loaded metadata in <strong class="source-inline">dataset_info</strong>:<p class="source-code">image_shape = dataset_info.features["image"].shape</p><p class="source-code">print(f'Shape of Images in the Dataset: \t{image_shape}')</p><p class="source-code">print(f'Number of Classes in the Dataset: \</p><p class="source-code">      \t{dataset_info.features["label"].num_classes}')</p><p class="source-code">names_of_classes = dataset_info.features["label"].names</p><p class="source-code">for name in names_of_classes:</p><p class="source-code">    print(f'Label for class "{name}": \</p><p class="source-code">          \t\t{dataset_info.features["label"].str2int(name)}')</p><p>You should get the following output:</p><div id="_idContainer266" class="IMG---Figure"><img src="image/B16341_07_09.jpg" alt="Figure 7.9: horses_or_humans dataset information&#13;&#10;"/></div><p class="figure-caption">Figure 7.9: horses_or_humans dataset information</p></li>
				<li>Now, view the number of images in the dataset and its distribution of classes:<p class="source-code">print(f'T<a id="_idTextAnchor166"/>otal examples in Train Dataset: \</p><p class="source-code">      \t{len(our_train_dataset)}')</p><p class="source-code">pos_tr_samples = sum(i['label'] for i in our_train_dataset)</p><p class="source-code">print(f'Horses in Train Dataset: \t\t{len(our_train_dataset) \</p><p class="source-code">                                      - pos_tr_samples}')</p><p class="source-code">print(f'Humans in Train Dataset: \t\t{pos_tr_samples}')</p><p class="source-code">print(f'\nTotal examples in Test Dataset: \</p><p class="source-code">      \t{len(our_test_dataset)}')</p><p class="source-code">pos_ts_samples = sum(i['label'] for i in our_test_dataset)</p><p class="source-code">print(f'Horses in Test Dataset: \t\t{len(our_test_dataset) \</p><p class="source-code">                                     - pos_ts_samples}')</p><p class="source-code">print(f'Humans in Test Dataset: \t\t{pos_ts_samples}') </p><p>You should get the following output:</p><div id="_idContainer267" class="IMG---Figure"><img src="image/B16341_07_10.jpg" alt="Figure 7.10: horses_or_humans dataset distribution&#13;&#10;"/></div><p class="figure-caption">Figure 7.10: horses_or_humans dataset distribution</p></li>
				<li>Now, view some sample images in the training dataset, using the <strong class="source-inline">tfds.show_examples()</strong> function:<p class="source-code">fig = tfds<a id="_idTextAnchor167"/>.show_examples(our_train_dataset, dataset_info)</p><p>This function is for interactive use, and it displays and returns a plot of images from the training dataset. </p><p>Your output should be something like the following:</p><div id="_idContainer268" class="IMG---Figure"><img src="image/B16341_07_11.jpg" alt="Figure 7.11: Sample training images&#13;&#10;"/></div><p class="figure-caption">Figure 7.11: Sample training images</p></li>
				<li>View some sample images in the test dataset:<p class="source-code">fig = tfds.show_examples(our_test_dataset, dataset_info)</p><p>You will get the following output:</p><div id="_idContainer269" class="IMG---Figure"><img src="image/B16341_07_12.jpg" alt="Figure 7.12: Sample test images&#13;&#10;"/></div><p class="figure-caption">Figure 7.12: Sample test images</p></li>
				<li>Finally, create your model with <strong class="source-inline">our_model = models.Sequential</strong>. Set up the first <strong class="source-inline">Conv2D</strong> layer and set <strong class="source-inline">filters</strong> to <strong class="source-inline">16</strong>. The kernel is <strong class="source-inline">3x3</strong>. Use ReLU activation. Because this is the first convolutional layer, you also need to set <strong class="source-inline">input_shape</strong> to <strong class="source-inline">image_shape</strong>, the dimensions of the color images you're working with. Now, add the <strong class="source-inline">MaxPool2D</strong> pooling layer. Then, add another <strong class="source-inline">Conv2D</strong> and <strong class="source-inline">MaxPool2D</strong> pair for more model depth, followed by the flatten and dense layers:<p class="source-code">our_cnn_model = models.Sequential([</p><p class="source-code">    layers.Conv2D(filters = 16, kernel_size = (3,3), \</p><p class="source-code">                  input_shape = image_shape),\</p><p class="source-code">    layers.Activation('relu'),\</p><p class="source-code">    layers.MaxPool2D(pool_size = (2, 2)),\</p><p class="source-code">    layers.Conv2D(filters = 32, kernel_size = (3,3)),\</p><p class="source-code">    layers.Activation('relu'),\</p><p class="source-code">    layers.MaxPool2D(pool_size = (2, 2)),\</p><p class="source-code">    layers.Flatten(),\</p><p class="source-code">    layers.Dense(units = 512),\</p><p class="source-code">    layers.Activation('relu'),\</p><p class="source-code">    layers.Dense(units = 1),\</p><p class="source-code">    layers.Activation('sigmoid')</p><p class="source-code">])</p></li>
				<li>Compile the model with <strong class="source-inline">RMSProp</strong> for <strong class="source-inline">optimizer</strong> set to the recommended default of <strong class="source-inline">0.001</strong>, <strong class="source-inline">loss</strong> as <strong class="source-inline">binary_crossentropy</strong>, and <strong class="source-inline">metrics</strong> set to <strong class="source-inline">acc</strong> for accuracy. Print the model summary using the <strong class="source-inline">summary()</strong> method:<p class="source-code">our_cnn_model.compile(optimizer=RMSprop(learning_rate=0.001), \</p><p class="source-code">                      loss='binary_crossentropy',\</p><p class="source-code">                      metrics=['acc'], loss_weights=None,\</p><p class="source-code">                      weighted_metrics=None, run_eagerly=None,\</p><p class="source-code">                      steps_per_execution=None)</p><p class="source-code">print(our_cnn_model.summary())</p><p>This will print the model summary with details on the layer type, output shape, and parameters:</p><div id="_idContainer270" class="IMG---Figure"><img src="image/B16341_07_13.jpg" alt="Figure 7.13: Model summary&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.13: Model summary</p>
			<p>In the preceding screenshot, you can see that there are layers and types listed on the left side. The layers are listed in order from first to last, top to bottom. The output shape is shown in the middle. There are several parameters for each layer listed alongside the assigned layer. At the bottom, you'll see a count of the total parameters, trainable parameters, and non-trainable parameters.</p>
			<p>You've been able to explore the convolutional layer and pooling layers quite a bit. Let's now dive into another important component when using image data: image augmentation.</p>
			<h1 id="_idParaDest-145"><a id="_idTextAnchor168"/>Image Augmentation</h1>
			<p>Augmentation is defined as making something better by making it greater in size or amount. This is exactly what data or image augmentation does. You use augmentation to provide the model with more versions of your image training data. Remember that the more data you have, the better the model's performance will be. By <em class="italic">augmenting</em> your data, you can transform your images in a way that makes the model generalize better on real data. To do this, you <em class="italic">transform</em> the images that you have at your disposal so that you can use your augmented images alongside your original image dataset to train with a greater variation and variety than you would have otherwise. This improves results and prevents overfitting. Take a look at the following three images:</p>
			<div>
				<div id="_idContainer271" class="IMG---Figure">
					<img src="image/B16341_07_14.jpg" alt="Figure 7.14: Augmented leopard images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.14: Augmented leopard images</p>
			<p>It's clear that this is the same leopard in all three images. They're just in different positions. Neural networks can still make sense of this due to convolution. However, with the use of image augmentation, you can improve the model's ability to learn <strong class="bold">translational invariance</strong>.</p>
			<p>Unlike most other types of data with images, you can shift, rotate, and move the images around to make variations of the original image. This creates more data, and with CNNs, more data and data variation will create a better-performing model. To be able to create these image augmentations, take a look at how you would do this in TensorFlow with the loaded <strong class="source-inline">tf.data.Dataset</strong> object. You will use the <strong class="source-inline">dataset.map()</strong> function to map preprocessing image augmentation functions to your dataset, that is, <strong class="source-inline">our_train_dataset</strong>:</p>
			<p class="source-code">from tensorflow import image as tfimage</p>
			<p class="source-code">from tensorflow.keras.preprocessing import image as kimage</p>
			<p>You will use the <strong class="source-inline">tensorflow.image</strong> and <strong class="source-inline">tensorflow.keras.preprocessing.image</strong> packages for this purpose. These packages have a lot of image manipulation functions that can be used for image data augmentation:</p>
			<p class="source-code">augment_dataset(image, label):</p>
			<p class="source-code">    image = kimage.random_shift(image, wrg = 0.1, hrg = 0.1)</p>
			<p class="source-code">    image = tfimage.random_flip_left_right(image)</p>
			<p class="source-code">    return image, label</p>
			<p>Additional functions include the following:</p>
			<ul>
				<li><strong class="source-inline">kimage.random_rotation</strong>: This function allows you to rotate an image randomly between specified degrees.</li>
				<li><strong class="source-inline">kimage.random_brightness</strong>: This function randomly adjusts the brightness level.</li>
				<li><strong class="source-inline">kimage.random_shear</strong>: This function applies shear transformations.</li>
				<li><strong class="source-inline">kimage.random_zoom</strong>: This function randomly zooms images.</li>
				<li><strong class="source-inline">tfimage.random_flip_left_right</strong>: This function randomly flips images horizontally.</li>
				<li><strong class="source-inline">tfimage.random_flip_up_down</strong>: This function randomly flips images vertically.</li>
			</ul>
			<p>In the next step, you will pass in the data that you want to augment with the <strong class="source-inline">tf.data.Dataset.map()</strong> function:</p>
			<p class="source-code">augment_dataset(image, label):</p>
			<p class="source-code">    image = kimage.random_shift(image, wrg = 0.1, hrg = 0.1)</p>
			<p class="source-code">    image = tfimage.random_flip_left_right(image)</p>
			<p class="source-code">    return image, label    </p>
			<p class="source-code">our_train_dataset = our_train_dataset.map(augment_dataset)</p>
			<p class="source-code">model.fit(our_train_dataset,\</p>
			<p class="source-code">          epochs=50,\</p>
			<p class="source-code">          validation_data=our_test_dataset)</p>
			<p>In the preceding code block, with <strong class="source-inline">fit()</strong>, you just need to pass the generator that you have already created. You need to pass in the <strong class="source-inline">epochs</strong> value. If you don't do this, the generator will never stop. The <strong class="source-inline">fit()</strong> function returns the history (plots loss per iteration and so on).</p>
			<p>You need some more functions to add to <strong class="source-inline">our_train_dataset</strong> before you can train the model on it. With <strong class="source-inline">batch()</strong> function, you specify how many images per batch you will train. With <strong class="source-inline">cache()</strong> function, you fit your dataset in memory to improve performance. With <strong class="source-inline">shuffle()</strong> function, you set the shuffle buffer of your dataset to the entire length of the dataset, for true randomness. <strong class="source-inline">prefetch()</strong> function is also used for good performance:</p>
			<p class="source-code">our_train_dataset = our_train_dataset.cache()</p>
			<p class="source-code">our_train_dataset = our_train_dataset.map(augment_dataset)</p>
			<p class="source-code">our_train_dataset = our_train_dataset.shuffle\</p>
			<p class="source-code">                    (len(our_train_dataset))</p>
			<p class="source-code">our_train_dataset = our_train_dataset.batch(128)</p>
			<p class="source-code">our_train_dataset = our_train_dataset.prefetch\</p>
			<p class="source-code">                    (tf.data.experimental.AUTOTUNE)</p>
			<p>Now that you've seen how you would implement augmentation in your training model, take a closer look at what some of those transformations are doing.</p>
			<p>Here's an example of <strong class="source-inline">random_rotation</strong>, <strong class="source-inline">random_shift</strong>, and <strong class="source-inline">random_brightnes</strong> implementation. Use the following code to randomly rotate an image up to an assigned value:</p>
			<p class="source-code">image = kimage.random_rotation(image, rg = 135)</p>
			<p>In <em class="italic">Figure 7.15</em>, you can see the outcome of <strong class="source-inline">random_rotation</strong>. </p>
			<div>
				<div id="_idContainer272" class="IMG---Figure">
					<img src="image/B16341_07_15.jpg" alt="Figure 7.15: Rotation range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.15: Rotation range</p>
			<p>The images were randomly rotated up to 135 degrees.</p>
			<p><strong class="source-inline">random_shift</strong> is used to randomly shift the pixels width-wise. Notice the <strong class="source-inline">.15</strong> in the following code, which means the image can be randomly shifted up to 15 pixels:</p>
			<p class="source-code">image = kimage.random_shift(image, wrg = 0.15, hrg = 0) </p>
			<p>The following figure shows the random adjustment of an image's width by up to 15 pixels:</p>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="image/B16341_07_16.jpg" alt="Figure 7.16: Width shift range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.16: Width shift range</p>
			<p>Again, <strong class="source-inline">random_shift</strong> is used here, which randomly adjusts the height by 15 pixels:</p>
			<p class="source-code">image = kimage.random_shift(image, wrg = 0, hrg = 0.15)</p>
			<p><em class="italic">Figure 7.17</em> shows the random adjustment of an image's height by up to 15 pixels:</p>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<img src="image/B16341_07_17.jpg" alt="Figure 7.17: Height shift range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.17: Height shift range</p>
			<p>For random brightness levels using <strong class="source-inline">random_brightness</strong>, you will use a float value range to lighten or darken the image by percentage. Anything below <strong class="source-inline">1.0</strong> will darken the image. So, in this example, the images are being darkened randomly between 10% and 90%:</p>
			<p class="source-code">image = kimage.random_brightness(image, brightness_range=(0.1,0.9))</p>
			<p>In the following figure, you've adjusted the brightness with <strong class="source-inline">random_brightness</strong>:</p>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<img src="image/B16341_07_18.jpg" alt="Figure 7.18: Brightness range&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.18: Brightness range</p>
			<p>Now that you've been exposed to some of the image augmentation options, take a look at how you can use batch normalization to drive performance improvement in models.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor169"/>Batch Normalization</h2>
			<p>In 2015, <strong class="bold">batch normalization</strong>, also called <strong class="bold">batch norm</strong>, was introduced by <em class="italic">Christian Szegedy</em> and <em class="italic">Sergey Ioffe</em>. Batch norm is a technique that reduces the number of training epochs to improve performance. Batch norm standardizes the inputs for a mini-batch and "normalizes" the input layer. It is most commonly used following a convolutional layer, as shown in the following figure:</p>
			<div>
				<div id="_idContainer276" class="IMG---Figure">
					<img src="image/B16341_07_19.jpg" alt="Figure 7.19: Batch norm&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.19: Batch norm</p>
			<p>The following figure shows one common way that batch normalization is implemented. In the following example, you can see that you have a batch norm layer following a convolutional layer three times. Then you have a flattening layer, followed by two dense layers:</p>
			<div>
				<div id="_idContainer277" class="IMG---Figure">
					<img src="image/B16341_07_20.jpg" alt="Figure 7.20: Layer sequences&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.20: Layer sequences</p>
			<p>Batch norm helps the model generalize better. With each batch that batch norm trains, the model has a different mean and standard deviation. Because the batch means and standard deviations each vary slightly from the true overall mean and standard deviation, these changes act as noise that you are training with, making the model perform better overall.</p>
			<p>The following is an example of <strong class="source-inline">BatchNormalization</strong> implementation. You can simply add a batch norm layer, followed by an activation layer:</p>
			<p class="source-code">model.add(layers.Conv2D(filters = 64, kernel_size = (3, 3), use_bias=False))</p>
			<p class="source-code">model.add(layers.BatchNormalization())</p>
			<p class="source-code">model.add(layers.Activation("relu"))</p>
			<p>So far, you've created a CNN model and learned how to utilize image augmentation. Now you will bring everything together and build a CNN with some additional convolutional layers in the following exercise.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor170"/>Exercise 7.04: Building a CNN with Additional Convolutional Layers</h2>
			<p>Your new employers were happy with what you were able to make in <em class="italic">Exercise 7.03</em>, <em class="italic">Building a CNN</em>. Now that the <strong class="bold">Minimal Viable Product</strong> (<strong class="bold">MVP</strong>), or prototype, is complete, it's time to build a better model.</p>
			<p>In this exercise, you will add additional ANN layers to your model. You will be adding additional layers to your convolutional base layer that you created earlier. You will be using the <strong class="source-inline">horses_or_humans</strong> dataset again.</p>
			<p>Let's get started.</p>
			<p>Because you're expanding on <em class="italic">Exercise 7.03</em>, <em class="italic">Building a CNN</em>, and using the same data, begin from where you left off with the last step in the previous exercise:</p>
			<ol>
				<li value="1">Create a function to rescale the images then apply the function to the train and test datasets using the <strong class="source-inline">map</strong> method. Continue building your train and test dataset pipelines using the <strong class="source-inline">cache</strong>, <strong class="source-inline">shuffle</strong>, <strong class="source-inline">batch</strong>, and <strong class="source-inline">prefetch</strong> methods of the dataset:<p class="source-code">normalization_layer = layers.Rescaling(1./255)</p><p class="source-code">our_train_dataset = our_train_dataset.map\</p><p class="source-code">                    (lambda x: (normalization_layer(x['image']), \</p><p class="source-code">                                                    x['label']), \</p><p class="source-code">                     num_parallel_calls = \</p><p class="source-code">                     tf.data.experimental.AUTOTUNE)</p><p class="source-code">our_train_dataset = our_train_dataset.cache()</p><p class="source-code">our_train_dataset = our_train_dataset.shuffle\</p><p class="source-code">                    (len(our_train_dataset))</p><p class="source-code">our_train_dataset = our_train_dataset.batch(128)</p><p class="source-code">our_train_dataset = \</p><p class="source-code">our_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)</p><p class="source-code">our_test_dataset = our_test_dataset.map\</p><p class="source-code">                   (lambda x: (normalization_layer(x['image']), \</p><p class="source-code">                                                   x['label']),\</p><p class="source-code">                    num_parallel_calls = \</p><p class="source-code">                    tf.data.experimental.AUTOTUNE)</p><p class="source-code">our_test_dataset = our_test_dataset.cache()</p><p class="source-code">our_test_dataset = our_test_dataset.batch(32)</p><p class="source-code">our_test_dataset = our_test_dataset.prefetch\</p><p class="source-code">                   (tf.data.experimental.AUTOTUNE)</p></li>
				<li>Fit the model. Specify the values of <strong class="source-inline">epochs</strong> and <strong class="source-inline">validation_steps</strong> and set <strong class="source-inline">verbose</strong> equal to <strong class="source-inline">1</strong>:<p class="source-code">history = our_cnn_model.fit\</p><p class="source-code">          (our_train_dataset, \</p><p class="source-code">          validation_data = our_test_dataset, \</p><p class="source-code">          epochs=15, \</p><p class="source-code">          validation_steps=8, \</p><p class="source-code">          verbose=1)</p><p>The output looks like this:</p><div id="_idContainer278" class="IMG---Figure"><img src="image/B16341_07_21.jpg" alt="Figure 7.21: Model fitting process&#13;&#10;"/></div><p class="figure-caption">Figure 7.21: Model fitting process</p></li>
				<li>Take a batch from the test dataset and plot the first image from the batch. Convert the image to an array, then use the model to predict what the image shows:<p class="source-code">from matplotlib.pyplot import imshow</p><p class="source-code">for images, lables in our_test_dataset.take(1):</p><p class="source-code">    imshow(np.asarray(images[0]))</p><p class="source-code">    image_to_test = kimage.img_to_array(images[0])</p><p class="source-code">    image_to_test = np.array([image_to_test])</p><p class="source-code">    prediction = our_cnn_model.predict(image_to_test)</p><p class="source-code">    print(prediction)</p><p class="source-code">    if prediction &gt; 0.5:</p><p class="source-code">        print("Image is a human")</p><p class="source-code">        else:</p><p class="source-code">        print("Image is a horse")</p><p>The output will have the following details:</p><div id="_idContainer279" class="IMG---Figure"><img src="image/B16341_07_22.jpg" alt="Figure 7.22: Output of image test with its metadata&#13;&#10;"/></div><p class="figure-caption">Figure 7.22: Output of image test with its metadata</p><p>For prediction, you have a picture of a person from the test set to see what the classification would be. </p></li>
				<li>Take a look at what's happening with each successive layer. Do this by creating a list containing all names of the layers within the CNN and another list containing predictions on a random sample from each of the layers in the list created previously. Next, iterate through the list of names of the layers and their respective predictions and plot the features:<p class="source-code">layer_outputs = []</p><p class="source-code">for layer in our_cnn_model.layers[1:]:</p><p class="source-code">    layer_outputs.append(layer.output)</p><p class="source-code">layer_names = []</p><p class="source-code">for layer in our_cnn_model.layers:</p><p class="source-code">    layer_names.append(layer.name)</p><p class="source-code">features_model = models.Model(inputs = our_cnn_model.input, \</p><p class="source-code">                              outputs = layer_outputs)</p><p class="source-code">random_sample = our_train_dataset.take(1)</p><p class="source-code">layer_predictions = features_model.predict(random_sample)</p><p class="source-code">for layer_name, prediction in zip(layer_names, \</p><p class="source-code">                                  layer_predictions):</p><p class="source-code">    if len(prediction.shape) != 4:</p><p class="source-code">        continue</p><p class="source-code">    num_features = prediction.shape[-1]</p><p class="source-code">    size = prediction.shape[1]</p><p class="source-code">    grid = np.zeros((size, size * num_features))</p><p class="source-code">    for i in range(num_features):</p><p class="source-code">        img = prediction[0, :, :, i]</p><p class="source-code">        img = ((((img - img.mean()) / img.std()) * 64) + 128)</p><p class="source-code">        img = np.clip(img, 0, 255).astype('uint8')</p><p class="source-code">        grid[:, i * size : (i + 1) * size] = img</p><p class="source-code">    scale = 20. / num_features</p><p class="source-code">    plt.figure(figsize=(scale * num_features, scale))</p><p class="source-code">    plt.title(layer_name)</p><p class="source-code">    plt.imshow(grid)</p><p>You should get something like the following:</p><div id="_idContainer280" class="IMG---Figure"><img src="image/B16341_07_23.jpg" alt="Figure 7.23: Transformation at different layers&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.23: Transformation at different layers</p>
			<p>Now that you have created your own CNN model and used it to determine whether an image was a horse or a human, you're now going to focus on how you can classify whether an image is or isn't a specific class.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor171"/>Binary Image Classification</h1>
			<p>Binary classification is the simplest approach for classification models as it classifies images into just two categories. In this chapter, we started with the convolutional operation and discussed how you use it as an image transformer. Then, you learned what a pooling layer does and the differences between max and average pooling. Next, we also looked at how a flattening layer converts a pooled feature map into a single column. Then, you learned how and why to use image augmentation, and how to use batch normalization. These are the key components that differentiate CNNs from other ANNs. </p>
			<p>After convolutional base layers, pooling, and normalization layers, CNNs are often structured like many ANNs you've built thus far, with a series of one or more dense layers. Much like other binary classifiers, binary image classifiers terminate with a dense layer with one unit and a sigmoid activation function. To provide more utility, image classifiers can be outfitted to classify more than two objects. Such classifiers are known generally as object classifiers, which you will learn about in the next section.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor172"/>Object Classification</h1>
			<p>In this section, you will learn about object detection and classification. The next step involves image classification for a dataset with more than two classes. The three different types of models for object classification we will cover are <strong class="bold">image classification</strong>, <strong class="bold">classification with localization</strong>, and <strong class="bold">detection</strong>:</p>
			<ul>
				<li><strong class="bold">Image classification</strong>: This involves training with a set number of classes and then trying to determine which of those classes is shown in the image. Think of the MNIST handwriting dataset. For these problems, you'll use a traditional CNN.</li>
				<li><strong class="bold">Classification with localization</strong>: With this type, the model tries to predict where the object is in the image space. For these models, you use a simplified <strong class="bold">You Only Look Once</strong> (<strong class="bold">YOLO</strong>) or R-CNN.</li>
				<li><strong class="bold">Detection</strong>: The last type is detection. This is where your model can detect several different objects and where they are located. For this, you use YOLO or an R-CNN:<div id="_idContainer281" class="IMG---Figure"><img src="image/B16341_07_24.jpg" alt="Figure 7.24: Object classification types&#13;&#10;"/></div></li>
			</ul>
			<p class="figure-caption">Figure 7.24: Object classification types</p>
			<p>Now, you'll take a brief look at image classification with the <strong class="source-inline">Fashion-MNIST</strong> dataset. <strong class="source-inline">Fashion-MNIST</strong> was compiled from a dataset of Zalando's article images. Zalando is a fashion-focused e-commerce company based in Berlin, Germany. The dataset consists of 10 classes with a training set of 60,000 <strong class="source-inline">28x28</strong> grayscale images and 10,000 test images.</p>
			<ol>
				<li value="1">Import TensorFlow:<p class="source-code">import tensorflow as tf</p></li>
				<li>Next, make some additional imports, such as for NumPy, Matplotlib, and of course, layers and models. You'll notice here that you will be using additional dropout layers. If you recall, dropout layers help prevent overfitting:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, \</p><p class="source-code">    Dropout, GlobalMaxPooling2D, Activation, Rescaling</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay</p><p class="source-code">import itertools</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Load the <strong class="source-inline">Fashion-MNIST</strong> dataset using <strong class="source-inline">tdfs</strong> in any one of the datasets that they have decided to include. Others include <strong class="source-inline">CIFAR-10</strong> and <strong class="source-inline">CIFAR-100</strong>, just to name a couple:<p class="source-code">(our_train_dataset, our_test_dataset), \</p><p class="source-code">dataset_info = tfds.load(\</p><p class="source-code">                         'fashion_mnist'</p><p class="source-code">                          , split = ['train', 'test']</p><p class="source-code">                          , data_dir = 'content/FashionMNIST/'</p><p class="source-code">                          , shuffle_files = True</p><p class="source-code">                          , as_supervised = True</p><p class="source-code">                          , with_info = True)</p><p class="source-code">assert isinstance(our_train_dataset, tf.data.Dataset)</p></li>
				<li>Check the data for its properties:<p class="source-code">image_shape = dataset_info.features["image"].shape</p><p class="source-code">print(f'Shape of Images in the Dataset: \t{image_shape}')</p><p class="source-code">num_classes = dataset_info.features["label"].num_classes</p><p class="source-code">print(f'Number of Classes in the Dataset: \t{num_classes}')</p><p class="source-code">names_of_classes = dataset_info.features["label"].names</p><p class="source-code">print(f'Names of Classes in the Dataset: \t{names_of_classes}\n')</p><p class="source-code">for name in names_of_classes:</p><p class="source-code">    print(f'Label for class \</p><p class="source-code">          "{name}":  \t\t{dataset_info.features["label"].\</p><p class="source-code">          str2int(name)}')</p><p>This will give you the following output:</p><div id="_idContainer282" class="IMG---Figure"><img src="image/B16341_07_25.jpg" alt="Figure 7.25: Details of properties for data&#13;&#10;"/></div><p class="figure-caption">Figure 7.25: Details of properties for data</p></li>
				<li>Now, print the total examples of the train and test data:<p class="source-code">print(f'Total examples in Train Dataset: \</p><p class="source-code">      \t{len(our_train_dataset)}')</p><p class="source-code">print(f'Total examples in Test Dataset: \</p><p class="source-code">      \t{len(our_test_dataset)}')</p><p>This will give you the following output:</p><div id="_idContainer283" class="IMG---Figure"><img src="image/B16341_07_26.jpg" alt="Figure 7.26: Details of train and test datasets&#13;&#10;"/></div><p class="figure-caption">Figure 7.26: Details of train and test datasets</p></li>
				<li>Build your model with the functional API:<p class="source-code">input_layer = Input(shape=image_shape)</p><p class="source-code">x = Conv2D(filters = 32, kernel_size = (3, 3), \</p><p class="source-code">           strides=2)(input_layer)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Conv2D(filters = 64, kernel_size = (3, 3), strides=2)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Conv2D(filters = 128, kernel_size = (3, 3), strides=2)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Flatten()(x)</p><p class="source-code">x = Dropout(rate = 0.2)(x)</p><p class="source-code">x = Dense(units = 512)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Dropout(rate = 0.2)(x)</p><p class="source-code">x = Dense(units = num_classes)(x)</p><p class="source-code">output = Activation('softmax')(x)</p><p class="source-code">our_classification_model = Model(input_layer, output)</p></li>
				<li>Compile and fit your model. With <strong class="source-inline">compile()</strong> method, use <strong class="source-inline">adam</strong> as your optimizer, set the loss to <strong class="source-inline">sparse_categorical_crossentropy</strong>, and set the <strong class="source-inline">accuracy</strong> metric. Then, call <strong class="source-inline">model.fit()</strong> on your training and validation sets:<p class="source-code">our_classification_model.compile(</p><p class="source-code">                   optimizer='adam', \</p><p class="source-code">                   loss='sparse_categorical_crossentropy',</p><p class="source-code">                   metrics=['accuracy'], loss_weights=None,</p><p class="source-code">                   weighted_metrics=None, run_eagerly=None,</p><p class="source-code">                   steps_per_execution=None</p><p class="source-code">)</p><p class="source-code">history = our_classification_model.fit(our_train_dataset, validation_data=our_test_dataset, epochs=15)</p><p>This will give the following as output:</p><div id="_idContainer284" class="IMG---Figure"><img src="image/B16341_07_27.jpg" alt="Figure 7.27: Function returning history&#13;&#10;"/></div><p class="figure-caption">Figure 7.27: Function returning history</p></li>
				<li>Use <strong class="source-inline">matplotlib.pyplot</strong> to plot the loss and accuracy:<p class="source-code">def plot_trend_by_epoch(tr_values, val_values, title):</p><p class="source-code">    epoch_number = range(len(tr_values))</p><p class="source-code">    plt.plot(epoch_number, tr_values, 'r')</p><p class="source-code">    plt.plot(epoch_number, val_values, 'b')</p><p class="source-code">    plt.title(title)</p><p class="source-code">    plt.xlabel('epochs')</p><p class="source-code">    plt.legend(['Training '+title, 'Validation '+title])</p><p class="source-code">    plt.figure()</p><p class="source-code">hist_dict = history.history</p><p class="source-code">tr_accuracy, val_accuracy = hist_dict['accuracy'], \</p><p class="source-code">                            hist_dict['val_accuracy']</p><p class="source-code">plot_trend_by_epoch(tr_accuracy, val_accuracy, "Accuracy")</p><p>This will give the following plot as output:</p><div id="_idContainer285" class="IMG---Figure"><img src="image/B16341_07_28.jpg" alt="Figure 7.28: Accuracy plot using matplotlib.pyplot &#13;&#10;"/></div><p class="figure-caption">Figure 7.28: Accuracy plot using matplotlib.pyplot </p></li>
				<li>Plot the validation loss and training loss. Use the following code:<p class="source-code">tr_loss, val_loss = hist_dict['loss'], hist_dict['val_loss']</p><p class="source-code">plot_trend_by_epoch(tr_loss, val_loss, "Loss")</p><p>This will give the following plot as output:</p><div id="_idContainer286" class="IMG---Figure"><img src="image/B16341_07_29.jpg" alt="Figure 7.29: Validation loss and training loss&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.29: Validation loss and training loss</p>
			<p>As you can see from the accuracy and loss curves as a function of epochs, the accuracy increases, and loss decreases. On the validation set, both begin to plateau, which is a good signal to stop training to prevent overfitting to the training dataset.</p>
			<p>In the next exercise, you will build a CNN to classify images into 10 distinct classes from the <strong class="source-inline">CIFAR-10</strong> dataset.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor173"/>Exercise 7.05: Building a CNN</h2>
			<p>The start-up now wants to expand its capabilities and to work with more classes and larger image datasets. Your challenge is to accurately predict the class of an image.</p>
			<p>The dataset you will be using is the <strong class="source-inline">CIFAR-10</strong> dataset, a dataset containing 60,000 <strong class="source-inline">32x32</strong> color images across 10 classes: airplanes, automobiles, birds, cats, deer, dogs, frogs, horses, ships, and trucks. Each class has 6,000 images and the entire dataset contains 50,000 training images and 10,000 test images.</p>
			<p>More info on the dataset can be found at <em class="italic">Learning Multiple Layers of Features from Tiny Images</em> (<a href="http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf">http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf</a>), <em class="italic">Alex Krizhevsky</em>, <em class="italic">2009</em>:</p>
			<ol>
				<li value="1">Start a new Jupyter notebook and import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Import the other additional libraries that are needed:<p class="source-code">import numpy as np</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, \</p><p class="source-code">    Dropout, GlobalMaxPooling2D, Activation, Rescaling</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from sklearn import metrics import confusion_matrix, \</p><p class="source-code">    ConfusionMatrixDisplay</p><p class="source-code">import itertools</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li>Load the <strong class="source-inline">CIFAR-10</strong> dataset directly from <strong class="source-inline">tfds</strong> as follows:<p class="source-code">(our_train_dataset, our_test_dataset), \</p><p class="source-code">dataset_info = tfds.load('cifar10',\</p><p class="source-code">                         split = ['train', 'test'],\</p><p class="source-code">                         data_dir = 'content/Cifar10/',\</p><p class="source-code">                         shuffle_files = True,\</p><p class="source-code">                         as_supervised = True,\</p><p class="source-code">                         with_info = True)</p><p class="source-code">assert isinstance(our_train_dataset, tf.data.Dataset)</p></li>
				<li>Print the properties of your dataset using the following code:<p class="source-code">image_shape = dataset_info.features["image"].shape</p><p class="source-code">print(f'Shape of Images in the Dataset: \t{image_shape}')</p><p class="source-code">num_classes = dataset_info.features["label"].num_classes</p><p class="source-code">print(f'Number of Classes in the Dataset: \t{num_classes}')</p><p class="source-code">names_of_classes = dataset_info.features["label"].names</p><p class="source-code">print(f'Names of Classes in the Dataset: \t{names_of_classes}\n')</p><p class="source-code">for name in names_of_classes:</p><p class="source-code">    print(f'Label for class "{name}": \</p><p class="source-code">          \t\t{dataset_info.features["label"].str2int(name)}')</p><p class="source-code">print(f'Total examples in Train Dataset: \</p><p class="source-code">      \t{len(our_train_dataset)}')</p><p class="source-code">print(f'Total examples in Test Dataset: \</p><p class="source-code">      \t{len(our_test_dataset)}')</p><p>This will give the following output with the properties and the number of classes:</p><div id="_idContainer287" class="IMG---Figure"><img src="image/B16341_07_30.jpg" alt="Figure 7.30: Number of classes&#13;&#10;"/></div><p class="figure-caption">Figure 7.30: Number of classes</p></li>
				<li>Build the train and test data pipelines, as shown in <em class="italic">Exercise 7.03</em>, <em class="italic">Building a CNN</em>:<p class="source-code">normalization_layer = Rescaling(1./255)</p><p class="source-code">our_train_dataset = our_train_dataset.map\</p><p class="source-code">                    (lambda x, y: (normalization_layer(x), y),\</p><p class="source-code">                     num_parallel_calls = \</p><p class="source-code">                     tf.data.experimental.AUTOTUNE)</p><p class="source-code">our_train_dataset = our_train_dataset.cache()</p><p class="source-code">our_train_dataset = our_train_dataset.shuffle\</p><p class="source-code">                    (len(our_train_dataset))</p><p class="source-code">our_train_dataset = our_train_dataset.batch(128)</p><p class="source-code">our_train_dataset = our_train_dataset.prefetch\</p><p class="source-code">                    (tf.data.experimental.AUTOTUNE)</p><p class="source-code">our_test_dataset = our_test_dataset.map\</p><p class="source-code">                   (lambda x, y: (normalization_layer(x), y),\</p><p class="source-code">                    num_parallel_calls = \</p><p class="source-code">                    tf.data.experimental.AUTOTUNE)</p><p class="source-code">our_test_dataset = our_test_dataset.cache()</p><p class="source-code">our_test_dataset = our_test_dataset.batch(1024)</p><p class="source-code">our_test_dataset = our_test_dataset.prefetch\</p><p class="source-code">                   (tf.data.experimental.AUTOTUNE)</p></li>
				<li>Build the model using the functional API. Set the shape, layer types, strides, and activation functions:<p class="source-code">input_layer = Input(shape=image_shape)</p><p class="source-code">x = Conv2D(filters = 32, \</p><p class="source-code">           kernel_size = (3, 3), strides=2)(input_layer)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Conv2D(filters = 64, kernel_size = (3, 3), strides=2)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Conv2D(filters = 128, kernel_size = (3, 3), strides=2)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Flatten()(x)</p><p class="source-code">x = Dropout(rate = 0.5)(x)</p><p class="source-code">x = Dense(units = 1024)(x)</p><p class="source-code">x = Activation('relu')(x)</p><p class="source-code">x = Dropout(rate = 0.2)(x)</p><p class="source-code">x = Dense(units = num_classes)(x)</p><p class="source-code">output = Activation('softmax')(x)</p><p class="source-code">our_classification_model = Model(input_layer, output)</p></li>
				<li>Compile and fit your model. Be sure to use your GPU for this, if possible, as it will speed up the process quite a bit. If you decide not to use the GPU and your machine has difficulty in terms of computation, you can decrease the number of epochs accordingly:<p class="source-code">our_classification_model.compile(</p><p class="source-code">                      optimizer='adam', \</p><p class="source-code">                      loss='sparse_categorical_crossentropy',</p><p class="source-code">                      metrics=['accuracy'], loss_weights=None,</p><p class="source-code">                      weighted_metrics=None, run_eagerly=None,</p><p class="source-code">                      steps_per_execution=None</p><p class="source-code">)</p><p class="source-code">print(our_classification_model.summary())</p><p class="source-code">history = our_classification_model.fit(our_train_dataset, validation_data=our_test_dataset, epochs=15)</p><p>The function will return the following history:</p><div id="_idContainer288" class="IMG---Figure"><img src="image/B16341_07_31.jpg" alt="Figure 7.31: Fitting the model&#13;&#10;"/></div><p class="figure-caption">Figure 7.31: Fitting the model</p></li>
				<li>Get a visual representation of the model's performance by plotting your loss and accuracy per epoch:<p class="source-code">def plot_trend_by_epoch(tr_values, val_values, title):</p><p class="source-code">    epoch_number = range(len(tr_values))</p><p class="source-code">    plt.plot(epoch_number, tr_values, 'r')</p><p class="source-code">    plt.plot(epoch_number, val_values, 'b')</p><p class="source-code">    plt.title(title)</p><p class="source-code">    plt.xlabel('epochs')</p><p class="source-code">    plt.legend(['Training '+title, 'Validation '+title])</p><p class="source-code">    plt.figure()</p><p class="source-code">hist_dict = history.history</p><p class="source-code">tr_loss, val_loss = hist_dict['loss'], hist_dict['val_loss']</p><p class="source-code">plot_trend_by_epoch(tr_loss, val_loss, "Loss")</p><p>This will produce the following plot:</p><div id="_idContainer289" class="IMG---Figure"><img src="image/B16341_07_32.jpg" alt="Figure 7.32: Loss plot&#13;&#10;"/></div><p class="figure-caption">Figure 7.32: Loss plot</p></li>
				<li>Next, get an accuracy plot by using the following code:<p class="source-code">tr_accuracy, val_accuracy = hist_dict['accuracy'], \</p><p class="source-code">                            hist_dict['val_accuracy']</p><p class="source-code">plot_trend_by_epoch(tr_accuracy, val_accuracy, "Accuracy")</p><p>This will give the following plot:</p><div id="_idContainer290" class="IMG---Figure"><img src="image/B16341_07_33.jpg" alt="Figure 7.33: Accuracy plot&#13;&#10;"/></div><p class="figure-caption">Figure 7.33: Accuracy plot</p></li>
				<li>Plot the confusion matrix without normalization: <p class="source-code">test_labels = []</p><p class="source-code">test_images = []</p><p class="source-code">for image, label in tfds.as_numpy(our_test_dataset.unbatch()):</p><p class="source-code">    test_images.append(image)</p><p class="source-code">    test_labels.append(label)</p><p class="source-code">test_labels = np.array(test_labels)</p><p class="source-code">predictions = our_classification_model.predict(our_test_dataset).argmax(axis=1)</p><p class="source-code">conf_matrix = confusion_matrix(test_labels, predictions)</p><p class="source-code">disp = ConfusionMatrixDisplay(conf_matrix, \</p><p class="source-code">                              display_labels = names_of_classes)</p><p class="source-code">fig = plt.figure(figsize = (12, 12))</p><p class="source-code">axis = fig.add_subplot(111)</p><p class="source-code">disp.plot(values_format = 'd', ax = axis)</p><p>This will give the following output:</p><div id="_idContainer291" class="IMG---Figure"><img src="image/B16341_07_34.jpg" alt="Figure 7.34: Confusion matrix without normalization&#13;&#10;"/></div><p class="figure-caption">Figure 7.34: Confusion matrix without normalization</p></li>
				<li>Use the following code to plot the confusion matrix with normalization:<p class="source-code">conf_matrix = conf_matrix.astype\</p><p class="source-code">              ('float') / conf_matrix.sum(axis=1) \</p><p class="source-code">              [:, np.newaxis]</p><p class="source-code">disp = ConfusionMatrixDisplay(\</p><p class="source-code">       conf_matrix, display_labels = names_of_classes)</p><p class="source-code">fig = plt.figure(figsize = (12, 12))</p><p class="source-code">axis = fig.add_subplot(111)</p><p class="source-code">disp.plot(ax = axis)</p><p>The output will look like this:</p><div id="_idContainer292" class="IMG---Figure"><img src="image/B16341_07_35.jpg" alt="Figure 7.35: Confusion matrix with normalization&#13;&#10;"/></div><p class="figure-caption">Figure 7.35: Confusion matrix with normalization</p></li>
				<li>Take a look at one of the images that the model got wrong. Plot one of the incorrect predictions with the following code:<p class="source-code">incorrect_predictions = np.where(predictions != test_labels)[0]</p><p class="source-code">index = np.random.choice(incorrect_predictions)</p><p class="source-code">plt.imshow(test_images[index])</p><p class="source-code">print(f'True label: {names_of_classes[test_labels[index]]}')</p><p class="source-code">print(f'Predicted label: {names_of_classes[predictions[index]]}')</p><p>The output will look like this:</p><div id="_idContainer293" class="IMG---Figure"><img src="image/B16341_07_36.jpg" alt="Figure 7.36: True versus predicted results&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.36: True versus predicted results</p>
			<p>You'll notice it says <strong class="source-inline">True label: bird</strong> and <strong class="source-inline">Predicted label: cat</strong>. This means that the model predicted that this image was a cat, but it was a bird. The image is blurry since the resolution is only <strong class="source-inline">32x32</strong>; however, the results are not bad. It would be fair to say that it is difficult for a human to identify whether the image was a dog or a cat.</p>
			<p>Now that you have completed this chapter, it's time to put everything that you've learned to the test with <em class="italic">Activity 7.01</em>, <em class="italic">Building a CNN with More ANN Layers</em>, where you'll be building a CNN with additional ANN layers.</p>
			<h2 id="_idParaDest-151">Activity 7.01: Building a CNN with M<a id="_idTextAnchor174"/>ore ANN Layers</h2>
			<p>The start-up that you've been working for has loved your work so far. They have tasked you with creating a new model that is capable of classifying images from 100 different classes.</p>
			<p>In this activity, you'll be putting everything that you've learned to use as you build your own classifier with <strong class="source-inline">CIFAR-100</strong>. <strong class="source-inline">CIFAR-100</strong> is a more advanced version of the <strong class="source-inline">CIFAR-10</strong> dataset, with 100 classes, and is commonly used for benchmarking performance in machine learning research.</p>
			<ol>
				<li value="1">Start a new Jupyter notebook.</li>
				<li>Import the TensorFlow library.</li>
				<li>Import the additional libraries that you will need, including NumPy, Matplotlib, Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, Activation, Model, confusion_matrix, and itertools.</li>
				<li>Load the <strong class="source-inline">CIFAR-100</strong> dataset directly from <strong class="source-inline">tensorflow_datasets</strong> and view its properties from the metadata, and build a train and test data pipeline:<div id="_idContainer294" class="IMG---Figure"><img src="image/B16341_07_37.jpg" alt="Figure 7.37: Properties of the CIFAR-100 dataset&#13;&#10;"/></div><p class="figure-caption">Figure 7.37: Properties of the CIFAR-100 dataset</p></li>
				<li>Create a function to rescale images. Then, build a test and train data pipeline by rescaling, caching, shuffling, batching, and prefetching the images.</li>
				<li>Build the model using the functional API using <strong class="source-inline">Conv2D</strong> and <strong class="source-inline">Flatten</strong>, among others.</li>
				<li>Compile and fit the model using <strong class="source-inline">model.compile</strong> and <strong class="source-inline">model.fit</strong>:<div id="_idContainer295" class="IMG---Figure"><img src="image/B16341_07_38.jpg" alt="Figure 7.38: Model fitting&#13;&#10;"/></div><p class="figure-caption">Figure 7.38: Model fitting</p></li>
				<li>Plot the loss with <strong class="source-inline">plt.plot</strong>. Remember to use the history collected during the <strong class="source-inline">model.fit()</strong> procedure:<div id="_idContainer296" class="IMG---Figure"><img src="image/B16341_07_39.jpg" alt="Figure 7.39: Loss versus epochs&#13;&#10;"/></div><p class="figure-caption">Figure 7.39: Loss versus epochs</p></li>
				<li>Plot the accuracy with <strong class="source-inline">plt.plot</strong>:<div id="_idContainer297" class="IMG---Figure"><img src="image/B16341_07_40.jpg" alt="Figure 7.40: Accuracy versus epochs&#13;&#10;"/></div><p class="figure-caption">Figure 7.40: Accuracy versus epochs</p></li>
				<li>Specify the labels for the different classes in your dataset.</li>
				<li>Display a misclassified example with <strong class="source-inline">plt.imshow</strong>:<p> </p><div id="_idContainer298" class="IMG---Figure"><img src="image/B16341_07_41.jpg" alt="Figure 7.41: Wrong classification example&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 7.41: Wrong classification example</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor272">this link</a>.</p>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor175"/>Summary</h1>
			<p>This chapter covered CNNs. We reviewed core concepts such as neurons, layers, model architecture, and tensors to understand how to create effective CNNs.</p>
			<p>You learned about the convolution operation and explored kernels and feature maps. We analyzed how to assemble a CNN, and then explored the different types of pooling layers and when to apply them.</p>
			<p>You then learned about the stride operation and how padding is used to create extra space around images if needed. Then, we delved into the flattening layer and how it is able to convert data into a 1D array for the next layer. You put everything that you learned to the test in the final activity, as you were presented with several classification problems, including <strong class="source-inline">CIFAR-10</strong> and even <strong class="source-inline">CIFAR-100</strong>.</p>
			<p>In completing this chapter, you are now well on your way to being able to implement CNNs to confront image classification problems head-on and with confidence.</p>
			<p>In the next chapter, you'll learn about pre-trained models and how to utilize them for your own applications by adding ANN layers on top of the pre-trained model and fine-tuning the weights given your own training data.</p>
		</div>
	</body></html>