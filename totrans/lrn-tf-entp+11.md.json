["```\ngit clone https://github.com/PacktPublishing/learn-tensorflow-enterprise.git\n```", "```\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import tensorflow_datasets as tfds\n    import os\n    import IPython\n    import time\n    from absl import flags\n    from absl import logging\n    from absl import app\n    ```", "```\n    FLAGS = flags.FLAGS\n    # flag name, default value, explanation/help.\n    tf.compat.v1.flags.DEFINE_string('model_dir', 'default_model_dir', 'Directory or bucket for storing checkpoint model.')\n    tf.compat.v1.flags.DEFINE_bool('fine_tuning_choice', False, 'Retrain base parameters')\n    tf.compat.v1.flags.DEFINE_integer('train_batch_size', 32, 'Number of samples in a training batch')\n    tf.compat.v1.flags.DEFINE_integer('validation_batch_size', 40, 'Number of samples in a validation batch')\n    tf.compat.v1.flags.DEFINE_string('distribution_strategy', 'tpu', 'Distribution strategy for training.')\n    tf.compat.v1.flags.DEFINE_integer('train_epochs', 3, 'Number of epochs for training')\n    tf.compat.v1.flags.DEFINE_string('data_dir', 'tf_datasets/flower_photos', 'training data path')\n    tf.compat.v1.flags.DEFINE_integer('num_gpus', 4, 'Number of GPU per worker')\n    tf.compat.v1.flags.DEFINE_string('cache_dir', '../imagenet_resnet_v2_50_feature_vector_4' , 'Location of cached model')\n    ```", "```\n    def model_default():\n        flags_obj = flags.FLAGS\n        os.environ['TFHUB_CACHE_DIR'] = flags_obj.cache_dir\n        IMAGE_SIZE = (224, 224)\n        model = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)), \n        hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4', trainable=flags_obj.fine_tuning_choice),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units = 64, \n                                         activation = 'relu', \n                        kernel_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(5, activation='softmax', \n                                       name = 'custom_class')\n        ])\n        model.build([None, 224, 224, 3])\n         model.compile(\n            optimizer=tf.keras.optimizers.SGD(lr=1e-2, \t                 \n                                               momentum=0.5), \n            loss=tf.keras.losses.CategoricalCrossentropy(\n                      from_logits=True, label_smoothing=0.1),\n            metrics=['accuracy'])\n      return model\n    ```", "```\n    def decode_and_resize(serialized_example):\n        # resized image should be [224, 224, 3] and normalized to value range [0, 255] \n        # label is integer index of class.\n            parsed_features = tf.io.parse_single_example(\n        serialized_example,\n        features = {\n        'image/channels' :  tf.io.FixedLenFeature([],  \t \t                                                tf.int64),\n        'image/class/label' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/text' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/colorspace' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/encoded' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/filename' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/format' : tf.io.FixedLenFeature([], \t \t     \t                                               tf.string),\n        'image/height' : tf.io.FixedLenFeature([], tf.int64),\n        'image/width' : tf.io.FixedLenFeature([], tf.int64)\n        })\n        image = tf.io.decode_jpeg(parsed_features[\n                                'image/encoded'], channels=3)\n        label = tf.cast(parsed_features['image/class/label'],  \n                                                    tf.int32)\n        label_txt = tf.cast(parsed_features[\n                              'image/class/text'], tf.string)\n        label_one_hot = tf.one_hot(label, depth = 5)\n        resized_image = tf.image.resize(image, [224, 224], \n                                             method='nearest')\n        return resized_image, label_one_hot\n    ```", "```\n    def normalize(image, label):\n        #Convert `image` from [0, 255] -> [0, 1.0] floats \n        image = tf.cast(image, tf.float32) / 255.\n        return image, label\n    ```", "```\n    def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n        # This is a small dataset, only load it once, and keep it in memory.\n        # use `.cache(filename)` to cache preprocessing work for datasets that don't\n        # fit in memory.\n        flags_obj = flags.FLAGS\n        if cache:\n            if isinstance(cache, str):\n                ds = ds.cache(cache)\n            else:\n                ds = ds.cache()\n        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n        # Repeat forever\n        ds = ds.repeat()\n        ds = ds.batch(flags_obj.train_batch_size)\n        # `prefetch` lets the dataset fetch batches in the background while the model\n        # is training.\n        AUTOTUNE = tf.data.experimental.AUTOTUNE\n        ds = ds.prefetch(buffer_size=AUTOTUNE)\n    return ds\n    ```", "```\n    def main(_):\n        flags_obj = flags.FLAGS\n           if flags_obj.distribution_strategy == 'tpu':\n            resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n            tf.config.experimental_connect_to_cluster(resolver)\n            tf.tpu.experimental.initialize_tpu_system(resolver)\n            strategy = tf.distribute.experimental.TPUStrategy(resolver)\n            strategy_scope = strategy.scope()\n            print('All devices: ', tf.config.list_logical_devices('TPU'))\n        elif flags_obj.distribution_strategy == 'gpu':\n            strategy = tf.distribute.MirroredStrategy()\n            strategy_scope = strategy.scope()\n            devices = ['device:GPU:%d' % i for i in \n                        range(flags_obj.num_gpus)]\n        else:\n            strategy = tf.distribute.MirroredStrategy()\n            strategy_scope = strategy.scope()\n     print('NUMBER OF DEVICES: ', \n                               strategy.num_replicas_in_sync)\n    ```", "```\n        ## identify data paths and sources\n        root_dir = flags_obj.data_dir # this is gs://<bucket>/folder or file path where tfrecord is found\n        file_pattern = '{}/image_classification_builder-train*.tfrecord*'.format(root_dir)\n        val_file_pattern = '{}/image_classification_builder-validation*.tfrecord*'.format(root_dir)\n        file_list = tf.io.gfile.glob(file_pattern)\n        all_files = tf.data.Dataset.list_files( \n                              tf.io.gfile.glob(file_pattern))\n        val_file_list = tf.io.gfile.glob(val_file_pattern)\n        val_all_files = tf.data.Dataset.list_files( \n                          tf.io.gfile.glob(val_file_pattern))\n        train_all_ds = tf.data.TFRecordDataset(all_files, \n            num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    val_all_ds = tf.data.TFRecordDataset(val_all_files, \n            num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    ```", "```\n        # perform data engineering \n        dataset = train_all_ds.map(decode_and_resize)\n        val_dataset = val_all_ds.map(decode_and_resize)\n        # Create dataset for training run\n        BATCH_SIZE = flags_obj.train_batch_size\n        VALIDATION_BATCH_SIZE = \n                              flags_obj.validation_batch_size\n        dataset = dataset.map(normalize, \n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n        val_dataset = val_dataset.map(normalize, \n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    val_ds = val_dataset.batch(VALIDATION_BATCH_SIZE)\n        AUTOTUNE = tf.data.experimental.AUTOTUNE\n        train_ds = prepare_for_training(dataset)\n    ```", "```\n        NUM_CLASSES = 5\n        IMAGE_SIZE = (224, 224)\n            train_sample_size=0\n        for raw_record in train_all_ds:\n            train_sample_size += 1\n        print('TRAIN_SAMPLE_SIZE = ', train_sample_size)\n        validation_sample_size=0\n        for raw_record in val_all_ds:\n            validation_sample_size += 1\n        print('VALIDATION_SAMPLE_SIZE = ', \n                                      validation_sample_size)\n        STEPS_PER_EPOCHS = train_sample_size // BATCH_SIZE\n    VALIDATION_STEPS = validation_sample_size \n                                     // VALIDATION_BATCH_SIZE\n    ```", "```\n        model = model_default()\n        checkpoint_prefix = os.path.join(flags_obj.model_dir, \n                                        'train_ckpt_{epoch}')\n        callbacks = [\n        tf.keras.callbacks.TensorBoard(log_dir=os.path.join(flags_obj.model_dir, 'tensorboard_logs')),\n        tf.keras.callbacks.ModelCheckpoint(\n                                  filepath=checkpoint_prefix,\n            save_weights_only=True)]\n    ```", "```\n         model.fit(\n            train_ds,\n            epochs=flags_obj.train_epochs, \n            steps_per_epoch=STEPS_PER_EPOCHS,\n            validation_data=val_ds,\n            validation_steps=VALIDATION_STEPS,\n            callbacks=callbacks)\n    ```", "```\n        logging.info('INSIDE MAIN FUNCTION user input model_dir %s', flags_obj.model_dir)\n            timestr = time.strftime('%Y%m%d-%H%M%S')\n        output_folder = flags_obj.model_dir + '-' + timestr\n        if not os.path.exists(output_folder):\n            os.mkdir(output_folder)\n            print('Directory ' , output_folder , ' Created ')\n        else:\n            print('Directory ' , output_folder , ' already exists')   \n        model_save_dir = os.path.join(output_folder, \n                                                'save_model')\n        model.save(model_save_dir)\n    if __name__ == '__main__':\n        app.run(main)\n    ```", "```\npython3 default_trainer.py \\\n```", "```\n--distribution_strategy=default \\\n```", "```\n--fine_tuning_choice=False \\\n```", "```\n--train_batch_size=32 \\\n```", "```\n--validation_batch_size=40 \\\n```", "```\n--train_epochs=5 \\\n```", "```\n--data_dir=tf_datasets/flower_photos \\\n```", "```\n--model_dir=trained_resnet_vector\n```", "```\nimport tensorflow as tf\n```", "```\nimport numpy as np\n```", "```\nimport matplotlib.pyplot as plt\n```", "```\nfrom PIL import Image, ImageOps\n```", "```\nimport IPython.display as display\n```", "```\npath_saved_model = 'trained_resnet_vector-unquantized/save_model'\n```", "```\ntrained_model = tf.saved_model.load(path_saved_model)\n```", "```\nsignature_list = list(trained_model.signatures.keys())\n```", "```\nsignature_list\n```", "```\n['serving_default']\n```", "```\ninfer = trained_model.signatures[signature_list[0]]\n```", "```\nprint(infer.structured_outputs)\n```", "```\n{'custom_class': TensorSpec(shape=(None, 5), dtype=tf.float32, name='custom_class')}\n```", "```\nroot_dir = ' tf_datasets/flower_photos'\n```", "```\ntest_pattern = '{}/image_classification_builder-test.tfrecord*'.format(root_dir)\n```", "```\ntest_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_pattern))\n```", "```\ntest_all_ds = tf.data.TFRecordDataset(test_all_files,\n```", "```\nnum_parallel_reads=tf.data.experimental.AUTOTUNE)\n```", "```\nsample_size = 0\n```", "```\nfor raw_record in test_all_ds:\n```", "```\n    sample_size += 1\n```", "```\nprint('Sample size: ', sample_size)\n```", "```\nSample size:  50\n```", "```\ndef decode_and_resize(serialized_example):\n```", "```\n    # resized image should be [224, 224, 3] and normalized to value range [0, 255] \n```", "```\n    # label is integer index of class.\n```", "```\n        parsed_features = tf.io.parse_single_example(\n```", "```\n    serialized_example,\n```", "```\n    features = {\n```", "```\n    'image/channels' :  tf.io.FixedLenFeature([], tf.int64),\n```", "```\n    'image/class/label' :  tf.io.FixedLenFeature([], tf.int64),\n```", "```\n    'image/class/text' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/colorspace' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/encoded' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/filename' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/format' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/height' : tf.io.FixedLenFeature([], tf.int64),\n```", "```\n    'image/width' : tf.io.FixedLenFeature([], tf.int64)\n```", "```\n    })\n```", "```\n    image = tf.io.decode_jpeg(parsed_features['image/encoded'], \t                                                    channels=3)\n```", "```\n    label = tf.cast(parsed_features['image/class/label'],         \t                                                      tf.int32)\n```", "```\n    label_txt = tf.cast(parsed_features['image/class/text'], \t                                                     tf.string)\n```", "```\n    label_one_hot = tf.one_hot(label, depth = 5)\n```", "```\n    resized_image = tf.image.resize(image, [224, 224],  \t     \t                                              method='nearest')\n```", "```\n    return resized_image, label_one_hot\n```", "```\ndef normalize(image, label):\n```", "```\n    #Convert `image` from [0, 255] -> [0, 1.0] floats \n```", "```\n    image = tf.cast(image, tf.float32) / 255\\. \n```", "```\n   return image, label\n```", "```\ndecoded = test_all_ds.map(decode_and_resize)\n```", "```\nnormed = decoded.map(normalize)\n```", "```\nnp_img_holder = np.empty((0, 224, 224,3), float)\n```", "```\nnp_lbl_holder = np.empty((0, 5), int)\n```", "```\nfor img, lbl in normed:\n```", "```\n    r = img.numpy() # image value extracted\n```", "```\n    rx = np.expand_dims(r, axis=0) \n```", "```\n    lx = np.expand_dims(lbl, axis=0) \n```", "```\n    np_img_holder = np.append(np_img_holder, rx, axis=0) \n```", "```\nnp_lbl_holder = np.append(np_lbl_holder, lx, axis=0) \n```", "```\n%matplotlib inline\n```", "```\nplt.figure()\n```", "```\nfor i in range(len(np_img_holder)):\n```", "```\n    plt.subplot(10, 5, i+1)\n```", "```\n    plt.axis('off')\n```", "```\nplt.imshow(np.asarray(np_img_holder[i]))\n```", "```\n    x = np.expand_dims(np_img_holder[0], axis=0)\n    x.shape\n    (1, 224, 224, 3)\n    ```", "```\n    xf = tf.dtypes.cast(x, tf.float32)\n    ```", "```\n    prediction = infer(xf)\n    ```", "```\n    prediction.get('custom_class')\n    ```", "```\n    <tf.Tensor: shape=(1, 5), dtype=float32, numpy=\n    array([[1.5271275e-04, 2.0515859e-05, 1.0230409e-06, 2.9591745e-06, 9.9982280e-01]], dtype=float32)>\n    ```", "```\n    predicted_prob_array = prediction.get('custom_class').numpy()\n    idx = np.argmax(predicted_prob_array)\n    print(idx)\n    ```", "```\n    feature_description = {\n        'image/channels' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/label' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/text' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/colorspace' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/encoded' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/filename' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/format' : tf.io.FixedLenFeature([], \t \t \t                                               tf.string),\n        'image/height' : tf.io.FixedLenFeature([], tf.int64),\n        'image/width' : tf.io.FixedLenFeature([], tf.int64)\n    }\n    def _parse_function(example_proto):\n      return tf.io.parse_single_example(example_proto, \t \t                                     feature_description)\n    parsd_ds = test_all_ds.map(_parse_function)\n    val_label_map = {}\n    # getting label mapping\n    for image_features in parsd_ds.take(50):\n        label_idx = image_features[\n                                 'image/class/label'].numpy()\n        label_str = image_features[\n                         'image/class/text'].numpy().decode()\n        if label_idx not in val_label_map:\n            val_label_map[label_idx] = label_str\n    ```", "```\n    val_label_map\n    ```", "```\n    {4: 'tulips', 3: 'dandelion', 1: 'sunflowers', 2: 'daisy', 0: 'roses'}\n    ```", "```\n    print(val_label_map.get(idx))\n    ```", "```\n    tulip\n    ```", "```\n    batched_input = tf.dtypes.cast(np_img_holder, tf.float32)\n    batch_predicted_prob_array = infer(batched_input)\n    ```", "```\n    def lookup(np_entry, dictionary):\n      \tclass_key = np.argmax(np_entry)\n    \treturn dictionary.get(class_key)\n    ```", "```\n    actual = []\n    for i in range(len(np_lbl_holder)):\n        plain_text_label = lookup(np_lbl_holder[i], \n                                               val_label_map)\n        actual.append(plain_text_label)\n    ```", "```\n    predicted_label = []\n    for i in range(sample_size):\n    batch_prediction = batch_predicted_prob_array.get('custom_class').numpy()\n    plain_text_label = lookup(batch_prediction[i], \n                                               val_label_map)\n    predicted_label.append(plain_text_label)\n    ```", "```\n    from sklearn.metrics import accuracy_score\n    accuracy=accuracy_score(actual, predicted_label)\n    print(accuracy)\n    0.82\n    ```", "```\n    import tensorflow as tf\n    import pathlib\n    import os\n    import numpy as np\n    from matplotlib.pyplot import imshow\n    import matplotlib.pyplot as plt\n    root_dir = '../train_base_model'\n    model_dir = ' trained_resnet_vector-unquantized/save_model'\n    saved_model_dir = os.path.join(root_dir, model_dir)\n    trained_model = tf.saved_model.load(saved_model_dir)\n    ```", "```\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    ```", "```\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    ```", "```\n    converter.target_spec.supported_types = [tf.float16]\n    tflite_model = converter.convert()\n    ```", "```\n    root_dir = ''\n    tflite_model_dir = trained_resnet_vector-unquantized\n    to_save_tflite_model_dir = os.path.join(root_dir, \n                                            tflite_model_dir)\n    ```", "```\n    saved_tflite_model_dir = pathlib.Path(\n                                    to_save_tflite_model_dir)\n    ```", "```\n    saved_tflite_model_dir.mkdir(exist_ok=True, parents=True) \n    ```", "```\n    tgt = pathlib.Path(tflite_models_dir, \n                            'converted_model_reduced.tflite')\n    ```", "```\n    tgt.write_bytes(tflite_model)\n    ```", "```\n    47487392\n    ```", "```\n    tgt = pathlib.Path(tflite_models_dir, \n                            'converted_model_reduced.tflite')\n    ```", "```\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    ```", "```\n    input_details[0]['shape']\n    array([  1, 224, 224,   3], dtype=int32)\n    output_details[0]['shape']\n    array([1, 5], dtype=int32)\n    ```", "```\ninput_data = np.array(np.expand_dims(np_img_holder[0], axis=0), dtype=np.float32)\n```", "```\ninterpreter.set_tensor(input_details[0]['index'], input_data)\n```", "```\ninterpreter.invoke()\n```", "```\noutput_data = interpreter.get_tensor(output_details[0]['index'])\n```", "```\nprint(output_data)\n```", "```\n[[1.5181543e-04 2.0090181e-05 1.0022727e-06 2.8991076e-06 9.9982423e-01]]\n```", "```\nlookup(output_data, val_label_map)\n```", "```\n'tulips'\n```", "```\n    def batch_predict(input_raw, input_tensor, output_tensor, dictionary):\n        input_data = np.array(np.expand_dims(input_raw, \n                                   axis=0), dtype=np.float32)\n        interpreter.set_tensor(input_tensor[0]['index'], \n                                                  input_data)\n        interpreter.invoke()\n        interpreter_output = interpreter.get_tensor(\n                                   output_tensor[0]['index'])\n        plain_text_label = lookup(interpreter_output, \n                                                   dictionary)\n        return plain_text_label\n    ```", "```\n    batch_quantized_prediction = []\n    for i in range(sample_size):\n        plain_text_label = batch_predict(np_img_holder[i], input_details, output_details, val_label_map)\n        batch_quantized_prediction.append(plain_text_label)\n    ```", "```\n    quantized_accuracy = accuracy_score(actual, batched_quantized_prediiction)\n    print(quantized_accuracy)\n    ```", "```\n    0.82\n    ```", "```\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n```", "```\nconverter.target_spec.supported_types = [tf.float16]\n```", "```\ntflite_model = converter.convert()\n```", "```\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n```", "```\ntflite_model = converter.convert()\n```", "```\n    import tensorflow as tf\n    import pathlib\n    import os\n    import numpy as np\n    from matplotlib.pyplot import imshow\n    import matplotlib.pyplot as plt\n    root_dir = ''\n    model_dir = 'trained_resnet_vector-unquantized/save_model'\n    saved_model_dir = os.path.join(root_dir, model_dir)\n    ```", "```\n    converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    ```", "```\n    root_dir = ''\n    tflite_models_dir = 'trained_resnet_vector-unquantized/tflite_hybrid_model'\n    to_save_tflite_model_dir = os.path.join(root_dir,           tflite_models_dir)\n    saved_tflite_models_dir = pathlib.Path(to_save_tflite_model_dir) \n    saved_tflite_models_dir.mkdir(exist_ok=True, parents=True\n    tgt = pathlib.Path(to_save_tflite_model_dir, 'converted_model_reduced.tflite')\n    tgt.write_bytes(tflite_model)\n    ```", "```\n    24050608\n    ```", "```\n    root_dir = '../train_base_model/tf_datasets/flower_photos'\n    test_pattern = '{}/image_classification_builder-test.tfrecord*'.format(root_dir)\n    test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_pattern))\n    test_all_ds = tf.data.TFRecordDataset(test_all_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    ```", "```\n    sample_size = 0\n    for raw_record in test_all_ds:\n        sample_size += 1\n    print('Sample size: ', sample_size)\n    ```", "```\n    def decode_and_resize(serialized_example):\n        # resized image should be [224, 224, 3] and normalized to value range [0, 255] \n        # label is integer index of class.\n            parsed_features = tf.io.parse_single_example(\n        serialized_example,\n        features = {\n        'image/channels' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/label' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/text' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/colorspace' : tf.io.FixedLenFeature([], \t  \t                                               tf.string),\n        'image/encoded' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/filename' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/format' : tf.io.FixedLenFeature([], \t \t       \t                                               tf.string),\n        'image/height' : tf.io.FixedLenFeature([], tf.int64),\n        'image/width' : tf.io.FixedLenFeature([], tf.int64)\n        })\n        image = tf.io.decode_jpeg(parsed_features[\n                                'image/encoded'], channels=3)\n        label = tf.cast(parsed_features['image/class/label'], \n                                                     tf.int32)\n        label_txt = tf.cast(parsed_features[\n                              'image/class/text'], tf.string)\n        label_one_hot = tf.one_hot(label, depth = 5)\n        resized_image = tf.image.resize(image, [224, 224], \n                                             method='nearest')\n        return resized_image, label_one_hot\n    def normalize(image, label):\n        #Convert `image` from [0, 255] -> [0, 1.0] floats \n        image = tf.cast(image, tf.float32) / 255\\. \n        return image, label\n    ```", "```\n    decoded = test_all_ds.map(decode_and_resize)\n    normed = decoded.map(normalize)\n    ```", "```\n    np_img_holder = np.empty((0, 224, 224,3), float)\n    np_lbl_holder = np.empty((0, 5), int)\n    for img, lbl in normed:\n        r = img.numpy() # image value extracted\n        rx = np.expand_dims(r, axis=0) \n        lx = np.expand_dims(lbl, axis=0) \n        np_img_holder = np.append(np_img_holder, rx, axis=0) \n        np_lbl_holder = np.append(np_lbl_holder, lx, axis=0) \n    ```", "```\n    actual = []\n    for i in range(len(np_lbl_holder)):\n        class_key = np.argmax(np_lbl_holder[i])\n        actual.append(val_label_map.get(class_key))\n    ```", "```\n    %matplotlib inline\n    plt.figure()\n    for i in range(len(np_img_holder)):\n        plt.subplot(10, 5, i+1)\n        plt.axis('off')\n    imshow(np.asarray(np_img_holder[i]))\n    ```", "```\nfeature_description = {\n```", "```\n    'image/channels' :  tf.io.FixedLenFeature([], tf.int64),\n```", "```\n    'image/class/label' :  tf.io.FixedLenFeature([], tf.int64),\n```", "```\n    'image/class/text' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/colorspace' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/encoded' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/filename' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/format' : tf.io.FixedLenFeature([], tf.string),\n```", "```\n    'image/height' : tf.io.FixedLenFeature([], tf.int64),\n```", "```\n    'image/width' : tf.io.FixedLenFeature([], tf.int64)\n```", "```\n}\n```", "```\ndef _parse_function(example_proto):\n```", "```\n  return tf.io.parse_single_example(example_proto, \n```", "```\n                                           feature_description)\n```", "```\nparsd_ds = test_all_ds.map(_parse_function)\n```", "```\nval_label_map = {}\n```", "```\n# getting label mapping\n```", "```\nfor image_features in parsd_ds.take(50):\n```", "```\n    label_idx = image_features['image/class/label'].numpy()\n```", "```\n    label_str = image_features['image/class/text'].numpy().decode()\n```", "```\n    if label_idx not in val_label_map:\n```", "```\n        val_label_map[label_idx] = label_str\n```", "```\n{4: 'tulips', 3: 'dandelion', 1: 'sunflowers', 2: 'daisy', 0: 'roses'}\n```", "```\n    interpreter = tf.lite.Interpreter(model_path=str(tgt))\n    interpreter.allocate_tensors()\n    ```", "```\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    ```", "```\n    input_data = np.array(np.expand_dims(np_img_holder[0], axis=0), dtype=np.float32)\n    interpreter.set_tensor(input_details[0]['index'], input_data)\n    interpreter.invoke()\n    output_data = interpreter.get_tensor(output_details[0]['index'])\n    print(output_data)\n    ```", "```\n    [[1.1874483e-04 1.3445899e-05 8.4869811e-07 2.8064751e-06 9.9986410e-01]]\n    ```", "```\n    def lookup(np_entry, dictionary):\n        class_key = np.argmax(np_entry)\n        return dictionary.get(class_key)\n    lookup(output_data, val_label_map)\n    ```", "```\n    'tulips'\n    ```", "```\n    def batch_predict(input_raw, input_tensor, output_tensor, dictionary):\n        input_data = np.array(np.expand_dims(input_raw, \t  \t                               axis=0), dtype=np.float32)\n        interpreter.set_tensor(input_tensor[0]['index'], \t \t                                              input_data)\n        interpreter.invoke()\n        interpreter_output = interpreter.get_tensor(\n                                   output_tensor[0]['index'])\n        plain_text_label = lookup(interpreter_output, \t \t                                               dictionary)\n        return plain_text_label\n    ```", "```\n    batch_quantized_prediction = []\n    for i in range(sample_size):\n        plain_text_label = batch_predict(np_img_holder[i], input_details, output_details, val_label_map)\n    batch_quantized_prediction.append(plain_text_label)\n    ```", "```\n    accuracy=accuracy_score(actual, batch_quantized_prediction)\n    print(accuracy)\n    ```", "```\n    0.82\n    ```", "```\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import numpy as np\n    import os\n    import pathlib\n    root_dir = '../train_base_model/tf_datasets/flower_photos'\n    file_pattern = '{}/image_classification_builder-train*.tfrecord*'.format(root_dir)\n    val_file_pattern = '{}/image_classification_builder-validation*.tfrecord*'.format(root_dir)\n    file_list = tf.io.gfile.glob(file_pattern)\n    all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(file_pattern))\n    val_file_list = tf.io.gfile.glob(val_file_pattern)\n    val_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(val_file_pattern))\n    train_all_ds = tf.data.TFRecordDataset(all_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    val_all_ds = tf.data.TFRecordDataset(val_all_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    ```", "```\n    def decode_and_resize(serialized_example):\n        # resized image should be [224, 224, 3] and normalized to value range [0, 255] \n        # label is integer index of class.\n            parsed_features = tf.io.parse_single_example(\n        serialized_example,\n        features = {\n        'image/channels' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/label' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/text' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/colorspace' : tf.io.FixedLenFeature([],  \t  \t                                               tf.string),\n        'image/encoded' : tf.io.FixedLenFeature([],  \t   \t                                               tf.string),\n        'image/filename' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/format' : tf.io.FixedLenFeature([],  \t \t                                               tf.string),\n        'image/height' : tf.io.FixedLenFeature([], tf.int64),\n        'image/width' : tf.io.FixedLenFeature([], tf.int64)\n        })\n        image = tf.io.decode_jpeg(parsed_features[\n                                'image/encoded'], channels=3)\n        label = tf.cast(parsed_features['image/class/label'],  \t                                                tf.int32)\n        label_txt = tf.cast(parsed_features[\n                              'image/class/text'], tf.string)\n        label_one_hot = tf.one_hot(label, depth = 5)\n        resized_image = tf.image.resize(image, [224, 224], \t                                         method='nearest')\n        return resized_image, label_one_hot\n    def normalize(image, label):\n        #Convert `image` from [0, 255] -> [0, 1.0] floats \n        image = tf.cast(image, tf.float32) / 255\\. + 0.5\n        return image, label\n    ```", "```\n    def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n        # This is a small dataset, only load it once, and\n        # keep it in memory.\n        # use `.cache(filename)` to cache preprocessing work\n        # for datasets that don't fit in memory.\n        if cache:\n            if isinstance(cache, str):\n                ds = ds.cache(cache)\n            else:\n                ds = ds.cache()\n        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n        # Repeat forever\n        ds = ds.repeat()\n        ds = ds.batch(32)\n        # `prefetch` lets the dataset fetch batches in the\n        # background while the model is training.\n        AUTOTUNE = tf.data.experimental.AUTOTUNE\n        ds = ds.prefetch(buffer_size=AUTOTUNE)\n        return ds\n    ```", "```\n    # perform data engineering \n    dataset = train_all_ds.map(decode_and_resize)\n    val_dataset = val_all_ds.map(decode_and_resize)\n    ```", "```\n    # Create dataset for training run\n    BATCH_SIZE = 32\n    VALIDATION_BATCH_SIZE = 40\n    dataset = dataset.map(normalize, \n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    val_dataset = val_dataset.map(normalize, \n            num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    val_ds = val_dataset.batch(VALIDATION_BATCH_SIZE)\n        AUTOTUNE = tf.data.experimental.AUTOTUNE\n    train_ds = prepare_for_training(dataset)\n    ```", "```\n    NUM_CLASSES = 5\n    IMAGE_SIZE = (224, 224)\n        train_sample_size=0\n    for raw_record in train_all_ds:\n        train_sample_size += 1\n    print('TRAIN_SAMPLE_SIZE = ', train_sample_size)\n    validation_sample_size=0\n    for raw_record in val_all_ds:\n        validation_sample_size += 1\n    print('VALIDATION_SAMPLE_SIZE = ', \n                                      validation_sample_size)\n    STEPS_PER_EPOCHS = train_sample_size // BATCH_SIZE\n    VALIDATION_STEPS = validation_sample_size // \n                                        VALIDATION_BATCH_SIZE\n    ```", "```\n    TRAIN_SAMPLE_SIZE =  3540\n    VALIDATION_SAMPLE_SIZE =  80\n    ```", "```\n    model = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n        hub.KerasLayer(\n        https://tfhub.dev/google/imagenet/resnet_v1_101/feature_vector/4',\n        trainable=False),\n        tf.keras.layers.Dense(NUM_CLASSES, \n                 activation='softmax', name = 'custom_class')\n    ])\n    model.build([None, 224, 224, 3])\n    model.compile(\n      optimizer=tf.keras.optimizers.SGD(lr=0.005, \n                                               momentum=0.9), \n      loss=tf.keras.losses.CategoricalCrossentropy(\n                      from_logits=True, label_smoothing=0.1),\n      metrics=['accuracy'])\n    model.summary()\n    ```", "```\n    checkpoint_prefix = os.path.join('trained_resnet_vector', 'train_ckpt_{epoch}')\n    callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(\n                                  filepath=checkpoint_prefix,\n        save_weights_only=True)]\n    model.fit(\n            train_ds,\n            epochs=3, \n            steps_per_epoch=STEPS_PER_EPOCHS,\n            validation_data=val_ds,\n            validation_steps=VALIDATION_STEPS,\n            callbacks=callbacks)\n    ```", "```\n    saved_model_path = os.path.join(root_dir, 'custom_cnn/full_resnet_vector_saved_model')\n    tf.saved_model.save(model, saved_model_path)\n    ```", "```\n    SavedModel format. In order to properly quantize the input and output layers, we need to provide some typical data. We will use the validation data, which contains 80 samples of 5 classes of flower images: \n    ```", "```\n    decoded = val_all_ds.map(decode_and_resize)\n    normed = decoded.map(normalize)\n    ```", "```\n    np_img_holder = np.empty((0, 224, 224,3), float)\n    np_lbl_holder = np.empty((0, 5), int)\n    for img, lbl in normed:\n        r = img.numpy()\n        rx = np.expand_dims(r, axis=0) \n        lx = np.expand_dims(lbl, axis=0) \n        np_img_holder = np.append(np_img_holder, rx, axis=0) \n        np_lbl_holder = np.append(np_lbl_holder, lx, axis=0) \n    ```", "```\n    def data_generator():\n      for input_tensor in tf.data.Dataset.from_tensor_slices(np_img_holder.astype(np.float32)).batch(1).take(sample_size):\n        yield [input_tensor]\n    ```", "```\n    sample_size = 0\n    for raw_record in val_all_ds:\n        sample_size += 1\n    print('Sample size: ', sample_size)\n    ```", "```\n    Sample size:  80\n    ```", "```\n    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    converter.representative_dataset = data_generator\n    ```", "```\n    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n    ```", "```\n    converter.inference_input_type = tf.uint8\n    converter.inference_output_type = tf.uint8\n    tflite_model_quant = converter.convert()\n    ```", "```\n    interpreter = tf.lite.Interpreter(model_content=tflite_model_quant)\n    input_type = interpreter.get_input_details()[0]['dtype']\n    print('input: ', input_type)\n    output_type = interpreter.get_output_details()[0]['dtype']\n    print('output: ', output_type)\n    ```", "```\n    input:  <class 'numpy.uint8'>\n    output:  <class 'numpy.uint8'>\n    ```", "```\n    tflite_models_dir = 'quantized_resnet_vector/tflite_int8_model'\n    to_save_tflite_model_dir = os.path.join(root_dir, tflite_models_dir)\n    saved_tflite_models_dir = pathlib.Path(to_save_tflite_model_dir) \n    saved_tflite_models_dir.mkdir(exist_ok=True, parents=True) \n    tgt = pathlib.Path(to_save_tflite_model_dir, 'converted_model_reduced.tflite')\n    tgt.write_bytes(tflite_model_quant)\n    ```", "```\n    44526000\n    ```", "```\n    test_pattern = '{}/image_classification_builder-test.tfrecord*'.format(root_dir)\n    test_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(test_pattern))\n    test_all_ds = tf.data.TFRecordDataset(test_all_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    ```", "```\n    sample_size = 0\n    for raw_record in test_all_ds:\n        sample_size += 1\n    print('Sample size: ', sample_size)\n    ```", "```\n    decoded = test_all_ds.map(decode_and_resize)\n    ```", "```\n    np_img_holder = np.empty((0, 224, 224,3), float)\n    np_lbl_holder = np.empty((0, 5), int)\n    for img, lbl in decoded:\n        r = img.numpy() \n        rx = np.expand_dims(r, axis=0) \n        lx = np.expand_dims(lbl, axis=0) \n        np_img_holder = np.append(np_img_holder, rx, axis=0) \n        np_lbl_holder = np.append(np_lbl_holder, lx, axis=0) \n    ```", "```\n    feature_description = {\n        'image/channels' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/label' :  tf.io.FixedLenFeature([], \t \t                                                tf.int64),\n        'image/class/text' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/colorspace' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/encoded' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/filename' : tf.io.FixedLenFeature([], \t \t                                               tf.string),\n        'image/format' : tf.io.FixedLenFeature([], \t \t   \t                                               tf.string),\n        'image/height' : tf.io.FixedLenFeature([], tf.int64),\n        'image/width' : tf.io.FixedLenFeature([], tf.int64)\n    }\n    ```", "```\n    def _parse_function(example_proto):\n      return tf.io.parse_single_example(example_proto, \n                                         feature_description)\n    parsd_ds = test_all_ds.map(_parse_function)\n    ```", "```\n    val_label_map = {}\n    # getting label mapping\n    for image_features in parsd_ds.take(30):\n        label_idx = image_features[\n                                 'image/class/label'].numpy()\n        label_str = image_features[\n                         'image/class/text'].numpy().decode()\n        if label_idx not in val_label_map:\n            val_label_map[label_idx] = label_str\n    ```", "```\n    val_label_map\n    ```", "```\n    {0: 'roses', 1: 'sunflowers', 2: 'daisy', 3: 'dandelion', 4: 'tulips'}\n    ```", "```\n    def lookup(np_entry, dictionary):\n        class_key = np.argmax(np_entry)\n        return dictionary.get(class_key)\n    ```", "```\n    actual = []\n    for i in range(len(np_lbl_holder)):\n        class_key = np.argmax(np_lbl_holder[i])\n        actual.append(val_label_map.get(class_key))\n    ```", "```\n    def batch_predict(input_raw, input_tensor, output_tensor, dictionary):\n        input_data = np.array(np.expand_dims(input_raw, \t\n                                     axis=0), dtype=np.uint8)\n        interpreter.set_tensor(input_tensor['index'], \n                                                  input_data)\n        interpreter.invoke()\n        interpreter_output = interpreter.get_tensor(\n                                      output_tensor['index'])\n        plain_text_label = lookup(interpreter_output, \n                                                   dictionary)\n        return plain_text_label\n    ```", "```\n    interpreter = tf.lite.Interpreter(model_path=str(tgt))\n    interpreter.allocate_tensors()\n    # Get input and output tensors.\n    input_details = interpreter.get_input_details()[0]\n    output_details = interpreter.get_output_details()[0]\n    ```", "```\n    batch_quantized_prediction = []\n    for i in range(sample_size):\n        plain_text_label = batch_predict(np_img_holder[i], \n                input_details, output_details, val_label_map)\n        batch_quantized_prediction.append(plain_text_label)\n    ```", "```\n    from sklearn.metrics import accuracy_score\n    accuracy_score(actual, batch_quantized_prediction)\n    ```", "```\n    0.86\n    ```"]