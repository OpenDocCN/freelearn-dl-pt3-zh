<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Imitation Learning with the DAgger Algorithm</h1>
                </header>
            
            <article>
                
<p><span>The ability of an algorithm to learn only from rewards is a very important characteristic that led us to develop reinforcement learning algorithms. This enables an agent to learn and improve its policy from scratch without additional supervision. Despite this, there are situations where other expert agents are already employed in a given environment. <strong>Imitation learning</strong> (<strong>IL</strong>) algorithms leverage the expert by imitating their actions and learning the policy from them.</span></p>
<p><span>This chapter focuses on imitation learning. Although different to reinforcement learning, imitation learning offers great opportunities and capabilities, especially in environments with very large state spaces and sparse rewards. Obviously, imitation learning is possible only when a more expert agent to imitate is available.</span></p>
<p>The chapter will focus on the main concepts and features of imitation learning methods. We'll implement an imitation learning algorithm called DAgger, and teach an agent to play Flappy Bird. This will help you to master this new family of algorithms and appreciate their basic principles.</p>
<p>In the last section of this chapter, we'll introduce <strong>inverse reinforcement learning</strong> (<strong>IRL</strong>). IRL is a method that extracts and learns the behaviors of another agent in terms of values and rewards; that is, IRL learns the reward function.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li><span>The imitation approach</span></li>
<li>Playing with Flappy Bird</li>
<li>Understanding the dataset aggregation algorithm</li>
<li>IRL</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>After a brief theoretical introduction to grasp the core concepts behind the imitation learning algorithms, we'll implement a real IL algorithm. However, we'll provide only the main and most interesting parts. Thus, if you are interested in the full implementation, you can find it in the GitHub repository of this book: <a href="https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python">https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installation of Flappy Bird</h1>
                </header>
            
            <article>
                
<p>Later, we'll run our IL algorithm on a revisited version of a famous game called Flappy Bird (<a href="https://en.wikipedia.org/wiki/Flappy_Bird">https://en.wikipedia.org/wiki/Flappy_Bird</a>). In this section, we'll give you all the commands needed to install it.</p>
<p>But before installing the environment of the game, we need to take care of a few additional libraries:</p>
<ul>
<li>In Ubuntu, the procedure is as follows:</li>
</ul>
<pre style="padding-left: 60px"><strong>$ sudo apt-get install git python3-dev python3-numpy libsdl-image1.2-dev libsdl-mixer1.2-dev libsdl-ttf2.0-dev libsmpeg-dev libsdl1.2-dev libportmidi-dev libswscale-dev libavformat-dev libavcodec-dev libfreetype6-dev</strong><br/><strong>$ sudo pip install pygame</strong></pre>
<ul>
<li>If you are a Mac user, you can install the libraries with the following commands:</li>
</ul>
<pre style="padding-left: 60px"><strong>$ brew install sdl sdl_ttf sdl_image sdl_mixer portmidi </strong><br/><strong>$ pip install -c https://conda.binstar.org/quasiben pygame</strong></pre>
<ul>
<li>Then, for both Ubuntu and Mac users, the procedure is the following:</li>
</ul>
<ol start="1">
<li>First, you have to clone PLE. The cloning is done with the following line of code:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/ntasfi/PyGame-Learning-Environment</strong></pre>
<p style="padding-left: 60px"><span>PLE is a set of environments that also includes Flappy Bird. Thus, by installing PLE, you'll obtain Flappy Bird. </span></p>
<ol start="2">
<li>Then, you have to enter the<span> </span><kbd>PyGame-Learning-Environment</kbd><span> </span>folder:</li>
</ol>
<pre style="padding-left: 60px"><strong>cd PyGame-Learning-Environment</strong></pre>
<ol start="3">
<li>And finally, run the installation with the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>sudo pip install -e .</strong></pre>
<p style="padding-left: 60px">Now, you should be able to use Flappy Bird.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The imitation approach</h1>
                </header>
            
            <article>
                
<p>IL is the art of acquiring a new skill by emulating an expert. <span>This property of learning from imitation is not strictly necessary for learning sequential decision-making policies but nowadays, it is essential in plenty of problems. Some tasks cannot be solved through mere reinforcement learning, and bootstrapping a policy from the </span><span>enormous </span><span>spaces of complex environments is a key factor. </span>The following diagram represents a high-level view of the core components involved in the imitation learning process:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1996 image-border" src="assets/c59df7c9-96f1-4662-9cda-5da6e9588fce.png" style="width:32.25em;height:20.67em;"/></p>
<p>If intelligent agents (the experts) already exist in an environment, they can be used to provide a huge amount of information to a new agent (the learner) about the behaviors needed to accomplish the task and navigate the environment. In this situation, the newer agent can learn much faster without the need to learn from scratch. The expert agent can also be used as a teacher to instruct and feed back to the new agent on its performing. Note the difference here. The expert can be used both as a guide to follow and as a supervisor to correct the mistakes of the student.</p>
<p class="mce-root"/>
<p>If either the model of the guide, or the supervisor, is available, an imitation learning algorithm can leverage them. You can now understand why imitation learning plays such an important role and why we cannot leave it out of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The driving assistant example</h1>
                </header>
            
            <article>
                
<p>To grasp these key concepts <span>better, </span>we can use the example of a teenager learning to drive. Let's assume that they have never been in a car, that this is the first time they are seeing one, and that they don't have any knowledge of how it works. There are three approaches to learning: </p>
<ol>
<li>They are given the keys and have to learn all by themselves, with no supervision at all.</li>
<li>Before being given the keys, they sit in the passenger seat for 100 hours and look at the expert driving in different weather conditions and on different roads.</li>
<li>They observe the expert driving but, most importantly, they have sessions where the expert provides feedback while driving. For example, the expert can give real-time instructions on how to park the car, and give direct feedback on how to stay in a lane.</li>
</ol>
<p>As you may have guessed, the first case is a reinforcement learning approach where the agent has only sparse rewards from not breaking the car, pedestrians not yelling at them, and so on.</p>
<p>Regarding the second case, this is a passive IL approach with the competence that is acquired from the pure reproduction of the expert's actions. Overall, it's very close to a supervised learning approach.</p>
<p>The third and final case is an active IL approach that gives rise to a <em>real</em> imitation learning approach. In this case, it is required that, during the training phase, the expert instructs the learner on every move the learner makes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing IL and RL</h1>
                </header>
            
            <article>
                
<p>Let's go more in-depth with the IL approach by highlighting the differences vis-à-vis RL. This contrast is very important. In imitation learning, the learner is not aware of any reward. This constraint can have very big implications.</p>
<p>Going back to our example, the apprentice can only replicate <span>the expert's moves</span> as closely as possible, be it in a passive or an active way. Not having objective rewards from the environment, they are constrained to the subjective supervision of the expert. Thus, even if they wanted to, they aren't able to improve and <span><span>understand the teacher's reasoning</span></span>.</p>
<p>So, IL should be seen as a way to copy the moves of the expert but without knowing its main goal. In our example, it's as if the young driver assimilates <span>the trajectories of the teacher </span>very well but, still, they don't know the motivations that made the teacher choose them. Without being aware of the reward, an agent trained with imitation learning cannot maximize the total reward as executed in RL.</p>
<p>This highlights the main differences between IL and RL. The former lacks the understanding of the main objective, and thus cannot surpass the teacher. The latter instead lacks a direct supervision signal and, in most cases, has <span>access </span>only to a sparse reward. This situation is clearly depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1997 image-border" src="assets/09a97c8b-02fd-4770-94cd-ecb16c86a8c3.png" style="width:39.08em;height:32.58em;"/></p>
<p>The diagram o<span>n the left represents </span>the usual RL cycle, while on the right, the imitation learning cycle is represented. Here, the learner doesn't receive any reward; just the state and action given by the expert.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The role of the expert in imitation learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">The terms <em>expert</em>, <em>teacher</em>, and <em>supervisor</em> <span>refer to the same concept </span>when speaking of imitation learning algorithms. They express a figure from which the new agent (the learner) can learn.</p>
<p class="mce-root">Fundamentally, the expert can be of any form, from a real human expert to an expert system. The first case is more obvious and adopted. What you are doing is teaching an algorithm to perform a task that a human is already able to do. The advantages are evident and it can be employed in a vast number of tasks.</p>
<p class="mce-root">The second case may not be so common. <span>One of the valid motivations</span> behind choosing a new algorithm trained with IL can be attributed to a slow expert system that, due to technical limitations, cannot be improved. For example, the teacher could be an accurate, but slow, tree search algorithm that is not able to perform at a decent speed at inference time. A deep neural network could be employed in its place. The training of the neural network under the supervision of the tree search algorithm could take some time but, once trained, it could perform much faster during runtime. </p>
<p>By now, it should be clear that t<span>he quality of the policy coming from the learner is largely due to the quality of the information provided by the expert. The performance of the teacher is an upper limit to the final performances of the scholar. A poor teacher will always provide bad data to the learner. Thus, t</span>he expert is a key component that sets the bar for the quality of the final agent. With a weak teacher, we cannot pretend to obtain good policies.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The IL structure</h1>
                </header>
            
            <article>
                
<p>Now that all the ingredients of imitation learning have been tackled, we can elaborate on the algorithms and approaches that can be used in order to design a full imitation learning algorithm. </p>
<p>The most straightforward way to tackle the imitation problem is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1998 image-border" src="assets/7d182866-bc4e-4b9d-84b4-b882be2af54f.png" style="width:230.67em;height:74.00em;"/></p>
<p>The preceding diagram can be summarized in two main steps:</p>
<ul>
<li>An expert collects data from the environment.</li>
<li>A policy is learned through supervised learning on the dataset.</li>
</ul>
<p>Unfortunately, despite supervised learning being the imitation algorithm for excellence, most of the time, it doesn't work.</p>
<p>To understand why the supervised learning approach isn't a good alternative, we have to recall the foundations of supervised learning. We are mostly interested in two basic principles: the training and test set should belong to the same distribution, and the data should be independent and identically distributed (i.i.d). However, a policy should be tolerant of different trajectories and be robust to eventual distribution shifts.</p>
<p>If an agent is trained using only a supervised learning approach to drive a car, whenever it shifts a little bit from the expert trajectories, it will be in a new state never seen before, and that will create a distribution mismatch. In this new state, the agent will be uncertain about the next action to take. In a usual supervised learning problem, it doesn't matter too much. If a prediction is missed, this will not have an influence on the next prediction. However, in an imitation learning problem, the algorithm is learning a policy and the i.i.d property is no longer valid because subsequent actions are strictly correlated to each other. Thus, they will have consequences and a compounding effect on all the others.</p>
<p>In our example of the self-driving car, once the distribution has changed from that of the expert, the correct path will be very difficult to recover, since bad actions will accumulate and lead to dramatic consequences. The longer the trajectory, the worse the effect of imitation learning. To clarify, supervised learning problems with i.i.d. data can be seen as having a trajectory of length 1. No consequences on the next actions are found. The paradigm we have just presented is what we referred to <span>previously </span>as <em>passive</em> learning.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To overcome the distributional shift that can have catastrophic effects on policies learned using <em>passive</em> imitation, different techniques can be adopted. Some are hacks<em>,</em> while others are more algorithmic variations. Two of these strategies that work well are the following:</p>
<ul>
<li>Learning a model that generalizes very well on the data without overfitting</li>
<li>Using an active imitation in addition to the passive one</li>
</ul>
<p>Because the first is more of a broad challenge, we will concentrate on the second strategy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing active with passive imitation</h1>
                </header>
            
            <article>
                
<p>We introduced the term <em>active imitation</em> in the previous example, with the teenager learning to drive a car. Specifically, we referred to it in the situation in which the learner was driving with additional feedback from the expert. In general, for active imitation, we mean learning from on-policy data with the actions assigned by the expert.</p>
<p>Speaking in terms of input <em>s</em> (the state or observation) and output <em>a</em> (the action), in passive learning, s and a both come from the expert. In active learning, s is sampled from the learner, and a is the action that the expert would have taken in state s. The objective of the newbie agent is to learn a mapping, <sub><img class="fm-editor-equation" src="assets/6d071f93-e760-4783-ba8a-76a29b1a221d.png" style="width:3.17em;height:1.42em;"/></sub>.</p>
<p>Active learning with on-policy data allows the learner to fix small deviations from the expert trajectory that <span>the learner wouldn't know how to correct</span> with only passive imitation. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing Flappy Bird</h1>
                </header>
            
            <article>
                
<p>Later in this chapter, we'll develop and test an IL algorithm called DAgger on a new environment. The environment named Flappy Bird emulates the famous Flappy Bird game. Here, our mission is to give you the tools needed to implement code using this environment, starting from the explanation of the interface.</p>
<p><span>Flappy Bird belongs to the <strong>PyGame Learning Environment</strong> (<strong>PLE</strong>), a set of environments that mimic the <strong>Arcade Learning Environment</strong> (<strong>ALE</strong>) interface. This is similar to the <strong>Gym</strong> interface, and later we'll see the differences, although it's simple to use.</span></p>
<p>The goal of Flappy Bird is to make the bird fly through vertical pipes without hitting them. It is controlled by only one action that makes it flap its wings. If it doesn't fly, it progresses in a decreasing trajectory determined by gravity. A screenshot of the environment is shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2000 image-border" src="assets/dab636b8-a119-4ae9-91f0-4ca19952eccf.png" style="width:13.67em;height:21.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to use the environment</h1>
                </header>
            
            <article>
                
<p>In the following steps, we will see how to use the environment.</p>
<ol>
<li>In order to use Flappy Bird in our <span><span>P</span></span>ython scripts, firstly, we need to import PLE and Flappy Bird:</li>
</ol>
<div>
<pre style="padding-left: 60px"><span>from</span><span> ple.games.flappybird </span><span>import</span><span> FlappyBird<br/></span><span>from</span><span> ple </span><span>import</span><span> </span><span>PLE</span></pre></div>
<ol start="2">
<li>Then, we instance a <kbd>FlappyBird</kbd> object and pass it to <kbd>PLE</kbd> with a few parameters:</li>
</ol>
<pre style="padding-left: 60px">game = FlappyBird()<br/>p = PLE(game, fps=30, display_screen=False)</pre>
<p style="padding-left: 60px">Here, with <kbd>display_screen</kbd>, you can choose whether to display the screen.</p>
<ol start="3">
<li>The environment is <span>initialized </span>by calling the <kbd>init()</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">p.init()</pre>
<p style="padding-left: 60px">To interact and get the state of the environment, we primarily use four functions:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><kbd>p.act(act)</kbd>, to execute the <kbd>act</kbd> <span>action i</span>n the game. <kbd>act(act)</kbd> returns the reward obtained from the action performed.</li>
<li><kbd>p.game_over()</kbd>, to check whether the game reached a final state.</li>
<li><kbd>p.reset_game()</kbd>, to reset the game to the initial conditions.</li>
<li><kbd>p.getGameState()</kbd>, to obtain the current state of the environment. We could also use <kbd>p.getScreenRGB()</kbd> if we want to obtain the RGB observations (that is, the full screen) of the environment. </li>
</ul>
</li>
</ul>
<ol start="4">
<li>Putting everything together, a simple script that plays Flappy Bird for five games can be designed as in the following code snippet. Note that in order to make it work, you still have to define the <kbd>get_action(state)</kbd> <span>function </span>that returns an action given a state:</li>
</ol>
<pre style="padding-left: 60px">from ple.games.flappybird import FlappyBird<br/>from ple import PLE<br/><br/>game = FlappyBird()<br/>p = PLE(game, fps=30, display_screen=False)<br/>p.init()<br/><br/>reward = 0<br/><br/>for _ in range(5):<br/>    reward += p.act(get_action(p.getGameState()))<br/><br/>    if p.game_over():<br/>        p.reset_game()</pre>
<p>A couple of things to point out here are as follows:</p>
<ul>
<li><kbd>getGameState()</kbd> returns a dictionary with the position, velocity, and the distance of the player, as well as the position of the next pipe and the following one. Before giving the state to the policymaker that we represented here with the <kbd>get_action</kbd> function, the dictionary is converted to a NumPy array and normalized.</li>
<li><kbd>act(action)</kbd> expects <kbd>None</kbd> <span>as input </span>if no action has to be performed, or <kbd>119</kbd> if the bird has to flap its wings in order to fly higher.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the dataset aggregation algorithm</h1>
                </header>
            
            <article>
                
<p>One of the most successful algorithms that learns from demonstrations is <strong>Dataset Aggregation</strong> (<strong><span>DAgger</span></strong>). This is an iterative policy meta-algorithm that performs well under the distribution of states induced. The most notable feature of DAgger is that it addresses the distribution mismatch by proposing an active method in which the expert teaches the learner how to recover from the learner's mistakes.</p>
<p>A classic IL algorithm learns a classifier that predicts expert behaviors. This means that the model fits <span>a dataset consisting of training examples, observed by an expert. The inputs are the observations, and the actions are the desired output values. However, following the previous reasoning, the predictions of the learner affect the future state or observation visited, violating the i.i.d assumption.</span></p>
<p><span>DAgger deals with the change in distribution by iterating a pipeline of aggregation of new data sampled from the learner multiple times, and training with the aggregated dataset. A simple diagram of the algorithm is shown here:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2001 image-border" src="assets/fe1faf16-a462-463c-a02b-3e0531dd25b7.png" style="width:223.92em;height:91.75em;"/></p>
<p><span>The expert populates the dataset used by the classifier, but, depending on the iteration, the action performed in the environment may come from the expert or the learner.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The DAgger algorithm</h1>
                </header>
            
            <article>
                
<p>Specifically, DAgger proceeds by iterating the following procedure. At the first iteration, a dataset <em>D</em> of trajectories is created from the expert policy and used to train a first policy <img class="fm-editor-equation" src="assets/dd21f2a8-7117-44ca-96d1-97e70f418dfc.png" style="width:1.25em;height:0.92em;"/> that best fits those trajectories without overfitting them. Then, during iteration <em>i</em>, new trajectories are collected with the learned policy <img class="fm-editor-equation" src="assets/f9898e60-eb25-48ed-a2df-4a73bbc2b431.png" style="width:1.08em;height:0.92em;"/> and added to the dataset <em>D</em>. After that, the aggregated dataset <em>D</em> with the new and old trajectories is used to train a new policy, <img class="fm-editor-equation" src="assets/34d844e4-4668-46b2-b95e-6621b653c169.png" style="width:2.42em;height:1.08em;"/>.</p>
<p class="mce-root">As per the report in the Dagger paper <span>(</span><a href="https://arxiv.org/pdf/1011.0686.pdf">https://arxiv.org/pdf/1011.0686.pdf</a><span>), </span>there is an active on-policy learning that outperforms many other imitation learning algorithms, and it's also able to learn very complex policies with the help of deep neural networks.</p>
<p>Additionally, at iteration <em>i</em>, the policy can be modified so that the expert takes control of a number of actions. This technique better leverages the expert and lets the learner <span>gradually </span>assume control over the environment.</p>
<p>The pseudocode of the algorithm can clarify this further:</p>
<pre>Initialize <img class="fm-editor-equation" src="assets/8947eff9-78a4-426e-8a15-2a9a40a07f81.png" style="width:2.67em;height:0.75em;"/><br/>Initialize <img class="fm-editor-equation" src="assets/5c1f4c33-8ca3-48d2-8aec-320d34e9888f.png" style="width:3.08em;height:1.00em;"/> (<img class="fm-editor-equation" src="assets/fc869bec-6da5-48e8-bc57-d5f51bb73dec.png" style="width:1.08em;height:0.92em;"/> is the expert policy)<br/><br/><strong>for</strong> i <img class="fm-editor-equation" src="assets/b3b4e545-face-4d13-a53d-e352577d319e.png" style="width:1.75em;height:0.92em;"/>:<br/>    &gt; Populate dataset <img class="fm-editor-equation" src="assets/65de4a07-84ca-4a9f-bc6d-37017b7ee821.png" style="width:1.17em;height:0.92em;"/> with <img class="fm-editor-equation" src="assets/2c7061e2-9d64-4273-ac0a-55f87495824a.png" style="width:4.00em;height:1.17em;"/>. States are given by <img class="fm-editor-equation" src="assets/4006c698-d6fd-40a0-842d-c3035d3aa9e3.png" style="width:1.00em;height:0.83em;"/> (sometimes the expert could take the control over it) and actions are given by the expert <img class="fm-editor-equation" src="assets/fd435a8f-612f-4ae6-acf8-4b1c795a5c52.png" style="width:1.08em;height:0.92em;"/><br/><br/>    &gt; Train a classifier <img class="fm-editor-equation" src="assets/1d322222-7353-4600-b0ab-2ec20dc3769f.png" style="width:1.92em;height:0.83em;"/> on the aggregate dataset <img class="fm-editor-equation" src="assets/e334a857-dd3f-4cbe-98e4-de3d0eb634fd.png" style="width:4.58em;height:0.83em;"/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation of DAgger</h1>
                </header>
            
            <article>
                
<p>The code is divided into three main parts:</p>
<ul>
<li>Load the expert inference function to predict an action given a state.</li>
<li>Create a computational graph for the learner.</li>
<li>Create the DAgger iterations to build the dataset and train the new policy.</li>
</ul>
<p>Here, we'll explain the most interesting parts, leaving the others for your personal interest. You can check the remaining code and the complete version in the book's GitHub repository.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the expert inference model</h1>
                </header>
            
            <article>
                
<p>The expert should be a policy that takes <span>a state</span> as input and returns the best action. Despite this, it can be anything. In particular, for these experiments, we used an agent trained with Proximal Policy Optimization (PPO) as the expert. In principle, this doesn't make any sense, but we adopted this solution for academic purposes, to facilitate integration with the imitation learning algorithms.</p>
<p><span>The expert's model</span> trained with PPO has been saved on file so that we can easily restore it with its trained weights. Three steps are required to restore the graph and make it usable:</p>
<ol>
<li>Import the meta graph. The computational graph can be restored with <kbd><span>tf.train.import_meta_graph</span></kbd>.</li>
<li><span>Restore the weights. Now, we have to load the pretrained weights on the computational graph we have just imported. The weights have been saved in the latest checkpoint and they can be restored with </span><kbd><span>tf.train.</span><span>latest_checkpoint(session, checkpoint)</span></kbd>.</li>
<li>Access the output tensors. The tensors of the restored graph are accessed with <kbd>graph.get_tensor_by_name(tensor_name)</kbd><span>, where <kbd>tensor_name</kbd> is the tensor's name in the graph.</span></li>
</ol>
<p><span>The following lines of code summarize the entire process:</span></p>
<div>
<pre><span>def expert():<br/>    graph </span><span>=</span><span> tf.</span><span>get_default_graph</span><span>()<br/></span><span>    sess_expert </span><span>=</span><span> tf.</span><span>Session</span><span>(</span><span>graph</span><span>=</span><span>graph)<br/><br/></span><span>    saver </span><span>=</span><span> tf.train.</span><span>import_meta_graph</span><span>(</span><span>'</span>expert/model.ckpt.meta<span>'</span><span>)<br/></span><span>    saver.</span><span>restore</span><span>(sess_expert,tf.train.</span><span>latest_checkpoint</span><span>(</span><span>'</span>expert/<span>'</span><span>))<br/><br/></span><span>    p_argmax </span><span>=</span><span> graph.</span><span>get_tensor_by_name</span><span>(</span><span>'actor_nn/max_act:0'</span><span>) <br/></span><span>    obs_ph </span><span>=</span><span> graph.</span><span>get_tensor_by_name</span><span>(</span><span>'obs:0'</span><span>) </span></pre></div>
<p>Then, because we are only interested in a simple function that returns an expert action given a state, we can design the <kbd>expert</kbd> function in such a way that it returns that function. Thus, inside <kbd>expert()</kbd>, we define an inner function called <kbd>expert_policy(state)</kbd> and return it as output of <kbd>expert()</kbd>:</p>
<div>
<pre><span>    def</span><span> </span><span>expert_policy</span><span>(</span><span>state</span><span>):<br/></span><span>        act </span><span>=</span><span> sess_expert.</span><span>run</span><span>(p_argmax, </span><span>feed_dict</span><span>=</span><span>{obs_ph:[state]})<br/></span><span>        return</span><span> np.</span><span>squeeze</span><span>(act)<br/><br/></span><span>    return</span><span> expert_policy</span></pre></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the learner's computational graph</h1>
                </header>
            
            <article>
                
<p>All the following code is located inside a function called <kbd>DAgger</kbd>, which takes <span>some hyperparameters that we'll see throughout the code</span> as arguments.</p>
<p>The learner's computational graph is simple as its only goal is to build a classifier. In our case, there are only two actions to predict, one for doing nothing, and the other to make the bird flap its wings. We can instantiate two placeholders, one for the input state, and one for the <em>ground-truth</em> actions that are those of the expert. The actions are an integer corresponding to the action taken. In the case of two possible actions, they are just 0 (do nothing) or 1 (fly).</p>
<p>The steps to build such a computational graph are the following:</p>
<ol>
<li>Create a deep neural network, specifically, a fully connected multilayer perceptron with a ReLu activations function in the hidden layers and a linear function on the final layer.</li>
<li>For every input state, take the action with the highest value. This is done using the <kbd>tf.math.argmax(tensor,axis)</kbd> function with <kbd>axis=1</kbd>.</li>
<li>Convert the action's placeholders in a one-hot tensor. This is needed because the logits and labels that we'll use in the loss function should have dimensions,<span> </span><kbd>[batch_size, num_classes]</kbd><span>. However, </span>our labels named <kbd>act_ph</kbd> have shapes,<span> </span><kbd>[batch_size]</kbd>. Therefore, we convert them to the desired shape with one-hot encoding.<span> </span><kbd>tf.one_hot</kbd><span> </span>is the TensorFlow function that does just that.</li>
<li>Create the loss function. We use the softmax cross-entropy loss function. This is a standard loss function used for discrete classification with mutually exclusive classes, just like in our case. The loss function is computed using <span><kbd>softmax_cross_entropy_with_logits_v2(labels, logits)</kbd> </span>between the logits and the labels.</li>
<li>Lastly, the mean of the softmax cross-entropy is computed across the batch and minimized using Adam.</li>
</ol>
<p>These five steps are implemented in the following lines:</p>
<div>
<pre><span>    obs_ph </span><span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>, obs_dim), </span><span>dtype</span><span>=</span><span>tf.float32, </span><span>name</span><span>=</span><span>'obs'</span><span>)<br/></span><span>    act_ph </span><span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>,), </span><span>dtype</span><span>=</span><span>tf.int32, </span><span>name</span><span>=</span><span>'act'</span><span>)<br/></span><span>    <br/>    p_logits </span><span>=</span><span> </span><span>mlp</span><span>(obs_ph, hidden_sizes, act_dim, tf.nn.relu, </span><span>last_activation</span><span>=</span><span>None</span><span>)<br/></span><span>    act_max </span><span>=</span><span> tf.math.</span><span>argmax</span><span>(p_logits, </span><span>axis</span><span>=</span><span>1</span><span>)<br/></span><span>    act_onehot </span><span>=</span><span> tf.</span><span>one_hot</span><span>(act_ph, </span><span>depth</span><span>=</span><span>act_dim)<br/></span><span>    <br/>    p_loss </span><span>=</span><span> tf.</span><span>reduce_mean</span><span>(tf.nn.</span><span>softmax_cross_entropy_with_logits_v2</span><span>(</span><span>labels</span><span>=</span><span>act_onehot, </span><span>logits</span><span>=</span><span>p_logits))<br/></span><span>    p_opt </span><span>=</span><span> tf.train.</span><span>AdamOptimizer</span><span>(p_lr).</span><span>minimize</span><span>(p_loss)</span></pre></div>
<p>We can then initialize a session, the global variables, and define a function, <kbd>learner_policy(state)</kbd>. This function, given a state, returns the action with a higher probability chosen by the learner (this is the same thing we did for the expert):</p>
<div>
<pre><span>    sess </span><span>=</span><span> tf.</span><span>Session</span><span>()<br/></span><span>    sess.</span><span>run</span><span>(tf.</span><span>global_variables_initializer</span><span>())<br/><br/></span><span>    def</span><span> </span><span>learner_policy</span><span>(</span><span>state</span><span>):<br/></span><span>        action </span><span>=</span><span> sess.</span><span>run</span><span>(act_max, </span><span>feed_dict</span><span>=</span><span>{obs_ph:[state]})<br/></span><span>        return</span><span> np.</span><span>squeeze</span><span>(action)</span></pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a DAgger loop</h1>
                </header>
            
            <article>
                
<p>It's now time to set up the core of the DAgger algorithm. The outline has already been defined in the pseudocode in <span><em>The DAgger algorithm</em> </span>section, but let's take a more in-depth look at how it works:</p>
<ol>
<li>Initialize the dataset composed of two lists, <kbd>X</kbd> and <kbd>y</kbd>, where we'll put the states visited and the expert target actions. We also initialize the environment:</li>
</ol>
<div>
<pre><span>    X </span><span>=</span><span> []<br/></span><span>    y </span><span>=</span><span> []<br/><br/></span><span>    env </span><span>=</span><span> </span><span>FlappyBird</span><span>()<br/></span><span>    env </span><span>=</span><span> </span><span>PLE</span><span>(env, </span><span>fps</span><span>=</span><span>30</span><span>, </span><span>display_screen</span><span>=</span><span>False</span><span>)<br/></span><span>    env.</span><span>init</span><span>() </span></pre></div>
<ol start="2">
<li><span>Iterate across all the DAgger iterations. A</span>t the beginning of every DAgger iteration, we have to reinitialize the learner computational graph <span>(because we retrain the learner on every iteration on the new dataset)</span>, reset the environment, and run a number of random actions. At the start of each game, we run a few random actions to add a stochastic component to the deterministic environment. The result will be a more robust policy:</li>
</ol>
<div>
<pre><span>    for</span><span> it </span><span>in</span><span> </span><span>range</span><span>(dagger_iterations):<br/></span><span>        sess.</span><span>run</span><span>(tf.</span><span>global_variables_initializer</span><span>())<br/></span><span>        env.</span><span>reset_game</span><span>()<br/></span><span>        no_op</span><span>(env)<br/><br/></span><span>        game_rew </span><span>=</span><span> </span><span>0<br/></span><span>        rewards </span><span>=</span><span> []</span></pre></div>
<p class="mce-root"/>
<ol start="3">
<li>Collect new data by interacting with the environment. As we said previously, the first iteration contains the expert that has to choose the actions by calling <kbd>expert_policy</kbd>, but, in the following iterations, the learner <span>progressively</span> takes control. The learned policy is executed by the <kbd>learner_policy</kbd> <span>function. </span>The dataset is collected by appending to <kbd>X</kbd> (the input variable) the current state of the game, and by appending to <kbd>y</kbd> (the output variable) the actions that the expert would have taken in that state. When the game is over, the game is reset and <kbd>game_rew</kbd> is set to <kbd>0</kbd>. The code is as follows:</li>
</ol>
<div>
<pre><span>        for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(step_iterations):<br/></span><span>            state </span><span>=</span><span> </span><span>flappy_game_state</span><span>(env)<br/><br/></span><span>            if</span><span> np.random.</span><span>rand</span><span>() </span><span>&lt;</span><span> (</span><span>1</span><span> </span><span>-</span><span> it</span><span>/</span><span>5</span><span>):<br/></span><span>                action </span><span>=</span><span> </span><span>expert_policy</span><span>(state)<br/></span><span>            else</span><span>:<br/></span><span>                action </span><span>=</span><span> </span><span>learner_policy</span><span>(state)<br/><br/></span><span>            action </span><span>=</span><span> </span><span>119</span><span> </span><span>if</span><span> action </span><span>==</span><span> </span><span>1</span><span> </span><span>else</span><span> </span><span>None<br/><br/></span><span>            rew </span><span>=</span><span> env.</span><span>act</span><span>(action)<br/></span><span>            rew </span><span>+=</span><span> env.</span><span>act</span><span>(action)<br/><br/></span><span>            X.</span><span>append</span><span>(state)<br/></span><span>            y.</span><span>append</span><span>(</span><span>expert_policy</span><span>(state)) <br/></span><span>            game_rew </span><span>+=</span><span> rew<br/><br/></span><span>            if</span><span> env.</span><span>game_over</span><span>():<br/></span><span>                env.</span><span>reset_game</span><span>()<br/>                np_op(env)<br/><br/></span><span>                rewards.</span><span>append</span><span>(game_rew)<br/></span><span>                game_rew </span><span>=</span><span> </span><span>0</span></pre></div>
<p style="padding-left: 60px">Note that the actions are performed twice. This is done to reduce the number of actions every second to 15 instead of 30, as required by the environment.</p>
<ol start="4">
<li>Train the new policy on the aggregated dataset. The pipeline is standard. The dataset is shuffled and divided into mini-batches of length <kbd>batch_size</kbd>. Then, t<span>he optimization is repeated by running <kbd>p_opt</kbd> for a number of epochs equals to </span><kbd>train_epochs</kbd><span> </span>on each mini-batch. This is done with the following code:</li>
</ol>
<div>
<pre><span>        n_batches </span><span>=</span><span> </span><span>int</span><span>(np.</span><span>floor</span><span>(</span><span>len</span><span>(X)</span><span>/</span><span>batch_size))<br/>        <br/>        </span><span>shuffle </span><span>=</span><span> np.</span><span>arange</span><span>(</span><span>len</span><span>(X))<br/></span><span>        np.random.</span><span>shuffle</span><span>(shuffle)<br/></span><span>        shuffled_X </span><span>=</span><span> np.</span><span>array</span><span>(X)[shuffle]<br/></span><span>        shuffled_y </span><span>=</span><span> np.</span><span>array</span><span>(y)[shuffle]<br/></span><span><br/>        ep_loss = []<br/>            for</span><span> _ </span><span>in</span><span> </span><span>range</span><span>(train_epochs):<br/><br/>                </span><span>for</span><span> b </span><span>in</span><span> </span><span>range</span><span>(n_batches):<br/>                    </span><span>p_start </span><span>=</span><span> b</span><span>*</span><span>batch_size<br/>                    </span><span>tr_loss, _ </span><span>=</span><span> sess.</span><span>run</span><span>([p_loss, p_opt], </span><span>feed_dict</span><span>=<br/>                            </span><span>obs_ph:shuffled_X[p_start:p_start</span><span>+</span><span>batch_size], <br/></span><span>                            act_ph:shuffled_y[p_start:p_start</span><span>+</span><span>batch_size]})<br/></span><span> <br/>                    ep_loss.</span><span>append</span><span>(tr_loss)<br/>        </span>print<span>(</span><span>'Ep:'</span><span>, it, np.</span><span>mean</span><span>(ep_loss), </span><span>'Test:'</span><span>, np.</span><span>mean</span><span>(</span><span>test_agent</span><span>(learner_policy)))</span></pre></div>
<p style="padding-left: 60px"><kbd>test_agent</kbd> tests <kbd>learner_policy</kbd> on a few games to understand how well the learner is performing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the results on Flappy Bird</h1>
                </header>
            
            <article>
                
<p>Before showing the results of the imitation learning approach, we want to provide some numbers so that you can compare these with those of a reinforcement learning algorithm. <span>We know that this is not a fair comparison (the two algorithms work on very different conditions), but nevertheless, they underline why imitation learning can be rewarding when an expert is available.</span></p>
<p>The expert has been trained with proximal policy optimization for about 2 million steps and, after about 400,000 steps, reached a plateau score of about 138.</p>
<p>We tested DAgger on Flappy Bird with the following hyperparameters:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 38%" class="CDPAlignCenter CDPAlign"><strong>Hyperparameter</strong></td>
<td style="width: 34.9274%" class="CDPAlignCenter CDPAlign"><strong>Variable name</strong></td>
<td style="width: 25.4598%" class="CDPAlignCenter CDPAlign"><strong>Value</strong></td>
</tr>
<tr>
<td style="width: 38%" class="CDPAlignCenter CDPAlign">Learner hidden layers</td>
<td style="width: 34.9274%" class="CDPAlignCenter CDPAlign">hidden_sizes</td>
<td style="width: 25.4598%" class="CDPAlignCenter CDPAlign">16,16</td>
</tr>
<tr>
<td style="width: 38%" class="CDPAlignCenter CDPAlign">DAgger iterations</td>
<td style="width: 34.9274%" class="CDPAlignCenter CDPAlign">dagger_iterations</td>
<td style="width: 25.4598%" class="CDPAlignCenter CDPAlign">8</td>
</tr>
<tr>
<td style="width: 38%" class="CDPAlignCenter CDPAlign">Learning rate</td>
<td style="width: 34.9274%" class="CDPAlignCenter CDPAlign">p_lr</td>
<td style="width: 25.4598%" class="CDPAlignCenter CDPAlign">1e-4</td>
</tr>
<tr>
<td style="width: 38%" class="CDPAlignCenter CDPAlign">Number of steps for every DAgger iteration</td>
<td style="width: 34.9274%" class="CDPAlignCenter CDPAlign">step_iterations</td>
<td style="width: 25.4598%" class="CDPAlignCenter CDPAlign">100</td>
</tr>
<tr>
<td style="width: 38%" class="CDPAlignCenter CDPAlign">Mini-batch size</td>
<td style="width: 34.9274%" class="CDPAlignCenter CDPAlign">batch_size</td>
<td style="width: 25.4598%" class="CDPAlignCenter CDPAlign">50</td>
</tr>
<tr>
<td style="width: 38%" class="CDPAlignCenter CDPAlign">Training epochs</td>
<td style="width: 34.9274%" class="CDPAlignCenter CDPAlign">train_epochs</td>
<td style="width: 25.4598%" class="CDPAlignCenter CDPAlign">2000</td>
</tr>
</tbody>
</table>
<p class="mce-root"/>
<p>The plot in the following screenshot shows the trend of the performance of DAgger with respect to the number of steps taken:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2002 image-border" src="assets/48524175-156e-43d5-aa99-ff195df2dec0.png" style="width:45.92em;height:27.83em;"/></p>
<p>The horizontal line represents the average performance reached by the expert. From the results, we can see that a few hundred steps are sufficient to reach the performance of the expert. However, compared with the experience required by PPO to train the expert, this represents about a 100-fold increase in sample efficiency.</p>
<p>Again, this is not a fair comparison as the methods are in different contexts, but it highlights that whenever an expert is available, it is suggested that you use an imitation learning approach (perhaps at least to learn a starting policy).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">IRL</h1>
                </header>
            
            <article>
                
<p>One of the biggest limitations of IL lies in its inability to learn other trajectories to reach a goal, except those learned from the expert. By imitating an expert, the learner is constrained to the range of behaviors of its teacher. They are not aware of the end goal that the expert is trying to reach. Thus, these methods are only useful when there's no intention to perform better than the teacher.</p>
<p class="mce-root"/>
<p>IRL is an RL algorithm, such as IL, that uses an expert to learn. The difference is that IRL uses the expert to learn its reward function. Therefore, instead of copying the demonstrations, as is done in imitation learning, IRL figures out the goal of the expert. Once the reward function is learned, the agent uses it to learn the policy.</p>
<p>With the demonstrations used <span>only </span>to understand the goal of the expert, the agent is not bound to the actions of the teacher and can finally learn better strategies. For example, a self-driving car that learns by IRL would understand that the goal is to go from point A to point B in the minimum amount of time, while reducing the damage to things and people. The car would then learn a policy by itself <span>(for example, with an RL algorithm) </span>that maximizes this reward function.</p>
<p>However, IRL also has a number of challenges that limit its applicability. The expert's demonstration may not be optimal, and, as a result, the learner may not be able to achieve its full potential and may remain stuck in the wrong reward function. The other challenge lies in the evaluation of the learned reward function.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we took a break from reinforcement learning algorithms and explored a new type of learning called imitation learning. The novelty of this new paradigm lies in the way in which the learning takes place; that is, the resulting policy imitates the behavior of an expert. This paradigm differentiates from reinforcement learning in the absence of a reward signal and in its ability <span>to leverage the incredible source of information brought by the expert entity.</span></p>
<p>We saw that the dataset from which the learner learns can be expanded with additional state action pairs to increase the confidence of the learner in new situations. This process is called data aggregation. Moreover, new data could come from the new learned policy and, in this case, we talked about on-policy data (as it comes from the same policy learned). This integration of on-policy states with expert feedback is a very valuable approach that increases the quality of the learner.</p>
<p>We then explored and developed one of the most successful imitation learning algorithms, called DAgger, and applied it to learn the Flappy Bird <span>game</span>.</p>
<p>However, because imitation learning algorithms only copy the behavior of an expert, these systems cannot do better than the expert. Therefore, we introduced inverse reinforcement learning, which overcomes this problem by inferring the reward function from the expert. In this way, the policy can be learned independently of the teacher. </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>In the next chapter, we'll take a look at another set of algorithms for solving sequential tasks; namely, evolutionary algorithms. You'll learn the mechanisms and advantages of these black-box optimization algorithms so that you'll be able to adopt them in challenging environments. Furthermore, we'll delve into an evolutionary algorithm<span> </span>called evolution strategy in greater depth and implement it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Is imitation learning considered a reinforcement learning technique?</li>
<li>Would you use imitation learning to build an unbitable agent in Go?</li>
<li>What's the full name of DAgger?</li>
<li>What's the main strength of DAgger?</li>
<li>Where would you use IRL instead of IL?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>To read the original paper that introduced DAgger, checkout the following paper, <em><span>A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</span></em>: <a href="https://arxiv.org/pdf/1011.0686.pdf">https://arxiv.org/pdf/1011.0686.pdf</a>.</li>
<li>To learn more about imitation learning algorithms, checkout the following paper, <em>Global Overview of Imitation Learning</em>: <a href="https://arxiv.org/pdf/1801.06503.pdf">https://arxiv.org/pdf/1801.06503.pdf</a>.</li>
<li><span>To learn more about inverse reinforcement learning, checkout the following survey, <em>A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress</em>: <a href="https://arxiv.org/pdf/1806.06877.pdf">https://arxiv.org/pdf/1806.06877.pdf</a>.</span></li>
</ul>


            </article>

            
        </section>
    </body></html>