["```\nimport dgl\nimport dgl.data\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom dgl.nn.tensorflow import GraphConv \n```", "```\ndataset = dgl.data.CoraGraphDataset() \n```", "```\n NumNodes: 2708\n  NumEdges: 10556\n  NumFeats: 1433\n  NumClasses: 7\n  NumTrainingSamples: 140\n  NumValidationSamples: 500\n  NumTestSamples: 1000\nDone saving data into cached files. \n```", "```\nclass NodeClassifier(tf.keras.Model):\n  def __init__(self, g, in_feats, h_feats, num_classes):\n    super(NodeClassifier, self).__init__()\n    self.g = g\n    self.conv1 = GraphConv(in_feats, h_feats, activation=tf.nn.relu)\n    self.conv2 = GraphConv(h_feats, num_classes)\n  def call(self, in_feat):\n    h = self.conv1(self.g, in_feat)\n    h = self.conv2(self.g, h)\n    return h\ng = dataset[0]\nmodel = NodeClassifier(\n  g, g.ndata[\"feat\"].shape[1], 16, dataset.num_classes) \n```", "```\ndef set_gpu_if_available():\n  device = \"/cpu:0\"\n  gpus = tf.config.list_physical_devices(\"GPU\")\n  if len(gpus) > 0:\n    device = gpus[0]\n  return device\ndevice = set_gpu_if_available()\ng = g.to(device) \n```", "```\ndef do_eval(model, features, labels, mask):\n  logits = model(features, training=False)\n  logits = logits[mask]\n  labels = labels[mask]\n  preds = tf.math.argmax(logits, axis=1)\n  acc = tf.reduce_mean(tf.cast(preds == labels, dtype=tf.float32))\n  return acc.numpy().item() \n```", "```\nNUM_HIDDEN = 16\nLEARNING_RATE = 1e-2\nWEIGHT_DECAY = 5e-4\nNUM_EPOCHS = 200\nwith tf.device(device):\n  feats = g.ndata[\"feat\"]\n  labels = g.ndata[\"label\"]\n  train_mask = g.ndata[\"train_mask\"]\n  val_mask = g.ndata[\"val_mask\"]\n  test_mask = g.ndata[\"test_mask\"]\n  in_feats = feats.shape[1]\n  n_classes = dataset.num_classes\n  n_edges = dataset[0].number_of_edges()\n  model = NodeClassifier(g, in_feats, NUM_HIDDEN, n_classes)\n  loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n  optimizer = tfa.optimizers.AdamW(\n    learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n  best_val_acc, best_test_acc = 0, 0\n  history = []\n  for epoch in range(NUM_EPOCHS):\n    with tf.GradientTape() as tape:\n      logits = model(feats)\n      loss = loss_fcn(labels[train_mask], logits[train_mask])\n      grads = tape.gradient(loss, model.trainable_weights)\n      optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n    val_acc = do_eval(model, feats, labels, val_mask)\n    history.append((epoch + 1, loss.numpy().item(), val_acc))\n    if epoch % 10 == 0:\n      print(\"Epoch {:3d} | train loss: {:.3f} | val acc: {:.3f}\".format(\n         epoch, loss.numpy().item(), val_acc)) \n```", "```\nEpoch   0 | train loss: 1.946 | val acc: 0.134\nEpoch  10 | train loss: 1.836 | val acc: 0.544\nEpoch  20 | train loss: 1.631 | val acc: 0.610\nEpoch  30 | train loss: 1.348 | val acc: 0.688\nEpoch  40 | train loss: 1.032 | val acc: 0.732\nEpoch  50 | train loss: 0.738 | val acc: 0.760\nEpoch  60 | train loss: 0.504 | val acc: 0.774\nEpoch  70 | train loss: 0.340 | val acc: 0.776\nEpoch  80 | train loss: 0.233 | val acc: 0.780\nEpoch  90 | train loss: 0.164 | val acc: 0.780\nEpoch 100 | train loss: 0.121 | val acc: 0.784\nEpoch 110 | train loss: 0.092 | val acc: 0.784\nEpoch 120 | train loss: 0.073 | val acc: 0.784\nEpoch 130 | train loss: 0.059 | val acc: 0.784\nEpoch 140 | train loss: 0.050 | val acc: 0.786\nEpoch 150 | train loss: 0.042 | val acc: 0.786\nEpoch 160 | train loss: 0.037 | val acc: 0.786\nEpoch 170 | train loss: 0.032 | val acc: 0.784\nEpoch 180 | train loss: 0.029 | val acc: 0.784\nEpoch 190 | train loss: 0.026 | val acc: 0.784 \n```", "```\ntest_acc = do_eval(model, feats, labels, test_mask)\nprint(\"Test acc: {:.3f}\".format(test_acc)) \n```", "```\nTest acc: 0.779 \n```", "```\nimport dgl.data\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nfrom dgl.nn import GraphConv\nfrom sklearn.model_selection import train_test_split \n```", "```\ndataset = dgl.data.GINDataset(\"PROTEINS\", self_loop=True)\nprint(\"node feature dimensionality:\", dataset.dim_nfeats)\nprint(\"number of graph categories:\", dataset.gclasses)\nprint(\"number of graphs in dataset:\", len(dataset)) \n```", "```\nnode feature dimensionality: 3\nnumber of graph categories: 2\nnumber of graphs in dataset: 1113 \n```", "```\ntv_dataset, test_dataset = train_test_split(\n  dataset, shuffle=True, test_size=0.2)\ntrain_dataset, val_dataset = train_test_split(\n  tv_dataset, test_size=0.1)\nprint(len(train_dataset), len(val_dataset), len(test_dataset)) \n```", "```\nclass GraphClassifier(tf.keras.Model):\n  def __init__(self, in_feats, h_feats, num_classes):\n    super(GraphClassifier, self).__init__()\n    self.conv1 = GraphConv(in_feats, h_feats, activation=tf.nn.relu)\n    self.conv2 = GraphConv(h_feats, num_classes)\n  def call(self, g, in_feat):\n    h = self.conv1(g, in_feat)\n    h = self.conv2(g, h)\n    g.ndata[\"h\"] = h\n    return dgl.mean_nodes(g, \"h\") \n```", "```\nHIDDEN_SIZE = 16\nBATCH_SIZE = 16\nLEARNING_RATE = 1e-2\nNUM_EPOCHS = 20\ndevice = set_gpu_if_available()\ndef do_eval(model, dataset):\n  total_acc, total_recs = 0, 0\n  indexes = tf.data.Dataset.from_tensor_slices(range(len(dataset)))\n  indexes = indexes.batch(batch_size=BATCH_SIZE)\n  for batched_indexes in indexes:\n    graphs, labels = zip(*[dataset[i] for i in batched_indexes])\n    batched_graphs = dgl.batch(graphs)\n    batched_labels = tf.convert_to_tensor(labels, dtype=tf.int64)\n    batched_graphs = batched_graphs.to(device)\n    logits = model(batched_graphs, batched_graphs.ndata[\"attr\"])\n    batched_preds = tf.math.argmax(logits, axis=1)\n    acc = tf.reduce_sum(tf.cast(batched_preds == batched_labels,\n                                dtype=tf.float32))\n    total_acc += acc.numpy().item()\n    total_recs += len(batched_labels)\n  return total_acc / total_recs \n```", "```\nwith tf.device(device):\n  model = GraphClassifier(\n    dataset.dim_nfeats, HIDDEN_SIZE, dataset.gclasses)\n  optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n  loss_fcn = tf.keras.losses.SparseCategoricalCrossentropy(\n    from_logits=True)\n  train_indexes = tf.data.Dataset.from_tensor_slices(\n    range(len(train_dataset)))\n  train_indexes = train_indexes.batch(batch_size=BATCH_SIZE)\n  for epoch in range(NUM_EPOCHS):\n    total_loss = 0\n    for batched_indexes in train_indexes:\n      with tf.GradientTape() as tape:\n        graphs, labels = zip(*[train_dataset[i] for i in batched_indexes])\n        batched_graphs = dgl.batch(graphs)\n        batched_labels = tf.convert_to_tensor(labels, dtype=tf.int32)\n        batched_graphs = batched_graphs.to(device)\n        logits = model(batched_graphs, batched_graphs.ndata[\"attr\"])\n        loss = loss_fcn(batched_labels, logits)\n        grads = tape.gradient(loss, model.trainable_weights)\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n        total_loss += loss.numpy().item()\n\n    val_acc = do_eval(model, val_dataset)\n    print(\"Epoch {:3d} | train_loss: {:.3f} | val_acc: {:.3f}\".format(\n        epoch, total_loss, val_acc)) \n```", "```\nEpoch   0 | train_loss: 34.401 | val_acc: 0.629\nEpoch   1 | train_loss: 33.868 | val_acc: 0.629\nEpoch   2 | train_loss: 33.554 | val_acc: 0.618\nEpoch   3 | train_loss: 33.184 | val_acc: 0.640\nEpoch   4 | train_loss: 32.822 | val_acc: 0.652\nEpoch   5 | train_loss: 32.499 | val_acc: 0.663\nEpoch   6 | train_loss: 32.227 | val_acc: 0.663\nEpoch   7 | train_loss: 32.009 | val_acc: 0.697\nEpoch   8 | train_loss: 31.830 | val_acc: 0.685\nEpoch   9 | train_loss: 31.675 | val_acc: 0.685\nEpoch  10 | train_loss: 31.580 | val_acc: 0.685\nEpoch  11 | train_loss: 31.525 | val_acc: 0.708\nEpoch  12 | train_loss: 31.485 | val_acc: 0.708\nEpoch  13 | train_loss: 31.464 | val_acc: 0.708\nEpoch  14 | train_loss: 31.449 | val_acc: 0.708\nEpoch  15 | train_loss: 31.431 | val_acc: 0.708\nEpoch  16 | train_loss: 31.421 | val_acc: 0.708\nEpoch  17 | train_loss: 31.411 | val_acc: 0.708\nEpoch  18 | train_loss: 31.404 | val_acc: 0.719\nEpoch  19 | train_loss: 31.398 | val_acc: 0.719 \n```", "```\ntest_acc = do_eval(model, test_dataset)\nprint(\"test accuracy: {:.3f}\".format(test_acc)) \n```", "```\ntest accuracy: 0.677 \n```", "```\nimport dgl\nimport dgl.data\nimport dgl.function as fn\nimport tensorflow as tf\nimport itertools\nimport numpy as np\nimport scipy.sparse as sp\nfrom dgl.nn import SAGEConv\nfrom sklearn.metrics import roc_auc_score \n```", "```\ndataset = dgl.data.CoraGraphDataset()\ng = dataset[0] \n```", "```\nu, v = g.edges()\n# positive edges\neids = np.arange(g.number_of_edges())\neids = np.random.permutation(eids)\ntest_size = int(len(eids) * 0.2)\nval_size = int((len(eids) - test_size) * 0.1)\ntrain_size = g.number_of_edges() - test_size - val_size\nu = u.numpy()\nv = v.numpy()\ntest_pos_u = u[eids[0:test_size]]\ntest_pos_v = v[eids[0:test_size]]\nval_pos_u = u[eids[test_size:test_size + val_size]]\nval_pos_v = v[eids[test_size:test_size + val_size]]\ntrain_pos_u = u[eids[test_size + val_size:]]\ntrain_pos_v = v[eids[test_size + val_size:]]\n# negative edges\nadj = sp.coo_matrix((np.ones(len(u)), (u, v)))\nadj_neg = 1 - adj.todense() - np.eye(g.number_of_nodes())\nneg_u, neg_v = np.where(adj_neg != 0)\nneg_eids = np.random.choice(len(neg_u), g.number_of_edges())\ntest_neg_u = neg_u[neg_eids[:test_size]]\ntest_neg_v = neg_v[neg_eids[:test_size]]\nval_neg_u = neg_u[neg_eids[test_size:test_size + val_size]]\nval_neg_v = neg_v[neg_eids[test_size:test_size + val_size]]\ntrain_neg_u = neg_u[neg_eids[test_size + val_size:]]\ntrain_neg_v = neg_v[neg_eids[test_size + val_size:]]\n# remove edges from training graph\ntest_edges = eids[:test_size]\nval_edges = eids[test_size:test_size + val_size]\ntrain_edges = eids[test_size + val_size:]\ntrain_g = dgl.remove_edges(g, np.concatenate([test_edges, val_edges])) \n```", "```\nclass LinkPredictor(tf.keras.Model):\n  def __init__(self, g, in_feats, h_feats):\n    super(LinkPredictor, self).__init__()\n    self.g = g\n    self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n    self.relu1 = tf.keras.layers.Activation(tf.nn.relu)\n    self.conv2 = SAGEConv(h_feats, h_feats, 'mean')\n  def call(self, in_feat):\n    h = self.conv1(self.g, in_feat)\n    h = self.relu1(h)\n    h = self.conv2(self.g, h)\n    return h \n```", "```\ntrain_pos_g = dgl.graph((train_pos_u, train_pos_v), \n  num_nodes=g.number_of_nodes())\ntrain_neg_g = dgl.graph((train_neg_u, train_neg_v), \n  num_nodes=g.number_of_nodes())\nval_pos_g = dgl.graph((val_pos_u, val_pos_v), \n  num_nodes=g.number_of_nodes())\nval_neg_g = dgl.graph((val_neg_u, val_neg_v), \n  num_nodes=g.number_of_nodes())\ntest_pos_g = dgl.graph((test_pos_u, test_pos_v), \n  num_nodes=g.number_of_nodes())\ntest_neg_g = dgl.graph((test_neg_u, test_neg_v), \n  num_nodes=g.number_of_nodes()) \n```", "```\nclass DotProductPredictor(tf.keras.Model):\n  def call(self, g, h):\n    with g.local_scope():\n      g.ndata['h'] = h\n      # Compute a new edge feature named 'score' by a dot-product \n      # between the source node feature 'h' and destination node \n      # feature 'h'.\n      g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n      # u_dot_v returns a 1-element vector for each edge so you \n      # need to squeeze it.\n      return g.edata['score'][:, 0] \n```", "```\nclass MLPPredictor(tf.keras.Model):\n  def __init__(self, h_feats):\n    super().__init__()\n    self.W1 = tf.keras.layers.Dense(h_feats, activation=tf.nn.relu)\n    self.W2 = tf.keras.layers.Dense(1)\n  def apply_edges(self, edges):\n    h = tf.concat([edges.src[\"h\"], edges.dst[\"h\"]], axis=1)\n    return {\n      \"score\": self.W2(self.W1(h))[:, 0]\n    }\n  def call(self, g, h):\n    with g.local_scope():\n      g.ndata['h'] = h\n      g.apply_edges(self.apply_edges)\n      return g.edata['score'] \n```", "```\nHIDDEN_SIZE = 16\nLEARNING_RATE = 1e-2\nNUM_EPOCHS = 100\nmodel = LinkPredictor(train_g, train_g.ndata['feat'].shape[1], \n    HIDDEN_SIZE)\noptimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nloss_fcn = tf.keras.losses.BinaryCrossentropy(from_logits=True)\npred = DotProductPredictor() \n```", "```\ndef compute_loss(pos_score, neg_score):\n    scores = tf.concat([pos_score, neg_score], axis=0)\n    labels = tf.concat([\n      tf.ones(pos_score.shape[0]),\n      tf.zeros(neg_score.shape[0])\n    ], axis=0\n)\n    return loss_fcn(labels, scores)\ndef compute_auc(pos_score, neg_score):\n    scores = tf.concat([pos_score, neg_score], axis=0).numpy()\n    labels = tf.concat([\n      tf.ones(pos_score.shape[0]),\n      tf.zeros(neg_score.shape[0])\n    ], axis=0).numpy()\n    return roc_auc_score(labels, scores) \n```", "```\nfor epoch in range(NUM_EPOCHS):\n  in_feat = train_g.ndata[\"feat\"]\n  with tf.GradientTape() as tape:\n    h = model(in_feat)\n    pos_score = pred(train_pos_g, h)\n    neg_score = pred(train_neg_g, h)\n    loss = compute_loss(pos_score, neg_score)\n    grads = tape.gradient(loss, model.trainable_weights)\n    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n  val_pos_score = pred(val_pos_g, h)\n  val_neg_score = pred(val_neg_g, h)\n  val_auc = compute_auc(val_pos_score, val_neg_score)\n  if epoch % 5 == 0:\n    print(\"Epoch {:3d} | train_loss: {:.3f}, val_auc: {:.3f}\".format(\n      epoch, loss, val_auc)) \n```", "```\nEpoch   0 | train_loss: 0.693, val_auc: 0.566\nEpoch   5 | train_loss: 0.681, val_auc: 0.633\nEpoch  10 | train_loss: 0.626, val_auc: 0.746\nEpoch  15 | train_loss: 0.569, val_auc: 0.776\nEpoch  20 | train_loss: 0.532, val_auc: 0.805\nEpoch  25 | train_loss: 0.509, val_auc: 0.820\nEpoch  30 | train_loss: 0.492, val_auc: 0.824\nEpoch  35 | train_loss: 0.470, val_auc: 0.833\nEpoch  40 | train_loss: 0.453, val_auc: 0.835\nEpoch  45 | train_loss: 0.431, val_auc: 0.842\nEpoch  50 | train_loss: 0.410, val_auc: 0.851\nEpoch  55 | train_loss: 0.391, val_auc: 0.859\nEpoch  60 | train_loss: 0.371, val_auc: 0.861\nEpoch  65 | train_loss: 0.350, val_auc: 0.861\nEpoch  70 | train_loss: 0.330, val_auc: 0.861\nEpoch  75 | train_loss: 0.310, val_auc: 0.862\nEpoch  80 | train_loss: 0.290, val_auc: 0.860\nEpoch  85 | train_loss: 0.269, val_auc: 0.856\nEpoch  90 | train_loss: 0.249, val_auc: 0.852\nEpoch  95 | train_loss: 0.228, val_auc: 0.848 \n```", "```\npos_score = tf.stop_gradient(pred(test_pos_g, h))\nneg_score = tf.stop_gradient(pred(test_neg_g, h))\nprint('Test AUC', compute_auc(pos_score, neg_score)) \n```", "```\nTest AUC 0.8266960571287392 \n```", "```\nimport dgl\nimport dgl.data\nimport dgl.function as fn\nimport tensorflow as tf\nclass CustomGraphSAGE(tf.keras.layers.Layer):\n  def __init__(self, in_feat, out_feat):\n    super(CustomGraphSAGE, self).__init__()\n    # A linear submodule for projecting the input and neighbor \n    # feature to the output.\n    self.linear = tf.keras.layers.Dense(out_feat, activation=tf.nn.relu)\n  def call(self, g, h):\n    with g.local_scope():\n        g.ndata[\"h\"] = h\n        # update_all is a message passing API.\n        g.update_all(message_func=fn.copy_u('h', 'm'),\n                     reduce_func=fn.mean('m', 'h_N'))\n        h_N = g.ndata['h_N']\n        h_total = tf.concat([h, h_N], axis=1)\n        return self.linear(h_total) \n```", "```\nclass CustomGNN(tf.keras.Model):\n  def __init__(self, g, in_feats, h_feats, num_classes):\n    super(CustomGNN, self).__init__()\n    self.g = g\n    self.conv1 = CustomGraphSAGE(in_feats, h_feats)\n    self.relu1 = tf.keras.layers.Activation(tf.nn.relu)\n    self.conv2 = CustomGraphSAGE(h_feats, num_classes)\n  def call(self, in_feat):\n    h = self.conv1(self.g, in_feat)\n    h = self.relu1(h)\n    h = self.conv2(self.g, h)\n    return h \n```", "```\nclass CustomWeightedGraphSAGE(tf.keras.layers.Layer):\n  def __init__(self, in_feat, out_feat):\n    super(CustomWeightedGraphSAGE, self).__init__()\n    # A linear submodule for projecting the input and neighbor \n    # feature to the output.\n    self.linear = tf.keras.layers.Dense(out_feat, activation=tf.nn.relu)\n  def call(self, g, h, w):\n    with g.local_scope():\n      g.ndata['h'] = h\n      g.edata['w'] = w\n      g.update_all(message_func=fn.u_mul_e('h', 'w', 'm'),\n                   reduce_func=fn.mean('m', 'h_N'))\n      h_N = g.ndata['h_N']\n      h_total = tf.concat([h, h_N], axis=1)\n      return self.linear(h_total) \n```", "```\ng.edata[\"w\"] = tf.cast(\n   tf.random.uniform((g.num_edges(), 1), minval=3, maxval=10, \n                     dtype=tf.int32),\n   dtype=tf.float32) \n```", "```\nclass KarateClubDataset(DGLDataset):\n  def __init__(self):\n    super().__init__(name=\"karate_club\")\n  def __getitem__(self, i):\n    return self.graph\n  def __len__(self):\n    return 1\n  def process(self):\n    G = nx.karate_club_graph()\n    nodes = [node for node in G.nodes]\n    edges = [edge for edge in G.edges]\n    node_features = tf.random.uniform(\n        (len(nodes), 10), minval=0, maxval=1, dtype=tf.dtypes.float32)\n    label2int = {\"Mr. Hi\": 0, \"Officer\": 1}\n    node_labels = tf.convert_to_tensor(\n        [label2int[G.nodes[node][\"club\"]] for node in nodes])\n    edge_features = tf.random.uniform(\n        (len(edges), 1), minval=3, maxval=10, dtype=tf.dtypes.int32)\n    edges_src = tf.convert_to_tensor([u for u, v in edges])\n    edges_dst = tf.convert_to_tensor([v for u, v in edges])\n    self.graph = dgl.graph((edges_src, edges_dst), num_nodes=len(nodes))\n    self.graph.ndata[\"feat\"] = node_features\n    self.graph.ndata[\"label\"] = node_labels\n    self.graph.edata[\"weight\"] = edge_features\n    # assign masks indicating the split (training, validation, test)\n    n_nodes = len(nodes)\n    n_train = int(n_nodes * 0.6)\n    n_val = int(n_nodes * 0.2)\n    train_mask = tf.convert_to_tensor(\n      np.hstack([np.ones(n_train), np.zeros(n_nodes - n_train)]),\n      dtype=tf.bool)\n    val_mask = tf.convert_to_tensor(\n      np.hstack([np.zeros(n_train), np.ones(n_val), \n                 np.zeros(n_nodes - n_train - n_val)]),\n      dtype=tf.bool)\n    test_mask = tf.convert_to_tensor(\n      np.hstack([np.zeros(n_train + n_val), \n                 np.ones(n_nodes - n_train - n_val)]),\n      dtype=tf.bool)\n    self.graph.ndata[\"train_mask\"] = train_mask\n    self.graph.ndata[\"val_mask\"] = val_mask\n    self.graph.ndata[\"test_mask\"] = test_mask \n```", "```\ndataset = KarateClubDataset()\ng = dataset[0]\nprint(g) \n```", "```\nGraph(num_nodes=34, \n      num_edges=78,\n      ndata_schemes={\n        'feat': Scheme(shape=(10,), dtype=tf.float32),\n        'label': Scheme(shape=(), dtype=tf.int32),\n        'train_mask': Scheme(shape=(), dtype=tf.bool),\n        'val_mask': Scheme(shape=(), dtype=tf.bool),\n        'test_mask': Scheme(shape=(), dtype=tf.bool)\n      }\n      edata_schemes={\n         'weight': Scheme(shape=(1,), dtype=tf.int32)\n      }\n) \n```", "```\nfrom networkx.exception import NetworkXError\nclass SyntheticDataset(DGLDataset):\n  def __init__(self):\n    super().__init__(name=\"synthetic\")\n  def __getitem__(self, i):\n    return self.graphs[i], self.labels[i]\n  def __len__(self):\n    return len(self.graphs)\n  def process(self):\n    self.graphs, self.labels = [], []\n    num_graphs = 0\n    while(True):\n      d = np.random.randint(3, 10)\n      n = np.random.randint(5, 10)\n      if ((n * d) % 2) != 0:\n        continue\n      if n < d:\n        continue\n      try:\n        g = nx.random_regular_graph(d, n)\n      except NetworkXError:\n        continue\n      g_edges = [edge for edge in g.edges]\n      g_src = [u for u, v in g_edges]\n      g_dst = [v for u, v in g_edges]\n      g_num_nodes = len(g.nodes)\n      label = np.random.randint(0, 2)\n      # create graph and add to list of graphs and labels\n      dgl_graph = dgl.graph((g_src, g_dst), num_nodes=g_num_nodes)\n      dgl_graph.ndata[\"feats\"] = tf.random.uniform(\n          (g_num_nodes, 10), minval=0, maxval=1, dtype=tf.dtypes.float32)\n      self.graphs.append(dgl_graph)\n      self.labels.append(label)\n      num_graphs += 1\n      if num_graphs > 100:\n        break\n    self.labels = tf.convert_to_tensor(self.labels, dtype=tf.dtypes.int64) \n```", "```\ndataset = SyntheticDataset()\ngraph, label = dataset[0]   \nprint(graph)\nprint(\"label:\", label) \n```", "```\nGraph(num_nodes=6, num_edges=15,\n      ndata_schemes={\n        'feats': Scheme(shape=(10,), dtype=tf.float32)}\n      edata_schemes={})\nlabel: tf.Tensor(0, shape=(), dtype=int64) \n```"]