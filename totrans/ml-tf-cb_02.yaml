- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The TensorFlow Way
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 1*, *Getting Started with TensorFlow 2.x* we introduced how TensorFlow
    creates tensors and uses variables. In this chapter, we'll introduce how to put
    together all these objects using eager execution, thus dynamically setting up
    a computational graph. From this, we can set up a simple classifier and see how
    well it performs.
  prefs: []
  type: TYPE_NORMAL
- en: Also, remember that the current and updated code from this book is available
    online on GitHub at [https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook](https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook).
  prefs: []
  type: TYPE_NORMAL
- en: 'Over the course of this chapter, we''ll introduce the key components of how
    TensorFlow operates. Then, we''ll tie it together to create a simple classifier
    and evaluate the outcomes. By the end of the chapter, you should have learned
    about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Operations using eager execution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layering nested operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with multiple layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing loss functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing backpropagation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with batch and stochastic training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining everything together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start working our way through more and more complex recipes, demonstrating
    the TensorFlow way of handling and solving data problems.
  prefs: []
  type: TYPE_NORMAL
- en: Operations using eager execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thanks to *Chapter 1*, *Getting Started with TensorFlow 2.x* we can already
    create objects such as variables in TensorFlow. Now we will introduce operations
    that act on such objects. In order to do so, we'll return to eager execution with
    a new basic recipe showing how to manipulate matrices. This recipe, and the following
    ones, are still basic ones, but over the course of the chapter, we'll combine
    these basic recipes into more complex ones.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start, we load TensorFlow and NumPy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: That's all we need to get started; now we can proceed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this example, we'll use what we have learned so far, and send each number
    in a list to be computed by TensorFlow commands and print the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we declare our tensors and variables. Here, out of all the various ways
    we could feed data into the variable using TensorFlow, we will create a NumPy
    array to feed into our variable and then use it for our operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once you get accustomed to working with TensorFlow variables, constants, and
    functions, it will become natural to start from NumPy array data, progress to
    scripting data structures and operations, and test their results as you go.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using eager execution, TensorFlow immediately evaluates the operation values,
    instead of manipulating the symbolic handles referred to the nodes of a computational
    graph to be later compiled and executed. You can therefore just iterate through
    the results of the multiplicative operation and print the resulting values using
    the `.NumPy` method, which returns a NumPy object from a TensorFlow tensor.
  prefs: []
  type: TYPE_NORMAL
- en: Layering nested operations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll learn how to put multiple operations to work; it is important
    to know how to chain operations together. This will set up layered operations
    to be executed by our network. In this recipe, we will multiply a placeholder
    by two matrices and then perform addition. We will feed in two matrices in the
    form of a three-dimensional NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: This is another easy-peasy recipe to give you ideas about how to code in TensorFlow
    using common constructs such as functions or classes, improving readability and
    code modularity. Even if the final product is a neural network, we're still writing
    a computer program, and we should abide by programming best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we just need to import TensorFlow and NumPy, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We're now ready to move forward with our recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will feed in two NumPy arrays of size *3* x *5*. We will multiply each matrix
    by a constant of size *5* x *1,* which will result in a matrix of size *3* x *1*.
    We will then multiply this by a *1* x *1* matrix resulting in a *3* x *1* matrix
    again. Finally, we add a *3* x *1* matrix at the end, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create the data to feed in and the corresponding placeholder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create the constants that we will use for matrix multiplication and
    addition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we declare the operations to be eagerly executed. As good practice, we
    create functions that execute the operations we need:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we nest our functions and display the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using functions (and also classes, as we are going to cover) will help you write
    clearer code. That makes debugging more effective and allows easy maintenance
    and reuse of code.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Thanks to eager execution, there''s no longer a need to resort to the "kitchen
    sink" programming style (meaning that you put almost everything in the global
    scope of the program; see [https://stackoverflow.com/questions/33779296/what-is-exact-meaning-of-kitchen-sink-in-programming](https://stackoverflow.com/questions/33779296/what-is-exact-meaning-of-kitchen-sink-in-programming))
    that was so common when using TensorFlow 1.x. At the moment, you can adopt either
    a functional programming style or an object-oriented one, such as the one we present
    in this brief example, where you can arrange all your operations and computations
    in a more logical and understandable way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Classes can help you organize your code and reuse it better than functions,
    thanks to class inheritance.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In all the examples in this recipe, we've had to declare the data shape and
    know the outcome shape of the operations before we run the data through the operations.
    This is not always the case. There may be a dimension or two that we do not know
    beforehand or some that can vary during our data processing. To take this into
    account, we designate the dimension or dimensions that can vary (or are unknown)
    as value `None`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to initialize a variable to have an unknown amount of rows, we
    would write the following line and then we can assign values of arbitrary row
    numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It is fine for matrix multiplication to have flexible rows because that won't
    affect the arrangement of our operations. This will come in handy in later chapters
    when we are feeding data in multiple batches of varying batch sizes.
  prefs: []
  type: TYPE_NORMAL
- en: While the use of *None* as a dimension allows us to use variably-sized dimensions,
    I always recommend that you be as explicit as possible when filling out dimensions.
    If the size of our data is known in advance, then we should explicitly write that
    size as the dimensions. The use of `None` as a dimension is recommended to be
    limited to the batch size of the data (or however many data points we are computing
    on at once).
  prefs: []
  type: TYPE_NORMAL
- en: Working with multiple layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have covered multiple operations, we will cover how to connect various
    layers that have data propagating through them. In this recipe, we will introduce
    how to best connect various layers, including custom layers. The data we will
    generate and use will be representative of small random images. It is best to
    understand this type of operation with a simple example and see how we can use
    some built-in layers to perform calculations. The first layer we will explore
    is called a **moving window**. We will perform a small moving window average across
    a 2D image and then the second layer will be a custom operation layer.
  prefs: []
  type: TYPE_NORMAL
- en: Moving windows are useful for everything related to time series. Though there
    are layers specialized for sequences, a moving window may prove useful when you
    are analyzing, for instance, MRI scans (neuroimages) or sound spectrograms.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we will see that the computational graph can get large and hard to
    look at. To address this, we will also introduce ways to name operations and create
    scopes for layers.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To start, you have to load the usual packages – NumPy and TensorFlow – using
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Let's now progress to the recipe. This time things are getting more complex
    and interesting.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We proceed with the recipe as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create our sample 2D image with NumPy. This image will be a *4* x *4* pixel
    image. We will create it in four dimensions; the first and last dimensions will
    have a size of `1` (we keep the batch dimension distinct, so you can experiment
    with changing its size). Note that some TensorFlow image functions will operate
    on four-dimensional images. Those four dimensions are image number, height, width,
    and channel, and to make it work with one channel, we explicitly set the last
    dimension to `1`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: To create a moving window average across our *4* x *4* image, we will use a
    built-in function that will convolute a constant across a window of the shape *2* x *2*.
    The function we will use is `conv2d()`; this function is quite commonly used in
    image processing and in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'This function takes a piecewise product of the window and a filter we specify.
    We must also specify a stride for the moving window in both directions. Here,
    we will compute four moving window averages: the upper-left, upper-right, lower-left,
    and lower-right four pixels. We do this by creating a *2* x *2* window and having
    strides of length `2` in each direction. To take the average, we will convolute
    the *2* x *2* window with a constant of `0.25`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are also naming this layer `Moving_Avg_Window` by using the name argument
    of the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'To figure out the output size of a convolutional layer, we can use the following
    formula: Output = (*W* – *F* + 2*P*)/*S* + 1), where *W* is the input size, *F* is
    the filter size, *P* is the padding of zeros, and *S* is the stride.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we define a custom layer that will operate on the *2* x *2* output of
    the moving window average. The custom function will first multiply the input by
    another *2* x *2* matrix tensor, and then add `1` to each entry. After this, we
    take the sigmoid of each element and return the *2* x *2* matrix. Since matrix
    multiplication only operates on two-dimensional matrices, we need to drop the
    extra dimensions of our image that are of size `1`. TensorFlow can do this with
    the built-in `squeeze()` function. Here, we define the new layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have to arrange the two layers in the network. We will do this by calling
    one layer function after the other, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we just feed in the *4* x *4* image into the functions. Finally, we can
    check the result, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Let's now understand more in depth how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first layer is named `Moving_Avg_Window`. The second is a collection of
    operations called `Custom_Layer`. Data processed by these two layers is first
    collapsed on the left and then expanded on the right. As shown by the example,
    you can wrap all the layers into functions and call them, one after the other,
    in a way that later layers process the outputs of previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing loss functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, we will cover some of the main loss functions that we can use
    in TensorFlow. Loss functions are a key aspect of machine learning algorithms.
    They measure the distance between the model outputs and the target (truth) values.
  prefs: []
  type: TYPE_NORMAL
- en: In order to optimize our machine learning algorithms, we will need to evaluate
    the outcomes. Evaluating outcomes in TensorFlow depends on specifying a loss function.
    A loss function tells TensorFlow how good or bad the predictions are compared
    to the desired result. In most cases, we will have a set of data and a target
    on which to train our algorithm. The loss function compares the target to the
    prediction (it measures the distance between the model outputs and the target
    truth values) and provides a numerical quantification between the two.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first start a computational graph and load `matplotlib`, a Python plotting
    package, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now that we are ready to plot, let's proceed to the recipe without further ado.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we will talk about loss functions for regression, which means predicting
    a continuous dependent variable. To start, we will create a sequence of our predictions
    and a target as a tensor. We will output the results across 500 x values between
    `-1` and `1`. See the *How it works...* section for a plot of the outputs. Use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The L2 norm loss is also known as the Euclidean loss function. It is just the
    square of the distance to the target. Here, we will compute the loss function
    as if the target is zero. The L2 norm is a great loss function because it is very
    curved near the target and algorithms can use this fact to converge to the target
    more slowly the closer it gets to zero. We can implement this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: TensorFlow has a built-in form of the L2 norm, called `tf.nn.l2_loss()`. This
    function is actually half the L2 norm. In other words, it is the same as the previous
    one but divided by 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The L1 norm loss is also known as the **absolute loss function**. Instead of
    squaring the difference, we take the absolute value. The L1 norm is better for
    outliers than the L2 norm because it is not as steep for larger values. One issue
    to be aware of is that the L1 norm is not smooth at the target, and this can result
    in algorithms not converging well. It appears as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Pseudo-Huber loss is a continuous and smooth approximation to the **Huber loss
    function**. This loss function attempts to take the best of the L1 and L2 norms
    by being convex near the target and less steep for extreme values. The form depends
    on an extra parameter, `delta`, which dictates how steep it will be. We will plot
    two forms, *delta1 = 0.25* and *delta2 = 5*, to show the difference, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now, we'll move on to loss functions for classification problems. Classification
    loss functions are used to evaluate loss when predicting categorical outcomes.
    Usually, the output of our model for a class category is a real-value number between
    `0` and `1`. Then, we choose a cutoff (0.5 is commonly chosen) and classify the
    outcome as being in that category if the number is above the cutoff. Next, we'll
    consider various loss functions for categorical outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we will need to redefine our predictions `(x_vals)` and `target`.
    We will save the outputs and plot them in the next section. Use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Hinge loss is mostly used for support vector machines but can be used in neural
    networks as well. It is meant to compute a loss among two target classes, `1`
    and -`1`. In the following code, we are using the target value `1`, so the closer
    our predictions are to `1`, the lower the loss value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Cross-entropy loss for a binary case is also sometimes referred to as the **logistic
    loss function**. It comes about when we are predicting the two classes `0` or
    `1`. We wish to measure a distance from the actual class (`0` or `1`) to the predicted
    value, which is usually a real number between `0` and `1`. To measure this distance,
    we can use the cross-entropy formula from information theory, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Sigmoid cross-entropy loss is very similar to the previous loss function except
    we transform the `x` values using the sigmoid function before we put them in the
    cross-entropy loss, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Weighted cross-entropy loss is a weighted version of sigmoid cross-entropy
    loss. We provide a weight on the positive target. For an example, we will weight
    the positive target by `0.5`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Softmax cross-entropy loss operates on non-normalized outputs. This function
    is used to measure a loss when there is only one target category instead of multiple.
    Because of this, the function transforms the outputs into a probability distribution
    via the softmax function and then computes the loss function from a true probability
    distribution, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Sparse softmax cross-entropy loss is almost the same as softmax cross-entropy
    loss, except instead of the target being a probability distribution, it is an
    index of which category is `true`. Instead of a sparse all-zero target vector
    with one value of `1`, we just pass in the index of the category that is the `true`
    value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Now let's understand better how such loss functions operate by plotting them
    on a graph.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is how to use `matplotlib` to plot the regression loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following plot as output from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Plotting various regression loss functions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is how to use `matplotlib` to plot the various classification loss functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following plot from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Plots of classification loss functions'
  prefs: []
  type: TYPE_NORMAL
- en: Each of these loss curves provides different advantages to the neural network
    optimizing it. We are now going to discuss this a little bit more.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here is a table summarizing the properties and benefits of the different loss functions
    that we have just graphically described:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Loss function | Use | Benefits | Disadvantages |'
  prefs: []
  type: TYPE_TB
- en: '| L2 | Regression | More stable | Less robust |'
  prefs: []
  type: TYPE_TB
- en: '| L1 | Regression | More robust | Less stable |'
  prefs: []
  type: TYPE_TB
- en: '| Pseudo-Huber | Regression | More robust and stable | One more parameter |'
  prefs: []
  type: TYPE_TB
- en: '| Hinge | Classification | Creates a max margin for use in SVM | Unbounded
    loss affected by outliers |'
  prefs: []
  type: TYPE_TB
- en: '| Cross-entropy | Classification | More stable | Unbounded loss, less robust
    |'
  prefs: []
  type: TYPE_TB
- en: The remaining classification loss functions all have to do with the type of
    cross-entropy loss. The cross-entropy sigmoid loss function is for use on unscaled
    logits and is preferred over computing the sigmoid loss and then the cross-entropy
    loss, because TensorFlow has better built-in ways to handle numerical edge cases.
    The same goes for softmax cross-entropy and sparse softmax cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the classification loss functions described here are for two-class predictions.
    This can be extended to multiple classes by summing the cross-entropy terms over
    each prediction/target.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also many other metrics to look at when evaluating a model. Here
    is a list of some more to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model metric | Description |'
  prefs: []
  type: TYPE_TB
- en: '| R-squared (coefficient of determination) | For linear models, this is the
    proportion of variance in the dependent variable that is explained by the independent
    data. For models with a larger number of features, consider using adjusted R-squared.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Root mean squared error | For continuous models, this measures the difference
    between prediction and actual via the square root of the average squared error.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Confusion matrix | For categorical models, we look at a matrix of predicted
    categories versus actual categories. A perfect model has all the counts along
    the diagonal. |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | For categorical models, this is the fraction of true positives over
    all predicted positives. |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | For categorical models, this is the fraction of true positives
    over all actual positives. |'
  prefs: []
  type: TYPE_TB
- en: '| F-score | For categorical models, this is the harmonic mean of precision
    and recall. |'
  prefs: []
  type: TYPE_TB
- en: In your choice of the right metric, you have to both evaluate the problem you
    have to solve (because each metric will behave differently and, depending on the
    problem at hand, some loss minimization strategies will prove better than others
    for our problem), and to experiment with the behavior of the neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing backpropagation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the benefits of using TensorFlow is that it can keep track of operations
    and automatically update model variables based on backpropagation. In this recipe,
    we will introduce how to use this aspect to our advantage when training machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will introduce how to change our variables in the model in such a way
    that a loss function is minimized. We have learned how to use objects and operations,
    and how to create loss functions that will measure the distance between our predictions
    and targets. Now, we just have to tell TensorFlow how to backpropagate errors
    through our network in order to update the variables in such a way to minimize
    the loss function. This is achieved by declaring an optimization function. Once
    we have an optimization function declared, TensorFlow will go through and figure
    out the backpropagation terms for all of our computations in the graph. When we
    feed data in and minimize the loss function, TensorFlow will modify our variables
    in the network accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we will do a very simple regression algorithm. We will sample
    random numbers from a normal distribution, with mean 1 and standard deviation
    0.1\. Then, we will run the numbers through one operation, which will be to multiply
    them by a weight tensor and then adding a bias tensor. From this, the loss function
    will be the L2 norm between the output and the target. Our target will show a
    high correlation with our input, so the task won't be too complex, yet the recipe
    will be interestingly demonstrative, and easily reusable for more complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: The second example is a very simple binary classification algorithm. Here, we
    will generate 100 numbers from two normal distributions, *N(-3,1)* and *N(3,1)*.
    All the numbers from *N(-3, 1)* will be in target class `0`, and all the numbers
    from *N(3, 1)* will be in target class `1`. The model to differentiate these classes
    (which are perfectly separable) will again be a linear model optimized accordingly
    to the sigmoid cross-entropy loss function, thus, at first operating a sigmoid
    transformation on the model result and then computing the cross-entropy loss function.
  prefs: []
  type: TYPE_NORMAL
- en: While specifying a good learning rate helps the convergence of algorithms, we
    must also specify a type of optimization. From the preceding two examples, we
    are using standard gradient descent. This is implemented with the `tf.optimizers.SGD` TensorFlow
    function.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll start with the regression example. First, we load the usual numerical
    Python packages that always accompany our recipes, `NumPy` and `TensorFlow`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Next, we create the data. In order to make everything easily replicable, we
    want to set the random seed to a specific value. We will always repeat this in
    our recipes, so we exactly obtain the same results; check yourself how chance
    may vary the results in the recipes, by simply changing the seed number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, in order to get assurance that the target and input have a good correlation,
    plot a scatterplot of the two variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Scatterplot of x_vals and y_vals'
  prefs: []
  type: TYPE_NORMAL
- en: 'We add the structure of the network (a linear model of the type *bX + a*) as
    a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we add our L2 Loss function to be applied to the results of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have to declare a way to optimize the variables in our graph. We declare
    an optimization algorithm. Most optimization algorithms need to know how far to
    step in each iteration. Such a distance is controlled by the learning rate. Setting
    it to a correct value is specific to the problem we are dealing with, so we can
    figure out a suitable setting only by experimenting. Anyway, if our learning rate
    is too high, our algorithm might overshoot the minimum, but if our learning rate
    is too low, our algorithm might take too long to converge.
  prefs: []
  type: TYPE_NORMAL
- en: 'The learning rate has a big influence on convergence and we will discuss it
    again at the end of the section. While we''re using the standard gradient descent
    algorithm, there are many other alternative options. There are, for instance,
    optimization algorithms that operate differently and can achieve a better or worse
    optimum depending on the problem. For a great overview of different optimization
    algorithms, see the paper by Sebastian Ruder in the *See also* section at the
    end of this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: There is a lot of theory on which learning rates are best. This is one of the
    harder things to figure out in machine learning algorithms. Good papers to read
    about how learning rates are related to specific optimization algorithms are listed
    in the *See also* section at the end of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can initialize our network variables (`weights` and `biases`) and set
    a recording list (named `history`) to help us visualize the optimization steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to loop through our training algorithm and tell TensorFlow
    to train many times. We will do this 100 times and print out results every 25^(th)
    iteration. To train, we will select a random *x* and *y* entry and feed it through
    the graph. TensorFlow will automatically compute the loss, and slightly change
    the weights and biases to minimize the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: In the loops, `tf.GradientTape()` allows TensorFlow to track the computations
    and calculate the gradient with respect to the observed variables. Every variable
    that is within the `GradientTape()` scope is monitored (please keep in mind that
    constants are not monitored, unless you explicitly state it with the command `tape.watch(constant)`).
    Once you've completed the monitoring, you can compute the gradient of a target
    in respect of a list of sources (using the command `tape.gradient(target, sources)`)
    and get back an eager tensor of the gradients that you can apply to the minimization
    process. The operation is automatically concluded with the updating of your sources
    (in our case, the `weights` and `biases` variables) with new values.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the training is completed, we can visualize how the optimization process
    operates over successive gradient applications:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: L2 loss through iterations in our recipe'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we will introduce the code for the simple classification example.
    We can use the same TensorFlow script, with some updates. Remember, we will attempt
    to find an optimal set of weights and biases that will separate the data into
    two different classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we pull in the data from two different normal distributions, `N(-3,
    1)` and `N(3, 1)`. We will also generate the target labels and visualize how the
    two classes are distributed along our predictor variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Class distribution on x_vals'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because the specific loss function for this problem is sigmoid cross-entropy,
    we update our loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we initialize our variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we loop through a randomly selected data point several hundred times
    and update the `weights` and `biases` variables accordingly. As we did before,
    every 25 iterations we will print out the value of our variables and the loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'A plot, also in this case, will reveal how the optimization proceeded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Sigmoid cross-entropy loss through iterations in our recipe'
  prefs: []
  type: TYPE_NORMAL
- en: The directionality of the plot is clear, though the trajectory is a bit bumpy
    because we are learning one example at a time, thus making the learning process
    decisively stochastic. The graph could also point out the need to try to decrease
    the learning rate a bit.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a recap and explanation, for both examples, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: We created the data. Both examples needed to load data into specific variables
    used by the function that computes the network.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We initialized variables. We used some random Gaussian values, but initialization
    is a topic on its own, since much of the final results may depend on how we initialize
    our network (just change the random seed before initialization to find it out).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We created a loss function. We used the L2 loss for regression and the cross-entropy
    loss for classification.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We defined an optimization algorithm. Both algorithms used gradient descent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We iterated across random data samples to iteratively update our variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned before, the optimization algorithm is sensitive to the choice
    of learning rate. It is important to summarize the effect of this choice in a
    concise manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning rate size | Advantages/disadvantages | Uses |'
  prefs: []
  type: TYPE_TB
- en: '| **Smaller learning rate** | Converges slower but more accurate results |
    If the solution is unstable, try lowering the learning rate first |'
  prefs: []
  type: TYPE_TB
- en: '| **Larger learning rate** | Less accurate, but converges faster | For some
    problems, helps prevent solutions from stagnating |'
  prefs: []
  type: TYPE_TB
- en: Sometimes, the standard gradient descent algorithm can be stuck or slow down
    significantly. This can happen when the optimization is stuck in the flat spot
    of a saddle. To combat this, the solution is taking into account a momentum term,
    which adds on a fraction of the prior step's gradient descent value. You can access
    this solution by setting the momentum and the Nesterov parameters, along with
    your learning rate, in `tf.optimizers.SGD` (see [https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/SGD](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/SGD)
    for more details).
  prefs: []
  type: TYPE_NORMAL
- en: Another variant is to vary the optimizer step for each variable in our models.
    Ideally, we would like to take larger steps for smaller moving variables and shorter
    steps for faster changing variables. We will not go into the mathematics of this
    approach, but a common implementation of this idea is called the **Adagrad algorithm**.
    This algorithm takes into account the whole history of the variable gradients.
    The function in TensorFlow for this is called `AdagradOptimizer()` ([https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adagrad](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adagrad)).
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, Adagrad forces the gradients to zero too soon because it takes into
    account the whole history. A solution to this is to limit how many steps we use.
    This is called the **Adadelta algorithm**. We can apply this by using the `AdadeltaOptimizer()` function
    ([https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adadelta](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adadelta)).
  prefs: []
  type: TYPE_NORMAL
- en: There are a few other implementations of different gradient descent algorithms.
    For these, refer to the TensorFlow documentation at [https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers](https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on optimization algorithms and learning rates, see the
    following papers and articles:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recipes from this chapter, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Implementing Loss Functions section.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The Implementing Backpropagation section.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kingma, D., Jimmy, L. Adam**: *A Method for Stochastic Optimization. ICLR*
    2015 [https://arxiv.org/pdf/1412.6980.pdf](https://arxiv.org/pdf/1412.6980.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ruder, S.** *An Overview of Gradient Descent Optimization Algorithms*. 2016 [https://arxiv.org/pdf/1609.04747v1.pdf](https://arxiv.org/pdf/1609.04747v1.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Zeiler, M.** *ADADelta: An Adaptive Learning Rate Method*. 2012 [https://arxiv.org/pdf/1212.5701.pdf](https://arxiv.org/pdf/1212.5701.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with batch and stochastic training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While TensorFlow updates our model variables according to backpropagation, it
    can operate on anything from a one-datum observation (as we did in the previous
    recipe) to a large batch of data at once. Operating on one training example can
    make for a very erratic learning process, while using too large a batch can be
    computationally expensive. Choosing the right type of training is crucial for
    getting our machine learning algorithms to converge to a solution.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order for TensorFlow to compute the variable gradients for backpropagation
    to work, we have to measure the loss on a sample or multiple samples. Stochastic
    training only works on one randomly sampled data-target pair at a time, just as
    we did in the previous recipe. Another option is to put a larger portion of the
    training examples in at a time and average the loss for the gradient calculation.
    The sizes of the training batch can vary, up to and including the whole dataset
    at once. Here, we will show how to extend the prior regression example, which
    used stochastic training, to batch training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by loading `NumPy`, `matplotlib`, and `TensorFlow`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now we just have to script our code and test our recipe in the *How to do it…*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by declaring a batch size. This will be how many data observations
    we will feed through the computational graph at one time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we just apply small modifications to the code used before for the regression
    problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our previous recipe, we have learned how to use matrix multiplication
    in our network and in our cost function. At this point, we just need to deal with
    inputs that are made of more rows as batches instead of single examples. We can
    even compare it with the previous approach, which we can now name stochastic optimization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Just running the code will retrain our network using batches. At this point,
    we need to evaluate the results, get some intuition about how it works, and reflect
    on the results. Let's proceed to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Batch training and stochastic training differ in their optimization methods
    and their convergence. Finding a good batch size can be difficult. To see how
    convergence differs between batch training and stochastic training, you are encouraged
    to change the batch size to various levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'A visual comparison of the two approaches will explain better how using batches
    for this problem resulted in the same optimization as stochastic training, though
    there were fewer fluctuations during the process. Here is the code to produce
    the plot of both the stochastic and batch losses for the same regression problem.
    Note that the batch loss is much smoother and the stochastic loss is much more
    erratic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Comparison of L2 loss when using stochastic and batch optimization'
  prefs: []
  type: TYPE_NORMAL
- en: Now our graph displays a smoother trend line. The persistent presence of bumps
    could be solved by reducing the learning rate and adjusting the batch size.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '| Type of training | Advantages | Disadvantages |'
  prefs: []
  type: TYPE_TB
- en: '| Stochastic | Randomness may help move out of local minimums | Generally needs
    more iterations to converge |'
  prefs: []
  type: TYPE_TB
- en: '| Batch | Finds minimums quicker | Takes more resources to compute |'
  prefs: []
  type: TYPE_TB
- en: Combining everything together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will combine everything we have illustrated so far and create
    a classifier for the iris dataset. The iris dataset is described in more detail
    in the *Working with data sources* recipe in *Chapter 1*, *Getting Started with
    TensorFlow*. We will load this data and make a simple binary classifier to predict
    whether a flower is the species Iris setosa or not. To be clear, this dataset
    has three species, but we will only predict whether a flower is a single species,
    Iris setosa or not, giving us a binary classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will start by loading the libraries and data and then transform the target
    accordingly. First, we load the libraries needed for our recipe. For the Iris
    dataset, we need the TensorFlow Datasets module, which we haven''t used before
    in our recipes. Note that we also load `matplotlib` here, because we would like
    to plot the resultant line afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As a starting point, let''s first declare our batch size using a global variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we load the iris data. We will also need to transform the target data
    to be just `1` or `0`, whether the target is setosa or not. Since the iris dataset
    marks setosa as a `0`, we will change all targets with the value `0` to `1`, and
    the other values all to `0`. We will also only use two features, petal length
    and petal width. These two features are the third and fourth entry in each row
    of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the previous chapter, we use the TensorFlow dataset functions to
    both load and operate the necessary transformations by creating a data generator
    that can dynamically feed our network with data, instead of keeping it in an in-memory
    NumPy matrix. As a first step, we load the data, specifying that we want to split
    it (using the parameters `split='train[:90%]'` and `split='train[90%:]'`). This
    allows us to reserve a part (10%) of the dataset for the model evaluation, using
    data that has not been part of the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: We also specify the parameter, `as_supervised=True`, that will allow us to access
    the data as tuples of features and labels when iterating from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Now we transform the dataset into an iterable generator by applying successive
    transformations. We shuffle the data, we define the batch to be returned by the
    iterable, and, most important, we apply a custom function that filters and transforms
    the features and labels returned from the dataset at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we define the linear model. The model will take the usual form *bX+a*.
    Remember that TensorFlow has loss functions with the sigmoid built in, so we just
    need to define the output of the model prior to the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we add our sigmoid cross-entropy loss function with TensorFlow''s built-in `sigmoid_cross_entropy_with_logits()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We also have to tell TensorFlow how to optimize our computational graph by
    declaring an optimizing method. We will want to minimize the cross-entropy loss.
    We will also choose `0.02` as our learning rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will train our linear model with 300 iterations. We will feed in the
    three data points that we require: petal length, petal width, and the target variable.
    Every 30 iterations, we will print the variable values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'If we plot the loss against the iterations, we can acknowledge from the smoothness
    of the reduction of the loss over time how the learning has been quite an easy
    task for the linear model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Cross-entropy error for the Iris setosa data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll conclude by checking the performance on our reserved test data. This
    time we just take the examples from the test dataset. As expected, the resulting
    cross-entropy value is analogous to the training one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'The next set of commands extracts the model variables and plots the line on
    a graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The resultant graph is in the *How it works...* section, where we also discuss
    the validity and reproducibility of the obtained results.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our goal was to fit a line between the Iris setosa points and the other two
    species using only petal width and petal length. If we plot the points, and separate
    the area of the plot where classifications are zero from the area where classifications
    are one with a line, we see that we have achieved this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16254_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Plot of Iris setosa and non-setosa for petal width versus petal
    length; the solid line is the linear separator that we achieved after 300 iterations'
  prefs: []
  type: TYPE_NORMAL
- en: The way the separating line is defined depends on the data, the network architecture,
    and the learning process. Different starting situations, even due to the random
    initialization of the neural network's weights, may provide you with a slightly
    different solution.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While we achieved our objective of separating the two classes with a line, it
    may not be the best model for separating two classes. For instance, after adding
    new observations, we may realize that our solution badly separates the two classes.
    As we progress into the next chapter, we will start dealing with recipes that
    address these problems by providing testing, randomization, and specialized layers
    that will increase the generalization capabilities of our recipes.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For information about the Iris dataset, see the documentation at [https://archive.ics.uci.edu/ml/datasets/iris](https://archive.ics.uci.edu/ml/datasets/iris).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you want to understand more about decision boundaries drawing for machine
    learning algorithms, we warmly suggest this excellent Medium article from Navoneel
    Chakrabarty: [https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d](https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
