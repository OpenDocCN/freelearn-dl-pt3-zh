<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Object Detection – CIFAR-10 Example</h1>
                </header>
            
            <article>
                
<p class="calibre2">After covering the basics and the intuition/motivation behind <strong class="calibre13">Convolution Neural Networks</strong> (<strong class="calibre13">CNNs</strong>), we are going to demonstrate this on one of the most popular datasets available for object detection. We'll also see how the initial layers of the CNN get very basic features about our objects, but the final convolutional layers will get more semantic-level features that are built up from those basic features in the first layers.</p>
<p class="calibre2">The following topics will be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">Object detection</li>
<li class="calibre8">CIFAR-10 object detection in mages—model building and training</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Object detection</h1>
                </header>
            
            <article>
                
<p class="calibre2">Wikipedia states that:</p>
<div class="packtquote">"Object detection – technology in the field of computer vision for finding and identifying objects in an image or video sequence. Humans recognize a multitude of objects in images with little effort, despite the fact that the image of the objects may vary somewhat in different view points, in many different sizes and scales or even when they are translated or rotated. Objects can even be recognized when they are partially obstructed from view. This task is still a challenge for computer vision systems. Many approaches to the task have been implemented over multiple decades."</div>
<p class="calibre2">Image analysis is one of the most prominent fields in deep learning. Images are easy to generate and handle, and they are exactly the right type of data for machine learning: easy to understand for human beings, but difficult for computers. Not surprisingly, image analysis played a key role in the history of deep neural networks.</p>
<div class="CDPAlignCenter"><img src="assets/0d82c9b0-e87a-4f16-a94c-805efcf6f9f5.jpg" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 11.1: Examples of detecting objects. Source: B. C. Russell, A. Torralba, C. Liu, R. Fergus, W. T. Freeman, Object Detection by Scene Alignment, Advances in Neural Information Processing Systems, 2007, at: http://bryanrussell.org/papers/nipsDetectionBySceneAlignment07.pdf</div>
<p class="calibre2">With the rise of autonomous cars, facial detection, smart video surveillance, and people-counting solutions, fast and accurate object detection systems are in a big demand. These systems include not only object recognition and classification in an image, but can also locate each one of them by drawing appropriate boxes around them. This makes object detection a harder task than its traditional computer vision predecessor, image classification.</p>
<p class="calibre2">In this chapter, we'll look at object detection — finding out which objects are in an image. For example, imagine a self-driving car that needs to detect other cars on the road as in <em class="calibre19">Figure 11.1</em>. There are lots of complicated algorithms for object detection. They often require huge datasets, very deep convolutional networks, and long training times.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CIFAR-10 – modeling, building, and training</h1>
                </header>
            
            <article>
                
<p class="calibre2">This example shows how to make a CNN for classifying images in the CIFAR-10 dataset. We'll be using a simple convolution neural network implementation of a couple of convolutions and fully connected layers.</p>
<p class="calibre2">Even though the network architecture is very simple, you will see how well it performs when trying to detect objects in the CIFAR-10 images.</p>
<p class="calibre2">So, let's start off this implementation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Used packages</h1>
                </header>
            
            <article>
                
<p class="calibre2">We import all the required packages for this implementation:</p>
<pre class="calibre21">%matplotlib inline<br class="title-page-name"/>%config InlineBackend.figure_format = 'retina'<br class="title-page-name"/><br class="title-page-name"/>from urllib.request import urlretrieve<br class="title-page-name"/>from os.path import isfile, isdir<br class="title-page-name"/>from tqdm import tqdm<br class="title-page-name"/>import tarfile<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import random<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>from sklearn.preprocessing import LabelBinarizer<br class="title-page-name"/>from sklearn.preprocessing import OneHotEncoder<br class="title-page-name"/><br class="title-page-name"/>import pickle<br class="title-page-name"/>import tensorflow as tf</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Loading the CIFAR-10 dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this implementation, we'll use CIFAR-10, which is one of the most widely used datasets for object detection. So, let's start off by defining a helper class to download and extract the CIFAR-10 dataset, if it's not already downloaded:</p>
<pre class="calibre21">cifar10_batches_dir_path = 'cifar-10-batches-py'<br class="title-page-name"/><br class="title-page-name"/>tar_gz_filename = 'cifar-10-python.tar.gz'<br class="title-page-name"/><br class="title-page-name"/>class DLProgress(tqdm):<br class="title-page-name"/>    last_block = 0<br class="title-page-name"/><br class="title-page-name"/>    def hook(self, block_num=1, block_size=1, total_size=None):<br class="title-page-name"/>        self.total = total_size<br class="title-page-name"/>        self.update((block_num - self.last_block) * block_size)<br class="title-page-name"/>        self.last_block = block_num<br class="title-page-name"/><br class="title-page-name"/>if not isfile(tar_gz_filename):<br class="title-page-name"/>    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Python Images Batches') as pbar:<br class="title-page-name"/>        urlretrieve(<br class="title-page-name"/>            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',<br class="title-page-name"/>            tar_gz_filename,<br class="title-page-name"/>            pbar.hook)<br class="title-page-name"/><br class="title-page-name"/>if not isdir(cifar10_batches_dir_path):<br class="title-page-name"/>    with tarfile.open(tar_gz_filename) as tar:<br class="title-page-name"/>        tar.extractall()<br class="title-page-name"/>        tar.close()</pre>
<p class="calibre2">After downloading and extracting the CIFAR-10 dataset, you will find out that it's already split into five batches. CIFAR-10 contains images for 10 categories/classes:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">airplane</kbd></li>
<li class="calibre8"><kbd class="calibre12">automobile</kbd></li>
<li class="calibre8"><kbd class="calibre12">bird</kbd></li>
<li class="calibre8"><kbd class="calibre12">cat</kbd></li>
<li class="calibre8"><kbd class="calibre12">deer</kbd></li>
<li class="calibre8"><kbd class="calibre12">dog</kbd></li>
<li class="calibre8"><kbd class="calibre12">frog</kbd></li>
<li class="calibre8"><kbd class="calibre12">horse</kbd></li>
<li class="calibre8"><kbd class="calibre12">ship</kbd></li>
<li class="calibre8"><kbd class="calibre12">truck</kbd></li>
</ul>
<p class="calibre2">Before we dive into building the core of the network, let's do some data analysis and preprocessing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis and preprocessing</h1>
                </header>
            
            <article>
                
<p class="calibre2">We need to analyze the dataset and do some basic preprocessing. So, let's start off by defining some helper functions that will enable us to load a specific batch from the five batches that we have and print some analysis about this batch and its samples:</p>
<pre class="calibre21"># Defining a helper function for loading a batch of images<br class="title-page-name"/>def load_batch(cifar10_dataset_dir_path, batch_num):<br class="title-page-name"/>    <br class="title-page-name"/>    with open(cifar10_dataset_dir_path + '/data_batch_' + str(batch_num), mode='rb') as file:<br class="title-page-name"/>        batch = pickle.load(file, encoding='latin1')<br class="title-page-name"/><br class="title-page-name"/>    input_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)<br class="title-page-name"/>    target_labels = batch['labels']<br class="title-page-name"/><br class="title-page-name"/>    return input_features, target_labels</pre>
<p class="calibre2">Then, we define a function that can help us display the stats of a specific sample from a specific batch:</p>
<pre class="calibre21">#Defining a function to show the stats for batch ans specific sample<br class="title-page-name"/>def batch_image_stats(cifar10_dataset_dir_path, batch_num, sample_num):<br class="title-page-name"/><br class="title-page-name"/>    batch_nums = list(range(1, 6))<br class="title-page-name"/><br class="title-page-name"/>    #checking if the batch_num is a valid batch number<br class="title-page-name"/>    if batch_num not in batch_nums:<br class="title-page-name"/>        print('Batch Num is out of Range. You can choose from these Batch nums: {}'.format(batch_nums))<br class="title-page-name"/>        return None<br class="title-page-name"/><br class="title-page-name"/>    input_features, target_labels = load_batch(cifar10_dataset_dir_path, batch_num)<br class="title-page-name"/><br class="title-page-name"/>    #checking if the sample_num is a valid sample number<br class="title-page-name"/>    if not (0 &lt;= sample_num &lt; len(input_features)):<br class="title-page-name"/>        print('{} samples in batch {}. {} is not a valid sample number.'.format(len(input_features), batch_num, sample_num))<br class="title-page-name"/>        return None<br class="title-page-name"/><br class="title-page-name"/>    print('\nStatistics of batch number {}:'.format(batch_num))<br class="title-page-name"/>    print('Number of samples in this batch: {}'.format(len(input_features)))<br class="title-page-name"/>    print('Per class counts of each Label: {}'.format(dict(zip(*np.unique(target_labels, return_counts=True)))))<br class="title-page-name"/><br class="title-page-name"/>    image = input_features[sample_num]<br class="title-page-name"/>    label = target_labels[sample_num]<br class="title-page-name"/>    cifar10_class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']<br class="title-page-name"/><br class="title-page-name"/>    print('\nSample Image Number {}:'.format(sample_num))<br class="title-page-name"/>    print('Sample image - Minimum pixel value: {} Maximum pixel value: {}'.format(image.min(), image.max()))<br class="title-page-name"/>    print('Sample image - Shape: {}'.format(image.shape))<br class="title-page-name"/>    print('Sample Label - Label Id: {} Name: {}'.format(label, cifar10_class_names[label]))<br class="title-page-name"/>    plt.axis('off')<br class="title-page-name"/>    plt.imshow(image)</pre>
<p class="calibre2">Now, we can use this function to play around with our dataset and visualize specific images:</p>
<pre class="calibre21"># Explore a specific batch and sample from the dataset<br class="title-page-name"/>batch_num = 3<br class="title-page-name"/>sample_num = 6<br class="title-page-name"/>batch_image_stats(cifar10_batches_dir_path, batch_num, sample_num)</pre>
<p class="calibre2">The output is as follows:</p>
<pre class="calibre21"><br class="title-page-name"/>Statistics of batch number 3:<br class="title-page-name"/>Number of samples in this batch: 10000<br class="title-page-name"/>Per class counts of each Label: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}<br class="title-page-name"/><br class="title-page-name"/>Sample Image Number 6:<br class="title-page-name"/>Sample image - Minimum pixel value: 30 Maximum pixel value: 242<br class="title-page-name"/>Sample image - Shape: (32, 32, 3)<br class="title-page-name"/>Sample Label - Label Id: 8 Name: ship</pre>
<div class="CDPAlignCenter"><img src="assets/ae827baa-2394-4eaf-93cd-018a7ae7b8a1.png" class="calibre109"/></div>
<div class="CDPAlignCenter1">Figure 11.2: Sample image 6 from batch 3</div>
<p class="calibre2">Before going ahead and feeding our dataset to the model, we need to normalize it to the range of zero to one.</p>
<p class="calibre2">Batch normalization optimizes network training. It has been shown to have several benefits:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Faster training</strong>: Each training step will be slower because of the extra calculations during the forward pass of the network and the additional hyperparameters to train during backward propagation passes of the network. However, it should converge much more quickly, so training should be faster overall.</li>
<li class="calibre8"><strong class="calibre1">Higher learning rates</strong>: The gradient descent algorithm mostly requires small learning rates for the network to converge to the loss function's minima. And as the neural networks get deeper, their gradient values get smaller and smaller during backpropagation, so they usually require even more iterations. Using the idea of batch normalization allows us to use much higher learning rates, which further increases the speed at which networks train.</li>
<li class="calibre8"><strong class="calibre1">Easy to initialize weights</strong>: Weight initialization can be difficult, and it will be even more difficult if we are using deep neural networks. Batch normalization seems to allow us to be much less careful about choosing our initial starting weights.</li>
</ul>
<p class="calibre2">So, let's proceed by defining a function that will be responsible for normalizing a list of input images so that all the pixel values of these images are between zero and one:</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21"><span>#Normalize CIFAR-10 images to be in the range of [0,1]<br class="title-page-name"/><br class="title-page-name"/>def normalize_images(images):<br class="title-page-name"/>    <br class="title-page-name"/>    # initial zero ndarray<br class="title-page-name"/>    normalized_images = np.zeros_like(images.astype(float))<br class="title-page-name"/>    <br class="title-page-name"/>    # The first images index is number of images where the other indices indicates<br class="title-page-name"/>    # hieight, width and depth of the image<br class="title-page-name"/>    num_images = images.shape[0]<br class="title-page-name"/>    <br class="title-page-name"/>    # Computing the minimum and maximum value of the input image to do the normalization based on them<br class="title-page-name"/>    maximum_value, minimum_value = images.max(), images.min()<br class="title-page-name"/>    <br class="title-page-name"/>    # Normalize all the pixel values of the images to be from 0 to 1<br class="title-page-name"/>    for img in range(num_images):<br class="title-page-name"/>        normalized_images[img,...] = (images[img, ...] - float(minimum_value)) / float(maximum_value - minimum_value)<br class="title-page-name"/><br class="title-page-name"/>    return normalized_images</span></pre></div>
</div>
</div>
<p class="calibre2">Next up, we need to implement another helper function to encode the labels of the input image. In this function, we will use one-hot encoding of sklearn, where each image label is represented by a vector of zeros except for the class index of the image that this vector represents.</p>
<p class="calibre2">The size of the output vector will be dependent on the number of classes that we have in the dataset, which is 10 classes in the case of CIFAR-10 data:</p>
<pre class="calibre21"><span>#encoding the input images. Each image will be represented by a vector of zeros except for the class index of the image <br class="title-page-name"/># that this vector represents. The length of this vector depends on number of classes that we have<br class="title-page-name"/># the dataset which is 10 in CIFAR-10<br class="title-page-name"/><br class="title-page-name"/>def one_hot_encode(images):<br class="title-page-name"/>    <br class="title-page-name"/>    num_classes = 10<br class="title-page-name"/>    <br class="title-page-name"/>    #use sklearn helper function of OneHotEncoder() to do that<br class="title-page-name"/>    encoder = OneHotEncoder(num_classes)<br class="title-page-name"/>    <br class="title-page-name"/>    #resize the input images to be 2D<br class="title-page-name"/>    input_images_resized_to_2d = np.array(images).reshape(-1,1)<br class="title-page-name"/>    one_hot_encoded_targets = encoder.fit_transform(input_images_resized_to_2d)<br class="title-page-name"/>    <br class="title-page-name"/>    return one_hot_encoded_targets.toarray()</span></pre>
<p class="calibre2">Now, it's time to call the preceding helper functions to do the preprocessing and persist the dataset so that we can use it later:</p>
<pre class="calibre21">def preprocess_persist_data(cifar10_batches_dir_path, normalize_images, one_hot_encode):<br class="title-page-name"/>    <br class="title-page-name"/>    <br class="title-page-name"/>    num_batches = 5<br class="title-page-name"/>    valid_input_features = []<br class="title-page-name"/>    valid_target_labels = []<br class="title-page-name"/><br class="title-page-name"/>    for batch_ind in range(1, num_batches + 1):<br class="title-page-name"/>        <br class="title-page-name"/>        #Loading batch<br class="title-page-name"/>        input_features, target_labels = load_batch(cifar10_batches_dir_path, batch_ind)<br class="title-page-name"/>        num_validation_images = int(len(input_features) * 0.1)<br class="title-page-name"/><br class="title-page-name"/>        # Preprocess the current batch and perisist it for future use<br class="title-page-name"/>        input_features = normalize_images(input_features[:-num_validation_images])<br class="title-page-name"/>        target_labels = one_hot_encode( target_labels[:-num_validation_images])<br class="title-page-name"/>        <br class="title-page-name"/>        #Persisting the preprocessed batch<br class="title-page-name"/>        pickle.dump((input_features, target_labels), open('preprocess_train_batch_' + str(batch_ind) + '.p', 'wb'))<br class="title-page-name"/>        <br class="title-page-name"/><br class="title-page-name"/>        # Define a subset of the training images to be used for validating our model<br class="title-page-name"/>        valid_input_features.extend(input_features[-num_validation_images:])<br class="title-page-name"/>        valid_target_labels.extend(target_labels[-num_validation_images:])<br class="title-page-name"/><br class="title-page-name"/>    # Preprocessing and persisting the validationi subset<br class="title-page-name"/>    input_features = normalize_images( np.array(valid_input_features))<br class="title-page-name"/>    target_labels = one_hot_encode(np.array(valid_target_labels))<br class="title-page-name"/>    <br class="title-page-name"/>    pickle.dump((input_features, target_labels), open('preprocess_valid.p', 'wb'))<br class="title-page-name"/>    <br class="title-page-name"/><br class="title-page-name"/>    #Now it's time to preporcess and persist the test batche<br class="title-page-name"/>    with open(cifar10_batches_dir_path + '/test_batch', mode='rb') as file:<br class="title-page-name"/>        test_batch = pickle.load(file, encoding='latin1')<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>    test_input_features = test_batch['data'].reshape((len(test_batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)<br class="title-page-name"/>    test_input_labels = test_batch['labels']<br class="title-page-name"/><br class="title-page-name"/>    # Normalizing and encoding the test batch<br class="title-page-name"/>    input_features = normalize_images( np.array(test_input_features))<br class="title-page-name"/>    target_labels = one_hot_encode(np.array(test_input_labels))<br class="title-page-name"/>    <br class="title-page-name"/>    pickle.dump((input_features, target_labels), open('preprocess_test.p', 'wb'))<br class="title-page-name"/>    <br class="title-page-name"/># Calling the helper function above to preprocess and persist the training, validation, and testing set<br class="title-page-name"/>preprocess_persist_data(cifar10_batches_dir_path, normalize_images, one_hot_encode)</pre>
<p class="calibre2">So, we have the preprocessed data saved to disk.</p>
<p class="calibre2">We <span class="calibre10">also </span>need to load the validation set for running the trained model on it at different epochs of the training process:</p>
<pre class="calibre21"># Load the Preprocessed Validation data<br class="title-page-name"/>valid_input_features, valid_input_labels = pickle.load(open('preprocess_valid.p', mode='rb'))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the network</h1>
                </header>
            
            <article>
                
<p class="calibre2">It's <span class="calibre10">now </span>time to build the core of our classification application, which is the computational graph of this CNN architecture, but to maximize the benefits of this implementation, we aren't going to use the TensorFlow layers API. Instead, we are going to use the TensorFlow neural network version of it.</p>
<p class="calibre2">So, let's start off by defining the model input placeholders which will input the images, target classes, and the keep probability parameter of the dropout layer (this helps us to reduce the complexity of the architecture by dropping some connections and hence reducing the chances of overfitting):</p>
<pre class="calibre21"><br class="title-page-name"/># Defining the model inputs<br class="title-page-name"/>def images_input(img_shape):<br class="title-page-name"/> return tf.placeholder(tf.float32, (None, ) + img_shape, name="input_images")<br class="title-page-name"/><br class="title-page-name"/>def target_input(num_classes):<br class="title-page-name"/> <br class="title-page-name"/> target_input = tf.placeholder(tf.int32, (None, num_classes), name="input_images_target")<br class="title-page-name"/> return target_input<br class="title-page-name"/><br class="title-page-name"/>#define a function for the dropout layer keep probability<br class="title-page-name"/>def keep_prob_input():<br class="title-page-name"/> return tf.placeholder(tf.float32, name="keep_prob")</pre>
<p class="calibre2">Next up, we need to use the TensorFlow neural network implementation version to build up our convolution layers with max pooling:</p>
<pre class="calibre21"># Applying a convolution operation to the input tensor followed by max pooling<br class="title-page-name"/>def conv2d_layer(input_tensor, conv_layer_num_outputs, conv_kernel_size, conv_layer_strides, pool_kernel_size, pool_layer_strides):<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/> input_depth = input_tensor.get_shape()[3].value<br class="title-page-name"/> weight_shape = conv_kernel_size + (input_depth, conv_layer_num_outputs,)<br class="title-page-name"/> <br class="title-page-name"/> <br class="title-page-name"/> #Defining layer weights and biases<br class="title-page-name"/> weights = tf.Variable(tf.random_normal(weight_shape))<br class="title-page-name"/> biases = tf.Variable(tf.random_normal((conv_layer_num_outputs,)))<br class="title-page-name"/> <br class="title-page-name"/> #Considering the biase variable<br class="title-page-name"/> conv_strides = (1,) + conv_layer_strides + (1,)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/> conv_layer = tf.nn.conv2d(input_tensor, weights, strides=conv_strides, padding='SAME')<br class="title-page-name"/> conv_layer = tf.nn.bias_add(conv_layer, biases)<br class="title-page-name"/><br class="title-page-name"/> conv_kernel_size = (1,) + conv_kernel_size + (1,)<br class="title-page-name"/><br class="title-page-name"/> pool_strides = (1,) + pool_layer_strides + (1,)<br class="title-page-name"/> pool_layer = tf.nn.max_pool(conv_layer, ksize=conv_kernel_size, strides=pool_strides, padding='SAME')<br class="title-page-name"/> return pool_layer</pre>
<p class="calibre2">As you have probably seen in the previous chapter, the output of the max pooling operation is a 4D tensor, which is not compatible with the required input format for the fully connected layers. So, we need to implement a flattened layer to convert the output of the max pooling layer from 4D to 2D tensor:</p>
<pre class="calibre21">#Flatten the output of max pooling layer to be fing to the fully connected layer which only accepts the output<br class="title-page-name"/># to be in 2D<br class="title-page-name"/>def flatten_layer(input_tensor):<br class="title-page-name"/>return tf.contrib.layers.flatten(input_tensor)</pre>
<p class="calibre2">Next up, we need to define a helper function that will enable us to add a fully connected layer to our architecture:</p>
<pre class="calibre21">#Define the fully connected layer that will use the flattened output of the stacked convolution layers<br class="title-page-name"/>#to do the actuall classification<br class="title-page-name"/>def fully_connected_layer(input_tensor, num_outputs):<br class="title-page-name"/> return tf.layers.dense(input_tensor, num_outputs)</pre>
<p class="calibre2">Finally, before using these helper functions to create the entire architecture, we need to create another one that will take the output of the fully connected layer and produce 10 real-valued corresponding to the number of classes that we have in the dataset:</p>
<pre class="calibre21">#Defining the output function<br class="title-page-name"/>def output_layer(input_tensor, num_outputs):<br class="title-page-name"/>    return  tf.layers.dense(input_tensor, num_outputs)</pre>
<p class="calibre2">So, let's go ahead and define the function that will put all these bits and pieces together and create a CNN with three convolution layers. Each one of them is followed by max pooling operations. We'll also have two fully connected layers, where each one of them is followed by a dropout layer to reduce the model complexity and prevent overfitting. Finally, we'll have the output layer to produce 10 real-valued vectors, where each value represents a score for each class being the correct one:</p>
<pre class="calibre21">def build_convolution_net(image_data, keep_prob):<br class="title-page-name"/> <br class="title-page-name"/> # Applying 3 convolution layers followed by max pooling layers<br class="title-page-name"/> conv_layer_1 = conv2d_layer(image_data, 32, (3,3), (1,1), (3,3), (3,3)) <br class="title-page-name"/> conv_layer_2 = conv2d_layer(conv_layer_1, 64, (3,3), (1,1), (3,3), (3,3))<br class="title-page-name"/> conv_layer_3 = conv2d_layer(conv_layer_2, 128, (3,3), (1,1), (3,3), (3,3))<br class="title-page-name"/><br class="title-page-name"/># Flatten the output from 4D to 2D to be fed to the fully connected layer<br class="title-page-name"/> flatten_output = flatten_layer(conv_layer_3)<br class="title-page-name"/><br class="title-page-name"/># Applying 2 fully connected layers with drop out<br class="title-page-name"/> fully_connected_layer_1 = fully_connected_layer(flatten_output, 64)<br class="title-page-name"/> fully_connected_layer_1 = tf.nn.dropout(fully_connected_layer_1, keep_prob)<br class="title-page-name"/> fully_connected_layer_2 = fully_connected_layer(fully_connected_layer_1, 32)<br class="title-page-name"/> fully_connected_layer_2 = tf.nn.dropout(fully_connected_layer_2, keep_prob)<br class="title-page-name"/> <br class="title-page-name"/> #Applying the output layer while the output size will be the number of categories that we have<br class="title-page-name"/> #in CIFAR-10 dataset<br class="title-page-name"/> output_logits = output_layer(fully_connected_layer_2, 10)<br class="title-page-name"/> <br class="title-page-name"/> #returning output<br class="title-page-name"/> return output_logits</pre>
<p class="calibre2">Let's call the preceding helper functions to build the network and define its loss and optimization criteria:</p>
<pre class="calibre21">#Using the helper function above to build the network<br class="title-page-name"/><br class="title-page-name"/>#First off, let's remove all the previous inputs, weights, biases form the previous runs<br class="title-page-name"/>tf.reset_default_graph()<br class="title-page-name"/><br class="title-page-name"/># Defining the input placeholders to the convolution neural network<br class="title-page-name"/>input_images = images_input((32, 32, 3))<br class="title-page-name"/>input_images_target = target_input(10)<br class="title-page-name"/>keep_prob = keep_prob_input()<br class="title-page-name"/><br class="title-page-name"/># Building the models<br class="title-page-name"/>logits_values = build_convolution_net(input_images, keep_prob)<br class="title-page-name"/><br class="title-page-name"/># Name logits Tensor, so that is can be loaded from disk after training<br class="title-page-name"/>logits_values = tf.identity(logits_values, name='logits')<br class="title-page-name"/><br class="title-page-name"/># defining the model loss<br class="title-page-name"/>model_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_values, labels=input_images_target))<br class="title-page-name"/><br class="title-page-name"/># Defining the model optimizer<br class="title-page-name"/>model_optimizer = tf.train.AdamOptimizer().minimize(model_cost)<br class="title-page-name"/><br class="title-page-name"/># Calculating and averaging the model accuracy<br class="title-page-name"/>correct_prediction = tf.equal(tf.argmax(logits_values, 1), tf.argmax(input_images_target, 1))<br class="title-page-name"/>accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='model_accuracy')<br class="title-page-name"/>tests.test_conv_net(build_convolution_net)</pre>
<p class="calibre2">Now that we have built the computational architecture of this network, it's time to kick off the training process and see some results.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training</h1>
                </header>
            
            <article>
                
<p class="calibre2">So, let's define a helper function that will make us able to kick off the training process. This function will take the input images, one-hot encoding of the target classes, and the keep probability value as input. Then, it will feed these values to the computational graph and call the model optimizer:</p>
<pre class="calibre21">#Define a helper function for kicking off the training process<br class="title-page-name"/>def train(session, model_optimizer, keep_probability, in_feature_batch, target_batch):<br class="title-page-name"/>session.run(model_optimizer, feed_dict={input_images: in_feature_batch, input_images_target: target_batch, keep_prob: keep_probability})</pre>
<p class="calibre2">We'll need to validate our model during different time steps in the training process, so we are going to define a helper function that will print out the accuracy of the model on the validation set:</p>
<pre class="calibre21">#Defining a helper funcitno for print information about the model accuracy and it's validation accuracy as well<br class="title-page-name"/>def print_model_stats(session, input_feature_batch, target_label_batch, model_cost, model_accuracy):<br class="title-page-name"/>    <br class="title-page-name"/>    validation_loss = session.run(model_cost, feed_dict={input_images: input_feature_batch, input_images_target: target_label_batch, keep_prob: 1.0})<br class="title-page-name"/>    validation_accuracy = session.run(model_accuracy, feed_dict={input_images: input_feature_batch, input_images_target: target_label_batch, keep_prob: 1.0})<br class="title-page-name"/>    <br class="title-page-name"/>    print("Valid Loss: %f" %(validation_loss))<br class="title-page-name"/>    print("Valid accuracy: %f" % (validation_accuracy))</pre>
<p class="calibre2">Let's also define the model hyperparameters, which we can use to tune the model for better performance:</p>
<pre class="calibre21"># Model Hyperparameters<br class="title-page-name"/>num_epochs = 100<br class="title-page-name"/>batch_size = 128<br class="title-page-name"/>keep_probability = 0.5</pre>
<p class="calibre2">Now, let's kick off the training process, but only for a single batch of the CIFAR-10 dataset, and see what the model accuracy based on this batch is.</p>
<p class="calibre2">Before that, however, we are going to define a helper function that will load a batch training and also separate the input images from the target classes:</p>
<pre class="calibre21"># Splitting the dataset features and labels to batches<br class="title-page-name"/>def batch_split_features_labels(input_features, target_labels, train_batch_size):<br class="title-page-name"/>    for start in range(0, len(input_features), train_batch_size):<br class="title-page-name"/>        end = min(start + train_batch_size, len(input_features))<br class="title-page-name"/>        yield input_features[start:end], target_labels[start:end]<br class="title-page-name"/><br class="title-page-name"/>#Loading the persisted preprocessed training batches<br class="title-page-name"/>def load_preprocess_training_batch(batch_id, batch_size):<br class="title-page-name"/>    filename = 'preprocess_train_batch_' + str(batch_id) + '.p'<br class="title-page-name"/>    input_features, target_labels = pickle.load(open(filename, mode='rb'))<br class="title-page-name"/><br class="title-page-name"/>    # Returning the training images in batches according to the batch size defined above<br class="title-page-name"/>    return batch_split_features_labels(input_features, target_labels, train_batch_size)</pre>
<p class="calibre2">Now, let's start the training process for one batch:</p>
<pre class="calibre21">print('Training on only a Single Batch from the CIFAR-10 Dataset...')<br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/> <br class="title-page-name"/> # Initializing the variables<br class="title-page-name"/> sess.run(tf.global_variables_initializer())<br class="title-page-name"/> <br class="title-page-name"/> # Training cycle<br class="title-page-name"/> for epoch in range(num_epochs):<br class="title-page-name"/> batch_ind = 1<br class="title-page-name"/> <br class="title-page-name"/> for batch_features, batch_labels in load_preprocess_training_batch(batch_ind, batch_size):<br class="title-page-name"/> train(sess, model_optimizer, keep_probability, batch_features, batch_labels)<br class="title-page-name"/> <br class="title-page-name"/> print('Epoch number {:&gt;2}, CIFAR-10 Batch Number {}: '.format(epoch + 1, batch_ind), end='')<br class="title-page-name"/> print_model_stats(sess, batch_features, batch_labels, model_cost, accuracy)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch number 85, CIFAR-10 Batch Number 1: Valid Loss: 1.490792<br class="title-page-name"/>Valid accuracy: 0.550000<br class="title-page-name"/>Epoch number 86, CIFAR-10 Batch Number 1: Valid Loss: 1.487118<br class="title-page-name"/>Valid accuracy: 0.525000<br class="title-page-name"/>Epoch number 87, CIFAR-10 Batch Number 1: Valid Loss: 1.309082<br class="title-page-name"/>Valid accuracy: 0.575000<br class="title-page-name"/>Epoch number 88, CIFAR-10 Batch Number 1: Valid Loss: 1.446488<br class="title-page-name"/>Valid accuracy: 0.475000<br class="title-page-name"/>Epoch number 89, CIFAR-10 Batch Number 1: Valid Loss: 1.430939<br class="title-page-name"/>Valid accuracy: 0.550000<br class="title-page-name"/>Epoch number 90, CIFAR-10 Batch Number 1: Valid Loss: 1.484480<br class="title-page-name"/>Valid accuracy: 0.525000<br class="title-page-name"/>Epoch number 91, CIFAR-10 Batch Number 1: Valid Loss: 1.345774<br class="title-page-name"/>Valid accuracy: 0.575000<br class="title-page-name"/>Epoch number 92, CIFAR-10 Batch Number 1: Valid Loss: 1.425942<br class="title-page-name"/>Valid accuracy: 0.575000<br class="title-page-name"/><br class="title-page-name"/>Epoch number 93, CIFAR-10 Batch Number 1: Valid Loss: 1.451115<br class="title-page-name"/>Valid accuracy: 0.550000<br class="title-page-name"/>Epoch number 94, CIFAR-10 Batch Number 1: Valid Loss: 1.368719<br class="title-page-name"/>Valid accuracy: 0.600000<br class="title-page-name"/>Epoch number 95, CIFAR-10 Batch Number 1: Valid Loss: 1.336483<br class="title-page-name"/>Valid accuracy: 0.600000<br class="title-page-name"/>Epoch number 96, CIFAR-10 Batch Number 1: Valid Loss: 1.383425<br class="title-page-name"/>Valid accuracy: 0.575000<br class="title-page-name"/>Epoch number 97, CIFAR-10 Batch Number 1: Valid Loss: 1.378877<br class="title-page-name"/>Valid accuracy: 0.625000<br class="title-page-name"/>Epoch number 98, CIFAR-10 Batch Number 1: Valid Loss: 1.343391<br class="title-page-name"/>Valid accuracy: 0.600000<br class="title-page-name"/>Epoch number 99, CIFAR-10 Batch Number 1: Valid Loss: 1.319342<br class="title-page-name"/>Valid accuracy: 0.625000<br class="title-page-name"/>Epoch number 100, CIFAR-10 Batch Number 1: Valid Loss: 1.340849<br class="title-page-name"/>Valid accuracy: 0.525000</pre>
<p class="calibre2">As you can see, the validation accuracy is not that good while training only on a single batch. Let's see how the validation accuracy is going to change based on <span class="calibre10">only</span><span class="calibre10"> </span><span class="calibre10">a full training process of the model:</span></p>
<pre class="calibre21">model_save_path = './cifar-10_classification'<br class="title-page-name"/><br class="title-page-name"/>with tf.Session() as sess:<br class="title-page-name"/> # Initializing the variables<br class="title-page-name"/> sess.run(tf.global_variables_initializer())<br class="title-page-name"/> <br class="title-page-name"/> # Training cycle<br class="title-page-name"/> for epoch in range(num_epochs):<br class="title-page-name"/> <br class="title-page-name"/> # iterate through the batches<br class="title-page-name"/> num_batches = 5<br class="title-page-name"/> <br class="title-page-name"/> for batch_ind in range(1, num_batches + 1):<br class="title-page-name"/> for batch_features, batch_labels in load_preprocess_training_batch(batch_ind, batch_size):<br class="title-page-name"/> train(sess, model_optimizer, keep_probability, batch_features, batch_labels)<br class="title-page-name"/> <br class="title-page-name"/> print('Epoch number{:&gt;2}, CIFAR-10 Batch Number {}: '.format(epoch + 1, batch_ind), end='')<br class="title-page-name"/> print_model_stats(sess, batch_features, batch_labels, model_cost, accuracy)<br class="title-page-name"/> <br class="title-page-name"/> # Save the trained Model<br class="title-page-name"/> saver = tf.train.Saver()<br class="title-page-name"/> save_path = saver.save(sess, model_save_path)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch number94, CIFAR-10 Batch Number 5: Valid Loss: 0.316593<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number95, CIFAR-10 Batch Number 1: Valid Loss: 0.285429<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number95, CIFAR-10 Batch Number 2: Valid Loss: 0.347411<br class="title-page-name"/>Valid accuracy: 0.825000<br class="title-page-name"/>Epoch number95, CIFAR-10 Batch Number 3: Valid Loss: 0.232483<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number95, CIFAR-10 Batch Number 4: Valid Loss: 0.294707<br class="title-page-name"/>Valid accuracy: 0.900000<br class="title-page-name"/>Epoch number95, CIFAR-10 Batch Number 5: Valid Loss: 0.299490<br class="title-page-name"/>Valid accuracy: 0.975000<br class="title-page-name"/>Epoch number96, CIFAR-10 Batch Number 1: Valid Loss: 0.302191<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number96, CIFAR-10 Batch Number 2: Valid Loss: 0.347043<br class="title-page-name"/>Valid accuracy: 0.750000<br class="title-page-name"/>Epoch number96, CIFAR-10 Batch Number 3: Valid Loss: 0.252851<br class="title-page-name"/>Valid accuracy: 0.875000<br class="title-page-name"/>Epoch number96, CIFAR-10 Batch Number 4: Valid Loss: 0.291433<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number96, CIFAR-10 Batch Number 5: Valid Loss: 0.286192<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number97, CIFAR-10 Batch Number 1: Valid Loss: 0.277105<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number97, CIFAR-10 Batch Number 2: Valid Loss: 0.305842<br class="title-page-name"/>Valid accuracy: 0.850000<br class="title-page-name"/>Epoch number97, CIFAR-10 Batch Number 3: Valid Loss: 0.215272<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number97, CIFAR-10 Batch Number 4: Valid Loss: 0.313761<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number97, CIFAR-10 Batch Number 5: Valid Loss: 0.313503<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number98, CIFAR-10 Batch Number 1: Valid Loss: 0.265828<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number98, CIFAR-10 Batch Number 2: Valid Loss: 0.308948<br class="title-page-name"/>Valid accuracy: 0.800000<br class="title-page-name"/>Epoch number98, CIFAR-10 Batch Number 3: Valid Loss: 0.232083<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number98, CIFAR-10 Batch Number 4: Valid Loss: 0.298826<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number98, CIFAR-10 Batch Number 5: Valid Loss: 0.297230<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number99, CIFAR-10 Batch Number 1: Valid Loss: 0.304203<br class="title-page-name"/>Valid accuracy: 0.900000<br class="title-page-name"/>Epoch number99, CIFAR-10 Batch Number 2: Valid Loss: 0.308775<br class="title-page-name"/>Valid accuracy: 0.825000<br class="title-page-name"/>Epoch number99, CIFAR-10 Batch Number 3: Valid Loss: 0.225072<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number99, CIFAR-10 Batch Number 4: Valid Loss: 0.263737<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number99, CIFAR-10 Batch Number 5: Valid Loss: 0.278601<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number100, CIFAR-10 Batch Number 1: Valid Loss: 0.293509<br class="title-page-name"/>Valid accuracy: 0.950000<br class="title-page-name"/>Epoch number100, CIFAR-10 Batch Number 2: Valid Loss: 0.303817<br class="title-page-name"/>Valid accuracy: 0.875000<br class="title-page-name"/>Epoch number100, CIFAR-10 Batch Number 3: Valid Loss: 0.244428<br class="title-page-name"/>Valid accuracy: 0.900000<br class="title-page-name"/>Epoch number100, CIFAR-10 Batch Number 4: Valid Loss: 0.280712<br class="title-page-name"/>Valid accuracy: 0.925000<br class="title-page-name"/>Epoch number100, CIFAR-10 Batch Number 5: Valid Loss: 0.278625<br class="title-page-name"/>Valid accuracy: 0.950000</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's test the trained model against the test set part of the CIFAR-10 dataset. First, we are going to define a helper function that will help us to visualize the predictions of some sample images and their corresponding true labels:</p>
<pre class="calibre21">#A helper function to visualize some samples and their corresponding predictions<br class="title-page-name"/>def display_samples_predictions(input_features, target_labels, samples_predictions):<br class="title-page-name"/> <br class="title-page-name"/> num_classes = 10<br class="title-page-name"/> <br class="title-page-name"/> cifar10_class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']<br class="title-page-name"/><br class="title-page-name"/>label_binarizer = LabelBinarizer()<br class="title-page-name"/> label_binarizer.fit(range(num_classes))<br class="title-page-name"/> label_inds = label_binarizer.inverse_transform(np.array(target_labels))<br class="title-page-name"/><br class="title-page-name"/>fig, axies = plt.subplots(nrows=4, ncols=2)<br class="title-page-name"/> fig.tight_layout()<br class="title-page-name"/> fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)<br class="title-page-name"/><br class="title-page-name"/>num_predictions = 4<br class="title-page-name"/> margin = 0.05<br class="title-page-name"/> ind = np.arange(num_predictions)<br class="title-page-name"/> width = (1. - 2. * margin) / num_predictions<br class="title-page-name"/><br class="title-page-name"/>for image_ind, (feature, label_ind, prediction_indicies, prediction_values) in enumerate(zip(input_features, label_inds, samples_predictions.indices, samples_predictions.values)):<br class="title-page-name"/> prediction_names = [cifar10_class_names[pred_i] for pred_i in prediction_indicies]<br class="title-page-name"/> correct_name = cifar10_class_names[label_ind]<br class="title-page-name"/><br class="title-page-name"/>axies[image_ind][0].imshow(feature)<br class="title-page-name"/> axies[image_ind][0].set_title(correct_name)<br class="title-page-name"/> axies[image_ind][0].set_axis_off()<br class="title-page-name"/><br class="title-page-name"/>axies[image_ind][1].barh(ind + margin, prediction_values[::-1], width)<br class="title-page-name"/> axies[image_ind][1].set_yticks(ind + margin)<br class="title-page-name"/> axies[image_ind][1].set_yticklabels(prediction_names[::-1])<br class="title-page-name"/> axies[image_ind][1].set_xticks([0, 0.5, 1.0])</pre>
<p class="calibre2">Now, let's restore the trained model and test it against the test set:</p>
<pre class="calibre21">test_batch_size = 64<br class="title-page-name"/>save_model_path = './cifar-10_classification'<br class="title-page-name"/>#Number of images to visualize<br class="title-page-name"/>num_samples = 4<br class="title-page-name"/><br class="title-page-name"/>#Number of top predictions<br class="title-page-name"/>top_n_predictions = 4<br class="title-page-name"/><br class="title-page-name"/>#Defining a helper function for testing the trained model<br class="title-page-name"/>def test_classification_model():<br class="title-page-name"/><br class="title-page-name"/> input_test_features, target_test_labels = pickle.load(open('preprocess_test.p', mode='rb'))<br class="title-page-name"/> loaded_graph = tf.Graph()<br class="title-page-name"/>with tf.Session(graph=loaded_graph) as sess:<br class="title-page-name"/> <br class="title-page-name"/> # loading the trained model<br class="title-page-name"/> model = tf.train.import_meta_graph(save_model_path + '.meta')<br class="title-page-name"/> model.restore(sess, save_model_path)<br class="title-page-name"/><br class="title-page-name"/># Getting some input and output Tensors from loaded model<br class="title-page-name"/> model_input_values = loaded_graph.get_tensor_by_name('input_images:0')<br class="title-page-name"/> model_target = loaded_graph.get_tensor_by_name('input_images_target:0')<br class="title-page-name"/> model_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')<br class="title-page-name"/> model_logits = loaded_graph.get_tensor_by_name('logits:0')<br class="title-page-name"/> model_accuracy = loaded_graph.get_tensor_by_name('model_accuracy:0')<br class="title-page-name"/> <br class="title-page-name"/> # Testing the trained model on the test set batches<br class="title-page-name"/> test_batch_accuracy_total = 0<br class="title-page-name"/> test_batch_count = 0<br class="title-page-name"/> <br class="title-page-name"/> for input_test_feature_batch, input_test_label_batch in batch_split_features_labels(input_test_features, target_test_labels, test_batch_size):<br class="title-page-name"/> test_batch_accuracy_total += sess.run(<br class="title-page-name"/> model_accuracy,<br class="title-page-name"/> feed_dict={model_input_values: input_test_feature_batch, model_target: input_test_label_batch, model_keep_prob: 1.0})<br class="title-page-name"/> test_batch_count += 1<br class="title-page-name"/><br class="title-page-name"/>print('Test set accuracy: {}\n'.format(test_batch_accuracy_total/test_batch_count))<br class="title-page-name"/><br class="title-page-name"/># print some random images and their corresponding predictions from the test set results<br class="title-page-name"/> random_input_test_features, random_test_target_labels = tuple(zip(*random.sample(list(zip(input_test_features, target_test_labels)), num_samples)))<br class="title-page-name"/> <br class="title-page-name"/> random_test_predictions = sess.run(<br class="title-page-name"/> tf.nn.top_k(tf.nn.softmax(model_logits), top_n_predictions),<br class="title-page-name"/> feed_dict={model_input_values: random_input_test_features, model_target: random_test_target_labels, model_keep_prob: 1.0})<br class="title-page-name"/> <br class="title-page-name"/> display_samples_predictions(random_input_test_features, random_test_target_labels, random_test_predictions)<br class="title-page-name"/><br class="title-page-name"/>#Calling the function<br class="title-page-name"/>test_classification_model()<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>INFO:tensorflow:Restoring parameters from ./cifar-10_classification<br class="title-page-name"/>Test set accuracy: 0.7540007961783439</pre>
<div class="CDPAlignCenter"><img src="assets/fc105794-646e-4a0a-8fe0-c7c432a1ccca.png" class="calibre110"/></div>
<p class="calibre2">Let's visualize another example to see some errors:</p>
<div class="CDPAlignCenter"><img src="assets/ee5f68c7-4a4e-40f9-9009-a78465f6559e.png" class="calibre111"/></div>
<p class="calibre2">Now, we have a test accuracy of around 75%, which is not bad for a simple CNN like the one we have used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">This chapter showed us how to make a CNN for classifying images in the CIFAR-10 dataset. The classification accuracy was about 79% - 80% on the test set. The output of the convolutional layers was also plotted, but it was difficult to see how the neural network recognizes and classifies the input images. Better visualization techniques are needed.</p>
<p class="calibre2">Next up, we'll use one of the modern and exciting practice of deep learning, which is transfer learning. Transfer learning allows you to use data-greedy architectures of deep learning with small datasets.</p>


            </article>

            
        </section>
    </body></html>