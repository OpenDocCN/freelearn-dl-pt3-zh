- en: Representation Learning - Implementing Word Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning is a science that is mainly based on statistics and linear
    algebra. Applying matrix operations is very common among most machine learning
    or deep learning architectures because of backpropagation. This is the main reason
    deep learning, or machine learning in general, accepts only real-valued quantities
    as input. This fact contradicts many applications, such as machine translation,
    sentiment analysis, and so on; they have text as an input. So, in order to use
    deep learning for this application, we need to have it in the form that deep learning
    accepts!
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to introduce the field of representation learning,
    which is a way to learn a real-valued representation from text while preserving
    the semantics of the actual text. For example, the representation of love should
    be very close to the representation of adore because they are used in very similar
    contexts.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to representation learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A practical example of the skip-gram architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Skip-gram Word2Vec implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to representation learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the machine learning algorithms or architectures that we have used so far
    require the input to be real-valued or matrices of real-valued quantities, and
    that's a common theme in machine learning. For example, in the convolution neural
    network, we had to feed raw pixel values of images as model inputs. In this part,
    we are dealing with text, so we need to encode our text somehow and produce real-valued
    quantities that can be fed to a machine learning algorithm. In order to encode
    input text as real-valued quantities, we need to use an intermediate science called
    **Natural Language Processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned that in this kind of pipeline, where we feed text to a machine
    learning model such as sentiment analysis, this will be problematic and won't
    work because we won't be able to apply backpropagation or any other operations
    such as dot product on the input, which is a string. So, we need to use a mechanism
    of NLP that will enable us to build an intermediate representation of the text
    that can carry the same information as the text and also be fed to the machine
    learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to convert each word or token in the input text to a real-valued vector.
    These vectors will be useless if they don''t carry the patterns, information,
    meaning, and semantics of the original input. For example, as in real text, the
    two words love and adore are very similar to each other and carry the same meaning.
    We need the resultant real-valued vectors that will represent them to be close
    to each other and be in the same vector space. So, the vector representation of
    these two words along with another word that isn''t similar to them will be like
    this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7e0177f-a510-4ef9-9034-667ec7828f6b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.1: Vector representation of words'
  prefs: []
  type: TYPE_NORMAL
- en: There are many techniques that can be used for this task. This family of techniques
    is called **embeddings**, where you're embedding text into another real-valued
    vector space.
  prefs: []
  type: TYPE_NORMAL
- en: As we'll see later on, this vector space is very interesting actually, because
    you will find out that you can drive a word's vectors from other words that are
    similar to it or even do some geography in this space.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2Vec is one of the widely used embedding techniques in the area of NLP.
    This model creates real-valued vectors from input text by looking at the contextual
    information the input word appears in. So, you will find out that similar words
    will be mentioned in very similar contexts, and hence the model will learn that
    those two words should be placed close to each other in the particular embedding
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the statements in the following diagram, the model will learn that the
    words **love** and **adore** share very similar contexts and should be placed
    very close to each other in the resulting vector space. The context of like could
    be a bit similar as well to the word love, but it won''t be as close to love as
    the word adore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0ad5b13-4057-486b-ae7d-90aac51560df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.2: Sample of sentiment sentences'
  prefs: []
  type: TYPE_NORMAL
- en: The Word2Vec model also relies on semantic features of input sentences; for
    example, the two words adore and love are mainly used in a positive context and
    usually precede noun phrases or nouns. Again, the model will learn that these
    two words have something in common and it will be more likely to put the vector
    representation of these two vectors in a similar context. So, the structure of
    the sentence will tell the Word2Vec model a lot about similar words.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, people feed a large corpus of text to the Word2Vec model. The model
    will learn to produce similar vectors for similar words, and it will do so for
    each unique word in the input text.
  prefs: []
  type: TYPE_NORMAL
- en: All of these words' vectors will be combined and the final output will be an
    embedding matrix where each row represents the real-valued vector representation
    of a specific unique word.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/312e4a86-0742-47ae-9db6-ab700a6d4376.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.3: Example of Word2Vec model pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: So, the final output of the model will be an embedding matrix for all the unique
    words in the training corpus. Usually, good embedding matrices could contain millions
    of real-valued vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Word2Vec modeling uses a window to scan the sentence and then tries to predict
    the vector of the middle word of that window based on its contextual information;
    the Word2Vec model will scan a sentence at a time. Similar to any machine learning
    technique, we need to define a cost function for the Word2Vec model and its corresponding
    optimization criteria that will make the model capable of generating real-valued
    vectors for each unique image and also relate the vectors to each other based
    on their contextual information
  prefs: []
  type: TYPE_NORMAL
- en: Building Word2Vec model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will go through some deeper details of how can we build
    a Word2Vec model. As we mentioned previously, our final goal is to have a trained
    model that will able to generate real-valued vector representation for the input
    textual data which is also called word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: During the training of the model, we will use the maximum likelihood method
    ([https://en.wikipedia.org/wiki/Maximum_likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood)),
    which can be used to maximize the probability of the next word *w[t]* in the input
    sentence given the previous words that the model has seen, which we can call *h*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This maximum likelihood method will be expressed in terms of the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8312e4a7-cfe0-47dd-9b24-42670c3aca92.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the *score* function computes a value to represent the compatibility
    of the target word *w[t]* with respect to the context *h*. This model will be
    trained on the input sequences while training to maximize the likelihood on the
    training input data (log likelihood is used for mathematical simplicity and derivation
    with the log):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad309d16-c7ef-4cef-9e0e-f3b791538f1d.png)'
  prefs: []
  type: TYPE_IMG
- en: So, the *ML* method will try to maximize the above equation which, will result
    in a probabilistic language model. But the calculation of this is very computationally
    expensive, as we need to compute each probability using the score function for
    all the words in the
  prefs: []
  type: TYPE_NORMAL
- en: vocabulary *V* words *w'*, in the corresponding current context *h* of this
    model. This will happen at every training step.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/202fe239-8362-46dc-ab60-3a54281cf6cd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: General architecture of a probabilistic language model'
  prefs: []
  type: TYPE_NORMAL
- en: Because of the computational expensiveness of building the probabilistic language
    model, people tend to use different techniques that are less computationally expensive,
    such as **Continuous Bag-of-Words** (**CBOW**) and skip-gram models.
  prefs: []
  type: TYPE_NORMAL
- en: 'These models are trained to build a binary classification with logistic regression
    to separate between the real target words *w[t]* and *h* noise or imaginary words
    ![](img/91f9ba2d-f6dc-4587-b482-b348d4b4560b.png)**,** which is in the same context.
    The following diagram simplifies this idea using the CBOW technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4663156d-d15c-4994-8fa0-9942283b6244.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: General architecture of skip-gram model'
  prefs: []
  type: TYPE_NORMAL
- en: 'The next diagram, shows the two architectures that you can use for building
    the Word2Vec model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ec47f15-c055-4e80-8173-a86d0670dc70.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: different architectures for the Word2Vec model'
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more formal, the objective function of these techniques maximizes the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bbd635d1-b9bd-42b3-915e-d6fcdd476168.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '**![](img/e3e0bbb6-3b7d-4a79-bcfa-1311153492a7.png)** is the probability of
    the binary logistic regression based on the model seeing the word *w* in the context
    *h* in the dataset *D***,** which is calculated in terms of the θ vector. This
    vector represents the learned embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9653f073-6172-4710-800d-692f0ca28fbf.png)is the imaginary or noisy
    words that we can generate from a noisy probabilistic distribution, such as the
    unigram of the training input examples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To sum up, the objective of these models is to discriminate between real and
    imaginary inputs, and hence assign higher probability to real words and less probability
    for the case of imaginary or noisy words.
  prefs: []
  type: TYPE_NORMAL
- en: This objective is maximized when the model assigns high probabilities to real
    words and low probabilities to noise words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Technically, the process of assigning high probability to real words is is
    called **negative sampling** ([https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)),
    and there is good mathematical motivation for using this loss function: the updates
    it proposes approximate the updates of the softmax function in the limit. But
    computationally, it is especially appealing because computing the loss function
    now scales only with the number of noise words that we select (*k*), and not all
    words in the vocabulary (*V*). This makes it much faster to train. We will actually
    make use of the very similar **noise-contrastive estimation** (**NCE**) ([https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf](https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf))
    loss, for which TensorFlow has a handy helper function, `tf.nn.nce_loss()`.'
  prefs: []
  type: TYPE_NORMAL
- en: A practical example of the skip-gram architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go through a practical example and see how skip-gram models will work
    in this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: First off, we need to make a dataset of words and their corresponding context.
    Defining the context is up to us, but it has to make sense. So, we'll take a window
    around the target word and take a word from the right and another from the left.
  prefs: []
  type: TYPE_NORMAL
- en: 'By following this contextual technique, we will end up with the following set
    of words and their corresponding context:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The generated words and their corresponding context will be represented as
    pairs of `(context, target)`. The idea of skip-gram models is the inverse of CBOW
    ones. In the skip- gram model, we will try to predict the context of the word
    based on its target word. For example, considering the first pair, the skip-gram
    model will try to predict `the` and `brown` from the target word `quick`, and
    so on. So, we can rewrite our dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a set of input and output pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Let's try to mimic the training process at specific step *t*. So, the skip-gram
    model will take the first training sample where the input is the word `quick`
    and the target output is the word `the`. Next, we need to construct the noisy
    input as well, so we are going to randomly select from the unigrams of the input
    data. For simplicity, the size of the noisy vector will be only one. For example,
    we can select the word `sheep` as a noisy example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can go ahead and compute the loss between the real pair and the noisy
    one as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01d022e1-f1ed-4382-9b53-b3ecb87473e9.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal in this case is to update the θ parameter to improve the previous objective
    function. Typically, we can use gradient for this. So, we will try to calculate
    the gradient of the loss with respect to the objective function parameter θ, which
    will be represented by ![](img/534a4675-7ec2-4f49-8ff8-7e28502eaabf.png).
  prefs: []
  type: TYPE_NORMAL
- en: After the training process, we can visualize some results based on their reduced
    dimensions of the real-valued vector representation. You will find that this vector
    space is very interesting because you can do lots of interesting stuff with it.
    For example, you can learn Analogy in this space by saying that king is to queen
    as man is to woman. We can even derive the woman vector by subtracting the king
    vector from the queen one and adding the man; the result of this will be very
    close to the actual learned vector of the woman. You can also learn geography
    in this space.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a72b1e7-003f-43b2-9360-c26ff8e4fc94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.7: Projection of the learned vectors to two dimensions using t-distributed
    stochastic neighbor embedding (t-SNE) dimensionality reduction technique'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example gives very good intuition behind these vectors and how
    they'll be useful for most NLP applications such as machine translation or **part-of-speech**
    (**POS**) tagging.
  prefs: []
  type: TYPE_NORMAL
- en: Skip-gram Word2Vec implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After understanding the mathematical details of how skip-gram models work, we
    are going to implement skip-gram, which encodes words into real-valued vectors
    that have certain properties (hence the name Word2Vec). By implementing this architecture,
    you will get a clue of how the process of learning another representation works.
  prefs: []
  type: TYPE_NORMAL
- en: Text is the main input for a lot of natural language processing applications
    such as machine translation, sentiment analysis, and text to speech systems. So,
    learning a real-valued representation for the text will help us use different
    deep learning techniques for these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In the early chapters of this book, we introduced something called one-hot encoding,
    which produces a vector of zeros except for the index of the word that this vector
    represents. So, you may wonder why we are not using it here. This method is very
    inefficient because usually you have a big set of distinct words, maybe something
    like 50,000 words, and using one-hot encoding for this will produce a vector of
    49,999 entries set to zero and only one entry set to one.
  prefs: []
  type: TYPE_NORMAL
- en: Having a very sparse input like this will result in a huge waste of computation
    because of the matrix multiplications that we'd do in the hidden layers of the
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af1fa91e-a31e-42f8-bbce-edde8e8bb703.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.8: One-hot encoding which will result in huge waste of computation'
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned previously, the outcome of using one-hot encoding will be a
    very sparse vector, especially when you have a huge amount of distinct words that
    you want to encode.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows that when we multiply this sparse vector of all
    zeros except for one entry by a matrix of weights, the output will be only the
    row of the matrix that corresponds to the one value of the sparse vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3c6bad7-0f91-483c-aa1f-74c6c20a6a2f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.9: The effect of multiplying a one-hot vector with almost all zeros
    by hidden layer weight matrix'
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this huge waste of computation, we will be using embeddings, which
    is just a fully-connected layer with some embedding weights. In this layer, we
    skip this inefficient multiplication and look up the embedding weights of the
    embedding layer from something called **weight matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: So, instead of the waste that results from the computation, we are going to
    use this weight lookup this weight matrix to find the embedding weights. First,
    need to build this lookup take. To do this, we are going to encode all the input
    words as integers, as shown in the following figure, and then to get the corresponding
    values for this word, we are going to use its integer representation as the row
    number in this weight matrix. The process of finding the corresponding embedding
    values of a specific word is called **embedding lookup.** As mentioned previously,
    the embedding layer will be just a fully connected layer, where the number of
    units represents the embedding dimension.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/489f39af-fc62-473d-9b21-b34bbb90ba1d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.10: Tokenized lookup table'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that this process is very intuitive and straightforward; we just
    need to follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the lookup table that will be considered as a weight matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the embedding layer as a fully connected hidden layer with specific number
    of units (embedding dimensions)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the weight matrix lookup as an alternative for the computationally unnecessary
    matrix multiplication
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, train the lookup table as any weight matrix
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we mentioned earlier, we are going to build a skip-gram Word2Vec model in
    this section, which is an efficient way of learning a representation for words
    while preserving the semantic information that the words have.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's go ahead and build a Word2Vec model using the skip-gram architecture, which
    is proven to better than others.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and pre-processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to define some helper functions that will enable
    us to build a good Word2Vec model. For this implementation, we are going to use
    a cleaned version of Wikipedia ([http://mattmahoney.net/dc/textdata.html](http://mattmahoney.net/dc/textdata.html)).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start off by importing the required packages for this implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we are going to define a class that will be used to download the dataset
    if it was not downloaded before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can have a look at the first 100 characters of this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we are going to preprocess the text, so we are going to define a helper
    function that will help us to replace special characters such as punctuation ones
    into a know token. Also, to reduce the amount of noise in the input text, you
    might want to remove words that don''t appear frequently in the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s call this function on the input text and have a look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how many words and distinct words we have for the pre-processed
    version of the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: And here, I'm creating dictionaries to covert words to integers and backwards,
    that is, integers to words. The integers are assigned in descending frequency
    order, so the most frequent word (`the`) is given the integer `0`, the next most
    frequent gets `1`, and so on. The words are converted to integers and stored in
    the list `int_words`.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier in this section, we need to use the integer indexes of
    the words to look up their values in the weight matrix, so we are going to words
    to integers and integers to words. This will help us to look up the words and
    also get the actual word of a specific index. For example, the most repeated word
    in the input text will be indexed at position 0, followed by the second most repeated
    one, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s define a function to create this lookup table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s call the defined function to create the lookup table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'To build a more accurate model, we can remove words that don''t change the
    context much  as `of`, `for`, `the`, and so on. So, it is practically proven that
    we can build more accurate models while discarding these kinds of words. The process
    of removing context-irrelevant words from the context is called **subsampling**.
    In order to define a general mechanism for word discarding, Mikolov introduced
    a function for calculating the discard probability of a certain word, which is
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e9b5ce4-d28c-4a71-aa15-1527b32e8d9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where:'
  prefs: []
  type: TYPE_NORMAL
- en: '*t* is a threshold parameter for word discarding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f(w[i])* is the frequency of a specific target word *w[i]* in the input dataset'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we are going to implement a helper function that will calculate the discarding
    probability of each word in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have a more refined and clean version of the input text.
  prefs: []
  type: TYPE_NORMAL
- en: We mentioned that the skip-gram architecture considers the context of the target
    word while producing its real-valued representation, so it defines a window around
    the target word that has size *C*.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of treating all contextual words equally, we are going to assign less
    weight for words that are a bit far from the target word. For example, if we choose
    the size of the window to be *C = 4*, then we are going to select a random number
    *L* from the range of 1 to *C*, and then sample *L* words from the history and
    the future of the current word. For more details about this, refer to the Mikolov
    et al paper at: [https://arxiv.org/pdf/1301.3781.pdf](https://arxiv.org/pdf/1301.3781.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s go ahead and define this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, let''s define a generator function to generate a random batch from the
    training samples and get the contextual word for each word in that batch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Next up, we are going to use the following structure to build the computational
    graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f0d6926a-bce1-4013-82a6-f3f5ab664f81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.11: Model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: So, as mentioned previously, we are going to use an embedding layer that will
    try to learn a special real-valued representation for these words. Thus, the words
    will be fed as one-hot vectors. The idea is to train this network to build up
    the weight matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s start off by creating the input to our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The weight or embedding matrix that we are trying to build will have the following
    shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Also, we don't have to implement the lookup function ourselves because it's
    already available in Tensorflow: `tf.nn.embedding_lookup()`. So, it will use the
    integer encoding of the words and locate their corresponding rows in the weight
    matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight matrix will be randomly initialized from a uniform distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It's very inefficient to update all the embedding weights of the embedding layer
    at once. Instead of this, we will use the negative sampling technique which will
    only update the weight of the correct word with a small subset of the incorrect
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Also, we don't have to implement this function ourselves as it's already there
    in TensorFlow **`tf.nn.sampled_softmax_loss`:**
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'To validate our trained model, we are going to sample some frequent or common
    words and some uncommon words and try to print our their closest set of words
    based on the learned representation of the skip-gram architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Now, we have all the bits and pieces for our model and we are ready to kick
    off the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go ahead and kick off the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code snippet for 10 epochs, you will get the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see from the output, the network somehow learned some semantically
    useful representation of the input words. To help us get a clearer picture of
    the embedding matrix, we are going to use a dimensionality reduction technique
    such as t-SNE to reduce the real-valued vectors to two dimensions, and then we''ll
    visualize them and label each point with its corresponding word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/e7cd06e7-0156-4b62-84f1-7650c2c067d7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.12: A visualization of word vectors'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we went through the idea of representation learning and why
    it's useful for doing deep learning or machine learning in general on input that's
    not in a real-valued form. Also, we covered one of the adopted techniques for
    converting words into real-valued vectors—Word2Vec—which has very interesting
    properties. Finally, we implemented the Word2Vec model using the skip-gram architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, you will see the practical use of these learned representations in
    a sentiment analysis example, where we need to convert the input text to real-valued
    vectors.
  prefs: []
  type: TYPE_NORMAL
