<html><head></head><body>
		<div id="_idContainer060">
			<h1 id="_idParaDest-153"><em class="italic"><a id="_idTextAnchor177"/>Chapter 5</em>: Reducing Noise with Autoencoders</h1>
			<p>Among the most interesting families of deep neural networks is the autoencoder family. As their name suggests, their sole purpose is to digest their input, and then reconstruct it back into its original shape. In other words, an autoencoder learns to copy its input to its output. Why? Because the side effect of this process is what we are after: not to produce a tag or classification, but to learn an efficient, high-quality representation of the images that have been passed to the autoencoder. The name of such a representation is <strong class="bold">encoding</strong>.</p>
			<p>How do they achieve this? By training two networks in tandem: an <strong class="bold">encoder</strong>, which takes images and produces the encoding, and a <strong class="bold">decoder</strong>, which takes the encoding and tries to reconstruct the input from its information.</p>
			<p>In this chapter, we will cover the basics, starting with a simple fully connected implementation of an autoencoder. Later, we'll create a more common and versatile convolutional autoencoder. We will also learn how to apply autoencoders in more practical contexts, such as denoising images, detecting outliers in a dataset, and creating an inverse image search index. Sound interesting?</p>
			<p>In this chapter, we will cover the following recipes:</p>
			<ul>
				<li>Creating a simple fully connected autoencoder</li>
				<li><a id="_idTextAnchor178"/><a id="_idTextAnchor179"/>Creating a convolutional autoencoder</li>
				<li>Denoising images with autoencoders</li>
				<li>Spotting outliers using autoencoders</li>
				<li>Creating an inverse image search index with deep learning</li>
				<li>Implementing a variational autoencoder</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor180"/>Technical requirements</h1>
			<p>Although using a GPU is always a good idea, some of these recipes (especially <em class="italic">Creating a simple fully connected autoencoder</em>) work well with a mid-tier CPU, such as an Intel i5 or i7. If any particular recipe depends on external resources or requires preparatory steps, you'll find specific preparation instructions in the <em class="italic">Getting ready</em> section. You can promptly access all the code for this chapter here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3qrHYaF">https://bit.ly/3qrHYaF</a>.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor181"/><a id="_idTextAnchor182"/>Creating a simple fully connected autoencoder</h1>
			<p><strong class="bold">Autoencoders</strong> are unusual <a id="_idIndexMarker373"/>in their design, as well as in terms of their functionality. That's why it's a great idea to master the basics of implementing, perhaps, the simplest version of an autoencoder: a fully connected one.</p>
			<p>In this recipe, we'll <a id="_idIndexMarker374"/>implement a fully connected autoencoder to reconstruct the images in <strong class="source-inline">Fashion-MNIST</strong>, a standard dataset that requires minimal preprocessing, allowing us to focus on the autoencoder itself.</p>
			<p>Are you ready? Let's get started!</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor183"/>Getting ready</h2>
			<p>Fortunately, <strong class="source-inline">Fashion-MNIST</strong> comes bundled with TensorFlow, so we don't need to download it on our own.</p>
			<p>We'll use <strong class="source-inline">OpenCV</strong>, a famous computer vision library, to create a mosaic so that we can compare the original images with the ones reconstructed by the autoencoder. You can install <strong class="source-inline">OpenCV</strong> effortlessly with <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python</p>
			<p>Now that all the preparations have been handled, let's take a look at the recipe!</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor184"/>How to do it…</h2>
			<p>Follow these <a id="_idIndexMarker375"/>steps, to implement a simple yet capable autoencoder:</p>
			<ol>
				<li>Import the necessary packages to implement the fully connected autoencoder:<p class="source-code">import cv2</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist</p><p class="source-code">from tensorflow.keras.layers import *</p></li>
				<li>Define a function that will build the autoencoder's architecture. By default, the encoding or latent vector dimension is <em class="italic">128</em>, but <em class="italic">16</em>, <em class="italic">32</em>, and <em class="italic">64</em> are good values too:<p class="source-code">def build_autoencoder(input_shape=784, encoding_dim=128):</p><p class="source-code">    input_layer = Input(shape=(input_shape,))</p><p class="source-code">    encoded = Dense(units=512)(input_layer)</p><p class="source-code">    encoded = ReLU()(encoded)</p><p class="source-code">    encoded = Dense(units=256)(encoded)</p><p class="source-code">    encoded = ReLU()(encoded)</p><p class="source-code">    encoded = Dense(encoding_dim)(encoded)</p><p class="source-code">    encoding = ReLU()(encoded)</p><p class="source-code">    decoded = Dense(units=256)(encoding)</p><p class="source-code">    decoded = ReLU()(decoded)</p><p class="source-code">    decoded = Dense(units=512)(decoded)</p><p class="source-code">    decoded = ReLU()(decoded)</p><p class="source-code">    decoded = Dense(units=input_shape)(decoded)</p><p class="source-code">    decoded = Activation('sigmoid')(decoded)</p><p class="source-code">    return Model(input_layer, decoded)</p></li>
				<li>Define a <a id="_idIndexMarker376"/>function that will plot a sample of general images against their original counterparts, in order to visually assess the autoencoder's performance:<p class="source-code">def plot_original_vs_generated(original, generated):</p><p class="source-code">    num_images = 15</p><p class="source-code">    sample = np.random.randint(0, len(original), </p><p class="source-code">                               num_images)</p></li>
				<li>The previous block selects 15 random indices, which we'll use to pick the same sample images from the <strong class="source-inline">original</strong> and <strong class="source-inline">generated</strong> batches. Next, let's define an inner function so that we can stack a sample of 15 images in a 3x5 grid:<p class="source-code">    def stack(data):</p><p class="source-code">        images = data[sample]</p><p class="source-code">        return np.vstack([np.hstack(images[:5]),</p><p class="source-code">                          np.hstack(images[5:10]),</p><p class="source-code">                          np.hstack(images[10:15])])</p></li>
				<li>Now, define another inner function so that we can add text on top of an image. This will be useful for distinguishing the generated images from the originals, as we'll see shortly:<p class="source-code">    def add_text(image, text, position):</p><p class="source-code">        pt1 = position</p><p class="source-code">        pt2 = (pt1[0] + 10 + (len(text) * 22),</p><p class="source-code">               pt1[1] - 45)</p><p class="source-code">        cv2.rectangle(image,</p><p class="source-code">                      pt1,</p><p class="source-code">                      pt2,</p><p class="source-code">                      (255, 255, 255),</p><p class="source-code">                      -1)</p><p class="source-code">        cv2.putText(image, text,</p><p class="source-code">                    position,</p><p class="source-code">                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,</p><p class="source-code">                    fontScale=1.3,</p><p class="source-code">                    color=(0, 0, 0),</p><p class="source-code">                    thickness=4)</p></li>
				<li>Wrap up <a id="_idIndexMarker377"/>this function by selecting the same images from the original and generated groups. Then, stack both groups together to form a mosaic, resize it so that it's 860x860 in size, label the original and generated tiles in the mosaic using <strong class="source-inline">add_text()</strong>, and display the result:<p class="source-code">    original = stack(original)</p><p class="source-code">    generated = stack(generated)</p><p class="source-code">    mosaic = np.vstack([original,</p><p class="source-code">                        generated])</p><p class="source-code">    mosaic = cv2.resize(mosaic, (860, 860), </p><p class="source-code">                        interpolation=cv2.INTER_AREA)</p><p class="source-code">    mosaic = cv2.cvtColor(mosaic, cv2.COLOR_GRAY2BGR)</p><p class="source-code">    add_text(mosaic, 'Original', (50, 100))</p><p class="source-code">    add_text(mosaic, 'Generated', (50, 520))</p><p class="source-code">    cv2.imshow('Mosaic', mosaic)</p><p class="source-code">    cv2.waitKey(0)</p></li>
				<li>Download (or load, if cached) <strong class="source-inline">Fashion-MNIST</strong>. Because this is not a classification problem, we are only keeping the images, not the labels:<p class="source-code">(X_train, _), (X_test, _) = fashion_mnist.load_data()</p></li>
				<li>Normalize the images:<p class="source-code">X_train = X_train.astype('float32') / 255.0</p><p class="source-code">X_test = X_test.astype('float32') / 255.0</p></li>
				<li>Reshape <a id="_idIndexMarker378"/>the images into vectors:<p class="source-code">X_train = X_train.reshape((X_train.shape[0], -1))</p><p class="source-code">X_test = X_test.reshape((X_test.shape[0], -1))</p></li>
				<li>Build the autoencoder and compile it. We'll use <strong class="source-inline">'adam'</strong> as the optimizer and mean squared Error (<strong class="source-inline">'mse'</strong>) as the loss function. Why? We're not interested in getting the classification right but reconstructing the input as closely as possible, which translates into minimizing the overall error:<p class="source-code">autoencoder = build_autoencoder()</p><p class="source-code">autoencoder.compile(optimizer='adam', loss='mse')</p></li>
				<li>Fit the autoencoder over 300 epochs, a figure high enough to allow the network to learn a good representation of the input. To speed up the training process a bit, we'll pass batches of <strong class="source-inline">1024</strong> vectors at a time (feel free to change the batch size based on your hardware capabilities). Notice how the input features are also the labels or targets:<p class="source-code">EPOCHS = 300</p><p class="source-code">BATCH_SIZE = 1024</p><p class="source-code">autoencoder.fit(X_train, X_train,</p><p class="source-code">                epochs=EPOCHS,</p><p class="source-code">                batch_size=BATCH_SIZE,</p><p class="source-code">                shuffle=True,</p><p class="source-code">                validation_data=(X_test, X_test))</p></li>
				<li>Make <a id="_idIndexMarker379"/>predictions on the test set (basically, generate copies of the test vectors):<p class="source-code">predictions = autoencoder.predict(X_test)</p></li>
				<li>Reshape the predictions and test vectors back to grayscale images of dimensions 28x28x1:<p class="source-code">original_shape = (X_test.shape[0], 28, 28)</p><p class="source-code">predictions = predictions.reshape(original_shape)</p><p class="source-code">X_test = X_test.reshape(original_shape)</p></li>
				<li>Generate a comparative plot of the original images against the ones produced by the autoencoder:<p class="source-code">plot_original_vs_generated(X_test, predictions)</p><p>Here's the result:</p></li>
			</ol>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B14768_05_001.jpg" alt="Figure 5.1 – Mosaic of the original images (top three rows) compared with the&#13;&#10;generated ones (bottom three rows)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Mosaic of the original images (top three rows) compared with the generated ones (bottom three rows)</p>
			<p>Judging by the results, our autoencoder did a pretty decent job. In all cases, the shape of the clothing items <a id="_idIndexMarker380"/>is well-preserved. However, it isn't as accurate at reconstructing the inner details, as shown by the T-shirt in the sixth row, fourth column, where the horizontal stripe in the original is missing in the produced copy.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor185"/>How it works…</h2>
			<p>In this recipe, we learned that autoencoders work by combining two networks into one: the encoder and <a id="_idIndexMarker381"/>the decoder. In the <strong class="source-inline">build_autoencoder()</strong> function, we implemented a fully connected autoencoding architecture, where the encoder portion takes a 784-element vector and outputs an encoding of 128 numbers. Then, the decoder picks up this encoding and expands it through several stacked dense (fully connected) layers, where the last one creates a 784-element vector (the same dimensions that the input contains). </p>
			<p>The training process thus consists of minimizing the distance or error between the input the encoder receives and the output the decoder produces. The only way to achieve this is to learn encodings that minimize the information loss when compressing the inputs. </p>
			<p>Although the loss function (in this case, <strong class="source-inline">MSE</strong>) is a good measure to see if the autoencoder is <a id="_idIndexMarker382"/>progressing in its learning, with these particular networks, visual verification is just as relevant, if not more. That's why we implemented the <strong class="source-inline">plot_original_vs_generated()</strong> function: to check that the copies look like their original counterparts. </p>
			<p>Why don't you try changing the encoding size? How does it affect the quality of the copies?</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor186"/>See also</h2>
			<p>If you're <a id="_idIndexMarker383"/>wondering why <strong class="source-inline">Fashion-MNIST</strong> exists at all, take a look at the official repository here: <a href="https://github.com/zalandoresearch/fashion-mnist">https://github.com/zalandoresearch/fashion-mnist</a>.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor187"/>Creating a convolutional autoencoder</h1>
			<p>As with regular neural networks, when it comes to images, using convolutions is usually the way to go. In the case of autoencoders, this is no different. In this recipe, we'll implement a <a id="_idIndexMarker384"/>convolutional autoencoder to reproduce images from <strong class="source-inline">Fashion-MNIST</strong>. </p>
			<p>The distinguishing factor is that in the decoder, we'll use reverse or transposed convolutions, which upscale volumes instead of downscaling them. This is what happens in traditional convolutional layers.</p>
			<p>This is an interesting recipe. Are you ready to begin?</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor188"/>Getting ready</h2>
			<p>Because there are convenience functions in TensorFlow for downloading <strong class="source-inline">Fashion-MNIST</strong>, we don't need to do any manual preparations on the data side. However, we must install <strong class="source-inline">OpenCV</strong> so that we can visualize the outputs of the autoencoder. This can be done with the following command:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python</p>
			<p>Without further ado, let's get started.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor189"/>How to do it…</h2>
			<p>Follow these <a id="_idIndexMarker385"/>steps to implement a fully functional convolutional autoencoder:</p>
			<ol>
				<li value="1">Let's import the necessary dependencies:<p class="source-code">import cv2</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist</p><p class="source-code">from tensorflow.keras.layers import * </p></li>
				<li>Define the <strong class="source-inline">build_autoencoder()</strong> function, which internally builds the autoencoder architecture and returns the encoder, the decoder, and the autoencoder itself. Start defining the input and the first set of 32 convolutional filters:<p class="source-code">def build_autoencoder(input_shape=(28, 28, 1),</p><p class="source-code">                      encoding_size=32,</p><p class="source-code">                      alpha=0.2):</p><p class="source-code">    inputs = Input(shape=input_shape)</p><p class="source-code">    encoder = Conv2D(filters=32,</p><p class="source-code">                     kernel_size=(3, 3),</p><p class="source-code">                     strides=2,</p><p class="source-code">                     padding='same')(inputs)</p><p class="source-code">    encoder = LeakyReLU(alpha=alpha)(encoder)</p><p class="source-code">    encoder = BatchNormalization()(encoder)</p><p>Define the second set of convolutions (64 this time):</p><p class="source-code">    encoder = Conv2D(filters=64,</p><p class="source-code">                     kernel_size=(3, 3),</p><p class="source-code">                     strides=2,</p><p class="source-code">                     padding='same')(encoder)</p><p class="source-code">    encoder = LeakyReLU(alpha=alpha)(encoder)</p><p class="source-code">    encoder = BatchNormalization()(encoder)</p><p>Define the <a id="_idIndexMarker386"/>output layers of the encoder:</p><p class="source-code">    encoder_output_shape = encoder.shape</p><p class="source-code">    encoder = Flatten()(encoder)</p><p class="source-code">    encoder_output = Dense(units=encoding_size)(encoder)</p><p class="source-code">    encoder_model = Model(inputs, encoder_output)</p></li>
				<li>In <em class="italic">Step 2</em>, we defined the encoder model, which is a regular convolutional neural network. The next block defines the decoder model, starting with the input and 64 transposed convolution filters:<p class="source-code">    decoder_input = Input(shape=(encoding_size,))</p><p class="source-code">    target_shape = tuple(encoder_output_shape[1:])</p><p class="source-code">    decoder = Dense(np.prod(target_shape))(decoder_input)</p><p class="source-code">    decoder = Reshape(target_shape)(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=64,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              strides=2,</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    decoder = LeakyReLU(alpha=alpha)(decoder)</p><p class="source-code">    decoder = BatchNormalization()(decoder)</p><p>Define the second set of transposed convolutions (32 this time):</p><p class="source-code">    decoder = Conv2DTranspose(filters=32,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              strides=2,</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    decoder = LeakyReLU(alpha=alpha)(decoder)</p><p class="source-code">    decoder = BatchNormalization()(decoder)</p><p>Define the <a id="_idIndexMarker387"/>output layer of the decoder:</p><p class="source-code">    decoder = Conv2DTranspose(filters=1,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    outputs = Activation('sigmoid')(decoder)</p><p class="source-code">    decoder_model = Model(decoder_input, outputs)</p></li>
				<li>The decoder uses <strong class="source-inline">Conv2DTranspose</strong> layers, which expand their inputs to generate larger output volumes. Notice that the further we go into the decoder, the fewer filters the <strong class="source-inline">Conv2DTranspose</strong> layers use. Finally, define the autoencoder:<p class="source-code">    encoder_model_output = encoder_model(inputs)</p><p class="source-code">    decoder_model_output = </p><p class="source-code">       decoder_model(encoder_model_output)</p><p class="source-code">    autoencoder_model = Model(inputs, </p><p class="source-code">       decoder_model_output)</p><p class="source-code">  return encoder_model, decoder_model, autoencoder_model</p><p>The autoencoder is the end-to-end architecture. This starts with the input layer, which goes into the encoder, and ends with an output layer, which is the result of passing the encoder's output through the decoder.</p></li>
				<li>Define a <a id="_idIndexMarker388"/>function that will plot a sample of general images against their original counterparts. This will help us visually assess the autoencoder's performance. (This is the same function we defined in the previous recipe. For a more complete explanation, refer to the <em class="italic">Creating a simple fully connected autoencoder</em> recipe of this chapter.) Take a look at the following code:<p class="source-code">def plot_original_vs_generated(original, generated):</p><p class="source-code">    num_images = 15</p><p class="source-code">    sample = np.random.randint(0, len(original), </p><p class="source-code">                               num_images)</p></li>
				<li>Define an inner helper function in order to stack a sample of images in a 3x5 grid:<p class="source-code">    def stack(data):</p><p class="source-code">        images = data[sample]</p><p class="source-code">        return np.vstack([np.hstack(images[:5]),</p><p class="source-code">                          np.hstack(images[5:10]),</p><p class="source-code">                          np.hstack(images[10:15])])</p></li>
				<li>Next, define a function that will put text on an image in a given position:<p class="source-code">def add_text(image, text, position):</p><p class="source-code">        pt1 = position</p><p class="source-code">        pt2 = (pt1[0] + 10 + (len(text) * 22),</p><p class="source-code">               pt1[1] - 45)</p><p class="source-code">        cv2.rectangle(image,</p><p class="source-code">                      pt1,</p><p class="source-code">                      pt2,</p><p class="source-code">                      (255, 255, 255),</p><p class="source-code">                      -1)</p><p class="source-code">        cv2.putText(image, text,</p><p class="source-code">                    position,</p><p class="source-code">                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,</p><p class="source-code">                    fontScale=1.3,</p><p class="source-code">                    color=(0, 0, 0),</p><p class="source-code">                    thickness=4)</p></li>
				<li>Finally, create <a id="_idIndexMarker389"/>a mosaic containing both the original and generated images:<p class="source-code">    original = stack(original)</p><p class="source-code">    generated = stack(generated)</p><p class="source-code">    mosaic = np.vstack([original,</p><p class="source-code">                        generated])</p><p class="source-code">    mosaic = cv2.resize(mosaic, (860, 860),</p><p class="source-code">                        interpolation=cv2.INTER_AREA)</p><p class="source-code">    mosaic = cv2.cvtColor(mosaic, cv2.COLOR_GRAY2BGR)</p><p class="source-code">    add_text(mosaic, 'Original', (50, 100))</p><p class="source-code">    add_text(mosaic, 'Generated', (50, 520))</p><p class="source-code">    cv2.imshow('Mosaic', mosaic)</p><p class="source-code">    cv2.waitKey(0)</p></li>
				<li>Download (or load, if cached) <strong class="source-inline">Fashion-MNIST</strong>. We are only interested in the images; therefore, we can drop the labels:<p class="source-code">(X_train, _), (X_test, _) = fashion_mnist.load_data()</p></li>
				<li>Normalize <a id="_idIndexMarker390"/>the images and add a channel dimension to them:<p class="source-code">X_train = X_train.astype('float32') / 255.0</p><p class="source-code">X_test = X_test.astype('float32') / 255.0</p><p class="source-code">X_train = np.expand_dims(X_train, axis=-1)</p><p class="source-code">X_test = np.expand_dims(X_test, axis=-1)</p></li>
				<li>Here, we are only interested in the autoencoder, so we'll ignore the other two return values of the <strong class="source-inline">build_autoencoder()</strong> function. However, in different circumstances, we could want to keep them. We'll train the model using <strong class="source-inline">'adam'</strong> and use <strong class="source-inline">'mse'</strong> as the loss function since we want to reduce the error, not optimize for classification accuracy:<p class="source-code">_, _, autoencoder = build_autoencoder(encoding_size=256)</p><p class="source-code">autoencoder.compile(optimizer='adam', loss='mse')</p></li>
				<li>Train the autoencoder over 300 epochs, in batches of 512 images at a time. Notice how the input images are also the labels:<p class="source-code">EPOCHS = 300</p><p class="source-code">BATCH_SIZE = 512</p><p class="source-code">autoencoder.fit(X_train, X_train,</p><p class="source-code">                epochs=EPOCHS,</p><p class="source-code">                batch_size=BATCH_SIZE,</p><p class="source-code">                shuffle=True,</p><p class="source-code">                validation_data=(X_test, X_test),</p><p class="source-code">                verbose=1)</p></li>
				<li>Make copies of the test set:<p class="source-code">predictions = autoencoder.predict(X_test)</p></li>
				<li>Reshape both <a id="_idIndexMarker391"/>the predictions and the test images back to 28x28 (no channel dimension):<p class="source-code">original_shape = (X_test.shape[0], 28, 28)</p><p class="source-code">predictions = predictions.reshape(original_shape)</p><p class="source-code">X_test = X_test.reshape(original_shape)</p><p class="source-code">predictions = (predictions * 255.0).astype('uint8')</p><p class="source-code">X_test = (X_test * 255.0).astype('uint8')</p></li>
				<li>Generate a comparative mosaic of the original images and the copies outputted by the autoencoder:<p class="source-code">plot_original_vs_generated(X_test, predictions)</p><p>Let's take a look at the result:</p></li>
			</ol>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B14768_05_002.jpg" alt="Figure 5.2 – Mosaic of the original images (top three rows), compared with &#13;&#10;those produced by the convolutional autoencoder (bottom three rows)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – Mosaic of the original images (top three rows), compared with those produced by the convolutional autoencoder (bottom three rows)</p>
			<p>As we can see, the <a id="_idIndexMarker392"/>autoencoder has learned a good encoding, which allowed it to reconstruct the input images with minimal detail loss. Let's head over to the next section to understand how it works!</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor190"/>How it works…</h2>
			<p>In this recipe, we learned that a convolutional autoencoder is one of the most common yet powerful <a id="_idIndexMarker393"/>members of this family of neural networks. The encoder portion of the architecture is a regular convolutional neural network that relies on convolutions and dense layers to downsize the output and produce a vector representation. The decoder is the interesting part because it has to deal with the converse problem: to reconstruct the input based on the synthesized feature vector, also known as an encoding.</p>
			<p>How does it do this? By using a transposed convolution (<strong class="source-inline">Conv2DTranspose</strong>). Unlike traditional <strong class="source-inline">Conv2D</strong> layers, these produce shallower volumes (fewer filters), but they are wider and taller. The result is an output layer with only one filter, and 28x28 dimensions, which is the same shape as the input. Fascinating, isn't it?</p>
			<p>The training process consists of minimizing the error between the output (the generated copies) and the input (the original images). Therefore, MSE is a fitting loss function because it provides us with this very information. </p>
			<p>Finally, we assessed <a id="_idIndexMarker394"/>the performance of the autoencoder by visually inspecting a sample of test images, along with their synthetic counterparts.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In an autoencoder, the size of the encoding is crucial to guarantee the decoder has enough information to reconstruct the input.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor191"/>See also</h2>
			<p>Here's a great <a id="_idIndexMarker395"/>explanation of transposed convolutions: <a href="https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba">https://towardsdatascience.com/transposed-convolution-demystified-84ca81b4baba</a>.</p>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor192"/>Denoising images with autoencoders</h1>
			<p>Using images to reconstruct their input is great, but are there more useful ways to apply autoencoders? Of course <a id="_idIndexMarker396"/>there are! One of them is <a id="_idIndexMarker397"/>image denoising. As the name suggests, this is the act of restoring damaged images by replacing the corrupted pixels and regions with sensible values. </p>
			<p>In this recipe, we'll purposely damage the images in <strong class="source-inline">Fashion-MNIST</strong>, and then train an autoencoder to denoise them. </p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor193"/>Getting ready</h2>
			<p><strong class="source-inline">Fashion-MNIST</strong> can easily be accessed using the convenience functions TensorFlow provides, so we don't need to manually download the dataset. On the other hand, because we'll be creating some visualizations using <strong class="source-inline">OpenCV</strong>, we must install it, as follows:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor194"/>How to do it…</h2>
			<p>Follow these <a id="_idIndexMarker398"/>steps to implement a convolutional <a id="_idIndexMarker399"/>autoencoder capable of restoring damaged images:</p>
			<ol>
				<li value="1">Import the required packages:<p class="source-code">import cv2</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist</p><p class="source-code">from tensorflow.keras.layers import *</p></li>
				<li>Define the <strong class="source-inline">build_autoencoder()</strong> function, which creates the corresponding neural architecture. Notice that this is the same architecture we implemented in the previous recipe; therefore, we won't go into too much detail here. For an in-depth explanation, please refer to the <em class="italic">Creating a convolutional autoencoder</em> recipe:<p class="source-code">def build_autoencoder(input_shape=(28, 28, 1),</p><p class="source-code">                      encoding_size=128,</p><p class="source-code">                      alpha=0.2):</p><p class="source-code">    inputs = Input(shape=input_shape)</p><p class="source-code">    encoder = Conv2D(filters=32,</p><p class="source-code">                     kernel_size=(3, 3),</p><p class="source-code">                     strides=2,</p><p class="source-code">                     padding='same')(inputs)</p><p class="source-code">    encoder = LeakyReLU(alpha=alpha)(encoder)</p><p class="source-code">    encoder = BatchNormalization()(encoder)</p><p class="source-code">    encoder = Conv2D(filters=64,</p><p class="source-code">                     kernel_size=(3, 3),</p><p class="source-code">                     strides=2,</p><p class="source-code">                     padding='same')(encoder)</p><p class="source-code">    encoder = LeakyReLU(alpha=alpha)(encoder)</p><p class="source-code">    encoder = BatchNormalization()(encoder)</p><p class="source-code">    encoder_output_shape = encoder.shape</p><p class="source-code">    encoder = Flatten()(encoder)</p><p class="source-code">    encoder_output = </p><p class="source-code">      Dense(units=encoding_size)(encoder)</p><p class="source-code">    encoder_model = Model(inputs, encoder_output)</p></li>
				<li>Now that <a id="_idIndexMarker400"/>we've created the encoder model, let's <a id="_idIndexMarker401"/>create the decoder:<p class="source-code">    decoder_input = Input(shape=(encoding_size,))</p><p class="source-code">    target_shape = tuple(encoder_output_shape[1:])</p><p class="source-code">    decoder = </p><p class="source-code">    Dense(np.prod(target_shape))(decoder_input)</p><p class="source-code">    decoder = Reshape(target_shape)(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=64,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              strides=2,</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    decoder = LeakyReLU(alpha=alpha)(decoder)</p><p class="source-code">    decoder = BatchNormalization()(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=32,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              strides=2,</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    decoder = LeakyReLU(alpha=alpha)(decoder)</p><p class="source-code">    decoder = BatchNormalization()(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=1,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    outputs = Activation('sigmoid')(decoder)</p><p class="source-code">    decoder_model = Model(decoder_input, outputs)</p></li>
				<li>Finally, define <a id="_idIndexMarker402"/>the autoencoder itself and <a id="_idIndexMarker403"/>return the three models:<p class="source-code">    encoder_model_output = encoder_model(inputs)</p><p class="source-code">    decoder_model_output = </p><p class="source-code">    decoder_model(encoder_model_output)</p><p class="source-code">    autoencoder_model = Model(inputs, </p><p class="source-code">                              decoder_model_output)</p><p class="source-code">    return encoder_model, decoder_model, autoencoder_model</p></li>
				<li>Define the <strong class="source-inline">plot_original_vs_generated()</strong> function, which creates a comparative mosaic of the original and generated images. We'll use this function later to show the noisy images and their restored counterparts. Similar to <strong class="source-inline">build_autoencoder()</strong>, this function works in the same way we defined it in the <em class="italic">Creating a simple fully connected autoencoder</em> recipe, so if you want a detailed explanation, please review that recipe:<p class="source-code">def plot_original_vs_generated(original, generated):</p><p class="source-code">    num_images = 15</p><p class="source-code">    sample = np.random.randint(0, len(original), </p><p class="source-code">                              num_images)</p></li>
				<li>Define an <a id="_idIndexMarker404"/>inner helper function that will stack <a id="_idIndexMarker405"/>a sample of images in a 3x5 grid:<p class="source-code">    def stack(data):</p><p class="source-code">        images = data[sample]</p><p class="source-code">        return np.vstack([np.hstack(images[:5]),</p><p class="source-code">                          np.hstack(images[5:10]),</p><p class="source-code">                          np.hstack(images[10:15])])</p></li>
				<li>Define a function that will put custom text on top of an image, in a certain location:<p class="source-code">def add_text(image, text, position):</p><p class="source-code">        pt1 = position</p><p class="source-code">        pt2 = (pt1[0] + 10 + (len(text) * 22),</p><p class="source-code">               pt1[1] - 45)</p><p class="source-code">        cv2.rectangle(image,</p><p class="source-code">                      pt1,</p><p class="source-code">                      pt2,</p><p class="source-code">                      (255, 255, 255),</p><p class="source-code">                      -1)</p><p class="source-code">        cv2.putText(image, text,</p><p class="source-code">                    position,</p><p class="source-code">                    fontFace=cv2.FONT_HERSHEY_SIMPLEX,</p><p class="source-code">                    fontScale=1.3,</p><p class="source-code">                    color=(0, 0, 0),</p><p class="source-code">                    thickness=4)</p></li>
				<li>Create the <a id="_idIndexMarker406"/>mosaic with both the original <a id="_idIndexMarker407"/>and the generated images, label each sub-grid, and display the result:<p class="source-code">    original = stack(original)</p><p class="source-code">    generated = stack(generated)</p><p class="source-code">    mosaic = np.vstack([original,</p><p class="source-code">                        generated])</p><p class="source-code">    mosaic = cv2.resize(mosaic, (860, 860),</p><p class="source-code">                        interpolation=cv2.INTER_AREA)</p><p class="source-code">    mosaic = cv2.cvtColor(mosaic, cv2.COLOR_GRAY2BGR)</p><p class="source-code">    add_text(mosaic, 'Original', (50, 100))</p><p class="source-code">    add_text(mosaic, 'Generated', (50, 520))</p><p class="source-code">    cv2.imshow('Mosaic', mosaic)</p><p class="source-code">    cv2.waitKey(0)</p></li>
				<li>Load <strong class="source-inline">Fashion-MNIST</strong> using TensorFlow's handy function. We will only keep the images since the labels are unnecessary:<p class="source-code">(X_train, _), (X_test, _) = fashion_mnist.load_data()</p></li>
				<li>Normalize the <a id="_idIndexMarker408"/>images and add a single color <a id="_idIndexMarker409"/>channel to them using <strong class="source-inline">np.expand_dims()</strong>:<p class="source-code">X_train = X_train.astype('float32') / 255.0</p><p class="source-code">X_test = X_test.astype('float32') / 255.0</p><p class="source-code">X_train = np.expand_dims(X_train, axis=-1)</p><p class="source-code">X_test = np.expand_dims(X_test, axis=-1)</p></li>
				<li>Generate two tensors with the same dimensions as <strong class="source-inline">X_train</strong> and <strong class="source-inline">X_test</strong>, respectively. These will <a id="_idIndexMarker410"/>correspond to random <strong class="bold">Gaussian</strong> noise that has a mean and standard deviation equal to <strong class="source-inline">0.5</strong>:<p class="source-code">train_noise = np.random.normal(loc=0.5, scale=0.5,</p><p class="source-code">                               size=X_train.shape)</p><p class="source-code">test_noise = np.random.normal(loc=0.5, scale=0.5,</p><p class="source-code">                              size=X_test.shape)</p></li>
				<li>Purposely damage both <strong class="source-inline">X_train</strong> and <strong class="source-inline">X_test</strong> by adding <strong class="source-inline">train_noise</strong> and <strong class="source-inline">test_noise</strong>, respectively. Make sure that the values remain between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong> using <strong class="source-inline">np.clip()</strong>: <p class="source-code">X_train_noisy = np.clip(X_train + train_noise, 0, 1)</p><p class="source-code">X_test_noisy = np.clip(X_test + test_noise, 0, 1)</p></li>
				<li>Create the autoencoder and compile it. We'll use <strong class="source-inline">'adam'</strong> as our optimizer and <strong class="source-inline">'mse'</strong> as our loss function, given that we're interested in reducing the error instead of improving accuracy:<p class="source-code">_, _, autoencoder = build_autoencoder(encoding_size=128)</p><p class="source-code">autoencoder.compile(optimizer='adam', loss='mse')</p></li>
				<li>Fit the model for <strong class="source-inline">300</strong> epochs, on batches of <strong class="source-inline">1024</strong> noisy images at a time. Notice that <a id="_idIndexMarker411"/>the features are the noisy images, while <a id="_idIndexMarker412"/>the labels or targets are the original ones, prior to being damaged:<p class="source-code">EPOCHS = 300</p><p class="source-code">BATCH_SIZE = 1024</p><p class="source-code">autoencoder.fit(X_train_noisy, X_train,</p><p class="source-code">                epochs=EPOCHS,</p><p class="source-code">                batch_size=BATCH_SIZE,</p><p class="source-code">                shuffle=True,</p><p class="source-code">                validation_data=(X_test_noisy,X_test))</p></li>
				<li>Make predictions with the trained model. Reshape both the noisy and generated images back to 28x28, and scale them up to the [0, 255] range: <p class="source-code">predictions = autoencoder.predict(X_test)</p><p class="source-code">original_shape = (X_test_noisy.shape[0], 28, 28)</p><p class="source-code">predictions = predictions.reshape(original_shape)</p><p class="source-code">X_test_noisy = X_test_noisy.reshape(original_shape)</p><p class="source-code">predictions = (predictions * 255.0).astype('uint8')</p><p class="source-code">X_test_noisy = (X_test_noisy * 255.0).astype('uint8')</p></li>
				<li>Finally, display the mosaic of noisy versus restored images:<p class="source-code">plot_original_vs_generated(X_test_noisy, predictions)</p><p>Here's the result:</p></li>
			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B14768_05_003.jpg" alt="Figure 5.3 – Mosaic of noisy images (top) versus the ones restored by the network (bottom)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – Mosaic of noisy images (top) versus the ones restored by the network (bottom)</p>
			<p>Look how <a id="_idIndexMarker413"/>damaged the images at the top are! The good <a id="_idIndexMarker414"/>news is that, in most instances, the autoencoder did a good job of restoring them. However, it couldn't denoise the images closer to the edges of the mosaic properly, which is a sign that more experimentation can be done to improve their performance (to be fair, these bad examples are hard to discern, even for humans).</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor195"/>How it works…</h2>
			<p>The novelty in this recipe is the practical use of the convolutional autoencoder. Both the network and other building blocks have been covered in depth in the last two recipes, so let's focus on the denoising problem itself.</p>
			<p>To recreate a real-life scenario of damaged images, we added a heavy amount of Gaussian noise to both the training and test sets in the <strong class="source-inline">Fashion-MNIST</strong> dataset. This kind of noise is known as salt and pepper because the damaged image looks as though it had these seasonings spilled all over it. </p>
			<p>To teach our <a id="_idIndexMarker415"/>autoencoder how the images once looked, we used <a id="_idIndexMarker416"/>the noisy ones as the features and the originals as the target or labels. This way, after 300 epochs, the network learned an encoding capable of, on many occasions, mapping salt and peppered instances to satisfyingly restored versions of them. </p>
			<p>Nonetheless, the model is not perfect, as we saw in the mosaic, where the network was unable to restore the images at the edges of the grid. This is a demonstration of how difficult repairing a damaged image can <a id="_idTextAnchor196"/><a id="_idTextAnchor197"/>be.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor198"/>Spotting outliers using autoencoders</h1>
			<p>Another great application of autoencoders is outlier detection. The idea behind this use case is that <a id="_idIndexMarker417"/>the autoencoder will learn an encoding with <a id="_idIndexMarker418"/>a very small error for the most common classes in a dataset, while its ability to reproduce scarcely represented categories (outliers) will be much more error-prone.</p>
			<p>With this premise in mind, in this recipe, we'll rely on a convolutional autoencoder to detect outliers in a subsample of <strong class="source-inline">Fashion-MNIST</strong>. </p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor199"/>Getting ready</h2>
			<p>To install <strong class="source-inline">OpenCV</strong>, use the following <strong class="source-inline">pip</strong> command:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python</p>
			<p>We'll rely on TensorFlow's built-in convenience functions to load the <strong class="source-inline">Fashion-MNIST</strong> dataset.</p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor200"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the required packages:<p class="source-code">import cv2</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist as fmnist</p><p class="source-code">from tensorflow.keras.layers import *</p></li>
				<li>Set a <a id="_idIndexMarker419"/>random seed to guarantee reproducibility:<p class="source-code">SEED = 84</p><p class="source-code">np.random.seed(SEED)</p></li>
				<li>Define a <a id="_idIndexMarker420"/>function that will build the autoencoder architecture. This function follows the same structure we studied in the <em class="italic">Creating a convolutional autoencoder</em> recipe, so if you want a deeper explanation, please go back to that recipe. Let's start by creating the encoder model:<p class="source-code">def build_autoencoder(input_shape=(28, 28, 1),</p><p class="source-code">                      encoding_size=96,</p><p class="source-code">                      alpha=0.2):</p><p class="source-code">    inputs = Input(shape=input_shape)</p><p class="source-code">    encoder = Conv2D(filters=32,</p><p class="source-code">                     kernel_size=(3, 3),</p><p class="source-code">                     strides=2,</p><p class="source-code">                     padding='same')(inputs)</p><p class="source-code">    encoder = LeakyReLU(alpha=alpha)(encoder)</p><p class="source-code">    encoder = BatchNormalization()(encoder)</p><p class="source-code">    encoder = Conv2D(filters=64,</p><p class="source-code">                     kernel_size=(3, 3),</p><p class="source-code">                     strides=2,</p><p class="source-code">                     padding='same')(encoder)</p><p class="source-code">    encoder = LeakyReLU(alpha=alpha)(encoder)</p><p class="source-code">    encoder = BatchNormalization()(encoder)</p><p class="source-code">    encoder_output_shape = encoder.shape</p><p class="source-code">    encoder = Flatten()(encoder)</p><p class="source-code">    encoder_output = Dense(encoding_size)(encoder)</p><p class="source-code">    encoder_model = Model(inputs, encoder_output)</p></li>
				<li>Next, build <a id="_idIndexMarker421"/>the <a id="_idIndexMarker422"/>decoder:<p class="source-code">    decoder_input = Input(shape=(encoding_size,))</p><p class="source-code">    target_shape = tuple(encoder_output_shape[1:])</p><p class="source-code">    decoder = Dense(np.prod(target_shape))(decoder_input)</p><p class="source-code">    decoder = Reshape(target_shape)(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=64,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              strides=2,</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    decoder = LeakyReLU(alpha=alpha)(decoder)</p><p class="source-code">    decoder = BatchNormalization()(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=32,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              strides=2,</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    decoder = LeakyReLU(alpha=alpha)(decoder)</p><p class="source-code">    decoder = BatchNormalization()(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=1,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    outputs = Activation('sigmoid')(decoder)</p><p class="source-code">    decoder_model = Model(decoder_input, outputs)</p></li>
				<li>Lastly, build <a id="_idIndexMarker423"/>the autoencoder and return <a id="_idIndexMarker424"/>the three models:<p class="source-code">    encoder_model_output = encoder_model(inputs)</p><p class="source-code">    decoder_model_output = </p><p class="source-code">    decoder_model(encoder_model_output)</p><p class="source-code">    autoencoder_model = Model(inputs, </p><p class="source-code">                              decoder_model_output)</p><p class="source-code">    return encoder_model, decoder_model, autoencoder_model</p></li>
				<li>Next, define a function that will contrive a dataset of two classes, where one of them represents an anomaly or outlier. Start by selecting the instances corresponding to the two classes of interest, and then shuffle them to break any possible ordering bias: <p class="source-code">def create_anomalous_dataset(features,</p><p class="source-code">                             labels,</p><p class="source-code">                             regular_label,</p><p class="source-code">                             anomaly_label,</p><p class="source-code">                             corruption_proportion=0.01):</p><p class="source-code">    regular_data_idx = np.where(labels == </p><p class="source-code">                                regular_label)[0]</p><p class="source-code">    anomalous_data_idx = np.where(labels == </p><p class="source-code">                                  anomaly_label)[0]</p><p class="source-code">    np.random.shuffle(regular_data_idx)</p><p class="source-code">    np.random.shuffle(anomalous_data_idx)</p></li>
				<li>Next, from the <a id="_idIndexMarker425"/>anomalous category, select <a id="_idIndexMarker426"/>a number of instances proportional to <strong class="source-inline">corruption_proportion</strong>. Finally, create the final dataset by merging the regular instances with the outliers:<p class="source-code">    num_anomalies = int(len(regular_data_idx) *</p><p class="source-code">                        corruption_proportion)</p><p class="source-code">    anomalous_data_idx = </p><p class="source-code">            anomalous_data_idx[:num_anomalies]</p><p class="source-code">    data = np.vstack([features[regular_data_idx],</p><p class="source-code">                      features[anomalous_data_idx]])</p><p class="source-code">    np.random.shuffle(data)</p><p class="source-code">    return data</p></li>
				<li>Load <strong class="source-inline">Fashion-MNIST</strong>. Merge both the train and test sets into a single dataset:<p class="source-code">(X_train, y_train), (X_test, y_test) = fmnist.load_data()</p><p class="source-code">X = np.vstack([X_train, X_test])</p><p class="source-code">y = np.hstack([y_train, y_test])</p></li>
				<li>Define the <a id="_idIndexMarker427"/>regular and anomalous <a id="_idIndexMarker428"/>labels, and then create the anomalous dataset:<p class="source-code">REGULAR_LABEL = 5  # Sandal</p><p class="source-code">ANOMALY_LABEL = 0  # T-shirt/top</p><p class="source-code">data = create_anomalous_dataset(X, y,</p><p class="source-code">                                REGULAR_LABEL,</p><p class="source-code">                                ANOMALY_LABEL)</p></li>
				<li>Add a channel dimension to the dataset, normalize it, and divide it into 80% for training and 20% for testing:<p class="source-code">data = np.expand_dims(data, axis=-1)</p><p class="source-code">data = data.astype('float32') / 255.0</p><p class="source-code">X_train, X_test = train_test_split(data,</p><p class="source-code">                                   train_size=0.8,</p><p class="source-code">                                   random_state=SEED)</p></li>
				<li>Build the autoencoder and compile it. We'll use <strong class="source-inline">'adam'</strong> as the optimizer and <strong class="source-inline">'mse'</strong> as the loss function since this gives us a good measure of the model's error:<p class="source-code">_, _, autoencoder = build_autoencoder(encoding_size=256)</p><p class="source-code">autoencoder.compile(optimizer='adam', loss='mse')</p></li>
				<li>Train the <a id="_idIndexMarker429"/>autoencoder for 300 epochs, on batches <a id="_idIndexMarker430"/>of <strong class="source-inline">1024</strong> images at a time:<p class="source-code">EPOCHS = 300</p><p class="source-code">BATCH_SIZE = 1024</p><p class="source-code">autoencoder.fit(X_train, X_train,</p><p class="source-code">                epochs=EPOCHS,</p><p class="source-code">                batch_size=BATCH_SIZE,</p><p class="source-code">                validation_data=(X_test, X_test))</p></li>
				<li>Make predictions on the data to find the outliers. We'll compute the mean squared error between the original image and the one produced by the autoencoder:<p class="source-code">decoded = autoencoder.predict(data)</p><p class="source-code">mses = []</p><p class="source-code">for original, generated in zip(data, decoded):</p><p class="source-code">    mse = np.mean((original - generated) ** 2)</p><p class="source-code">    mses.append(mse)</p></li>
				<li>Select the indices of the images with errors greater than the 99.9% quantile. These will be our outliers:<p class="source-code">threshold = np.quantile(mses, 0.999)</p><p class="source-code">outlier_idx = np.where(np.array(mses) &gt;= threshold)[0]</p><p class="source-code">print(f'Number of outliers: {len(outlier_idx)}')</p></li>
				<li>Save a <a id="_idIndexMarker431"/>comparative image of the original and <a id="_idIndexMarker432"/>generated images for each outlier:<p class="source-code">decoded = (decoded * 255.0).astype('uint8')</p><p class="source-code">data = (data * 255.0).astype('uint8')</p><p class="source-code">for i in outlier_idx:</p><p class="source-code">    image = np.hstack([data[i].reshape(28, 28),</p><p class="source-code">                       decoded[i].reshape(28, 28)])</p><p class="source-code">    cv2.imwrite(f'{i}.jpg', image)</p><p>Here's an example of an outlier:</p></li>
			</ol>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B14768_05_004.jpg" alt="Figure 5.4 – Left: Original outlier. Right: Reconstructed image.&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Left: Original outlier. Right: Reconstructed image.</p>
			<p>As we can see, we can harness the knowledge stored in the encoding learned by the autoencoder to easily detect anomalous or uncommon images in a dataset. We'll look at this in more detail in the next section.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor201"/>How it works…</h2>
			<p>The idea behind this recipe is very simple: outliers, by definition, are rare occurrences of an event or class within a dataset. Therefore, when we train an autoencoder on a dataset that contains outliers, it won't have sufficient time nor examples to learn a proper representation of them. </p>
			<p>By leveraging <a id="_idIndexMarker433"/>the low confidence (in other words, the high error) the <a id="_idIndexMarker434"/>network will display when reconstructing anomalous images (in this example, T-shirts), we can select the worst copies in order to spot outliers. </p>
			<p>However, for this technique to work, the autoencoder must be great at reconstructing the regular classes (for instance, sandals); otherwise, the false positive rate will be too high.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor202"/>Creating an inverse image search index with deep learning</h1>
			<p>Because the whole point of an autoencoder is to learn an encoding or a low-dimensional representation <a id="_idIndexMarker435"/>of a set of <a id="_idIndexMarker436"/>images, they make for great feature extractors. Furthermore, we can use them as the perfect building blocks of image search indices, as we'll discover in this recipe.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor203"/>Getting ready</h2>
			<p>Let's install <strong class="source-inline">OpenCV</strong> with <strong class="source-inline">pip</strong>. We'll use it to visualize the outputs of our autoencoder, in order to visually assess the effectiveness of the image search index:</p>
			<p class="source-code">$&gt; pip install opencv-python</p>
			<p>We'll start implementing the recipe in the next section.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor204"/>How to do it…</h2>
			<p>Follow these steps to create your own image search index:</p>
			<ol>
				<li value="1">Import the necessary libraries:<p class="source-code">import cv2</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist</p><p class="source-code">from tensorflow.keras.layers import *</p></li>
				<li>Define <strong class="source-inline">build_autoencoder()</strong>, which <a id="_idIndexMarker437"/>instantiates the autoencoder. First, let's assemble the <a id="_idIndexMarker438"/>encoder part:<p class="source-code">def build_autoencoder(input_shape=(28, 28, 1),</p><p class="source-code">                      encoding_size=32,</p><p class="source-code">                      alpha=0.2):</p><p class="source-code">    inputs = Input(shape=input_shape)</p><p class="source-code">    encoder = Conv2D(filters=32,</p><p class="source-code">                     kernel_size=(3, 3),</p><p class="source-code">                     strides=2,</p><p class="source-code">                     padding='same')(inputs)</p><p class="source-code">    encoder = LeakyReLU(alpha=alpha)(encoder)</p><p class="source-code">    encoder = BatchNormalization()(encoder)</p><p class="source-code">    encoder = Conv2D(filters=64,</p><p class="source-code">                     kernel_size=(3, 3),</p><p class="source-code">                     strides=2,</p><p class="source-code">                     padding='same')(encoder)</p><p class="source-code">    encoder = LeakyReLU(alpha=alpha)(encoder)</p><p class="source-code">    encoder = BatchNormalization()(encoder)</p><p class="source-code">    encoder_output_shape = encoder.shape</p><p class="source-code">    encoder = Flatten()(encoder)</p><p class="source-code">    encoder_output = Dense(units=encoding_size,</p><p class="source-code">                           name='encoder_output')(encoder)</p></li>
				<li>The <a id="_idIndexMarker439"/>next step is <a id="_idIndexMarker440"/>to define the decoder portion:<p class="source-code">    target_shape = tuple(encoder_output_shape[1:])</p><p class="source-code">    decoder = Dense(np.prod(target_shape))(encoder _output)</p><p class="source-code">    decoder = Reshape(target_shape)(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=64,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              strides=2,</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    decoder = LeakyReLU(alpha=alpha)(decoder)</p><p class="source-code">    decoder = BatchNormalization()(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=32,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              strides=2,</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    decoder = LeakyReLU(alpha=alpha)(decoder)</p><p class="source-code">    decoder = BatchNormalization()(decoder)</p><p class="source-code">    decoder = Conv2DTranspose(filters=1,</p><p class="source-code">                              kernel_size=(3, 3),</p><p class="source-code">                              padding='same')(decoder)</p><p class="source-code">    outputs = Activation(activation='sigmoid',</p><p class="source-code">                         </p><p class="source-code">                     name='decoder_output')(decoder)</p></li>
				<li>Finally, build the autoencoder and return it:<p class="source-code">    autoencoder_model = Model(inputs, outputs)</p><p class="source-code">    return autoencoder_model</p></li>
				<li>Define <a id="_idIndexMarker441"/>a function <a id="_idIndexMarker442"/>that will compute the Euclidean distance between two vectors:<p class="source-code">def euclidean_dist(x, y):</p><p class="source-code">    return np.linalg.norm(x - y)</p></li>
				<li>Define the <strong class="source-inline">search()</strong> function, which uses the search index (a dictionary of feature vectors paired with their corresponding images) to retrieve the most similar results to a query vector:<p class="source-code">def search(query_vector, search_index, </p><p class="source-code">           max_results=16):</p><p class="source-code">    vectors = search_index['features']</p><p class="source-code">    results = []</p><p class="source-code">    for i in range(len(vectors)):</p><p class="source-code">        distance = euclidean_dist(query_vector, </p><p class="source-code">                                   vectors[i])</p><p class="source-code">        results.append((distance, </p><p class="source-code">                       search_index['images'][i]))</p><p class="source-code">    results = sorted(results, </p><p class="source-code">                     key=lambda p: p[0])[:max_results]</p><p class="source-code">    return results</p></li>
				<li>Load the <strong class="source-inline">Fashion-MNIST</strong> dataset. Keep only the images:<p class="source-code">(X_train, _), (X_test, _) = fashion_mnist.load_data()     </p></li>
				<li>Normalize <a id="_idIndexMarker443"/>the images <a id="_idIndexMarker444"/>and add a color channel dimension:<p class="source-code">X_train = X_train.astype('float32') / 255.0</p><p class="source-code">X_test = X_test.astype('float32') / 255.0</p><p class="source-code">X_train = np.expand_dims(X_train, axis=-1)</p><p class="source-code">X_test = np.expand_dims(X_test, axis=-1)</p></li>
				<li>Build the autoencoder and compile it. We'll use <strong class="source-inline">'adam'</strong> as the optimizer and <strong class="source-inline">'mse'</strong> as the loss function since this gives us a good measure of the model's error:<p class="source-code">autoencoder = build_autoencoder()</p><p class="source-code">autoencoder.compile(optimizer='adam', loss='mse')</p></li>
				<li>Train the autoencoder for 10 epochs, on batches of <strong class="source-inline">512</strong> images at a time:<p class="source-code">EPOCHS = 50</p><p class="source-code">BATCH_SIZE = 512</p><p class="source-code">autoencoder.fit(X_train, X_train,</p><p class="source-code">                epochs=EPOCHS,</p><p class="source-code">                batch_size=BATCH_SIZE,</p><p class="source-code">                shuffle=True,</p><p class="source-code">                validation_data=(X_test, X_test))</p></li>
				<li>Create a new model, which we'll use as a feature extractor. It'll receive the same inputs as the autoencoder and will output the encoding learned by the autoencoder. In essence, we are using the encoder part of the autoencoder to turn images into vectors:<p class="source-code">fe_input = autoencoder.input</p><p class="source-code">fe_output = autoencoder.get_layer('encoder_output').output</p><p class="source-code">feature_extractor = Model(inputs=fe_input, </p><p class="source-code">                         outputs=fe_output)</p></li>
				<li>Create <a id="_idIndexMarker445"/>the search <a id="_idIndexMarker446"/>index, comprised of the feature vectors of <strong class="source-inline">X_train</strong>, along with the original images (which must be reshaped back to 28x28 and rescaled to the range [0, 255]):<p class="source-code">train_vectors = feature_extractor.predict(X_train)</p><p class="source-code">X_train = (X_train * 255.0).astype('uint8')</p><p class="source-code">X_train = X_train.reshape((X_train.shape[0], 28, 28))</p><p class="source-code">search_index = {</p><p class="source-code">    'features': train_vectors,</p><p class="source-code">    'images': X_train</p><p class="source-code">}</p></li>
				<li>Compute the feature vectors of <strong class="source-inline">X_test</strong>, which we will use as our sample of query images. Also, reshape <strong class="source-inline">X_test</strong> to 28x28 and rescale its values to the range [0, 255]:<p class="source-code">test_vectors = feature_extractor.predict(X_test)</p><p class="source-code">X_test = (X_test * 255.0).astype('uint8')</p><p class="source-code">X_test = X_test.reshape((X_test.shape[0], 28, 28))</p></li>
				<li>Select 16 random test images (with their corresponding feature vectors) to use as queries:<p class="source-code">sample_indices = np.random.randint(0, X_test.shape[0],16)</p><p class="source-code">sample_images = X_test[sample_indices]</p><p class="source-code">sample_queries = test_vectors[sample_indices]</p></li>
				<li>Perform <a id="_idIndexMarker447"/>a search <a id="_idIndexMarker448"/>for each of the images in the test sample and save a side-to-side visual comparison of the test query, along with the results fetched from the index (which, remember, is comprised of the train data):<p class="source-code">for i, (vector, image) in \</p><p class="source-code">        enumerate(zip(sample_queries, sample_images)):</p><p class="source-code">    results = search(vector, search_index)</p><p class="source-code">    results = [r[1] for r in results]</p><p class="source-code">    query_image = cv2.resize(image, (28 * 4, 28 * 4),</p><p class="source-code">                          interpolation=cv2.INTER_AREA)</p><p class="source-code">    results_mosaic = </p><p class="source-code">             np.vstack([np.hstack(results[0:4]),</p><p class="source-code">                        np.hstack(results[4:8]),</p><p class="source-code">                        np.hstack(results[8:12]),</p><p class="source-code">                        np.hstack(results[12:16])])</p><p class="source-code">    result_image = np.hstack([query_image, </p><p class="source-code">                             results_mosaic])</p><p class="source-code">    cv2.imwrite(f'{i}.jpg', result_image)</p><p>Here's an example of a search result:</p></li>
			</ol>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B14768_05_005.jpg" alt="Figure 5.5 – Left: Query image of a shoe. Right: The best 16 search results, all of which contain shoes too&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Left: Query image of a shoe. Right: The best 16 search results, all of which contain shoes too</p>
			<p>As the preceding image demonstrates, our image search index is a success! We'll see how it works in the next section.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor205"/>How it works…</h2>
			<p>In this recipe, we learned how to leverage the distinguishing trait of an autoencoder, which is to <a id="_idIndexMarker449"/>learn an encoding <a id="_idIndexMarker450"/>that greatly compresses the information in the input images, resulting in minimal loss of information. Then, we used the encoder part of a convolutional autoencoder to extract the features of fashion item photos and construct an image search index.</p>
			<p>By doing this, using this index as a search engine is as easy as computing the Euclidean distance between a query vector (corresponding to a query image) and all the images in the index, selecting only those that are closest to the query.</p>
			<p>The most important aspect in our solution is to train an autoencoder that is good enough to produce high-quality vectors, since they make or break the search engine.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor206"/>See also</h2>
			<p>The implementation is based on the great work of Dong <em class="italic">et al.</em>, whose paper can be read here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5/recipe5">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch5/recipe5</a>.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor207"/>Implementing a variational autoencoder</h1>
			<p>Some of the most modern and complex use cases of autoencoders are <strong class="bold">Variational Autoencoders</strong> (<strong class="bold">VAEs</strong>). They <a id="_idIndexMarker451"/>differ from the rest of the autoencoders in that, instead of learning an arbitrary function, they learn a probability distribution of the input images. We can then sample this distribution to produce new, unseen data points. </p>
			<p>A <strong class="bold">VAE</strong> is, in fact, a generative model, and in this recipe, we'll implement one.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor208"/>Getting ready</h2>
			<p>We don't need any special preparation for this recipe, so let's get started right away!</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor209"/>How to do it…</h2>
			<p>Follow these steps <a id="_idIndexMarker452"/>to learn how to implement and train a <strong class="bold">VAE</strong>:</p>
			<ol>
				<li value="1">Import the necessary packages:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras import backend as K</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import mse</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p></li>
				<li>Because we'll be using the <strong class="source-inline">tf.function</strong> annotation soon, we must tell TensorFlow to run functions eagerly:<p class="source-code">tf.config.experimental_run_functions_eagerly(True)</p></li>
				<li>Define a class that will encapsulate our implementation of the <strong class="bold">variational autoencoder</strong>. The constructor receives the dimensions of the input vector, the dimensions <a id="_idIndexMarker453"/>of the intermediate encoding, and the dimensions of the latent space (the probability distribution):<p class="source-code">class VAE(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 original_dimension=784,</p><p class="source-code">                 encoding_dimension=512,</p><p class="source-code">                 latent_dimension=2):</p><p class="source-code">        self.original_dimension = original_dimension</p><p class="source-code">        self.encoding_dimension = encoding_dimension</p><p class="source-code">        self.latent_dimension = latent_dimension</p><p><strong class="source-inline">self.z_log_var</strong> and <strong class="source-inline">self.z_mean</strong> are the parameters of the latent Gaussian distribution that we'll learn:</p><p class="source-code">        self.z_log_var = None</p><p class="source-code">        self.z_mean = None</p></li>
				<li>Define some members that will store the inputs and outputs of the <strong class="bold">VAE</strong> network, as well as the three models; that is, <strong class="source-inline">encoder</strong>, <strong class="source-inline">decoder</strong>, and <strong class="source-inline">vae</strong>:<p class="source-code">        self.inputs = None</p><p class="source-code">        self.outputs = None</p><p class="source-code">        self.encoder = None</p><p class="source-code">        self.decoder = None</p><p class="source-code">        self.vae = None</p></li>
				<li>Define the <strong class="source-inline">build_vae()</strong> method, which builds the variational autoencoder architecture (notice <a id="_idIndexMarker454"/>that we are using dense layers instead of convolutions):<p class="source-code">    def build_vae(self):</p><p class="source-code">        self.inputs = Input(shape=(self.original_dimension,))</p><p class="source-code">        x = Dense(self.encoding_dimension)(self.inputs)</p><p class="source-code">        x = ReLU()(x)</p><p class="source-code">        self.z_mean = Dense(self.latent_dimension)(x)</p><p class="source-code">        self.z_log_var = Dense(self.latent_dimension)(x)</p><p class="source-code">        z = Lambda(sampling)([self.z_mean, </p><p class="source-code">                             self.z_log_var])</p><p class="source-code">        self.encoder = Model(self.inputs,</p><p class="source-code">                             [self.z_mean, </p><p class="source-code">                             self.z_log_var, z])</p><p>Notice that the encoder is just a fully connected network that produces three outputs: <strong class="source-inline">self.z_mean</strong>, which is the mean of the Gaussian distribution we are training to model, <strong class="source-inline">self.z_log_var</strong>, which is the logarithmic variance of this distribution, and <strong class="source-inline">z</strong>, a sample point in that probability space. In order to generate the <strong class="source-inline">z</strong> simple, we must wrap a custom function, <strong class="source-inline">sampling()</strong> (implemented in <em class="italic">Step 5</em>), in a <strong class="source-inline">Lambda</strong> layer. </p></li>
				<li>Next, define the decoder:<p class="source-code">        latent_inputs = Input(shape=(self.latent_dimension,))</p><p class="source-code">        x = Dense(self.encoding_dimension)(latent_inputs)</p><p class="source-code">        x = ReLU()(x)</p><p class="source-code">        self.outputs = Dense(self.original_dimension)(x)</p><p class="source-code">        self.outputs = Activation('sigmoid')(self.outputs)</p><p class="source-code">        self.decoder = Model(latent_inputs, </p><p class="source-code">                             self.outputs)</p></li>
				<li>The decoder is just another fully connected network. The decoder will take samples <a id="_idIndexMarker455"/>from the latent dimension in order to reconstruct the inputs. Finally, connect the encoder and decoder to create the <strong class="bold">VAE</strong> model:<p class="source-code">        self.outputs = self.encoder(self.inputs)[2]</p><p class="source-code">        self.outputs = self.decoder(self.outputs)</p><p class="source-code">        self.vae = Model(self.inputs, self.outputs)</p></li>
				<li>Define the <strong class="source-inline">train()</strong> method, which trains the variational autoencoder. Therefore, it receives the train and test data, as well as the number of epochs and the batch size:<p class="source-code">    @tf.function</p><p class="source-code">    def train(self, X_train,</p><p class="source-code">              X_test, </p><p class="source-code">              epochs=50, </p><p class="source-code">              batch_size=64):</p></li>
				<li>Define the reconstruction loss as the MSE between the inputs and outputs:<p class="source-code">        reconstruction_loss = mse(self.inputs, </p><p class="source-code">                                  self.outputs)</p><p class="source-code">        reconstruction_loss *= self.original_dimension</p><p><strong class="source-inline">kl_loss</strong> is the <strong class="bold">Kullback-Leibler</strong> divergence <a id="_idIndexMarker456"/>between the learned latent distribution and the prior distribution. It is used <a id="_idIndexMarker457"/>as a regularization term for <strong class="source-inline">reconstruction_loss</strong>:</p><p class="source-code">        kl_loss = (1 + self.z_log_var -</p><p class="source-code">                   K.square(self.z_mean) -</p><p class="source-code">                   K.exp(self.z_log_var))</p><p class="source-code">        kl_loss = K.sum(kl_loss, axis=-1)</p><p class="source-code">        kl_loss *= -0.5</p><p class="source-code">        vae_loss = K.mean(reconstruction_loss + kl_loss)</p></li>
				<li>Configure the <strong class="source-inline">self.vae</strong> model so that it uses <strong class="source-inline">vae_loss</strong> and <strong class="source-inline">Adam()</strong> as the optimizer (with a learning rate of 0.003). Then, fit the network over the specified number of epochs. Finally, return the three models:<p class="source-code">        self.vae.add_loss(vae_loss)</p><p class="source-code">        self.vae.compile(optimizer=Adam(lr=1e-3))</p><p class="source-code">        self.vae.fit(X_train,</p><p class="source-code">                     epochs=epochs,</p><p class="source-code">                     batch_size=batch_size,</p><p class="source-code">                     validation_data=(X_test, None))</p><p class="source-code">        return self.encoder, self.decoder, self.vae</p></li>
				<li>Define a function that will generate a random sample or point from the latent space, given the two relevant parameters (passed in the <strong class="source-inline">arguments</strong> array); that is, <strong class="source-inline">z_mean</strong> and <strong class="source-inline">z_log_var</strong>:<p class="source-code">def sampling(arguments):</p><p class="source-code">    z_mean, z_log_var = arguments</p><p class="source-code">    batch = K.shape(z_mean)[0]</p><p class="source-code">    dimension = K.int_shape(z_mean)[1]</p><p class="source-code">    epsilon = K.random_normal(shape=(batch, dimension))</p><p class="source-code">    return z_mean + K.exp(0.5 * z_log_var) * epsilon</p><p>Notice that <strong class="source-inline">epsilon</strong> is a random Gaussian vector.</p></li>
				<li>Define a <a id="_idIndexMarker458"/>function that will generate and plot images generated from the latent space. This will give us an idea of the <strong class="bold">shapes</strong> that are closer to the distribution, and the ones that are nearer to the tails of the curve:<p class="source-code">def generate_and_plot(decoder, grid_size=5):</p><p class="source-code">    cell_size = 28</p><p class="source-code">    figure_shape = (grid_size * cell_size,</p><p class="source-code">                    grid_size * cell_size)</p><p class="source-code">    figure = np.zeros(figure_shape)</p></li>
				<li>Create a range of values that span from -4 to 4 in both the X and Y axes. We'll use these to generate and visualize samples at each location:<p class="source-code">    grid_x = np.linspace(-4, 4, grid_size)</p><p class="source-code">    grid_y = np.linspace(-4, 4, grid_size)[::-1]</p></li>
				<li>Use the decoder to generate a new sample for each combination of <strong class="source-inline">z_mean</strong> and <strong class="source-inline">z_log_var</strong>:<p class="source-code">    for i, z_log_var in enumerate(grid_y):</p><p class="source-code">        for j, z_mean in enumerate(grid_x):</p><p class="source-code">            z_sample = np.array([[z_mean, z_log_var]])</p><p class="source-code">            generated = decoder.predict(z_sample)[0]</p></li>
				<li>Reshape <a id="_idIndexMarker459"/>the sample and place it in the corresponding cell in the grid:<p class="source-code">            fashion_item = </p><p class="source-code">                   generated.reshape(cell_size,</p><p class="source-code">                                    cell_size)</p><p class="source-code">            y_slice = slice(i * cell_size, </p><p class="source-code">                            (i + 1) * cell_size)</p><p class="source-code">            x_slice = slice(j * cell_size, </p><p class="source-code">                            (j + 1) * cell_size)</p><p class="source-code">            figure[y_slice, x_slice] = fashion_item</p></li>
				<li>Add the ticks and axes labels, and then display the plot:<p class="source-code">    plt.figure(figsize=(10, 10))</p><p class="source-code">    start = cell_size // 2</p><p class="source-code">    end = (grid_size - 2) * cell_size + start + 1</p><p class="source-code">    pixel_range = np.arange(start, end, cell_size)</p><p class="source-code">    sample_range_x = np.round(grid_x, 1)</p><p class="source-code">    sample_range_y = np.round(grid_y, 1)</p><p class="source-code">    plt.xticks(pixel_range, sample_range_x)</p><p class="source-code">    plt.yticks(pixel_range, sample_range_y)</p><p class="source-code">    plt.xlabel('z_mean')</p><p class="source-code">    plt.ylabel('z_log_var')</p><p class="source-code">    plt.imshow(figure)</p><p class="source-code">    plt.show()</p></li>
				<li>Load <a id="_idIndexMarker460"/>the <strong class="source-inline">Fashion-MNIST</strong> dataset. Normalize the images and add a color channel to them:<p class="source-code">(X_train, _), (X_test, _) = fashion_mnist.load_data()</p><p class="source-code">X_train = X_train.astype('float32') / 255.0</p><p class="source-code">X_test = X_test.astype('float32') / 255.0</p><p class="source-code">X_train = X_train.reshape((X_train.shape[0], -1))</p><p class="source-code">X_test = X_test.reshape((X_test.shape[0], -1))</p></li>
				<li>Instantiate and build the <strong class="bold">variational autoencoder</strong>:<p class="source-code">vae = VAE(original_dimension=784,</p><p class="source-code">          encoding_dimension=512,</p><p class="source-code">          latent_dimension=2)</p><p class="source-code">vae.build_vae()</p></li>
				<li>Train the models for 100 epochs:<p class="source-code">_, decoder_model, vae_model = vae.train(X_train, X_test, </p><p class="source-code">                                        epochs=100)</p></li>
				<li>Use the decoder to generate new images and plot the result:<p class="source-code">generate_and_plot(decoder_model, grid_size=7)</p><p>Here's the result:	</p></li>
			</ol>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B14768_05_006.jpg" alt="Figure 5.6 – Visualization of the latent space learned by the VAE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Visualization of the latent space learned by the VAE</p>
			<p>Here, we can see the collection of points that comprise the latent space and the corresponding <a id="_idIndexMarker461"/>clothing item for each of these points. This is a representation of the probability distribution the network learned, in which the item at the center of such a distribution resembles a T-shirt, while the ones at the edges look more like pants, sweaters, and shoes.</p>
			<p>Let's move on to the next section.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor210"/>How it works…</h2>
			<p>In this recipe, we learned that a <strong class="bold">variational autoencoder</strong> is an advanced, more complex type <a id="_idIndexMarker462"/>of autoencoder that, instead of learning an arbitrary, vanilla function to map inputs to outputs, learns a probability distribution of the inputs. This gives it the ability to generate new, unseen images that make it a precursor of more modern generative models, such as <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>).</p>
			<p>The architecture <a id="_idIndexMarker463"/>is not that different from the others autoencoder we studied in this chapter. The key to understanding the power of a <strong class="bold">VAE</strong> is that the link between the encoder and the decoder is a random sample, <strong class="source-inline">z</strong>, which we generate using the <strong class="source-inline">sampling()</strong> function, within a Lambda layer. </p>
			<p>This means <a id="_idIndexMarker464"/>that in each iteration, the whole network is optimizing the <strong class="source-inline">z_mean</strong> and <strong class="source-inline">z_log_var</strong> parameters so that it closely resembles the probability distribution of the inputs. It does this because it's the only way the random samples (<strong class="source-inline">z</strong>) are going to be of such high qualit<a id="_idTextAnchor211"/><a id="_idTextAnchor212"/>y that the decoder will be able to generate better, more realistic outputs. </p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor213"/>See also</h2>
			<p>A key component we can use to tune the <strong class="bold">VAE</strong> is the <strong class="bold">Kullback-Leibler</strong> divergence, which <a id="_idIndexMarker465"/>you can read more about here: <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence</a>. </p>
			<p>Note that <strong class="bold">VAE</strong>s are the perfect runway to generative models, which we'll cover in depth in the next chapter!</p>
		</div>
	</body></html>