["```\nimport os\nimport random\nfrom typing import Dict\nimport gym\nimport numpy as np\nimport pandas as pd\nfrom gym import spaces\n```", "```\n    from trading_utils import TradeVisualizer\n    ```", "```\n    env_config = {\n        \"exchange\": \"Gemini\", # Cryptocurrency exchange\n        # (Gemini, coinbase, kraken, etc.)\n        \"ticker\": \"BTCUSD\", # CryptoFiat\n        \"frequency\": \"daily\", # daily/hourly/minutes\n        \"opening_account_balance\": 100000,\n        # Number of steps (days) of data provided to the \n        # agent in one observation.\n        \"observation_horizon_sequence_length\": 30,\n        \"order_size\": 1, # Number of coins to buy per \n        # buy/sell order\n    }\n    ```", "```\n    class CryptoTradingEnv(gym.Env):\n        def __init__(self, env_config: Dict = env_config):\n            super(CryptoTradingEnv, self).__init__()\n            self.ticker = env_config.get(\"ticker\", \"BTCUSD\")\n            data_dir = os.path.join(os.path.dirname(os.path.\\\n                                 realpath(__file__)), \"data\")\n            self.exchange = env_config[\"exchange\"]\n            freq = env_config[\"frequency\"]\n            if freq == \"daily\":\n                self.freq_suffix = \"d\"\n            elif freq == \"hourly\":\n                self.freq_suffix = \"1hr\"\n            elif freq == \"minutes\":\n                self.freq_suffix = \"1min\"\n    ```", "```\n            self.ticker_file_stream = os.path.join(\n                f\"{data_dir}\",\n                f\"{'_'.join([self.exchange, self.ticker,\n                             self.freq_suffix])}.csv\",\n            )\n            assert os.path.isfile(\n                self.ticker_file_stream\n            ), f\"Cryptocurrency data file stream not found \\\n              at: data/{self.ticker_file_stream}.csv\"\n            # Cryptocurrency exchange data stream. An offline \n            # file stream is used. Alternatively, a web\n            # API can be used to pull live data.\n            self.ohlcv_df = pd.read_csv(self.ticker_file_\\\n                stream, skiprows=1).sort_values(by=\"Date\"\n            )\n    ```", "```\n    self.opening_account_balance = env_config[\"opening_account_balance\"]\n    ```", "```\n            # Action: 0-> Hold; 1-> Buy; 2 ->Sell;\n            self.action_space = spaces.Discrete(3)\n            self.observation_features = [\n                \"Open\",\n                \"High\",\n                \"Low\",\n                \"Close\",\n                \"Volume BTC\",\n                \"Volume USD\",\n            ]\n            self.horizon = env_config.get(\n                       \"observation_horizon_sequence_length\")\n            self.observation_space = spaces.Box(\n                low=0,\n                high=1,\n                shape=(len(self.observation_features), \n                           self.horizon + 1),\n                dtype=np.float,\n            )\n    ```", "```\n            self.order_size = env_config.get(\"order_size\")\n    ```", "```\n    def step(self, action):\n            # Execute one step within the trading environment\n            self.execute_trade_action(action)\n            self.current_step += 1\n            reward = self.account_value - \\\n                     self.opening_account_balance  \n                    # Profit (loss)\n            done = self.account_value <= 0 or \\\n                   self.current_step >= len(\n                self.ohlcv_df.loc[:, \"Open\"].values\n            )\n            obs = self.get_observation()\n            return obs, reward, done, {}\n    ```", "```\n    def reset(self):\n            # Reset the state of the environment to an \n            # initial state\n            self.cash_balance = self.opening_account_balance\n            self.account_value = self.opening_account_balance\n            self.num_coins_held = 0\n            self.cost_basis = 0\n            self.current_step = 0\n            self.trades = []\n            if self.viz is None:\n                self.viz = TradeVisualizer(\n                    self.ticker,\n                    self.ticker_file_stream,\n                    \"TFRL-Cookbook Ch4-CryptoTradingEnv\",\n                    skiprows=1,  # Skip the first line with \n                    # the data download source URL\n                )\n            return self.get_observation()\n    ```", "```\n        def render(self, **kwargs):\n            # Render the environment to the screen\n            if self.current_step > self.horizon:\n                self.viz.render(\n                    self.current_step,\n                    self.account_value,\n                    self.trades,\n                    window_size=self.horizon,\n                )\n    ```", "```\n        def close(self):\n            if self.viz is not None:\n                self.viz.close()\n                self.viz = None\n    ```", "```\n        def execute_trade_action(self, action):\n            if action == 0:  # Hold position\n                return\n\n    ```", "```\n            order_type = \"buy\" if action == 1 else \"sell\"\n            # Stochastically determine the current price \n            # based on Market Open & Close\n            current_price = random.uniform(\n                self.ohlcv_df.loc[self.current_step, \"Open\"],\n                self.ohlcv_df.loc[self.current_step, \n                                  \"Close\"],\n            )\n    ```", "```\n            if order_type == \"buy\":\n                allowable_coins = \\\n                    int(self.cash_balance / current_price)\n                if allowable_coins < self.order_size:\n                    # Not enough cash to execute a buy order\n                    return\n                # Simulate a BUY order and execute it at \n                # current_price\n                num_coins_bought = self.order_size\n                current_cost = self.cost_basis * \\\n                               self.num_coins_held\n                additional_cost = num_coins_bought * \\\n                                  current_price\n                self.cash_balance -= additional_cost\n                self.cost_basis = (current_cost + \\\n                    additional_cost) / (\n                    self.num_coins_held + num_coins_bought\n                )\n                self.num_coins_held += num_coins_bought\n    ```", "```\n                self.trades.append(\n                    {\n                        \"type\": \"buy\",\n                        \"step\": self.current_step,\n                        \"shares\": num_coins_bought,\n                        \"proceeds\": additional_cost,\n                    }\n                )\n    ```", "```\n            elif order_type == \"sell\":\n                # Simulate a SELL order and execute it at \n                # current_price\n                if self.num_coins_held < self.order_size:\n                    # Not enough coins to execute a sell \n                    # order\n                    return\n                num_coins_sold = self.order_size\n                self.cash_balance += num_coins_sold * \\\n                                     current_price\n                self.num_coins_held -= num_coins_sold\n                sale_proceeds = num_coins_sold * \\\n                                current_price\n                self.trades.append(\n                    {\n                        \"type\": \"sell\",\n                        \"step\": self.current_step,\n                        \"shares\": num_coins_sold,\n                        \"proceeds\": sale_proceeds,\n                    }\n                )\n    ```", "```\n            if self.num_coins_held == 0:\n                self.cost_basis = 0\n            # Update account value\n            self.account_value = self.cash_balance + \\\n                                 self.num_coins_held * \\\n                                 current_price\n    ```", "```\n    if __name__ == \"__main__\":\n        env = CryptoTradingEnv()\n        obs = env.reset()\n        for _ in range(600):\n            action = env.action_space.sample()\n            next_obs, reward, done, _ = env.step(action)\n            env.render()\n    ```", "```\nimport os\nimport random\nfrom typing import Dict\nimport cv2\nimport gym\nimport numpy as np\nimport pandas as pd\nfrom gym import spaces\nfrom trading_utils import TradeVisualizer\n```", "```\n    env_config = {\n        \"exchange\": \"Gemini\",  # Cryptocurrency exchange \n        # (Gemini, coinbase, kraken, etc.)\n        \"ticker\": \"ETHUSD\",  # CryptoFiat\n        \"frequency\": \"daily\",  # daily/hourly/minutes\n        \"opening_account_balance\": 100000,\n        # Number of steps (days) of data provided to the \n        # agent in one observation\n        \"observation_horizon_sequence_length\": 30,\n        \"order_size\": 1,  # Number of coins to buy per \n        # buy/sell order\n    }\n    ```", "```\n    class CryptoTradingVisualEnv(gym.Env):\n        def __init__(self, env_config: Dict = env_config):\n            \"\"\"Cryptocurrency trading environment for RL \n            agents\n            The observations are cryptocurrency price info \n            (OHLCV) over a horizon as specified in \n            env_config. Action space is discrete to perform \n            buy/sell/hold trades.\n            Args:\n                ticker(str, optional): Ticker symbol for the\\\n                crypto-fiat currency pair.\n                Defaults to \"ETHUSD\".\n                env_config (Dict): Env configuration values\n            \"\"\"\n            super(CryptoTradingVisualEnv, self).__init__()\n            self.ticker = env_config.get(\"ticker\", \"ETHUSD\")\n            data_dir = os.path.join(os.path.dirname(os.path.\\\n                                 realpath(__file__)), \"data\")\n            self.exchange = env_config[\"exchange\"]\n            freq = env_config[\"frequency\"]\n    ```", "```\n        if freq == \"daily\":\n                self.freq_suffix = \"d\"\n            elif freq == \"hourly\":\n                self.freq_suffix = \"1hr\"\n            elif freq == \"minutes\":\n                self.freq_suffix = \"1min\"\n            self.ticker_file_stream = os.path.join(\n                f\"{data_dir}\",\n                f\"{'_'.join([self.exchange, self.ticker, \\\n                             self.freq_suffix])}.csv\",\n            )\n            assert os.path.isfile(\n                self.ticker_file_stream\n            ), f\"Cryptocurrency exchange data file stream \\\n            not found at: data/{self.ticker_file_stream}.csv\"\n            # Cryptocurrency exchange data stream. An offline \n            # file stream is used. Alternatively, a web\n            # API can be used to pull live data.\n            self.ohlcv_df = pd.read_csv(self.ticker_file_\\\n                stream, skiprows=1).sort_values(\n                by=\"Date\"\n            )\n    ```", "```\n            self.opening_account_balance = \\\n                env_config[\"opening_account_balance\"]\n            # Action: 0-> Hold; 1-> Buy; 2 ->Sell;\n            self.action_space = spaces.Discrete(3)\n            self.observation_features = [\n                \"Open\",\n                \"High\",\n                \"Low\",\n                \"Close\",\n                \"Volume ETH\",\n                \"Volume USD\",\n            ]\n            self.obs_width, self.obs_height = 128, 128\n            self.horizon = env_config.get(\"\n                observation_horizon_sequence_length\")\n            self.observation_space = spaces.Box(\n                low=0, high=255, shape=(128, 128, 3),\n                dtype=np.uint8,\n            )\n            self.order_size = env_config.get(\"order_size\")\n            self.viz = None  # Visualizer\n    ```", "```\n        def reset(self):\n            # Reset the state of the environment to an \n            # initial state\n            self.cash_balance = self.opening_account_balance\n            self.account_value = self.opening_account_balance\n            self.num_coins_held = 0\n            self.cost_basis = 0\n            self.current_step = 0\n            self.trades = []\n            if self.viz is None:\n                self.viz = TradeVisualizer(\n                    self.ticker,\n                    self.ticker_file_stream,\n                    \"TFRL-Cookbook\\\n                       Ch4-CryptoTradingVisualEnv\",\n                    skiprows=1,\n                )\n            return self.get_observation()\n    ```", "```\n        def get_observation(self):\n            \"\"\"Return a view of the Ticker price chart as \n               image observation\n            Returns:\n                img_observation(np.ndarray): Image of ticker\n                candle stick plot with volume bars as \n                observation\n            \"\"\"\n            img_observation = \\\n                self.viz.render_image_observation(\n                self.current_step, self.horizon\n            )\n            img_observation = cv2.resize(\n                img_observation, dsize=(128, 128), \n                interpolation=cv2.INTER_CUBIC\n            )\n            return img_observation\n    ```", "```\n        def execute_trade_action(self, action):\n            if action == 0:  # Hold position\n                return\n            order_type = \"buy\" if action == 1 else \"sell\"\n            # Stochastically determine the current price\n            # based on Market Open & Close\n            current_price = random.uniform(\n                self.ohlcv_df.loc[self.current_step, \"Open\"],\n                self.ohlcv_df.loc[self.current_step, \n                                  \"Close\"],\n            )\n    ```", "```\n                # Buy Order             allowable_coins = \\\n                    int(self.cash_balance / current_price)\n                if allowable_coins < self.order_size:\n                    # Not enough cash to execute a buy order\n                    return\n                # Simulate a BUY order and execute it at \n                # current_price\n                num_coins_bought = self.order_size\n                current_cost = self.cost_basis * \\\n                               self.num_coins_held\n                additional_cost = num_coins_bought * \\\n                                  current_price\n                self.cash_balance -= additional_cost\n                self.cost_basis = \\\n                    (current_cost + additional_cost) / (\n                    self.num_coins_held + num_coins_bought\n                )\n                self.num_coins_held += num_coins_bought\n                self.trades.append(\n                    {\n                        \"type\": \"buy\",\n                        \"step\": self.current_step,\n                        \"shares\": num_coins_bought,\n                        \"proceeds\": additional_cost,\n                    }\n                )\n    ```", "```\n               # Simulate a SELL order and execute it at \n               # current_price\n                if self.num_coins_held < self.order_size:\n                    # Not enough coins to execute a sell\n                    # order\n                    return\n                num_coins_sold = self.order_size\n                self.cash_balance += num_coins_sold * \\\n                                     current_price\n                self.num_coins_held -= num_coins_sold\n                sale_proceeds = num_coins_sold * \\\n                                current_price\n                self.trades.append(\n                    {\n                        \"type\": \"sell\",\n                        \"step\": self.current_step,\n                        \"shares\": num_coins_sold,\n                        \"proceeds\": sale_proceeds,\n                    }\n                )\n    ```", "```\n            if self.num_coins_held == 0:\n                self.cost_basis = 0\n            # Update account value\n            self.account_value = self.cash_balance + \\\n                                 self.num_coins_held * \\\n                                 current_price\n    ```", "```\n        def step(self, action):\n            # Execute one step within the trading environment\n            self.execute_trade_action(action)\n            self.current_step += 1\n            reward = self.account_value - \\\n                self.opening_account_balance  # Profit (loss)\n            done = self.account_value <= 0 or \\\n                     self.current_step >= len(\n                self.ohlcv_df.loc[:, \"Open\"].values\n            )\n            obs = self.get_observation()\n            return obs, reward, done, {}\n    ```", "```\n        def render(self, **kwargs):\n            # Render the environment to the screen\n            if self.current_step > self.horizon:\n                self.viz.render(\n                    self.current_step,\n                    self.account_value,\n                    self.trades,\n                    window_size=self.horizon,\n                )\n    ```", "```\n    if __name__ == \"__main__\":\n        env = CryptoTradingVisualEnv()\n        obs = env.reset()\n        for _ in range(600):\n            action = env.action_space.sample()\n            next_obs, reward, done, _ = env.step(action)\n            env.render()\n    ```", "```\nimport os\nimport random\nfrom typing import Dict\nimport cv2\nimport gym\nimport numpy as np\nimport pandas as pd\nfrom gym import spaces\nfrom trading_utils import TradeVisualizer\n```", "```\n    env_config = {\n        \"exchange\": \"Gemini\",  # Cryptocurrency exchange \n         # (Gemini, coinbase, kraken, etc.)\n        \"ticker\": \"BTCUSD\",  # CryptoFiat\n        \"frequency\": \"daily\",  # daily/hourly/minutes\n        \"opening_account_balance\": 100000,\n        # Number of steps (days) of data provided to the \n        # agent in one observation\n        \"observation_horizon_sequence_length\": 30,\n    }\n    ```", "```\n    class CryptoTradingVisualContinuousEnv(gym.Env):\n        def __init__(self, env_config: Dict = env_config):\n            \"\"\"Cryptocurrency trading environment for RL \n            agents with continuous action space\n            Args:\n                ticker (str, optional): Ticker symbol for the \n                crypto-fiat currency pair.\n                Defaults to \"BTCUSD\".\n                env_config (Dict): Env configuration values\n            \"\"\"\n            super(CryptoTradingVisualContinuousEnv, \n                  self).__init__()\n            self.ticker = env_config.get(\"ticker\", \"BTCUSD\")\n            data_dir = os.path.join(os.path.dirname(os.path.\\\n                                 realpath(__file__)), \"data\")\n            self.exchange = env_config[\"exchange\"]\n            freq = env_config[\"frequency\"]\n            if freq == \"daily\":\n                self.freq_suffix = \"d\"\n            elif freq == \"hourly\":\n                self.freq_suffix = \"1hr\"\n            elif freq == \"minutes\":\n                self.freq_suffix = \"1min\"\n    ```", "```\n            self.ticker_file_stream = os.path.join(\n                f\"{data_dir}\",\n                f\"{'_'.join([self.exchange, self.ticker, \\\n                             self.freq_suffix])}.csv\",\n            )\n            assert os.path.isfile(\n                self.ticker_file_stream\n            ), f\"Cryptocurrency exchange data file stream \\\n            not found at: data/{self.ticker_file_stream}.csv\"\n            # Cryptocurrency exchange data stream. An offline \n            # file stream is used. Alternatively, a web\n            # API can be used to pull live data.\n            self.ohlcv_df = pd.read_csv(\n                self.ticker_file_stream, \n                skiprows=1).sort_values(by=\"Date\"\n            )\n            self.opening_account_balance = \\\n                env_config[\"opening_account_balance\"]\n    ```", "```\n            self.action_space = spaces.Box(\n                low=np.array([-1]), high=np.array([1]), \\\n                             dtype=np.float\n            )\n            self.observation_features = [\n                \"Open\",\n                \"High\",\n                \"Low\",\n                \"Close\",\n                \"Volume BTC\",\n                \"Volume USD\",\n            ]\n            self.obs_width, self.obs_height = 128, 128\n            self.horizon = env_config.get(\n                       \"observation_horizon_sequence_length\")\n            self.observation_space = spaces.Box(\n                low=0, high=255, shape=(128, 128, 3), \n                dtype=np.uint8,\n            )\n    ```", "```\n        def step(self, action):\n            # Execute one step within the environment\n            self.execute_trade_action(action)\n            self.current_step += 1\n            reward = self.account_value - \\\n                self.opening_account_balance  # Profit (loss)\n            done = self.account_value <= 0 or \\\n                    self.current_step >= len(\n                self.ohlcv_df.loc[:, \"Open\"].values\n            )\n            obs = self.get_observation()\n            return obs, reward, done, {}\n    ```", "```\n        def execute_trade_action(self, action):\n            if action == 0:  # Indicates \"HODL\" action\n                # HODL position; No trade to be executed\n                return\n            order_type = \"buy\" if action > 0 else \"sell\"\n            order_fraction_of_allowable_coins = abs(action)\n            # Stochastically determine the current price \n            # based on Market Open & Close\n            current_price = random.uniform(\n                self.ohlcv_df.loc[self.current_step, \"Open\"],\n                self.ohlcv_df.loc[self.current_step,\n                                  \"Close\"],\n            )\n    ```", "```\n            if order_type == \"buy\":\n                allowable_coins = \\\n                    int(self.cash_balance / current_price)\n                # Simulate a BUY order and execute it at \n                # current_price\n                num_coins_bought = int(allowable_coins * \\\n                order_fraction_of_allowable_coins)\n                current_cost = self.cost_basis * \\\n                               self.num_coins_held\n                additional_cost = num_coins_bought * \\\n                                  current_price\n                self.cash_balance -= additional_cost\n                self.cost_basis = (current_cost + \\\n                                   additional_cost) / (\n                    self.num_coins_held + num_coins_bought\n                )\n                self.num_coins_held += num_coins_bought\n                if num_coins_bought > 0:\n                    self.trades.append(\n                        {\n                            \"type\": \"buy\",\n                            \"step\": self.current_step,\n                            \"shares\": num_coins_bought,\n                            \"proceeds\": additional_cost,\n                        }\n                    )\n    ```", "```\n            elif order_type == \"sell\":\n                # Simulate a SELL order and execute it at \n                # current_price\n                num_coins_sold = int(\n                    self.num_coins_held * \\\n                    order_fraction_of_allowable_coins\n                )\n                self.cash_balance += num_coins_sold * \\\n                                     current_price\n                self.num_coins_held -= num_coins_sold\n                sale_proceeds = num_coins_sold * \\\n                                current_price\n                if num_coins_sold > 0:\n                    self.trades.append(\n                        {\n                            \"type\": \"sell\",\n                            \"step\": self.current_step,\n                            \"shares\": num_coins_sold,\n                            \"proceeds\": sale_proceeds,\n                        }\n                    )\n    ```", "```\n            if self.num_coins_held == 0:\n                self.cost_basis = 0\n            # Update account value\n            self.account_value = self.cash_balance + \\\n                                 self.num_coins_held * \\\n                                 current_price\n    ```", "```\n    if __name__ == \"__main__\":\n        env = CryptoTradingVisualContinuousEnv()\n        obs = env.reset()\n        for _ in range(600):\n            action = env.action_space.sample()\n            next_obs, reward, done, _ = env.step(action)\n            env.render()\n    ```", "```\nmport functools\nimport os\nimport random\nfrom collections import deque\nfrom functools import reduce\nimport imageio\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_probability as tfp\nfrom tensorflow.keras.layers import Concatenate, Dense, Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom crypto_trading_continuous_env import CryptoTradingContinuousEnv\n```", "```\n    def actor(state_shape, action_shape, units=(512, 256, 64)):\n        state_shape_flattened = \\\n            functools.reduce(lambda x, y: x * y, state_shape)\n        state = Input(shape=state_shape_flattened)\n        x = Dense(units[0], name=\"L0\", activation=\"relu\")\\\n                  (state)\n        for index in range(1, len(units)):\n            x = Dense(units[index],name=\"L{}\".format(index),\\ \n                      activation=\"relu\")(x)\n        actions_mean = Dense(action_shape[0], \\\n                        name=\"Out_mean\")(x)\n        actions_std = Dense(action_shape[0], \\\n                       name=\"Out_std\")(x)\n        model = Model(inputs=state, \n                      outputs=[actions_mean, actions_std])\n        return model\n    ```", "```\n    def critic(state_shape, action_shape, units=(512, 256, 64)):\n        state_shape_flattened = \\\n            functools.reduce(lambda x, y: x * y, state_shape)\n        inputs = [Input(shape=state_shape_flattened),\n                  Input(shape=action_shape)]\n        concat = Concatenate(axis=-1)(inputs)\n        x = Dense(units[0], name=\"Hidden0\", \n                  activation=\"relu\")(concat)\n        for index in range(1, len(units)):\n            x = Dense(units[index], \n                      name=\"Hidden{}\".format(index),\n                      activation=\"relu\")(x)\n        output = Dense(1, name=\"Out_QVal\")(x)\n        model = Model(inputs=inputs, outputs=output)\n        return model\n    ```", "```\n    def update_target_weights(model, target_model, tau=0.005):\n        weights = model.get_weights()\n        target_weights = target_model.get_weights()\n        for i in range(len(target_weights)):  # set tau% of\n        # target model to be new weights\n            target_weights[i] = weights[i] * tau + \\\n                                target_weights[i] * (1 - tau)\n        target_model.set_weights(target_weights)\n    ```", "```\n    class SAC(object):\n        def __init__(\n            self,\n            env,\n            lr_actor=3e-5,\n            lr_critic=3e-4,\n            actor_units=(64, 64),\n            critic_units=(64, 64),\n            auto_alpha=True,\n            alpha=0.2,\n            tau=0.005,\n            gamma=0.99,\n            batch_size=128,\n            memory_cap=100000,\n        ):\n            self.env = env\n            self.state_shape = env.observation_space.shape  \n            # shape of observations\n            self.action_shape = env.action_space.shape  \n            # number of actions\n            self.action_bound = (env.action_space.high - \\\n                                 env.action_space.low) / 2\n            self.action_shift = (env.action_space.high + \\\n                                 env.action_space.low) / 2\n            self.memory = deque(maxlen=int(memory_cap))\n    ```", "```\n            # Define and initialize actor network\n            self.actor = actor(self.state_shape, \n                              self.action_shape, actor_units)\n            self.actor_optimizer = \\\n                Adam(learning_rate=lr_actor)\n            self.log_std_min = -20\n            self.log_std_max = 2\n            print(self.actor.summary())\n    ```", "```\n            self.critic_1 = critic(self.state_shape, \n                             self.action_shape, critic_units)\n            self.critic_target_1 = critic(self.state_shape,\n                             self.action_shape, critic_units)\n            self.critic_optimizer_1 = \\\n                 Adam(learning_rate=lr_critic)\n            update_target_weights(self.critic_1, \\\n                             self.critic_target_1, tau=1.0)\n            self.critic_2 = critic(self.state_shape, \\\n                             self.action_shape, critic_units)\n            self.critic_target_2 = critic(self.state_shape,\\\n                             self.action_shape, critic_units)\n            self.critic_optimizer_2 = \\\n                Adam(learning_rate=lr_critic)\n            update_target_weights(self.critic_2, \\\n                self.critic_target_2, tau=1.0)\n            print(self.critic_1.summary())\n    ```", "```\n            self.auto_alpha = auto_alpha\n            if auto_alpha:\n                self.target_entropy = \\\n                    -np.prod(self.action_shape)\n                self.log_alpha = \\\n                    tf.Variable(0.0, dtype=tf.float64)\n                self.alpha = \\\n                    tf.Variable(0.0, dtype=tf.float64)\n                self.alpha.assign(tf.exp(self.log_alpha))\n                self.alpha_optimizer = \\\n                    Adam(learning_rate=lr_actor)\n            else:\n                self.alpha = tf.Variable(alpha, \n                                         dtype=tf.float64)\n    ```", "```\n            self.gamma = gamma  # discount factor\n            self.tau = tau  # target model update\n            self.batch_size = batch_size\n    ```", "```\n        def process_actions(self, mean, log_std, test=False, \n        eps=1e-6):\n            std = tf.math.exp(log_std)\n            raw_actions = mean\n            if not test:\n                raw_actions += tf.random.normal(shape=mean.\\\n                               shape, dtype=tf.float64) * std\n            log_prob_u = tfp.distributions.Normal(loc=mean,\n                            scale=std).log_prob(raw_actions)\n            actions = tf.math.tanh(raw_actions)\n            log_prob = tf.reduce_sum(log_prob_u - \\\n                        tf.math.log(1 - actions ** 2 + eps))\n            actions = actions * self.action_bound + \\\n                       self.action_shift\n            return actions, log_prob\n    ```", "```\n        def act(self, state, test=False, use_random=False):\n            state = state.reshape(-1)  # Flatten state\n            state = \\\n             np.expand_dims(state, axis=0).astype(np.float64)\n            if use_random:\n                a = tf.random.uniform(\n                    shape=(1, self.action_shape[0]), \\\n                    minval=-1, maxval=1, dtype=tf.float64\n                )\n            else:\n                means, log_stds = self.actor.predict(state)\n                log_stds = tf.clip_by_value(log_stds, \n                                            self.log_std_min,\n                                            self.log_std_max)\n                a, log_prob = self.process_actions(means,\n                                                   log_stds, \n                                                   test=test)\n            q1 = self.critic_1.predict([state, a])[0][0]\n            q2 = self.critic_2.predict([state, a])[0][0]\n            self.summaries[\"q_min\"] = tf.math.minimum(q1, q2)\n            self.summaries[\"q_mean\"] = np.mean([q1, q2])\n            return a\n    ```", "```\n        def remember(self, state, action, reward, next_state, \n        done):\n            state = state.reshape(-1)  # Flatten state\n            state = np.expand_dims(state, axis=0)\n            next_state = next_state.reshape(-1)  \n           # Flatten next-state\n            next_state = np.expand_dims(next_state, axis=0)\n            self.memory.append([state, action, reward, \n                                next_state, done])\n    ```", "```\n        def replay(self):\n            if len(self.memory) < self.batch_size:\n                return\n            samples = random.sample(self.memory, self.batch_size)\n            s = np.array(samples).T\n            states, actions, rewards, next_states, dones = [\n                np.vstack(s[i, :]).astype(np.float) for i in\\\n                range(5)\n            ]\n    ```", "```\n            with tf.GradientTape(persistent=True) as tape:\n                # next state action log probs\n                means, log_stds = self.actor(next_states)\n                log_stds = tf.clip_by_value(log_stds, \n                                            self.log_std_min,\n                                            self.log_std_max)\n                next_actions, log_probs = \\\n                    self.process_actions(means, log_stds)\n    ```", "```\n                current_q_1 = self.critic_1([states, \n                                             actions])\n                current_q_2 = self.critic_2([states,\n                                             actions])\n                next_q_1 = self.critic_target_1([next_states, \n                                               next_actions])\n                next_q_2 = self.critic_target_2([next_states,\n                                               next_actions])\n                next_q_min = tf.math.minimum(next_q_1,\n                                              next_q_2)\n                state_values = next_q_min - self.alpha * \\\n                                            log_probs\n                target_qs = tf.stop_gradient(\n                    rewards + state_values * self.gamma * \\\n                    (1.0 - dones)\n                )\n                critic_loss_1 = tf.reduce_mean(\n                    0.5 * tf.math.square(current_q_1 - \\\n                                         target_qs)\n                )\n                critic_loss_2 = tf.reduce_mean(\n                    0.5 * tf.math.square(current_q_2 - \\\n                                         target_qs)\n                )\n    ```", "```\n                means, log_stds = self.actor(states)\n                log_stds = tf.clip_by_value(log_stds, \n                                            self.log_std_min,\n                                            self.log_std_max)\n                actions, log_probs = \\\n                    self.process_actions(means, log_stds)\n    ```", "```\n                current_q_1 = self.critic_1([states, \n                                             actions])\n                current_q_2 = self.critic_2([states,\n                                             actions])\n                current_q_min = tf.math.minimum(current_q_1,\n                                                current_q_2)\n                actor_loss = tf.reduce_mean(self.alpha * \\\n                                   log_probs - current_q_min)\n                if self.auto_alpha:\n                    alpha_loss = -tf.reduce_mean(\n                        (self.log_alpha * \\\n                        tf.stop_gradient(log_probs + \\\n                                        self.target_entropy))\n                    )\n            critic_grad = tape.gradient(\n                critic_loss_1, \n                self.critic_1.trainable_variables\n            )  \n            self.critic_optimizer_1.apply_gradients(\n                zip(critic_grad, \n                self.critic_1.trainable_variables)\n            )\n    ```", "```\n            critic_grad = tape.gradient(\n                critic_loss_2, \n            self.critic_2.trainable_variables\n            )  # compute actor gradient\n            self.critic_optimizer_2.apply_gradients(\n                zip(critic_grad, \n                self.critic_2.trainable_variables)\n            )\n            actor_grad = tape.gradient(\n                actor_loss, self.actor.trainable_variables\n            )  # compute actor gradient\n            self.actor_optimizer.apply_gradients(\n                zip(actor_grad, \n                    self.actor.trainable_variables)\n            )\n    ```", "```\n            # tensorboard info\n            self.summaries[\"q1_loss\"] = critic_loss_1\n            self.summaries[\"q2_loss\"] = critic_loss_2\n            self.summaries[\"actor_loss\"] = actor_loss\n            if self.auto_alpha:\n                # optimize temperature\n                alpha_grad = tape.gradient(alpha_loss, \n                                           [self.log_alpha])\n                self.alpha_optimizer.apply_gradients(\n                           zip(alpha_grad, [self.log_alpha]))\n                self.alpha.assign(tf.exp(self.log_alpha))\n                # tensorboard info\n                self.summaries[\"alpha_loss\"] = alpha_loss\n    ```", "```\n        def train(self, max_epochs=8000, random_epochs=1000,\n        max_steps=1000, save_freq=50):\n            current_time = datetime.datetime.now().\\\n                               strftime(\"%Y%m%d-%H%M%S\")\n            train_log_dir = os.path.join(\"logs\", \n                       \"TFRL-Cookbook-Ch4-SAC\", current_time)\n            summary_writer = \\\n                tf.summary.create_file_writer(train_log_dir)\n            done, use_random, episode, steps, epoch, \\\n            episode_reward = (\n                False,\n                True,\n                0,\n                0,\n                0,\n                0,\n            )\n            cur_state = self.env.reset()\n    ```", "```\n            while epoch < max_epochs:\n                if steps > max_steps:\n                    done = True\n                if done:\n                    episode += 1\n                    print(\n                        \"episode {}: {} total reward, \n                         {} alpha, {} steps, \n                         {} epochs\".format(\n                            episode, episode_reward, \n                            self.alpha.numpy(), steps, epoch\n                        )\n                    )\n                    with summary_writer.as_default():\n                        tf.summary.scalar(\n                            \"Main/episode_reward\", \\\n                             episode_reward, step=episode\n                        )\n                        tf.summary.scalar(\n                            \"Main/episode_steps\", \n                             steps, step=episode)\n                    summary_writer.flush()\n                    done, cur_state, steps, episode_reward =\\\n                         False, self.env.reset(), 0, 0\n                    if episode % save_freq == 0:\n                        self.save_model(\n                            \"sac_actor_episode{}.h5\".\\\n                                 format(episode),\n                            \"sac_critic_episode{}.h5\".\\\n                                 format(episode),\n                        )\n    ```", "```\n                if epoch > random_epochs and \\\n                    len(self.memory) > self.batch_size:\n                    use_random = False\n                action = self.act(cur_state, \\\n                   use_random=use_random)  # determine action\n                next_state, reward, done, _ = \\\n                    self.env.step(action[0])  # act on env\n                # self.env.render(mode='rgb_array')\n                self.remember(cur_state, action, reward, \n                            next_state, done)  #add to memory\n                self.replay()  # train models through memory\n                # replay\n                update_target_weights(\n                    self.critic_1, self.critic_target_1, \n                    tau=self.tau\n                )  # iterates target model\n                update_target_weights(self.critic_2, \n                self.critic_target_2, \n                tau=self.tau)\n                cur_state = next_state\n                episode_reward += reward\n                steps += 1\n                epoch += 1\n    ```", "```\n                # Tensorboard update\n                with summary_writer.as_default():\n                    if len(self.memory) > self.batch_size:\n                        tf.summary.scalar(\n                            \"Loss/actor_loss\", \n                             self.summaries[\"actor_loss\"], \n                             step=epoch\n                        )\n                        tf.summary.scalar(\n                            \"Loss/q1_loss\", \n                             self.summaries[\"q1_loss\"], \n                             step=epoch\n                        )\n                        tf.summary.scalar(\n                            \"Loss/q2_loss\", \n                           self.summaries[\"q2_loss\"], \n                           step=epoch\n                        )\n                        if self.auto_alpha:\n                            tf.summary.scalar(\n                                \"Loss/alpha_loss\", \n                                self.summaries[\"alpha_loss\"],\n                                step=epoch\n                            )\n                    tf.summary.scalar(\"Stats/alpha\", \n                                      self.alpha, step=epoch)\n                    if self.auto_alpha:\n                        tf.summary.scalar(\"Stats/log_alpha\",\n                                  self.log_alpha, step=epoch)\n                    tf.summary.scalar(\"Stats/q_min\", \n                        self.summaries[\"q_min\"], step=epoch)\n                    tf.summary.scalar(\"Stats/q_mean\", \n                        self.summaries[\"q_mean\"], step=epoch)\n                    tf.summary.scalar(\"Main/step_reward\", \n                                       reward, step=epoch)\n                summary_writer.flush()\n    ```", "```\n            self.save_model(\n                \"sac_actor_final_episode{}.h5\".format(episode),\n                \"sac_critic_final_episode{}.h5\".format(episode),\n            )\n    ```", "```\n        def save_model(self, a_fn, c_fn):\n            self.actor.save(a_fn)\n            self.critic_1.save(c_fn)\n    ```", "```\n        def load_actor(self, a_fn):\n            self.actor.load_weights(a_fn)\n            print(self.actor.summary())\n        def load_critic(self, c_fn):\n            self.critic_1.load_weights(c_fn)\n            self.critic_target_1.load_weights(c_fn)\n            self.critic_2.load_weights(c_fn)\n            self.critic_target_2.load_weights(c_fn)\n            print(self.critic_1.summary())\n    ```", "```\n        def test(self, render=True, fps=30, \n        filename=\"test_render.mp4\"):\n            cur_state, done, rewards = self.env.reset(), \\\n                                        False, 0\n            video = imageio.get_writer(filename, fps=fps)\n            while not done:\n                action = self.act(cur_state, test=True)\n                next_state, reward, done, _ = \\\n                                     self.env.step(action[0])\n                cur_state = next_state\n                rewards += reward\n                if render:\n                    video.append_data(\n                        self.env.render(mode=\"rgb_array\"))\n            video.close()\n            return rewards\n    ```", "```\n    if __name__ == \"__main__\":\n        gym_env = CryptoTradingContinuousEnv()\n        sac = SAC(gym_env)\n        # Load Actor and Critic from previously saved \n        # checkpoints\n        # sac.load_actor(\"sac_actor_episodexyz.h5\")\n        # sac.load_critic(\"sac_critic_episodexyz.h5\")\n        sac.train(max_epochs=100000, random_epochs=10000, \n                  save_freq=50)\n        reward = sac.test()\n        print(reward)\n    ```", "```\ntensorboard –-logdir=logs\n```"]