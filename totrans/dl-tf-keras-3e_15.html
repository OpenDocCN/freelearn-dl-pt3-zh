<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer800">
<h1 class="chapterNumber">15</h1>
<h1 class="chapterTitle" id="_idParaDest-395">Tensor Processing Unit</h1>
<p class="normal">This chapter introduces the <strong class="keyWord">Tensor Processing Unit</strong> (<strong class="keyWord">TPU</strong>), a special chip developed at Google for ultra-fast execution of neural network mathematical operations. As with <strong class="keyWord">Graphics Processing Units</strong> (<strong class="keyWord">GPUs</strong>), the idea here is to have a special processor focusing only on very fast matrix operations, with no support for all the other operations normally supported by <strong class="keyWord">Central Processing Units</strong> (<strong class="keyWord">CPUs</strong>). However, the additional improvement with TPUs is to remove from the chip any hardware support for graphics operations normally present in GPUs (rasterization, texture mapping, frame buffer operations, and so on). Think of a TPU as a special purpose co-processor specialized for deep learning, being focused on matrix or tensor operations. In this chapter, we will compare CPUs and GPUs with the four generations of TPUs and with Edge TPUs. All these accelerators are available as of April 2022. The chapter will include code examples of using TPUs.</p>
<p class="normal">In this chapter, you will learn the following:</p>
<ul>
<li class="bulletList">C/G/T processing units</li>
<li class="bulletList">Four generations of TPUs and Edge TPUs</li>
<li class="bulletList">TPU performance</li>
<li class="bulletList">How to use TPUs with Colab</li>
</ul>
<p class="normal">So with that, let’s begin.</p>
<h1 class="heading-1" id="_idParaDest-396">C/G/T processing units</h1>
<p class="normal">In this section we discuss CPUs, GPUs, and TPUs. Before discussing TPUs, it will be useful for us to review CPUs and GPUs.</p>
<h2 class="heading-2" id="_idParaDest-397">CPUs and GPUs</h2>
<p class="normal">You are probably somewhat familiar with the concept of a CPU, a general-purpose chip sitting in each computer, tablet, and smartphone. CPUs are in charge of all of the computations: from logical controls, to arithmetic, to register operations, to operations with memory, and many others. CPUs are <a id="_idIndexMarker1455"/>subject to the well-known Moore’s law [1], which states that the number of transistors in a dense integrated circuit doubles about every two years. </p>
<p class="normal">Many people believe that we are currently in an era where this trend cannot be sustained for long, and indeed it has already declined during the past decade. Therefore, we need some additional technology if we want to support the demand for faster and faster computation to process the ever-growing amount of data that is available out there.</p>
<p class="normal">One improvement came from<a id="_idIndexMarker1456"/> GPUs, special-purpose chips that are perfect for fast graphics operations such as matrix multiplication, rasterization, frame-buffer manipulation, texture mapping, and many others. In addition to computer graphics where matrix multiplications are applied to pixels of images, GPUs turned out to be a great match for deep learning. This is a funny story of serendipity (serendipity is the occurrence and development of events by chance in a happy or beneficial way) – a great example of technology created for one goal and then being met with staggering success in a domain completely unrelated to the one it was originally envisioned for.</p>
<h2 class="heading-2" id="_idParaDest-398">TPUs</h2>
<p class="normal">One problem encountered in using GPUs for deep learning is that these chips are made for graphics and gaming, not only <a id="_idIndexMarker1457"/>for fast matrix computations. This would of course be the case, given that the G in GPU stands for Graphics! GPUs led to unbelievable improvements in deep learning but, in the case of tensor operations for neural networks, large parts of the chip are not used at all. For deep learning, there is no need for rasterization, no need for frame-buffer manipulation, and no need for texture mapping. The only thing that is necessary is a very efficient way to compute matrix and tensor operations. It should be no surprise that GPUs are not necessarily the ideal solution for deep learning, since CPUs and GPUs were designed long before deep learning became successful.</p>
<p class="normal">Before going into technical details, let’s first discuss the fascinating genesis of Tensor Processing Unit version 1, or TPU v1. In 2013, Jeff Dean, the Chief of the Brain Division at Google, estimated (see <em class="italic">Figure 15.1</em>) that if all the people owning a mobile phone were talking in calls for only 3 minutes more per day, then Google would have needed two or three times more servers to process this data. This would have been an unaffordable case of success-disaster, i.e., where great success has led to problems that cannot be properly managed. It was clear that neither CPUs nor GPUs were a suitable solution. So, Google decided that they needed something completely new – something that would allow a 10x growth in performance with no<a id="_idIndexMarker1458"/> significant cost increase. That’s how TPU v1 was born! What is impressive is that it took only 15 months from initial design to production. You can find more details about this story in Jouppi et al., 2014 [3], where a detailed report about different inference workloads seen at Google in 2013 is also reported:</p>
<figure class="mediaobject"><img alt="Table  Description automatically generated" height="304" src="../Images/B18331_15_01.png" width="749"/></figure>
<p class="packt_figref">Figure 15.1: Different inference workloads seen at Google in 2013 (source [3])</p>
<p class="normal">Let’s talk a bit about the<a id="_idIndexMarker1459"/> technical details. A TPU v1 is a special device (or an <strong class="keyWord">Application-Specific Integrated Circuit</strong>, or <strong class="keyWord">ASIC</strong> for short) designed for super-efficient tensor operations. TPUs follow the philosophy <em class="italic">less is more.</em> This philosophy has an important consequence: TPUs do not have all the graphic components that are needed for GPUs. Because of this, they are both very efficient from an energy consumption perspective, and frequently much faster than GPUs. So far, there have been four generations of TPUs. Let’s review them.</p>
<h1 class="heading-1" id="_idParaDest-399">Four generations of TPUs, plus Edge TPU</h1>
<p class="normal">As discussed, TPUs are domain-specific processors expressly optimized for matrix operations. Now, you might remember that the basic <a id="_idIndexMarker1460"/>operation of matrix multiplication is a dot product between a line from one matrix and a column from the other matrix. For instance, given a matrix multiplication <img alt="" height="42" src="../Images/B18331_15_001.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="179"/>, computing <em class="italic">Y</em>[<em class="italic">i</em>, 0] is:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_15_002.png" style="height: 1.25em !important;" width="1479"/></p>
<p class="normal">The sequential implementation of this operation is time-consuming for large matrices. A brute-force computation has a time complexity of <em class="italic">O</em>(<em class="italic">n</em><sup class="superscript">3</sup>) for <em class="italic">n</em> x <em class="italic">n</em> matrices so it’s not feasible for running large computations.</p>
<h2 class="heading-2" id="_idParaDest-400">First generation TPU</h2>
<p class="normal">The first generation TPU (TPU v1) was announced in May 2016 at Google I/O. TPU v1 [1] supports matrix multiplication<a id="_idIndexMarker1461"/> using 8-bit arithmetic. TPU v1 is specialized for deep learning inference but it does not work for training. For training there is a need to perform floating-point operations, as discussed in the following paragraphs.</p>
<p class="normal">A key function of TPU is the “systolic” matrix multiplication. Let’s see what this means. Remember that the core of deep learning is a core product <img alt="" height="42" src="../Images/B18331_15_001.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="179"/>, where, for instance, the basic operation to compute <em class="italic">Y</em>[<em class="italic">i</em>, 0] is:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_15_004.png" style="height: 1.25em !important;" width="1163"/></p>
<p class="normal">“Systolic” matrix multiplication allows multiple <em class="italic">Y</em>[<em class="italic">i</em>, <em class="italic">j</em>] values to be computed in parallel. Data flows in a coordinated manner and, indeed, in medicine the term “systolic” refers to heart contractions and how blood flows rhythmically in our veins. Here systolic refers to the data flow that pulses inside the TPU. It can be proven that a systolic multiplication algorithm is less expensive than the brute-force one [2]. A TPU v1 has a <strong class="keyWord">Matrix Multiply Unit</strong> (<strong class="keyWord">MMU</strong>) running systolic multiplications on 256 x 256 cores so that 65,536 multiplications can be computed in parallel in one single shot. In addition, a TPU v1 sits in a rack and it is not directly accessible. Instead, a CPU acts as the host controlling data transfer and sending commands to the TPU for performing tensor multiplications, for computing convolutions, and for applying activation functions. The CPU <img alt="" height="38" src="../Images/B18331_15_005.png" style="height: 0.95em !important; vertical-align: 0.01em !important;" width="38"/> TPU v1 communication happens via a standard PCIe 3.0 bus. From this perspective, a TPU v1 is closer in spirit to a <strong class="keyWord">Floating-Point Unit</strong> (<strong class="keyWord">FPU</strong>) coprocessor<a id="_idIndexMarker1462"/> than it is to a GPU. However, a TPU v1 has the ability to run whole inference models to reduce dependence on the host CPU. <em class="italic">Figure 15.2</em> represents TPU v1, as shown in [3]. As you see in the figure, the processing unit is connected via a PCI port, and it fetches weights via a standard DDR4 DRAM chip. Multiplication happens within the MMU with systolic processing. Activation functions are then <a id="_idIndexMarker1463"/>applied to the results. The MMU and the unified buffer for activations take up a lot of space. There is an area where the activation functions are computed.</p>
<figure class="mediaobject"><img alt="" height="342" src="../Images/B18331_15_02.png" width="879"/></figure>
<p class="packt_figref">Figure 15.2: TPU v1 design schema (source [3])</p>
<p class="normal">TPU v1s are manufactured on a 28 nm process node with a die size of ≤ 331 mm2, a clock speed of 700 MHz, 28 MiB of on-chip memory, 4 MiB of 32-bit accumulators, and a 256 x 256 systolic array of 8-bit multipliers. For this reason, we can get 700 MHz*65,536 (multipliers) <img alt="" height="38" src="../Images/B18331_09_004.png" style="height: 0.95em !important; vertical-align: 0.01em !important;" width="33"/> 92 Tera operations/sec. This is an amazing performance for matrix multiplications; <em class="italic">Figure 15.3</em> shows the TPU circuit board and flow of data for the systolic matrix multiplication performed by the MMU. In addition, TPU v1 has an 8 GiB of dual-channel 2133 MHz DDR3 SDRAM offering 34 GB/s of bandwidth. The external memory is standard, and it is used to store and fetch the weights used during the inference. Notice also that TPU v1 has a thermal design power of 28–40 watts, which is certainly low consumption compared to GPUs and CPUs. Moreover, TPU v1s are normally mounted in a PCI slot used for SATA disks so they do not require any modification in the host server [3]. Up to four cards can be mounted on each server. <em class="italic">Figure 15.3</em> shows a TPU v1 card and the process of systolic computation:</p>
<figure class="mediaobject"><img alt="Diagram, schematic  Description automatically generated" height="340" src="../Images/B18331_15_03.png" width="879"/></figure>
<p class="packt_figref">Figure 15.3: On the left you can see a TPU v1 board, and on the right an example of how the data is processed during the systolic computation</p>
<p class="normal">If you want to have a <a id="_idIndexMarker1464"/>look at TPU performance compared to GPUs and CPUs, you can refer to [3] and see (in a log-log scale graph) that the performance is two orders of magnitude higher than a Tesla K80 GPU. The graph shows a “rooftop” performance, which is growing until the point where it reaches the peak and then it is constant. </p>
<p class="normal">The higher the roof, the better performance is:</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="586" src="../Images/B18331_15_04.png" width="687"/></figure>
<p class="packt_figref">Figure 15.4: TPU v1 peak performance can be up to 3x higher than a Tesla K80</p>
<h2 class="heading-2" id="_idParaDest-401">Second generation TPU</h2>
<p class="normal">The second generation <a id="_idIndexMarker1465"/>TPUs (TPU2s) were announced in 2017. In this case, the memory bandwidth is increased to 600 GB/s and performance reaches 45 TFLOPS. Four TPU2s are arranged in a module with 180 TFLOPS of performance. Then 64 modules are grouped into a pod with 11.5 PFLOPS of performance. TPU2s adopt floating-point arithmetic and therefore they are suitable for both training and inference.</p>
<p class="normal">A TPU2 has an MNU for matrix multiplications of 128*128 cores and a <strong class="keyWord">Vector Processing Unit</strong> (<strong class="keyWord">VPU</strong>) for all other tasks <a id="_idIndexMarker1466"/>such as applying activations etc. The VPU handles float32 and int32 computations. The MXU on the other hand operates in a mixed precision 16–32 bit floating-point format.</p>
<p class="normal">Each TPU v2 chip has two cores, and up to four chips are mounted on each board. In TPU v2, Google adopted a new<a id="_idIndexMarker1467"/> floating-point model called bfloat16 The idea is to sacrifice some resolution but still be very good for deep learning. This reduction in resolution allows us to improve the performance of the TPU2s, which are more power-efficient than the v1s. Indeed, It can be proven that a smaller mantissa helps reduce the physical silicon area and multiplier power. Therefore, the bfloat16 uses the same standard IEEE 754 <a id="_idIndexMarker1468"/>single-precision floating-point format, but it truncates the mantissa field from 23 bits to just 7 bits. </p>
<p class="normal">Preserving the exponent bits allows the format to keep the same range as the 32-bit single precision. This allows for relatively simpler conversion between the two data types:</p>
<figure class="mediaobject"><img alt="" height="319" src="../Images/B18331_15_05.png" width="750"/></figure>
<p class="packt_figref">Figure 15.5: Cloud TPU v2 and Cloud TPU v3</p>
<p class="normal">Google offers access to these TPU v2 and TPU v3 via <strong class="keyWord">Google Compute Engine</strong> (<strong class="keyWord">GCE</strong>), and via <strong class="keyWord">Google Kubernetes Engine</strong> (<strong class="keyWord">GKE</strong>). Plus, it is possible to use them for free via Colab.</p>
<h2 class="heading-2" id="_idParaDest-402">Third generation TPU</h2>
<p class="normal">The third generation TPUs (TPU3) were <a id="_idIndexMarker1469"/>announced in 2018 [4]. TPU3s are 2x faster than TPU2 and they are grouped in 4x larger pods. In total, this is an 8x performance increase. Cloud TPU v3 Pods can deliver more than 100 petaflops of computing power. On the other hand, Cloud TPU v2 Pods released in alpha in 2018 can achieve 11.5 petaflops – another impressive improvement. As of 2019, both TPU2 and TPU3 are in production at different prices:</p>
<figure class="mediaobject"><img alt="" height="240" src="../Images/B18331_15_06.png" width="825"/></figure>
<p class="packt_figref">Figure 15.6: Google announced TPU v2 and v3 Pods in beta at the Google I/O 2019</p>
<p class="normal">A TPU v3 board has four TPU chips, eight cores, and liquid cooling. Google has adopted ultra-high-speed interconnect hardware derived from supercomputer technology for connecting thousands of TPUs with very low latency. </p>
<p class="normal">Each time a parameter is updated on a single TPU, all the <a id="_idIndexMarker1470"/>others are informed via a reduce-all algorithm typically adopted for parallel computation. So, you can think about a TPU v3 as one of the fastest supercomputers available today for matrix and tensor operations, with thousands of TPUs inside it.</p>
<h2 class="heading-2" id="_idParaDest-403">Fourth generation TPUs</h2>
<p class="normal">Google’s fourth generation TPU ASIC has more than double the matrix multiplication TFLOPs of TPU v3, a considerable <a id="_idIndexMarker1471"/>boost in memory bandwidth, and more advances in interconnect technology. Each TPU v4 chip provides more than 2x the compute power of a TPU v3 chip – up to 275 peak TFLOPS. Each TPU v4 Pod delivers 1.1 exaflops/s of peak performance. Google claims that TPU v4 Pods are used extensively to develop research breakthroughs such as MUM and LaMDA, and improve core products such as Search, Assistant, and Translate (see <a href="https://blog.google/technology/developers/io21-helpful-google/"><span class="url">https://blog.google/technology/developers/io21-helpful-google/</span></a>). As of April 2022, TPU v4s are only available in preview (<em class="italic">Figure 15.7</em>):</p>
<figure class="mediaobject"><img alt="" height="317" src="../Images/B18331_15_07.png" width="876"/></figure>
<p class="packt_figref">Figure 15.7: A TPU v4 chip and a portion of a TPU v4 Pod – source: https://twitter.com/google/status/1394785686683783170 </p>
<p class="normal">In this section, we have introduced four generations of TPUs. Before concluding, I wanted to mention that it is possible to save money by using preemptible Cloud TPUs for fault-tolerant<a id="_idIndexMarker1472"/> machine learning workloads. These workloads include but are not limited to long training runs with checkpointing or batch prediction on large datasets.</p>
<h2 class="heading-2" id="_idParaDest-404">Edge TPU</h2>
<p class="normal">In addition to the three generations of TPUs already discussed, in 2018 Google announced a special generation<a id="_idIndexMarker1473"/> of TPUs running on the edge. This <a id="_idIndexMarker1474"/>TPU is particularly appropriate for the <strong class="keyWord">Internet of Things </strong>(<strong class="keyWord">IoT</strong>) and for supporting TensorFlow Lite<a id="_idIndexMarker1475"/> on mobile and IoT. An individual Edge TPU can perform 4 trillion (fixed-point) operations per second (4 TOPS), using only 2 watts of power. An Edge TPU is designed for small, low-power devices and is ideal for on-device ML, with it being fast and power-efficient. Edge TPUs support the TensorFlow Lite development framework (see <em class="italic">Figure 15.8</em>). At the end of 2019, Google announced the Pixel 4 smartphone<a id="_idIndexMarker1476"/> containing an Edge TPU called the Pixel Neural Core:</p>
<figure class="mediaobject"><img alt="A picture containing logo  Description automatically generated" height="363" src="../Images/B18331_15_08.png" width="396"/></figure>
<p class="packt_figref">Figure 15.8: Two Edge TPUs on one penny – source: https://coral.ai/docs/edgetpu/faq/#what-is-the-edge-tpu </p>
<p class="normal">With this we conclude<a id="_idIndexMarker1477"/> the introduction to TPU v1, v2, v3, v4, and Edge TPU. In<a id="_idIndexMarker1478"/> the next section we will briefly discuss performance.</p>
<h1 class="heading-1" id="_idParaDest-405">TPU performance</h1>
<p class="normal">Discussing performance is always<a id="_idIndexMarker1479"/> difficult because it is important to first define the metrics that we are going to measure, and the set of workloads that we are going to use as benchmarks. For instance, Google reported an impressive linear scaling for TPU v2 used with ResNet-50 [4] (see <em class="italic">Figure 15.9</em> and <em class="italic">Figure 15.10</em>):</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="431" src="../Images/B18331_15_09.png" width="640"/></figure>
<p class="packt_figref">Figure 15.9: Linear scalability in the number of TPUs v2 when increasing the number of images</p>
<p class="normal">In addition, you can find online a comparison of ResNet-50 [4] where a full Cloud TPU v2 Pod is &gt;200x faster than a V100 NVIDIA Tesla GPU for ResNet-50 training:</p>
<figure class="mediaobject"><img alt="A picture containing timeline  Description automatically generated" height="342" src="../Images/B18331_15_10.png" width="804"/></figure>
<p class="packt_figref">Figure 15.10: A full Cloud TPU v2 Pod is &gt;200x faster than a V100 NVIDIA Tesla GPU for training a ResNet-50 model</p>
<p class="normal">According to Google, TPU v4 givse top-line results<a id="_idIndexMarker1480"/> for MLPerf1.0 [5] when compared with Nvidia A100 GPUs (see <em class="italic">Figure 15.11</em>). Indeed, these accelerators are designed by keeping in mind the latest large models encompassing billions and sometimes trillions of parameters (think about GPT-3, T5, and the Switch Transformer):</p>
<figure class="mediaobject"><img alt="Chart, bar chart  Description automatically generated" height="506" src="../Images/B18331_15_11.png" width="576"/></figure>
<p class="packt_figref">Figure 15.11: MLPerf 1.0 TPU v4 Pod performance – source: https://cloud.google.com/blog/products/ai-machine-learning/google-wins-mlperf-benchmarks-with-tpu-v4 </p>
<h1 class="heading-1" id="_idParaDest-406">How to use TPUs with Colab</h1>
<p class="normal">In this section, we show how to use TPUs with Colab. Just<a id="_idIndexMarker1481"/> point your browser to <a href="https://colab.research.google.com/"><span class="url">https://colab.research.google.com/</span></a> and change the runtime from<a id="_idIndexMarker1482"/> the <strong class="screenText">Runtime</strong> menu<a id="_idIndexMarker1483"/> as shown in <em class="italic">Figure 15.12</em>. First, you’ll need to enable TPUs for the notebook, then navigate to <strong class="screenText">Edit</strong>→<strong class="screenText">Notebook settings</strong> and select <strong class="screenText">TPU</strong> from the <strong class="screenText">Hardware accelerator</strong> drop-down box:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, chat or text message  Description automatically generated" height="336" src="../Images/B18331_15_12.png" width="485"/></figure>
<p class="packt_figref">Figure 15.12: Setting TPU as the hardware accelerator</p>
<h2 class="heading-2" id="_idParaDest-407">Checking whether TPUs are available</h2>
<p class="normal">First of all, let’s check if there is<a id="_idIndexMarker1484"/> a TPU available, by using this simple code fragment that returns the IP address assigned to the TPU. Communication between the CPU<a id="_idIndexMarker1485"/> and TPU happens via <strong class="keyWord">gRPC</strong> (<strong class="keyWord">gRPC Remote Procedure Call</strong>), which is a modern, open-source, high-performance <strong class="keyWord">Remote Procedure Call</strong> (<strong class="keyWord">RPC</strong>) framework<a id="_idIndexMarker1486"/> that can run in any environment:</p>
<pre class="programlisting code"><code class="hljs-code">%tensorflow_version <span class="hljs-number">2.</span>x
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Tensorflow version "</span> + tf.__version__)
<span class="hljs-keyword">try</span>:
  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  <span class="hljs-comment"># TPU detection</span>
  <span class="hljs-built_in">print</span>(<span class="hljs-string">'Running on TPU '</span>, tpu.cluster_spec().as_dict()[<span class="hljs-string">'worker'</span>])
<span class="hljs-keyword">except</span> ValueError:
  <span class="hljs-keyword">raise</span> BaseException(<span class="hljs-string">'ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!'</span>)
tf.config.experimental_connect_to_cluster(tpu)
tf.tpu.experimental.initialize_tpu_system(tpu)
tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)
</code></pre>
<p class="normal">You should see something like the following:</p>
<pre class="programlisting con"><code class="hljs-con">Tensorflow version 2.8.0
Running on TPU  ['10.36.66.50:8470']
INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.
INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.
INFO:tensorflow:Initializing the TPU system: grpc://10.36.66.50:8470
INFO:tensorflow:Initializing the TPU system: grpc://10.36.66.50:8470
INFO:tensorflow:Finished initializing TPU system.
INFO:tensorflow:Finished initializing TPU system.
WARNING:absl:'tf.distribute.experimental.TPUStrategy' is deprecated, please use  the non experimental symbol 'tf.distribute.TPUStrategy' instead.
INFO:tensorflow:Found TPU system:
INFO:tensorflow:Found TPU system:
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Cores: 8
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Workers: 1
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
INFO:tensorflow:*** Num TPU Cores Per Worker: 8
</code></pre>
<p class="normal">We’ve confirmed that a<a id="_idIndexMarker1487"/> TPU is available! </p>
<h2 class="heading-2" id="_idParaDest-408">Keras MNIST TPU end-to-end training</h2>
<p class="normal">Referring to the notebook<a id="_idIndexMarker1488"/> available on Google <a id="_idIndexMarker1489"/>Research Colab (see <a href="https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/01_MNIST_TPU_Keras.ipynb#scrollTo=Hd5zB1G7Y9-7"><span class="url">https://colab.research.google.com/github/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/01_MNIST_TPU_Keras.ipynb#scrollTo=Hd5zB1G7Y9-7</span></a>), we can check how TPUs or GPUs are detected with this code snippet, which uses either TPUs or GPUs as a fallback:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">try</span>: <span class="hljs-comment"># detect TPUs</span>
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() <span class="hljs-comment"># TPU detection</span>
    strategy = tf.distribute.TPUStrategy(tpu)
<span class="hljs-keyword">except</span> ValueError: <span class="hljs-comment"># detect GPUs</span>
    strategy = tf.distribute.MirroredStrategy() <span class="hljs-comment"># for GPU or multi-GPU machines</span>
    <span class="hljs-comment">#strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU</span>
    <span class="hljs-comment">#strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Number of accelerators: "</span>, strategy.num_replicas_in_sync)
</code></pre>
<p class="normal">Note that the strategy <code class="inlineCode">tf.distribute.TPUStrategy(tpu)</code> is the only change you need in code for synchronous<a id="_idIndexMarker1490"/> training on TPUs and TPU Pods. Then, to run TF2 programs on TPUs, you can either use <code class="inlineCode">.compile</code> or <code class="inlineCode">.fit</code> APIs in <code class="inlineCode">tf.keras</code> with <code class="inlineCode">TPUStrategy</code>. </p>
<p class="normal">If you want you can write your own customized training loop by calling <code class="inlineCode">strategy.run</code> directly (see <a href="https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy"><span class="url">https://www.tensorflow.org/api_docs/python/tf/distribute/TPUStrategy</span></a>).</p>
<h1 class="heading-1" id="_idParaDest-409">Using pretrained TPU models</h1>
<p class="normal">Google offers a collection of models pretrained<a id="_idIndexMarker1491"/> with TPUs available in the GitHub <code class="inlineCode">tensorflow/tpu</code> repository (<a href="https://github.com/tensorflow/tpu"><span class="url">https://github.com/tensorflow/tpu</span></a>). Models include image recognition, object detection, low-resource models, machine translation and language models, speech recognition, and image generation. Whenever it is possible, my suggestion is to start with a pretrained model [6], and then fine-tune it or apply some form of transfer learning. As of April 2022, the following models are available:</p>
<table class="table-container" id="table001-8">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Image Recognition, Segmentation, and More</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Machine Translation and Language Models</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Speech Recognition</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Image Generation</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Image Recognition</strong></p>
<p class="normal">AmoebaNet-D</p>
<p class="normal">ResNet-50/101/152/2000</p>
<p class="normal">Inception v2/v3/v4</p>
<p class="normal"><strong class="keyWord">Object Detection</strong></p>
<p class="normal">RetinaNet</p>
<p class="normal">Mask R-CNN</p>
<p class="normal"><strong class="keyWord">Image Segmentation</strong></p>
<p class="normal">Mask R-CNN</p>
<p class="normal">DeepLab</p>
<p class="normal">RetinaNet</p>
<p class="normal"><strong class="keyWord">Low-Resource Models</strong></p>
<p class="normal">MnasNet</p>
<p class="normal">MobileNet</p>
<p class="normal">SqueezeNet</p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Machine Translation</strong></p>
<p class="normal">(transformer based)</p>
<p class="normal"><strong class="keyWord">Sentiment Analysis</strong></p>
<p class="normal">(transformer based)</p>
<p class="normal"><strong class="keyWord">Question Answer</strong></p>
<p class="normal"><strong class="keyWord">BERT</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">ASR </strong></p>
<p class="normal"><strong class="keyWord">Transformer</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Image Transformer</strong></p>
<p class="normal"><strong class="keyWord">DCGAN</strong></p>
<p class="normal"><strong class="keyWord">GAN</strong></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 15.1: State-of-the-art collection of models pretrained with TPUs available on GitHub</p>
<p class="normal">The best way to play with<a id="_idIndexMarker1492"/> the repository is to clone it on the Google Cloud console and use the environment available at <a href="https://github.com/tensorflow/tpu/blob/master/README.md"><span class="url">https://github.com/tensorflow/tpu/blob/master/README.md</span></a>. You should be able to browse what is shown in <em class="italic">Figure 15.13</em>:</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application, email  Description automatically generated" height="348" src="../Images/B18331_15_13.png" width="519"/></figure>
<p class="packt_figref">Figure 15.13: Cloud TPUs</p>
<p class="normal">If you click the button <strong class="screenText">OPEN IN GOOGLE CLOUD SHELL</strong>, then the system will clone the Git repository into your cloud shell and then open the shell (see <em class="italic">Figure 15.14</em>):</p>
<figure class="mediaobject"><img alt="Graphical user interface, text, application  Description automatically generated" height="591" src="../Images/B18331_15_14.png" width="879"/></figure>
<p class="packt_figref">Figure 15.14: Google Cloud Shell with the TPU Git repository cloned on your behalf</p>
<p class="normal">From there, you can play with a nice<a id="_idIndexMarker1493"/> Google Cloud TPU demo for training a ResNet-50 on MNIST with a TPU flock – a Compute Engine VM and Cloud TPU pair (see <em class="italic">Figure 15.15</em>):</p>
<figure class="mediaobject"><img alt="Text  Description automatically generated" height="494" src="../Images/B18331_15_15.png" width="419"/></figure>
<p class="packt_figref">Figure 15.15: Google Cloud TPU demo for training a ResNet-50 on MNIST with a TPU flock</p>
<p class="normal">I will leave this training demo<a id="_idIndexMarker1494"/> for you if you are interested in looking it up.</p>
<h1 class="heading-1" id="_idParaDest-410">Summary</h1>
<p class="normal">TPUs are very special ASIC chips developed at Google for executing neural network mathematical operations in an ultra-fast manner. The core of the computation is a systolic multiplier that computes multiple dot products (row * column) in parallel, thus accelerating the computation of basic deep learning operations. Think of a TPU as a special-purpose co-processor for deep learning that is focused on matrix or tensor operations. Google has announced four generations of TPUs so far, plus an additional Edge TPU for IoT. Cloud TPU v1 is a PCI-based specialized co-processor, with 92 teraops and inference only. Cloud TPU v2 achieves 180 teraflops and it supports training and inference. Cloud TPU v2 Pods released in alpha in 2018 can achieve 11.5 petaflops. Cloud TPU v3 achieves 420 teraflops with both training and inference support. Cloud TPU v3 Pods can deliver more than 100 petaflops of computing power. Each TPU v4 chip provides more than 2x the compute power of a TPU v3 chip – up to 275 peak TFLOPS. Each TPU v4 Pod delivers 1.1 exaflops/s of peak performance.</p>
<p class="normal">That’s a world-class supercomputer for tensor operations!</p>
<p class="normal">In the next chapter, we will see some other useful deep learning libraries.</p>
<h1 class="heading-1" id="_idParaDest-411">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Moore’s law: <a href="https://en.wikipedia.org/wiki/Moore%27s_law"><span class="url">https://en.wikipedia.org/wiki/Moore%27s_law</span></a></li>
<li class="numberedList">Milovanović, I. Ž. et al. (May 2010). <em class="italic">Forty-three ways of systolic matrix multiplication</em>. Article in International Journal of Computer Mathematics 87(6):1264–1276.</li>
<li class="numberedList">Jouppi, N. P. et al. (June 2014). <em class="italic">In-Datacenter Performance Analysis of a Tensor Processing Unit</em>. 44th International Symposium on Computer Architecture (ISCA).</li>
<li class="numberedList">Google TPU v2 performance: <a href="https://storage.googleapis.com/nexttpu/index.xhtml"><span class="url">https://storage.googleapis.com/nexttpu/index.xhtml</span></a></li>
<li class="numberedList">MLPerf site: <a href="https://mlperf.org/"><span class="url">https://mlperf.org/</span></a></li>
<li class="numberedList">A collection of models pretrained with TPU: <a href="https://cloud.google.com/tpu"><span class="url">https://cloud.google.com/tpu</span></a></li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>