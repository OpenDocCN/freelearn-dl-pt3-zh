- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bellman Equation and Dynamic Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that in reinforcement learning our goal
    is to find the optimal policy. The optimal policy is the policy that selects the
    correct action in each state so that the agent can get the maximum return and
    achieve its goal. In this chapter, we'll learn about two interesting classic reinforcement
    learning algorithms called the value and policy iteration methods, which we can
    use to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Before diving into the value and policy iteration methods directly, first, we
    will learn about the Bellman equation. The Bellman equation is ubiquitous in reinforcement
    learning and it is used for finding the optimal value and Q functions. We will
    understand what the Bellman equation is and how it finds the optimal value and
    Q functions.
  prefs: []
  type: TYPE_NORMAL
- en: After understanding the Bellman equation, we will learn about two interesting
    dynamic programming methods called value and policy iterations, which use the
    Bellman equation to find the optimal policy. At the end of the chapter, we will
    learn how to solve the Frozen Lake problem by finding an optimal policy using
    the value and policy iteration methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Bellman optimality equation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relationship between the value and Q functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic programming – value and policy iteration methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Solving the Frozen Lake problem using value and policy iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Bellman equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Bellman equation, named after Richard Bellman, helps us solve the **Markov
    decision process** (**MDP**). When we say solve the MDP, we mean finding the optimal
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: As stated in the introduction of the chapter, the Bellman equation is ubiquitous
    in reinforcement learning and is widely used for finding the optimal value and
    Q functions recursively. Computing the optimal value and Q functions is very important
    because once we have the optimal value or optimal Q function, then we can use
    them to derive the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll learn what exactly the Bellman equation is and how we
    can use it to find the optimal value and Q functions.
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman equation of the value function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Bellman equation states that the value of a state can be obtained as a
    sum of the immediate reward and the discounted value of the next state. Say we
    perform an action *a* in state *s* and move to the next state ![](img/B15558_03_001.png)
    and obtain a reward *r*, then the Bellman equation of the value function can be
    expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the above equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_003.png) implies the immediate reward obtained while performing
    an action *a* in state *s* and moving to the next state ![](img/B15558_03_004.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B15558_03_005.png) is the discount factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_03_006.png) implies the value of the next state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand the Bellman equation with an example. Say we generate a trajectory
    ![](img/B15558_03_007.png) using some policy ![](img/B15558_03_008.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.1: Trajectory'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we need to compute the value of state *s*[2]. According to the
    Bellman equation, the value of state *s*[2] is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_009.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, ![](img/B15558_03_010.png) implies the immediate
    reward we obtain while performing an action *a*[2] in state *s*[2] and moving
    to state *s*[3]. From the trajectory, we can tell that the immediate reward ![](img/B15558_03_011.png)
    is *r*[2]. And the term ![](img/B15558_03_012.png) is the discounted value of
    the next state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, according to the Bellman equation, the value of state *s*[2] is given
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the Bellman equation of the value function can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_014.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the superscript ![](img/B15558_01_047.png) implies that we are using policy
    ![](img/B15558_03_016.png). The right-hand side term ![](img/B15558_03_017.png)
    is often called the **Bellman backup**.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding Bellman equation works only when we have a deterministic environment.
    Let's suppose our environment is stochastic, then in that case, when we perform
    an action *a* in state *s*, it is not guaranteed that our next state will always
    be ![](img/B15558_03_018.png); it could be some other states too. For instance,
    look at the trajectory in *Figure 3.2*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, when we perform an action *a*[1] in state *s*[1], with a probability
    0.7, we reach state s[2], and with a probability 0.3, we reach state s[3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: Transition probability of performing action *a*[1] in state *s*[1]'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, when we perform action *a*[1] in state *s*[1], there is a 70% chance the
    next state will be *s*[2] and a 30% chance the next state will be *s*[3]. We learned
    that the Bellman equation is a sum of immediate reward and the discounted value
    of the next state. But when our next state is not guaranteed due to the stochasticity
    present in the environment, how can we define our Bellman equation?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we can slightly modify our Bellman equation with the expectations
    (the weighted average), that is, a sum of the Bellman backup multiplied by the
    corresponding transition probability of the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_020.png) denotes the transition probability of reaching ![](img/B15558_03_021.png)
    by performing an action *a* in state *s*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_03_022.png) denotes the Bellman backup'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand this equation better by considering the same trajectory we
    just used. As we notice, when we perform an action *a*[1] in state *s*[1], we
    go to *s*[2] with a probability of 0.70 and *s*[3] with a probability of 0.30\.
    Thus, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_023.png)![](img/B15558_03_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the Bellman equation of the value function including the stochasticity
    present in the environment using the expectation (weighted average) is expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_019.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Okay, but what if our policy is a stochastic policy? We learned that with a
    stochastic policy, we select actions based on a probability distribution; that
    is, instead of performing the same action in a state, we select an action based
    on the probability distribution over the action space. Let''s understand this
    with a different trajectory, shown in *Figure 3.3*. As we see, in state *s*[1],
    with a probability of 0.8, we select action *a*[1] and reach state *s*[2], and
    with a probability of 0.2, we select action *a*[2] and reach state *s*[3]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: Trajectory using a stochastic policy'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, when we use a stochastic policy, our next state will not always be the
    same; it will be different states with some probability. Now, how can we define
    the Bellman equation including the stochastic policy?
  prefs: []
  type: TYPE_NORMAL
- en: We learned that to include the stochasticity present in the environment in the
    Bellman equation, we took the expectation (the weighted average), that is, a sum
    of the Bellman backup multiplied by the corresponding transition probability of
    the next state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly, to include the stochastic nature of the policy in the Bellman equation,
    we can use the expectation (the weighted average), that is, a sum of the Bellman
    backup multiplied by the corresponding probability of action.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, our final Bellman equation of the value function can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation is also known as the **Bellman expectation equation**
    of the value function. We can also express the above equation in expectation form.
    Let''s recollect the definition of expectation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_027.png)'
  prefs: []
  type: TYPE_IMG
- en: In equation (1), ![](img/B15558_03_028.png) and ![](img/B15558_03_029.png) and
    ![](img/B15558_03_030.png) which denote the probability of the stochastic environment
    and stochastic policy, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we can write the Bellman equation of the value function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_031.png)'
  prefs: []
  type: TYPE_IMG
- en: The Bellman equation of the Q function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to compute the Bellman equation of the state-action value
    function, that is, the Q function. The Bellman equation of the Q function is very
    similar to the Bellman equation of the value function except for a small difference.
    Similar to the Bellman equation of the value function, the Bellman equation of
    the Q function states that the Q value of a state-action pair can be obtained
    as a sum of the immediate reward and the discounted Q value of the next state-action
    pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_032.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_033.png) implies the immediate reward obtained while performing
    an action *a* in state *s* and moving to the next state ![](img/B15558_03_034.png)'
  prefs:
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B15558_03_035.png) is the discount factor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_03_036.png) is the Q value of the next state-action pair'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand this with an example. Say we generate a trajectory ![](img/B15558_03_037.png)
    using some policy ![](img/B15558_03_038.png) as shown in *Figure 3.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: Trajectory'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we need to compute the Q value of a state-action pair (*s*[2],
    *a*[2]). Then, according to the Bellman equation, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the above equation, *R*(*s*[2], *a*[2], *a*[3]) represents the immediate
    reward we obtain while performing an action *a*[2] in state *s*[2] and moving
    to state *s*[3]. From the preceding trajectory, we can tell that the immediate
    reward *R*(*s*[2], *a*[2], *s*[3]) is *r*[2]. And the term ![](img/B15558_03_040.png)
    represents the discounted Q value of the next state-action pair. Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the Bellman equation for the Q function can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_042.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the superscript ![](img/B15558_03_038.png) implies that we are using the
    policy ![](img/B15558_03_038.png) and the right-hand side term ![](img/B15558_03_045.png)
    is the **Bellman backup**.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to what we learned in the Bellman equation of the value function, the
    preceding Bellman equation works only when we have a deterministic environment
    because in the stochastic environment our next state will not always be the same
    and it will be based on a probability distribution. Suppose we have a stochastic
    environment, then when we perform an action *a* in state *s*. It is not guaranteed
    that our next state will always be ![](img/B15558_03_046.png); it could be some
    other states too with some probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, just like we did in the previous section, we can use the expectation (the
    weighted average), that is, a sum of the Bellman backup multiplied by their corresponding
    transition probability of the next state, and rewrite our Bellman equation of
    the Q function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_047.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, when we use a stochastic policy, our next state will not always
    be the same; it will be different states with some probability. So, to include
    the stochastic nature of the policy, we can rewrite our Bellman equation with
    the expectation (the weighted average), that is, a sum of Bellman backup multiplied
    by the corresponding probability of action, just like we did in the Bellman equation
    of the value function. Thus, the Bellman equation of the Q function is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_048.png)'
  prefs: []
  type: TYPE_IMG
- en: But wait! There is a small change in the above equation. Why do we need to add
    the term ![](img/B15558_03_049.png) in the case of a Q function? Because in the
    value function *V*(*s*), we are given only a state *s* and we choose an action
    *a* based on the policy ![](img/B15558_03_050.png). So, we added the term ![](img/B15558_03_049.png)
    to include the stochastic nature of the policy. But in the case of the Q function
    *Q*(*s*, *a*), we will be given both state *s* and action *a*, so we don't need
    to add the term ![](img/B15558_03_049.png) in our equation since we are not selecting
    any action *a* based on the policy ![](img/B15558_03_050.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'However, if you look at the above equation, we need to select action ![](img/B15558_03_054.png)
    based on the policy ![](img/B15558_03_055.png) while computing the Q value of
    the next state-action pair ![](img/B15558_03_056.png) since ![](img/B15558_03_057.png)
    will not be given. So, we can just place the term ![](img/B15558_03_058.png) before
    the Q value of the next state-action pair. Thus, our final Bellman equation of
    the Q function can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_059.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Equation (3) is also known as the Bellman expectation equation of the Q function.
    We can also express the equation (3) in expectation form as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_060.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have understood what the Bellman expectation equation is, in the
    next section, we will learn about the Bellman optimality equation and explore
    how it is useful for finding the optimal Bellman value and Q functions.
  prefs: []
  type: TYPE_NORMAL
- en: The Bellman optimality equation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Bellman optimality equation gives the optimal Bellman value and Q functions.
    First, let''s look at the optimal Bellman value function. We learned that the
    Bellman equation of the value function is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_061.png)'
  prefs: []
  type: TYPE_IMG
- en: In the first chapter, we learned that the value function depends on the policy,
    that is, the value of the state varies based on the policy we choose. There can
    be many different value functions according to different policies. The optimal
    value function, ![](img/B15558_03_062.png) is the one that yields the maximum
    value compared to all the other value functions. Similarly, there can be many
    different Bellman value functions according to different policies. The optimal
    Bellman value function is the one that has the maximum value.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we compute the optimal Bellman value function that has the maximum
    value?
  prefs: []
  type: TYPE_NORMAL
- en: We can compute the optimal Bellman value function by selecting the action that
    gives the maximum value. But we don't know which action gives the maximum value,
    so, we compute the value of state using all possible actions, and then we select the
    maximum value as the value of the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, instead of using some policy ![](img/B15558_01_047.png) to select
    the action, we compute the value of the state using all possible actions, and
    then we select the maximum value as the value of the state. Since we are not using
    any policy, we can remove the expectation over the policy ![](img/B15558_03_055.png)
    and add the max over the action and express our optimal Bellman value function
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_065.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It''s just the same as the Bellman equation, except here we are taking a maximum
    over all the possible actions instead of the expectation (weighted average) over
    the policy since we are only interested in the maximum value. Let''s understand
    this with an example. Say we are in a state *s* and we have two possible actions
    in the state. Let the actions be 0 and 1\. Then ![](img/B15558_03_066.png) is
    given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_067.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe from the above equation, we compute the state value using
    all possible actions (0 and 1) and then select the maximum value as the value
    of the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s look at the optimal Bellman Q function. We learned that the Bellman
    equation of the Q function is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_068.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just like we learned with the optimal Bellman value function, instead of using
    the policy to select action ![](img/B15558_03_069.png) in the next state ![](img/B15558_03_070.png),
    we choose all possible actions in that state ![](img/B15558_03_004.png) and compute
    the maximum Q value. It can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_072.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s understand this with an example. Say we are in a state *s* with an action
    *a*. We perform action *a* in state *s* and reach the next state ![](img/B15558_03_073.png).
    We need to compute the Q value for the next state ![](img/B15558_03_018.png).
    There can be many actions in state ![](img/B15558_03_018.png). Let''s say we have
    two actions 0 and 1 in state ![](img/B15558_03_034.png). Then we can write the
    optimal Bellman Q function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_077.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, to summarize, the Bellman optimality equations of the value function
    and Q function are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_078.png)![](img/B15558_03_079.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also expand the expectation and rewrite the preceding Bellman optimality
    equations as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_080.png)![](img/B15558_03_081.png)'
  prefs: []
  type: TYPE_IMG
- en: The relationship between the value and Q functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s take a little detour and recap the value and Q functions we covered
    in *Chapter 1*, *Fundamentals of Reinforcement Learning*. We learned that the
    value of a state (value function) denotes the expected return starting from that
    state following a policy ![](img/B15558_03_082.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_083.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, the Q value of a state-action pair (Q function) represents the expected
    return starting from that state-action pair following a policy ![](img/B15558_03_084.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_085.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We learned that the optimal value function gives the maximum state-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_086.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And the optimal Q function gives the maximum state-action value (Q value):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_087.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Can we derive some relation between the optimal value function and optimal
    Q function? We know that the optimal value function has the maximum expected return
    when we start from a state *s* and the optimal Q function has the maximum expected
    return when we start from state *s* performing some action *a*. So, we can say
    that the optimal value function is the maximum of optimal Q value over all possible
    actions, and it can be expressed as follows (that is, we can derive V from Q):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_088.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Alright, now let''s get back to our Bellman equations. Before going ahead,
    let''s just recap the Bellman equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bellman expectation equation of the value function and Q function**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_03_089.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B15558_03_090.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '**Bellman** **optimality** **equation of the value function and Q function**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15558_03_091.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: '![](img/B15558_03_092.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: 'We learned that the optimal Bellman Q function is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_092.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we have an optimal value function ![](img/B15558_03_094.png), then we can
    use it to derive the preceding optimal Bellman Q function, (that is, we can derive
    *Q* from *V*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_095.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation is one of the most useful identities in reinforcement
    learning, and we will see how it will help us in finding the optimal policy in
    the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, to summarize, we learned that we can derive *V* from *Q*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_096.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And derive *Q* from *V*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_097.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation (8) in equation (7), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_080.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe, we just obtained the optimal Bellman value function. Now
    that we understand the Bellman equation and the relationship between the value
    and the Q function, we can move on to the next section on how to make use of these
    equations to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dynamic programming** (**DP**) is a technique for solving complex problems.
    In DP, instead of solving a complex problem as a whole, we break the problem into
    simple sub-problems, then for each sub-problem, we compute and store the solution.
    If the same subproblem occurs, we don''t recompute; instead, we use the already
    computed solution. Thus, DP helps in drastically minimizing the computation time.
    It has its applications in a wide variety of fields including computer science,
    mathematics, bioinformatics, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will learn about two important methods that use DP to find the optimal
    policy. The two methods are:'
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy iteration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that dynamic programming is a model-based method meaning that it will help
    us to find the optimal policy only when the model dynamics (transition probability)
    of the environment are known. If we don't have the model dynamics, we cannot apply
    DP methods.
  prefs: []
  type: TYPE_NORMAL
- en: The upcoming sections are explained with manual calculations, for a better understanding,
    follow along with a pen and paper.
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the value iteration method, we try to find the optimal policy. We learned
    that the optimal policy is the one that tells the agent to perform the correct
    action in each state. In order to find the optimal policy, first, we compute the
    optimal value function and once we have the optimal value function, we can use
    it to derive the optimal policy. Okay, how can we compute the optimal value function?
    We can use our optimal Bellman equation of the value function. We learned that,
    according to the Bellman optimality equation, the optimal value function can be
    computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the *The relationship between the value and Q functions* section, we learned
    that given the value function, we can derive the Q function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_100.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting (10) in (9), we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_088.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we can compute the optimal value function by just taking the maximum over
    the optimal Q function. So, in order to compute the value of a state, we compute
    the Q value for all state-action pairs. Then, we select the maximum Q value as
    the value of the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this with an example. Say we have two states, *s*[0] and
    *s*[1], and we have two possible actions in these states; let the actions be 0
    and 1\. First, we compute the Q value for all possible state-action pairs. *Table
    3.1* shows the Q values for all possible state-action pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.1: Q values of all possible state-action pairs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, in each state, we select the maximum Q value as the optimal value of
    a state. Thus, the value of state *s*[0] is 3 and the value of state *s*[1] is
    4\. The optimal value of the state (value function) is shown *Table 3.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.2: Optimal state values'
  prefs: []
  type: TYPE_NORMAL
- en: Once we obtain the optimal value function, we can use it to extract the optimal
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of how the value iteration method finds
    the optimal value function, in the next section, we will go into detail and learn
    how exactly the value iteration method works and how it finds the optimal policy
    from the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: The value iteration algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The algorithm of value iteration is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the optimal value function by taking the maximum over the Q function,
    that is, ![](img/B15558_03_088.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the optimal policy from the computed optimal value function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s go into detail and learn exactly how the above two steps work. For better
    understanding, let''s perform the value iteration manually. Consider the small
    grid world environment shown in *Figure 3.5*. Let''s say we are in state **A**
    and our goal is to reach state **C** without visiting the shaded state **B**,
    and say we have two actions, 0—left/right, and 1—up/down:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: Grid world environment'
  prefs: []
  type: TYPE_NORMAL
- en: Can you think of what the optimal policy is here? The optimal policy here is
    the one that tells us to perform action 1 in state **A** so that we can reach
    **C** without visiting **B**. Now we will see how to find this optimal policy
    using value iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 3.3* shows the model dynamics of state **A**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.3: Model dynamics of state A'
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Compute the optimal value function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We can compute the optimal value function by computing the maximum over the
    Q function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_088.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, we compute the Q value for all state-action pairs and then we select
    the maximum Q value as the value of a state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Q value for a state *s* and action *a* can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_104.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For notation simplicity, we can denote ![](img/B15558_03_105.png) by ![](img/B15558_03_106.png)
    and ![](img/B15558_03_107.png) by ![](img/B15558_03_108.png)and rewrite the preceding
    equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_109.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, using the preceding equation, we can compute the Q function. If you look
    at the equation, to compute the Q function, we need the transition probability
    ![](img/B15558_03_106.png), the reward function ![](img/B15558_03_108.png)**,**and
    the value of the next state ![](img/B15558_03_112.png). The model dynamics provide
    us with the transition probability ![](img/B15558_03_106.png) and the reward function
    ![](img/B15558_03_108.png). But what about the value of the next state ![](img/B15558_03_115.png)?
    We don't know the value of any states yet. So, we will initialize the value function
    (state values) with random values or zeros as shown in *Table 3.4* and compute
    the Q function.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.4: Initial value table'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s compute the Q value of state **A**. We have two actions in state **A,**
    which are 0 and 1\. So, first let''s compute the Q value for state **A** and action
    0 (note that we use the discount factor ![](img/B15558_03_116.png) throughout
    this section):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_117.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s compute the Q value for state **A** and action 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_118.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q values for both the actions in state **A**, we can update
    the Q table as shown in *Table 3.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.5: Q table'
  prefs: []
  type: TYPE_NORMAL
- en: We learned that the optimal value of a state is just the max of the Q function.
    That is, ![](img/B15558_03_088.png). By looking at *Table 3.5*, we can say that
    the value of state **A**, *V*(*A*), is *Q*(*A*, 1) since *Q*(*A*, 1) has a higher
    value than *Q*(*A*, 0). Thus, *V*(*A*) = 0.9.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can update the value of state **A** in our value table as shown in *Table
    3.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.6: Updated value table'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in order to compute the value of state **B**, *V*(*B*), we compute
    the Q value of *Q*(*B*, 0) and *Q*(*B*, 1) and select the highest Q value as the
    value of state **B**. In the same way, to compute the values of other states,
    we compute the Q value for all state-action pairs and select the maximum Q value
    as the value of a state.
  prefs: []
  type: TYPE_NORMAL
- en: 'After computing the value of all the states, our updated value table may resemble
    *Table 3.7*. This is the result of the first iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.7: Value table from iteration 1'
  prefs: []
  type: TYPE_NORMAL
- en: However, the value function (value table) shown in *Table 3.7* obtained as a
    result of the first iteration is not an optimal one. But why? We learned that
    the optimal value function is the maximum of the optimal Q function. That is,
    ![](img/B15558_03_088.png). Thus to find the optimal value function, we need the
    optimal Q function. But the Q function may not be an optimal one in the first
    iteration as we computed the Q function based on the randomly initialized state
    values.
  prefs: []
  type: TYPE_NORMAL
- en: As the following shows, when we started off computing the Q function, we used
    the randomly initialized state values.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_31.png)'
  prefs: []
  type: TYPE_IMG
- en: So, what we can do is, in the next iteration, while computing the Q function,
    we can use the updated state values obtained as a result of the first iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, in the second iteration, to compute the value function, we compute
    the Q value of all state-action pairs and select the maximum Q value as the value
    of a state. In order to compute the Q value, we need to know the state values,
    in the first iteration, we used the randomly initialized state values. But in
    the second iteration, we use the updated state values (value table) obtained from
    the first iteration as the following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_32.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Iteration 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's compute the Q value of state **A**. Remember that while computing the
    Q value, we use the updated state values from the previous iteration.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s compute the Q value of state **A** and action 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_121.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s compute the Q value for state **A** and action 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_122.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we may observe, since the Q value of action 1 in state A is higher than
    action 0, the value of state A becomes 1.44\. Similarly, we compute the value
    for all the states and update the value table. *Table 3.8* shows the updated value
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.8: Value table from iteration 2'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 3**:'
  prefs: []
  type: TYPE_NORMAL
- en: We repeat the same steps we saw in the previous iteration and compute the value
    of all the states by selecting the maximum Q value. Remember that while computing
    the Q value, we use the updated state values (value table) obtained from the previous
    iteration. So, we use the updated state values from iteration 2 to compute the
    Q value.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 3.9* shows the updated state values obtained as a result of the third
    iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.9: Value table from iteration 3'
  prefs: []
  type: TYPE_NORMAL
- en: So, we repeat these steps for many iterations until we find the optimal value
    function. But how can we understand whether we have found the optimal value function
    or not? When the value function (value table) does not change over iterations
    or when it changes by a very small fraction, then we can say that we have attained
    convergence, that is, we have found an optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we find out whether the value table is changing or not changing
    from the previous iteration? We can calculate the difference between the value
    table obtained from the previous iteration and the value table obtained from the
    current iteration. If the difference is very small—say, the difference is less
    than a very small threshold number—then we can say that we have attained convergence
    as there is not much change in the value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s suppose *Table 3.10* shows the value table obtained as
    a result of **iteration 4**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.10: Value table from iteration 4'
  prefs: []
  type: TYPE_NORMAL
- en: As we can notice, the difference between the value table obtained as a result
    of iteration 4 and iteration 3 is very small. So, we can say that we have attained
    convergence and we take the value table obtained as a result of iteration 4 as
    our optimal value function. Please note that the above example is just for better
    understanding; in practice, we cannot attain convergence in just four iterations—it
    usually takes many iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have found the optimal value function, in the next step, we will
    use this optimal value function to extract an optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Extract the optimal policy from the optimal value function obtained
    from step 1
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As a result of *Step 1*, we obtained the optimal value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.11: Optimal value table (value function)'
  prefs: []
  type: TYPE_NORMAL
- en: Now, how can we extract the optimal policy from the obtained optimal value function?
  prefs: []
  type: TYPE_NORMAL
- en: We generally use the Q function to compute the policy. We know that the Q function
    gives the Q value for every state-action pair. Once we have the Q values for all
    state-action pairs, we extract the policy by selecting the action that has the
    maximum Q value in each state. For example, consider the Q table in *Table 3.12*.
    It shows the Q values for all state-action pairs. Now we can extract the policy
    from the Q function (Q table) by selecting action 1 in the state *s*[0] and action
    0 in the state *s*[1] as they have the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.12: Q table'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, now we compute the Q function using the optimal value function obtained
    from *Step 1*. Once we have the Q function, then we extract the policy by selecting
    the action that has the maximum Q value in each state. Since we are computing
    the Q function using the optimal value function, the policy extracted from the
    Q function will be the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that the Q function can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_123.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, while computing Q values, we use the optimal value function we obtained
    from *step 1*. After computing the Q function, we can extract the optimal policy
    by selecting the action that has the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_124.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For instance, let''s compute the Q value for all actions in state **A** using
    the optimal value function. The Q value for action 0 in state **A** is computed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_125.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Q value for action 1 in state **A** is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_126.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since *Q*(*A*, 1) is higher than *Q*(*A*, 0), our optimal policy will select
    action 1 as the optimal action in state **A**. *Table 3.13* shows the Q table
    after computing the Q values for all state-action pairs using the optimal value
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.13: Q table'
  prefs: []
  type: TYPE_NORMAL
- en: From this Q table, we pick the action in each state that has the maximum value
    as an optimal policy. Thus, our optimal policy would select action 1 in state
    **A**, action 1 in state **B**, and action 1 in state **C**.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, according to our optimal policy, if we perform action 1 in state **A**,
    we can reach state **C** without visiting state **B**.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we learned how to compute the optimal policy using the value
    iteration method. In the next section, we will learn how to implement the value
    iteration method to compute the optimal policy in the Frozen Lake environment
    using the Gym toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the Frozen Lake problem with value iteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned about the Frozen Lake environment. The
    Frozen Lake environment is shown in *Figure 3.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: Frozen Lake environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s recap the Frozen Lake environment a bit. In the Frozen Lake environment
    shown in *Figure 3.6*, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S** implies the starting state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F** implies the frozen states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H** implies the hole states'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G** implies the goal state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We learned that in the Frozen Lake environment, our goal is to reach the goal
    state **G** from the starting state **S** without visiting the hole states **H**.
    That is, while trying to reach the goal state **G** from the starting state **S**,
    if the agent visits the hole states **H**, then it will fall into the hole and
    die as *Figure 3.7* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: Agent falling into the hole'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we want the agent to avoid the hole states **H** to reach the goal state
    **G** as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: Agent reaches the goal state'
  prefs: []
  type: TYPE_NORMAL
- en: How can we achieve this goal? That is, how can we reach state **G** from **S**
    without visiting **H**? We learned that the optimal policy tells the agent to
    perform the correct action in each state. So, if we find the optimal policy, then
    we can reach state **G** from **S** without visiting state **H**. Okay, how can
    we find the optimal policy? We can use the value iteration method we just learned
    to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that all our states (**S** to **G**) will be encoded from 0 to 16 and
    all four actions—*left*, *down*, *up*, *right*—will be encoded from 0 to 3 in
    the Gym toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will learn how to find the optimal policy using the value
    iteration method so that the agent can reach state **G** from **S** without visiting
    **H**.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the Frozen Lake environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the Frozen Lake environment using the `render` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will display:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.9: Gym Frozen Lake environment'
  prefs: []
  type: TYPE_NORMAL
- en: As we can notice, our agent is in state **S** and it has to reach state **G**
    without visiting the **H** states. So, let's learn how to compute the optimal
    policy using the value iteration method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the value iteration method, we perform two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute the optimal value function by taking the maximum over the Q function,
    that is, ![](img/B15558_03_088.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the optimal policy from the computed optimal value function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: First, let's learn how to compute the optimal value function, and then we will
    see how to extract the optimal policy from the computed optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the optimal value function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We will define a function called `value_iteration` where we compute the optimal
    value function iteratively by taking the maximum over the Q function, that is,
    ![](img/B15558_03_088.png). For better understanding, let's closely look at every
    line of the function, and then we'll look at the complete function at the end,
    which will provide more clarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `value_iteration` function, which takes the environment as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the threshold number for checking the convergence of the value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We also set the discount factor ![](img/B15558_03_035.png) to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will initialize the value table by setting the value of all states
    to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the value table, that is, we learned that on every iteration, we use
    the updated value table (state values) from the previous iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute the value function (state value) by taking the maximum of the
    Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_088.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_03_123.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, for each state, we compute the Q values of all the actions in the state
    and then we update the value of the state as the one that has the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the Q value of all the actions, ![](img/B15558_03_123.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the value of the state as a maximum Q value, ![](img/B15558_03_088.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After computing the value table, that is, the value of all the states, we check
    whether the difference between the value table obtained in the current iteration
    and the previous iteration is less than or equal to a threshold value. If the
    difference is less than the threshold, then we break the loop and return the value
    table as our optimal value function as the following code shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have computed the optimal value function by taking the maximum of
    the Q values, let's see how to extract the optimal policy from the optimal value
    function.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the optimal policy from the optimal value function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the previous step, we computed the optimal value function. Now, let's see
    how to extract the optimal policy from the computed optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define a function called `extract_policy`, which takes `value_table`
    as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the discount factor ![](img/B15558_03_005.png) to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we initialize the policy with zeros, that is, we set the actions for
    all the states to be zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute the Q function using the optimal value function obtained from
    the previous step. We learned that the Q function can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_123.png)'
  prefs: []
  type: TYPE_IMG
- en: After computing the Q function, we can extract the policy by selecting the action
    that has the maximum Q value. Since we are computing the Q function using the
    optimal value function, the policy extracted from the Q function will be the optimal
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_124.png)'
  prefs: []
  type: TYPE_IMG
- en: As the following code shows, for each state, we compute the Q values for all
    the actions in the state and then we extract the policy by selecting the action
    that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the Q value of all the actions in the state, ![](img/B15558_03_123.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the policy by selecting the action that has the maximum Q value, ![](img/B15558_03_124.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Now, we will see how to extract the optimal policy in our Frozen
    Lake environment.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We learned that in the Frozen Lake environment, our goal is to find the optimal
    policy that selects the correct action in each state so that we can reach state
    **G** from state **A** without visiting the hole states.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the optimal value function using our `value_iteration` function
    by passing our Frozen Lake environment as the parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we extract the optimal policy from the optimal value function using our
    `extract_policy` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print the obtained optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print the following. As we can observe, our optimal
    policy tells us to perform the correct action in each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have learned what value iteration is and how to perform the value
    iteration method to compute the optimal policy in our Frozen Lake environment,
    in the next section, we will learn about another interesting method, called policy
    iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the value iteration method, first, we computed the optimal value function
    by taking the maximum over the Q function (Q values) iteratively. Once we found
    the optimal value function, we used it to extract the optimal policy. Whereas
    in policy iteration we try to compute the optimal value function using the policy
    iteratively, once we found the optimal value function, we can use it to extract
    the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s learn how to compute the value function using a policy. Say we
    have a policy ![](img/B15558_03_139.png), how can we compute the value function
    using the policy ![](img/B15558_03_140.png)? Here, we can use our Bellman equation.
    We learned that according to the Bellman equation, we can compute the value function
    using the policy ![](img/B15558_03_082.png) as the following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s suppose our policy is a deterministic policy, so we can remove the term
    ![](img/B15558_03_143.png) from the preceding equation since there is no stochasticity
    in the policy and rewrite our Bellman equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_144.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For notation simplicity, we can denote ![](img/B15558_03_145.png) by ![](img/B15558_03_106.png)
    and ![](img/B15558_03_147.png) with ![](img/B15558_03_108.png)and rewrite the
    preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_149.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus using the above equation we can compute the value function using a policy.
    Our goal is to find the optimal value function because once we have found the
    optimal value function, we can use it to extract the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: We will not be given any policy as an input. So, we will initialize the random
    policy and compute the value function using the random policy. Then we check if
    the computed value function is optimal or not. It will not be optimal since it
    is computed based on the random policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we will extract a new policy from the computed value function, then we
    will use the extracted new policy to compute the new value function, and then
    we will check if the new value function is optimal. If it''s optimal we will stop,
    else we repeat these steps for a series of iterations. For a better understanding,
    look at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 1**: Let ![](img/B15558_03_150.png) be the random policy. We use
    this random policy to compute the value function ![](img/B15558_03_151.png). Our
    value function will not be optimal as it is computed based on the random policy.
    So, from ![](img/B15558_03_152.png), we extract a new policy ![](img/B15558_03_153.png).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 2**: Now, we use the new policy ![](img/B15558_03_153.png) derived
    from the previous iteration to compute the new value function ![](img/B15558_03_155.png),
    then we check if ![](img/B15558_03_156.png) is optimal. If it is optimal, we stop,
    else from this value function ![](img/B15558_03_157.png), we extract a new policy
    ![](img/B15558_03_158.png).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 3**: Now, we use the new policy ![](img/B15558_03_159.png) derived
    from the previous iteration to compute the new value function ![](img/B15558_03_160.png),
    then we check if ![](img/B15558_03_161.png) is optimal. If it is optimal, we stop,
    else from this value function ![](img/B15558_03_162.png), we extract a new policy
    ![](img/B15558_03_163.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat this process for many iterations until we find the optimal value
    function ![](img/B15558_03_164.png) as the following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_165.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding step is called policy evaluation and improvement. Policy evaluation
    implies that at each step we evaluate the policy by checking if the value function
    computed using that policy is optimal. Policy improvement means that at each step
    we find the new improved policy to compute the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have found the optimal value function ![](img/B15558_03_166.png), then
    it implies that we have also found the optimal policy. That is, if ![](img/B15558_03_164.png)
    is optimal, then the policy that is used to compute ![](img/B15558_03_168.png)
    will be an optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a better understanding of how policy iteration works, let''s look into
    the below steps with pseudocode. In the first iteration, we will initialize a
    random policy and use it to compute the value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Since we computed the value function using the random policy, the computed value
    function will not be optimal. So, we need to find a new policy with which we can
    compute the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we extract a new policy from the value function computed using a random
    policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will use this new policy to compute the new value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If the new value function is optimal, we stop, else we repeat the preceding
    steps for a number of iterations until we find the optimal value function. The
    following pseudocode gives us a better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Wait! How do we say our value function is optimal? If the value function is
    not changing over iterations, then we can say that our value function is optimal.
    Okay, how can we check if the value function is not changing over iterations?
  prefs: []
  type: TYPE_NORMAL
- en: We learned that we compute the value function using a policy. If the policy
    is not changing over iterations, then our value function also doesn't change over
    the iterations. Thus, when the policy doesn't change over iterations, then we
    can say that we have found the optimal value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, over a series of iterations when the policy and new policy become the
    same, then we can say that we obtained the optimal value function. The following
    final pseudocode is given for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Thus, when the policy is not changing, that is, when the policy and new policy
    become the same, then we can say that we obtained the optimal value function and
    the policy that is used to compute the optimal value function will be the optimal
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that in the value iteration method, we compute the optimal value function
    using the maximum over Q function (Q value) iteratively and once we have found
    the optimal value function, we extract the optimal policy from it. But in the
    policy iteration method, we compute the optimal value function using the policy
    iteratively and once we have found the optimal value function, then the policy that
    is used to compute the optimal value function will be the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of how the policy iteration method works,
    in the next section, we will get into the details and learn how to compute policy
    iteration manually.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm – policy iteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The steps of the policy iteration algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize a random policy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value function using the given policy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract a new policy using the value function obtained from *step* 2
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If the extracted policy is the same as the policy used in *step 2*, then stop,
    else send the extracted new policy to *step 2* and repeat *steps 2* to *4*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s get into the details and learn how exactly the preceding steps
    work. For a clear understanding, let''s perform policy iteration manually. Let''s
    take the same grid world environment we used in the value iteration method. Let''s
    say we are in state **A** and our goal is to reach state **C** without visiting
    the shaded state **B**, and say we have two actions, 0 – *left*/*right* and 1
    – *up*/*down*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10: Grid world environment'
  prefs: []
  type: TYPE_NORMAL
- en: We know that in the above environment, the optimal policy is the one that tells
    us to perform action 1 in state **A** so that we can reach **C** without visiting
    **B**. Now, we will see how to find this optimal policy using policy iteration.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 3.14* shows the model dynamics of state **A**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.14: Model dynamics of state A'
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Initialize a random policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, we will initialize a random policy. As the following shows, our random
    policy tells us to perform action 1 in state **A**, 0 in state **B**, and action
    1 in state **C**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_169.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 2 – Compute the value function using the given policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This step is exactly the same as how we computed the value function in value
    iteration but with a small difference. In value iteration, we computed the value
    function by taking the maximum over the Q function. But here in policy iteration,
    we will compute the value function using the policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this step better, let''s quickly recollect how we compute the
    value function in value iteration. In value iteration, we compute the optimal
    value function as the maximum over the optimal Q function as the following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_088.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B15558_03_171.png)
  prefs: []
  type: TYPE_NORMAL
- en: 'In policy iteration, we compute the value function using a policy ![](img/B15558_03_172.png),
    unlike value iteration, where we computed the value function using the maximum
    over the Q function. The value function using a policy ![](img/B15558_03_140.png)
    can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_174.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you look at the preceding equation, to compute the value function, we need
    the transition probability ![](img/B15558_03_175.png), the reward function ![](img/B15558_03_176.png)and
    the value of the next state ![](img/B15558_03_177.png). The values of the transition
    probability ![](img/B15558_03_175.png) and the reward function ![](img/B15558_03_176.png)
    can be obtained from the model dynamics. But what about the value of the next
    state ![](img/B15558_03_180.png)? We don''t know the value of any states yet.
    So, we will initialize the value function (state values) with random values or
    zeros as *Figure 3.15* shows and compute the value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.15: Initial value table'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: Let's compute the value of state **A** (note that here, we only compute the
    value for the action given by the policy, unlike in value iteration, where we
    computed the Q value for all the actions in the state and selected the maximum
    value).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the action given by the policy for state **A** is 1 and we can compute
    the value of state **A** as the following shows (note that we have used a discount
    factor ![](img/B15558_03_181.png) throughout this section):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_182.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we compute the value for all the states using the action given by
    the policy. *Table 3.16* shows the updated state values obtained as a result of
    the first iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.16: Value table from iteration 1'
  prefs: []
  type: TYPE_NORMAL
- en: However, the value function (value table) shown in *Table 3.16* obtained as
    a result of the first iteration will not be accurate. That is, the state values
    (value function) will not be accurate according to the given policy.
  prefs: []
  type: TYPE_NORMAL
- en: Note that unlike the value iteration method, here we are not checking whether
    our value function is optimal or not; we just check whether our value function
    is accurately computed according to the given policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The value function will not be accurate because when we started off computing
    the value function using the given policy, we used the randomly initialized state
    values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, in the next iteration, while computing the value function, we will use
    the updated state values obtained as a result of the first iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_34.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Iteration 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: Now, in iteration 2, we compute the value function using the policy ![](img/B15558_03_082.png).
    Remember that while computing the value function, we will use the updated state
    values (value table) obtained from iteration 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s compute the value of state A:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_184.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we compute the value for all the states using the action given by
    the policy. *Table 3.17* shows the updated state values obtained as a result of
    the second iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.17: Value table from iteration 2'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 3**:'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in iteration 3, we compute the value function using the policy ![](img/B15558_03_185.png)
    and while computing the value function, we will use the updated state values (value
    table) obtained from iteration 2.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 3.18* shows the updated state values obtained from the third iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.18: Value table from iteration 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat this for many iterations until the value table does not change or
    changes very little over iterations. For example, let''s suppose *Table 3.19*
    shows the value table obtained as a result of **iteration 4**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.19: Value table from iteration 4'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the difference between the value tables obtained from iteration
    4 and iteration 3 is very small. So, we can say that the value table is not changing
    much over iterations and we stop at this iteration and take this as our final
    value function.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Extract a new policy using the value function obtained from the previous
    step
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As a result of *step 2*, we obtained the value function, which is computed
    using the given random policy. However, this value function will not be optimal
    as it is computed using the random policy. So will extract a new policy from the
    value function obtained in the previous step. The value function (value table)
    obtained from the previous step is shown in *Table 3.20*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.20: Value table from the previous step'
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, how can we extract a new policy from the value function? (Hint: This
    step is exactly the same as how we extracted a policy given the value function
    in *step 2* of the value iteration method.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to extract a new policy, we compute the Q function using the value
    function (value table) obtained from the previous step. Once we compute the Q
    function, we pick up actions in each state that have the maximum value as a new
    policy. We know that the Q function can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_186.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, while computing Q values, we use the value function we obtained from the
    previous step.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, let''s compute the Q value for all actions in state **A** using
    the value function obtained from the previous step. The Q value for action 0 in
    state **A** is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_187.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Q value for action 1 in state **A** is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_188.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Table 3.21* shows the Q table after computing the Q values for all state-action
    pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 3.21: Q table'
  prefs: []
  type: TYPE_NORMAL
- en: From this *Q* table, we pick up actions in each state that have the maximum
    value as a new policy.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_189.png)'
  prefs: []
  type: TYPE_IMG
- en: Step 4 – Check the new policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now we will check if the extracted new policy from *step 3* is the same as the
    policy we used in *step 2*. If it is the same, then we stop, else we send the
    extracted new policy to *step 2* and repeat *steps 2* to *4*.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in this section, we learned how to compute the optimal policy using the
    policy iteration method. In the next section, we will learn how to implement the
    policy iteration method to compute the optimal policy in the Frozen Lake environment
    using the Gym toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the Frozen Lake problem with policy iteration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned that in the Frozen Lake environment, our goal is to reach the goal
    state **G** from the starting state **S** without visiting the hole states **H**.
    Now, let's learn how to compute the optimal policy using the policy iteration
    method in the Frozen Lake environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the Frozen Lake environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We learned that in the policy iteration, we compute the value function using
    the policy iteratively. Once we have found the optimal value function, then the
    policy that is used to compute the optimal value function will be the optimal
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: So, first, let's learn how to compute the value function using the policy.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the value function using the policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This step is exactly the same as how we computed the value function in the value
    iteration method but with a small difference. Here, we compute the value function
    using the policy but in the value iteration method, we compute the value function
    by taking the maximum over Q values. Now, let's learn how to define a function
    that computes the value function using the given policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define a function called `compute_value_function`, which takes the policy
    as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the threshold value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the discount factor ![](img/B15558_03_190.png) value to 1.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will initialize the value table by setting all the state values to
    zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the value table; that is, we learned that on every iteration, we use
    the updated value table (state values) from the previous iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute the value function using the given policy. We learned that
    a value function can be computed according to some policy ![](img/B15558_01_047.png)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_192.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, for each state, we select the action according to the policy and then
    we update the value of the state using the selected action as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action in the state according to the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the value of the state using the selected action,![](img/B15558_03_192.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'After computing the value table, that is, the value of all the states, we check
    whether the difference between the value table obtained in the current iteration
    and the previous iteration is less than or equal to a threshold value. If it is
    less, then we break the loop and return the value table as an accurate value function
    of the given policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have computed the value function of the policy, let's see how to
    extract the policy from the value function.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting the policy from the value function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This step is exactly the same as how we extracted the policy from the value
    function in the value iteration method. Thus, similar to what we learned in the
    value iteration method, we define a function called `extract_policy` to extract
    a policy given the value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Putting it all together
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'First, let''s define a function called `policy_iteration`, which takes the
    environment as a parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We learned that in the policy iteration method, we begin by initializing a
    random policy. So, we will initialize the random policy, which selects the action
    0 in all the states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'For every iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the value function using the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Extract the new policy from the computed value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'If `policy` and `new_policy` are the same, then break the loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Else update the current `policy` to `new_policy`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s learn how to perform policy iteration and find the optimal policy
    in the Frozen Lake environment. So, we just feed the Frozen Lake environment to
    our `policy_iteration` function as shown here and get the optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We can print the optimal policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: As we can observe, our optimal policy tells us to perform the correct action
    in each state. Thus, we learned how to perform the policy iteration method to
    compute the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Is DP applicable to all environments?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In dynamic programming, that is, in the value and policy iteration methods,
    we try to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: '**Value iteration**: In the value iteration method, we compute the optimal
    value function by taking the maximum over the Q function (Q values) iteratively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_088.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_03_123.png). After finding the optimal value function,
    we extract the optimal policy from it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy iteration**: In the policy iteration method, we compute the optimal
    value function using the policy iteratively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_03_192.png)'
  prefs: []
  type: TYPE_IMG
- en: We will start off with the random policy and compute the value function. Once
    we have found the optimal value function, then the policy that is used to create
    the optimal value function will be the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: If you look at the preceding two equations, in order to find the optimal policy,
    we compute the value function and Q function. But to compute the value and the
    Q function, we need to know the transition probability ![](img/B15558_03_175.png)
    of the environment, and when we don't know the transition probability of the environment,
    we cannot compute the value and the Q function in order to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: That is, dynamic programming is a model-based method and to apply this method,
    we need to know the model dynamics (transition probability) of the environment,
    and when we don't know the model dynamics, we cannot apply the dynamic programming
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we find the optimal policy when we don't know the model dynamics
    of the environment? In such a case, we can use model-free methods. In the next
    chapter, we will learn about one of the interesting model-free methods, called
    Monte Carlo, and how it is used to find the optimal policy without requiring the
    model dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding the Bellman equation of the value
    and Q functions. We learned that, according to the Bellman equation, the value
    of a state is the sum of the immediate reward, and the discounted value of the
    next state and the value of a state-action pair is the sum of the immediate reward
    and the discounted value of the next state-action pair. Then we learned about
    the optimal Bellman value function and the Q function, which gives the maximum
    value.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we learned about the relation between the value and Q functions.
    We learned that the value function can be extracted from the Q function as ![](img/B15558_03_088.png)
    and then we learned that the Q function can be extracted from the value function
    as ![](img/B15558_03_199.png).
  prefs: []
  type: TYPE_NORMAL
- en: Later we learned about two interesting methods called value iteration and policy
    iteration, which use dynamic programming to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: In the value iteration method, first, we compute the optimal value function
    by taking the maximum over the Q function iteratively. Once we have found the
    optimal value function, then we use it to extract the optimal policy. In the policy
    iteration method, we try to compute the optimal value function using the policy
    iteratively. Once we have found the optimal value function, then the policy that
    is used to create the optimal value function will be extracted as the optimal
    policy.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s try answering the following questions to assess our knowledge of what
    we learned in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the Bellman equation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between the Bellman expectation and Bellman optimality
    equations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we derive the value function from the Q function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we derive the Q function from the value function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the steps involved in value iteration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the steps involved in policy iteration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does policy iteration differ from value iteration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
