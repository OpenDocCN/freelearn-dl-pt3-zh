<html><head></head><body>
		<div id="_idContainer127">
			<h1 id="_idParaDest-294"><em class="italic"><a id="_idTextAnchor345"/>Chapter 11</em>: Streamlining Network Implementation with AutoML</h1>
			<p>Computer vision, particularly when combined with deep learning, is a field that's not suitable for the faint of heart! While in traditional computer programming, we have a limited set of options for debugging and experimentation, this is not the case in machine learning.</p>
			<p>Of course, the stochastic nature of machine learning itself plays a role in making the process of creating a good enough solution difficult, but so do the myriad of parameters, variables, knobs, and settings we need to get right to unlock the true power of a neural network for a particular problem. </p>
			<p>Selecting a proper architecture is just the beginning because we also need to consider preprocessing techniques, learning rates, optimizers, loss functions, and data splits, among a multiplicity of other factors.</p>
			<p>My point is that deep learning is hard! Where do you start? Wouldn't it be great if we had a way to ease the burden of searching through such an ample spectrum of combinations? </p>
			<p>Well, it exists! It's <a id="_idIndexMarker1092"/>called <strong class="bold">Automatic Machine Learning</strong> (<strong class="bold">AutoML</strong>), and in this chapter, we'll learn how to leverage one of the most promising tools in this field, built on<a id="_idIndexMarker1093"/> top of TensorFlow, known as <strong class="bold">AutoKeras</strong>. </p>
			<p>In this chapter, we are going to cover the following recipes:</p>
			<ul>
				<li>Creating a simple image classifier with AutoKeras</li>
				<li>Creating a simple image regressor with AutoKeras</li>
				<li>Exporting and importing a model in AutoKeras</li>
				<li>Controlling architecture generation with AutoKeras' AutoModel</li>
				<li>Predicting age and gender with AutoKeras</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-295"><a id="_idTextAnchor346"/>Technical requirements</h1>
			<p>One of the first things you'll notice is that <strong class="bold">AutoML</strong> is very resource-intensive, so accessing a <strong class="bold">GPU</strong> is a must if you want to replicate and extend the recipes we'll discuss in this chapter. Also, because we'll be using <strong class="bold">AutoKeras</strong> in all the examples provided, install it as follows:</p>
			<p class="source-code">$&gt; pip install git+https://github.com/keras-team/keras-tuner.git@1.0.2rc2 autokeras pydot graphviz</p>
			<p>The <strong class="bold">AutoKeras</strong> version we'll be using in this chapter only works with TensorFlow 2.3, so ensure you have it installed as well (if you prefer, you can create a different environment altogether). In the <em class="italic">Getting ready</em> section of each recipe, you'll find any preparatory information needed. As usual, the code shown in this chapter is available at <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch11">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch11</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/2Na6XRz">https://bit.ly/2Na6XRz</a>.</p>
			<h1 id="_idParaDest-296"><a id="_idTextAnchor347"/>Creating a simple image classifier with AutoKeras</h1>
			<p>Image<a id="_idIndexMarker1094"/> classification must be the de facto application<a id="_idIndexMarker1095"/> of neural networks for computer vision. However, as we know, depending on the complexity of the dataset, the availability of information, and countless other factors, the process of creating a proper image classifier can be quite cumbersome at times.</p>
			<p>In this recipe, we'll implement an image classifier effortlessly thanks to the magic of <strong class="bold">AutoML</strong>. Don't believe me? Let's begin and see for ourselves!</p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor348"/>How to do it…</h2>
			<p>By the end of this recipe, you'll have implemented an image classifier in a dozen lines of code or less! Let's get started:</p>
			<ol>
				<li>Import all the required modules:<p class="source-code">from autokeras import ImageClassifier</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist as fm</p><p>For the sake of simplicity, we'll use the well-known <strong class="source-inline">Fashion-MNIST</strong> dataset, a more challenging version of the famous <strong class="source-inline">MNIST</strong>.</p></li>
				<li>Load <a id="_idIndexMarker1096"/>the <a id="_idIndexMarker1097"/>train and test data:<p class="source-code">(X_train, y_train), (X_test, y_test) = fm.load_data()</p></li>
				<li>Normalize the images to the range [0, 1]: <p class="source-code">X_train = X_train.astype('float32') / 255.0</p><p class="source-code">X_test = X_test.astype('float32') / 255.0</p></li>
				<li>Define the number of epochs we'll allow each possible network (known as a trial) to train:<p class="source-code">EPOCHS = 10</p></li>
				<li>Here's where the magic happens. Define an instance of <strong class="source-inline">ImageClassifier()</strong>:<p class="source-code">classifier = ImageClassifier(seed=9, max_trials=10)</p><p>Notice that we are seeding the classifier with 9 and allowing it to find a suitable network 10 times. We're doing this so that the <strong class="bold">Neural Architecture Search</strong> (<strong class="bold">NAS</strong>) process <a id="_idIndexMarker1098"/>terminates in a reasonable amount of time (to learn more about <strong class="bold">NAS</strong>, please refer to the <em class="italic">See also</em> section).</p></li>
				<li>Fit the classifier on the test data over 10 epochs (per trial):<p class="source-code">classifier.fit(X_train, y_train, epochs=EPOCHS)</p></li>
				<li>Lastly, evaluate the best classifier on the test set and print out the accuracy:<p class="source-code">print(classifier.evaluate(X_test, y_test))</p><p>After a while (let's not forget the library is training 10 models with varying complexity), we should obtain an accuracy of 93%, give or take. That's not bad, considering we didn't even write 10 lines of code!</p></li>
			</ol>
			<p>We'll discuss what we've done a bit more in the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor349"/>How it works…</h2>
			<p>In this recipe, we created the most effortless image classifier ever! We delegated all major decisions to the <strong class="bold">AutoML</strong> tool, <strong class="bold">AutoKeras</strong>. From selecting an architecture, to which optimizer to use, all such decisions were made by the framework.</p>
			<p>You might have<a id="_idIndexMarker1099"/> noticed that we limited the search space<a id="_idIndexMarker1100"/> by specifying a maximum of 10 trials and 10 epochs per trial. We did this so that the program terminates in a reasonable amount of time, but as you might suspect, these parameters can also be trusted to <strong class="bold">AutoKeras</strong>.</p>
			<p>Despite all the autonomy <strong class="bold">AutoML</strong> has, we can guide the framework if we wish. What <strong class="bold">AutoML</strong> offers is, as its name suggests, a way to automate the search for a good enough combination for a particular problem. However, this doesn't mean that human expertise and prior knowledge is not necessary. In fact, it is often the case that a well-crafted network, typically the product of thoroughly studying the data, often performs better than one found by <strong class="bold">AutoML</strong> with no prior information whatsoever.</p>
			<p>In the end, <strong class="bold">AutoML</strong> is a tool, and as such, it should be used to enhance our mastery of deep learning, not to replace it – because it can't.</p>
			<h2 id="_idParaDest-299"><a id="_idTextAnchor350"/>See also</h2>
			<p>You can learn <a id="_idIndexMarker1101"/>more about <strong class="bold">NAS</strong> here: <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">https://en.wikipedia.org/wiki/Neural_architecture_search</a>.</p>
			<h1 id="_idParaDest-300"><a id="_idTextAnchor351"/>Creating a simple image regressor with AutoKeras</h1>
			<p>The power and <a id="_idIndexMarker1102"/>usefulness of <strong class="bold">AutoKeras</strong> is not <a id="_idIndexMarker1103"/>limited to image classification. Although not as popular, image regression is a similar problem where we want to predict a continuous quantity based on the spatial information in an image.</p>
			<p>In this recipe, we'll train an image regressor to predict people's ages while using <strong class="bold">AutoML</strong>.</p>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-301"><a id="_idTextAnchor352"/>Getting ready</h2>
			<p>We'll be using <strong class="source-inline">APPA-REAL</strong> dataset in this recipe, which contains 7,591 images labeled with the real and apparent ages for a wide range of subjects. You can read more about the dataset and download it from <a href="http://chalearnlap.cvc.uab.es/dataset/26/description/#">http://chalearnlap.cvc.uab.es/dataset/26/description/#</a>. Decompress the data in a directory of your preference. For the purposes of this recipe, we'll assume the dataset is located within the <strong class="source-inline">~/.keras/datasets/appa-real-release</strong> folder. </p>
			<p>Here are some sample images:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B14768_11_001.jpg" alt="Figure 11.1 – Sample images from the APPA-REAL dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 – Sample images from the APPA-REAL dataset</p>
			<p>Let's implement this recipe!</p>
			<h2 id="_idParaDest-302"><a id="_idTextAnchor353"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the modules we will be using:<p class="source-code">import csv</p><p class="source-code">import pathlib</p><p class="source-code">import numpy as np</p><p class="source-code">from autokeras import ImageRegressor</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Each subset (train, test, and validation) of the dataset is defined in a CSV file. There, among <a id="_idIndexMarker1104"/>many other columns, we <a id="_idIndexMarker1105"/>have the path to the image and the real age of the person depicted in a photo. In this step, we will define the <strong class="source-inline">load_mapping()</strong> function, which will create a map from the image paths to the labels that we'll use to load the actual data in memory:<p class="source-code">def load_mapping(csv_path, faces_path):</p><p class="source-code">    mapping = {}</p><p class="source-code">    with open(csv_path, 'r') as f:</p><p class="source-code">        reader = csv.DictReader(f)</p><p class="source-code">        for line in reader:</p><p class="source-code">            file_name = line["file_name"].rsplit(".")[0]</p><p class="source-code">           key = f'{faces_path}/{file_name}.jpg_face.jpg'</p><p class="source-code">            mapping[key] = int(line['real_age'])</p><p class="source-code">    return mapping</p></li>
				<li>Define the <strong class="source-inline">get_image_and_labels()</strong> function, which takes the mapping produced by the <strong class="source-inline">load_mapping()</strong> function and returns an array of images (normalized to the range [-1, 1]) and an array of the corresponding ages:<p class="source-code">def get_images_and_labels(mapping):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path, label in mapping.items():</p><p class="source-code">        try:</p><p class="source-code">            image = load_img(image_path, target_size=(64, </p><p class="source-code">                                                    64))</p><p class="source-code">            image = img_to_array(image)</p><p class="source-code">            images.append(image)</p><p class="source-code">            labels.append(label)</p><p class="source-code">        except FileNotFoundError:</p><p class="source-code">            continue</p><p class="source-code">    return (np.array(images) - 127.5) / 127.5, \</p><p class="source-code">           np.array(labels).astype('float32')</p><p>Notice that <a id="_idIndexMarker1106"/>each image has been resized <a id="_idIndexMarker1107"/>so that its dimensions are 64x64x3. This is necessary because the images in the dataset don't have homogeneous dimensions.</p></li>
				<li>Define the paths to the CSV files to create the data mappings for each subset:<p class="source-code">base_path = (pathlib.Path.home() / '.keras' / 'datasets' </p><p class="source-code">             /'appa-real-release')</p><p class="source-code">train_csv_path = str(base_path / 'gt_train.csv')</p><p class="source-code">test_csv_path = str(base_path / 'gt_test.csv')</p><p class="source-code">val_csv_path = str(base_path / 'gt_valid.csv')</p></li>
				<li>Define the paths to the directories where the images for each subset live:<p class="source-code">train_faces_path = str(base_path / 'train')</p><p class="source-code">test_faces_path = str(base_path / 'test')</p><p class="source-code">val_faces_path = str(base_path / 'valid')</p></li>
				<li>Create <a id="_idIndexMarker1108"/>the<a id="_idIndexMarker1109"/> mappings for each subset:<p class="source-code">train_mapping = load_mapping(train_csv_path, </p><p class="source-code">                            train_faces_path)</p><p class="source-code">test_mapping = load_mapping(test_csv_path, </p><p class="source-code">                           test_faces_path)</p><p class="source-code">val_mapping = load_mapping(val_csv_path, </p><p class="source-code">                           val_faces_path)</p></li>
				<li>Get the images and labels for each subset:<p class="source-code">X_train, y_train = get_images_and_labels(train_mapping)</p><p class="source-code">X_test, y_test = get_images_and_labels(test_mapping)</p><p class="source-code">X_val, y_val = get_images_and_labels(val_mapping)</p></li>
				<li>We'll train each network in a trial for a maximum of 15 epochs:<p class="source-code">EPOCHS = 15</p></li>
				<li>We instantiate an <strong class="source-inline">ImageRegressor()</strong> object, which encapsulates the <strong class="bold">AutoML</strong> logic that searches for the best regressor. It will perform 10 trials, and for the sake of reproducibility, we'll seed it with 9. Notice that we are explicitly telling <strong class="bold">AutoKeras</strong> to use <strong class="source-inline">adam</strong> as the optimizer:<p class="source-code">regressor = ImageRegressor(seed=9,</p><p class="source-code">                           max_trials=10,</p><p class="source-code">                           optimizer='adam')</p></li>
				<li>Fit the regressor. Notice that we are passing our own validation set. If we don't do this, <strong class="bold">AutoKeras</strong> takes 20% of the training data to validate its experiments by default:<p class="source-code">regressor.fit(X_train, y_train,</p><p class="source-code">              epochs=EPOCHS,</p><p class="source-code">              validation_data=(X_val, y_val))</p></li>
				<li>Finally, we must evaluate the best regressor on the test data and print its performance metric:<p class="source-code">print(regressor.evaluate(X_test, y_test))</p><p>After a while, we<a id="_idIndexMarker1110"/> should obtain a test loss<a id="_idIndexMarker1111"/> of 241.248, which is not bad if we take into account that the bulk of our work consisted of loading the dataset.</p></li>
			</ol>
			<p>Let's move on to the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-303"><a id="_idTextAnchor354"/>How it works…</h2>
			<p>In this recipe, we delegated the creation of a model to an <strong class="bold">AutoML</strong> framework, similar to what we did in the <em class="italic">Creating a simple image classifier with AutoKeras</em> recipe. However, this time, our goal was to solve a regression problem, namely predicting the age of a person based on a photo of their face, instead of a classification one.</p>
			<p>This time, because we used a real-world dataset, we had to implement several helper functions to load the data and make it the proper shape for <strong class="bold">AutoKeras</strong> to use it. However, after doing this, we let the framework take the wheel, leveraging its built-in <strong class="bold">NAS</strong> algorithm to find the best possible model in a span of 15 iterations.</p>
			<p>We obtained a respectable 241.248 loss on the test set. Predicting the age of a person is not an easy task, even though it might appear that it is at first. I invite you to take a closer look at the <em class="italic">APPA-REAL</em> CSV files so that you can see the deviation in the human estimates of people's ages!</p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor355"/>See also</h2>
			<p>You can learn more about <strong class="bold">NAS</strong> here: <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">https://en.wikipedia.org/wiki/Neural_architecture_search</a>.</p>
			<h1 id="_idParaDest-305"><a id="_idTextAnchor356"/>Exporting and importing a model in AutoKeras</h1>
			<p>One worry<a id="_idIndexMarker1112"/> we might have when working with <strong class="bold">AutoML</strong> is the <a id="_idIndexMarker1113"/>black-box nature of the tools available. Do we<a id="_idIndexMarker1114"/> have control over the produced models? Can we <a id="_idIndexMarker1115"/>extend them? Understand them? Reuse them?</p>
			<p>Of course we can! The good thing about <strong class="bold">AutoKeras</strong> is that it is built on top of TensorFlow, so despite its sophistication, under the hood, the models being trained are just TensorFlow graphs that we can export and tweak and tune later if we need to. </p>
			<p>In this recipe, we'll learn how to export a model trained on <strong class="bold">AutoKeras</strong>, and then import it as a plain old TensorFlow network.</p>
			<p>Are you ready? Let's begin.</p>
			<h2 id="_idParaDest-306"><a id="_idTextAnchor357"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the necessary dependencies:<p class="source-code">from autokeras import *</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist as fm</p><p class="source-code">from tensorflow.keras.models import load_model</p><p class="source-code">from tensorflow.keras.utils import plot_model</p></li>
				<li>Load the train and test splits of the <strong class="source-inline">Fashion-MNIST</strong> dataset:<p class="source-code">(X_train, y_train), (X_test, y_test) = fm.load_data()</p></li>
				<li>Normalize the data to the [0, 1] interval:<p class="source-code">X_train = X_train.astype('float32') / 255.0</p><p class="source-code">X_test = X_test.astype('float32') / 255.0</p></li>
				<li>Define the number of epochs we'll train each network for:<p class="source-code">EPOCHS = 10</p></li>
				<li>Create an <strong class="source-inline">ImageClassifier()</strong> that'll try to find to best possible classifier, over 20 trials, with <a id="_idIndexMarker1116"/>each one trained for 10 epochs. We <a id="_idIndexMarker1117"/>will <a id="_idIndexMarker1118"/>instruct <strong class="bold">AutoKeras</strong> to use <strong class="source-inline">adam</strong> as <a id="_idIndexMarker1119"/>the optimizer and seed <strong class="source-inline">ImageClassifier()</strong> for the sake of reproducibility:<p class="source-code">classifier = ImageClassifier(seed=9,</p><p class="source-code">                             max_trials=20,</p><p class="source-code">                             optimizer='adam')</p></li>
				<li>Fit the classifier. We'll allow <strong class="bold">AutoKeras</strong> to automatically pick 20% of the training data for validation:<p class="source-code">classifier.fit(X_train, y_train, epochs=EPOCHS)</p></li>
				<li>Export the best model and save it to disk:<p class="source-code">model = classifier.export_model()</p><p class="source-code">model.save('model.h5')</p></li>
				<li>Load the model back into memory:<p class="source-code">model = load_model('model.h5',</p><p class="source-code">                   custom_objects=CUSTOM_OBJECTS)</p></li>
				<li>Evaluate the training model on the test set:<p class="source-code">print(classifier.evaluate(X_test, y_test))</p></li>
				<li>Print a text summary of the best model:<p class="source-code">print(model.summary())</p></li>
				<li>Lastly, generate a graph of the architecture of the best model found by <strong class="bold">AutoKeras</strong>:<p class="source-code">plot_model(model,</p><p class="source-code">           show_shapes=True,</p><p class="source-code">           show_layer_names=True,</p><p class="source-code">           to_file='model.png')</p><p>After 20 trials, the best model that was created by <strong class="bold">AutoKeras</strong> achieves 91.5% accuracy<a id="_idIndexMarker1120"/> on <a id="_idIndexMarker1121"/>the<a id="_idIndexMarker1122"/> test set. The following screenshot shows<a id="_idIndexMarker1123"/> the model's summary:</p></li>
			</ol>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B14768_11_002.jpg" alt="Figure 11.2 – AutoKeras' best model summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 – AutoKeras' best model summary</p>
			<p>The following diagram shows the model's architecture:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B14768_11_003.jpg" alt="Figure 11.3 – AutoKeras' best model architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 – AutoKeras' best model architecture</p>
			<p>In <em class="italic">Figure 11.2</em>, we can<a id="_idIndexMarker1124"/> see the network <strong class="bold">AutoKeras</strong> deemed <a id="_idIndexMarker1125"/>the most suitable for <strong class="bold">Fashion-MNIST</strong>, at <a id="_idIndexMarker1126"/>least within the bounds we<a id="_idIndexMarker1127"/> established. You can take a closer look at the full architecture in the companion GitHub repository.</p>
			<p>Let's move on to the next section.</p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor358"/>How it works…</h2>
			<p>In this recipe, we demonstrated that <strong class="bold">AutoML</strong> can work as a great starting point when we're tackling a new computer vision problem. How? We can use it to produce well-performing models out of the gate, which we can then extend based on our domain knowledge of the dataset at hand.</p>
			<p>The formula to do this is straightforward: let <strong class="bold">AutoML</strong> do the grunt work for a while; then, export the best network and import it into the confines of TensorFlow so that you can build your <a id="_idIndexMarker1128"/>solution <a id="_idIndexMarker1129"/>on <a id="_idIndexMarker1130"/>top<a id="_idIndexMarker1131"/> of it.</p>
			<p>This not only showcases the usability of tools such as <strong class="bold">AutoKeras</strong>, but allows us to peak behind the curtain, understanding the building blocks of the models engendered by <strong class="bold">NAS</strong>.</p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor359"/>See also</h2>
			<p>The basis of <strong class="bold">AutoKeras</strong> is <strong class="bold">NAS</strong>. You can read more about it here (it's pretty interesting!): <a href="https://en.wikipedia.org/wiki/Neural_architecture_search">https://en.wikipedia.org/wiki/Neural_architecture_search</a>.</p>
			<h1 id="_idParaDest-309">Controlling architecture <a id="_idTextAnchor360"/><a id="_idTextAnchor361"/>generation with AutoKeras' AutoModel</h1>
			<p>Letting <strong class="bold">AutoKeras</strong> automagically<a id="_idIndexMarker1132"/> figure out what architecture works best is great, but it can be time-consuming – unacceptably so at times.</p>
			<p>Can we exert more control? Can we hint at which options work best for our particular problem? Can we meet <strong class="bold">AutoML</strong> halfway by providing a set of guidelines it must follow according to our prior knowledge or preference, but still give it enough leeway to experiment?</p>
			<p>Yes, we can, and in this recipe, you'll learn how by utilizing a special feature in <strong class="bold">AutoKeras</strong> known as AutoModel! </p>
			<h2 id="_idParaDest-310"><a id="_idTextAnchor362"/>How to do it…</h2>
			<p>Follow these steps to learn how to customize the search space of the <strong class="bold">NAS</strong> algorithm with <strong class="source-inline">AutoModel</strong>:</p>
			<ol>
				<li value="1">The first thing we need to do is import all the necessary dependencies:<p class="source-code">from autokeras import *</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist as fm</p><p class="source-code">from tensorflow.keras.models import load_model</p><p class="source-code">from tensorflow.keras.utils import *</p></li>
				<li>Because<a id="_idIndexMarker1133"/> we'll be training our customized model on <strong class="source-inline">Fashion-MNIST</strong>, we must load the train and test splits, respectively:<p class="source-code">(X_train, y_train), (X_test, y_test) = fm.load_data()</p></li>
				<li>To avoid numerical instability issues, let's normalize the images of both splits so that they're in the range [0, 1]:<p class="source-code">X_train = X_train.astype('float32')</p><p class="source-code">X_test = X_test.astype('float32')</p></li>
				<li>Define the <strong class="source-inline">create_automodel()</strong> function, which defines the custom search space of the underlying <strong class="bold">NAS</strong> algorithm as a series of blocks arranged in a graph structure. Each <strong class="source-inline">Block</strong> is in charge of a defined task, such as image augmentation, normalization, image processing, or classification. First, we must define the input block, which will be normalized and augmented through the <strong class="source-inline">Normalization()</strong> and <strong class="source-inline">ImageAugmentation()</strong> blocks, respectively:<p class="source-code">def create_automodel(max_trials=10):</p><p class="source-code">    input = ImageInput()</p><p class="source-code">    x = Normalization()(input)</p><p class="source-code">    x = ImageAugmentation(horizontal_flip=False,</p><p class="source-code">                          vertical_flip=False)(x)</p><p>Notice that we disabled horizontal and vertical flipping in the <strong class="source-inline">ImageAugmentation()</strong> block. This is because these operations alter the class of images in <strong class="source-inline">Fashion-MNIST</strong>.</p></li>
				<li>Now, we'll bifurcate the graph. The left branch searches for vanilla convolutional layers, thanks to <strong class="source-inline">ConvBlock()</strong>. On the right branch, we'll explore more sophisticated Xception-like architectures (for more information about<a id="_idIndexMarker1134"/> the <strong class="bold">Xception</strong> architecture, refer to the <em class="italic">See also</em> section): <p class="source-code">    left = ConvBlock()(x)</p><p class="source-code">    right = XceptionBlock(pretrained=True)(x)</p><p>In the previous snippet, we instructed <strong class="bold">AutoKeras</strong> to only explore <strong class="bold">Xception</strong> architectures pre-trained on ImageNet. </p></li>
				<li>We'll merge the left and right branches, flatten them, and pass the result through a <strong class="source-inline">DenseBlock()</strong>, which, as its name suggests, searches for fully connected combinations of layers:<p class="source-code">    x = Merge()([left, right])</p><p class="source-code">    x = SpatialReduction(reduction_type='flatten')(x)</p><p class="source-code">    x = DenseBlock()(x)</p></li>
				<li>The output of this graph will be a <strong class="source-inline">ClassificationHead()</strong>. This is because we're dealing with a classification problem. Notice that we don't specify the number of classes. This is because <strong class="bold">AutoKeras</strong> infers this information from the data:<p class="source-code">    output = ClassificationHead()(x)</p></li>
				<li>We can close <strong class="source-inline">create_automodel()</strong> by building and returning an <strong class="source-inline">AutoModel()</strong> instance. We must specify the inputs and outputs, as well as the maximum number of trials to perform:<p class="source-code">    return AutoModel(inputs=input,</p><p class="source-code">                     outputs=output,</p><p class="source-code">                     overwrite=True,</p><p class="source-code">                     max_trials=max_trials)</p></li>
				<li>Let's train each trial model for 10 epochs:<p class="source-code">EPOCHS = 10</p></li>
				<li>Create the <strong class="source-inline">AutoModel</strong> and fit it: <p class="source-code">model = create_automodel()</p><p class="source-code">model.fit(X_train, y_train, epochs=EPOCHS)</p></li>
				<li>Let's <a id="_idIndexMarker1135"/>export the best model:<p class="source-code">model = model.export_model()</p></li>
				<li>Evaluate the model on the test set:<p class="source-code">print(model.evaluate(X_test, to_categorical(y_test)))</p></li>
				<li>Plot the architecture of the best model:<p class="source-code">plot_model(model,</p><p class="source-code">           show_shapes=True,</p><p class="source-code">           show_layer_names=True,</p><p class="source-code">           to_file='automodel.png')</p><p>The final architecture I obtained achieved 90% accuracy on the test set, although your results may vary. What's even more interesting is the structure of the generated model:</p></li>
			</ol>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B14768_11_004.jpg" alt="Figure 11.4 – AutoKeras' best model architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 – AutoKeras' best model architecture</p>
			<p>The preceding<a id="_idIndexMarker1136"/> diagram reveals that <strong class="source-inline">AutoModel</strong> produced a network according to the blueprint we laid out in <strong class="source-inline">create_automodel()</strong>.</p>
			<p>Now, let's move on to the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-311"><a id="_idTextAnchor363"/>How it works…</h2>
			<p>In this recipe, we took advantage of <strong class="bold">AutoKeras'</strong> <strong class="source-inline">AutoModel</strong> module to trim down the search space. This is a very useful feature when we have an idea of what our final model should look like. This leads to huge time gains because we don't allow <strong class="bold">AutoKeras</strong> to waste time trying out unfruitful, useless combinations. One example of such bad combinations can be seen in <em class="italic">Step 4</em>, where we told <strong class="bold">AutoKeras</strong> not to try to flip images as part of its image augmentation scheme. This is because, due to the characteristics of our problem, this operation changes the classes of the numbers in <strong class="bold">Fashion-MNIST</strong>.</p>
			<p>Proof that we steered <strong class="bold">AutoKeras</strong> down the path we wanted is in the architecture of the final model, where we had layers that correspond to each of the blocks specified in the search <a id="_idIndexMarker1137"/>graph defined in the <strong class="source-inline">create_automodel()</strong> function.</p>
			<p>Impressive, right?</p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor364"/>See also</h2>
			<p>One thing we didn't do here is implement our own <strong class="source-inline">Block</strong>, which is possible in <strong class="bold">AutoKeras</strong>. Why don't you give it a try? You can start by reading the docs here: <a href="https://autokeras.com/tutorial/customized/">https://autokeras.com/tutorial/customized/</a>. For a list of all available blocks, go to <a href="https://autokeras.com/block/">https://autokeras.com/block/</a>. In this recipe, we used Xception-like layers. To find out more about Xception, you can read the original paper: <a href="https://arxiv.org/abs/1610.02357">https://arxiv.org/abs/1610.02357</a>. </p>
			<h1 id="_idParaDest-313"><a id="_idTextAnchor365"/>Predicting age and gender with AutoKeras</h1>
			<p>In this recipe, we'll <a id="_idIndexMarker1138"/>study a practical application of AutoML that can be used as a template to create prototypes, MVPs, or just to tackle real-world applications with the help of AutoML.</p>
			<p>More concretely, we'll create an age and gender classification program with a twist: the architecture of both the gender and age classifiers will be the responsibility of <strong class="bold">AutoKeras</strong>. We'll be in charge of getting and shaping the data, as well as creating the framework to test the solution on our own images.</p>
			<p>I hope you're ready because we are about to begin!</p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor366"/>Getting ready</h2>
			<p>We need a couple of external libraries, such as OpenCV, <strong class="source-inline">scikit-learn</strong>, and <strong class="source-inline">imutils</strong>. All these dependencies can be installed at once, as follows:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python scikit-learn imutils</p>
			<p>On the data side, we'll use the <strong class="bold">Adience</strong> dataset, which contains 26,580 images of 2,284 subjects, along with their gender and age. To download the data, go to <a href="https://talhassner.github.io/home/projects/Adience/Adience-data.html">https://talhassner.github.io/home/projects/Adience/Adience-data.html</a>. </p>
			<p>Next, you'll need to navigate to the <strong class="bold">Download</strong> section and enter your name and email, as shown in the following screenshot: </p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B14768_11_005.jpg" alt="Figure 11.5 – Enter your information to receive the credentials of the FTP server where the data is&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5 – Enter your information to receive the credentials of the FTP server where the data is</p>
			<p>Once you hit <a id="_idIndexMarker1139"/>the <strong class="bold">Submit</strong> button, you'll get the credentials required for the FTP server where the data is located. You can access this here: <a href="http://www.cslab.openu.ac.il/download/">http://www.cslab.openu.ac.il/download/</a>.</p>
			<p>Make sure that you click on the first link, labeled <strong class="bold">Adience OUI Unfiltered faces for gender and age classification</strong>: </p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B14768_11_006.jpg" alt="Figure 11.6 – Going to the highlighted link&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.6 – Going to the highlighted link</p>
			<p>Enter the <a id="_idIndexMarker1140"/>credentials you received previously and access the second link, named <strong class="bold">AdienceBenchmarkOfUnfilteredFacesForGenderAndAgeClassification</strong>:</p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B14768_11_007.jpg" alt="Figure 11.7 – Clicking the highlighted link&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.7 – Clicking the highlighted link</p>
			<p>Finally, download <strong class="source-inline">aligned.tar.gz</strong>, <strong class="source-inline">fold_frontal_0_data.txt</strong>, <strong class="source-inline">fold_frontal_1_data.txt</strong>, <strong class="source-inline">fold_frontal_2_data.txt</strong>, <strong class="source-inline">fold_frontal_3_data.txt</strong>, <strong class="source-inline">and fold_frontal_4_data.txt</strong>:</p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B14768_11_008.jpg" alt="Figure 11.8 – Downloading aligned.tar.gz and all the fold_frontal_*_data.txt files&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.8 – Downloading aligned.tar.gz and all the fold_frontal_*_data.txt files</p>
			<p>Unzip <strong class="source-inline">aligned.tar.gz</strong> into a directory of your preference as <strong class="source-inline">adience</strong>. Inside that directory, create a subdirectory named <strong class="source-inline">folds</strong>, and move all the <strong class="source-inline">fold_frontal_*_data.txt</strong> files inside it. For the purposes of this recipe, we'll assume the dataset is located within <strong class="source-inline">~/.keras/datasets/adience</strong>. </p>
			<p>Here are some sample images:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B14768_11_009.jpg" alt="Figure 11.9 – Sample images from the Adience dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.9 – Sample images from the Adience dataset</p>
			<p>Let's<a id="_idIndexMarker1141"/> implement this recipe!</p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor367"/>How to do it…</h2>
			<p>Complete these steps to implement an age and gender classifier using <strong class="bold">AutoML</strong>:</p>
			<ol>
				<li value="1">The first thing we need to do is import all the necessary dependencies:<p class="source-code">import csv</p><p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import cv2</p><p class="source-code">import imutils</p><p class="source-code">import numpy as np</p><p class="source-code">from autokeras import *</p><p class="source-code">from sklearn.preprocessing import LabelEncoder</p><p class="source-code">from tensorflow.keras.models import load_model</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define the base path to the <strong class="source-inline">Adience</strong> dataset, as well as the folds (which contain the relationships between the images and the ages and genders of their subjects, in CSV format):<p class="source-code">base_path = (pathlib.Path.home() / '.keras' / 'datasets' </p><p class="source-code">                 /'adience')</p><p class="source-code">folds_path = str(base_path / 'folds')</p></li>
				<li>The ages<a id="_idIndexMarker1142"/> in <strong class="source-inline">Adience</strong> are expressed as intervals, groups, or brackets. Here, we will define an array that we will use to map the reported age in the folds to the correct bracket:<p class="source-code">AGE_BINS = [(0, 2), (4, 6), (8, 13), (15, 20), (25, 32), </p><p class="source-code">            (38, 43), (48, 53), (60, 99)]</p></li>
				<li>Define the <strong class="source-inline">age_to_bin()</strong> function, which takes an input as it appears in a fold CSV row and maps it to the corresponding bin. For instance, if the input is <strong class="source-inline">(27, 29)</strong>, the output will be <strong class="source-inline">25_32</strong>:<p class="source-code">def age_to_bin(age):</p><p class="source-code">    age = age.replace('(', '').replace(')', '').</p><p class="source-code">                                    split(',')</p><p class="source-code">    lower, upper = [int(x.strip()) for x in age]</p><p class="source-code">    for bin_low, bin_up in AGE_BINS:</p><p class="source-code">        if lower &gt;= bin_low and upper &lt;= bin_up:</p><p class="source-code">            label = f'{bin_low}_{bin_up}'</p><p class="source-code">            return label</p></li>
				<li>Define a function that will compute the area of a rectangle. We'll use this later to get the largest face detection possible: <p class="source-code">def rectangle_area(r):</p><p class="source-code">    return (r[2] - r[0]) * (r[3] - r[1])</p></li>
				<li>We'll also draw a bounding box around the detected face, captioned with the recognized <a id="_idIndexMarker1143"/>age and gender: <p class="source-code">def plot_face(image, age_gender, detection):</p><p class="source-code">    frame_x, frame_y, frame_width, frame_height = detection</p><p class="source-code">    cv2.rectangle(image,</p><p class="source-code">                  (frame_x, frame_y),</p><p class="source-code">                  (frame_x + frame_width,</p><p class="source-code">                   frame_y + frame_height),</p><p class="source-code">                  color=(0, 255, 0),</p><p class="source-code">                  thickness=2)</p><p class="source-code">    cv2.putText(image,</p><p class="source-code">                age_gender,</p><p class="source-code">                (frame_x, frame_y - 10),</p><p class="source-code">                fontFace=cv2.FONT_HERSHEY_SIMPLEX,</p><p class="source-code">                fontScale=0.45,</p><p class="source-code">                color=(0, 255, 0),</p><p class="source-code">                thickness=2)</p><p class="source-code">    return image</p></li>
				<li>Define the <strong class="source-inline">predict()</strong> function, which we'll use to predict both the age and gender (depending on <strong class="source-inline">model</strong>) of a person whose face was passed into the <strong class="source-inline">roi</strong> parameter:<p class="source-code">def predict(model, roi):</p><p class="source-code">    roi = cv2.resize(roi, (64, 64))</p><p class="source-code">    roi = roi.astype('float32') / 255.0</p><p class="source-code">    roi = img_to_array(roi)</p><p class="source-code">    roi = np.expand_dims(roi, axis=0)</p><p class="source-code">    predictions = model.predict(roi)[0]</p><p class="source-code">    return predictions</p></li>
				<li>Define the<a id="_idIndexMarker1144"/> lists where we'll store all the images, ages, and genders of the dataset:<p class="source-code">images = []</p><p class="source-code">ages = []</p><p class="source-code">genders = []</p></li>
				<li>Iterate over each fold file. These will be in CSV format:<p class="source-code">folds_pattern = os.path.sep.join([folds_path, '*.txt'])</p><p class="source-code">for fold_path in glob(folds_pattern):</p><p class="source-code">    with open(fold_path, 'r') as f:</p><p class="source-code">        reader = csv.DictReader(f, delimiter='\t')</p></li>
				<li>If the age or gender fields are not well-defined, skip the current line:<p class="source-code">        for line in reader:</p><p class="source-code">            if ((line['age'][0] != '(') or</p><p class="source-code">                    (line['gender'] not in {'m', 'f'})):</p><p class="source-code">                Continue</p></li>
				<li>Map the age to a valid bin. If we get <strong class="source-inline">None</strong> from <strong class="source-inline">age_to_bin()</strong>, this means the age doesn't <a id="_idIndexMarker1145"/>correspond to any of our defined categories, so we must skip this record:<p class="source-code">            age_label = age_to_bin(line['age'])</p><p class="source-code">            if age_label is None:</p><p class="source-code">                continue</p></li>
				<li>Load the image:<p class="source-code">            aligned_face_file = </p><p class="source-code">                           (f'landmark_aligned_face.'</p><p class="source-code">                                 f'{line["face_id"]}.'</p><p class="source-code">                          f'{line["original_image"]}')</p><p class="source-code">            image_path = os.path.sep.join(</p><p class="source-code">                             [str(base_path),</p><p class="source-code">                             line["user_id"],</p><p class="source-code">                           aligned_face_file])</p><p class="source-code">            image = load_img(image_path, </p><p class="source-code">                             target_size=(64, 64))</p><p class="source-code">            image = img_to_array(image)</p></li>
				<li>Append the image, age, and gender to the corresponding collections:<p class="source-code">            images.append(image)</p><p class="source-code">            ages.append(age_label)</p><p class="source-code">            genders.append(line['gender'])</p></li>
				<li>Create two copies of the images, one for each problem (age classification and gender prediction): <p class="source-code">age_images = np.array(images).astype('float32') / 255.0</p><p class="source-code">gender_images = np.copy(images)</p></li>
				<li>Encode the age and genders:<p class="source-code">gender_enc = LabelEncoder()</p><p class="source-code">age_enc = LabelEncoder()</p><p class="source-code">gender_labels = gender_enc.fit_transform(genders)</p><p class="source-code">age_labels = age_enc.fit_transform(ages)</p></li>
				<li>Define the <a id="_idIndexMarker1146"/>number of trials and epochs per trial. These parameters affect both models:<p class="source-code">EPOCHS = 100</p><p class="source-code">MAX_TRIALS = 10</p></li>
				<li>If there's a trained version of the age classifier, load it; otherwise, train an <strong class="source-inline">ImageClassifier()</strong> from scratch and save it to disk:<p class="source-code">if os.path.exists('age_model.h5'):</p><p class="source-code">    age_model = load_model('age_model.h5')</p><p class="source-code">else:</p><p class="source-code">    age_clf = ImageClassifier(seed=9,</p><p class="source-code">                              max_trials=MAX_TRIALS,</p><p class="source-code">                              project_name='age_clf',</p><p class="source-code">                              overwrite=True)</p><p class="source-code">    age_clf.fit(age_images, age_labels, epochs=EPOCHS)</p><p class="source-code">    age_model = age_clf.export_model()</p><p class="source-code">    age_model.save('age_model.h5')</p></li>
				<li>If there's a trained version of the gender classifier, load it; otherwise, train an <strong class="source-inline">ImageClassifier()</strong> from scratch and save it to disk:<p class="source-code">if os.path.exists('gender_model.h5'):</p><p class="source-code">    gender_model = load_model('gender_model.h5')</p><p class="source-code">else:</p><p class="source-code">    gender_clf = ImageClassifier(seed=9,</p><p class="source-code">                                 </p><p class="source-code">                               max_trials=MAX_TRIALS,</p><p class="source-code">                            project_name='gender_clf',</p><p class="source-code">                                 overwrite=True)</p><p class="source-code">    gender_clf.fit(gender_images, gender_labels,</p><p class="source-code">                   epochs=EPOCHS)</p><p class="source-code">    gender_model = gender_clf.export_model()</p><p class="source-code">    gender_model.save('gender_model.h5')</p></li>
				<li>Read a <a id="_idIndexMarker1147"/>test image from disk:<p class="source-code">image = cv2.imread('woman.jpg')</p></li>
				<li>Create a <strong class="bold">Haar Cascades</strong> face <a id="_idIndexMarker1148"/>detector. (This is a topic outside the scope of this book. If you want to learn more about Haar Cascades, go to the <em class="italic">See also</em> section of this recipe.) Use the following code to do so:<p class="source-code">cascade_file = 'resources/haarcascade_frontalface_default.xml'</p><p class="source-code">det = cv2.CascadeClassifier(cascade_file)</p></li>
				<li>Resize the image so that it is 380 pixels wide. Thanks to the <strong class="source-inline">imutils.resize()</strong> function, we can rest assured that the result will preserve the aspect ratio. This is because the function computes the height automatically to guarantee this condition:<p class="source-code">image = imutils.resize(image, width=380)</p></li>
				<li>Create a copy of the original image so that we can draw the detections on it:<p class="source-code">copy = image.copy()</p></li>
				<li>Convert the<a id="_idIndexMarker1149"/> image into grayscale and pass it through the face detector:<p class="source-code">gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</p><p class="source-code">detections = \</p><p class="source-code">    det.detectMultiScale(gray,</p><p class="source-code">                         scaleFactor=1.1,</p><p class="source-code">                         minNeighbors=5,</p><p class="source-code">                         minSize=(35, 35),</p><p class="source-code">                      flags=cv2.CASCADE_SCALE_IMAGE)</p></li>
				<li>Verify whether there are detections and fetch the one with the largest area:<p class="source-code">if len(detections) &gt; 0:</p><p class="source-code">    detections = sorted(detections, key=rectangle_area)</p><p class="source-code">    best_detection = detections[-1]</p></li>
				<li>Extract the region of interest (<strong class="source-inline">roi</strong>) corresponding to the detected face and extract its age and gender:<p class="source-code">    (frame_x, frame_y,</p><p class="source-code">     frame_width, frame_height) = best_detection</p><p class="source-code">    roi = image[frame_y:frame_y + frame_height,</p><p class="source-code">                frame_x:frame_x + frame_width]</p><p class="source-code">    age_pred = predict(age_model, roi).argmax()</p><p class="source-code">    age = age_enc.inverse_transform([age_pred])[0]</p><p class="source-code">    gender_pred = predict(gender_model, roi).argmax()</p><p class="source-code">    gender = gender_enc.inverse_transform([gender_pred])[0]</p><p>Notice that we use each encoder to revert back to a human-readable label for both the <a id="_idIndexMarker1150"/>predicted age and gender.</p></li>
				<li>Plot the predicted age and gender on the original image and show the result:<p class="source-code">    clone = plot_face(copy,</p><p class="source-code">                      f'Gender: {gender} - Age: </p><p class="source-code">                       {age}',</p><p class="source-code">                      best_detection)</p><p class="source-code">    cv2.imshow('Result', copy)</p><p class="source-code">    cv2.waitKey(0)</p><p class="callout-heading">Important note</p><p class="callout">The first time you execute this script, you'll have to wait a very long time – probably more than 24 hours (depending on your hardware). This is because each model is trained for a high number of trials and epochs. However, subsequent runs should be way faster because the program will load the trained classifiers.</p><p>We can see an example of a successful prediction of both age and gender in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B14768_11_010.jpg" alt="Figure 11.10 – Our models state the person in the photo is female and is between 25 and 32 years of age. Seems about right, doesn't it?"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.10 – Our models state the person in the photo is female and is between 25 and 32 years of age. Seems about right, doesn't it?</p>
			<p>Isn't it truly <a id="_idIndexMarker1151"/>amazing how the heavy lifting was done by <strong class="bold">AutoKeras</strong>? We're living in the future!</p>
			<h2 id="_idParaDest-316"><a id="_idTextAnchor368"/>How it works…</h2>
			<p>In this recipe, we implemented a practical solution to a surprisingly challenging problem: age and gender prediction. </p>
			<p>Why is this challenging? The apparent age of a person can vary, depending on multiple factors, such as ethnicity, gender, health, and other life conditions. We humans are not as great as we think we are at estimating the age of a man or a woman based solely on their physical features.</p>
			<p>For instance, a mostly healthy 25-year-old person will look vastly different than another 25-year-old that's a heavy drinker and smoker.</p>
			<p>Either way, we trusted the power of <strong class="bold">AutoML</strong> to find two models: one for gender classification and another for age prediction. We must highlight that, in this case, we framed age prediction as a classification problem instead of a regression one. This is because it makes it a bit easier to select an age range instead of producing a precise quantity.</p>
			<p>After a long wait (we trained both models over 100 epochs per trial), we obtained two competent networks that we integrated into a framework that automatically detects a face in a photo, and using these models, tags them with the predicted age and gender.</p>
			<p>As you may have noticed, we relied on <strong class="source-inline">ImageClassifier()</strong>, which means we gave 100% control of the<a id="_idIndexMarker1152"/> network creation process to <strong class="bold">AutoKeras</strong>. An interesting extension is to use <strong class="source-inline">AutoModel</strong> to narrow down the search space, therefore arriving at potentially better solutions at a fraction of the time. Why don't you give it a try?</p>
			<h2 id="_idParaDest-317"><a id="_idTextAnchor369"/>See also</h2>
			<p>Read the following paper to learn how the authors of the <strong class="bold">Adience</strong> dataset solve this problem: <a href="https://talhassner.github.io/home/projects/cnn_agegender/CVPR2015_CNN_AgeGenderEstimation.pdf">https://talhassner.github.io/home/projects/cnn_agegender/CVPR2015_CNN_AgeGenderEstimation.pdf</a>. To learn<a id="_idIndexMarker1153"/> more about the Haar Cascade classifier we used previously, read this tutorial: <a href="https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html">https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html</a>.</p>
		</div>
	</body></html>