["```\nimport keras \n```", "```\nimport tensorflow as tf\nfrom tensorflow import keras \n```", "```\n    layer.get_weights() \n    ```", "```\n    layer.set_weights(weights) \n    ```", "```\n    layer.input\n    layer.output \n    ```", "```\n    layer.get_input_at(node_index)\n    layer.get_output_at(node_index) \n    ```", "```\n    layer.input_shape\n    layer.output_shape \n    ```", "```\n    layer.get_input_shape_at(node_index)\n    layer.get_output_shape_at(node_index) \n    ```", "```\n    layer.get_config() \n    ```", "```\n    layer.from_config(config) \n    ```", "```\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Dense\nimport numpy as np \n```", "```\n    model = tf.keras.Sequential([\n        # Add a fully connected layer with 1024 units to the model\n        tf.keras.layers.Dense(1024, input_dim=64),\n        # Add an activation layer with ReLU activation function\n        tf.keras.layers.Activation('relu'),\n        # Add a fully connected layer with 256 units to the model\n        tf.keras.layers.Dense(256),\n        # Add an activation layer with ReLU activation function\n        tf.keras.layers.Activation('relu'),\n        # Add a fully connected layer with 10 units to the model\n        tf.keras.layers.Dense(10),\n        # Add an activation layer with softmax activation function\n        tf.keras.layers.Activation('softmax')\n    ]) \n    ```", "```\n    model = tf.keras.Sequential()\n    # Add a fully connected layer with 1024 units to the model\n    model.add(tf.keras.layers.Dense(1024, input_dim=64))\n    # Add an activation layer with ReLU activation function\n    model.add(tf.keras.layers.Activation(relu))\n    # Add a fully connected layer with 256 units to the model\n    model.add(tf.keras.layers.Dense(256))\n    # Add an activation layer with ReLU activation function\n    model.add(tf.keras.layers.Activation('relu'))\n    # Add a fully connected Layer with 10 units to the model\n    model.add(tf.keras.layers.Dense(10))\n    # Add an activation layer with softmax activation function\n    model.add(tf.keras.layers.Activation('softmax')) \n    ```", "```\n        # Creation of a dense layer with a sigmoid activation function:\n        Dense(256, activation='sigmoid')\n        # Or:\n        Dense(256, activation=tf.keras.activations.sigmoid) \n        ```", "```\n        # A dense layer with a kernel initialized to a truncated normal distribution:\n        Dense(256, kernel_initializer='random_normal')\n        # A dense layer with a bias vector initialized with a constant value of 5.0:\n        Dense(256, bias_initializer=tf.keras.initializers.Constant(value=5)) \n        ```", "```\n        # A dense layer with L1 regularization of factor 0.01 applied to the kernel matrix:\n        Dense(256, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n        # A dense layer with L2 regularization of factor 0.01 applied to the bias vector:\n        Dense(256, bias_regularizer=tf.keras.regularizers.l2(0.01)) \n        ```", "```\n    Dense(256, input_dim=(64)) \n    ```", "```\n     Dense(256, input_dim=(64), batch_size=10) \n    ```", "```\n    model.compile(\n        optimizer=\"adam\", \n        loss=\"categorical_crossentropy\",\n        metrics=[\"accuracy\"]\n    ) \n    ```", "```\n    data = np.random.random((2000, 64))\n    labels = np.random.random((2000, 10))\n    val_data = np.random.random((500, 64))\n    val_labels = np.random.random((500, 10))\n    test_data = np.random.random((500, 64))\n    test_labels = np.random.random((500, 10)) \n    ```", "```\n    model.fit(data, labels, epochs=10, batch_size=50,\n              validation_data=(val_data, val_labels)) \n    ```", "```\n    model.evaluate(data, labels, batch_size=50) \n    ```", "```\n    result = model.predict(data, batch_size=50) \n    ```", "```\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.layers import Input, Dense, TimeDistributed\nimport keras.models \n```", "```\n    mnist = tf.keras.datasets.mnist\n    (X_mnist_train, y_mnist_train), (X_mnist_test, y_mnist_test) = mnist.load_data() \n    ```", "```\n    inputs = tf.keras.Input(shape=(28,28)) \n    ```", "```\n    flatten_layer = keras.layers.Flatten() \n    ```", "```\n    flatten_output = flatten_layer(inputs) \n    ```", "```\n    dense_layer = tf.keras.layers.Dense(50, activation='relu') \n    ```", "```\n    dense_output = dense_layer(flatten_output) \n    ```", "```\n    predictions = tf.keras.layers.Dense(10, activation='softmax')(dense_output) \n    ```", "```\n    model = keras.Model(inputs=inputs, outputs=predictions) \n    ```", "```\n    model.summary() \n    ```", "```\n    model.compile(optimizer='sgd',\n                 loss='sparse_categorical_crossentropy',\n                 metrics=['accuracy'])\n    model.fit(X_mnist_train, y_mnist_train,\n              validation_data=(X_mnist_train, y_mnist_train),\n              epochs=10) \n    ```", "```\n    x = Input(shape=(784,))\n    # y will contain the prediction for x\n    y = model(x) \n    ```", "```\n    from keras.layers import TimeDistributed\n    # Input tensor for sequences of 50 timesteps,\n    # Each containing a 28x28 dimensional matrix.\n    input_sequences = tf.keras.Input(shape=(10, 28, 28))\n    # We will apply the previous model to each sequence so one for each timestep.\n    # The MNIST model returns a vector with 10 probabilities (one for each digit).\n    # The TimeDistributed output will be a sequence of 50 vectors of size 10.\n    processed_sequences = tf.keras.layers.TimeDistributed(model)(input_sequences) \n    ```", "```\n    house_data_inputs = tf.keras.Input(shape=(128,), name='house_data')\n    x = tf.keras.layers.Dense(64, activation='relu')(house_data_inputs)\n    block_1_output = tf.keras.layers.Dense(32, activation='relu')(x) \n    ```", "```\n    house_picture_inputs = tf.keras.Input(shape=(128,128,3), name='house_picture')\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(house_picture_inputs)\n    x = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n    block_2_output = tf.keras.layers.Flatten()(x) \n    ```", "```\n    x = tf.keras.layers.concatenate([block_1_output, block_2_output]) \n    ```", "```\n    price_pred = tf.keras.layers.Dense(1, name='price', activation='relu')(x) \n    ```", "```\n    time_elapsed_pred = tf.keras.layers.Dense(2, name='elapsed_time', activation='softmax')(x) \n    ```", "```\n    model = keras.Model([house_data_inputs, house_picture_inputs],\n                       [price_pred, time_elapsed_pred],\n                       name='toy_house_pred') \n    ```", "```\n    keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True) \n    ```", "```\n# Variable-length sequence of integers\ntext_input_a = tf.keras.Input(shape=(None,), dtype='int32')\n# Variable-length sequence of integers\ntext_input_b = tf.keras.Input(shape=(None,), dtype='int32')\n# Embedding for 1000 unique words mapped to 128-dimensional vectors\nshared_embedding = tf.keras.layers.Embedding(1000, 128)\n# Reuse the same layer to encode both inputs\nencoded_input_a = shared_embedding(text_input_a)\nencoded_input_b = shared_embedding(text_input_b) \n```", "```\n    resnet = tf.keras.applications.resnet.ResNet50() \n    ```", "```\n    intermediate_layers = [layer.output for layer in resnet.layers] \n    ```", "```\n    intermediate_layers[:10] \n    ```", "```\n     [<tf.Tensor 'input_7:0' shape=(None, 224, 224, 3) dtype=float32>,\n     <tf.Tensor 'conv1_pad/Pad:0' shape=(None, 230, 230, 3) dtype=float32>,\n     <tf.Tensor 'conv1_conv/BiasAdd:0' shape=(None, 112, 112, 64) dtype=float32>,\n     <tf.Tensor 'conv1_bn/cond/Identity:0' shape=(None, 112, 112, 64) dtype=float32>,\n     <tf.Tensor 'conv1_relu/Relu:0' shape=(None, 112, 112, 64) dtype=float32>,\n     <tf.Tensor 'pool1_pad/Pad:0' shape=(None, 114, 114, 64) dtype=float32>,\n     <tf.Tensor 'pool1_pool/MaxPool:0' shape=(None, 56, 56, 64) dtype=float32>,\n     <tf.Tensor 'conv2_block1_1_conv/BiasAdd:0' shape=(None, 56, 56, 64) dtype=float32>,\n     <tf.Tensor 'conv2_block1_1_bn/cond/Identity:0' shape=(None, 56, 56, 64) dtype=float32>,\n     <tf.Tensor 'conv2_block1_1_relu/Relu:0' shape=(None, 56, 56, 64) dtype=float32>] \n    ```", "```\n    feature_layers = intermediate_layers[:-2] \n    ```", "```\n    feat_extraction_model = keras.Model(inputs=resnet.input, outputs=feature_layers) \n    ```", "```\nimport tensorflow as tf\nfrom tensorflow import keras \n```", "```\n    class MyCustomDense(tf.keras.layers.Layer):\n        # Initialize this class with the number of units\n        def __init__(self, units):\n            super(MyCustomDense, self).__init__()\n            self.units = units\n\n        # Define the weights and the bias\n        def build(self, input_shape):\n            self.w = self.add_weight(shape=(input_shape[-1], self.units),\n                                initializer='random_normal',\n                                trainable=True)\n            self.b = self.add_weight(shape=(self.units,),\n                                initializer='random_normal',\n                                trainable=True)\n\n        # Applying this layer transformation to the input tensor\n        def call(self, inputs):\n            return tf.matmul(inputs, self.w) + self.b\n\n        # Function to retrieve the configuration\n        def get_config(self):\n            return {'units': self.units} \n    ```", "```\n    # Create an input layer\n    inputs = keras.Input((12,4))\n    # Add an instance of MyCustomeDense layer\n    outputs = MyCustomDense(2)(inputs)\n    # Create a model\n    model = keras.Model(inputs, outputs)\n    # Get the model config\n    config = model.get_config() \n    ```", "```\n    new_model = keras.Model.from_config(config, \n                                  custom_objects={'MyCustomDense': MyCustomDense}) \n    ```", "```\n    mnist = tf.keras.datasets.mnist\n    (X_mnist_train, y_mnist_train), (X_mnist_test, y_mnist_test) = mnist.load_data()\n    train_mnist_features = X_mnist_train/255\n    test_mnist_features = X_mnist_test/255 \n    ```", "```\n    class MyMNISTModel(tf.keras.Model):\n        def __init__(self, num_classes):\n            super(MyMNISTModel, self).__init__(name='my_mnist_model')\n            self.num_classes = num_classes\n            self.flatten_1 = tf.keras.layers.Flatten()\n            self.dropout = tf.keras.layers.Dropout(0.1)\n            self.dense_1 = tf.keras.layers.Dense(50, activation='relu')\n            self.dense_2 = tf.keras.layers.Dense(10, activation='softmax')\n        def call(self, inputs, training=False):\n            x = self.flatten_1(inputs)\n            # Apply dropout only during the training phase\n            x = self.dense_1(x)\n            if training:\n                x = self.dropout(x, training=training)\n            return self.dense_2(x) \n    ```", "```\n    my_mnist_model = MyMNISTModel(10)\n    # Compile\n    my_mnist_model.compile(optimizer='sgd',\n                          loss='sparse_categorical_crossentropy',\n                          metrics=['accuracy'])\n    # Train\n    my_mnist_model.fit(train_features, y_train,\n                      validation_data=(test_features, y_test),\n                      epochs=10) \n    ```", "```\nimport tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nfrom tensorflow.keras.preprocessing.sequence import TimeseriesGenerator, pad_sequences, skipgrams, make_sampling_table\nfrom tensorflow.keras.preprocessing.text import text_to_word_sequence, one_hot, hashing_trick, Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense \n```", "```\n    series = np.array([i for i in range(10)])\n    print(series) \n    ```", "```\n    array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]) \n    ```", "```\n    generator = TimeseriesGenerator(data = series,\n                                   targets = series,\n                                   length=5,\n                                   batch_size=1,\n                                   shuffle=False,\n                                   reverse=False) \n    ```", "```\n    # number of samples\n    print('Samples: %d' % len(generator)) \n    ```", "```\n    for i in range(len(generator)):\n        x, y = generator[i]\n        print('%s => %s' % (x, y)) \n    ```", "```\n    [[0 1 2 3 4]] => [5]\n    [[1 2 3 4 5]] => [6]\n    [[2 3 4 5 6]] => [7]\n    [[3 4 5 6 7]] => [8]\n    [[4 5 6 7 8]] => [9] \n    ```", "```\n    model = Sequential()\n    model.add(Dense(10, activation='relu', input_dim=5))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mse') \n    ```", "```\n    model.fit(generator, epochs=10) \n    ```", "```\n    sentences = [[\"What\", \"do\", \"you\", \"like\", \"?\"],\n                 [\"I\", \"like\", \"basket-ball\", \"!\"],\n                 [\"And\", \"you\", \"?\"],\n                 [\"I\", \"like\", \"coconut\", \"and\", \"apple\"]] \n    ```", "```\n    text_set = set(np.concatenate(sentences))\n    vocab_to_int = dict(zip(text_set, range(len(text_set))))\n    int_to_vocab = {vocab_to_int[word]:word for word in vocab_to_int.keys()} \n    ```", "```\n    encoded_sentences = []\n    for sentence in sentences:\n        encoded_sentence = [vocab_to_int[word] for word in sentence]\n        encoded_sentences.append(encoded_sentence)\n    encoded_sentences \n    ```", "```\n    [[8, 4, 7, 6, 0], [5, 6, 2, 3], [10, 7, 0], [5, 6, 1, 9, 11]] \n    ```", "```\n    pad_sequences(encoded_sentences) \n    ```", "```\n    array([[ 8,  4,  7,  6,  0],\n           [ 0,  5,  6,  2,  3],\n           [ 0,  0, 10,  7,  0],\n           [ 5,  6,  1,  9, 11]], dtype=int32) \n    ```", "```\n    pad_sequences(encoded_sentences, maxlen = 7) \n    ```", "```\n    array([[ 0,  0,  8,  4,  7,  6,  0],\n           [ 0,  0,  0,  5,  6,  2,  3],\n           [ 0,  0,  0,  0, 10,  7,  0],\n           [ 0,  0,  5,  6,  1,  9, 11]], dtype=int32) \n    ```", "```\n    pad_sequences(encoded_sentences, maxlen = 3) \n    ```", "```\n    array([[ 7,  6,  0],\n           [ 6,  2,  3],\n           [10,  7,  0],\n           [ 1,  9, 11]], dtype=int32) \n    ```", "```\n    pad_sequences(encoded_sentences, maxlen = 3, truncating='post') \n    ```", "```\n    array([[ 8,  4,  7],\n           [ 5,  6,  2],\n           [10,  7,  0],\n           [ 5,  6,  1]], dtype=int32) \n    ```", "```\n    sentence = \"I like coconut and apple\"\n    encoded_sentence = [vocab_to_int[word] for word in sentence.split()]\n    vocabulary_size = len(encoded_sentence) \n    ```", "```\n    pairs, labels = skipgrams(encoded_sentence, \n                              vocabulary_size, \n                              window_size=1,\n                              negative_samples=0) \n    ```", "```\n    for i in range(len(pairs)):\n        print(\"({:s} , {:s} ) -> {:d}\".format(\n              int_to_vocab[pairs[i][0]], \n              int_to_vocab[pairs[i][1]], \n              labels[i])) \n    ```", "```\n    (coconut , and ) -> 1\n    (apple , ! ) -> 0\n    (and , coconut ) -> 1\n    (apple , and ) -> 1\n    (coconut , do ) -> 0\n    (like , I ) -> 1\n    (and , apple ) -> 1\n    (like , coconut ) -> 1\n    (coconut , do ) -> 0\n    (I , like ) -> 1\n    (coconut , like ) -> 1\n    (and , do ) -> 0\n    (like , coconut ) -> 0\n    (I , ! ) -> 0\n    (like , ! ) -> 0\n    (and , coconut ) -> 0 \n    ```", "```\n    sentence = \"I like coconut , I like apple\" \n    ```", "```\n    text_to_word_sequence(sentence, lower=False) \n    ```", "```\n    ['I', 'like', 'coconut', 'I', 'like', 'apple'] \n    ```", "```\n    text_to_word_sequence(sentence, lower=True, filters=[]) \n    ```", "```\n    ['i', 'like', 'coconut', ',', 'i', 'like', 'apple'] \n    ```", "```\n    sentences = [[\"What\", \"do\", \"you\", \"like\", \"?\"],\n                 [\"I\", \"like\", \"basket-ball\", \"!\"],\n                 [\"And\", \"you\", \"?\"],\n                 [\"I\", \"like\", \"coconut\", \"and\", \"apple\"]] \n    ```", "```\n    # create the tokenizer\n    t = Tokenizer()\n    # fit the tokenizer on the documents\n    t.fit_on_texts(sentences) \n    ```", "```\n    print(t.word_counts) \n    ```", "```\n    OrderedDict([('what', 1), ('do', 1), ('you', 2), ('like', 3), ('?', 2), ('i', 2), ('basket-ball', 1), ('!', 1), ('and', 2), ('coconut', 1), ('apple', 1)]) \n    ```", "```\n    print(t.document_count) \n    ```", "```\n    4 \n    ```", "```\n    print(t.word_index) \n    ```", "```\n    {'like': 1, 'you': 2, '?': 3, 'i': 4, 'and': 5, 'what': 6, 'do': 7, 'basket-ball': 8, '!': 9, 'coconut': 10, 'apple': 11} \n    ```", "```\n    print(t.word_docs) \n    ```", "```\n    defaultdict(<class 'int'>, {'do': 1, 'like': 3, 'what': 1, 'you': 2, '?': 2, '!': 1, 'basket-ball': 1, 'i': 2, 'and': 2, 'coconut': 1, 'apple': 1}) \n    ```", "```\n    t.texts_to_matrix(sentences, mode='binary') \n    ```", "```\n     [[0\\. 1\\. 1\\. 1\\. 0\\. 0\\. 1\\. 1\\. 0\\. 0\\. 0\\. 0.]\n     [0\\. 1\\. 0\\. 0\\. 1\\. 0\\. 0\\. 0\\. 1\\. 1\\. 0\\. 0.]\n     [0\\. 0\\. 1\\. 1\\. 0\\. 1\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.]\n     [0\\. 1\\. 0\\. 0\\. 1\\. 1\\. 0\\. 0\\. 0\\. 0\\. 1\\. 1.]] \n    ```", "```\n    t.texts_to_matrix(sentences, mode='count') \n    ```", "```\n    [[0\\. 1\\. 1\\. 1\\. 0\\. 0\\. 1\\. 1\\. 0\\. 0\\. 0\\. 0.]\n     [0\\. 1\\. 0\\. 0\\. 1\\. 0\\. 0\\. 0\\. 1\\. 1\\. 0\\. 0.]\n     [0\\. 0\\. 1\\. 1\\. 0\\. 1\\. 0\\. 0\\. 0\\. 0\\. 0\\. 0.]\n     [0\\. 1\\. 0\\. 0\\. 1\\. 1\\. 0\\. 0\\. 0\\. 0\\. 1\\. 1.]] \n    ```", "```\n    # Load CIFAR10 Dataset\n    (x_cifar10_train, y_cifar10_train), (x_cifar10_test, y_cifar10_test) = tf.keras.datasets.cifar10.load_data() \n    ```", "```\n    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n        rotation_range=15,\n        width_shift_range=3,\n        height_shift_range=3,\n        horizontal_flip=True) \n    ```", "```\n    it= datagen.flow(x_cifar10_train, y_cifar10_train, batch_size = 32) \n    ```", "```\n    model = tf.keras.models.Sequential([\n       tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\", input_shape=[32, 32, 3]),\n       tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n       tf.keras.layers.MaxPool2D(pool_size=2),\n       tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n       tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n       tf.keras.layers.MaxPool2D(pool_size=2),\n       tf.keras.layers.Flatten(),\n       tf.keras.layers.Dense(128, activation=\"relu\"),\n       tf.keras.layers.Dense(10, activation=\"softmax\")\n    ])\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n                 optimizer=tf.keras.optimizers.SGD(lr=0.01),\n                 metrics=[\"accuracy\"]) \n    ```", "```\n    history = model.fit(it, epochs=10,\n                        steps_per_epoch=len(x_cifar10_train) / 32,\n                        validation_data=(x_cifar10_test,                                           y_cifar10_test)) \n    ```"]