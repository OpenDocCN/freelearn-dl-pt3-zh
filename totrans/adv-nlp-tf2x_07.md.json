["```\n$ wget http://images.cocodataset.org/zips/train2014.zip\n$ wget http://images.cocodataset.org/zips/val2014.zip\n$ wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip \n```", "```\n$ mkdir data\n$ mv train2014 data/\n$ mv val2014 data/\n$ mv annotations data/ \n```", "```\nvalcaptions = json.load(open(\n    './data/annotations/captions_val2014.json', 'r'))\ntrcaptions = json.load(open(\n    './data/annotations/captions_train2014.json', 'r'))\n# inspect the annotations\nprint(trcaptions.keys())\ndict_keys(['info', 'images', 'licenses', 'annotations']) \n```", "```\nprefix = \"./data/\"\nval_prefix = prefix + 'val2014/'\ntrain_prefix = prefix + 'train2014/'\n# training images\ntrimages = {x['id']: x['file_name'] for x in trcaptions['images']}\n# validation images\n# take all images from validation except 5k - karpathy split\nvalset = len(valcaptions['images']) - 5000 # leave last 5k \nvalimages = {x['id']: x['file_name'] for x in valcaptions['images'][:valset]}\ntruevalimg = {x['id']: x['file_name'] for x in valcaptions['images'][valset:]} \n```", "```\n# we flatten to (caption, image_path) structure\ndata = list()\nerrors = list()\nvalidation = list() \n```", "```\nfor item in trcaptions['annotations']:\n    if int(item['image_id']) in trimages:\n        fpath = train_prefix + trimages[int(item['image_id'])]\n        caption = item['caption']\n        data.append((caption, fpath))\n    else:\n        errors.append(item) \n```", "```\nfor item in valcaptions['annotations']:\n    caption = item['caption']\n    if int(item['image_id']) in valimages:\n        fpath = val_prefix + valimages[int(item['image_id'])]\n        data.append((caption, fpath))\n    elif int(item['image_id']) in truevalimg: # reserved\n        fpath = val_prefix + truevalimg[int(item['image_id'])]\n        validation.append((caption, fpath))\n    else:\n        errors.append(item) \n```", "```\n# persist for future use\nwith open(prefix + 'data.csv', 'w') as file:\n    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n    writer.writerows(data)\n# persist for future use\nwith open(prefix + 'validation.csv', 'w') as file:\n    writer = csv.writer(file, quoting=csv.QUOTE_ALL)\n    writer.writerows(validation)\nprint(\"TRAINING: Total Number of Captions: {},  Total Number of Images: {}\".format(\n    len(data), len(trimages) + len(valimages)))\nprint(\"VALIDATION/TESTING: Total Number of Captions: {},  Total Number of Images: {}\".format(\n    len(validation), len(truevalimg)))\nprint(\"Errors: \", errors) \n```", "```\nTRAINING: Total Number of Captions: 591751,  Total Number of Images: 118287\nVALIDATION/TESTING: Total Number of Captions: 25016,  Total Number of Images: 5000\nErrors:  [] \n```", "```\ntulip = Image.open(\"chap7-tulip.jpg\") \n# convert to gray scale image\ntulip_grey = tulip.convert('L')\ntulip_ar = np.array(tulip_grey) \n```", "```\n# Sobel Filter\nkernel_1 = np.array([[1, 0, -1],\n                     [2, 0, -2],\n                     [1, 0, -1]])        # Vertical edge \nkernel_2 = np.array([[1, 2, 1],\n                     [0, 0, 0],\n                     [-1, -2, -1]])      # Horizontal edge \nout1 = convolve2d(tulip_ar, kernel_1)    # vertical filter\nout2 = convolve2d(tulip_ar, kernel_2)    # horizontal filter\n# Create a composite image from the two edge detectors\nout3 = np.sqrt(out1**2 + out2**2) \n```", "```\nprefix = './data/'\nsave_prefix = prefix + \"features/\"  # for storing prefixes\nannot = prefix + 'data.csv'\n# load the pre-processed file\ninputs = pd.read_csv(annot, header=None, names=[\"caption\", \"image\"]) \n```", "```\n# We are going to use the last residual block of # the ResNet50 architecture\n# which has dimension 7x7x2048 and store into individual file\ndef load_image(image_path, size=(224, 224)):\n    # pre-processes images for ResNet50 in batches \n    image = tf.io.read_file(image_path)\n    image = tf.io.decode_jpeg(image, channels=3)\n    image = tf.image.resize(image, size)\n    image = **preprocess_input(image)**  # from keras.applications.ResNet50\n    return image, image_path \n```", "```\nuniq_images = sorted(inputs['image'].unique())  \nprint(\"Unique images: \", len(uniq_images))  # 118,287 images \n```", "```\nimage_dataset = tf.data.Dataset.from_tensor_slices(uniq_images)\nimage_dataset = image_dataset.map(\n    load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE).batch(16) \n```", "```\nrs50 = tf.keras.applications.ResNet50(\n    include_top=False,\n    weights=\"imagenet\", \n    input_shape=(224, 224, 3)\n)\nnew_input = rs50.input\nhidden_layer = rs50.layers[-1].output\nfeatures_extract = tf.keras.Model(new_input, hidden_layer)\nfeatures_extract.summary() \n```", "```\n__________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to\n==================================================================\ninput_1 (InputLayer)            [(None, 224, 224, 3) 0\n__________________________________________________________________\n<CONV BLOCK 1>\n__________________________________________________________________\n<CONV BLOCK 2>\n__________________________________________________________________\n<CONV BLOCK 3>\n__________________________________________________________________\n<CONV BLOCK 4>\n__________________________________________________________________\n<CONV BLOCK 5>\n==================================================================\nTotal params: 23,587,712\nTrainable params: 23,534,592\nNon-trainable params: 53,120\n__________________________________________________________________ \n```", "```\nsave_prefix = prefix + \"features/\"\ntry:\n    # Create this directory \n    os.mkdir(save_prefix)\nexcept FileExistsError:\n    pass # Directory already exists \n```", "```\nfor img, path in tqdm(image_dataset):\n    batch_features = features_extract(img)\n    batch_features = tf.reshape(batch_features,\n                                (batch_features.shape[0], -1,                                  batch_features.shape[3]))\n    for feat, p in zip(batch_features, path):\n        filepath = p.numpy().decode(\"utf-8\")\n        filepath = save_prefix + filepath.split('/')[-1][:-3] + \"npy\"\n        np.save(filepath, feat.numpy())\nprint(\"Images saved as npy files\") \n```", "```\n# Now, read the labels and create a subword tokenizer with it\n# ~8K vocab size\ncap_tokenizer = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n    inputs['caption'].map(lambda x: x.lower().strip()).tolist(),\n    target_vocab_size=2**13, reserved_tokens=['<s>', '</s>'])\ncap_tokenizer.save_to_file(\"captions\") \n```", "```\ndef get_angles(pos, i, d_model):\n    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n    return pos * angle_rates \n```", "```\ndef positional_encoding(position, d_model):\n    angle_rads = get_angles(np.arange(position)[:, np.newaxis],\n                            np.arange(d_model)[np.newaxis, :],\n                            d_model)\n    # apply sin to even indices in the array; 2i\n    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n    # apply cos to odd indices in the array; 2i+1\n    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n    pos_encoding = angle_rads[np.newaxis, ...]\n    return tf.cast(pos_encoding, dtype=tf.float32) \n```", "```\ndef create_padding_mask(seq):\n    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n    # add extra dimensions to add the padding\n    # to the attention logits.\n    return seq[:, tf.newaxis, tf.newaxis, :]  \n    # (batch_size, 1, 1, seq_len)\n# while decoding, we dont have recurrence and dont want Decoder\n# to see tokens from the future\ndef create_look_ahead_mask(size):\n    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n    return mask  # (seq_len, seq_len) \n```", "```\ndef scaled_dot_product_attention(q, k, v, mask):\n    # (..., seq_len_q, seq_len_k)\n    matmul_qk = tf.matmul(q, k, transpose_b=True)\n    # scale matmul_qk\n    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n    # add the mask to the scaled tensor.\n    if mask is not None:\n        scaled_attention_logits += (mask * -1e9)\n    # softmax is normalized on the last axis (seq_len_k)     # so that the scores\n    # add up to 1.\n    attention_weights = tf.nn.softmax(\n                        scaled_attention_logits,\n                        axis=-1)  # (..., seq_len_q, seq_len_k)\n    output = tf.matmul(attention_weights, v)  \n    # (..., seq_len_q, depth_v)\n    return output, attention_weights \n```", "```\nclass MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads):\n        super(MultiHeadAttention, self).__init__()\n        self.num_heads = num_heads\n        self.d_model = d_model\n        **assert** **d_model % self.num_heads ==** **0**\n        self.depth = d_model // self.num_heads\n        self.wq = tf.keras.layers.Dense(d_model)\n        self.wk = tf.keras.layers.Dense(d_model)\n        self.wv = tf.keras.layers.Dense(d_model)\n        self.dense = tf.keras.layers.Dense(d_model) \n```", "```\n def call(self, v, k, q, mask):\n        batch_size = tf.shape(q)[0]\n        q = self.wq(q)  # (batch_size, seq_len, d_model)\n        k = self.wk(k)  # (batch_size, seq_len, d_model)\n        v = self.wv(v)  # (batch_size, seq_len, d_model)\n        # (batch_size, num_heads, seq_len_q, depth)\n        **q = self.split_heads(q, batch_size)**\n        # (batch_size, num_heads, seq_len_k, depth)\n        **k = self.split_heads(k, batch_size)**\n        # (batch_size, num_heads, seq_len_v, depth)\n        **v = self.split_heads(v, batch_size)**\n        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n        # (batch_size, seq_len_q, num_heads, depth)\n        scaled_attention = tf.transpose(scaled_attention, \n                                                   perm=[0, 2, 1, 3])\n        concat_attention = tf.reshape(scaled_attention,\n                                        (batch_size, -1,\n                              self.d_model))  \n        # (batch_size, seq_len_q, d_model)\n        # (batch_size, seq_len_q, d_model)\n        output = self.dense(concat_attention)\n        return output, attention_weights \n```", "```\n def split_heads(self, x, batch_size):\n        \"\"\"\n        Split the last dimension into (num_heads, depth).\n        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n        \"\"\"\n        x = tf.reshape(x, (batch_size, -1, \nself.num_heads, self.depth))\n        return tf.transpose(x, perm=[0, 2, 1, 3]) \n```", "```\ndef point_wise_feed_forward_network(d_model, dff):\n    return tf.keras.Sequential([\n        # (batch_size, seq_len, dff)\n        tf.keras.layers.Dense(dff, activation='relu'),\n        tf.keras.layers.Dense(d_model)\n        # (batch_size, seq_len, d_model)\n    ]) \n```", "```\nclass VisualEncoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, dff,\n                 maximum_position_encoding=**49**, dropout_rate=0.1,\n                 use_pe=True):\n        # we have 7x7 images from ResNet50, \n        # and each pixel is an input token\n        # which has been embedded into 2048 dimensions by ResNet\n        super(VisualEncoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n\n        **# FC layer replaces embedding layer in traditional encoder**\n        **# this FC layers takes 49x2048 image** \n        **# and projects into model dims**\n        **self.fc = tf.keras.layers.Dense(d_model, activation=****'relu'****)**\n        self.pos_encoding = positional_encoding(\n                                         maximum_position_encoding,\n                                      self.d_model)\n        self.enc_layers = [EncoderLayer(d_model, num_heads, \n                                        dff, dropout_rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n\n        self.use_pe = use_pe \n```", "```\n def call(self, x, training, mask):\n        # all inp image sequences are always 49, so mask not needed\n        seq_len = tf.shape(x)[1]\n        # adding embedding and position encoding.\n        # input size should be batch_size, 49, 2048)\n        # output dims should be (batch_size, 49, d_model)\n        x = self.fc(x)\n        # scaled dot product attention\n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32)) \n        if self.use_pe:\n            x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x = self.enc_layers[i](\n                x, training, mask)  # mask shouldnt be needed\n        return x  # (batch_size, 49, d_model) \n```", "```\nclass EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(EncoderLayer, self).__init__()\n        self.mha = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(\n                                                        epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(\n                                                        epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n    def call(self, x, training, mask):\n        # (batch_size, input_seq_len, d_model)\n        attn_output, _ = self.mha(x, x, x, mask)\n        attn_output = self.dropout1(attn_output, \n                                      training=training)\n        # (batch_size, input_seq_len, d_model)\n        **out1 = self.layernorm1(x + attn_output)** **# Residual connection**\n\n        # (batch_size, input_seq_len, d_model)\n        ffn_output = self.ffn(out1)  \n        ffn_output = self.dropout2(ffn_output, training=training)\n        # (batch_size, input_seq_len, d_model)\n        **out2 = self.layernorm2(out1 + ffn_output)** **# Residual conx**\n        return out2 \n```", "```\nclass DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, d_model, num_heads, dff, rate=0.1):\n        super(DecoderLayer, self).__init__()\n        self.mha1 = MultiHeadAttention(d_model, num_heads)\n        self.mha2 = MultiHeadAttention(d_model, num_heads)\n        self.ffn = point_wise_feed_forward_network(d_model, dff)\n        self.layernorm1 = tf.keras.layers.LayerNormalization(\n                                                       epsilon=1e-6)\n        self.layernorm2 = tf.keras.layers.LayerNormalization(\n                                                       epsilon=1e-6)\n        self.layernorm3 = tf.keras.layers.LayerNormalization(\n                                                       epsilon=1e-6)\n        self.dropout1 = tf.keras.layers.Dropout(rate)\n        self.dropout2 = tf.keras.layers.Dropout(rate)\n        self.dropout3 = tf.keras.layers.Dropout(rate) \n```", "```\n def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        # enc_output.shape == (batch_size, input_seq_len, d_model)\n        **attn1, attn_weights_block1 = self.mha1(**\n            **x, x, x, look_ahead_mask)**\n        # args ^ => (batch_size, target_seq_len, d_model)\n        attn1 = self.dropout1(attn1, training=training)\n        out1 = self.layernorm1(attn1 + x) # residual\n        attn2, attn_weights_block2 = self.mha2(\n            enc_output, enc_output, out1, padding_mask)  \n        # args ^ =>  (batch_size, target_seq_len, d_model)\n        attn2 = self.dropout2(attn2, training=training)\n        # (batch_size, target_seq_len, d_model)\n        out2 = self.layernorm2(attn2 + out1)\n        ffn_output = self.ffn(out2)  \n        ffn_output = self.dropout3(ffn_output, training=training)\n        # (batch_size, target_seq_len, d_model)\n        out3 = self.layernorm3(ffn_output + out2)\n        return out3, attn_weights_block1, attn_weights_block2 \n```", "```\nclass Decoder(tf.keras.layers.Layer):\n    def __init__(self, num_layers, d_model, num_heads, \n                 dff, target_vocab_size,\n                 maximum_position_encoding, rate=0.1):\n        super(Decoder, self).__init__()\n        self.d_model = d_model\n        self.num_layers = num_layers\n        self.embedding = tf.keras.layers.Embedding(\n                                        target_vocab_size, d_model)\n        self.pos_encoding = positional_encoding(\n                                maximum_position_encoding, \n                                  d_model)\n        self.dec_layers = [DecoderLayer(d_model, num_heads, \n                                           dff, rate)\n                           for _ in range(num_layers)]\n        self.dropout = tf.keras.layers.Dropout(rate) \n```", "```\n def call(self, x, enc_output, training,\n             look_ahead_mask, padding_mask):\n        seq_len = tf.shape(x)[1]\n        attention_weights = {}\n        x = self.embedding(x)  \n        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n        x += self.pos_encoding[:, :seq_len, :]\n        x = self.dropout(x, training=training)\n        for i in range(self.num_layers):\n            x, block1, block2 = self.dec_layers[i](x, enc_output,\n                       training, look_ahead_mask, padding_mask)\n        attention_weights['decoder_layer{}_block1'.format(i + 1)]  = block1\n        attention_weights['decoder_layer{}_block2'.format(i + 1)] = block2\n        # x.shape == (batch_size, target_seq_len, d_model)\n        return x, attention_weights \n```", "```\nclass Transformer(tf.keras.Model):\n    def __init__(self, num_layers, d_model, num_heads, dff,\n                 target_vocab_size, pe_input, pe_target, rate=0.1,\n                 use_pe=True):\n        super(Transformer, self).__init__()\n        self.encoder = VisualEncoder(num_layers, d_model, \n                                     num_heads, dff,\n                                     pe_input, rate, use_pe)\n        self.decoder = Decoder(num_layers, d_model, num_heads, \n                       dff, target_vocab_size, pe_target, rate)\n        self.final_layer = tf.keras.layers.Dense(\n                                       target_vocab_size)\n    def call(self, inp, tar, training, enc_padding_mask,\n             look_ahead_mask, dec_padding_mask):\n        # (batch_size, inp_seq_len, d_model)\n        enc_output = self.encoder(inp, training, enc_padding_mask)\n        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n        dec_output, attention_weights = self.decoder(\n                                tar, enc_output, training, \n                                look_ahead_mask, dec_padding_mask)\n        # (batch_size, tar_seq_len, target_vocab_size)\n        final_output = self.final_layer(dec_output)\n        return final_output, attention_weights \n```", "```\nprefix = './data/'\nsave_prefix = prefix + \"features/\"  # for storing prefixes\nannot = prefix + 'data.csv'\ninputs = pd.read_csv(annot, header=None, \n                      names=[\"caption\", \"image\"])\nprint(\"Data file loaded\") \n```", "```\ncap_tokenizer = \\\n          tfds.features.text.SubwordTextEncoder.load_from_file(\n                                                    \"captions\")\nprint(cap_tokenizer.encode(\n                  \"A man riding a wave on top of a surfboard.\".lower())\n)\nprint(\"Tokenizer hydrated\")\n# Max length of captions split by spaces\nlens = inputs['caption'].map(lambda x: len(x.split()))\n# Max length of captions after tokenization\n# tfds demonstrated in earlier chapters\n# This is a quick way if data fits in memory\nlens = inputs['caption'].map(\n                lambda x: len(cap_tokenizer.encode(x.lower()))\n)\n# We will set this as the max length of captions\n# which cover 99% of the captions without truncation\nmax_len = int(lens.quantile(0.99) + 1)  # for special tokens \n```", "```\nstart = '<s>'\nend = '</s>'\ninputs['tokenized'] = inputs['caption'].map(\n    lambda x: start + x.lower().strip() + end)\ndef tokenize_pad(x):\n    x = cap_tokenizer.encode(x)\n    if len(x) < max_len:\n        x = x + [0] * int(max_len - len(x))\n    return x[:max_len]\ninputs['tokens'] = inputs.tokenized.map(lambda x: tokenize_pad(x)) \n```", "```\n# now to compute a column with the new name of the saved \n# image feature file\ninputs['img_features'] = inputs['image'].map(lambda x:\n                                             save_prefix +\n                                             x.split('/')[-1][:-3]\n                                             + 'npy') \n```", "```\ncaptions = inputs.tokens.tolist()\nimg_names = inputs.img_features.tolist()\n# Load the numpy file with extracted ResNet50 feature\ndef load_image_feature(img_name, cap):\n    img_tensor = np.load(img_name.decode('utf-8'))\n    return img_tensor, cap\ndataset = tf.data.Dataset.from_tensor_slices((img_train, \n                                              cap_train))\n# Use map to load the numpy files in parallel\ndataset = dataset.map(lambda item1, item2: tf.numpy_function(\n    load_image_feature, [item1, item2], [tf.float32, tf.int32]),\n    num_parallel_calls=tf.data.experimental.AUTOTUNE) \n```", "```\n# Small Model\nnum_layers = 4\nd_model = 128\ndff = d_model * 4\nnum_heads = 8 \n```", "```\n# BERT Base Model\n# num_layers = 12\n# d_model = 768\n# dff = d_model * 4\n# num_heads = 12 \n```", "```\ntarget_vocab_size = cap_tokenizer.vocab_size  \n# already includes start/end tokens\ndropout_rate = 0.1\nEPOCHS = 20  # should see results in 4-10 epochs also\ntransformer = vt.Transformer(num_layers, d_model, num_heads, dff,\n                             target_vocab_size,\n                             pe_input=49,  # 7x7 pixels\n                             pe_target=target_vocab_size,\n                             rate=dropout_rate,\n                             use_pe=False\n                             ) \n```", "```\nModel: \"transformer\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nvisual_encoder (VisualEncode multiple                  1055360   \n_________________________________________________________________\ndecoder (Decoder)            multiple                  2108544   \n_________________________________________________________________\ndense_65 (Dense)             multiple                  1058445   \n=================================================================\nTotal params: 4,222,349\nTrainable params: 4,222,349\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\nclass CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(self, d_model, warmup_steps=4000):\n        super(CustomSchedule, self).__init__()\n        self.d_model = d_model\n        self.d_model = tf.cast(self.d_model, tf.float32)\n        self.warmup_steps = warmup_steps\n    def __call__(self, step):\n        arg1 = tf.math.rsqrt(step)\n        arg2 = step * (self.warmup_steps ** -1.5)\n        return tf.math.rsqrt(self.d_model) * \\\n                tf.math.minimum(arg1, arg2)\nlearning_rate = CustomSchedule(d_model)\noptimizer = tf.keras.optimizers.Adam(learning_rate, \n                                     beta_1=0.9, beta_2=0.98,\n                                     epsilon=1e-9) \n```", "```\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n                              from_logits=True, reduction='none')\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n    return tf.reduce_sum(loss_) / tf.reduce_sum(mask)\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\ntrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(\n                        name='train_accuracy') \n```", "```\ncheckpoint_path = \"./checkpoints/train-small-model-40ep\"\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, \n                           max_to_keep=5)\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!') \n```", "```\ndef create_masks(inp, tar):\n    # Encoder padding mask - This should just be 1's\n    # input shape should be (batch_size, 49, 2048)\n    inp_seq = tf.ones([inp.shape[0], inp.shape[1]])  \n    enc_padding_mask = vt.create_padding_mask(inp_seq)\n    # Used in the 2nd attention block in the Decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = vt.create_padding_mask(inp_seq)\n    # Used in the 1st attention block in the Decoder.\n    # It is used to pad and mask future tokens in the input \n    # received by the decoder.\n    look_ahead_mask = vt.create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = vt.create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, \n                                  look_ahead_mask)\n    return enc_padding_mask, combined_mask, dec_padding_mask \n```", "```\n@tf.function\ndef train_step(inp, tar):\n    tar_inp = tar[:, :-1]\n    tar_real = tar[:, 1:]\n    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n    with tf.GradientTape() as tape:\n        predictions, _ = transformer(inp, tar_inp,\n                                     True,\n                                     enc_padding_mask,\n                                     combined_mask,\n                                     dec_padding_mask)\n        loss = loss_function(tar_real, predictions)\n    gradients = tape.gradient(loss, \n                                transformer.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, \n                                   transformer.trainable_variables))\n    train_loss(loss)\n    train_accuracy(tar_real, predictions) \n```", "```\n# setup training parameters\nBUFFER_SIZE = 1000\nBATCH_SIZE = 64  # can +/- depending on GPU capacity\n# Shuffle and batch\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\ndataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n# Begin Training\nfor epoch in range(EPOCHS):\n    start_tm = time.time()\n    train_loss.reset_states()\n    train_accuracy.reset_states()\n    # inp -> images, tar -> caption\n    for (batch, (inp, tar)) in enumerate(dataset):\n        train_step(inp, tar)\n        if batch % 100 == 0:\n            ts = datetime.datetime.now().strftime(\n                                      \"%d-%b-%Y (%H:%M:%S)\")\n            print('[{}] Epoch {} Batch {} Loss {:.6f} Accuracy'+\\ \n                   '{:.6f}'.format(ts, epoch + 1, batch,\n                                   train_loss.result(),\n                                   train_accuracy.result()))\n    if (epoch + 1) % 2 == 0:\n        ckpt_save_path = ckpt_manager.save()\n        print('Saving checkpoint for epoch {} at {}'.format(\n                               epoch + 1,\n                               ckpt_save_path))\n    print('Epoch {} Loss {:.6f} Accuracy {:.6f}'.format(epoch + 1,\n                                       train_loss.result(),\n                                       train_accuracy.result()))\n    print('Time taken for 1 epoch: {} secs\\n'.format(\n                                     time.time() - start_tm)) \n```", "```\n(tf24nlp) $ python caption-training.py \n```", "```\ncap_tokenizer = tfds.features.text.SubwordTextEncoder.load_from_file(\"captions\") \n```", "```\n# Small Model\nnum_layers = 4\nd_model = 128\ndff = d_model * 4\nnum_heads = 8\ntarget_vocab_size = cap_tokenizer.vocab_size  # already includes \n                                              # start/end tokens\ndropout_rate = 0\\. # immaterial during inference\ntransformer = vt.Transformer(num_layers, d_model, num_heads, dff,\n                             target_vocab_size,\n                             pe_input=49,  # 7x7 pixels\n                             pe_target=target_vocab_size,\n                             rate=dropout_rate\n                             ) \n```", "```\ncheckpoint_path = \"./checkpoints/train-small-model-nope-40ep\"\nckpt = tf.train.Checkpoint(transformer=transformer,\n                           optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, \n                                            max_to_keep=5)\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    print ('Latest checkpoint restored!!') \n```", "```\n# Helper function for creating masks\ndef create_masks(inp, tar):\n    # Encoder padding mask - This should just be 1's\n    # input shape should be (batch_size, 49, 2048)\n    inp_seq = tf.ones([inp.shape[0], inp.shape[1]])  \n    enc_padding_mask = vt.create_padding_mask(inp_seq)\n    # Used in the 2nd attention block in the Decoder.\n    # This padding mask is used to mask the encoder outputs.\n    dec_padding_mask = vt.create_padding_mask(inp_seq)\n    # Used in the 1st attention block in the Decoder.\n    # It is used to pad and mask future tokens in the input received by\n    # the decoder.\n    look_ahead_mask = vt.create_look_ahead_mask(tf.shape(tar)[1])\n    dec_target_padding_mask = vt.create_padding_mask(tar)\n    combined_mask = tf.maximum(dec_target_padding_mask, \n                                 look_ahead_mask)\n    return enc_padding_mask, combined_mask, dec_padding_mask \n```", "```\ndef evaluate(inp_img, max_len=21):\n    start_token = cap_tokenizer.encode(\"<s>\")[0]\n    end_token = cap_tokenizer.encode(\"</s>\")[0]\n\n    encoder_input = inp_img # batch of 1\n\n    # start token for caption\n    decoder_input = [start_token]\n    output = tf.expand_dims(decoder_input, 0)\n    for i in range(max_len):\n        enc_padding_mask, combined_mask, dec_padding_mask = \\\n                create_masks(encoder_input, output)\n\n        # predictions.shape == (batch_size, seq_len, vocab_size)\n        predictions, attention_weights = transformer(\n                                               encoder_input, \n                                               output,\n                                               False,\n                                               enc_padding_mask,\n                                               combined_mask,\n                                               dec_padding_mask)\n        # select the last word from the seq_len dimension\n        predictions = predictions[: ,-1:, :]  \n        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), \n                                  tf.int32)\n\n        # return the result if predicted_id is equal to end token\n        if predicted_id == end_token:\n            return tf.squeeze(output, axis=0), attention_weights\n\n        # concatenate the predicted_id to the output which is \n        # given to the decoder  as its input.\n        output = tf.concat([output, predicted_id], axis=-1)\n    return tf.squeeze(output, axis=0), attention_weights \n```", "```\ndef caption(image):\n    end_token = cap_tokenizer.encode(\"</s>\")[0]\n    result, attention_weights = evaluate(image)\n\n    predicted_sentence = cap_tokenizer.decode([i for i in result \n                                              if i > end_token])\n    print('Predicted Caption: {}'.format(predicted_sentence)) \n```", "```\nrs50 = tf.keras.applications.ResNet50(\n    include_top=False,\n    weights=\"imagenet\",  # no pooling\n    input_shape=(224, 224, 3)\n)\nnew_input = rs50.input\nhidden_layer = rs50.layers[-1].output\nfeatures_extract = tf.keras.Model(new_input, hidden_layer) \n```", "```\n# from keras\nimage = load_img(\"./beach-surf.jpg\", target_size=(224, 224)) \nimage = img_to_array(image)\nimage = np.expand_dims(image, axis=0)  # batch of one\nimage = preprocess_input(image)  # from resnet\neval_img = features_extract.predict(image)\ncaption(eval_img) \n```"]