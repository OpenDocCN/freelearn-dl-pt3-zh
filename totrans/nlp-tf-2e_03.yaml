- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2vec – Learning Word Embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss a topic of paramount importance in NLP—Word2vec,
    a data-driven technique for learning powerful numerical representations (that
    is, vectors) of words or tokens in a language. Languages are complex. This warrants
    sound language understanding capabilities in the models we build to solve NLP
    problems. When transforming words to a numerical representation, a lot of methods
    aren’t able to sufficiently capture the semantics and contextual information that
    word carries. For example, the feature representation of the word *forest* should
    be very different from *oven* as these words are rarely used in similar contexts,
    whereas the representations of *forest* and *jungle* should be very similar. Not
    being able to capture this information leads to underperforming models.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec tries to overcome this problem by learning word representations by
    consuming large amounts of text.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec is called a *distributed representation*, as the semantics of the word
    are captured by the activation pattern of the full representation vector, in contrast
    to a single element of the representation vector (for example, setting a single
    element in the vector to 1 and rest to 0 for a single word).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn the mechanics of several Word2vec algorithms.
    But first, we will discuss the classical approaches to solving this problem and
    their limitations. This then motivates us to look at learning neural-network-based
    Word2vec algorithms that deliver state-of-the-art performance when finding good
    word representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will train a model on a dataset and analyze the representations learned
    by the model. We visualize (using t-SNE, a visualization technique for high-dimensional
    data) these learned word embeddings for a set of words on a 2D canvas in *Figure
    3.1*. If you take a closer look, you will see that similar things are placed close
    to each other (for example, numbers in the cluster in the middle):'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_01.png](img/B14070_03_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3\. 1: An example visualization of learned word embeddings using t-SNE'
  prefs: []
  type: TYPE_NORMAL
- en: '**t-Distributed Stochastic Neighbor Embedding** (**t-SNE**)'
  prefs: []
  type: TYPE_NORMAL
- en: This is a dimensionality reduction technique that projects high-dimensional
    data to a two-dimensional space. This allows us to imagine how high-dimensional
    data is distributed in space, because humans are generally not so good at intuitively
    understanding data in more than three dimensions. You will learn about t-SNE in
    more detail in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter covers this information through the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a word representation or meaning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classical approaches to learning word representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Word2vec – a neural network-based approach to learning word representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The skip-gram algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Continuous Bag-of-Words algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained a thorough understanding of
    how the history of word representations has led to Word2vec, how to utilize two
    different Word2vec algorithms, and the vital importance of Word2vec for NLP.
  prefs: []
  type: TYPE_NORMAL
- en: What is a word representation or meaning?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is meant by the word *meaning*? This is more of a philosophical question
    than a technical one. So, we will not try to discern the best answer for this
    question, but accept a more modest answer, that is, *meaning* is the idea conveyed
    by or some representation associated with a word. For example, when you hear the
    word “cat” you conjure up a mental picture of something that meows, has four legs,
    has a tail, and so on; then, if you hear the word “dog,” you again formulate a
    mental image of something that barks, has a bigger body than a cat, has four legs,
    has a tail, and so on. In this new space (that is, the mental pictures), it is
    easier for you to understand that cats and dogs are similar than by just looking
    at the words. Since the primary objective of NLP is to achieve human-like performance
    in linguistic tasks, it is sensible to explore principled ways of representing
    words for machines. To achieve this, we will use algorithms that can analyze a
    given text corpus and come up with good numerical representations of words (that
    is, word embeddings) such that words that fall within similar contexts (for example,
    *one* and *two*, *I* and *we*) will have similar numerical representations compared
    to words that are unrelated (for example, *cat* and *volcano*).
  prefs: []
  type: TYPE_NORMAL
- en: First, we will discuss some classical approaches to achieve this and then move
    on to understanding recent, more sophisticated methods that use neural networks
    to learn feature representations and deliver state-of-the-art performance.
  prefs: []
  type: TYPE_NORMAL
- en: Classical approaches to learning word representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss some of the classical approaches used for numerically
    representing words. It is important to have an understanding of the alternatives
    to word vectors, as these methods are still used in the real world, especially
    when limited data is available.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, we will discuss common representations, such as **one-hot
    encoding** and **Term Frequency-Inverse Document Frequency** (**TF-IDF**).
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoded representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the simpler ways of representing words is to use the one-hot encoded
    representation. This means that if we have a vocabulary of size *V*, for each
    *i*^(th) word *w*[i], we will represent the word *w*[i] with a *V*-length vector
    [0, 0, 0, …, 0, 1, 0, …, 0, 0, 0] where the *i*^(th) element is 1 and other elements
    are 0\. As an example, consider this sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bob and Mary are good friends.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The one-hot encoded representation of each word might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Bob*: [1,0,0,0,0,0]'
  prefs: []
  type: TYPE_NORMAL
- en: '*and*: [0,1,0,0,0,0]'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mary*: [0,0,1,0,0,0]'
  prefs: []
  type: TYPE_NORMAL
- en: '*are*: [0,0,0,1,0,0]'
  prefs: []
  type: TYPE_NORMAL
- en: '*good*: [0,0,0,0,1,0]'
  prefs: []
  type: TYPE_NORMAL
- en: '*friends*: [0,0,0,0,0,1]'
  prefs: []
  type: TYPE_NORMAL
- en: However, as you might have already figured out, this representation has many
    drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: This representation does not encode the similarity between words in any way
    and completely ignores the context in which the words are used. Let’s consider
    the dot product between the word vectors as the similarity measure. The more similar
    two vectors are, the higher the dot product is for those two vectors. For example,
    the representation of the words *car* and *automobile* will have a similarity
    distance of 0, while *car* and *pencil* will also have the same value.
  prefs: []
  type: TYPE_NORMAL
- en: This method becomes extremely ineffective for large vocabularies. Also, for
    a typical NLP task, the vocabulary easily can exceed 50,000 words. Therefore,
    the word representation matrix for 50,000 words will result in a very sparse 50,000
    × 50,000 matrix.
  prefs: []
  type: TYPE_NORMAL
- en: However, one-hot encoding plays an important role even in state-of-the-art word
    embedding learning algorithms. We use one-hot encoding to represent words numerically
    and feed them into neural networks so that the neural networks can learn better
    and smaller numerical feature representations of the words.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoding is also known as a localist representation (the opposite to
    the distributed representation), as the feature representation is decided by the
    activation of a single element in the vector.
  prefs: []
  type: TYPE_NORMAL
- en: We will now discuss another technique for representing words, known as the TF-IDF
    method.
  prefs: []
  type: TYPE_NORMAL
- en: The TF-IDF method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**TF-IDF** is a frequency-based method that takes into account the frequency
    with which a word appears in a corpus. This is a word representation in the sense
    that it represents the importance of a specific word in a given document. Intuitively,
    the higher the frequency of the word, the more important that word is in the document.
    For example, in a document about cats, the word *cats* will appear more often
    than in a document that isn’t about cats. However, just calculating the frequency
    would not work because words such as *this* and *is* are very frequent in documents
    but do not contribute much information. TF-IDF takes this into consideration and
    gives values of near-zero for such common words.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, *TF* stands for term frequency and *IDF* stands for inverse document
    frequency:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF(w*[i]*)* = number of times w[i] appear / total number of words'
  prefs: []
  type: TYPE_NORMAL
- en: '*IDF(w*[i]*)* = log(total number of documents / number of documents with w[i]
    in it)'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF-IDF(w*[i]*)* = TF(w[i]) x IDF(w[i])'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do a quick exercise. Consider two documents:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Document 1: *This is about cats. Cats are great companions*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Document 2: *This is about dogs. Dogs are very loyal*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let’s crunch some numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF-IDF (cats, doc1)* = (2/8) * log(2/1) = 0.075'
  prefs: []
  type: TYPE_NORMAL
- en: '*TF-IDF (this, doc2)* = (1/8) * log(2/2) = 0.0'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the word *cats* is informative, while *this* is not. This is the
    desired behavior we needed in terms of measuring the importance of words.
  prefs: []
  type: TYPE_NORMAL
- en: Co-occurrence matrix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Co-occurrence matrices, unlike one-hot-encoded representations, encode the
    context information of words, but require maintaining a V × V matrix. To understand
    the co-occurrence matrix, let’s take two example sentences:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Jerry and Mary are friends*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Jerry buys flowers for Mary*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The co-occurrence matrix will look like the following matrix. We only show
    one half of the matrix, as it is symmetrical:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Jerry | and | Mary | are | friends | buys | flowers | for |'
  prefs: []
  type: TYPE_TB
- en: '| Jerry | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| and |  | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Mary |  |  | 0 | 1 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| are |  |  |  | 0 | 1 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| friends |  |  |  |  | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| buys |  |  |  |  |  | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| flowers |  |  |  |  |  |  | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| for |  |  |  |  |  |  |  | 0 |'
  prefs: []
  type: TYPE_TB
- en: However, it is not hard to see that maintaining such a co-occurrence matrix
    comes at a cost as the size of the matrix grows polynomially with the size of
    the vocabulary. Furthermore, it is not straightforward to incorporate a context
    window size larger than 1\. One option is to have a weighted count, where the
    weight for a word in the context deteriorates with the distance from the word
    of interest.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, these methods are very limited in their representational power.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the one-hot encoded method, all words will have the same vector
    distance to each other. The TF-IDF method represents a word with a single number
    and is unable to capture the semantics of words. Finally calculating the co-occurrence
    matrix is very expensive and provides limited information about a word’s context.
  prefs: []
  type: TYPE_NORMAL
- en: We end our discussion about simple representations of words here. In the following
    section, we will first develop an intuitive understanding of word embeddings by
    working through an example. Then we will define a loss function so that we can
    use machine learning to learn word embeddings. Also, we will discuss two Word2vec
    algorithms, namely, the **skip-gram** and **Continuous Bag-of-Words** (**CBOW**)
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive understanding of Word2vec – an approach to learning word representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: “You shall know a word by the company it keeps.”
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ''
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – J.R. Firth
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: This statement, uttered by J. R. Firth in 1957, lies at the very foundation
    of Word2vec, as Word2vec techniques use the context of a given word to learn its
    semantics.
  prefs: []
  type: TYPE_NORMAL
- en: Word2vec is a groundbreaking approach that allows computers to learn the meaning
    of words without any human intervention. Also, Word2vec learns numerical representations
    of words by looking at the words surrounding a given word.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can test the correctness of the preceding quote by imagining a real-world
    scenario. Imagine you are sitting an exam and you find this sentence in your first
    question: “Mary is a very stubborn child. Her pervicacious nature always gets
    her in trouble.” Now, unless you are very clever, you might not know what *pervicacious*
    means. In such a situation, you automatically will be compelled to look at the
    phrases surrounding the word of interest. In our example, *pervicacious* is surrounded
    by *stubborn*, *nature*, and *trouble*. Looking at these three words is enough
    to determine that pervicacious in fact means the state of being stubborn. I think
    this is adequate evidence to observe the importance of context for a word’s meaning.'
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s discuss the basics of Word2vec. As already mentioned, Word2vec learns
    the meaning of a given word by looking at its context and representing it numerically.
  prefs: []
  type: TYPE_NORMAL
- en: By *context*, we refer to a fixed number of words in front of and behind the
    word of interest. Let’s take a hypothetical corpus with *N* words. Mathematically,
    this can be represented by a sequence of words denoted by *w*[0], *w*[1], …, *w*[i],
    and *w*[N], where *w*[i] is the *i*^(th) word in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Next, if we want to find a good algorithm that is capable of learning word meanings,
    given a word, our algorithm should be able to predict the context words correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means that the following probability should be high for any given word
    *w*[i]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_001.png)'
  prefs: []
  type: TYPE_IMG
- en: To arrive at the right-hand side of the equation, we need to assume that given
    the target word (*w*[i]), the context words are independent of each other (for
    example, *w*[i-2] and *w*[i-1] are independent). Though not entirely true, this
    approximation makes the learning problem practical and works well in practice.
    Let’s go through an example to understand the computations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise: does queen = king – he + she?'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before proceeding further, let’s do a small exercise to understand how maximizing
    the previously mentioned probability leads to finding good meaning (or representations)
    of words. Consider the following very small corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '*There was a very rich king. He had a beautiful queen. She was very kind*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep the exercise simple, let’s do some manual preprocessing and remove
    the punctuation and the uninformative words:'
  prefs: []
  type: TYPE_NORMAL
- en: '*was rich king he had beautiful queen she was kind*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s form a set of tuples for each word with their context words in the
    format (*target word --> context word 1*, *context word 2*). We will assume a
    context window size of 1 on either side:'
  prefs: []
  type: TYPE_NORMAL
- en: '*was --> rich*'
  prefs: []
  type: TYPE_NORMAL
- en: '*rich --> was, king*'
  prefs: []
  type: TYPE_NORMAL
- en: '*king --> rich, he*'
  prefs: []
  type: TYPE_NORMAL
- en: '*he --> king, had*'
  prefs: []
  type: TYPE_NORMAL
- en: '*had --> he, beautiful*'
  prefs: []
  type: TYPE_NORMAL
- en: '*beautiful --> had, queen*'
  prefs: []
  type: TYPE_NORMAL
- en: '*queen --> beautiful, she*'
  prefs: []
  type: TYPE_NORMAL
- en: '*she --> queen, was*'
  prefs: []
  type: TYPE_NORMAL
- en: '*was --> she, kind*'
  prefs: []
  type: TYPE_NORMAL
- en: '*kind --> was*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, our goal is to be able to predict the words on the right given the
    word on the left. To do this, for a given word, the words on the right-side context
    should share a high numerical or geometrical similarity with the words on the
    left-side context. In other words, the word of interest should be conveyed by
    the surrounding words. Now let’s consider actual numerical vectors to understand
    how this works. For simplicity, let’s only consider the tuples highlighted in
    bold. Let’s begin by assuming the following for the word *rich*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*rich --> [0,0]*'
  prefs: []
  type: TYPE_NORMAL
- en: To be able to correctly predict *was* and *king* from *rich*, was and *king*
    should have high similarity with the word *rich*. The Euclidean distance will
    be used to measure the distance between words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try the following values for the words *king* and *rich*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*king --> [0,1]*'
  prefs: []
  type: TYPE_NORMAL
- en: '*was --> [-1,0]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This works out fine as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dist(rich,king)* = 1.0'
  prefs: []
  type: TYPE_NORMAL
- en: '*Dist(rich,was)* = 1.0'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *Dist* is the Euclidean distance between two words. This is illustrated
    in *Figure 3.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_04.jpg](img/B14070_03_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: The positioning of word vectors for the words “rich”, “was” and
    “king”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s consider the following tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '*king --> rich, he*'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have established the relationship between *king* and *rich* already. However,
    it is not done yet; the more we see a relationship, the closer these two words
    should be. So, let’s first adjust the vector of king so that it is a bit closer
    to *rich*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*king --> [0,0.8]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will need to add the word *he* to the picture. The word *he* should
    be closer to *king*. This is all the information that we have right now about
    the word *he: he --> [0.5,0.8]*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this moment, the graph with the words looks like *Figure 3.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_05.jpg](img/B14070_03_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: The positioning of word vectors for the words “rich”, “was”, “king,”
    and “he”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s proceed with the next two tuples: *queen --> beautiful, she* and
    *she --> queen, was*. Note that I have swapped the order of the tuples as this
    makes it easier for us to understand the example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*she --> queen, was*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will have to use our prior knowledge of English to proceed further.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a reasonable decision to place the word *she* the same distance from
    *was* that *he* is from *was*, because their usage in the context of the word
    *was* is equivalent. Therefore, let’s use this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*she --> [0.5,0.6]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will use the word *queen* close to the word *she: queen --> [0.0,0.6]*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated in *Figure 3.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_06.jpg](img/B14070_03_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: The positioning of word vectors for the words “rich,” “was,” “king,”
    “he,” “she,” and “queen”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we only have the following tuple:'
  prefs: []
  type: TYPE_NORMAL
- en: '*queen --> beautiful, she*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the word *beautiful* is found. It should be approximately the same distance
    from the words *queen* and *she*. Let’s use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*beautiful --> [0.25,0]*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we have the following graph depicting the relationships between words.
    When we observe *Figure 3.6*, it seems to be a very intuitive representation of
    the meanings of words:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_07.jpg](img/B14070_03_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.5: The positioning of word vectors for the words “rich,” “was,” “king,”
    “he,” “she,” “queen,” and “beautiful”'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s look at the question that has been lurking in our minds since the
    beginning of this exercise. Are the quantities in this equation equivalent: *queen*
    = *king* - *he* + *she*? Well, we’ve got all the resources that we’ll need to
    solve this mystery now. Let’s try the right-hand side of the equation first:'
  prefs: []
  type: TYPE_NORMAL
- en: = *king* – *he* + *she*
  prefs: []
  type: TYPE_NORMAL
- en: = *[0,0.8]* – *[0.5,0.8]* + *[0.5,0.6]*
  prefs: []
  type: TYPE_NORMAL
- en: = *[0,0.6]*
  prefs: []
  type: TYPE_NORMAL
- en: It all works out in the end. If you look at the word vector we obtained for
    the word *queen*, you see that this is exactly the same as the answer we deduced
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is a crude way to show how word embeddings are learned, and this
    might differ from the exact positions of word embeddings learned using an algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Also keep in mind that this is an unrealistically scaled-down exercise with
    regard to what a real-world corpus might look like. So, you will not be able to
    work out these values by hand just by crunching a dozen numbers. Sophisticated
    function approximators such as neural networks do this job for us. But, to use
    neural networks, we need to formulate our problem in a mathematically assertive
    way. However, this is a good exercise to show the power of word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a good understanding of how Word2vec enables us to learn word
    representations, let’s look at the actual algorithms Word2vec utilizes in the
    next two sections.
  prefs: []
  type: TYPE_NORMAL
- en: The skip-gram algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first algorithm we will talk about is known as the **skip-gram algorithm**:
    a type of Word2vec algorithm. As we have discussed in numerous places, the meaning
    of a word can be elicited from the contextual words surrounding it. However, it
    is not entirely straightforward to develop a model that exploits this way of learning
    word meanings. The skip-gram algorithm, introduced by Mikolov et al. in 2013,
    is an algorithm that does exploit the context of the words in a written text to
    learn good word embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s go through the skip-gram algorithm step by step. First, we will discuss
    the data preparation process. Understanding the format of the data puts us in
    a great position to understand the algorithm. We will then discuss the algorithm
    itself. Finally, we will implement the algorithm using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: From raw text to semi-structured text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to design a mechanism to extract a dataset that can be fed to
    our learning model. Such a dataset should be a set of tuples of the format (target,
    context). Moreover, this needs to be created in an unsupervised manner. That is,
    a human should not have to manually engineer the labels for the data. In summary,
    the data preparation process should do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Capture the surrounding words of a given word (that is, the context)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run in an unsupervised manner
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The skip-gram model uses the following approach to design a dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given word *w*[i], a context window size of *m* is assumed. By *context
    window size*, we mean the number of words considered as context on a single side.
    Therefore, for *w*[i], the context window (including the target word *w*[i]) will
    be of size *2m+1* and will look like this: *[w*[i-m]*, …, w*[i-1]*, w*[i]*, w*[i+1]*,
    …, w*[i+m]*]*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, (target, context) tuples are formed as *[…, (w*[i]*, w*[i-m]*), …, (w*[i]*,w*[i-1]*),
    (w*[i]*,w*[i+1]*), …, (w*[i]*,w*[i+m]*), …]*; here, ![](img/B14070_03_002.png)
    and *N* is the number of words in the text. Let’s use the following sentence and
    a context window size (*m*) of 1:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The dog barked at the mailman*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, the dataset would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*[(dog, The), (dog, barked), (barked, dog), (barked, at), …, (the, at), (the,
    mailman)]*'
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is in the *(target, context)* format, we can use a neural network
    to learn the word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the skip-gram algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let’s identify the variables and notation we need to learn the word embeddings.
    To store the word embeddings, we need two V × D matrices, where *V* is the vocabulary
    size and *D* is the dimensionality of the word embeddings (that is, the number
    of elements in the vector that represent a single word). *D* is a user-defined
    hyperparameter. The higher *D* is, the more expressive the word embeddings learned
    will be. We need two matrices, one to represent the context words and one to represent
    the target words. These matrices will be referred to as the *context* *embedding
    space (or context embedding layer)* and the *target* *embedding space (or target
    embedding layer)*, or in general as the embedding space (or the embedding layer).
  prefs: []
  type: TYPE_NORMAL
- en: Each word will be represented with a unique ID in the range [1, *V+1*]. These
    IDs are passed to the embedding layer to look up corresponding vectors. To generate
    these IDs, we will use a special object called a Tokenizer that’s available in
    TensorFlow. Let’s refer to an example target-context tuple (w[i], w[j]), where
    the target word ID is *w*[i], and one of the context words is w[j]. The corresponding
    target embedding of *w*[i] is *t*[i], and the corresponding context embedding
    of *w*[j] is *c*[j]. Each target-context tuple is accompanied by a label (0 or
    1), denoted by *y*[i], where true target-context pairs will get a label of 1,
    and negative (or false) target-context candidates will get a label of 0\. It is
    easy to generate negative target-context candidates by sampling a word that does
    not appear in the context of a given target as the context word. We will talk
    about this in more detail later.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have defined the necessary variables. Next, for each input
    *w*[i], we will look up the embedding vectors from the context embedding layer
    corresponding to the input. This operation provides us with *c*[i], which is a
    *D*-sized vector (that is, a *D*-long embedding vector). We do the same for the
    input *w*[j], using the context embedding space to retrieve *c*[j]. Afterward,
    we calculate the prediction output for (*w*[i] *,w*[i]*)* using the following
    transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*logit(w*[i]*, w*[i]*)* = *c*[i] *.t*[j]'
  prefs: []
  type: TYPE_NORMAL
- en: '*ŷ*[ij] = *sigmoid(logit(w*[i]*, w*[i]*))*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, *logit(w*[i]*, w*[i]*)* represents the unnormalized scores (that is, logits),
    *ŷ*[i] is a single-valued predicted output (representing the probability of context
    word belonging in the context of the target word).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will visualize both the conceptual (*Figure 3.7*) and implementation (*Figure
    3.8*) views of the skip-gram model. Here is a summary of the notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*V*: This is the size of the vocabulary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D*: This is the dimensionality of the embedding layer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*[i]: Target word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*[j]: Context word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t*[i]: Target embedding of the word *w*[i]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*c*[j]: Context embedding of the word *w*[j]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[i]: This is the one-hot-encoded output word corresponding to *x*[i]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ŷ*[i]: This is the predicted output for *x*[i]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*logit(w*[i]*, w*[j]*)*: This is the unnormalized score for the input *x*[i]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B14070_03_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: The conceptual skip-gram model'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: The implementation of the skip-gram model'
  prefs: []
  type: TYPE_NORMAL
- en: Using both the existing and derived entities, we can now use the cross-entropy
    loss function to calculate the loss for a given data point *[(w*[i]*, w*[j]*),
    y*[i]*]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For binary labels, the cross-entropy loss for a single sample ![](img/B14070_03_003.png)
    is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/B14070_03_005.png) is the predicted label for ![](img/B14070_03_006.png).
    For multi-class classification problems, we generalize the loss by computing the
    term ![](img/B14070_03_007.png) for each class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_008.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B14070_03_009.png) represents the value of the ![](img/B14070_03_010.png)
    index of the ![](img/B14070_03_011.png), where ![](img/B14070_03_011.png) is a
    one hot encoded vector representing the label of the data point.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, when training neural networks, this loss is computed for each sample
    in a given batch, then averaged to compute the loss of the batch. Finally, the
    batch losses are averaged over all the batches in the dataset to compute the final
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why does the original word embeddings paper use two embedding layers?**'
  prefs: []
  type: TYPE_NORMAL
- en: The original paper (by Mikolov et al., 2013) uses two distinct V × D embedding
    spaces to denote words in the target space (words when used as the target) and
    words in the contextual space (words used as context words). One motivation to
    do this is that a word does not occur in its own context often. So, we want to
    minimize the probability of such things happening.
  prefs: []
  type: TYPE_NORMAL
- en: For example, for the target word *dog*, it is highly unlikely that the word
    *dog* is also found in its context (*P(dog*|*dog) ~ 0*). Intuitively, if we feed
    the (*w*[i]=*dog* and *w*[j]=*dog*) data point to the neural network, we are asking
    the neural network to give a higher loss if the neural network predicts *dog*
    as a context word of *dog*.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, we are asking the word embedding of the word *dog* to have a
    very high distance to the word embedding of the word *dog*. This creates a strong
    contradiction as the distance between the embeddings of the same word will be
    0\. Therefore, we cannot achieve this if we only have a single embedding space.
  prefs: []
  type: TYPE_NORMAL
- en: However, having two separate embedding spaces for target words and contextual
    words allows us to have this property because this way we have two separate embedding
    vectors for the same word. In practice, as long as you avoid feeding input-output
    tuples, having the same word as input and output allows us to work with a single
    embedding space and eliminates the need for two distinct embedding layers.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now implement the data generation process with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing and running the skip-gram algorithm with TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are now going to get our hands dirty with TensorFlow and implement the algorithm
    from end to end. First, we will discuss the data we’re going to use and how TensorFlow
    can help us to get that data in the format the model accepts. We will implement
    the skip-gram algorithm with TensorFlow and finally train the model and evaluate
    it on data that was prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the data generators with TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First, we will investigate how data can be generated in the correct format for
    the model. For this exercise, we are going to use the BBC news articles dataset
    available at [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).
    It contains 2,225 news articles belonging to 5 topics, business, entertainment,
    politics, sport, and tech, which were published on the BBC website between 2004-2005.
  prefs: []
  type: TYPE_NORMAL
- en: 'We write the function `download_data()` below to download the data to a given
    folder and extract it from its compressed format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The function first creates the `data_dir` if it doesn’t exist. Next, if the
    `bbc-fulltext.zip` file does not exist, it will be downloaded from the provided
    URL. If `bbc-fulltext.zip` has not been extracted yet, it will be extracted to
    `data_dir`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can call this function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we are going to focus on reading the data contained in the news
    articles (in `.txt` format) into the memory. To do that, we will define the `read_data()`
    function, which takes a data directory path (`data_dir`), and reads the `.txt`
    files (except for the README file) found in the data directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'With the `read_data()` function defined, let’s use it to read in the data and
    print some samples as well as some statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As we said at the beginning of this section, there are 2,225 stories with close
    to a million words. In the next step, we need to tokenize each story (in the form
    of a long string) to a list of tokens (or words). Along with that, we will perform
    some preprocessing on the text:'
  prefs: []
  type: TYPE_NORMAL
- en: Lowercase all the characters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove punctuation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these can be achieved with the `tensorflow.keras.preprocessing.text.Tokenizer`
    object. We can define a Tokenizer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you can see some of the most popular keyword arguments and their default
    values used when defining a Tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '`num_words` – Defines the size of the vocabulary. Defaults to `None`, meaning
    it will consider all the words appearing in the text corpus. If set to the integer
    n, it will only consider the n most common words appearing in the corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filters` – Defines any characters that need to be omitted during preprocessing.
    By default, it defines a string containing most of the common punctuation marks
    and symbols.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lower` – Defines whether the text needs to be converted to lowercase.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` – Defines the character that the words will be tokenized on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the Tokenizer is defined, you can call its `fit_on_texts()` method with
    a list of strings (where each string is a news article) so that the Tokenizer
    will learn the vocabulary and map the words to unique IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a moment to analyze what the Tokenizer has produced after it has
    been fitted on the text. Once it has been fitted, the Tokenizer will have two
    important attributes populated: `word_index` and `index_word`. Here `word_index`
    is a dictionary that maps each word to a unique ID. The `index_word` attribute
    is the opposite of `word_index`, that is, a dictionary that maps each unique word
    ID to the corresponding word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note how we are using the length of the `word_index` dictionary to derive the
    vocabulary size. We need an additional 1 as the ID 0 is a reserved ID and will
    not be used for any word. This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The more frequent a word is in the corpus, the lower the ID will be. Words
    such as “the”, “to” and “of” which tend to be common (and are called stop words)
    are in fact the most common words. As the next step, we are going to refine our
    Tokenizer object to have a limited-sized vocabulary. Because we are working with
    a relatively small corpus, we have to make sure the vocabulary is not too large,
    as it can lead to poorly learned word vectors due to the lack of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Since we have a total vocabulary of more than 30,000 words, we’ll restrict the
    size of the vocabulary to 15,000\. This means the Tokenizer will only keep the
    most common 15,000 words as the vocabulary. When we restrict a vocabulary this
    way, a new problem arises. As the Tokenizer’s vocabulary does not encompass all
    possible words in the true vocabulary, out-of-vocabulary words (or OOV words)
    can rear their heads. Some solutions are to replace OOV words with a special token
    (such as <`UNK`>) or remove them from the corpus. This is possible by passing
    the string you want to replace OOV tokens with to the `oov_token` argument in
    the Tokenizer. In this case, we will remove OOV words. If we are careful when
    setting the size of the vocabulary, omitting some of the rare words would not
    harm learning the context of words accurately.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can have a look at the transformation done on the text by the Tokenizer
    as follows. Let’s convert a string of the first 100 characters of the first story
    in our corpus (stored in the `news_stories` variable):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Then we can call the `tokenizer`'s `texts_to_sequences()` method to convert
    a list of documents (where each document is a string) to a list of list of word
    IDs (that is, each document is converted to a list of word IDs).
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have our Tokenizer sorted. There’s nothing left to do but to convert
    all of our news articles to sequences of word IDs with a single line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s move on to generating skip-grams using the `tf.keras.preprocessing.sequence.skipgrams()`
    function, provided by TensorFlow. We call the function on a sample phrase representing
    the first 5 words extracted from the first article in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Let’s consider a window size of 1\. This means, for a given target word, we
    define the context as one word from each side of the target word.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We have all the ingredients to define extract skip-grams from the sample phrase
    we chose as follows. When run, this function will output data in the exact format
    we need the data in, that is, (target-context) tuples as inputs and corresponding
    labels (0 or 1) as outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s take a moment to reflect on some of the important arguments that have
    been used:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence` `(list[str]` or `list[int])` – A list of words or word IDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocabulary_size` `(int)` – Size of the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` `(int)` – Size of the window to be considered for the context.
    `window_size` defines the length on each side.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_samples` `(int)` – Fraction of negative candidates to generate. For
    example, a value of 1 means there will be an equal number of positive and negative
    skipgram candidates. A value of 0 means there will not be any negative candidates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle` `(bool)` – Whether to shuffle the generated inputs or not.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`categorical (bool)` – Whether to produce labels as categorical (that is, one-hot
    encoded) or integers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_table` `(np.ndarray)` – An array of the same size as the vocabulary.
    An element in a given position in the array represents the probability of sampling
    the word indexed by that position in the Tokenizer’s word ID to word mapping.
    As we will see soon, this is a handy way to avoid common uninformative words being
    over-sampled much.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` `(int)` – If shuffling is enabled, this is the random seed to be used
    for shuffling.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the inputs and labels generated, let’s print some data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: For example, since the word “sales” appears in the context of the word “ad”,
    it is considered a positive candidate. On the other hand, since the word “racing”
    (randomly sampled from the vocabulary) does not appear in the context of the word
    “ad”, it is added as a negative candidate.
  prefs: []
  type: TYPE_NORMAL
- en: When selecting negative candidates, the `skipgrams()` function selects them
    randomly, giving uniform weights to all the words in the vocabulary. However,
    the original paper explains that this can lead to poor performance. A better strategy
    is to use the unigram distribution as a prior for selecting negative context words.
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be wondering what a unigram distribution is. It represents the frequency
    counts of unigrams (or tokens) found in the text. Then the frequency counts are
    easily converted to probabilities (or normalized frequencies) by dividing them
    by the sum of all frequencies. The most amazing thing is that you don’t have to
    compute this by hand for every corpus of text! It turns out that if you take any
    sufficiently large corpus of text, compute the normalized frequencies of unigrams,
    and order them from high to low, you’ll see that the corpus approximately follows
    a certain constant distribution. For the word with rank *math* in a corpus of
    *math* unigrams, the normalized frequency *f*[k] is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_013.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *math* is a hyperparameter that can be tuned to match the true distribution
    more closely. This is known as *Zipf’s law*. In other words, if you have a vocabulary
    where words are ranked (ID-ed) from most common to least common, you can approximate
    the normalized frequency of each word using Zipf’s law. We will be sampling words
    according to the probabilities output through Zipf’s law instead of giving equal
    probabilities to the words. This means words are sampled according to their presence
    (that is, the more frequent, the higher the chance of being sampled) in the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we can use the `tf.random.log_uniform_candidate_sampler()` function.
    This function takes a batch of positive context candidates of shape `[b, num_true]`,
    where `b` is the batch size and `num_true` is the number of true candidates per
    example (1 for the skip-gram model), and it outputs a [`num_sampled`] sized array,
    where `num_sampled` is the number of negative samples we need. We will discuss
    the nitty-gritty of this function soon, while going through an exercise. But let’s
    first generate some positive candidates using the `tf.keras.preprocessing.sequence.skipgrams()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we’re specifying `negative_samples=0`, as we will be generating negative
    samples with the candidate sampler. Let’s now discuss how we can use the `tf.random.log_uniform_candidate_sampler()`
    function to generate negative candidates. Here we will first use this function
    to generate negative candidates for a single word:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'This function takes the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`true_classes` `(np.ndarray` or `tf.Tensor)` – A tensor containing true target
    words. This needs to be a [`b, num_true`] sized array, where `num_true` denotes
    the number of true context candidates per example. Since we have one context word
    per example, this is 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_true` `(int)` – The number of true context terms per example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_sampled` `(int)` – The number of negative samples to generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unique` `(bool)` – Whether to generate unique samples or with replacement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`range_max` `(int)` – The size of the vocabulary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sampled_candidates` `(tf.Tensor)` – A tensor of size [`num_sampled`] containing
    negative candidates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`true_expected_count` `(tf.Tensor)` – A tensor of size [`b, num_true`]; the
    probability of each true candidate being sampled (according to Zipf’s law)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampled_expected_count` `(tf.Tensor)` – A tensor of size [`num_sampled`];
    the probabilities of each negative sample occurring along with true candidates,
    if sampled from the corpus'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will not worry too much about the latter two entities. The most important
    to us is `sampled_candidates`. When calling the function, we have to make sure
    `true_classes` has the shape `[b, num_true]`. In our case, we will run this in
    a single input word ID, which will be in the shape [1, 1]. It returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, putting everything together, let’s write a data generator function that
    generates batches of data for the model. This function, named `skip_gram_data_generator()`,
    takes the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` `(List[List[int]])` – A list of list of word IDs. This is the output
    generated by the Tokenizer’s `texts_to_sequences()` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` `(int)` – The window size for the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` `(int)` – The batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_samples` `(int)` – The number of negative samples per example to
    generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocabulary_size` `(int)` – The vocabulary size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` – The random seed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It will return a batch of data containing:'
  prefs: []
  type: TYPE_NORMAL
- en: A batch of target word IDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of corresponding context word IDs (both positive and negative)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of labels (0 and 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function signature looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we are going to shuffle the news articles so that every time we generate
    data, they are fetched in a different order. This helps the model to generalize
    better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, for each text sequence in the corpus we generate positive skip grams.
    `positive_skip_grams` contains tuples of (target, context) word pairs in that
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we are passing a `sampling_table` argument. This is another strategy
    to enhance the performance of Word2vec models. `sampling_table` is simply an array
    that is the same size as your vocabulary and specifies a probability at each index
    of the array with which the word indexed by that index will be sampled during
    skip gram generation. This technique is known as subsampling. Each word *w*[i]
    is sampled with the probability given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_014.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *t* is a tunable parameter. It defaults to 0.00001 for a large enough
    corpus. In TensorFlow, you can generate this table easily as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'You don’t need the exact frequencies to compute the sampling table, as we can
    leverage Zipf’s law to approximate those frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For each tuple contained in `positive_skip_grams`, we generate `negative_samples`
    number of negative candidates. We then populate targets, contexts, and label lists
    with both positive and negative candidates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then convert these to arrays as follows and randomly shuffle the data.
    When shuffling, you have to make sure all the arrays are consistently shuffled.
    Otherwise, you will corrupt the labels associated with the inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, batches of data are generated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will look at the specifics of the model we’re going to use.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the skip-gram architecture with TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now walk through an implementation of the skip-gram algorithm that uses
    the TensorFlow library. The full exercise is available in `ch3_word2vec.ipynb`
    in the `Ch03-Word-Vectors` exercise directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s define the hyperparameters of the model. You are free to change
    these hyperparameters to see how they affect final performance (for example, `batch_size
    = 1024` or `batch_size = 2048`). However, since this is a simpler problem than
    the more complex real-world problems, you might not see any significant differences
    (unless you change them to extremes, for example, `batch_size = 1` or `num_sampled
    = 1`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Next, we define the model. To do this, we will be relying on the Functional
    API of Keras. We need to go beyond the simplest API, that is, the Sequential API,
    as this model requires two input streams (one for the context and one for the
    target).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start off with an import. Then we will clear any current running sessions,
    to make sure there aren’t any other models occupying the hardware:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define two input layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Note how the shape is defined as `()`. When defining the `shape` argument, the
    actual output shape will have a new undefined dimension (i.e. `None` sized) added.
    In other words, the final output shape will be `[None]`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define two embedding layers: a target embedding layer and a context
    embedding layer. These layers will be used to look up the embeddings for target
    and context word IDs that will be generated by the input generation function.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'With the embedding layers defined, let’s look up the embeddings for the word
    IDs that will be fed to the input layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: We now need to compute the dot product of `target_out` and `context_out`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we are going to use the `tf.keras.layers.Dot` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we define our model as a `tf.keras.models.Model` object, where we
    specify `inputs` and `outputs` arguments. `inputs` need to be one or more input
    layers, and `outputs` can be one or more outputs produced by a series of `tf.keras.layers`
    objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We compile the model using a loss function and an optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s see a summary of our model by calling the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Training and evaluating the model will be the next item on our agenda.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our training process is going to be very simple as we have defined a function
    to generate batches of data in the exact format the model needs them in. But before
    we go ahead with training the model, we need to think about how we evaluate word
    vector models. The idea of word vectors is that words sharing semantic similarity
    will have a smaller distance between them, whereas words with no similarity will
    be far apart. To compute the similarities between words, we can use the cosine
    distance. We picked a set of random word IDs and stored them in `valid_term_ids`
    during our hyperparameter discussion. We will implement a way to compute the closest
    `k` words to each of those terms at the end of every epoch.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we utilize Keras callbacks. Keras callbacks give you a way to execute
    some important operation(s) at the end of every training iteration, epoch, prediction
    step, and so on. You can see a full list of the available callbacks at [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks).
    Since we need a bespoke evaluation mechanism designed for word vectors, we will
    need to implement our own callback. Our callback will take a list of word IDs
    intended as the validation words, a model containing the embedding matrix, and
    a Tokenizer to decode word IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: The evaluation will be done at the end of a training epoch, therefore we will
    override the `on_epoch_end()` function. The function extracts the embeddings from
    the context embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: Then the embeddings are normalized to have a unit length. Afterward, embeddings
    corresponding to validation words are extracted to a separate matrix called `valid_embeddings`.
    Then the cosine distance is computed between the validation embeddings and all
    word embeddings, which results in a `[valid_size, vocabulary size]` sized matrix.
    From this, we extract the top `k` similar words and display them through `print`
    statements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the model can be trained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We are simply defining an instance of the callback first. Next, we train the
    model for several epochs. In each, we generate skip gram data (while shuffling
    the order of the articles) and call `skip_gram_model.fit()` on the data. Here’s
    the result after five epochs of training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we denote some of the most sensible word vectors learned. For example,
    we can see that two of the most similar words to the word “months” are “days”
    and “weeks”. The title “mr” is accompanied by male names such as “scott” and “tony”.
    The word “premier” appears as a similar word to “champion”. You can further experiment
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: Different negative candidate sampling methods available at [https://www.tensorflow.org/api_docs/python/tf/random](https://www.tensorflow.org/api_docs/python/tf/random)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different hyperparameter choices (such as the embedding size and the number
    of negative samples)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed the skip-gram algorithm from end to end. We saw
    how we can use functions in TensorFlow to transform data. Then we implemented
    the skip-gram architecture using layers in Keras and the Functional API. Finally,
    we trained the model and visually inspected its performance on some test data.
    We will now discuss another popular Word2vec algorithm known as the **Continuous
    Bag-of-Words** (**CBOW**) model.
  prefs: []
  type: TYPE_NORMAL
- en: The Continuous Bag-of-Words algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CBOW model works in a similar way to the skip-gram algorithm, with one
    significant change in the problem formulation. In the skip-gram model, we predict
    the context words from the target word. However, in the CBOW model, we predict
    the target word from contextual words. Let’s compare what data looks like for
    the skip-gram algorithm and the CBOW model by taking the previous example sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The dog barked at the mailman.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the skip-gram algorithm, the data tuples—*(input word, output word)*—might
    look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '*(dog, the)*, *(dog, barked)*, *(barked, dog)*, and so on'
  prefs: []
  type: TYPE_NORMAL
- en: 'For CBOW, the data tuples would look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*([the, barked], dog)*, *([dog, at], barked)*, and so on'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, the input of the CBOW has a dimensionality of 2 × m × D, where
    *m* is the context window size and *D* is the dimensionality of the embeddings.
    The conceptual model of CBOW is shown in *Figure 3.13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_29.png](img/B14070_03_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: The CBOW model'
  prefs: []
  type: TYPE_NORMAL
- en: We will not go into great detail about the intricacies of CBOW as it is quite
    similar to skip-gram. For example, once the embeddings are aggregated (that is,
    concatenated or summed), they flow through a softmax layer to finally compute
    the same loss as we did with the skip-gram algorithm. However, we will discuss
    the algorithm’s implementation (though not in depth) to get a clear understanding
    of how to properly implement CBOW. The full implementation of CBOW is available
    at `ch3_word2vec.ipynb` in the `Ch03-Word-Vectors` exercise folder.
  prefs: []
  type: TYPE_NORMAL
- en: Generating data for the CBOW algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, unlike for the skip-gram algorithm, we do not have a handy function
    to generate data for the CBOW algorithm at our disposal. Therefore, we will need
    to implement this function ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the implementation of this function (named `cbow_grams()`) in `ch3_word2vec.ipynb`
    in the `Ch03-Word-Vectors` folder. The procedure will be quite similar to the
    one we used for skip-grams. However, the format of the data will be slightly different.
    Therefore, we will discuss the format of the data returned by this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The function takes the same arguments as the `skip_gram_data_generator()` function
    we discussed earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` `(List[List[int]])` – A list of list of word IDs. This is the output
    generated by Tokenizer’s `texts_to_sequences()` function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` `(int)` – The window size for the context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` `(int)` – The batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_samples` `(int)` – The number of negative samples per example to
    generate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocabulary_size` `(int)` – The vocabulary size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` – The random seed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data returned also has a slightly different format. It will return a batch
    of data containing:'
  prefs: []
  type: TYPE_NORMAL
- en: A batch of target word IDs, these target words are both positive and negative.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of corresponding context word IDs. Unlike skip-grams, for CBOW, we need
    all the words in the context, not just one. For example, if we define a batch
    size of `b` and window size of `w`, this will be a `[b, 2w]` sized tensor.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch or labels (0 and 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now learn about the specifics of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing CBOW in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the same hyperparameters as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as before, let’s first clear out any remaining sessions, if there are
    any:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We define two input layers. Note how the second input layer is defined to have
    `2 x window_size` dimensions. This means the final shape of that layer will be
    `[None, 2 x window_size]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now define two embedding layers: one for the context words and one for
    the target words. We will feed the inputs from the input layers and produce `context_out`
    and `target_out`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'If you look at the shape of `context_out`, you will see that it has the shape
    `[None, 2, 128]`, where `2` is `2 x window_size`, due to taking the whole context
    around a word. This needs to be reduced to `[None, 128]` by taking the average
    of all the context words. This is done by using a Lambda layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass a `Lambda` function to the `tf.keras.layers.Lambda` layer to reduce
    the `context_out` tensor on the second dimension to produce a `[None, 128]` sized
    tensor. With both the `target_out` and `mean_context_out` tensors having the shape
    `[None, 128]`, we can compute the dot product of the two to produce an output
    tensor `[None, 1]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'With that, we can define the final model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Similar to `skip_gram_model`, we will compile `cbow_model` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Again, if you would like to see the summary of the model, you can run `cbow_model.summary()`.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model training is identical to how we trained the skip-gram model. First,
    let’s define a callback to find the top k words similar to the words defined in
    the `valid_term_ids` set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we train `cbow_model` for several epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'The output should look like the following. We have cherry-picked some of the
    most sensible word vectors learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: From visual inspection, it seems CBOW has learned some effective word vectors.
    Similar to the skip-gram model, it has picked words like “years” and “days” as
    similar to “months”. Numerical values such as “5bn” have “5m” and “7bn” around
    them. But it’s important to remember that visual inspection is just a quick and
    dirty way to evaluate word vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, word vectors are evaluated on some downstream tasks. One of the
    popular tasks is the word analogical reasoning task. It focuses on answering questions
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Athens is to Greece as Baghdad to ____*'
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is `Iraq`. How is the answer computed? If the word vectors are sensible,
    then:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Word2vec(Athens) – Word2vec(Greece) = Word2vec(Baghdad) – Word2vec(Iraq)`'
  prefs: []
  type: TYPE_NORMAL
- en: or
  prefs: []
  type: TYPE_NORMAL
- en: '`Word2vec(Iraq) = Word2vec(Baghdad) - Word2vec(Athens) + Word2vec(Greece)`'
  prefs: []
  type: TYPE_NORMAL
- en: The answer is computed as the vector given by `Word2vec(Baghdad) - Word2vec(Athens)
    + Word2vec(Greece)`. The next step for this analogy task would be to see if the
    most similar vector to the resulting vector is given by the word Iraq. This way,
    accuracy can be computed for an analogy reasoning task. However, we will not utilize
    this task in this chapter, as our dataset is not big enough to perform well in
    this task.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we conclude our discussion on the CBOW algorithm. Though CBOW shares similarities
    with the skip-gram algorithm, it had some architectural differences as well as
    differences in data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word embeddings have become an integral part of many NLP tasks and are widely
    used for tasks such as machine translation, chatbots, image caption generation,
    and language modeling. Not only do word embeddings act as a dimensionality reduction
    technique (compared to one-hot encoding), they also give a richer feature representation
    than other techniques. In this chapter, we discussed two popular neural-network-based
    methods for learning word representations, namely the skip-gram model and the
    CBOW model.
  prefs: []
  type: TYPE_NORMAL
- en: First, we discussed the classical approaches to this problem to develop an understanding
    of how word representations were learned in the past. We discussed various methods,
    such as using WordNet, building a co-occurrence matrix of the words, and calculating
    TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we explored neural-network-based word representation learning methods.
    First, we worked out an example by hand to understand how word embeddings or word
    vectors can be calculated to help us understand the computations involved.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discussed the first word-embedding learning algorithm—the skip-gram
    model. We then learned how to prepare the data to be used for learning. Later,
    we examined how to design a loss function that allows us to use word embeddings
    using the context words of a given word. Finally, we discussed how to implement
    the skip-gram algorithm using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Then we reviewed the next choice for learning word embeddings—the CBOW model.
    We also discussed how CBOW differs from the skip-gram model. Finally, we discussed
    a TensorFlow implementation of CBOW as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn several other word embedding learning techniques
    known as Global Vectors, or GloVe, and Embeddings from Language Models, or ELMo.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)'
  prefs: []
  type: TYPE_IMG
