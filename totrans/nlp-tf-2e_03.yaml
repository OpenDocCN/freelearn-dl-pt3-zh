- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Word2vec – Learning Word Embeddings
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Word2vec – 学习词嵌入
- en: In this chapter, we will discuss a topic of paramount importance in NLP—Word2vec,
    a data-driven technique for learning powerful numerical representations (that
    is, vectors) of words or tokens in a language. Languages are complex. This warrants
    sound language understanding capabilities in the models we build to solve NLP
    problems. When transforming words to a numerical representation, a lot of methods
    aren’t able to sufficiently capture the semantics and contextual information that
    word carries. For example, the feature representation of the word *forest* should
    be very different from *oven* as these words are rarely used in similar contexts,
    whereas the representations of *forest* and *jungle* should be very similar. Not
    being able to capture this information leads to underperforming models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论NLP中一个至关重要的话题——Word2vec，一种数据驱动的技术，用于学习语言中词或符号的强大数值表示（即向量）。语言是复杂的，这要求我们在构建解决NLP问题的模型时具备良好的语言理解能力。将词转换为数值表示时，许多方法无法充分捕捉词所携带的语义和上下文信息。例如，词*forest*的特征表示应与*oven*的表示有很大不同，因为这两个词很少在类似的语境中使用，而*forest*和*jungle*的表示应该非常相似。无法捕捉到这些信息会导致模型性能不佳。
- en: Word2vec tries to overcome this problem by learning word representations by
    consuming large amounts of text.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec试图通过大量文本学习词表示来克服这个问题。
- en: Word2vec is called a *distributed representation*, as the semantics of the word
    are captured by the activation pattern of the full representation vector, in contrast
    to a single element of the representation vector (for example, setting a single
    element in the vector to 1 and rest to 0 for a single word).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec被称为*分布式表示*，因为词的语义通过完整表示向量的激活模式来捕获，这与表示向量中的单一元素（例如，将向量中的单一元素设置为1，其余为0以表示单个词）不同。
- en: In this chapter, we will learn the mechanics of several Word2vec algorithms.
    But first, we will discuss the classical approaches to solving this problem and
    their limitations. This then motivates us to look at learning neural-network-based
    Word2vec algorithms that deliver state-of-the-art performance when finding good
    word representations.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习几个Word2vec算法的工作原理。但首先，我们将讨论解决此问题的经典方法及其局限性。然后，这促使我们研究基于神经网络的Word2vec算法，这些算法在找到良好的词表示时能够提供最先进的性能。
- en: 'We will train a model on a dataset and analyze the representations learned
    by the model. We visualize (using t-SNE, a visualization technique for high-dimensional
    data) these learned word embeddings for a set of words on a 2D canvas in *Figure
    3.1*. If you take a closer look, you will see that similar things are placed close
    to each other (for example, numbers in the cluster in the middle):'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在一个数据集上训练一个模型，并分析模型学习到的表示。我们使用t-SNE（一种用于高维数据可视化的技术）将这些学习到的词嵌入可视化，在*图3.1*的二维画布上展示。如果仔细观察，你会发现相似的事物被放置得很近（例如，中间聚集的数字）：
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_01.png](img/B14070_03_01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_01.png](img/B14070_03_01.png)'
- en: 'Figure 3\. 1: An example visualization of learned word embeddings using t-SNE'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：使用t-SNE可视化学习到的词嵌入示例
- en: '**t-Distributed Stochastic Neighbor Embedding** (**t-SNE**)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**t-分布式随机邻居嵌入**（**t-SNE**）'
- en: This is a dimensionality reduction technique that projects high-dimensional
    data to a two-dimensional space. This allows us to imagine how high-dimensional
    data is distributed in space, because humans are generally not so good at intuitively
    understanding data in more than three dimensions. You will learn about t-SNE in
    more detail in the next chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种降维技术，将高维数据投影到二维空间。这使我们能够想象高维数据在空间中的分布，因为人类通常不太擅长直观理解超过三维的数据。你将在下一章详细了解t-SNE。
- en: 'This chapter covers this information through the following main topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章通过以下几个主要主题涵盖此信息：
- en: What is a word representation or meaning?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是词表示或词义？
- en: Classical approaches to learning word representations
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习词表示的经典方法
- en: Word2vec – a neural network-based approach to learning word representation
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2vec – 基于神经网络的词表示学习方法
- en: The skip-gram algorithm
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跳字模型算法
- en: The Continuous Bag-of-Words algorithm
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续词袋模型算法
- en: By the end of this chapter, you will have gained a thorough understanding of
    how the history of word representations has led to Word2vec, how to utilize two
    different Word2vec algorithms, and the vital importance of Word2vec for NLP.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将全面了解单词表示的历史如何发展到Word2vec，如何使用两种不同的Word2vec算法，以及Word2vec在NLP中的至关重要性。
- en: What is a word representation or meaning?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是单词表示或意义？
- en: What is meant by the word *meaning*? This is more of a philosophical question
    than a technical one. So, we will not try to discern the best answer for this
    question, but accept a more modest answer, that is, *meaning* is the idea conveyed
    by or some representation associated with a word. For example, when you hear the
    word “cat” you conjure up a mental picture of something that meows, has four legs,
    has a tail, and so on; then, if you hear the word “dog,” you again formulate a
    mental image of something that barks, has a bigger body than a cat, has four legs,
    has a tail, and so on. In this new space (that is, the mental pictures), it is
    easier for you to understand that cats and dogs are similar than by just looking
    at the words. Since the primary objective of NLP is to achieve human-like performance
    in linguistic tasks, it is sensible to explore principled ways of representing
    words for machines. To achieve this, we will use algorithms that can analyze a
    given text corpus and come up with good numerical representations of words (that
    is, word embeddings) such that words that fall within similar contexts (for example,
    *one* and *two*, *I* and *we*) will have similar numerical representations compared
    to words that are unrelated (for example, *cat* and *volcano*).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: “*meaning*”这个词是什么意思？这是一个哲学性的问题，更多的是哲学性的问题，而非技术性的问题。所以，我们不会尝试去找出这个问题的最佳答案，而是接受一个更为谦逊的答案，即*meaning*是通过一个词所传达的想法或与之相关的某种表示。例如，当你听到“cat”这个词时，你会在脑海中浮现出一只会喵喵叫、有四条腿、有尾巴的动物的画面；接着，如果你听到“dog”这个词，你又会想象出一只会汪汪叫、比猫体型大、有四条腿、有尾巴的动物。在这个新空间中（即脑海中的画面），你比仅仅通过文字理解，更容易看出猫和狗之间的相似性。由于自然语言处理（NLP）的主要目标是在人类语言任务中实现类似人类的表现，因此探索为机器表示单词的有原则的方式是明智的。为了实现这一目标，我们将使用可以分析给定文本语料库并生成单词的良好数值表示的算法（即，词嵌入），这样那些处于相似语境中的单词（例如，*one*和*two*，*I*和*we*）将具有相似的数值表示，而与之无关的单词（例如，*cat*和*volcano*）则会有不同的表示。
- en: First, we will discuss some classical approaches to achieve this and then move
    on to understanding recent, more sophisticated methods that use neural networks
    to learn feature representations and deliver state-of-the-art performance.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将讨论一些经典的方法来实现这一点，然后转向理解近期更为复杂的利用神经网络学习特征表示并取得最先进表现的方法。
- en: Classical approaches to learning word representation
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 经典的单词表示学习方法
- en: In this section, we will discuss some of the classical approaches used for numerically
    representing words. It is important to have an understanding of the alternatives
    to word vectors, as these methods are still used in the real world, especially
    when limited data is available.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些用于数值表示单词的经典方法。了解单词向量的替代方法非常重要，因为这些方法在实际应用中仍然被使用，尤其是在数据有限的情况下。
- en: More specifically, we will discuss common representations, such as **one-hot
    encoding** and **Term Frequency-Inverse Document Frequency** (**TF-IDF**).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地，我们将讨论一些常见的表示方法，如**独热编码**和**词频-逆文档频率**（**TF-IDF**）。
- en: One-hot encoded representation
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 独热编码表示
- en: 'One of the simpler ways of representing words is to use the one-hot encoded
    representation. This means that if we have a vocabulary of size *V*, for each
    *i*^(th) word *w*[i], we will represent the word *w*[i] with a *V*-length vector
    [0, 0, 0, …, 0, 1, 0, …, 0, 0, 0] where the *i*^(th) element is 1 and other elements
    are 0\. As an example, consider this sentence:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 表示单词的一个简单方法是使用独热编码表示。这意味着，如果我们有一个大小为*V*的词汇表，对于每个第*i*个单词*w*[i]，我们将用一个长度为*V*的向量[0,
    0, 0, …, 0, 1, 0, …, 0, 0, 0]来表示这个单词，其中第*i*个元素是1，其他元素为0。例如，考虑以下句子：
- en: '*Bob and Mary are good friends.*'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bob和Mary是好朋友。*'
- en: 'The one-hot encoded representation of each word might look like this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个单词的独热编码表示可能如下所示：
- en: '*Bob*: [1,0,0,0,0,0]'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '*Bob*: [1,0,0,0,0,0]'
- en: '*and*: [0,1,0,0,0,0]'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '*and*: [0,1,0,0,0,0]'
- en: '*Mary*: [0,0,1,0,0,0]'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*Mary*: [0,0,1,0,0,0]'
- en: '*are*: [0,0,0,1,0,0]'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '*are*: [0,0,0,1,0,0]'
- en: '*good*: [0,0,0,0,1,0]'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*good*: [0,0,0,0,1,0]'
- en: '*friends*: [0,0,0,0,0,1]'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*friends*: [0,0,0,0,0,1]'
- en: However, as you might have already figured out, this representation has many
    drawbacks.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你可能已经发现的那样，这种表示有许多缺点。
- en: This representation does not encode the similarity between words in any way
    and completely ignores the context in which the words are used. Let’s consider
    the dot product between the word vectors as the similarity measure. The more similar
    two vectors are, the higher the dot product is for those two vectors. For example,
    the representation of the words *car* and *automobile* will have a similarity
    distance of 0, while *car* and *pencil* will also have the same value.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方式并没有编码单词之间的相似性，完全忽略了单词使用的上下文。让我们考虑单词向量之间的点积作为相似性度量。两个向量越相似，它们的点积就越高。例如，*car*
    和 *automobile* 的表示将具有 0 的相似性距离，而 *car* 和 *pencil* 的表示也将具有相同的值。
- en: This method becomes extremely ineffective for large vocabularies. Also, for
    a typical NLP task, the vocabulary easily can exceed 50,000 words. Therefore,
    the word representation matrix for 50,000 words will result in a very sparse 50,000
    × 50,000 matrix.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大词汇量的情况，这种方法变得非常低效。此外，对于典型的自然语言处理任务，词汇量很容易超过 50,000 个单词。因此，对于 50,000 个单词，词表示矩阵将生成一个非常稀疏的
    50,000 × 50,000 矩阵。
- en: However, one-hot encoding plays an important role even in state-of-the-art word
    embedding learning algorithms. We use one-hot encoding to represent words numerically
    and feed them into neural networks so that the neural networks can learn better
    and smaller numerical feature representations of the words.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一热编码在最先进的词嵌入学习算法中仍然发挥着重要作用。我们使用一热编码将单词表示为数字，并将其输入神经网络，以便神经网络能够更好地学习单词的更小的数字特征表示。
- en: One-hot encoding is also known as a localist representation (the opposite to
    the distributed representation), as the feature representation is decided by the
    activation of a single element in the vector.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 一热编码（one-hot encoding）也被称为局部表示（与分布式表示相对），因为特征表示是通过向量中单个元素的激活来决定的。
- en: We will now discuss another technique for representing words, known as the TF-IDF
    method.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将讨论另一种表示单词的技术，称为 TF-IDF 方法。
- en: The TF-IDF method
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF 方法
- en: '**TF-IDF** is a frequency-based method that takes into account the frequency
    with which a word appears in a corpus. This is a word representation in the sense
    that it represents the importance of a specific word in a given document. Intuitively,
    the higher the frequency of the word, the more important that word is in the document.
    For example, in a document about cats, the word *cats* will appear more often
    than in a document that isn’t about cats. However, just calculating the frequency
    would not work because words such as *this* and *is* are very frequent in documents
    but do not contribute much information. TF-IDF takes this into consideration and
    gives values of near-zero for such common words.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**TF-IDF** 是一种基于频率的方法，考虑到单词在语料库中出现的频率。这是一种词表示，表示了一个特定单词在给定文档中的重要性。直观地说，单词出现的频率越高，说明这个单词在文档中的重要性越大。例如，在关于猫的文档中，单词
    *cats* 会比在不涉及猫的文档中出现得更频繁。然而，仅仅计算频率是不够的，因为像 *this* 和 *is* 这样的单词在文档中非常频繁，但并没有提供太多信息。TF-IDF
    考虑了这一点，并给这些常见单词分配接近零的值。'
- en: 'Again, *TF* stands for term frequency and *IDF* stands for inverse document
    frequency:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，*TF* 代表词频（term frequency），*IDF* 代表逆文档频率（inverse document frequency）：
- en: '*TF(w*[i]*)* = number of times w[i] appear / total number of words'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF(w*[i]*)* = w[i] 出现的次数 / 单词总数'
- en: '*IDF(w*[i]*)* = log(total number of documents / number of documents with w[i]
    in it)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*IDF(w*[i]*)* = log(文档总数 / 包含 w[i] 的文档数量)'
- en: '*TF-IDF(w*[i]*)* = TF(w[i]) x IDF(w[i])'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF-IDF(w*[i]*)* = TF(w[i]) x IDF(w[i])'
- en: 'Let’s do a quick exercise. Consider two documents:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们做一个快速练习。考虑两个文档：
- en: 'Document 1: *This is about cats. Cats are great companions*.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文档 1: *This is about cats. Cats are great companions*.'
- en: 'Document 2: *This is about dogs. Dogs are very loyal*.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '文档 2: *This is about dogs. Dogs are very loyal*.'
- en: 'Now let’s crunch some numbers:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来做一些计算：
- en: '*TF-IDF (cats, doc1)* = (2/8) * log(2/1) = 0.075'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF-IDF (cats, doc1)* = (2/8) * log(2/1) = 0.075'
- en: '*TF-IDF (this, doc2)* = (1/8) * log(2/2) = 0.0'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '*TF-IDF (this, doc2)* = (1/8) * log(2/2) = 0.0'
- en: Therefore, the word *cats* is informative, while *this* is not. This is the
    desired behavior we needed in terms of measuring the importance of words.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单词 *cats* 是有信息量的，而 *this* 不是。这就是我们在衡量单词重要性时所需要的期望行为。
- en: Co-occurrence matrix
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 共现矩阵
- en: 'Co-occurrence matrices, unlike one-hot-encoded representations, encode the
    context information of words, but require maintaining a V × V matrix. To understand
    the co-occurrence matrix, let’s take two example sentences:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 共现矩阵不同于 one-hot 编码表示，它编码了词汇的上下文信息，但需要维持一个 V × V 的矩阵。为了理解共现矩阵，我们来看两个例子句子：
- en: '*Jerry and Mary are friends*.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*杰瑞和玛丽是朋友*。'
- en: '*Jerry buys flowers for Mary*.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*杰瑞为玛丽买花*。'
- en: 'The co-occurrence matrix will look like the following matrix. We only show
    one half of the matrix, as it is symmetrical:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 共现矩阵将如下所示。我们仅显示矩阵的一半，因为它是对称的：
- en: '|  | Jerry | and | Mary | are | friends | buys | flowers | for |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|  | Jerry | and | Mary | are | friends | buys | flowers | for |'
- en: '| Jerry | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Jerry | 0 | 1 | 0 | 0 | 0 | 1 | 0 | 0 |'
- en: '| and |  | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| and |  | 0 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| Mary |  |  | 0 | 1 | 0 | 0 | 0 | 1 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| Mary |  |  | 0 | 1 | 0 | 0 | 0 | 1 |'
- en: '| are |  |  |  | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| are |  |  |  | 0 | 1 | 0 | 0 | 0 |'
- en: '| friends |  |  |  |  | 0 | 0 | 0 | 0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| friends |  |  |  |  | 0 | 0 | 0 | 0 |'
- en: '| buys |  |  |  |  |  | 0 | 1 | 0 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| buys |  |  |  |  |  | 0 | 1 | 0 |'
- en: '| flowers |  |  |  |  |  |  | 0 | 1 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| flowers |  |  |  |  |  |  | 0 | 1 |'
- en: '| for |  |  |  |  |  |  |  | 0 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| for |  |  |  |  |  |  |  | 0 |'
- en: However, it is not hard to see that maintaining such a co-occurrence matrix
    comes at a cost as the size of the matrix grows polynomially with the size of
    the vocabulary. Furthermore, it is not straightforward to incorporate a context
    window size larger than 1\. One option is to have a weighted count, where the
    weight for a word in the context deteriorates with the distance from the word
    of interest.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，很容易看出，维持这样的共现矩阵是有成本的，因为随着词汇表大小的增加，矩阵的大小会呈多项式增长。此外，增加一个大于 1 的上下文窗口大小也并不简单。一种选择是使用加权计数，其中词汇在上下文中的权重随着与目标词的距离而减小。
- en: As you can see, these methods are very limited in their representational power.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这些方法在表示能力上非常有限。
- en: For example, in the one-hot encoded method, all words will have the same vector
    distance to each other. The TF-IDF method represents a word with a single number
    and is unable to capture the semantics of words. Finally calculating the co-occurrence
    matrix is very expensive and provides limited information about a word’s context.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 one-hot 编码方法中，所有单词之间的向量距离是相同的。TF-IDF 方法用一个单一的数字表示一个词，无法捕捉词汇的语义。最后，计算共现矩阵非常昂贵，并且提供的关于词汇上下文的信息有限。
- en: We end our discussion about simple representations of words here. In the following
    section, we will first develop an intuitive understanding of word embeddings by
    working through an example. Then we will define a loss function so that we can
    use machine learning to learn word embeddings. Also, we will discuss two Word2vec
    algorithms, namely, the **skip-gram** and **Continuous Bag-of-Words** (**CBOW**)
    algorithms.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里结束关于词汇简单表示的讨论。在接下来的部分中，我们将通过实例首先培养对词嵌入的直观理解。然后我们将定义一个损失函数，以便使用机器学习来学习词嵌入。此外，我们还将讨论两种
    Word2vec 算法，分别是 **skip-gram** 和 **连续词袋（CBOW）** 算法。
- en: An intuitive understanding of Word2vec – an approach to learning word representation
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对 Word2vec 的直观理解 —— 一种学习词汇表示的方法
- en: “You shall know a word by the company it keeps.”
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “你将通过一个词汇的伴侣知道它的含义。”
- en: ''
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – J.R. Firth
  id: totrans-74
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – J.R. Firth
- en: This statement, uttered by J. R. Firth in 1957, lies at the very foundation
    of Word2vec, as Word2vec techniques use the context of a given word to learn its
    semantics.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这句话由 J. R. Firth 在 1957 年说出，它是 Word2vec 的基础，因为 Word2vec 技术利用给定词汇的上下文来学习其语义。
- en: Word2vec is a groundbreaking approach that allows computers to learn the meaning
    of words without any human intervention. Also, Word2vec learns numerical representations
    of words by looking at the words surrounding a given word.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 是一种开创性的方法，它允许计算机在没有任何人工干预的情况下学习词汇的含义。此外，Word2vec 通过观察给定词汇周围的词汇来学习词汇的数值表示。
- en: 'We can test the correctness of the preceding quote by imagining a real-world
    scenario. Imagine you are sitting an exam and you find this sentence in your first
    question: “Mary is a very stubborn child. Her pervicacious nature always gets
    her in trouble.” Now, unless you are very clever, you might not know what *pervicacious*
    means. In such a situation, you automatically will be compelled to look at the
    phrases surrounding the word of interest. In our example, *pervicacious* is surrounded
    by *stubborn*, *nature*, and *trouble*. Looking at these three words is enough
    to determine that pervicacious in fact means the state of being stubborn. I think
    this is adequate evidence to observe the importance of context for a word’s meaning.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过想象一个真实场景来测试前面引述的正确性。想象你正在参加考试，在第一题中遇到这句话：“玛丽是一个非常固执的孩子。她固执的天性总是让她惹上麻烦。”现在，除非你非常聪明，否则你可能不知道
    *pervicacious* 的意思。在这种情况下，你会自动被迫查看周围的词组。在我们的例子中，*pervicacious* 被 *固执*，*天性* 和 *麻烦*
    包围。看这三个词就足够判断 *pervicacious* 其实意味着固执的状态。我认为这足以证明上下文对单词含义的重要性。
- en: Now let’s discuss the basics of Word2vec. As already mentioned, Word2vec learns
    the meaning of a given word by looking at its context and representing it numerically.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一下 Word2vec 的基础知识。正如前面提到的，Word2vec 通过观察给定单词的上下文来学习该单词的含义，并以数字的形式表示它。
- en: By *context*, we refer to a fixed number of words in front of and behind the
    word of interest. Let’s take a hypothetical corpus with *N* words. Mathematically,
    this can be represented by a sequence of words denoted by *w*[0], *w*[1], …, *w*[i],
    and *w*[N], where *w*[i] is the *i*^(th) word in the corpus.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 *上下文*，我们指的是单词前后固定数量的词。假设我们有一个包含 *N* 个单词的假设语料库。用数学表示，这可以表示为一个单词序列，记为 *w*[0]，*w*[1]，…，*w*[i]
    和 *w*[N]，其中 *w*[i] 是语料库中的第 *i* 个单词。
- en: Next, if we want to find a good algorithm that is capable of learning word meanings,
    given a word, our algorithm should be able to predict the context words correctly.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果我们想找到一个能够学习单词含义的好算法，给定一个单词，我们的算法应该能够正确预测上下文单词。
- en: 'This means that the following probability should be high for any given word
    *w*[i]:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，对于任何给定的单词 *w*[i]，以下概率应该很高：
- en: '![](img/B14070_03_001.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B14070_03_001.png)'
- en: To arrive at the right-hand side of the equation, we need to assume that given
    the target word (*w*[i]), the context words are independent of each other (for
    example, *w*[i-2] and *w*[i-1] are independent). Though not entirely true, this
    approximation makes the learning problem practical and works well in practice.
    Let’s go through an example to understand the computations.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得到等式的右边，我们需要假设，在给定目标单词（*w*[i]）的情况下，上下文单词彼此之间是独立的（例如，*w*[i-2] 和 *w*[i-1] 是独立的）。尽管这并不完全正确，但这种近似使得学习问题变得可行，并且在实践中效果良好。让我们通过一个例子来理解这些计算。
- en: 'Exercise: does queen = king – he + she?'
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习：queen = king - he + she 吗？
- en: 'Before proceeding further, let’s do a small exercise to understand how maximizing
    the previously mentioned probability leads to finding good meaning (or representations)
    of words. Consider the following very small corpus:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们做一个小练习，了解如何通过最大化之前提到的概率来找到单词的良好含义（或表示）。考虑以下一个非常小的语料库：
- en: '*There was a very rich king. He had a beautiful queen. She was very kind*.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*曾经有一位非常富有的国王。他有一位美丽的王后。她非常善良*。'
- en: 'To keep the exercise simple, let’s do some manual preprocessing and remove
    the punctuation and the uninformative words:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化练习，让我们手动预处理，去除标点符号和无信息的词：
- en: '*was rich king he had beautiful queen she was kind*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*曾经有富有的国王他有美丽的王后她很善良*'
- en: 'Now let’s form a set of tuples for each word with their context words in the
    format (*target word --> context word 1*, *context word 2*). We will assume a
    context window size of 1 on either side:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为每个单词及其上下文单词形成一组元组，格式为（*目标单词 --> 上下文单词1*，*上下文单词2*）。我们假设上下文窗口大小为两边各 1：
- en: '*was --> rich*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*was --> 富有*'
- en: '*rich --> was, king*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*富有 --> 曾经有, 国王*'
- en: '*king --> rich, he*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*国王 --> 富有, 他*'
- en: '*he --> king, had*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*he --> 国王, had*'
- en: '*had --> he, beautiful*'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*有 --> 他, 美丽的*'
- en: '*beautiful --> had, queen*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*美丽的 --> 有, 王后*'
- en: '*queen --> beautiful, she*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*王后 --> 美丽的, 她*'
- en: '*she --> queen, was*'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*她 --> 王后, 是*'
- en: '*was --> she, kind*'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*是 --> 她, 善良*'
- en: '*kind --> was*'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '*善良 --> 曾经有*'
- en: 'Remember, our goal is to be able to predict the words on the right given the
    word on the left. To do this, for a given word, the words on the right-side context
    should share a high numerical or geometrical similarity with the words on the
    left-side context. In other words, the word of interest should be conveyed by
    the surrounding words. Now let’s consider actual numerical vectors to understand
    how this works. For simplicity, let’s only consider the tuples highlighted in
    bold. Let’s begin by assuming the following for the word *rich*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '*rich --> [0,0]*'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: To be able to correctly predict *was* and *king* from *rich*, was and *king*
    should have high similarity with the word *rich*. The Euclidean distance will
    be used to measure the distance between words.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s try the following values for the words *king* and *rich*:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '*king --> [0,1]*'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '*was --> [-1,0]*'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: 'This works out fine as the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '*Dist(rich,king)* = 1.0'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '*Dist(rich,was)* = 1.0'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, *Dist* is the Euclidean distance between two words. This is illustrated
    in *Figure 3.3*:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_04.jpg](img/B14070_03_02.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.2: The positioning of word vectors for the words “rich”, “was” and
    “king”'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s consider the following tuple:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '*king --> rich, he*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 'We have established the relationship between *king* and *rich* already. However,
    it is not done yet; the more we see a relationship, the closer these two words
    should be. So, let’s first adjust the vector of king so that it is a bit closer
    to *rich*:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '*king --> [0,0.8]*'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will need to add the word *he* to the picture. The word *he* should
    be closer to *king*. This is all the information that we have right now about
    the word *he: he --> [0.5,0.8]*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'At this moment, the graph with the words looks like *Figure 3.4*:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_05.jpg](img/B14070_03_03.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.3: The positioning of word vectors for the words “rich”, “was”, “king,”
    and “he”'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s proceed with the next two tuples: *queen --> beautiful, she* and
    *she --> queen, was*. Note that I have swapped the order of the tuples as this
    makes it easier for us to understand the example:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '*she --> queen, was*'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will have to use our prior knowledge of English to proceed further.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a reasonable decision to place the word *she* the same distance from
    *was* that *he* is from *was*, because their usage in the context of the word
    *was* is equivalent. Therefore, let’s use this:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '*she --> [0.5,0.6]*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will use the word *queen* close to the word *she: queen --> [0.0,0.6]*.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated in *Figure 3.5*:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_06.jpg](img/B14070_03_04.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.4: The positioning of word vectors for the words “rich,” “was,” “king,”
    “he,” “she,” and “queen”'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we only have the following tuple:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '*queen --> beautiful, she*'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the word *beautiful* is found. It should be approximately the same distance
    from the words *queen* and *she*. Let’s use the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '*beautiful --> [0.25,0]*'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '*beautiful --> [0.25,0]*'
- en: 'Now we have the following graph depicting the relationships between words.
    When we observe *Figure 3.6*, it seems to be a very intuitive representation of
    the meanings of words:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了以下图表，描绘了词之间的关系。当我们观察*图3.6*时，它似乎是对单词含义的非常直观的表示：
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_07.jpg](img/B14070_03_05.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_07.jpg](img/B14070_03_05.png)'
- en: 'Figure 3.5: The positioning of word vectors for the words “rich,” “was,” “king,”
    “he,” “she,” “queen,” and “beautiful”'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：词向量在“rich”，“was”，“king”，“he”，“she”，“queen”，“beautiful”这些词上的位置
- en: 'Now, let’s look at the question that has been lurking in our minds since the
    beginning of this exercise. Are the quantities in this equation equivalent: *queen*
    = *king* - *he* + *she*? Well, we’ve got all the resources that we’ll need to
    solve this mystery now. Let’s try the right-hand side of the equation first:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下从一开始就萦绕在我们心中的问题。这些方程式中的数量是等价的吗：*皇后* = *国王* - *他* + *她*？好了，我们现在拥有了解开这个谜题所需的所有资源。让我们先尝试方程式的右侧：
- en: = *king* – *he* + *she*
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: = *国王* – *他* + *她*
- en: = *[0,0.8]* – *[0.5,0.8]* + *[0.5,0.6]*
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: = *[0,0.8]* – *[0.5,0.8]* + *[0.5,0.6]*
- en: = *[0,0.6]*
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: = *[0,0.6]*
- en: It all works out in the end. If you look at the word vector we obtained for
    the word *queen*, you see that this is exactly the same as the answer we deduced
    earlier.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最终一切都得到了验证。如果你看我们为单词*queen*得到的词向量，你会发现这与我们之前推导出的答案完全相同。
- en: Note that this is a crude way to show how word embeddings are learned, and this
    might differ from the exact positions of word embeddings learned using an algorithm.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这是一种粗略的方式来展示如何学习词嵌入，并且这可能与使用算法学习到的词嵌入的确切位置有所不同。
- en: Also keep in mind that this is an unrealistically scaled-down exercise with
    regard to what a real-world corpus might look like. So, you will not be able to
    work out these values by hand just by crunching a dozen numbers. Sophisticated
    function approximators such as neural networks do this job for us. But, to use
    neural networks, we need to formulate our problem in a mathematically assertive
    way. However, this is a good exercise to show the power of word vectors.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，请记住，这个练习在规模上不现实，与真实世界语料库的样子相比有很大的简化。因此，您无法仅通过手动计算几个数字来推导出这些值。复杂的函数逼近器，如神经网络，替我们完成了这项工作。但是，为了使用神经网络，我们需要以数学上严谨的方式来表述问题。然而，这个练习是展示词向量能力的一个很好的方法。
- en: Now that we have a good understanding of how Word2vec enables us to learn word
    representations, let’s look at the actual algorithms Word2vec utilizes in the
    next two sections.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对Word2vec如何帮助我们学习词表示有了更好的理解，接下来让我们看一下Word2vec在接下来的两节中使用的实际算法。
- en: The skip-gram algorithm
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跳字模型算法
- en: 'The first algorithm we will talk about is known as the **skip-gram algorithm**:
    a type of Word2vec algorithm. As we have discussed in numerous places, the meaning
    of a word can be elicited from the contextual words surrounding it. However, it
    is not entirely straightforward to develop a model that exploits this way of learning
    word meanings. The skip-gram algorithm, introduced by Mikolov et al. in 2013,
    is an algorithm that does exploit the context of the words in a written text to
    learn good word embeddings.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要讲解的第一个算法被称为**跳字模型算法**：一种Word2vec算法。如我们在多个地方讨论过的，单词的含义可以从其上下文单词中推断出来。然而，开发一个利用这种方式来学习单词含义的模型并不是完全直接的。由Mikolov等人在2013年提出的跳字模型算法，正是利用文本中单词的上下文来学习良好的词嵌入。
- en: Let’s go through the skip-gram algorithm step by step. First, we will discuss
    the data preparation process. Understanding the format of the data puts us in
    a great position to understand the algorithm. We will then discuss the algorithm
    itself. Finally, we will implement the algorithm using TensorFlow.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步步讲解跳字模型算法。首先，我们将讨论数据准备过程。了解数据的格式使我们能够更好地理解算法。然后，我们将讨论算法本身。最后，我们将使用TensorFlow实现该算法。
- en: From raw text to semi-structured text
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从原始文本到半结构化文本
- en: 'First, we need to design a mechanism to extract a dataset that can be fed to
    our learning model. Such a dataset should be a set of tuples of the format (target,
    context). Moreover, this needs to be created in an unsupervised manner. That is,
    a human should not have to manually engineer the labels for the data. In summary,
    the data preparation process should do the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设计一个机制来提取可以输入到学习模型的数据集。这样的数据集应该是（目标词，上下文词）格式的元组集合。此外，这一过程需要以无监督的方式进行。也就是说，不需要人工为数据手动标注标签。总之，数据准备过程应该完成以下工作：
- en: Capture the surrounding words of a given word (that is, the context)
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕捉给定单词的周围单词（即上下文）
- en: Run in an unsupervised manner
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以无监督的方式运行
- en: 'The skip-gram model uses the following approach to design a dataset:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: skip-gram 模型采用以下方法设计数据集：
- en: 'For a given word *w*[i], a context window size of *m* is assumed. By *context
    window size*, we mean the number of words considered as context on a single side.
    Therefore, for *w*[i], the context window (including the target word *w*[i]) will
    be of size *2m+1* and will look like this: *[w*[i-m]*, …, w*[i-1]*, w*[i]*, w*[i+1]*,
    …, w*[i+m]*]*.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的单词 *w*[i]，假设其上下文窗口大小为 *m*。所谓的*上下文窗口大小*，是指在单侧考虑的上下文单词数量。因此，对于 *w*[i]，上下文窗口（包括目标单词
    *w*[i]）的大小将为 *2m+1*，并且将呈现如下形式： *[w*[i-m]*, …, w*[i-1]*, w*[i]*, w*[i+1]*, …, w*[i+m]*]*。
- en: 'Next, (target, context) tuples are formed as *[…, (w*[i]*, w*[i-m]*), …, (w*[i]*,w*[i-1]*),
    (w*[i]*,w*[i+1]*), …, (w*[i]*,w*[i+m]*), …]*; here, ![](img/B14070_03_002.png)
    and *N* is the number of words in the text. Let’s use the following sentence and
    a context window size (*m*) of 1:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，(目标词, 上下文词) 的元组将被构建为 *[…, (w*[i]*, w*[i-m]*), …, (w*[i]*,w*[i-1]*), (w*[i]*,w*[i+1]*),
    …, (w*[i]*,w*[i+m]*), …]*；其中，![](img/B14070_03_002.png) 和 *N* 是文本中的单词数。让我们使用以下句子，设定上下文窗口大小（*m*）为
    1：
- en: '*The dog barked at the mailman*.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '*The dog barked at the mailman*。'
- en: 'For this example, the dataset would be as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，数据集将如下所示：
- en: '*[(dog, The), (dog, barked), (barked, dog), (barked, at), …, (the, at), (the,
    mailman)]*'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*[(dog, The), (dog, barked), (barked, dog), (barked, at), …, (the, at), (the,
    mailman)]*'
- en: Once the data is in the *(target, context)* format, we can use a neural network
    to learn the word embeddings.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据转化为 *(目标词, 上下文词)* 的格式，我们就可以使用神经网络来学习词向量。
- en: Understanding the skip-gram algorithm
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 skip-gram 算法
- en: First, let’s identify the variables and notation we need to learn the word embeddings.
    To store the word embeddings, we need two V × D matrices, where *V* is the vocabulary
    size and *D* is the dimensionality of the word embeddings (that is, the number
    of elements in the vector that represent a single word). *D* is a user-defined
    hyperparameter. The higher *D* is, the more expressive the word embeddings learned
    will be. We need two matrices, one to represent the context words and one to represent
    the target words. These matrices will be referred to as the *context* *embedding
    space (or context embedding layer)* and the *target* *embedding space (or target
    embedding layer)*, or in general as the embedding space (or the embedding layer).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要确定学习词向量所需的变量和符号。为了存储词向量，我们需要两个 V × D 的矩阵，其中 *V* 是词汇表的大小，*D* 是词向量的维度（即表示单个单词的向量中的元素个数）。*D*
    是一个用户定义的超参数。*D* 越大，学习到的词向量就越具有表现力。我们需要两个矩阵，一个用于表示上下文单词，另一个用于表示目标单词。这些矩阵将被称为 *上下文*
    *嵌入空间（或上下文嵌入层）* 和 *目标* *嵌入空间（或目标嵌入层）*，或者通常称为嵌入空间（或嵌入层）。
- en: Each word will be represented with a unique ID in the range [1, *V+1*]. These
    IDs are passed to the embedding layer to look up corresponding vectors. To generate
    these IDs, we will use a special object called a Tokenizer that’s available in
    TensorFlow. Let’s refer to an example target-context tuple (w[i], w[j]), where
    the target word ID is *w*[i], and one of the context words is w[j]. The corresponding
    target embedding of *w*[i] is *t*[i], and the corresponding context embedding
    of *w*[j] is *c*[j]. Each target-context tuple is accompanied by a label (0 or
    1), denoted by *y*[i], where true target-context pairs will get a label of 1,
    and negative (or false) target-context candidates will get a label of 0\. It is
    easy to generate negative target-context candidates by sampling a word that does
    not appear in the context of a given target as the context word. We will talk
    about this in more detail later.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have defined the necessary variables. Next, for each input
    *w*[i], we will look up the embedding vectors from the context embedding layer
    corresponding to the input. This operation provides us with *c*[i], which is a
    *D*-sized vector (that is, a *D*-long embedding vector). We do the same for the
    input *w*[j], using the context embedding space to retrieve *c*[j]. Afterward,
    we calculate the prediction output for (*w*[i] *,w*[i]*)* using the following
    transformation:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '*logit(w*[i]*, w*[i]*)* = *c*[i] *.t*[j]'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '*ŷ*[ij] = *sigmoid(logit(w*[i]*, w*[i]*))*'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Here, *logit(w*[i]*, w*[i]*)* represents the unnormalized scores (that is, logits),
    *ŷ*[i] is a single-valued predicted output (representing the probability of context
    word belonging in the context of the target word).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'We will visualize both the conceptual (*Figure 3.7*) and implementation (*Figure
    3.8*) views of the skip-gram model. Here is a summary of the notation:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '*V*: This is the size of the vocabulary'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*D*: This is the dimensionality of the embedding layer'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*[i]: Target word'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*[j]: Context word'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t*[i]: Target embedding of the word *w*[i]'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*c*[j]: Context embedding of the word *w*[j]'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[i]: This is the one-hot-encoded output word corresponding to *x*[i]'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ŷ*[i]: This is the predicted output for *x*[i]'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*logit(w*[i]*, w*[j]*)*: This is the unnormalized score for the input *x*[i]'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B14070_03_06.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.6: The conceptual skip-gram model'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_07.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.7: The implementation of the skip-gram model'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Using both the existing and derived entities, we can now use the cross-entropy
    loss function to calculate the loss for a given data point *[(w*[i]*, w*[j]*),
    y*[i]*]*.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'For binary labels, the cross-entropy loss for a single sample ![](img/B14070_03_003.png)
    is computed as:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_004.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/B14070_03_005.png) is the predicted label for ![](img/B14070_03_006.png).
    For multi-class classification problems, we generalize the loss by computing the
    term ![](img/B14070_03_007.png) for each class:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_008.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B14070_03_009.png) represents the value of the ![](img/B14070_03_010.png)
    index of the ![](img/B14070_03_011.png), where ![](img/B14070_03_011.png) is a
    one hot encoded vector representing the label of the data point.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ![](img/B14070_03_009.png) 表示 ![](img/B14070_03_010.png) 索引的值，![](img/B14070_03_011.png)
    是表示数据点标签的一维独热编码向量。
- en: Typically, when training neural networks, this loss is computed for each sample
    in a given batch, then averaged to compute the loss of the batch. Finally, the
    batch losses are averaged over all the batches in the dataset to compute the final
    loss.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在训练神经网络时，这个损失值是针对给定批次中的每个样本计算的，然后取平均以计算批次的损失值。最后，批次的损失值在数据集中的所有批次上取平均，以计算最终的损失值。
- en: '**Why does the original word embeddings paper use two embedding layers?**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么原始的词嵌入论文使用了两个嵌入层？**'
- en: The original paper (by Mikolov et al., 2013) uses two distinct V × D embedding
    spaces to denote words in the target space (words when used as the target) and
    words in the contextual space (words used as context words). One motivation to
    do this is that a word does not occur in its own context often. So, we want to
    minimize the probability of such things happening.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 原始论文（由Mikolov等人，2013年）使用了两个不同的V × D嵌入空间来表示目标空间中的单词（作为目标使用的单词）和上下文空间中的单词（作为上下文单词使用的单词）。这样做的一个动机是单词在自己的上下文中出现的频率较低。因此，我们希望尽量减少这种情况发生的概率。
- en: For example, for the target word *dog*, it is highly unlikely that the word
    *dog* is also found in its context (*P(dog*|*dog) ~ 0*). Intuitively, if we feed
    the (*w*[i]=*dog* and *w*[j]=*dog*) data point to the neural network, we are asking
    the neural network to give a higher loss if the neural network predicts *dog*
    as a context word of *dog*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于目标单词 *dog*，很少可能在它的上下文中也出现单词 *dog*（*P(dog*|*dog) ~ 0*）。直观地说，如果我们将数据点（*w*[i]=*dog*
    和 *w*[j]=*dog*）输入神经网络，我们要求神经网络如果预测 *dog* 为 *dog* 的上下文单词时，给出较高的损失值。
- en: In other words, we are asking the word embedding of the word *dog* to have a
    very high distance to the word embedding of the word *dog*. This creates a strong
    contradiction as the distance between the embeddings of the same word will be
    0\. Therefore, we cannot achieve this if we only have a single embedding space.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们要求单词 *dog* 的词嵌入与单词 *dog* 的词嵌入之间有非常大的距离。这会产生一个强烈的矛盾，因为同一单词的嵌入之间的距离应该是0。因此，如果我们只有一个嵌入空间，无法实现这一点。
- en: However, having two separate embedding spaces for target words and contextual
    words allows us to have this property because this way we have two separate embedding
    vectors for the same word. In practice, as long as you avoid feeding input-output
    tuples, having the same word as input and output allows us to work with a single
    embedding space and eliminates the need for two distinct embedding layers.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，拥有两个独立的目标单词和上下文单词的嵌入空间使我们能够具备这一特性，因为这样我们为同一个单词拥有了两个独立的嵌入向量。在实践中，只要避免将输入输出元组相同，输入和输出都是同一个单词时，我们可以使用单一的嵌入空间，并且不需要两个独立的嵌入层。
- en: Let’s now implement the data generation process with TensorFlow.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用TensorFlow来实现数据生成过程。
- en: Implementing and running the skip-gram algorithm with TensorFlow
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现和运行skip-gram算法
- en: We are now going to get our hands dirty with TensorFlow and implement the algorithm
    from end to end. First, we will discuss the data we’re going to use and how TensorFlow
    can help us to get that data in the format the model accepts. We will implement
    the skip-gram algorithm with TensorFlow and finally train the model and evaluate
    it on data that was prepared.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将深入探索TensorFlow，并从头到尾实现该算法。首先，我们将讨论我们要使用的数据以及TensorFlow如何帮助我们将数据转换为模型所接受的格式。我们将使用TensorFlow实现skip-gram算法，最后训练模型并在准备好的数据上进行评估。
- en: Implementing the data generators with TensorFlow
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用TensorFlow实现数据生成器
- en: First, we will investigate how data can be generated in the correct format for
    the model. For this exercise, we are going to use the BBC news articles dataset
    available at [http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html).
    It contains 2,225 news articles belonging to 5 topics, business, entertainment,
    politics, sport, and tech, which were published on the BBC website between 2004-2005.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将研究如何以模型接受的正确格式生成数据。在这个练习中，我们将使用[http://mlg.ucd.ie/datasets/bbc.html](http://mlg.ucd.ie/datasets/bbc.html)中提供的BBC新闻文章数据集。该数据集包含2,225篇新闻文章，涵盖5个主题：商业、娱乐、政治、体育和科技，这些文章是在2004年至2005年间发布在BBC网站上的。
- en: 'We write the function `download_data()` below to download the data to a given
    folder and extract it from its compressed format:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The function first creates the `data_dir` if it doesn’t exist. Next, if the
    `bbc-fulltext.zip` file does not exist, it will be downloaded from the provided
    URL. If `bbc-fulltext.zip` has not been extracted yet, it will be extracted to
    `data_dir`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'We can call this function as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With that, we are going to focus on reading the data contained in the news
    articles (in `.txt` format) into the memory. To do that, we will define the `read_data()`
    function, which takes a data directory path (`data_dir`), and reads the `.txt`
    files (except for the README file) found in the data directory:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'With the `read_data()` function defined, let’s use it to read in the data and
    print some samples as well as some statistics:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will print the following:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As we said at the beginning of this section, there are 2,225 stories with close
    to a million words. In the next step, we need to tokenize each story (in the form
    of a long string) to a list of tokens (or words). Along with that, we will perform
    some preprocessing on the text:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: Lowercase all the characters
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove punctuation
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'All of these can be achieved with the `tensorflow.keras.preprocessing.text.Tokenizer`
    object. We can define a Tokenizer as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here, you can see some of the most popular keyword arguments and their default
    values used when defining a Tokenizer:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '`num_words` – Defines the size of the vocabulary. Defaults to `None`, meaning
    it will consider all the words appearing in the text corpus. If set to the integer
    n, it will only consider the n most common words appearing in the corpus.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`filters` – Defines any characters that need to be omitted during preprocessing.
    By default, it defines a string containing most of the common punctuation marks
    and symbols.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lower` – Defines whether the text needs to be converted to lowercase.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`split` – Defines the character that the words will be tokenized on.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once the Tokenizer is defined, you can call its `fit_on_texts()` method with
    a list of strings (where each string is a news article) so that the Tokenizer
    will learn the vocabulary and map the words to unique IDs:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let’s take a moment to analyze what the Tokenizer has produced after it has
    been fitted on the text. Once it has been fitted, the Tokenizer will have two
    important attributes populated: `word_index` and `index_word`. Here `word_index`
    is a dictionary that maps each word to a unique ID. The `index_word` attribute
    is the opposite of `word_index`, that is, a dictionary that maps each unique word
    ID to the corresponding word:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note how we are using the length of the `word_index` dictionary to derive the
    vocabulary size. We need an additional 1 as the ID 0 is a reserved ID and will
    not be used for any word. This will output the following:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The more frequent a word is in the corpus, the lower the ID will be. Words
    such as “the”, “to” and “of” which tend to be common (and are called stop words)
    are in fact the most common words. As the next step, we are going to refine our
    Tokenizer object to have a limited-sized vocabulary. Because we are working with
    a relatively small corpus, we have to make sure the vocabulary is not too large,
    as it can lead to poorly learned word vectors due to the lack of data:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since we have a total vocabulary of more than 30,000 words, we’ll restrict the
    size of the vocabulary to 15,000\. This means the Tokenizer will only keep the
    most common 15,000 words as the vocabulary. When we restrict a vocabulary this
    way, a new problem arises. As the Tokenizer’s vocabulary does not encompass all
    possible words in the true vocabulary, out-of-vocabulary words (or OOV words)
    can rear their heads. Some solutions are to replace OOV words with a special token
    (such as <`UNK`>) or remove them from the corpus. This is possible by passing
    the string you want to replace OOV tokens with to the `oov_token` argument in
    the Tokenizer. In this case, we will remove OOV words. If we are careful when
    setting the size of the vocabulary, omitting some of the rare words would not
    harm learning the context of words accurately.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 'We can have a look at the transformation done on the text by the Tokenizer
    as follows. Let’s convert a string of the first 100 characters of the first story
    in our corpus (stored in the `news_stories` variable):'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Then we can call the `tokenizer`'s `texts_to_sequences()` method to convert
    a list of documents (where each document is a string) to a list of list of word
    IDs (that is, each document is converted to a list of word IDs).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will print out:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We now have our Tokenizer sorted. There’s nothing left to do but to convert
    all of our news articles to sequences of word IDs with a single line of code:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s move on to generating skip-grams using the `tf.keras.preprocessing.sequence.skipgrams()`
    function, provided by TensorFlow. We call the function on a sample phrase representing
    the first 5 words extracted from the first article in the dataset:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This will output:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s consider a window size of 1\. This means, for a given target word, we
    define the context as one word from each side of the target word.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We have all the ingredients to define extract skip-grams from the sample phrase
    we chose as follows. When run, this function will output data in the exact format
    we need the data in, that is, (target-context) tuples as inputs and corresponding
    labels (0 or 1) as outputs:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s take a moment to reflect on some of the important arguments that have
    been used:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '`sequence` `(list[str]` or `list[int])` – A list of words or word IDs.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocabulary_size` `(int)` – Size of the vocabulary.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` `(int)` – Size of the window to be considered for the context.
    `window_size` defines the length on each side.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_samples` `(int)` – Fraction of negative candidates to generate. For
    example, a value of 1 means there will be an equal number of positive and negative
    skipgram candidates. A value of 0 means there will not be any negative candidates.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shuffle` `(bool)` – Whether to shuffle the generated inputs or not.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`categorical (bool)` – Whether to produce labels as categorical (that is, one-hot
    encoded) or integers.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampling_table` `(np.ndarray)` – An array of the same size as the vocabulary.
    An element in a given position in the array represents the probability of sampling
    the word indexed by that position in the Tokenizer’s word ID to word mapping.
    As we will see soon, this is a handy way to avoid common uninformative words being
    over-sampled much.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` `(int)` – If shuffling is enabled, this is the random seed to be used
    for shuffling.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With the inputs and labels generated, let’s print some data:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will produce:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For example, since the word “sales” appears in the context of the word “ad”,
    it is considered a positive candidate. On the other hand, since the word “racing”
    (randomly sampled from the vocabulary) does not appear in the context of the word
    “ad”, it is added as a negative candidate.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: When selecting negative candidates, the `skipgrams()` function selects them
    randomly, giving uniform weights to all the words in the vocabulary. However,
    the original paper explains that this can lead to poor performance. A better strategy
    is to use the unigram distribution as a prior for selecting negative context words.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be wondering what a unigram distribution is. It represents the frequency
    counts of unigrams (or tokens) found in the text. Then the frequency counts are
    easily converted to probabilities (or normalized frequencies) by dividing them
    by the sum of all frequencies. The most amazing thing is that you don’t have to
    compute this by hand for every corpus of text! It turns out that if you take any
    sufficiently large corpus of text, compute the normalized frequencies of unigrams,
    and order them from high to low, you’ll see that the corpus approximately follows
    a certain constant distribution. For the word with rank *math* in a corpus of
    *math* unigrams, the normalized frequency *f*[k] is given by:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_013.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: Here, *math* is a hyperparameter that can be tuned to match the true distribution
    more closely. This is known as *Zipf’s law*. In other words, if you have a vocabulary
    where words are ranked (ID-ed) from most common to least common, you can approximate
    the normalized frequency of each word using Zipf’s law. We will be sampling words
    according to the probabilities output through Zipf’s law instead of giving equal
    probabilities to the words. This means words are sampled according to their presence
    (that is, the more frequent, the higher the chance of being sampled) in the corpus.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we can use the `tf.random.log_uniform_candidate_sampler()` function.
    This function takes a batch of positive context candidates of shape `[b, num_true]`,
    where `b` is the batch size and `num_true` is the number of true candidates per
    example (1 for the skip-gram model), and it outputs a [`num_sampled`] sized array,
    where `num_sampled` is the number of negative samples we need. We will discuss
    the nitty-gritty of this function soon, while going through an exercise. But let’s
    first generate some positive candidates using the `tf.keras.preprocessing.sequence.skipgrams()`
    function:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Note that we’re specifying `negative_samples=0`, as we will be generating negative
    samples with the candidate sampler. Let’s now discuss how we can use the `tf.random.log_uniform_candidate_sampler()`
    function to generate negative candidates. Here we will first use this function
    to generate negative candidates for a single word:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This function takes the following arguments:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '`true_classes` `(np.ndarray` or `tf.Tensor)` – A tensor containing true target
    words. This needs to be a [`b, num_true`] sized array, where `num_true` denotes
    the number of true context candidates per example. Since we have one context word
    per example, this is 1.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_true` `(int)` – The number of true context terms per example.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`num_sampled` `(int)` – The number of negative samples to generate.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`unique` `(bool)` – Whether to generate unique samples or with replacement.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`range_max` `(int)` – The size of the vocabulary.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It returns:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '`sampled_candidates` `(tf.Tensor)` – A tensor of size [`num_sampled`] containing
    negative candidates'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`true_expected_count` `(tf.Tensor)` – A tensor of size [`b, num_true`]; the
    probability of each true candidate being sampled (according to Zipf’s law)'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sampled_expected_count` `(tf.Tensor)` – A tensor of size [`num_sampled`];
    the probabilities of each negative sample occurring along with true candidates,
    if sampled from the corpus'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will not worry too much about the latter two entities. The most important
    to us is `sampled_candidates`. When calling the function, we have to make sure
    `true_classes` has the shape `[b, num_true]`. In our case, we will run this in
    a single input word ID, which will be in the shape [1, 1]. It returns the following:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, putting everything together, let’s write a data generator function that
    generates batches of data for the model. This function, named `skip_gram_data_generator()`,
    takes the following arguments:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` `(List[List[int]])` – A list of list of word IDs. This is the output
    generated by the Tokenizer’s `texts_to_sequences()` function.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` `(int)` – The window size for the context.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` `(int)` – The batch size.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_samples` `(int)` – The number of negative samples per example to
    generate.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocabulary_size` `(int)` – The vocabulary size.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` – The random seed.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It will return a batch of data containing:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: A batch of target word IDs
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of corresponding context word IDs (both positive and negative)
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of labels (0 and 1)
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The function signature looks as follows:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'First, we are going to shuffle the news articles so that every time we generate
    data, they are fetched in a different order. This helps the model to generalize
    better:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, for each text sequence in the corpus we generate positive skip grams.
    `positive_skip_grams` contains tuples of (target, context) word pairs in that
    order:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Note that we are passing a `sampling_table` argument. This is another strategy
    to enhance the performance of Word2vec models. `sampling_table` is simply an array
    that is the same size as your vocabulary and specifies a probability at each index
    of the array with which the word indexed by that index will be sampled during
    skip gram generation. This technique is known as subsampling. Each word *w*[i]
    is sampled with the probability given by the following equation:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_03_014.png)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
- en: Here, *t* is a tunable parameter. It defaults to 0.00001 for a large enough
    corpus. In TensorFlow, you can generate this table easily as follows.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'You don’t need the exact frequencies to compute the sampling table, as we can
    leverage Zipf’s law to approximate those frequencies:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'For each tuple contained in `positive_skip_grams`, we generate `negative_samples`
    number of negative candidates. We then populate targets, contexts, and label lists
    with both positive and negative candidates:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We will then convert these to arrays as follows and randomly shuffle the data.
    When shuffling, you have to make sure all the arrays are consistently shuffled.
    Otherwise, you will corrupt the labels associated with the inputs:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, batches of data are generated as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Next, we will look at the specifics of the model we’re going to use.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the skip-gram architecture with TensorFlow
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will now walk through an implementation of the skip-gram algorithm that uses
    the TensorFlow library. The full exercise is available in `ch3_word2vec.ipynb`
    in the `Ch03-Word-Vectors` exercise directory.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s define the hyperparameters of the model. You are free to change
    these hyperparameters to see how they affect final performance (for example, `batch_size
    = 1024` or `batch_size = 2048`). However, since this is a simpler problem than
    the more complex real-world problems, you might not see any significant differences
    (unless you change them to extremes, for example, `batch_size = 1` or `num_sampled
    = 1`):'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Next, we define the model. To do this, we will be relying on the Functional
    API of Keras. We need to go beyond the simplest API, that is, the Sequential API,
    as this model requires two input streams (one for the context and one for the
    target).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start off with an import. Then we will clear any current running sessions,
    to make sure there aren’t any other models occupying the hardware:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will define two input layers:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note how the shape is defined as `()`. When defining the `shape` argument, the
    actual output shape will have a new undefined dimension (i.e. `None` sized) added.
    In other words, the final output shape will be `[None]`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we define two embedding layers: a target embedding layer and a context
    embedding layer. These layers will be used to look up the embeddings for target
    and context word IDs that will be generated by the input generation function.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'With the embedding layers defined, let’s look up the embeddings for the word
    IDs that will be fed to the input layers:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We now need to compute the dot product of `target_out` and `context_out`.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we are going to use the `tf.keras.layers.Dot` layer:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we define our model as a `tf.keras.models.Model` object, where we
    specify `inputs` and `outputs` arguments. `inputs` need to be one or more input
    layers, and `outputs` can be one or more outputs produced by a series of `tf.keras.layers`
    objects:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We compile the model using a loss function and an optimizer:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Let’s see a summary of our model by calling the following:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'This will output:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Training and evaluating the model will be the next item on our agenda.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the model
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our training process is going to be very simple as we have defined a function
    to generate batches of data in the exact format the model needs them in. But before
    we go ahead with training the model, we need to think about how we evaluate word
    vector models. The idea of word vectors is that words sharing semantic similarity
    will have a smaller distance between them, whereas words with no similarity will
    be far apart. To compute the similarities between words, we can use the cosine
    distance. We picked a set of random word IDs and stored them in `valid_term_ids`
    during our hyperparameter discussion. We will implement a way to compute the closest
    `k` words to each of those terms at the end of every epoch.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'For this, we utilize Keras callbacks. Keras callbacks give you a way to execute
    some important operation(s) at the end of every training iteration, epoch, prediction
    step, and so on. You can see a full list of the available callbacks at [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks).
    Since we need a bespoke evaluation mechanism designed for word vectors, we will
    need to implement our own callback. Our callback will take a list of word IDs
    intended as the validation words, a model containing the embedding matrix, and
    a Tokenizer to decode word IDs:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The evaluation will be done at the end of a training epoch, therefore we will
    override the `on_epoch_end()` function. The function extracts the embeddings from
    the context embedding layer.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: Then the embeddings are normalized to have a unit length. Afterward, embeddings
    corresponding to validation words are extracted to a separate matrix called `valid_embeddings`.
    Then the cosine distance is computed between the validation embeddings and all
    word embeddings, which results in a `[valid_size, vocabulary size]` sized matrix.
    From this, we extract the top `k` similar words and display them through `print`
    statements.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the model can be trained as follows:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We are simply defining an instance of the callback first. Next, we train the
    model for several epochs. In each, we generate skip gram data (while shuffling
    the order of the articles) and call `skip_gram_model.fit()` on the data. Here’s
    the result after five epochs of training:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here, we denote some of the most sensible word vectors learned. For example,
    we can see that two of the most similar words to the word “months” are “days”
    and “weeks”. The title “mr” is accompanied by male names such as “scott” and “tony”.
    The word “premier” appears as a similar word to “champion”. You can further experiment
    with:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Different negative candidate sampling methods available at [https://www.tensorflow.org/api_docs/python/tf/random](https://www.tensorflow.org/api_docs/python/tf/random)
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different hyperparameter choices (such as the embedding size and the number
    of negative samples)
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we discussed the skip-gram algorithm from end to end. We saw
    how we can use functions in TensorFlow to transform data. Then we implemented
    the skip-gram architecture using layers in Keras and the Functional API. Finally,
    we trained the model and visually inspected its performance on some test data.
    We will now discuss another popular Word2vec algorithm known as the **Continuous
    Bag-of-Words** (**CBOW**) model.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: The Continuous Bag-of-Words algorithm
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The CBOW model works in a similar way to the skip-gram algorithm, with one
    significant change in the problem formulation. In the skip-gram model, we predict
    the context words from the target word. However, in the CBOW model, we predict
    the target word from contextual words. Let’s compare what data looks like for
    the skip-gram algorithm and the CBOW model by taking the previous example sentence:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '*The dog barked at the mailman.*'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'For the skip-gram algorithm, the data tuples—*(input word, output word)*—might
    look like this:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '*(dog, the)*, *(dog, barked)*, *(barked, dog)*, and so on'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'For CBOW, the data tuples would look like the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '*([the, barked], dog)*, *([dog, at], barked)*, and so on'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'Consequently, the input of the CBOW has a dimensionality of 2 × m × D, where
    *m* is the context window size and *D* is the dimensionality of the embeddings.
    The conceptual model of CBOW is shown in *Figure 3.13*:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '![C:\Users\gauravg\Desktop\14070\CH03\B08681_03_29.png](img/B14070_03_08.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.8: The CBOW model'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: We will not go into great detail about the intricacies of CBOW as it is quite
    similar to skip-gram. For example, once the embeddings are aggregated (that is,
    concatenated or summed), they flow through a softmax layer to finally compute
    the same loss as we did with the skip-gram algorithm. However, we will discuss
    the algorithm’s implementation (though not in depth) to get a clear understanding
    of how to properly implement CBOW. The full implementation of CBOW is available
    at `ch3_word2vec.ipynb` in the `Ch03-Word-Vectors` exercise folder.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Generating data for the CBOW algorithm
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, unlike for the skip-gram algorithm, we do not have a handy function
    to generate data for the CBOW algorithm at our disposal. Therefore, we will need
    to implement this function ourselves.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: You can find the implementation of this function (named `cbow_grams()`) in `ch3_word2vec.ipynb`
    in the `Ch03-Word-Vectors` folder. The procedure will be quite similar to the
    one we used for skip-grams. However, the format of the data will be slightly different.
    Therefore, we will discuss the format of the data returned by this function.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'The function takes the same arguments as the `skip_gram_data_generator()` function
    we discussed earlier:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '`sequences` `(List[List[int]])` – A list of list of word IDs. This is the output
    generated by Tokenizer’s `texts_to_sequences()` function.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`window_size` `(int)` – The window size for the context.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` `(int)` – The batch size.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`negative_samples` `(int)` – The number of negative samples per example to
    generate.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vocabulary_size` `(int)` – The vocabulary size.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seed` – The random seed.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The data returned also has a slightly different format. It will return a batch
    of data containing:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: A batch of target word IDs, these target words are both positive and negative.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch of corresponding context word IDs. Unlike skip-grams, for CBOW, we need
    all the words in the context, not just one. For example, if we define a batch
    size of `b` and window size of `w`, this will be a `[b, 2w]` sized tensor.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A batch or labels (0 and 1).
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now learn about the specifics of the algorithm.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Implementing CBOW in TensorFlow
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the same hyperparameters as before:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Just as before, let’s first clear out any remaining sessions, if there are
    any:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We define two input layers. Note how the second input layer is defined to have
    `2 x window_size` dimensions. This means the final shape of that layer will be
    `[None, 2 x window_size]`:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Let’s now define two embedding layers: one for the context words and one for
    the target words. We will feed the inputs from the input layers and produce `context_out`
    and `target_out`:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'If you look at the shape of `context_out`, you will see that it has the shape
    `[None, 2, 128]`, where `2` is `2 x window_size`, due to taking the whole context
    around a word. This needs to be reduced to `[None, 128]` by taking the average
    of all the context words. This is done by using a Lambda layer:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We pass a `Lambda` function to the `tf.keras.layers.Lambda` layer to reduce
    the `context_out` tensor on the second dimension to produce a `[None, 128]` sized
    tensor. With both the `target_out` and `mean_context_out` tensors having the shape
    `[None, 128]`, we can compute the dot product of the two to produce an output
    tensor `[None, 1]`:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'With that, we can define the final model as follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Similar to `skip_gram_model`, we will compile `cbow_model` as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Again, if you would like to see the summary of the model, you can run `cbow_model.summary()`.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating the model
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The model training is identical to how we trained the skip-gram model. First,
    let’s define a callback to find the top k words similar to the words defined in
    the `valid_term_ids` set:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next, we train `cbow_model` for several epochs:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output should look like the following. We have cherry-picked some of the
    most sensible word vectors learned:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: From visual inspection, it seems CBOW has learned some effective word vectors.
    Similar to the skip-gram model, it has picked words like “years” and “days” as
    similar to “months”. Numerical values such as “5bn” have “5m” and “7bn” around
    them. But it’s important to remember that visual inspection is just a quick and
    dirty way to evaluate word vectors.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: 'Typically, word vectors are evaluated on some downstream tasks. One of the
    popular tasks is the word analogical reasoning task. It focuses on answering questions
    like:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '*Athens is to Greece as Baghdad to ____*'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'The answer is `Iraq`. How is the answer computed? If the word vectors are sensible,
    then:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '`Word2vec(Athens) – Word2vec(Greece) = Word2vec(Baghdad) – Word2vec(Iraq)`'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: or
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '`Word2vec(Iraq) = Word2vec(Baghdad) - Word2vec(Athens) + Word2vec(Greece)`'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: The answer is computed as the vector given by `Word2vec(Baghdad) - Word2vec(Athens)
    + Word2vec(Greece)`. The next step for this analogy task would be to see if the
    most similar vector to the resulting vector is given by the word Iraq. This way,
    accuracy can be computed for an analogy reasoning task. However, we will not utilize
    this task in this chapter, as our dataset is not big enough to perform well in
    this task.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Here, we conclude our discussion on the CBOW algorithm. Though CBOW shares similarities
    with the skip-gram algorithm, it had some architectural differences as well as
    differences in data.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word embeddings have become an integral part of many NLP tasks and are widely
    used for tasks such as machine translation, chatbots, image caption generation,
    and language modeling. Not only do word embeddings act as a dimensionality reduction
    technique (compared to one-hot encoding), they also give a richer feature representation
    than other techniques. In this chapter, we discussed two popular neural-network-based
    methods for learning word representations, namely the skip-gram model and the
    CBOW model.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: First, we discussed the classical approaches to this problem to develop an understanding
    of how word representations were learned in the past. We discussed various methods,
    such as using WordNet, building a co-occurrence matrix of the words, and calculating
    TF-IDF.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Next, we explored neural-network-based word representation learning methods.
    First, we worked out an example by hand to understand how word embeddings or word
    vectors can be calculated to help us understand the computations involved.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Next, we discussed the first word-embedding learning algorithm—the skip-gram
    model. We then learned how to prepare the data to be used for learning. Later,
    we examined how to design a loss function that allows us to use word embeddings
    using the context words of a given word. Finally, we discussed how to implement
    the skip-gram algorithm using TensorFlow.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: Then we reviewed the next choice for learning word embeddings—the CBOW model.
    We also discussed how CBOW differs from the skip-gram model. Finally, we discussed
    a TensorFlow implementation of CBOW as well.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn several other word embedding learning techniques
    known as Global Vectors, or GloVe, and Embeddings from Language Models, or ELMo.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
