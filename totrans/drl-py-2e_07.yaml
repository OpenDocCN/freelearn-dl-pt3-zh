- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Foundations
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in the previous chapters, we have learned how several reinforcement learning
    algorithms work and how they find the optimal policy. In the upcoming chapters,
    we will learn about **Deep Reinforcement Learning** (**DRL**), which is a combination
    of deep learning and reinforcement learning. To understand DRL, we need to have
    a strong foundation in deep learning. So, in this chapter, we will learn several
    important deep learning algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a subset of machine learning and it is all about neural networks.
    Deep learning has been around for a decade, but the reason it is so popular right
    now is because of the computational advancements and availability of huge volumes
    of data. With this huge volume of data, deep learning algorithms can outperform
    classic machine learning algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: We will start off the chapter by understanding what biological and artificial
    neurons are, and then we will learn about **Artificial Neural Networks** (**ANN**s)
    and how to implement them. Moving forward, we will learn about several interesting
    deep learning algorithms such as the **Recurrent Neural Network** (**RNN**), **Long
    Short-Term Memory** (**LSTM**), **Convolutional Neural Network** (**CNN**), and
    **Generative Adversarial Network** (**GAN**).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: Biological and artificial neurons
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ANNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM RNNs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin the chapter by understanding how biological and artificial neurons
    work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Biological and artificial neurons
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before going ahead, first, we will explore what neurons are and how neurons
    in our brain actually work, and then we will learn about artificial neurons.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: A **neuron** can be defined as the basic computational unit of the human brain.
    Neurons are the fundamental units of our brain and nervous system. Our brain encompasses
    approximately 100 billion neurons. Each and every neuron is connected to one another
    through a structure called a **synapse**, which is accountable for receiving input
    from the external environment via sensory organs, for sending motor instructions
    to our muscles, and for performing other activities.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: A neuron can also receive inputs from other neurons through a branchlike structure
    called a **dendrite**. These inputs are strengthened or weakened; that is, they
    are weighted according to their importance and then they are summed together in
    the cell body called the **soma**. From the cell body, these summed inputs are
    processed and move through the **axons** and are sent to the other neurons.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.1* shows a basic single biological neuron:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Biological neuron'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how artificial neurons work. Let''s suppose we have three inputs
    *x*[1], *x*[2], and *x*[3], to predict the output *y*. These inputs are multiplied
    by weights *w*[1], *w*[2], and *w*[3] and are summed together as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_001.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: 'But why are we multiplying these inputs by weights? Because all of the inputs
    are not equally important in calculating the output *y*. Let''s say that *x*[2]
    is more important in calculating the output compared to the other two inputs.
    Then, we assign a higher value to *w*[2] than the other two weights. So, upon
    multiplying weights with inputs, *x*[2] will have a higher value than the other
    two inputs. In simple terms, weights are used for strengthening the inputs. After
    multiplying inputs with the weights, we sum them together and we add a value called
    bias, *b*:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_002.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
- en: 'If you look at the preceding equation closely, it may look familiar. Doesn''t
    *z* look like the equation of linear regression? Isn''t it just the equation of
    a straight line? We know that the equation of a straight line is given as:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_003.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
- en: Here, *m* is the weights (coefficients), *x* is the input, and *b* is the bias
    (intercept).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, yes. Then, what is the difference between neurons and linear regression?
    In neurons, we introduce non-linearity to the result, *z*, by applying a function
    *f*(.) called the **activation** or **transfer function**. Thus, our output becomes:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_004.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.2* shows a single artificial neuron:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Artificial neuron'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: So, a neuron takes the input, *x*, multiples it by weights, *w,* and adds bias,
    *b,* forms *z*, and then we apply the activation function on *z* and get the output,
    *y*.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: ANN and its layers
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While neurons are really cool, we cannot just use a single neuron to perform
    complex tasks. This is the reason our brain has billions of neurons, stacked in
    layers, forming a network. Similarly, artificial neurons are arranged in layers.
    Each and every layer will be connected in such a way that information is passed
    from one layer to another.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical ANN consists of the following layers:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layer
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output layer
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each layer has a collection of neurons, and the neurons in one layer interact
    with all the neurons in the other layers. However, neurons in the same layer will
    not interact with one another. This is simply because neurons from the adjacent
    layers have connections or edges between them; however, neurons in the same layer
    do not have any connections. We use the term **nodes** or **units** to represent
    the neurons in the ANN.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.3* shows a typical ANN:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_03.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: ANN'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **input layer** is where we feed input to the network. The number of neurons
    in the input layer is the number of inputs we feed to the network. Each input
    will have some influence on predicting the output. However, no computation is
    performed in the input layer; it is just used for passing information from the
    outside world to the network.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layer
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any layer between the input layer and the output layer is called a **hidden
    layer**. It processes the input received from the input layer. The hidden layer
    is responsible for deriving complex relationships between input and output. That
    is, the hidden layer identifies the pattern in the dataset. It is majorly responsible
    for learning the data representation and for extracting the features.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层之间的任何层都称为**隐藏层**。它处理从输入层接收到的输入。隐藏层负责推导输入和输出之间的复杂关系。也就是说，隐藏层识别数据集中的模式。它主要负责学习数据表示和提取特征。
- en: There can be any number of hidden layers; however, we have to choose a number
    of hidden layers according to our use case. For a very simple problem, we can
    just use one hidden layer, but while performing complex tasks such as image recognition,
    we use many hidden layers, where each layer is responsible for extracting important
    features. The network is called a **deep neural network** when we have many hidden layers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层的数量可以是任意的；然而，我们必须根据使用场景选择隐藏层的数量。对于非常简单的问题，我们只需要使用一个隐藏层，但在执行复杂任务（如图像识别）时，我们使用许多隐藏层，每一层负责提取重要特征。当我们有许多隐藏层时，网络被称为**深度神经网络**。
- en: Output layer
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输出层
- en: After processing the input, the hidden layer sends its result to the output
    layer. As the name suggests, the output layer emits the output. The number of
    neurons in the output layer is based on the type of problem we want our network
    to solve.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完输入后，隐藏层将其结果发送到输出层。顾名思义，输出层发出最终输出。输出层的神经元数量取决于我们希望网络解决的问题类型。
- en: If it is a binary classification, then the number of neurons in the output layer
    is one, and it tells us which class the input belongs to. If it is a multi-class
    classification say, with five classes, and if we want to get the probability of
    each class as an output, then the number of neurons in the output layer is five,
    each emitting the probability. If it is a regression problem, then we have one
    neuron in the output layer.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果是二分类问题，则输出层中的神经元数量为 1，表示输入属于哪个类别。如果是多分类问题，比如有五个类别，并且我们希望得到每个类别的概率作为输出，则输出层中的神经元数量为五，每个神经元输出一个概率。如果是回归问题，则输出层只有一个神经元。
- en: Exploring activation functions
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索激活函数
- en: An **activation function**, also known as a **transfer function**, plays a vital
    role in neural networks. It is used to introduce non-linearity in neural networks.
    As we learned before, we apply the activation function to the input, which is
    multiplied by weights and added to the bias, that is, *f*(*z*), where *z = (input
    * weights) + bias* and *f*(.) is the activation function.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**激活函数**，也称为**传输函数**，在神经网络中起着至关重要的作用。它用于引入神经网络中的非线性。正如我们之前所学，我们将激活函数应用于输入，输入会被权重乘以并加上偏置，即*f*(*z*)，其中
    *z = (输入 * 权重) + 偏置*，*f*(.) 是激活函数。'
- en: If we do not apply the activation function, then a neuron simply resembles the
    linear regression. The aim of the activation function is to introduce a non-linear
    transformation to learn the complex underlying patterns in the data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不应用激活函数，那么神经元就仅仅类似于线性回归。激活函数的目的是引入非线性变换，以学习数据中复杂的潜在模式。
- en: Now let's look at some of the interesting commonly used activation functions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一些常用的激活函数。
- en: The sigmoid function
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid 函数
- en: 'The **sigmoid function** is one of the most commonly used activation functions.
    It scales the value between 0 and 1\. The sigmoid function can be defined as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sigmoid 函数**是最常用的激活函数之一。它将值缩放到 0 和 1 之间。Sigmoid 函数可以定义如下：'
- en: '![](img/B15558_07_005.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_005.png)'
- en: 'It is an S-shaped curve shown in *Figure 7.4*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 它是一个 S 形曲线，如*图 7.4*所示：
- en: '![](img/B15558_07_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_04.png)'
- en: 'Figure 7.4: Sigmoid function'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：Sigmoid 函数
- en: It is differentiable, meaning that we can find the slope of the curve at any
    two points. It is **monotonic**, which implies it is either entirely non-increasing
    or non-decreasing. The sigmoid function is also known as a **logistic** function.
    As we know that probability lies between 0 and 1, and since the sigmoid function
    squashes the value between 0 and 1, it is used for predicting the probability
    of output.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 它是可微的，这意味着我们可以在任意两个点之间找到曲线的斜率。它是**单调**的，这意味着它要么完全是非递增的，要么是非递减的。Sigmoid 函数也被称为**逻辑斯蒂**函数。我们知道概率值介于
    0 和 1 之间，而由于 Sigmoid 函数将值压缩到 0 到 1 之间，它被用于预测输出的概率。
- en: The tanh function
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: tanh 函数
- en: 'A **hyperbolic tangent** (**tanh**) function outputs the value between -1 to
    +1 and is expressed as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**双曲正切**（**tanh**）函数输出的值介于-1到+1之间，表示如下：'
- en: '![](img/B15558_07_006.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_006.png)'
- en: 'It also resembles the S-shaped curve. Unlike the sigmoid function, which is
    centered on 0.5, the tanh function is 0-centered, as shown in the following diagram:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 它也类似于S形曲线。与sigmoid函数中心在0.5不同，tanh函数是以0为中心，如下图所示：
- en: '![](img/B15558_07_05.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_05.png)'
- en: 'Figure 7.5: tanh function'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：tanh函数
- en: The Rectified Linear Unit function
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修正线性单元函数
- en: 'The **Rectified Linear Unit** (**ReLU**) function is another one of the most
    commonly used activation functions. It outputs a value from zero to infinity.
    It is basically a **piecewise** function and can be expressed as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**修正线性单元**（**ReLU**）函数是另一个最常用的激活函数之一。它输出一个从零到无穷大的值。它本质上是一个**分段**函数，可以表示如下：'
- en: '![](img/B15558_07_007.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_007.png)'
- en: 'That is, *f*(*x*) returns zero when the value of *x* is less than zero and
    *f*(*x*) returns *x* when the value of *x* is greater than or equal to zero. It
    can also be expressed as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，当*x*的值小于零时，*f*(*x*)返回零；当*x*的值大于或等于零时，*f*(*x*)返回*x*。它也可以表示如下：
- en: '![](img/B15558_07_008.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_008.png)'
- en: '*Figure 7.6* shows the ReLU function:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.6*显示了ReLU函数：'
- en: '![](img/B15558_07_06.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_06.png)'
- en: 'Figure 7.6: ReLU function'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：ReLU函数
- en: As we can see in the preceding diagram, when we feed any negative input to the
    ReLU function, it converts the negative input to zero.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的图示中看到的，当我们将任何负输入传递给ReLU函数时，它会将负输入转换为零。
- en: The softmax function
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: softmax函数
- en: The **softmax function** is basically the generalization of the sigmoid function.
    It is usually applied to the final layer of the network and while performing multi-class
    classification tasks. It gives the probabilities of each class for being output
    and thus, the sum of softmax values will always equal 1.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**softmax函数**本质上是sigmoid函数的推广。它通常应用于网络的最后一层，并且在执行多类分类任务时使用。它给出每个类别的输出概率，因此，softmax值的总和总是等于1。'
- en: 'It can be represented as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以表示如下：
- en: '![](img/B15558_07_009.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_009.png)'
- en: 'As shown in the *Figure 7.7*, the softmax function converts its inputs to probabilities:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图7.7*所示，softmax函数将其输入转换为概率：
- en: '![](img/B15558_07_07.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_07.png)'
- en: 'Figure 7.7: Softmax function'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：Softmax函数
- en: Now that we have learned about different activation functions, in the next section,
    we will learn about forward propagation in ANNs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了不同的激活函数，在接下来的部分中，我们将学习人工神经网络（ANNs）中的前向传播。
- en: Forward propagation in ANNs
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络中的前向传播
- en: 'In this section, we will see how an ANN learns where neurons are stacked up
    in layers. The number of layers in a network is equal to the number of hidden
    layers plus the number of output layers. We don''t take the input layer into account
    when calculating the number of layers in a network. Consider a two-layer neural
    network with one input layer, *x*, one hidden layer, *h*, and one output layer,
    *y*, as shown in the following diagram:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到人工神经网络如何学习，其中神经元堆叠在不同的层中。网络中的层数等于隐藏层的数量加上输出层的数量。在计算网络的层数时，我们不考虑输入层。考虑一个包含一个输入层*x*、一个隐藏层*h*和一个输出层*y*的两层神经网络，如下图所示：
- en: '![](img/B15558_07_08.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_08.png)'
- en: 'Figure 7.8: Forward propagation in ANN'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：ANN中的前向传播
- en: Let's consider we have two inputs, *x*[1] and *x*[2], and we have to predict
    the output, ![](img/B15558_07_010.png). Since we have two inputs, the number of
    neurons in the input layer is two. We set the number of neurons in the hidden
    layer to four, and the number of neurons in the output layer to one. Now, the
    inputs are multiplied by weights, and then we add bias and propagate the resultant
    value to the hidden layer where the activation function is applied.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个输入，*x*[1]和*x*[2]，我们需要预测输出，![](img/B15558_07_010.png)。由于我们有两个输入，输入层中的神经元数量为两个。我们将隐藏层的神经元数量设为四个，输出层的神经元数量设为一个。现在，输入会与权重相乘，然后我们加上偏差，并将结果值传递到隐藏层，在那里应用激活函数。
- en: Before that, we need to initialize the weight matrix. In the real world, we
    don't know which input is more important than the other so that we can weight
    them and compute the output. Therefore, we randomly initialize the weights and
    bias value. The weight and the bias value between the input to the hidden layer
    are represented by *W*[xh] and *b*[h], respectively. What about the dimensions
    of the weight matrix? The dimensions of the weight matrix must be *the number
    of neurons in the current layer* x *the number of neurons in the next layer*.
    Why is that?
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Because it is a basic matrix multiplication rule. To multiply any two matrices,
    *AB*, the number of columns in matrix *A* must be equal to the number of rows
    in matrix *B*. So, the dimension of the weight matrix, *W*[xh], should be *the
    number of neurons in the input layer* x *the number of neurons in the hidden layer*,
    that is, 2 x 4:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_011.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation represents, ![](img/B15558_07_012.png). Now, this is
    passed to the hidden layer. In the hidden layer, we apply an activation function
    to *z*[1]. Let''s use the sigmoid ![](img/B15558_07_013.png) activation function.
    Then, we can write:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_014.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
- en: 'After applying the activation function, we again multiply result *a*[1] by
    a new weight matrix and add a new bias value that is flowing between the hidden
    layer and the output layer. We can denote this weight matrix and bias as *W*[hy]
    and *b*[y], respectively. The dimension of the weight matrix, *W*[hy], will be
    *the number of neurons in the hidden layer* x *the number of neurons in the output
    layer*. Since we have four neurons in the hidden layer and one neuron in the output
    layer, the *W*[hy] matrix dimension will be 4 x 1\. So, we multiply *a*[1] by
    the weight matrix, *W*[hy], and add bias, *b*[y], and pass the result *z*[2] to
    the next layer, which is the output layer:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_015.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
- en: 'Now, in the output layer, we apply the sigmoid function to *z*[2], which will
    result in an output value:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_016.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'This whole process from the input layer to the output layer is known as **forward
    propagation**. Thus, in order to predict the output value, inputs are propagated
    from the input layer to the output layer. During this propagation, they are multiplied
    by their respective weights on each layer and an activation function is applied
    on top of them. The complete forward propagation steps are given as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_011.png)![](img/B15558_07_014.png)![](img/B15558_07_015.png)![](img/B15558_07_016.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
- en: 'The preceding forward propagation steps can be implemented in Python as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Forward propagation is cool, isn''t it? But how do we know whether the output
    generated by the neural network is correct? We define a new function called the
    **cost function** (*J*), also known as the **loss function** (*L*), which tells
    us how well our neural network is performing. There are many different cost functions.
    We will use the mean squared error as a cost function, which can be defined as
    the mean of the squared difference between the actual output and the predicted
    output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播很酷，对吧？但我们怎么知道神经网络生成的输出是否正确呢？我们定义了一个新的函数，称为**成本函数**（*J*），也叫**损失函数**（*L*），它告诉我们神经网络的表现如何。成本函数有很多种不同的类型。我们将使用均方误差作为成本函数，可以定义为实际输出和预测输出之间平方差的平均值：
- en: '![](img/B15558_07_021.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_021.png)'
- en: Here, *n* is the number of training samples, *y* is the actual output, and ![](img/B15558_07_022.png)
    is the predicted output.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*n*是训练样本的数量，*y*是实际输出，![](img/B15558_07_022.png)是预测输出。
- en: Okay, so we learned that a cost function is used for assessing our neural network;
    that is, it tells us how good our neural network is at predicting the output.
    But the question is where is our network actually learning? In forward propagation,
    the network is just trying to predict the output. But how does it learn to predict
    the correct output? In the next section, we will examine this.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们了解到成本函数是用来评估神经网络的，也就是说，它告诉我们神经网络在预测输出方面的好坏。但是问题是，我们的网络到底是怎么学习的呢？在正向传播中，网络只是试图预测输出。那么它是如何学习预测正确的输出呢？在下一部分，我们将探讨这个问题。
- en: How does an ANN learn?
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络是如何学习的？
- en: If the cost or loss is very high, then it means that our network is not predicting
    the correct output. So, our objective is to minimize the cost function so that
    our neural network predictions will be better. How can we minimize the cost function?
    That is, how can we minimize the loss/cost? We learned that the neural network
    makes predictions using forward propagation. So, if we can change some values
    in the forward propagation, we can predict the correct output and minimize the
    loss. But what values can we change in the forward propagation? Obviously, we
    can't change input and output. We are now left with weights and bias values. Remember
    that we just initialized weight matrices randomly. Since the weights are random,
    they are not going to be perfect. Now, we will update these weight matrices (*W*[xh]
    and *W*[hy]) in such a way that our neural network gives a correct output. How
    do we update these weight matrices? Here comes a new technique called **gradient
    descent**.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果成本或损失非常高，那么说明我们的网络没有预测出正确的输出。因此，我们的目标是最小化成本函数，使得神经网络的预测更加准确。我们如何最小化成本函数呢？也就是说，我们如何最小化损失/成本呢？我们已经了解到，神经网络是通过正向传播来进行预测的。那么，如果我们能够在正向传播中改变一些值，我们就能预测出正确的输出并最小化损失。但我们可以改变正向传播中的哪些值呢？显然，我们不能改变输入和输出。现在，我们剩下的就是权重和偏置值。记住，我们刚开始是随机初始化了权重矩阵。由于这些权重是随机的，它们不可能是完美的。现在，我们将更新这些权重矩阵（*W*[xh]
    和 *W*[hy]），使得我们的神经网络能够给出正确的输出。我们如何更新这些权重矩阵呢？这时出现了一种新技术，叫做**梯度下降法**。
- en: With gradient descent, the neural network learns the optimal values of the randomly
    initialized weight matrices. With the optimal values of weights, our network can
    predict the correct output and minimize the loss.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过梯度下降法，神经网络可以学习到随机初始化的权重矩阵的最优值。有了这些最优的权重值，我们的网络就可以预测正确的输出并最小化损失。
- en: Now, we will explore how the optimal values of weights are learned using gradient
    descent. Gradient descent is one of the most commonly used optimization algorithms.
    It is used for minimizing the cost function, which allows us to minimize the error
    and obtain the lowest possible error value. But how does gradient descent find
    the optimal weights? Let's begin with an analogy.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨如何使用梯度下降法学习权重的最优值。梯度下降法是最常用的优化算法之一。它用于最小化成本函数，从而帮助我们最小化误差并获得可能的最低误差值。但梯度下降法是如何找到最优权重的呢？我们从一个类比开始。
- en: Imagine we are on top of a hill, as shown in the following diagram, and we want
    to reach the lowest point on the hill. There could be many regions that look like
    the lowest points on the hill, but we have to reach the point that is actually
    the lowest of all.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们站在山顶，如下图所示，想要到达山的最低点。山上可能有许多看起来像是最低点的地方，但我们必须找到那个真正最低的点。
- en: 'That is, we should not be stuck at a point believing it is the lowest point
    when the global lowest point exists:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们不应该停留在一个点，认为它是最低点，尽管全局最低点存在：
- en: '![](img/B15558_07_09.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_09.png)'
- en: 'Figure 7.9: Analogy of gradient descent'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9：梯度下降的类比
- en: 'Similarly, we can represent our cost function as follows. It is a plot of cost
    against weights. Our objective is to minimize the cost function. That is, we have
    to reach the lowest point where the cost is the minimum. The solid dark point
    in the following diagram shows the randomly initialized weights. If we move this
    point downward, then we can reach the point where the cost is the minimum:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以将成本函数表示如下。它是成本与权重的图像。我们的目标是最小化成本函数。也就是说，我们必须到达成本最小的最低点。下图中的实心黑点表示随机初始化的权重。如果我们将这个点向下移动，就可以到达成本最小的点：
- en: '![](img/B15558_07_10.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_10.png)'
- en: 'Figure 7.10: Gradient descent'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10：梯度下降
- en: But how can we move this point (initial weight) downward? How can we descend
    and reach the lowest point? Gradients are used for moving from one point to another.
    So, we can move this point (initial weight) by calculating a gradient of the cost
    function with respect to that point (initial weights), which is ![](img/B15558_07_023.png).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何将这个点（初始权重）向下移动呢？我们如何下降并到达最低点？梯度用于从一个点移动到另一个点。因此，我们可以通过计算成本函数相对于该点（初始权重）的梯度来移动这个点（初始权重），即
    ![](img/B15558_07_023.png)。
- en: 'Gradients are the derivatives that are actually the slope of a tangent line,
    as illustrated in the following diagram. So, by calculating the gradient, we descend
    (move downward) and reach the lowest point where the cost is the minimum. Gradient
    descent is a first-order optimization algorithm, which means we only take into
    account the first derivative when performing the updates:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度是导数，实际上是切线的斜率，如下图所示。因此，通过计算梯度，我们可以下降（向下移动）并到达成本最小的最低点。梯度下降是一种一阶优化算法，这意味着我们在执行更新时只考虑一阶导数：
- en: '![](img/B15558_07_11.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_11.png)'
- en: 'Figure 7.11: Gradient descent'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11：梯度下降
- en: Thus, with gradient descent, we move our weights to a position where the cost
    is minimum. But still, how do we update the weights?
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用梯度下降，我们将权重移动到成本最小的位置。但仍然，如何更新权重呢？
- en: 'As a result of forward propagation, we are in the output layer. We will now
    **backpropagate** the network from the output layer to the input layer and calculate
    the gradient of the cost function with respect to all the weights between the
    output and the input layer so that we can minimize the error. After calculating
    gradients, we update our old weights using the weight update rule:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 通过前向传播，我们到达了输出层。现在，我们将从输出层反向传播网络到输入层，并计算成本函数相对于输出层和输入层之间所有权重的梯度，以便我们最小化误差。计算完梯度后，我们将使用权重更新规则来更新旧的权重：
- en: '![](img/B15558_07_024.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_024.png)'
- en: This implies *weights = weights -α* x *gradients*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 *权重 = 权重 - α* x *梯度*。
- en: What is ![](img/B15558_07_025.png)? It is called the **learning rate**. As shown
    in the following diagram, if the learning rate is small, then we take a small
    step downward and our gradient descent can be slow.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 那么 ![](img/B15558_07_025.png) 是什么？它被称为**学习率**。如以下图所示，如果学习率较小，那么我们会向下迈出较小的一步，梯度下降的速度可能会很慢。
- en: 'If the learning rate is large, then we take a large step and our gradient descent
    will be fast, but we might fail to reach the global minimum and become stuck at
    a local minimum. So, the learning rate should be chosen optimally:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果学习率较大，那么我们会迈出较大的一步，梯度下降的速度会很快，但我们可能无法到达全局最小值，而是停留在局部最小值。因此，学习率应该选择得最优：
- en: '![](img/B15558_07_12.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_07_12.png)'
- en: 'Figure 7.12: Effect of learning rate'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12：学习率的影响
- en: This whole process of backpropagating the network from the output layer to the
    input layer and updating the weights of the network using gradient descent to
    minimize the loss is called **backpropagation**. Now that we have a basic understanding
    of backpropagation, we will strengthen our understanding by learning about this
    in detail, step by step. We are going to look at some interesting math, so put
    on your calculus hats and follow the steps.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个从输出层反向传播网络到输入层并使用梯度下降更新网络权重以最小化损失的过程被称为**反向传播**。现在我们已经对反向传播有了基本了解，我们将通过一步步详细学习来加深理解。接下来我们将探讨一些有趣的数学内容，所以戴上你的微积分帽子，跟着步骤走。
- en: 'So, we have two weights, one *W*[xh], which is the input to hidden layer weights,
    and the other *W*[hy], which is the hidden to output layer weights. We need to
    find the optimal values for these two weights that will give us the fewest errors.
    So, we need to calculate the derivative of the cost function *J* with respect
    to these weights. Since we are backpropagating, that is, going from the output
    layer to the input layer, our first weight will be *W*[hy]. So, now we need to
    calculate the derivative of *J* with respect to *W*[hy]. How do we calculate the
    derivative? First, let''s recall our cost function, *J*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_021.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: 'We cannot calculate the derivative directly from the preceding equation since
    there is no *W*[hy] term. So, instead of calculating the derivative directly,
    we calculate the partial derivative. Let''s recall our forward propagation equation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_016.png)![](img/B15558_07_015.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
- en: 'First, we will calculate a partial derivative with respect to ![](img/B15558_07_029.png),
    and then from ![](img/B15558_07_030.png) we will calculate the partial derivative
    with respect to *z*[2]. From *z*[2], we can directly calculate our derivative
    *W*[hy]. It is basically the chain rule. So, the derivative of *J* with respect
    to *W*[hy] becomes as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_031.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will compute each of the terms in the preceding equation:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_032.png)![](img/B15558_07_033.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B15558_07_034.png) is the derivative of our sigmoid activation
    function. We know that the sigmoid function is ![](img/B15558_07_035.png), so
    the derivative of the sigmoid function would be ![](img/B15558_07_036.png).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we have:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_037.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Thus, substituting all the preceding terms in equation *(1)* we can write:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_038.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Now we need to compute a derivative of *J* with respect to our next weight,
    *W*[xh].
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we cannot calculate the derivative of *W*[xh] directly from *J*
    as we don''t have any *W*[xh] terms in *J*. So, we need to use the chain rule.
    Let''s recall the forward propagation steps again:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_016.png)![](img/B15558_07_015.png)![](img/B15558_07_014.png)![](img/B15558_07_011.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Now, according to the chain rule, the derivative of *J* with respect to *W*[xh]
    is given as:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_043.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
- en: 'We have already seen how to compute the first two terms in the preceding equation;
    now, we will see how to compute the rest of the terms:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_044.png)![](img/B15558_07_045.png)![](img/B15558_07_046.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: 'Thus, substituting all the preceding terms in equation *(3)*, we can write:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_047.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: 'After we have computed gradients for both weights, *W*[hy] and *W*[xh], we
    will update our initial weights according to the weight update rule:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_048.png)![](img/B15558_07_049.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: That's it! This is how we update the weights of a network and minimize the loss.
    Now, let's see how to implement the backpropagation algorithm in Python.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'In both the equations *(2)* and *(4)*, we have the term ![](img/B15558_07_050.png),
    so instead of computing them again and again, we just call them `delta2`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we compute the gradient with respect to *W*[hy]. Refer to equation *(2)*:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We compute the gradient with respect to *W*[xh]. Refer to equation *(4)*:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We will update the weights according to our weight update rule equation *(5)*
    and *(6)* as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The complete code for the backpropagation is given as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: That's it. Apart from this, there are different variants of gradient descent
    methods such as stochastic gradient descent, mini-batch gradient descent, Adam,
    RMSprop, and more.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let''s familiarize ourselves with some of the frequently
    used terminology in neural networks:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward pass**: Forward pass implies forward propagating from the input layer
    to the output layer.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backward pass**: Backward pass implies backpropagating from the output layer
    to the input layer.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epoch**: The epoch specifies the number of times the neural network sees
    our whole training data. So, we can say one epoch is equal to one forward pass
    and one backward pass for all training samples.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: The batch size specifies the number of training samples we
    use in one forward pass and one backward pass.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of iterations**: The number of iterations implies the number of passes
    where *one pass = one forward pass + one backward pass*.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Say that we have 12,000 training samples and our batch size is 6,000\. Then
    it will take us two iterations to complete one epoch. That is, in the first iteration,
    we pass the first 6,000 samples and perform a forward pass and a backward pass;
    in the second iteration, we pass the next 6,000 samples and perform a forward
    pass and a backward pass. After two iterations, our neural network will see the
    whole 12,000 training samples, which makes it one epoch.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Putting all the concepts we have learned so far together, we will see how to
    build a neural network from scratch. We will understand how the neural network
    learns to perform the XOR gate operation. The XOR gate returns 1 only when exactly
    only one of its inputs is 1, else it returns 0, as shown in *Table 7.1*:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_13.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
- en: 'Table 7.1: XOR operation'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network from scratch
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform the XOR gate operation, we build a simple two-layer neural network,
    as shown in the following diagram. As you can see, we have an input layer with
    two nodes, a hidden layer with five nodes and an output layer comprising one node:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_14.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: ANN'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'We will understand step-by-step how a neural network learns the XOR logic:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the libraries:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Prepare the data as shown in the preceding XOR table:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define the number of nodes in each layer:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Initialize the weights and bias randomly. First, we initialize the input to
    hidden layer weights:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we initialize the hidden to output layer weights:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define the sigmoid activation function:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Define the derivative of the sigmoid function:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define the forward propagation:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Define the backward propagation:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Define the cost function:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Set the learning rate and the number of training iterations:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let''s start training the network with the following code:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Plot the cost function:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As you can observe in the following plot, the loss decreases over the training
    iterations:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_15.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Cost function'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have an overall understanding of ANNs and how they learn.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*The sun rises in the ____.*'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: If we were asked to predict the blank term in the preceding sentence, we would
    probably say east. Why would we predict that the word east would be the right
    word here? Because we read the whole sentence, understood the context, and predicted
    that the word east would be an appropriate word to complete the sentence.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: If we use a feedforward neural network (the one we learned in the previous section)
    to predict the blank, it would not predict the right word. This is due to the
    fact that in feedforward networks, each input is independent of other input and
    they make predictions based only on the current input, and they don't remember
    previous input.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the input to the network will just be the word preceding the blank, which
    is the word *the*. With this word alone as an input, our network cannot predict
    the correct word, because it doesn't know the context of the sentence, which means
    that it doesn't know the previous set of words to understand the context of the
    sentence and to predict an appropriate next word.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Here is where we use **Recurrent Neural Networks** (**RNNs**). They predict
    output not only based on the current input, but also on the previous hidden state.
    Why do they have to predict the output based on the current input and the previous
    hidden state? Why can't they just use the current input and the previous input?
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: This is because the previous input will only store information about the previous
    word, while the previous hidden state will capture the contextual information
    about all the words in the sentence that the network has seen so far. Basically,
    the previous hidden state acts like memory, and it captures the context of the
    sentence. With this context and the current input, we can predict the relevant
    word.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's take the same sentence, *The sun rises in the ____.* As
    shown in the following figure, we first pass the word *the* as an input, and then
    we pass the next word, *sun*, as input; but along with this, we also pass the
    previous hidden state, *h*[0]. So, every time we pass the input word, we also
    pass a previous hidden state as an input.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final step, we pass the word *the*, and also the previous hidden state
    *h*[3], which captures the contextual information about the sequence of words
    that the network has seen so far. Thus, *h*[3] acts as the memory and stores information
    about all the previous words that the network has seen. With *h*[3] and the current
    input word (*the*), we can predict the relevant next word:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_16.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: RNN'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, an RNN uses the previous hidden state as memory, which captures
    and stores the contextual information (input) that the network has seen so far.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are widely applied for use cases that involve sequential data, such as
    time series, text, audio, speech, video, weather, and much more. They have been
    greatly used in various **natural language processing** (**NLP**) tasks, such
    as language translation, sentiment analysis, text generation, and so on.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: The difference between feedforward networks and RNNs
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A comparison between an RNN and a feedforward network is shown in the *Figure
    7.16*:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_17.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Difference between feedforward network and RNN'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe in the preceding diagram, the RNN contains a looped connection
    in the hidden layer, which implies that we use the previous hidden state along
    with the input to predict the output.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Still confused? Let's look at the following unrolled version of an RNN. But
    wait; what is the unrolled version of an RNN?
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'It means that we roll out the network for a complete sequence. Let''s suppose
    that we have an input sentence with *T* words; then, we will have 0 to *T*—1 layers,
    one for each word, as shown in *Figure 7.17*:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_18.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.17: Unrolled RNN'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 7.17*, at the time step *t* = 1, the output *y*[1]
    is predicted based on the current input *x*[1] and the previous hidden state *h*[0].
    Similarly, at time step *t* = 2, *y*[2] is predicted using the current input *x*[2]
    and the previous hidden state *h*[1]. This is how an RNN works; it takes the current
    input and the previous hidden state to predict the output.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation in RNNs
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at how an RNN uses forward propagation to predict the output; but
    before we jump right in, let''s get familiar with the notations:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_19.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.18: Forward propagation in RNN'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure illustrates the following:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '*U* represents the input to hidden layer weight matrix'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W* represents the hidden to hidden layer weight matrix'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V* represents the hidden to output layer weight matrix'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The hidden state *h* at a time step *t* can be computed as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_051.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
- en: That is, *the* *hidden state at a time step, t = tanh([input to hidden layer
    weight x input]* + *[hidden to hidden layer weight x previous hidden state])*.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The output at a time step *t* can be computed as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_052.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: That is, *the* *output at a time step, t = softmax (hidden to output layer weight*
    x *hidden state at a time t)*.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also represent RNNs as shown in the following figure. As you can see,
    the hidden layer is represented by an RNN block, which implies that our network
    is an RNN, and previous hidden states are used in predicting the output:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_20.png)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.19: Forward propagation in an RNN'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.20* shows how forward propagation works in an unrolled version of
    an RNN:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_21.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.20: Unrolled version – forward propagation in an RNN'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize the initial hidden state *h*[init] with random values. As you
    can see in the preceding figure, the output, ![](img/B15558_07_053.png), is predicted
    based on the current input, *x*[0] and the previous hidden state, which is an
    initial hidden state, *h*[init], using the following formula:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_054.png)![](img/B15558_07_055.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, look at how the output, ![](img/B15558_07_056.png), is computed.
    It takes the current input, *x*[1], and the previous hidden state, *h*[0]:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_057.png)![](img/B15558_07_058.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
- en: Thus, in forward propagation to predict the output, RNN uses the current input
    and the previous hidden state.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagating through time
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We just learned how forward propagation works in RNNs and how it predicts the
    output. Now, we compute the loss, *L*, at each time step, *t*, to determine how
    well the RNN has predicted the output. We use the cross-entropy loss as our loss
    function. The loss *L* at a time step *t* can be given as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_059.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
- en: Here, *y*[t] is the actual output, and ![](img/B15558_07_060.png) is the predicted
    output at a time step *t*.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The final loss is a sum of the loss at all the time steps. Suppose that we
    have *T* - 1 layers; then, the final loss can be given as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_061.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.21* shows that the final loss is obtained by the sum of loss at all
    the time steps:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_22.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.21: Backpropagation in an RNN'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'We computed the loss, now our goal is to minimize the loss. How can we minimize
    the loss? We can minimize the loss by finding the optimal weights of the RNN.
    As we learned, we have three weights in RNNs: input to hidden, *U*, hidden to
    hidden, *W*, and hidden to output, *V*.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to find optimal values for all of these three weights to minimize the
    loss. We can use our favorite gradient descent algorithm to find the optimal weights.
    We begin by calculating the gradients of the loss function with respect to all
    the weights; then, we update the weights according to the weight update rule as
    follows:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_062.png)![](img/B15558_07_063.png)![](img/B15558_07_064.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
- en: However, we have a problem with the RNN. The gradient calculation involves calculating
    the gradient with respect to the activation function. When we calculate the gradient
    with respect to the sigmoid or tanh function, the gradient will become very small.
    When we further backpropagate the network over many time steps and multiply the
    gradients, the gradients will tend to get smaller and smaller. This is called
    a vanishing gradient problem.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Since the gradient vanishes over time, we cannot learn information about long-term
    dependencies, that is, RNNs cannot retain information for a long time in the memory.
    The vanishing gradient problem occurs not only in RNNs but also in other deep
    networks where we have many hidden layers and when we use sigmoid/tanh functions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: One solution to avoid vanishing gradient problem is to use ReLU as an activation
    function. However, we have a variant of the RNN called **Long Short-Term Memory**
    (**LSTM**), which can solve the vanishing gradient problem effectively. We will
    see how it works in the upcoming section.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: LSTM to the rescue
  id: totrans-276
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While backpropagating an RNN, we learned about a problem called **vanishing
    gradients**. Due to the vanishing gradient problem, we cannot train the network
    properly, and this causes the RNN to not retain long sequences in the memory.
    To understand what we mean by this, let''s consider a small sentence:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '*The sky is __*.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'An RNN can easily predict the blank as *blue* based on the information it has
    seen, but it cannot cover the long-term dependencies. What does that mean? Let''s
    consider the following sentence to understand the problem better:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '*Archie lived in China for 13 years. He loves listening to good music. He is
    a fan of comics. He is fluent in ____.*'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we were asked to predict the missing word in the preceding sentence,
    we would predict it as *Chinese*, but how did we predict that? We simply remembered
    the previous sentences and understood that Archie lived for 13 years in China.
    This led us to the conclusion that Archie might be fluent in Chinese. An RNN,
    on the other hand, cannot retain all of this information in its memory to say
    that Archie is fluent in Chinese.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: Due to the vanishing gradient problem, it cannot recollect/remember information
    for a long time in its memory. That is, when the input sequence is long, the RNN
    memory (hidden state) cannot hold all the information. To alleviate this, we use
    an LSTM cell.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM is a variant of an RNN that resolves the vanishing gradient problem and
    retains information in the memory as long as it is required. Basically, RNN cells
    are replaced with LSTM cells in the hidden units, as shown in *Figure 7.22*:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_23.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.22: LSTM network'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will understand how the LSTM cells works.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the LSTM cell
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What makes LSTM cells so special? How do LSTM cells achieve long-term dependency?
    How does it know what information to keep and what information to discard from
    the memory?
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all achieved by special structures called **gates**. As shown in the
    following diagram, a typical LSTM cell consists of three special gates called
    the input gate, output gate, and forget gate:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_24.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.23: LSTM gates'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'These three gates are responsible for deciding what information to add, output,
    and forget from the memory. With these gates, an LSTM cell effectively keeps information
    in the memory only as long as required. *Figure 7.24* shows a typical LSTM cell:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_25.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.24: LSTM cell'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the LSTM cell, the top horizontal line is called the cell state.
    It is where the information flows. Information on the cell state will be constantly
    updated by LSTM gates. Now, we will see the function of these gates:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '**Forget gate**: The forget gate is responsible for deciding what information
    should not be in the cell state. Look at the following statement:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '*Harry is a good singer. He lives in New York. Zayn is also a good singer.*'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: As soon as we start talking about Zayn, the network will understand that the
    subject has been changed from Harry to Zayn and the information about Harry is
    no longer required. Now, the forget gate will remove/forget information about
    Harry from the cell state.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '**Input gate**: The input gate is responsible for deciding what information
    should be stored in the memory. Let''s consider the same example:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '*Harry is a good singer. He lives in New York. Zayn is also a good singer.*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: So, after the forget gate removes information from the cell state, the input
    gate decides what information has to be in the memory. Here, since the information
    about Harry is removed from the cell state by the forget gate, the input gate
    decides to update the cell state with the information about Zayn.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '**Output gate**: The output gate is responsible for deciding what information
    should be shown from the cell state at a time, *t*. Now, consider the following
    sentence:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '*Zayn''s debut album was a huge success. Congrats ____.*'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Here, congrats is an adjective which is used to describe a noun. The output
    layer will predict Zayn (noun), to fill in the blank.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using LSTM, we can overcome the vanishing gradient problem faced in RNN.
    In the next section, we will learn another interesting algorithm called the **Convolutional
    Neural Network** (**CNN**).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: What are CNNs?
  id: totrans-306
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A CNN, also known as a **ConvNet**, is one of the most widely used deep learning
    algorithms for computer vision tasks. Let's say we are performing an image-recognition
    task. Consider the following image.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'We want our CNN to recognize that it contains a horse:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_26.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.25: Image containing a horse'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: How can we do that? When we feed the image to a computer, it basically converts
    it into a matrix of pixel values. The pixel values range from 0 to 255, and the
    dimensions of this matrix will be of [*image width* x *image height* x *number
    of channels*]. A grayscale image has one channel, and colored images have three
    channels, **red, green, and blue** (**RGB**).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a colored input image with a width of 11 and a height of 11,
    that is 11 x 11, then our matrix dimension would be *[11* x *11* x *3]*. As you
    can see in *[11* x *11* x *3]*, 11 x 11 represents the image width and height
    and 3 represents the channel number, as we have a colored image. So, we will have
    a 3D matrix.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: But it is hard to visualize a 3D matrix, so, for the sake of understanding,
    let's consider a grayscale image as our input. Since the grayscale image has only
    one channel, we will get a 2D matrix.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, the input grayscale image will be converted
    into a matrix of pixel values ranging from 0 to 255, with the pixel values representing
    the intensity of pixels at that point:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_27.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.26: Input image is converted to matrix of pixel values'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: The values given in the input matrix are just arbitrary values for our understanding.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, now we have an input matrix of pixel values. What happens next? How does
    the CNN come to understand that the image contains a horse? CNNs consists of the
    following three important layers:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pooling layer
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fully connected layer
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of these three layers, the CNN recognizes that the image contains
    a horse. Now we will explore each of these layers in detail.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  id: totrans-323
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The convolutional layer is the first and core layer of the CNN. It is one of
    the building blocks of a CNN and is used for extracting important features from
    the image.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: We have an image of a horse. What do you think are the features that will help
    us to understand that this is an image of a horse? We can say body structure,
    face, legs, tail, and so on. But how does the CNN understand these features? This
    is where we use a convolution operation that will extract all the important features
    from the image that characterize the horse. So, the convolution operation helps
    us to understand what the image is all about.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what exactly is this convolution operation? How it is performed? How does
    it extract the important features? Let's look at this in detail.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: As we know, every input image is represented by a matrix of pixel values. Apart
    from the input matrix, we also have another matrix called the **filter matrix**.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'The filter matrix is also known as a **kernel**, or simply a **filter**, as
    shown in the *Figure 7.27*:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_28.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.27: Input and filter matrix'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'We take the filter matrix, slide it over the input matrix by one pixel, perform
    element-wise multiplication, sum the results, and produce a single number. That''s
    pretty confusing, isn''t it? Let''s understand this better with the aid of the
    following diagram:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_29.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.28: Convolution operation'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the previous diagram, we took the filter matrix and placed
    it on top of the input matrix, performed element-wise multiplication, summed their
    results, and produced the single number. This is demonstrated as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_065.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
- en: 'Now, we slide the filter over the input matrix by one pixel and perform the
    same steps, as shown in *Figure 7.29*:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_30.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.29: Convolution operation'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'This is demonstrated as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_066.png)'
  id: totrans-340
  prefs: []
  type: TYPE_IMG
- en: 'Again, we slide the filter matrix by one pixel and perform the same operation,
    as shown in *Figure 7.30*:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_31.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.30: Convolution operation'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: 'This is demonstrated as follows:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_067.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
- en: 'Now, again, we slide the filter matrix over the input matrix by one pixel and
    perform the same operation, as shown in *Figure 7.31*:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_32.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.31: Convolution operation'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'That is:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_068.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
- en: Okay. What are we doing here? We are basically sliding the filter matrix over
    the entire input matrix by one pixel, performing element-wise multiplication and
    summing their results, which creates a new matrix called a **feature map** or
    **activation map**. This is called the **convolution operation**.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: As we've learned, the convolution operation is used to extract features, and
    the new matrix, that is, the feature maps, represents the extracted features.
    If we plot the feature maps, then we can see the features extracted by the convolution
    operation.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.32* shows the actual image (the input image) and the convolved image
    (the feature map). We can see that our filter has detected the edges from the
    actual image as a feature:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_33.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.32: Conversion of actual image to convolved image'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Various filters are used for extracting different features from the image.
    For instance, if we use a sharpen filter, ![](img/B15558_07_069.png), then it
    will sharpen our image, as shown in the following figure:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_34.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.33: Sharpened image'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we have learned that with filters, we can extract important features
    from the image using the convolution operation. So, instead of using one filter,
    we can use multiple filters to extract different features from the image and produce
    multiple feature maps. So, the depth of the feature map will be the number of
    filters. If we use seven filters to extract different features from the image,
    then the depth of our feature map will be seven:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_35.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.34: Feature maps'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: Okay, we have learned that different filters extract different features from
    the image. But the question is, how can we set the correct values for the filter
    matrix so that we can extract the important features from the image? Worry not!
    We just initialize the filter matrix randomly, and the optimal values of the filter
    matrix, with which we can extract the important features from the images, will
    be learned through backpropagation. However, we just need to specify the size
    of the filter and the number of filters we want to use.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Strides
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have just learned how a convolution operation works. We slide over the input
    matrix with the filter matrix by one pixel and perform the convolution operation.
    But we don't have to only slide over the input matrix by one pixel, we can also
    slide over the input matrix by any number of pixels.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: The number of pixels we slide over the input matrix by the filter matrix is
    called a **stride**.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: 'If we set the stride to 2, then we slide over the input matrix with the filter
    matrix by two pixels. *Figure 7.35* shows a convolution operation with a stride
    of 2:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_36.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.35: Stride operation'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: But how do we choose the stride number? We just learned that a stride is the
    number of pixels along that we move our filter matrix. So, when the stride is
    set to a small number, we can encode a more detailed representation of the image
    than when the stride is set to a large number. However, a stride with a high value
    takes less time to compute than one with a low value.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the convolution operation, we are sliding over the input matrix with a
    filter matrix. But in some cases, the filter does not perfectly fit the input
    matrix. What do we mean by that? For example, let''s say we are performing a convolution
    operation with a stride of 2\. There exists a situation where, when we move our
    filter matrix by two pixels, it reaches the border and the filter matrix does
    not fit the input matrix. That is, some part of our filter matrix is outside the
    input matrix, as shown in the following diagram:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_37.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.36: Padding operation'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we perform padding. We can simply pad the input matrix with zeros
    so that the filter can fit the input matrix, as shown in *Figure 7.37*. Padding
    with zeros on the input matrix is called **same padding** or **zero padding**:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_38.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.37: Same padding'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of padding them with zeros, we can also simply discard the region of
    the input matrix where the filter doesn''t fit in. This is called **valid padding**:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_39.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.38: Valid padding'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Okay. Now, we are done with the convolution operation. As a result of the convolution
    operation, we have some feature maps. But the feature maps are too large in dimension.
    In order to reduce the dimensions of feature maps, we perform a pooling operation.
    This reduces the dimensions of the feature maps and keeps only the necessary details
    so that the amount of computation can be reduced.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: For example, to recognize a horse from the image, we need to extract and keep
    only the features of the horse; we can simply discard unwanted features, such
    as the background of the image and more. A pooling operation is also called a
    **downsampling** or **subsampling** operation, and it makes the CNN translation
    invariant. Thus, the pooling layer reduces spatial dimensions by keeping only
    the important features.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of pooling operations, including max pooling, average
    pooling, and sum pooling.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: 'In max pooling, we slide over the filter on the input matrix and simply take
    the maximum value from the filter window, as *Figure 7.39* shows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_40.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.39: Max pooling'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: In average pooling, we take the average value of the input matrix within the
    filter window, and in sum pooling, we sum all the values of the input matrix within
    the filter window.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we've learned how convolutional and pooling layers work. A CNN can have
    multiple convolutional layers and pooling layers. However, these layers will only
    extract features from the input image and produce the feature map; that is, they
    are just the feature extractors.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: Given any image, convolutional layers extract features from the image and produce
    a feature map. Now, we need to classify these extracted features. So, we need
    an algorithm that can classify these extracted features and tell us whether the
    extracted features are the features of a horse, or something else. In order to
    make this classification, we use a feedforward neural network. We flatten the
    feature map and convert it into a vector, and feed it as an input to the feedforward
    network.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: 'The feedforward network takes this flattened feature map as an input, applies
    an activation function, such as sigmoid, and returns the output, stating whether
    the image contains a horse or not; this is called a fully connected layer and
    is shown in the following diagram:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_41.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.40: Fully connected layer'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how all this fits together.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of CNNs
  id: totrans-395
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 7.41* shows the architecture of a CNN:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_42.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.41: Architecture of CNN'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: As you will notice, first we feed the input image to the convolutional layer,
    where we apply the convolution operation to extract important features from the
    image and create the feature maps. We then pass the feature maps to the pooling
    layer, where the dimensions of the feature maps will be reduced.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the previous diagram, we can have multiple convolutional and pooling
    layers, and we should also note that the pooling layer does not necessarily have
    to be there after every convolutional layer; there can be many convolutional layers
    followed by a pooling layer.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: So, after the convolutional and pooling layers, we flatten the resultant feature
    maps and feed it to a fully connected layer, which is basically a feedforward
    neural network that classifies the given input image based on the feature maps.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how CNNs work, in the next section, we will learn about
    another interesting algorithm called the generative adversarial network.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  id: totrans-403
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**) was first introduced by Ian
    J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
    Ozair, Aaron Courville, and Yoshua Bengio in their paper, *Generative Adversarial
    Networks*, in 2014.'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: GANs are used extensively for generating new data points. They can be applied
    to any type of dataset, but they are popularly used for generating images. Some
    of the applications of GANs include generating realistic human faces, converting
    grayscale images to colored images, translating text descriptions into realistic
    images, and many more.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have evolved so much in recent years that they can generate a very realistic
    image. The following figure shows the evolution of GANs in generating images over the
    course of five years:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_43.png)'
  id: totrans-407
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.42: Evolution of GANs over the years'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: Excited about GANs already? Now, we will see how exactly they work. Before going
    ahead, let's consider a simple analogy. Let's say you are the police and your
    task is to find counterfeit money, and the role of the counterfeiter is to create
    fake money and cheat the police.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'The counterfeiter constantly tries to create fake money in a way that is so
    realistic that it cannot be differentiated from the real money. But the police
    have to identify whether the money is real or fake. So, the counterfeiter and
    the police essentially play a two-player game where one tries to defeat the other.
    GANs work something like this. They consist of two important components:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Generator
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminator
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can perceive the generator as analogous to the counterfeiter, while the
    discriminator is analogous to the police. That is, the role of the generator is
    to create fake money, and the role of the discriminator is to identify whether
    the money is fake or real.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'Without going into detail, first, we will get a basic understanding of GANs.
    Let''s say we want our GAN to generate handwritten digits. How can we do that?
    First, we will take a dataset containing a collection of handwritten digits; say,
    the MNIST dataset. The generator learns the distribution of images in our dataset.
    Thus, it learns the distribution of handwritten digits in our training set. Once,
    it learns the distribution of the images in our dataset, and we feed a random
    noise to the generator, it will convert the random noise into a new handwritten
    digit similar to the one in our training set based on the learned distribution:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_44.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.43: Generator'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the discriminator is to perform a classification task. Given an
    image, it classifies it as real or fake; that is, whether the image is from the
    training set or the image is generated by the generator:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_45.png)'
  id: totrans-418
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.44: Discriminator'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: The generator component of GAN is basically a generative model, and the discriminator
    component is basically a discriminative model. Thus, the generator learns the
    distribution of the class and the discriminator learns the decision boundary of
    a class.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: 'As *Figure 7.45* shows, we feed random noise to the generator, and it then
    converts this random noise into a new image similar to the one we have in our
    training set, but not exactly the same as the images in the training set. The
    image generated by the generator is called a fake image, and the images in our
    training set are called real images. We feed both the real and fake images to
    the discriminator, which tells us the probability of them being real. It returns
    0 if the image is fake and 1 if the image is real:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_46.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.45: GAN'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of generators and discriminators, we
    will study each of the components in detail.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the generator
  id: totrans-425
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The generator component of a GAN is a generative model. When we say the generative
    model, there are two types of generative models—an **implicit** and an **explicit**
    density model. The implicit density model does not use any explicit density function
    to learn the probability distribution, whereas the explicit density model, as
    the name suggests, uses an explicit density function. GANs falls into the first
    category. That is, they are an implicit density model. Let's study in detail and
    understand how GANs are an implicit density model.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a generator, *G*. It is basically a neural network parametrized
    by ![](img/B15558_07_070.png). The role of the generator network is to generate
    new images. How do they do that? What should be the input to the generator?
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'We sample a random noise, *z*, from a normal or uniform distribution, *P*[z].
    We feed this random noise, *z*, as an input to the generator and then it converts
    this noise to an image:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_071.png)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
- en: Surprising, isn't it? How does the generator convert random noise to a realistic
    image?
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a dataset containing a collection of human faces and we want
    our generator to generate a new human face. First, the generator learns all the
    features of the face by learning the probability distribution of the images in
    our training set. Once the generator learns the correct probability distribution,
    it can generate totally new human faces.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: But how does the generator learn the distribution of the training set? That
    is, how does the generator learn the distribution of images of human faces in
    the training set?
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: A generator is nothing but a neural network. So, what happens is that the neural
    network learns the distribution of the images in our training set implicitly;
    let's call this distribution a generator distribution, *P*[g]. At the first iteration,
    the generator generates a really noisy image. But over a series of iterations,
    it learns the exact probability distribution of our training set and learns to
    generate a correct image by tuning its ![](img/B15558_07_070.png) parameter.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we are not using the uniform distribution *P*[z]
    for learning the distribution of our training set. It is only used for sampling
    random noise, and we feed this random noise as an input to the generator. The
    generator network implicitly learns the distribution of our training set and we
    call this distribution a generator distribution, *P*[g] and that is why we call
    our generator network an implicit density model.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the generator, let's move on to the discriminator.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the discriminator
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the discriminator is a discriminative model. Let's say
    we have a discriminator, *D*. It is also a neural network and it is parametrized
    by ![](img/B15558_07_073.png).
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the discriminator is to discriminate between two classes. That
    is, given an image *x*, it has to identify whether the image is from a real distribution
    or a fake distribution (generator distribution). That is, the discriminator has
    to identify whether the given input image is from the training set or the fake
    image generated by the generator:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_074.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
- en: Let's call the distribution of our training set the real data distribution,
    which is represented by *P*[r]. We know that the generator distribution is represented
    by *P*[g].
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: So, the discriminator *D* essentially tries to discriminate whether the image
    *x* is from *P*[r] or *P*[g].
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
- en: How do they learn, though?
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we just studied the role of the generator and discriminator, but how
    do they learn exactly? How does the generator learn to generate new realistic
    images and how does the discriminator learn to discriminate between images correctly?
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: We know that the goal of the generator is to generate an image in such a way
    as to fool the discriminator into believing that the generated image is from a
    real distribution.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: In the first iteration, the generator generates a noisy image. When we feed
    this image to the discriminator, it can easily detect that the image is from a
    generator distribution. The generator takes this as a loss and tries to improve
    itself, as its goal is to fool the discriminator. That is, if the generator knows
    that the discriminator is easily detecting the generated image as a fake image,
    then it means that it is not generating an image similar to those in the training
    set. This implies that it has not learned the probability distribution of the
    training set yet.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: So, the generator tunes its parameters in such a way as to learn the correct
    probability distribution of the training set. As we know that the generator is
    a neural network, we simply update the parameters of the network through backpropagation.
    Once it has learned the probability distribution of the real images, then it can
    generate images similar to the ones in the training set.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what about the discriminator? How does it learn? As we know, the role
    of the discriminator is to discriminate between real and fake images.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: If the discriminator incorrectly classifies the generated image; that is, if
    the discriminator classifies the fake image as a real image, then it implies that
    the discriminator has not learned to differentiate between the real and fake image.
    So, we update the parameter of the discriminator network through backpropagation
    to make the discriminator learn to classify between real and fake images.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: So, basically, the generator is trying to fool the discriminator by learning
    the real data distribution, *P*[r], and the discriminator is trying to find out
    whether the image is from a real or fake distribution. Now the question is, when
    do we stop training the network in light of the fact that the generator and discriminator
    are competing against each other?
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: Basically, the goal of the GAN is to generate images similar to the one in the
    training set. Say we want to generate a human face—we learn the distribution of
    images in the training set and generate new faces. So, for a generator, we need
    to find the optimal discriminator. What do we mean by that?
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that a generator distribution is represented by *P*[g] and the real
    data distribution is represented by *P*[r]. If the generator learns the real data
    distribution perfectly, then *P*[g] equals *P*[r], as *Figure 7.46* shows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_47.png)'
  id: totrans-452
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.46: Generator and real data distribution'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: When *P*[g] = *P*[r], then the discriminator cannot differentiate between whether
    the input image is from a real or a fake distribution, so it will just return
    0.5 as a probability, as the discriminator will become confused between the two
    distributions when they are the same.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for a generator, the optimal discriminator can be given as follows:'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_075.png)'
  id: totrans-456
  prefs: []
  type: TYPE_IMG
- en: So, when the discriminator just returns the probability of 0.5 for all the generator
    images, then we can say that the generator has learned the distribution of images
    in our training set and has fooled the discriminator successfully.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of a GAN
  id: totrans-458
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 7.47* shows the architecture of a GAN:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_48.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.47: Architecture of GAN'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, generator *G* takes the random noise, *z*,
    as input by sampling from a uniform or normal distribution and generates a fake
    image by implicitly learning the distribution of the training set.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: We sample an image, *x*, from the real data distribution, ![](img/B15558_07_076.png),
    and fake data distribution, ![](img/B15558_07_077.png), and feed it to the discriminator,
    *D*. We feed real and fake images to the discriminator and the discriminator performs
    a binary classification task. That is, it returns 0 when the image is fake and
    1 when the image is real.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying the loss function
  id: totrans-464
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will examine the loss function of the GAN. Before going ahead, let''s
    recap the notation:'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: A noise that is fed as an input to the generator is represented by *z*
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The uniform or normal distribution from which the noise *z* is sampled is represented
    by *P*[z]
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An input image is represented by *x*
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The real data distribution or the distribution of our training set is represented
    by *P*[r]
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fake data distribution or the distribution of the generator is represented
    by *P*[g]
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we write ![](img/B15558_07_078.png), it implies that image *x* is sampled
    from the real distribution, *P*[r]. Similarly, ![](img/B15558_07_079.png) denotes
    that image *x* is sampled from the generator distribution, *P*[g], and ![](img/B15558_07_080.png)
    implies that the generator input, *z*, is sampled from the uniform distribution,
    *P*[z].
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: We've learned that both the generator and discriminator are neural networks
    and both of them update their parameters through backpropagation. We now need
    to find the optimal generator parameter, ![](img/B15558_07_081.png), and the discriminator
    parameter, ![](img/B15558_07_082.png).
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  id: totrans-473
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we will look at the loss function of the discriminator. We know that the
    goal of the discriminator is to classify whether the image is a real or a fake
    image. Let's denote the discriminator by *D*.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function of the discriminator is given as follows:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_083.png)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
- en: What does this mean, though? Let's understand each of the terms one by one.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: First term
  id: totrans-478
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s look at the first term:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_084.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B15558_07_085.png) implies that we are sampling input *x* from
    the real data distribution, *P*[r], so *x* is a real image.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '*D*(*x*) implies that we are feeding the input image *x* to the discriminator
    *D*, and the discriminator will return the probability of input image *x* to be
    a real image. As *x* is sampled from real data distribution *P*[r], we know that
    *x* is a real image. So, we need to maximize the probability of *D*(*x*):'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_086.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
- en: 'But instead of maximizing raw probabilities, we maximize log probabilities,
    so, we can write the following:'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_087.png)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
- en: 'So, our final equation becomes the following:'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_088.png)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
- en: '![](img/B15558_07_089.png) represents the expectations of the log-likelihood
    of input images sampled from the real data distribution being real.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
- en: Second term
  id: totrans-489
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s look at the second term:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_090.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B15558_07_091.png) shows that we are sampling a random noise *z*
    from the uniform distribution *P*[z]. *G*(*z*) implies that the generator *G*
    takes the random noise *z* as an input and returns a fake image based on its implicitly
    learned distribution *P*[g].
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '*D*(*G*(*z*)) implies that we are feeding the fake image generated by the generator
    to the discriminator *D* and it will return the probability of the fake input
    image being a real image.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: 'If we subtract *D*(*G*(*z*)) from 1, then it will return the probability of
    the fake input image being a fake image:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_092.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
- en: 'Since we know *z* is not a real image, the discriminator will maximize this
    probability. That is, the discriminator maximizes the probability of *z* being
    classified as a fake image, so we write:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_093.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
- en: 'Instead of maximizing raw probabilities, we maximize the log probability:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_094.png)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
- en: '![](img/B15558_07_095.png) implies the expectations of the log likelihood of
    the input images generated by the generator being fake.'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
- en: Final term
  id: totrans-501
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'So, combining these two terms, the loss function of the discriminator is given
    as follows:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_083.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B15558_07_097.png) and ![](img/B15558_07_098.png) are the parameters
    of the generator and discriminator network respectively. So, the discriminator's
    goal is to find the right ![](img/B15558_07_099.png) so that it can classify the
    image correctly.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: Generator loss
  id: totrans-505
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The loss function of the generator is given as follows:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_100.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
- en: We know that the goal of the generator is to fool the discriminator to classify
    the fake image as a real image.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: In the *Discriminator loss* section, we saw that ![](img/B15558_07_101.png)
    implies the probability of classifying the fake input image as a fake image, and
    the discriminator maximizes the probabilities for correctly classifying the fake
    image as fake.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: 'But the generator wants to minimize this probability. As the generator wants
    to fool the discriminator, it minimizes this probability of a fake input image
    being classified as fake by the discriminator. Thus, the loss function of the
    generator can be expressed as follows:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_100.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
- en: Total loss
  id: totrans-512
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We just learned the loss function of the generator and the discriminator combining
    these two losses, and we write our final loss function as follows:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_103.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
- en: So, our objective function is basically a min-max objective function, that is,
    a maximization for the discriminator and a minimization for the generator, and
    we find the optimal generator parameter, ![](img/B15558_07_104.png), and discriminator
    parameter, ![](img/B15558_07_105.png), through backpropagating the respective
    networks.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we perform gradient ascent; that is, maximization on the discriminator:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_106.png)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
- en: 'And, we perform gradient descent; that is, minimization on the generator:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_107.png)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
- en: Summary
  id: totrans-520
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding biological and artificial neurons.
    Then we learned about ANNs and their layers. We learned different types of activation
    functions and how they are used to introduce nonlinearity in the network.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned about the forward and backward propagation in the neural network.
    Next, we learned how to implement an ANN. Moving on, we learned about RNNs and
    how they differ from feedforward networks. Next, we learned about the variant
    of the RNN called LSTM. Going forward, we learned about CNNs, how they use different
    types of layers, and the architecture of CNNs in detail.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about an interesting algorithm called
    GAN. We understood the generator and discriminator component of GAN and we also
    explored the architecture of GAN in detail. Followed by that, we examined the
    loss function of GAN in detail.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about one of the most popularly used deep
    learning frameworks, called TensorFlow.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-525
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assess our understanding of deep learning algorithms by answering the
    following questions:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: What is the activation function?
  id: totrans-527
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the softmax function.
  id: totrans-528
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an epoch?
  id: totrans-529
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the applications of RNNs?
  id: totrans-530
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the vanishing gradient problem.
  id: totrans-531
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different types of pooling operations?
  id: totrans-532
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the generator and discriminator components of GANs.
  id: totrans-533
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-534
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about deep learning algorithms, you can check out my book **Hands-on
    Deep Learning Algorithms with Python**, also published by Packt, at [https://www.packtpub.com/in/big-data-and-business-intelligence/hands-deep-learning-algorithms-python](https://www.packtpub.com/in/big-data-and-business-intelligence/hands-deep-learning-algorithms-python).
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
