- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Foundations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in the previous chapters, we have learned how several reinforcement learning
    algorithms work and how they find the optimal policy. In the upcoming chapters,
    we will learn about **Deep Reinforcement Learning** (**DRL**), which is a combination
    of deep learning and reinforcement learning. To understand DRL, we need to have
    a strong foundation in deep learning. So, in this chapter, we will learn several
    important deep learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning is a subset of machine learning and it is all about neural networks.
    Deep learning has been around for a decade, but the reason it is so popular right
    now is because of the computational advancements and availability of huge volumes
    of data. With this huge volume of data, deep learning algorithms can outperform
    classic machine learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: We will start off the chapter by understanding what biological and artificial
    neurons are, and then we will learn about **Artificial Neural Networks** (**ANN**s)
    and how to implement them. Moving forward, we will learn about several interesting
    deep learning algorithms such as the **Recurrent Neural Network** (**RNN**), **Long
    Short-Term Memory** (**LSTM**), **Convolutional Neural Network** (**CNN**), and
    **Generative Adversarial Network** (**GAN**).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Biological and artificial neurons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ANNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin the chapter by understanding how biological and artificial neurons
    work.
  prefs: []
  type: TYPE_NORMAL
- en: Biological and artificial neurons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before going ahead, first, we will explore what neurons are and how neurons
    in our brain actually work, and then we will learn about artificial neurons.
  prefs: []
  type: TYPE_NORMAL
- en: A **neuron** can be defined as the basic computational unit of the human brain.
    Neurons are the fundamental units of our brain and nervous system. Our brain encompasses
    approximately 100 billion neurons. Each and every neuron is connected to one another
    through a structure called a **synapse**, which is accountable for receiving input
    from the external environment via sensory organs, for sending motor instructions
    to our muscles, and for performing other activities.
  prefs: []
  type: TYPE_NORMAL
- en: A neuron can also receive inputs from other neurons through a branchlike structure
    called a **dendrite**. These inputs are strengthened or weakened; that is, they
    are weighted according to their importance and then they are summed together in
    the cell body called the **soma**. From the cell body, these summed inputs are
    processed and move through the **axons** and are sent to the other neurons.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.1* shows a basic single biological neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Biological neuron'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see how artificial neurons work. Let''s suppose we have three inputs
    *x*[1], *x*[2], and *x*[3], to predict the output *y*. These inputs are multiplied
    by weights *w*[1], *w*[2], and *w*[3] and are summed together as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But why are we multiplying these inputs by weights? Because all of the inputs
    are not equally important in calculating the output *y*. Let''s say that *x*[2]
    is more important in calculating the output compared to the other two inputs.
    Then, we assign a higher value to *w*[2] than the other two weights. So, upon
    multiplying weights with inputs, *x*[2] will have a higher value than the other
    two inputs. In simple terms, weights are used for strengthening the inputs. After
    multiplying inputs with the weights, we sum them together and we add a value called
    bias, *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If you look at the preceding equation closely, it may look familiar. Doesn''t
    *z* look like the equation of linear regression? Isn''t it just the equation of
    a straight line? We know that the equation of a straight line is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *m* is the weights (coefficients), *x* is the input, and *b* is the bias
    (intercept).
  prefs: []
  type: TYPE_NORMAL
- en: 'Well, yes. Then, what is the difference between neurons and linear regression?
    In neurons, we introduce non-linearity to the result, *z*, by applying a function
    *f*(.) called the **activation** or **transfer function**. Thus, our output becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_004.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.2* shows a single artificial neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Artificial neuron'
  prefs: []
  type: TYPE_NORMAL
- en: So, a neuron takes the input, *x*, multiples it by weights, *w,* and adds bias,
    *b,* forms *z*, and then we apply the activation function on *z* and get the output,
    *y*.
  prefs: []
  type: TYPE_NORMAL
- en: ANN and its layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While neurons are really cool, we cannot just use a single neuron to perform
    complex tasks. This is the reason our brain has billions of neurons, stacked in
    layers, forming a network. Similarly, artificial neurons are arranged in layers.
    Each and every layer will be connected in such a way that information is passed
    from one layer to another.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical ANN consists of the following layers:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hidden layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each layer has a collection of neurons, and the neurons in one layer interact
    with all the neurons in the other layers. However, neurons in the same layer will
    not interact with one another. This is simply because neurons from the adjacent
    layers have connections or edges between them; however, neurons in the same layer
    do not have any connections. We use the term **nodes** or **units** to represent
    the neurons in the ANN.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.3* shows a typical ANN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: ANN'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **input layer** is where we feed input to the network. The number of neurons
    in the input layer is the number of inputs we feed to the network. Each input
    will have some influence on predicting the output. However, no computation is
    performed in the input layer; it is just used for passing information from the
    outside world to the network.
  prefs: []
  type: TYPE_NORMAL
- en: Hidden layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Any layer between the input layer and the output layer is called a **hidden
    layer**. It processes the input received from the input layer. The hidden layer
    is responsible for deriving complex relationships between input and output. That
    is, the hidden layer identifies the pattern in the dataset. It is majorly responsible
    for learning the data representation and for extracting the features.
  prefs: []
  type: TYPE_NORMAL
- en: There can be any number of hidden layers; however, we have to choose a number
    of hidden layers according to our use case. For a very simple problem, we can
    just use one hidden layer, but while performing complex tasks such as image recognition,
    we use many hidden layers, where each layer is responsible for extracting important
    features. The network is called a **deep neural network** when we have many hidden layers.
  prefs: []
  type: TYPE_NORMAL
- en: Output layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After processing the input, the hidden layer sends its result to the output
    layer. As the name suggests, the output layer emits the output. The number of
    neurons in the output layer is based on the type of problem we want our network
    to solve.
  prefs: []
  type: TYPE_NORMAL
- en: If it is a binary classification, then the number of neurons in the output layer
    is one, and it tells us which class the input belongs to. If it is a multi-class
    classification say, with five classes, and if we want to get the probability of
    each class as an output, then the number of neurons in the output layer is five,
    each emitting the probability. If it is a regression problem, then we have one
    neuron in the output layer.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring activation functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **activation function**, also known as a **transfer function**, plays a vital
    role in neural networks. It is used to introduce non-linearity in neural networks.
    As we learned before, we apply the activation function to the input, which is
    multiplied by weights and added to the bias, that is, *f*(*z*), where *z = (input
    * weights) + bias* and *f*(.) is the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: If we do not apply the activation function, then a neuron simply resembles the
    linear regression. The aim of the activation function is to introduce a non-linear
    transformation to learn the complex underlying patterns in the data.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at some of the interesting commonly used activation functions.
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **sigmoid function** is one of the most commonly used activation functions.
    It scales the value between 0 and 1\. The sigmoid function can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is an S-shaped curve shown in *Figure 7.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: It is differentiable, meaning that we can find the slope of the curve at any
    two points. It is **monotonic**, which implies it is either entirely non-increasing
    or non-decreasing. The sigmoid function is also known as a **logistic** function.
    As we know that probability lies between 0 and 1, and since the sigmoid function
    squashes the value between 0 and 1, it is used for predicting the probability
    of output.
  prefs: []
  type: TYPE_NORMAL
- en: The tanh function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **hyperbolic tangent** (**tanh**) function outputs the value between -1 to
    +1 and is expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It also resembles the S-shaped curve. Unlike the sigmoid function, which is
    centered on 0.5, the tanh function is 0-centered, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: tanh function'
  prefs: []
  type: TYPE_NORMAL
- en: The Rectified Linear Unit function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The **Rectified Linear Unit** (**ReLU**) function is another one of the most
    commonly used activation functions. It outputs a value from zero to infinity.
    It is basically a **piecewise** function and can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_007.png)'
  prefs: []
  type: TYPE_IMG
- en: 'That is, *f*(*x*) returns zero when the value of *x* is less than zero and
    *f*(*x*) returns *x* when the value of *x* is greater than or equal to zero. It
    can also be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_008.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.6* shows the ReLU function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: ReLU function'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding diagram, when we feed any negative input to the
    ReLU function, it converts the negative input to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The softmax function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **softmax function** is basically the generalization of the sigmoid function.
    It is usually applied to the final layer of the network and while performing multi-class
    classification tasks. It gives the probabilities of each class for being output
    and thus, the sum of softmax values will always equal 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the *Figure 7.7*, the softmax function converts its inputs to probabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Softmax function'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned about different activation functions, in the next section,
    we will learn about forward propagation in ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation in ANNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will see how an ANN learns where neurons are stacked up
    in layers. The number of layers in a network is equal to the number of hidden
    layers plus the number of output layers. We don''t take the input layer into account
    when calculating the number of layers in a network. Consider a two-layer neural
    network with one input layer, *x*, one hidden layer, *h*, and one output layer,
    *y*, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Forward propagation in ANN'
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider we have two inputs, *x*[1] and *x*[2], and we have to predict
    the output, ![](img/B15558_07_010.png). Since we have two inputs, the number of
    neurons in the input layer is two. We set the number of neurons in the hidden
    layer to four, and the number of neurons in the output layer to one. Now, the
    inputs are multiplied by weights, and then we add bias and propagate the resultant
    value to the hidden layer where the activation function is applied.
  prefs: []
  type: TYPE_NORMAL
- en: Before that, we need to initialize the weight matrix. In the real world, we
    don't know which input is more important than the other so that we can weight
    them and compute the output. Therefore, we randomly initialize the weights and
    bias value. The weight and the bias value between the input to the hidden layer
    are represented by *W*[xh] and *b*[h], respectively. What about the dimensions
    of the weight matrix? The dimensions of the weight matrix must be *the number
    of neurons in the current layer* x *the number of neurons in the next layer*.
    Why is that?
  prefs: []
  type: TYPE_NORMAL
- en: 'Because it is a basic matrix multiplication rule. To multiply any two matrices,
    *AB*, the number of columns in matrix *A* must be equal to the number of rows
    in matrix *B*. So, the dimension of the weight matrix, *W*[xh], should be *the
    number of neurons in the input layer* x *the number of neurons in the hidden layer*,
    that is, 2 x 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding equation represents, ![](img/B15558_07_012.png). Now, this is
    passed to the hidden layer. In the hidden layer, we apply an activation function
    to *z*[1]. Let''s use the sigmoid ![](img/B15558_07_013.png) activation function.
    Then, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After applying the activation function, we again multiply result *a*[1] by
    a new weight matrix and add a new bias value that is flowing between the hidden
    layer and the output layer. We can denote this weight matrix and bias as *W*[hy]
    and *b*[y], respectively. The dimension of the weight matrix, *W*[hy], will be
    *the number of neurons in the hidden layer* x *the number of neurons in the output
    layer*. Since we have four neurons in the hidden layer and one neuron in the output
    layer, the *W*[hy] matrix dimension will be 4 x 1\. So, we multiply *a*[1] by
    the weight matrix, *W*[hy], and add bias, *b*[y], and pass the result *z*[2] to
    the next layer, which is the output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, in the output layer, we apply the sigmoid function to *z*[2], which will
    result in an output value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This whole process from the input layer to the output layer is known as **forward
    propagation**. Thus, in order to predict the output value, inputs are propagated
    from the input layer to the output layer. During this propagation, they are multiplied
    by their respective weights on each layer and an activation function is applied
    on top of them. The complete forward propagation steps are given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_011.png)![](img/B15558_07_014.png)![](img/B15558_07_015.png)![](img/B15558_07_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding forward propagation steps can be implemented in Python as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Forward propagation is cool, isn''t it? But how do we know whether the output
    generated by the neural network is correct? We define a new function called the
    **cost function** (*J*), also known as the **loss function** (*L*), which tells
    us how well our neural network is performing. There are many different cost functions.
    We will use the mean squared error as a cost function, which can be defined as
    the mean of the squared difference between the actual output and the predicted
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_021.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the number of training samples, *y* is the actual output, and ![](img/B15558_07_022.png)
    is the predicted output.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, so we learned that a cost function is used for assessing our neural network;
    that is, it tells us how good our neural network is at predicting the output.
    But the question is where is our network actually learning? In forward propagation,
    the network is just trying to predict the output. But how does it learn to predict
    the correct output? In the next section, we will examine this.
  prefs: []
  type: TYPE_NORMAL
- en: How does an ANN learn?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the cost or loss is very high, then it means that our network is not predicting
    the correct output. So, our objective is to minimize the cost function so that
    our neural network predictions will be better. How can we minimize the cost function?
    That is, how can we minimize the loss/cost? We learned that the neural network
    makes predictions using forward propagation. So, if we can change some values
    in the forward propagation, we can predict the correct output and minimize the
    loss. But what values can we change in the forward propagation? Obviously, we
    can't change input and output. We are now left with weights and bias values. Remember
    that we just initialized weight matrices randomly. Since the weights are random,
    they are not going to be perfect. Now, we will update these weight matrices (*W*[xh]
    and *W*[hy]) in such a way that our neural network gives a correct output. How
    do we update these weight matrices? Here comes a new technique called **gradient
    descent**.
  prefs: []
  type: TYPE_NORMAL
- en: With gradient descent, the neural network learns the optimal values of the randomly
    initialized weight matrices. With the optimal values of weights, our network can
    predict the correct output and minimize the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will explore how the optimal values of weights are learned using gradient
    descent. Gradient descent is one of the most commonly used optimization algorithms.
    It is used for minimizing the cost function, which allows us to minimize the error
    and obtain the lowest possible error value. But how does gradient descent find
    the optimal weights? Let's begin with an analogy.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine we are on top of a hill, as shown in the following diagram, and we want
    to reach the lowest point on the hill. There could be many regions that look like
    the lowest points on the hill, but we have to reach the point that is actually
    the lowest of all.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, we should not be stuck at a point believing it is the lowest point
    when the global lowest point exists:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Analogy of gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we can represent our cost function as follows. It is a plot of cost
    against weights. Our objective is to minimize the cost function. That is, we have
    to reach the lowest point where the cost is the minimum. The solid dark point
    in the following diagram shows the randomly initialized weights. If we move this
    point downward, then we can reach the point where the cost is the minimum:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: But how can we move this point (initial weight) downward? How can we descend
    and reach the lowest point? Gradients are used for moving from one point to another.
    So, we can move this point (initial weight) by calculating a gradient of the cost
    function with respect to that point (initial weights), which is ![](img/B15558_07_023.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradients are the derivatives that are actually the slope of a tangent line,
    as illustrated in the following diagram. So, by calculating the gradient, we descend
    (move downward) and reach the lowest point where the cost is the minimum. Gradient
    descent is a first-order optimization algorithm, which means we only take into
    account the first derivative when performing the updates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Gradient descent'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, with gradient descent, we move our weights to a position where the cost
    is minimum. But still, how do we update the weights?
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result of forward propagation, we are in the output layer. We will now
    **backpropagate** the network from the output layer to the input layer and calculate
    the gradient of the cost function with respect to all the weights between the
    output and the input layer so that we can minimize the error. After calculating
    gradients, we update our old weights using the weight update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_024.png)'
  prefs: []
  type: TYPE_IMG
- en: This implies *weights = weights -α* x *gradients*.
  prefs: []
  type: TYPE_NORMAL
- en: What is ![](img/B15558_07_025.png)? It is called the **learning rate**. As shown
    in the following diagram, if the learning rate is small, then we take a small
    step downward and our gradient descent can be slow.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the learning rate is large, then we take a large step and our gradient descent
    will be fast, but we might fail to reach the global minimum and become stuck at
    a local minimum. So, the learning rate should be chosen optimally:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Effect of learning rate'
  prefs: []
  type: TYPE_NORMAL
- en: This whole process of backpropagating the network from the output layer to the
    input layer and updating the weights of the network using gradient descent to
    minimize the loss is called **backpropagation**. Now that we have a basic understanding
    of backpropagation, we will strengthen our understanding by learning about this
    in detail, step by step. We are going to look at some interesting math, so put
    on your calculus hats and follow the steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have two weights, one *W*[xh], which is the input to hidden layer weights,
    and the other *W*[hy], which is the hidden to output layer weights. We need to
    find the optimal values for these two weights that will give us the fewest errors.
    So, we need to calculate the derivative of the cost function *J* with respect
    to these weights. Since we are backpropagating, that is, going from the output
    layer to the input layer, our first weight will be *W*[hy]. So, now we need to
    calculate the derivative of *J* with respect to *W*[hy]. How do we calculate the
    derivative? First, let''s recall our cost function, *J*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We cannot calculate the derivative directly from the preceding equation since
    there is no *W*[hy] term. So, instead of calculating the derivative directly,
    we calculate the partial derivative. Let''s recall our forward propagation equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_016.png)![](img/B15558_07_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we will calculate a partial derivative with respect to ![](img/B15558_07_029.png),
    and then from ![](img/B15558_07_030.png) we will calculate the partial derivative
    with respect to *z*[2]. From *z*[2], we can directly calculate our derivative
    *W*[hy]. It is basically the chain rule. So, the derivative of *J* with respect
    to *W*[hy] becomes as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_031.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will compute each of the terms in the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_032.png)![](img/B15558_07_033.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B15558_07_034.png) is the derivative of our sigmoid activation
    function. We know that the sigmoid function is ![](img/B15558_07_035.png), so
    the derivative of the sigmoid function would be ![](img/B15558_07_036.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, substituting all the preceding terms in equation *(1)* we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_038.png)'
  prefs: []
  type: TYPE_IMG
- en: Now we need to compute a derivative of *J* with respect to our next weight,
    *W*[xh].
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, we cannot calculate the derivative of *W*[xh] directly from *J*
    as we don''t have any *W*[xh] terms in *J*. So, we need to use the chain rule.
    Let''s recall the forward propagation steps again:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_016.png)![](img/B15558_07_015.png)![](img/B15558_07_014.png)![](img/B15558_07_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, according to the chain rule, the derivative of *J* with respect to *W*[xh]
    is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_043.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We have already seen how to compute the first two terms in the preceding equation;
    now, we will see how to compute the rest of the terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_044.png)![](img/B15558_07_045.png)![](img/B15558_07_046.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, substituting all the preceding terms in equation *(3)*, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_047.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After we have computed gradients for both weights, *W*[hy] and *W*[xh], we
    will update our initial weights according to the weight update rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_048.png)![](img/B15558_07_049.png)'
  prefs: []
  type: TYPE_IMG
- en: That's it! This is how we update the weights of a network and minimize the loss.
    Now, let's see how to implement the backpropagation algorithm in Python.
  prefs: []
  type: TYPE_NORMAL
- en: 'In both the equations *(2)* and *(4)*, we have the term ![](img/B15558_07_050.png),
    so instead of computing them again and again, we just call them `delta2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we compute the gradient with respect to *W*[hy]. Refer to equation *(2)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We compute the gradient with respect to *W*[xh]. Refer to equation *(4)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We will update the weights according to our weight update rule equation *(5)*
    and *(6)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The complete code for the backpropagation is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: That's it. Apart from this, there are different variants of gradient descent
    methods such as stochastic gradient descent, mini-batch gradient descent, Adam,
    RMSprop, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on, let''s familiarize ourselves with some of the frequently
    used terminology in neural networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward pass**: Forward pass implies forward propagating from the input layer
    to the output layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backward pass**: Backward pass implies backpropagating from the output layer
    to the input layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epoch**: The epoch specifies the number of times the neural network sees
    our whole training data. So, we can say one epoch is equal to one forward pass
    and one backward pass for all training samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: The batch size specifies the number of training samples we
    use in one forward pass and one backward pass.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of iterations**: The number of iterations implies the number of passes
    where *one pass = one forward pass + one backward pass*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Say that we have 12,000 training samples and our batch size is 6,000\. Then
    it will take us two iterations to complete one epoch. That is, in the first iteration,
    we pass the first 6,000 samples and perform a forward pass and a backward pass;
    in the second iteration, we pass the next 6,000 samples and perform a forward
    pass and a backward pass. After two iterations, our neural network will see the
    whole 12,000 training samples, which makes it one epoch.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Putting all the concepts we have learned so far together, we will see how to
    build a neural network from scratch. We will understand how the neural network
    learns to perform the XOR gate operation. The XOR gate returns 1 only when exactly
    only one of its inputs is 1, else it returns 0, as shown in *Table 7.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 7.1: XOR operation'
  prefs: []
  type: TYPE_NORMAL
- en: Building a neural network from scratch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform the XOR gate operation, we build a simple two-layer neural network,
    as shown in the following diagram. As you can see, we have an input layer with
    two nodes, a hidden layer with five nodes and an output layer comprising one node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: ANN'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will understand step-by-step how a neural network learns the XOR logic:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the data as shown in the preceding XOR table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the number of nodes in each layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Initialize the weights and bias randomly. First, we initialize the input to
    hidden layer weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we initialize the hidden to output layer weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the sigmoid activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the derivative of the sigmoid function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the forward propagation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the backward propagation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the cost function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the learning rate and the number of training iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s start training the network with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the cost function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can observe in the following plot, the loss decreases over the training
    iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Cost function'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have an overall understanding of ANNs and how they learn.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*The sun rises in the ____.*'
  prefs: []
  type: TYPE_NORMAL
- en: If we were asked to predict the blank term in the preceding sentence, we would
    probably say east. Why would we predict that the word east would be the right
    word here? Because we read the whole sentence, understood the context, and predicted
    that the word east would be an appropriate word to complete the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: If we use a feedforward neural network (the one we learned in the previous section)
    to predict the blank, it would not predict the right word. This is due to the
    fact that in feedforward networks, each input is independent of other input and
    they make predictions based only on the current input, and they don't remember
    previous input.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the input to the network will just be the word preceding the blank, which
    is the word *the*. With this word alone as an input, our network cannot predict
    the correct word, because it doesn't know the context of the sentence, which means
    that it doesn't know the previous set of words to understand the context of the
    sentence and to predict an appropriate next word.
  prefs: []
  type: TYPE_NORMAL
- en: Here is where we use **Recurrent Neural Networks** (**RNNs**). They predict
    output not only based on the current input, but also on the previous hidden state.
    Why do they have to predict the output based on the current input and the previous
    hidden state? Why can't they just use the current input and the previous input?
  prefs: []
  type: TYPE_NORMAL
- en: This is because the previous input will only store information about the previous
    word, while the previous hidden state will capture the contextual information
    about all the words in the sentence that the network has seen so far. Basically,
    the previous hidden state acts like memory, and it captures the context of the
    sentence. With this context and the current input, we can predict the relevant
    word.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, let's take the same sentence, *The sun rises in the ____.* As
    shown in the following figure, we first pass the word *the* as an input, and then
    we pass the next word, *sun*, as input; but along with this, we also pass the
    previous hidden state, *h*[0]. So, every time we pass the input word, we also
    pass a previous hidden state as an input.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the final step, we pass the word *the*, and also the previous hidden state
    *h*[3], which captures the contextual information about the sequence of words
    that the network has seen so far. Thus, *h*[3] acts as the memory and stores information
    about all the previous words that the network has seen. With *h*[3] and the current
    input word (*the*), we can predict the relevant next word:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: RNN'
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, an RNN uses the previous hidden state as memory, which captures
    and stores the contextual information (input) that the network has seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are widely applied for use cases that involve sequential data, such as
    time series, text, audio, speech, video, weather, and much more. They have been
    greatly used in various **natural language processing** (**NLP**) tasks, such
    as language translation, sentiment analysis, text generation, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between feedforward networks and RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A comparison between an RNN and a feedforward network is shown in the *Figure
    7.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.16: Difference between feedforward network and RNN'
  prefs: []
  type: TYPE_NORMAL
- en: As you can observe in the preceding diagram, the RNN contains a looped connection
    in the hidden layer, which implies that we use the previous hidden state along
    with the input to predict the output.
  prefs: []
  type: TYPE_NORMAL
- en: Still confused? Let's look at the following unrolled version of an RNN. But
    wait; what is the unrolled version of an RNN?
  prefs: []
  type: TYPE_NORMAL
- en: 'It means that we roll out the network for a complete sequence. Let''s suppose
    that we have an input sentence with *T* words; then, we will have 0 to *T*—1 layers,
    one for each word, as shown in *Figure 7.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.17: Unrolled RNN'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in *Figure 7.17*, at the time step *t* = 1, the output *y*[1]
    is predicted based on the current input *x*[1] and the previous hidden state *h*[0].
    Similarly, at time step *t* = 2, *y*[2] is predicted using the current input *x*[2]
    and the previous hidden state *h*[1]. This is how an RNN works; it takes the current
    input and the previous hidden state to predict the output.
  prefs: []
  type: TYPE_NORMAL
- en: Forward propagation in RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s look at how an RNN uses forward propagation to predict the output; but
    before we jump right in, let''s get familiar with the notations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.18: Forward propagation in RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure illustrates the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '*U* represents the input to hidden layer weight matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W* represents the hidden to hidden layer weight matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*V* represents the hidden to output layer weight matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The hidden state *h* at a time step *t* can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_051.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, *the* *hidden state at a time step, t = tanh([input to hidden layer
    weight x input]* + *[hidden to hidden layer weight x previous hidden state])*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output at a time step *t* can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_052.png)'
  prefs: []
  type: TYPE_IMG
- en: That is, *the* *output at a time step, t = softmax (hidden to output layer weight*
    x *hidden state at a time t)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also represent RNNs as shown in the following figure. As you can see,
    the hidden layer is represented by an RNN block, which implies that our network
    is an RNN, and previous hidden states are used in predicting the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.19: Forward propagation in an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.20* shows how forward propagation works in an unrolled version of
    an RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.20: Unrolled version – forward propagation in an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'We initialize the initial hidden state *h*[init] with random values. As you
    can see in the preceding figure, the output, ![](img/B15558_07_053.png), is predicted
    based on the current input, *x*[0] and the previous hidden state, which is an
    initial hidden state, *h*[init], using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_054.png)![](img/B15558_07_055.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, look at how the output, ![](img/B15558_07_056.png), is computed.
    It takes the current input, *x*[1], and the previous hidden state, *h*[0]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_057.png)![](img/B15558_07_058.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, in forward propagation to predict the output, RNN uses the current input
    and the previous hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagating through time
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We just learned how forward propagation works in RNNs and how it predicts the
    output. Now, we compute the loss, *L*, at each time step, *t*, to determine how
    well the RNN has predicted the output. We use the cross-entropy loss as our loss
    function. The loss *L* at a time step *t* can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_059.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y*[t] is the actual output, and ![](img/B15558_07_060.png) is the predicted
    output at a time step *t*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final loss is a sum of the loss at all the time steps. Suppose that we
    have *T* - 1 layers; then, the final loss can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_061.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Figure 7.21* shows that the final loss is obtained by the sum of loss at all
    the time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.21: Backpropagation in an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'We computed the loss, now our goal is to minimize the loss. How can we minimize
    the loss? We can minimize the loss by finding the optimal weights of the RNN.
    As we learned, we have three weights in RNNs: input to hidden, *U*, hidden to
    hidden, *W*, and hidden to output, *V*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to find optimal values for all of these three weights to minimize the
    loss. We can use our favorite gradient descent algorithm to find the optimal weights.
    We begin by calculating the gradients of the loss function with respect to all
    the weights; then, we update the weights according to the weight update rule as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_062.png)![](img/B15558_07_063.png)![](img/B15558_07_064.png)'
  prefs: []
  type: TYPE_IMG
- en: However, we have a problem with the RNN. The gradient calculation involves calculating
    the gradient with respect to the activation function. When we calculate the gradient
    with respect to the sigmoid or tanh function, the gradient will become very small.
    When we further backpropagate the network over many time steps and multiply the
    gradients, the gradients will tend to get smaller and smaller. This is called
    a vanishing gradient problem.
  prefs: []
  type: TYPE_NORMAL
- en: Since the gradient vanishes over time, we cannot learn information about long-term
    dependencies, that is, RNNs cannot retain information for a long time in the memory.
    The vanishing gradient problem occurs not only in RNNs but also in other deep
    networks where we have many hidden layers and when we use sigmoid/tanh functions.
  prefs: []
  type: TYPE_NORMAL
- en: One solution to avoid vanishing gradient problem is to use ReLU as an activation
    function. However, we have a variant of the RNN called **Long Short-Term Memory**
    (**LSTM**), which can solve the vanishing gradient problem effectively. We will
    see how it works in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM to the rescue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While backpropagating an RNN, we learned about a problem called **vanishing
    gradients**. Due to the vanishing gradient problem, we cannot train the network
    properly, and this causes the RNN to not retain long sequences in the memory.
    To understand what we mean by this, let''s consider a small sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The sky is __*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An RNN can easily predict the blank as *blue* based on the information it has
    seen, but it cannot cover the long-term dependencies. What does that mean? Let''s
    consider the following sentence to understand the problem better:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Archie lived in China for 13 years. He loves listening to good music. He is
    a fan of comics. He is fluent in ____.*'
  prefs: []
  type: TYPE_NORMAL
- en: Now, if we were asked to predict the missing word in the preceding sentence,
    we would predict it as *Chinese*, but how did we predict that? We simply remembered
    the previous sentences and understood that Archie lived for 13 years in China.
    This led us to the conclusion that Archie might be fluent in Chinese. An RNN,
    on the other hand, cannot retain all of this information in its memory to say
    that Archie is fluent in Chinese.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the vanishing gradient problem, it cannot recollect/remember information
    for a long time in its memory. That is, when the input sequence is long, the RNN
    memory (hidden state) cannot hold all the information. To alleviate this, we use
    an LSTM cell.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM is a variant of an RNN that resolves the vanishing gradient problem and
    retains information in the memory as long as it is required. Basically, RNN cells
    are replaced with LSTM cells in the hidden units, as shown in *Figure 7.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.22: LSTM network'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will understand how the LSTM cells works.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the LSTM cell
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: What makes LSTM cells so special? How do LSTM cells achieve long-term dependency?
    How does it know what information to keep and what information to discard from
    the memory?
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all achieved by special structures called **gates**. As shown in the
    following diagram, a typical LSTM cell consists of three special gates called
    the input gate, output gate, and forget gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.23: LSTM gates'
  prefs: []
  type: TYPE_NORMAL
- en: 'These three gates are responsible for deciding what information to add, output,
    and forget from the memory. With these gates, an LSTM cell effectively keeps information
    in the memory only as long as required. *Figure 7.24* shows a typical LSTM cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.24: LSTM cell'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you look at the LSTM cell, the top horizontal line is called the cell state.
    It is where the information flows. Information on the cell state will be constantly
    updated by LSTM gates. Now, we will see the function of these gates:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forget gate**: The forget gate is responsible for deciding what information
    should not be in the cell state. Look at the following statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Harry is a good singer. He lives in New York. Zayn is also a good singer.*'
  prefs: []
  type: TYPE_NORMAL
- en: As soon as we start talking about Zayn, the network will understand that the
    subject has been changed from Harry to Zayn and the information about Harry is
    no longer required. Now, the forget gate will remove/forget information about
    Harry from the cell state.
  prefs: []
  type: TYPE_NORMAL
- en: '**Input gate**: The input gate is responsible for deciding what information
    should be stored in the memory. Let''s consider the same example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Harry is a good singer. He lives in New York. Zayn is also a good singer.*'
  prefs: []
  type: TYPE_NORMAL
- en: So, after the forget gate removes information from the cell state, the input
    gate decides what information has to be in the memory. Here, since the information
    about Harry is removed from the cell state by the forget gate, the input gate
    decides to update the cell state with the information about Zayn.
  prefs: []
  type: TYPE_NORMAL
- en: '**Output gate**: The output gate is responsible for deciding what information
    should be shown from the cell state at a time, *t*. Now, consider the following
    sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Zayn''s debut album was a huge success. Congrats ____.*'
  prefs: []
  type: TYPE_NORMAL
- en: Here, congrats is an adjective which is used to describe a noun. The output
    layer will predict Zayn (noun), to fill in the blank.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, using LSTM, we can overcome the vanishing gradient problem faced in RNN.
    In the next section, we will learn another interesting algorithm called the **Convolutional
    Neural Network** (**CNN**).
  prefs: []
  type: TYPE_NORMAL
- en: What are CNNs?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A CNN, also known as a **ConvNet**, is one of the most widely used deep learning
    algorithms for computer vision tasks. Let's say we are performing an image-recognition
    task. Consider the following image.
  prefs: []
  type: TYPE_NORMAL
- en: 'We want our CNN to recognize that it contains a horse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.25: Image containing a horse'
  prefs: []
  type: TYPE_NORMAL
- en: How can we do that? When we feed the image to a computer, it basically converts
    it into a matrix of pixel values. The pixel values range from 0 to 255, and the
    dimensions of this matrix will be of [*image width* x *image height* x *number
    of channels*]. A grayscale image has one channel, and colored images have three
    channels, **red, green, and blue** (**RGB**).
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a colored input image with a width of 11 and a height of 11,
    that is 11 x 11, then our matrix dimension would be *[11* x *11* x *3]*. As you
    can see in *[11* x *11* x *3]*, 11 x 11 represents the image width and height
    and 3 represents the channel number, as we have a colored image. So, we will have
    a 3D matrix.
  prefs: []
  type: TYPE_NORMAL
- en: But it is hard to visualize a 3D matrix, so, for the sake of understanding,
    let's consider a grayscale image as our input. Since the grayscale image has only
    one channel, we will get a 2D matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, the input grayscale image will be converted
    into a matrix of pixel values ranging from 0 to 255, with the pixel values representing
    the intensity of pixels at that point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.26: Input image is converted to matrix of pixel values'
  prefs: []
  type: TYPE_NORMAL
- en: The values given in the input matrix are just arbitrary values for our understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, now we have an input matrix of pixel values. What happens next? How does
    the CNN come to understand that the image contains a horse? CNNs consists of the
    following three important layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The convolutional layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fully connected layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the help of these three layers, the CNN recognizes that the image contains
    a horse. Now we will explore each of these layers in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The convolutional layer is the first and core layer of the CNN. It is one of
    the building blocks of a CNN and is used for extracting important features from
    the image.
  prefs: []
  type: TYPE_NORMAL
- en: We have an image of a horse. What do you think are the features that will help
    us to understand that this is an image of a horse? We can say body structure,
    face, legs, tail, and so on. But how does the CNN understand these features? This
    is where we use a convolution operation that will extract all the important features
    from the image that characterize the horse. So, the convolution operation helps
    us to understand what the image is all about.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what exactly is this convolution operation? How it is performed? How does
    it extract the important features? Let's look at this in detail.
  prefs: []
  type: TYPE_NORMAL
- en: As we know, every input image is represented by a matrix of pixel values. Apart
    from the input matrix, we also have another matrix called the **filter matrix**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The filter matrix is also known as a **kernel**, or simply a **filter**, as
    shown in the *Figure 7.27*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.27: Input and filter matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'We take the filter matrix, slide it over the input matrix by one pixel, perform
    element-wise multiplication, sum the results, and produce a single number. That''s
    pretty confusing, isn''t it? Let''s understand this better with the aid of the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.28: Convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in the previous diagram, we took the filter matrix and placed
    it on top of the input matrix, performed element-wise multiplication, summed their
    results, and produced the single number. This is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_065.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we slide the filter over the input matrix by one pixel and perform the
    same steps, as shown in *Figure 7.29*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_30.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.29: Convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_066.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Again, we slide the filter matrix by one pixel and perform the same operation,
    as shown in *Figure 7.30*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_31.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.30: Convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is demonstrated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_067.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, again, we slide the filter matrix over the input matrix by one pixel and
    perform the same operation, as shown in *Figure 7.31*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_32.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.31: Convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'That is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_068.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay. What are we doing here? We are basically sliding the filter matrix over
    the entire input matrix by one pixel, performing element-wise multiplication and
    summing their results, which creates a new matrix called a **feature map** or
    **activation map**. This is called the **convolution operation**.
  prefs: []
  type: TYPE_NORMAL
- en: As we've learned, the convolution operation is used to extract features, and
    the new matrix, that is, the feature maps, represents the extracted features.
    If we plot the feature maps, then we can see the features extracted by the convolution
    operation.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.32* shows the actual image (the input image) and the convolved image
    (the feature map). We can see that our filter has detected the edges from the
    actual image as a feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.32: Conversion of actual image to convolved image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Various filters are used for extracting different features from the image.
    For instance, if we use a sharpen filter, ![](img/B15558_07_069.png), then it
    will sharpen our image, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_34.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.33: Sharpened image'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we have learned that with filters, we can extract important features
    from the image using the convolution operation. So, instead of using one filter,
    we can use multiple filters to extract different features from the image and produce
    multiple feature maps. So, the depth of the feature map will be the number of
    filters. If we use seven filters to extract different features from the image,
    then the depth of our feature map will be seven:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_35.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.34: Feature maps'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, we have learned that different filters extract different features from
    the image. But the question is, how can we set the correct values for the filter
    matrix so that we can extract the important features from the image? Worry not!
    We just initialize the filter matrix randomly, and the optimal values of the filter
    matrix, with which we can extract the important features from the images, will
    be learned through backpropagation. However, we just need to specify the size
    of the filter and the number of filters we want to use.
  prefs: []
  type: TYPE_NORMAL
- en: Strides
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have just learned how a convolution operation works. We slide over the input
    matrix with the filter matrix by one pixel and perform the convolution operation.
    But we don't have to only slide over the input matrix by one pixel, we can also
    slide over the input matrix by any number of pixels.
  prefs: []
  type: TYPE_NORMAL
- en: The number of pixels we slide over the input matrix by the filter matrix is
    called a **stride**.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we set the stride to 2, then we slide over the input matrix with the filter
    matrix by two pixels. *Figure 7.35* shows a convolution operation with a stride
    of 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_36.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.35: Stride operation'
  prefs: []
  type: TYPE_NORMAL
- en: But how do we choose the stride number? We just learned that a stride is the
    number of pixels along that we move our filter matrix. So, when the stride is
    set to a small number, we can encode a more detailed representation of the image
    than when the stride is set to a large number. However, a stride with a high value
    takes less time to compute than one with a low value.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the convolution operation, we are sliding over the input matrix with a
    filter matrix. But in some cases, the filter does not perfectly fit the input
    matrix. What do we mean by that? For example, let''s say we are performing a convolution
    operation with a stride of 2\. There exists a situation where, when we move our
    filter matrix by two pixels, it reaches the border and the filter matrix does
    not fit the input matrix. That is, some part of our filter matrix is outside the
    input matrix, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_37.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.36: Padding operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we perform padding. We can simply pad the input matrix with zeros
    so that the filter can fit the input matrix, as shown in *Figure 7.37*. Padding
    with zeros on the input matrix is called **same padding** or **zero padding**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_38.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.37: Same padding'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of padding them with zeros, we can also simply discard the region of
    the input matrix where the filter doesn''t fit in. This is called **valid padding**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.38: Valid padding'
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Okay. Now, we are done with the convolution operation. As a result of the convolution
    operation, we have some feature maps. But the feature maps are too large in dimension.
    In order to reduce the dimensions of feature maps, we perform a pooling operation.
    This reduces the dimensions of the feature maps and keeps only the necessary details
    so that the amount of computation can be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: For example, to recognize a horse from the image, we need to extract and keep
    only the features of the horse; we can simply discard unwanted features, such
    as the background of the image and more. A pooling operation is also called a
    **downsampling** or **subsampling** operation, and it makes the CNN translation
    invariant. Thus, the pooling layer reduces spatial dimensions by keeping only
    the important features.
  prefs: []
  type: TYPE_NORMAL
- en: There are different types of pooling operations, including max pooling, average
    pooling, and sum pooling.
  prefs: []
  type: TYPE_NORMAL
- en: 'In max pooling, we slide over the filter on the input matrix and simply take
    the maximum value from the filter window, as *Figure 7.39* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_40.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.39: Max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: In average pooling, we take the average value of the input matrix within the
    filter window, and in sum pooling, we sum all the values of the input matrix within
    the filter window.
  prefs: []
  type: TYPE_NORMAL
- en: Fully connected layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we've learned how convolutional and pooling layers work. A CNN can have
    multiple convolutional layers and pooling layers. However, these layers will only
    extract features from the input image and produce the feature map; that is, they
    are just the feature extractors.
  prefs: []
  type: TYPE_NORMAL
- en: Given any image, convolutional layers extract features from the image and produce
    a feature map. Now, we need to classify these extracted features. So, we need
    an algorithm that can classify these extracted features and tell us whether the
    extracted features are the features of a horse, or something else. In order to
    make this classification, we use a feedforward neural network. We flatten the
    feature map and convert it into a vector, and feed it as an input to the feedforward
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feedforward network takes this flattened feature map as an input, applies
    an activation function, such as sigmoid, and returns the output, stating whether
    the image contains a horse or not; this is called a fully connected layer and
    is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_41.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.40: Fully connected layer'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how all this fits together.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 7.41* shows the architecture of a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_42.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.41: Architecture of CNN'
  prefs: []
  type: TYPE_NORMAL
- en: As you will notice, first we feed the input image to the convolutional layer,
    where we apply the convolution operation to extract important features from the
    image and create the feature maps. We then pass the feature maps to the pooling
    layer, where the dimensions of the feature maps will be reduced.
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the previous diagram, we can have multiple convolutional and pooling
    layers, and we should also note that the pooling layer does not necessarily have
    to be there after every convolutional layer; there can be many convolutional layers
    followed by a pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: So, after the convolutional and pooling layers, we flatten the resultant feature
    maps and feed it to a fully connected layer, which is basically a feedforward
    neural network that classifies the given input image based on the feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how CNNs work, in the next section, we will learn about
    another interesting algorithm called the generative adversarial network.
  prefs: []
  type: TYPE_NORMAL
- en: Generative adversarial networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative Adversarial Networks** (**GANs**) was first introduced by Ian
    J Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
    Ozair, Aaron Courville, and Yoshua Bengio in their paper, *Generative Adversarial
    Networks*, in 2014.'
  prefs: []
  type: TYPE_NORMAL
- en: GANs are used extensively for generating new data points. They can be applied
    to any type of dataset, but they are popularly used for generating images. Some
    of the applications of GANs include generating realistic human faces, converting
    grayscale images to colored images, translating text descriptions into realistic
    images, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'GANs have evolved so much in recent years that they can generate a very realistic
    image. The following figure shows the evolution of GANs in generating images over the
    course of five years:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_43.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.42: Evolution of GANs over the years'
  prefs: []
  type: TYPE_NORMAL
- en: Excited about GANs already? Now, we will see how exactly they work. Before going
    ahead, let's consider a simple analogy. Let's say you are the police and your
    task is to find counterfeit money, and the role of the counterfeiter is to create
    fake money and cheat the police.
  prefs: []
  type: TYPE_NORMAL
- en: 'The counterfeiter constantly tries to create fake money in a way that is so
    realistic that it cannot be differentiated from the real money. But the police
    have to identify whether the money is real or fake. So, the counterfeiter and
    the police essentially play a two-player game where one tries to defeat the other.
    GANs work something like this. They consist of two important components:'
  prefs: []
  type: TYPE_NORMAL
- en: Generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can perceive the generator as analogous to the counterfeiter, while the
    discriminator is analogous to the police. That is, the role of the generator is
    to create fake money, and the role of the discriminator is to identify whether
    the money is fake or real.
  prefs: []
  type: TYPE_NORMAL
- en: 'Without going into detail, first, we will get a basic understanding of GANs.
    Let''s say we want our GAN to generate handwritten digits. How can we do that?
    First, we will take a dataset containing a collection of handwritten digits; say,
    the MNIST dataset. The generator learns the distribution of images in our dataset.
    Thus, it learns the distribution of handwritten digits in our training set. Once,
    it learns the distribution of the images in our dataset, and we feed a random
    noise to the generator, it will convert the random noise into a new handwritten
    digit similar to the one in our training set based on the learned distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_44.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.43: Generator'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the discriminator is to perform a classification task. Given an
    image, it classifies it as real or fake; that is, whether the image is from the
    training set or the image is generated by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.44: Discriminator'
  prefs: []
  type: TYPE_NORMAL
- en: The generator component of GAN is basically a generative model, and the discriminator
    component is basically a discriminative model. Thus, the generator learns the
    distribution of the class and the discriminator learns the decision boundary of
    a class.
  prefs: []
  type: TYPE_NORMAL
- en: 'As *Figure 7.45* shows, we feed random noise to the generator, and it then
    converts this random noise into a new image similar to the one we have in our
    training set, but not exactly the same as the images in the training set. The
    image generated by the generator is called a fake image, and the images in our
    training set are called real images. We feed both the real and fake images to
    the discriminator, which tells us the probability of them being real. It returns
    0 if the image is fake and 1 if the image is real:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.45: GAN'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of generators and discriminators, we
    will study each of the components in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the generator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The generator component of a GAN is a generative model. When we say the generative
    model, there are two types of generative models—an **implicit** and an **explicit**
    density model. The implicit density model does not use any explicit density function
    to learn the probability distribution, whereas the explicit density model, as
    the name suggests, uses an explicit density function. GANs falls into the first
    category. That is, they are an implicit density model. Let's study in detail and
    understand how GANs are an implicit density model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a generator, *G*. It is basically a neural network parametrized
    by ![](img/B15558_07_070.png). The role of the generator network is to generate
    new images. How do they do that? What should be the input to the generator?
  prefs: []
  type: TYPE_NORMAL
- en: 'We sample a random noise, *z*, from a normal or uniform distribution, *P*[z].
    We feed this random noise, *z*, as an input to the generator and then it converts
    this noise to an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_071.png)'
  prefs: []
  type: TYPE_IMG
- en: Surprising, isn't it? How does the generator convert random noise to a realistic
    image?
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a dataset containing a collection of human faces and we want
    our generator to generate a new human face. First, the generator learns all the
    features of the face by learning the probability distribution of the images in
    our training set. Once the generator learns the correct probability distribution,
    it can generate totally new human faces.
  prefs: []
  type: TYPE_NORMAL
- en: But how does the generator learn the distribution of the training set? That
    is, how does the generator learn the distribution of images of human faces in
    the training set?
  prefs: []
  type: TYPE_NORMAL
- en: A generator is nothing but a neural network. So, what happens is that the neural
    network learns the distribution of the images in our training set implicitly;
    let's call this distribution a generator distribution, *P*[g]. At the first iteration,
    the generator generates a really noisy image. But over a series of iterations,
    it learns the exact probability distribution of our training set and learns to
    generate a correct image by tuning its ![](img/B15558_07_070.png) parameter.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that we are not using the uniform distribution *P*[z]
    for learning the distribution of our training set. It is only used for sampling
    random noise, and we feed this random noise as an input to the generator. The
    generator network implicitly learns the distribution of our training set and we
    call this distribution a generator distribution, *P*[g] and that is why we call
    our generator network an implicit density model.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand the generator, let's move on to the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the discriminator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the name suggests, the discriminator is a discriminative model. Let's say
    we have a discriminator, *D*. It is also a neural network and it is parametrized
    by ![](img/B15558_07_073.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the discriminator is to discriminate between two classes. That
    is, given an image *x*, it has to identify whether the image is from a real distribution
    or a fake distribution (generator distribution). That is, the discriminator has
    to identify whether the given input image is from the training set or the fake
    image generated by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_074.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's call the distribution of our training set the real data distribution,
    which is represented by *P*[r]. We know that the generator distribution is represented
    by *P*[g].
  prefs: []
  type: TYPE_NORMAL
- en: So, the discriminator *D* essentially tries to discriminate whether the image
    *x* is from *P*[r] or *P*[g].
  prefs: []
  type: TYPE_NORMAL
- en: How do they learn, though?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we just studied the role of the generator and discriminator, but how
    do they learn exactly? How does the generator learn to generate new realistic
    images and how does the discriminator learn to discriminate between images correctly?
  prefs: []
  type: TYPE_NORMAL
- en: We know that the goal of the generator is to generate an image in such a way
    as to fool the discriminator into believing that the generated image is from a
    real distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In the first iteration, the generator generates a noisy image. When we feed
    this image to the discriminator, it can easily detect that the image is from a
    generator distribution. The generator takes this as a loss and tries to improve
    itself, as its goal is to fool the discriminator. That is, if the generator knows
    that the discriminator is easily detecting the generated image as a fake image,
    then it means that it is not generating an image similar to those in the training
    set. This implies that it has not learned the probability distribution of the
    training set yet.
  prefs: []
  type: TYPE_NORMAL
- en: So, the generator tunes its parameters in such a way as to learn the correct
    probability distribution of the training set. As we know that the generator is
    a neural network, we simply update the parameters of the network through backpropagation.
    Once it has learned the probability distribution of the real images, then it can
    generate images similar to the ones in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what about the discriminator? How does it learn? As we know, the role
    of the discriminator is to discriminate between real and fake images.
  prefs: []
  type: TYPE_NORMAL
- en: If the discriminator incorrectly classifies the generated image; that is, if
    the discriminator classifies the fake image as a real image, then it implies that
    the discriminator has not learned to differentiate between the real and fake image.
    So, we update the parameter of the discriminator network through backpropagation
    to make the discriminator learn to classify between real and fake images.
  prefs: []
  type: TYPE_NORMAL
- en: So, basically, the generator is trying to fool the discriminator by learning
    the real data distribution, *P*[r], and the discriminator is trying to find out
    whether the image is from a real or fake distribution. Now the question is, when
    do we stop training the network in light of the fact that the generator and discriminator
    are competing against each other?
  prefs: []
  type: TYPE_NORMAL
- en: Basically, the goal of the GAN is to generate images similar to the one in the
    training set. Say we want to generate a human face—we learn the distribution of
    images in the training set and generate new faces. So, for a generator, we need
    to find the optimal discriminator. What do we mean by that?
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that a generator distribution is represented by *P*[g] and the real
    data distribution is represented by *P*[r]. If the generator learns the real data
    distribution perfectly, then *P*[g] equals *P*[r], as *Figure 7.46* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_47.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.46: Generator and real data distribution'
  prefs: []
  type: TYPE_NORMAL
- en: When *P*[g] = *P*[r], then the discriminator cannot differentiate between whether
    the input image is from a real or a fake distribution, so it will just return
    0.5 as a probability, as the discriminator will become confused between the two
    distributions when they are the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for a generator, the optimal discriminator can be given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_075.png)'
  prefs: []
  type: TYPE_IMG
- en: So, when the discriminator just returns the probability of 0.5 for all the generator
    images, then we can say that the generator has learned the distribution of images
    in our training set and has fooled the discriminator successfully.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of a GAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 7.47* shows the architecture of a GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_48.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.47: Architecture of GAN'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding diagram, generator *G* takes the random noise, *z*,
    as input by sampling from a uniform or normal distribution and generates a fake
    image by implicitly learning the distribution of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: We sample an image, *x*, from the real data distribution, ![](img/B15558_07_076.png),
    and fake data distribution, ![](img/B15558_07_077.png), and feed it to the discriminator,
    *D*. We feed real and fake images to the discriminator and the discriminator performs
    a binary classification task. That is, it returns 0 when the image is fake and
    1 when the image is real.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying the loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we will examine the loss function of the GAN. Before going ahead, let''s
    recap the notation:'
  prefs: []
  type: TYPE_NORMAL
- en: A noise that is fed as an input to the generator is represented by *z*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The uniform or normal distribution from which the noise *z* is sampled is represented
    by *P*[z]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An input image is represented by *x*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The real data distribution or the distribution of our training set is represented
    by *P*[r]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fake data distribution or the distribution of the generator is represented
    by *P*[g]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When we write ![](img/B15558_07_078.png), it implies that image *x* is sampled
    from the real distribution, *P*[r]. Similarly, ![](img/B15558_07_079.png) denotes
    that image *x* is sampled from the generator distribution, *P*[g], and ![](img/B15558_07_080.png)
    implies that the generator input, *z*, is sampled from the uniform distribution,
    *P*[z].
  prefs: []
  type: TYPE_NORMAL
- en: We've learned that both the generator and discriminator are neural networks
    and both of them update their parameters through backpropagation. We now need
    to find the optimal generator parameter, ![](img/B15558_07_081.png), and the discriminator
    parameter, ![](img/B15558_07_082.png).
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now we will look at the loss function of the discriminator. We know that the
    goal of the discriminator is to classify whether the image is a real or a fake
    image. Let's denote the discriminator by *D*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The loss function of the discriminator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_083.png)'
  prefs: []
  type: TYPE_IMG
- en: What does this mean, though? Let's understand each of the terms one by one.
  prefs: []
  type: TYPE_NORMAL
- en: First term
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s look at the first term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_084.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B15558_07_085.png) implies that we are sampling input *x* from
    the real data distribution, *P*[r], so *x* is a real image.
  prefs: []
  type: TYPE_NORMAL
- en: '*D*(*x*) implies that we are feeding the input image *x* to the discriminator
    *D*, and the discriminator will return the probability of input image *x* to be
    a real image. As *x* is sampled from real data distribution *P*[r], we know that
    *x* is a real image. So, we need to maximize the probability of *D*(*x*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_086.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But instead of maximizing raw probabilities, we maximize log probabilities,
    so, we can write the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_087.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, our final equation becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_088.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B15558_07_089.png) represents the expectations of the log-likelihood
    of input images sampled from the real data distribution being real.'
  prefs: []
  type: TYPE_NORMAL
- en: Second term
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s look at the second term:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_090.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B15558_07_091.png) shows that we are sampling a random noise *z*
    from the uniform distribution *P*[z]. *G*(*z*) implies that the generator *G*
    takes the random noise *z* as an input and returns a fake image based on its implicitly
    learned distribution *P*[g].
  prefs: []
  type: TYPE_NORMAL
- en: '*D*(*G*(*z*)) implies that we are feeding the fake image generated by the generator
    to the discriminator *D* and it will return the probability of the fake input
    image being a real image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we subtract *D*(*G*(*z*)) from 1, then it will return the probability of
    the fake input image being a fake image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_092.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know *z* is not a real image, the discriminator will maximize this
    probability. That is, the discriminator maximizes the probability of *z* being
    classified as a fake image, so we write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_093.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of maximizing raw probabilities, we maximize the log probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_094.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B15558_07_095.png) implies the expectations of the log likelihood of
    the input images generated by the generator being fake.'
  prefs: []
  type: TYPE_NORMAL
- en: Final term
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'So, combining these two terms, the loss function of the discriminator is given
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_083.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B15558_07_097.png) and ![](img/B15558_07_098.png) are the parameters
    of the generator and discriminator network respectively. So, the discriminator's
    goal is to find the right ![](img/B15558_07_099.png) so that it can classify the
    image correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Generator loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The loss function of the generator is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_100.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that the goal of the generator is to fool the discriminator to classify
    the fake image as a real image.
  prefs: []
  type: TYPE_NORMAL
- en: In the *Discriminator loss* section, we saw that ![](img/B15558_07_101.png)
    implies the probability of classifying the fake input image as a fake image, and
    the discriminator maximizes the probabilities for correctly classifying the fake
    image as fake.
  prefs: []
  type: TYPE_NORMAL
- en: 'But the generator wants to minimize this probability. As the generator wants
    to fool the discriminator, it minimizes this probability of a fake input image
    being classified as fake by the discriminator. Thus, the loss function of the
    generator can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_100.png)'
  prefs: []
  type: TYPE_IMG
- en: Total loss
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We just learned the loss function of the generator and the discriminator combining
    these two losses, and we write our final loss function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_103.png)'
  prefs: []
  type: TYPE_IMG
- en: So, our objective function is basically a min-max objective function, that is,
    a maximization for the discriminator and a minimization for the generator, and
    we find the optimal generator parameter, ![](img/B15558_07_104.png), and discriminator
    parameter, ![](img/B15558_07_105.png), through backpropagating the respective
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we perform gradient ascent; that is, maximization on the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_106.png)'
  prefs: []
  type: TYPE_IMG
- en: 'And, we perform gradient descent; that is, minimization on the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_07_107.png)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding biological and artificial neurons.
    Then we learned about ANNs and their layers. We learned different types of activation
    functions and how they are used to introduce nonlinearity in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned about the forward and backward propagation in the neural network.
    Next, we learned how to implement an ANN. Moving on, we learned about RNNs and
    how they differ from feedforward networks. Next, we learned about the variant
    of the RNN called LSTM. Going forward, we learned about CNNs, how they use different
    types of layers, and the architecture of CNNs in detail.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about an interesting algorithm called
    GAN. We understood the generator and discriminator component of GAN and we also
    explored the architecture of GAN in detail. Followed by that, we examined the
    loss function of GAN in detail.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about one of the most popularly used deep
    learning frameworks, called TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assess our understanding of deep learning algorithms by answering the
    following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the activation function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the softmax function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is an epoch?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are some of the applications of RNNs?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the vanishing gradient problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the different types of pooling operations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Explain the generator and discriminator components of GANs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To learn more about deep learning algorithms, you can check out my book **Hands-on
    Deep Learning Algorithms with Python**, also published by Packt, at [https://www.packtpub.com/in/big-data-and-business-intelligence/hands-deep-learning-algorithms-python](https://www.packtpub.com/in/big-data-and-business-intelligence/hands-deep-learning-algorithms-python).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
