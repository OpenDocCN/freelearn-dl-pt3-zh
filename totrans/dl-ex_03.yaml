- en: Feature Engineering and Model Complexity – The Titanic Example Revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Model complexity and assessment is a must-do step toward building a successful
    data science system. There are lots of tools that you can use to assess and choose
    your model. In this chapter, we are going to address some of the tools that can
    help you to increase the value of your data by adding more descriptive features
    and extracting meaningful information from existing ones. We are also going to
    address other tools related optimal number features and learn why it's a problem
    to have a large number of features and fewer training samples/observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the topics that will be explained in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The curse of dimensionality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Titanic example revisited—all together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias-variance decomposition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning visibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering is one of the key components that contribute to the model's
    performance. A simple model with the right features can perform better than a
    complicated one with poor features. You can think of the feature engineering process
    as the most important step in determining your predictive model's success or failure.
    Feature engineering will be much easier if you understand the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature engineering is used extensively by anyone who uses machine learning
    to solve only one question, which is: **how do you get the most out of your data
    samples for predictive modeling**? This is the problem that the process and practice
    of feature engineering solves, and the success of your data science skills starts
    by knowing how to represent your data well.'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive modeling is a formula or rule that transforms a list of features
    or input variables (*x*[1], *x*[2],..., *x*[n]) into an output/target of interest
    (y). So, what is feature engineering? It's the process of creating new input variables
    or features (*z*[1], *z*[2], ..., *z*[n]) from existing input variables (*x*[1],
    *x*[2],..., *x*[n]). We don't just create any new features; the newly created
    features should contribute and be relevant to the model's output. Creating such
    features that will be relevant to the model's output will be an easy process with
    knowledge of the domain (such as marketing, medical, and so on). Even if machine
    learning practitioners interact with some domain experts during this process,
    the outcome of the feature engineering process will be much better.
  prefs: []
  type: TYPE_NORMAL
- en: An example where domain knowledge can be helpful is modeling the likelihood
    of rain, given a set of input variables/features (temperature, wind speed, and
    percentage of cloud cover). For this specific example, we can construct a new
    binary feature called **overcast**, where its value equals 1 or no whenever the
    percentage of cloud cover is less than 20%, and equals 0 or yes otherwise. In
    this example, domain knowledge was essential to specify the threshold or cut-off
    percentage. The more thoughtful and useful the inputs, the better the reliability
    and predictivity of your model.
  prefs: []
  type: TYPE_NORMAL
- en: Types of feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature engineering as a technique has three main subcategories. As a deep learning
    practitioner, you have the freedom to choose between them or combine them in some
    way.
  prefs: []
  type: TYPE_NORMAL
- en: Feature selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes called **feature importance**, this is the process of ranking the
    input variables according to their contribution to the target/output variable.
    Also, this process can be considered a ranking process of the input variables
    according to their value in the predictive ability of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Some learning methods do this kind of feature ranking or importance as part
    of their internal procedures (such as decision trees). Mostly, these kind of methods
    uses entropy to filter out the less valuable variables. In some cases, deep learning
    practitioners use such learning methods to select the most important features
    and then feed them into a better learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Dimensionality reduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction is sometimes feature extraction, and it is the process
    of combining the existing input variables into a new set of a much reduced number
    of input variables. One of the most used methods for this type of feature engineering
    is **principle component analysis** (**PCA**), which utilizes the variance in
    data to come up with a reduced number of input variables that don't look like
    the original input variables.
  prefs: []
  type: TYPE_NORMAL
- en: Feature construction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feature construction is a commonly used type of feature engineering, and  people usually
    refer to it when they talk about feature engineering. This technique is the process
    of handcrafting or constructing new features from raw data. In this type of feature
    engineering, domain knowledge is very useful to manually make up other features
    from existing ones. Like other feature engineering techniques, the purpose of
    feature construction is to increase the predictivity of your model. A simple example
    of feature construction is using the date stamp feature to generate two new features,
    such as AM and PM, which might be useful to distinguish between day and night.
    We can also transform/convert noisy numerical features into simpler, nominal ones
    by calculating the mean value of the noisy feature and then determining whether
    a given row is more than or less than that mean value.
  prefs: []
  type: TYPE_NORMAL
- en: Titanic example revisited
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to go through the Titanic example again but from
    a different perspective while using the feature engineering tool. In case you
    skipped [Chapter 2](6e292a27-8ff3-4d9c-9186-433455cb380c.xhtml), *Data Modeling
    in Action - The Titanic Example*, the Titanic example is a Kaggle competition
    with the purpose of predicting weather a specific passenger survived or not.
  prefs: []
  type: TYPE_NORMAL
- en: 'During this revisit of the Titanic example, we are going to use the scikit-learn
    and pandas libraries. So first off, let''s start by reading the train and test
    sets and get some statistics about the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to point out a few things about the preceding code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: As shown, we have used the `concat` function of pandas to combine the data frames
    of the train and test sets. This is useful for the feature engineering task as
    we need a full view of the distribution of the input variables/features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After combining both data frames, we need to do some modifications to the output
    data frame.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Missing values
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This step will be the first thing to think of after getting a new dataset from
    the customer, because there will be missing/incorrect data in nearly every dataset.
    In the next chapters, you will see that some learning algorithms are able to deal
    with missing values and others need you to handle missing data. During this example,
    we are going to use the random forest classifier from scikit-learn, which requires
    separate handling of missing data.
  prefs: []
  type: TYPE_NORMAL
- en: There are different approaches that you can use to handle missing data.
  prefs: []
  type: TYPE_NORMAL
- en: Removing any sample with missing values in it
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach won't be a good choice if you have a small dataset with lots of
    missing values, as removing the samples with missing values will produce useless
    data. It could be a quick and easy choice if you have lots of data, and removing
    it won't affect the original dataset much.
  prefs: []
  type: TYPE_NORMAL
- en: Missing value inputting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This approach is useful when you have categorical data. The intuition behind
    this approach is that missing values may correlate with other variables, and removing
    them will result in a loss of information that can affect the model significantly.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we have a binary variable with two possible values, -1 and
    1, we can add another value (0) to indicate a missing value. You can use the following
    code to replace the null values of the **Cabin** feature with `U0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Assigning an average value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is also one of the common approaches because of its simplicity. In the
    case of a numerical feature, you can just replace the missing values with the
    mean or median. You can also use this approach in the case of categorical variables
    by assigning the mode (the value that has the highest occurrence) to the missing
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code assigns the median of the non-missing values of the `Fare`
    feature to the missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Or, you can use the following code to find the value that has the highest occurrence
    in the `Embarked` feature and assign it to the missing values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Using a regression or another simple model to predict the values of missing
    variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the approach that we will use for the `Age` feature of the Titanic example.
    The `Age` feature is an important step towards predicting the survival of passengers,
    and applying the previous approach by taking the mean will make us lose some information.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Feature transformations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous two sections, we covered reading the train and test sets and
    combining them. We also handled some missing values. Now, we will use the random
    forest classifier of scikit-learn to predict the survival of passengers. Different
    implementations of the random forest algorithm accept different types of data.
    The scikit-learn implementation of random forest accepts only numeric data. So,
    we need to transform the categorical features into numerical ones.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Quantitative**: Quantitative features are measured in a numerical scale and
    can be meaningfully sorted. In the Titanic data samples, the `Age` feature is
    an example of a quantitative feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Qualitative**: Qualitative variables, also called **categorical variables**,
    are variables that are not numerical. They describe data that fits into categories.
    In the Titanic data samples, the `Embarked` (indicates the name of the departure
    port) feature is an example of a qualitative feature.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can apply different kinds of transformations to different variables. The
    following are some approaches that one can use to transform qualitative/categorical
    features.
  prefs: []
  type: TYPE_NORMAL
- en: Dummy features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: These variables are also known as categorical or binary features. This approach
    will be a good choice if we have a small number of distinct values for the feature
    to be transformed. In the Titanic data samples, the `Embarked` feature has only
    three distinct values (`S`, `C`, and `Q`) that occur frequently. So, we can transform
    the `Embarked` feature into three dummy variables, (`'Embarked_S'`, `'Embarked_C'`,
    and `'Embarked_Q'`) to be able to use the random forest classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will show you how to do this kind of transformation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Factorizing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This approach is used to create a numerical categorical feature from any other
    feature. In pandas, the `factorize()` function does that. This type of transformation
    is useful if your feature is an alphanumeric categorical variable. In the Titanic
    data samples, we can transform the `Cabin` feature into a categorical feature,
    representing the letter of the cabin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: We can also apply transformations to quantitative features by using one of the
    following approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This kind of transformation can be applied to numerical features only.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the Titanic data, the `Age` feature can reach 100, but the household
    income may be in millions. Some models are sensitive to the magnitude of values,
    so scaling such features will help those models perform better. Also, scaling
    can be used to squash a variable's values to be within a specific range.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will scale the `Age` feature by removing its mean from each
    value and scale to the unit variance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Binning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This kind of quantitative transformation is used to create quantiles. In this
    case, the quantitative feature values will be the transformed ordered variable.
    This approach is not a good choice for linear regression, but it might work well
    for learning algorithms that respond effectively when using ordered/categorical
    variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code applies this kind of transformation to the `Fare` feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Derived features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we applied some transformations to the Titanic data
    in order to be able to use the random forest classifier of scikit-learn (which
    only accepts numerical data). In this section, we are going to define another
    type of variable, which is derived from one or more other features.
  prefs: []
  type: TYPE_NORMAL
- en: Under this definition, we can say that some of the transformations in the previous
    section are also called **derived features**. In this section, we will look into
    other, complex transformations.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous sections, we mentioned that you need to use your feature engineering
    skills to derive new features to enhance the model's predictive power. We have
    also talked about the importance of feature engineering in the data science pipeline
    and why you should spend most of your time and effort coming up with useful features.
    Domain knowledge will be very helpful in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Very simple examples of derived features will be something like extracting the
    country code and/or region code from a telephone number. You can also extract
    the country/region from the GPS coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: The Titanic data is a very simple one and doesn't contain a lot of variables
    to work with, but we can try to derive some features from the text feature that
    we have in it.
  prefs: []
  type: TYPE_NORMAL
- en: Name
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `name` variable by itself is useless for most datasets, but it has two
    useful properties. The first one is the length of your name. For example, the
    length of your name may reflect something about your status and hence your ability
    to get on a lifeboat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The second interesting property is the `Name` title, which can also be used
    to indicate status and/or gender:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can also try to come up with other interesting features from the `Name`
    feature. For example, you might think of using the last name feature to find out
    the size of family members on the Titanic ship.
  prefs: []
  type: TYPE_NORMAL
- en: Cabin
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the Titanic data, the `Cabin` feature is represented by a letter, which
    indicates the deck, and a number, which indicates the room number. The room number
    increases towards the back of the boat, and this will provide some useful measure
    of the passenger''s location. We can also get the status of the passenger from
    the different decks, and this will help to determine who gets on the lifeboats:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Ticket
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code of the `Ticket` feature is not immediately clear, but we can do some
    guesses and try to group them. After looking at the Ticket feature, you may get
    these clues:'
  prefs: []
  type: TYPE_NORMAL
- en: Almost a quarter of the tickets begin with a character while the rest consist
    of only numbers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number part of the ticket code seems to have some indications about the
    class of the passenger. For example, numbers starting with 1 are usually first
    class tickets, 2 are usually second, and 3 are third. I say *usually* because
    it holds for the majority of examples, but not all. There are also ticket numbers
    starting with 4-9, and those are rare and almost exclusively third class.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several people can share a ticket number, which might indicate a family or close
    friends traveling together and acting like a family.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code tries to analyze the ticket feature code to come up with
    preceding clues:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Interaction features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Interaction features are obtained by performing mathematical operations on
    sets of features and indicate the effect of the relationship between variables.
    We use basic mathematical operations on the numerical features and see the effects
    of the relationship between variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This kind of feature engineering can produce lots of features. In the preceding
    code snippet, we used 9 features to generate 176 interaction features.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also remove highly correlated features as the existence of these features
    won''t add any information to the model. We can use Spearman''s correlation to
    identify and remove highly correlated features. The Spearman method has a rank
    coefficient in its output that can be used to identity the highly correlated features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The curse of dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to better explain the curse of dimensionality and the problem of overfitting,
    we are going to go through an example in which we have a set of images. Each image
    has a cat or a dog in it. So, we would like to build a model that can distinguish
    between the images with cats and the ones with dogs. Like the fish recognition
    system in *[Chapter 1](c6be0d67-2ba9-45ac-b6dd-116518853f42.xhtml)*, *Data science
    - Bird's-eye view*, we need to find an explanatory feature that the learning algorithm
    can use to distinguish between the two classes (cats and dogs). In this example,
    we can argue that color is a good descriptor to be used to differentiate between
    cats and dogs. So the average red, average blue, and average green colors can be
    used as explanatory features to distinguish between the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm will then combine these three features in some way to form a decision
    boundary between the two classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simple linear combination of the three features can be something like the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'These descriptive features will not be enough to get a good performing classifie,
    so we can decide to add more features that will enhance the model predictivity
    to discriminate between cats and dogs. For example, we can consider adding some
    features such as the texture of the image by calculating the average edge or gradient
    intensity in both dimensions of the image, X and Y. After adding these two features,
    the model accuracy will improve. We can even make the model/classifier get more
    accurate classification power by adding more and more features that are based
    on color, texture histograms, statistical moments, and so on. We can easily add
    a few hundred of these features to enhance the model''s predictivity. But the
    counter-intuitive results will be worse after increasing the features beyond some
    limit. You''ll better understand this by looking at *Figure 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c2fec23b-79b5-4122-bafc-90c2ce8c0df8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Model performance versus number of features'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 1* shows that as the number of features increases, the classifier''s
    performance increases as well, until we reach the optimal number of features.
    Adding more features based on the same size of the training set will then degrade
    the classifier''s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding the curse of dimensionality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we showed that the classifier's performance will decrease
    when the number of features exceeds a certain optimal point. In theory, if you
    have infinite training samples, the curse of dimensionality won't exist. So, the
    optimal number of features is totally dependent on the size of your data.
  prefs: []
  type: TYPE_NORMAL
- en: An approach that will help you to avoid the harm of this curse is to subset
    *M* features from the large number of features *N*, where *M << N*. Each feature
    from *M* can be a combination of some features in *N*. There are some algorithms
    that can do this for you. These algorithms somehow try to find useful, uncorrelated,
    and linear combinations of the original *N* features. A commonly used technique
    for this is **principle component analysis** (**PCA**). PCA tries to find a smaller
    number of features that capture the largest variance of the original data. You
    can find more insights and a full explanation of PCA at this interesting blog: [http://www.visiondummy.com/2014/05/feature-extraction-using-pca/](http://www.visiondummy.com/2014/05/feature-extraction-using-pca/).
  prefs: []
  type: TYPE_NORMAL
- en: 'A useful and easy way to apply PCA over your original training features is
    by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: In the Titanic example, we tried to build the classifier with and without applying
    PCA on the original features. Because we used the random forest classifier at
    the end, we found that applying PCA isn't very helpful; random forest works very
    well without any feature transformations, and even correlated features don't really
    affect the model much.
  prefs: []
  type: TYPE_NORMAL
- en: Titanic example revisited – all together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to put all the bits and pieces of feature engineering
    and dimensionality reduction together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Bias-variance decomposition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we knew how to select the best hyperparameters for
    our model. This set of best hyperparameters was chosen based on the measure of
    minimizing the cross validated error. Now, we need to see how the model will perform
    over the unseen data, or the so-called out-of-sample data, which refers to new
    data samples that haven't been seen during the model training phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following example: we have a data sample of size 10,000, and we
    are going to train the same model with different train set sizes and plot the
    test error at each step. For example, we are going to take out 1,000 as a test
    set and use the other 9,000 for training. So for the first training round, we
    will randomly select a train set of size 100 out of those 9,000 items. We''ll
    train the model based on the *best* selected set of hyperparameters, test the
    model with the test set, and finally plot the train (in-sample) error and the
    test (out-of-sample) error. We repeat this training, testing, and plotting operation
    for different train sizes (for example, repeat with 500 out of the 9,000, then
    1,000 out of the 9,000, and so on).'
  prefs: []
  type: TYPE_NORMAL
- en: After doing all this training, testing, and plotting, we will get a graph of
    two curves, representing the train and test errors with the same model but across
    different train set sizes. From this graph, we will get to know how good our model
    is.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output graph, which will contain two curves representing the training and
    testing error, will be one of the four possible shapes shown in *Figure 2*. The
    source of this different shapes is Andrew Ng''s Machine Learning course on Coursera
    ([https://www.coursera.org/learn/machine-learning](https://www.coursera.org/learn/machine-learning)).
    It''s a great course with lots of insights and best practices for machine learning
    newbies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9a88f3c0-8a00-4e65-816b-022e6450a072.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Possible shapes for plotting the training and testing error over
    different training set sizes'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when should we accept our model and put it into production? And when do
    we know that our model is not performing well over the test set and hence won''t
    have a bad generalization error? The answer to these questions depends on the
    shape that you get from plotting the train error versus the test error on different
    training set sizes:'
  prefs: []
  type: TYPE_NORMAL
- en: If your shape looks like the *top left* one, it represents a low training error
    and generalizes well over the test set. This shape is a winner and you should
    go ahead and use this model in production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your shape is similar to the *top right* one, it represents a high training
    error (the model didn't manage to learn from the training samples) and even has
    worse generalization performance over the test set. This shape is a complete failure
    and you need to go back and see what's wrong with your data, chosen learning algorithm,
    and/or selected hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your shape is similar to the *bottom left* one, it represents a bad training
    error as the model didn't manage to capture the underlying structure of the data,
    which also fits the new test data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your shape is similar to the *bottom right* one, it represents high bias
    and variance. This means that your model hasn't figured out the training data
    very well and hence didn't generalize well over the testing set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias and variance are the components that we can use to figure out how good
    our model is. In supervised learning, there are two opposing sources of errors,
    and using the learning curves in *Figure 2*, we can figure out due to which component(s)
    our model is suffering. The problem of having high variance and low bias is called
    **overfitting**, which means that the model performed well over the training samples
    but didn't generalize well on the test set. On the other hand, the problem of
    having high bias and low variance is called **underfitting**, which means that
    the model didn't make use of the data and didn't manage to estimate the output/target
    from the input features. There are different approaches one can use to avoid getting
    into one of these problems. But usually, enhancing one of them will come at the
    expense of the second one.
  prefs: []
  type: TYPE_NORMAL
- en: We can solve the situation of high variance by adding more features from which
    the model can learn. This solution will most likely increase the bias, so you
    need to make some sort of trade-off between them.
  prefs: []
  type: TYPE_NORMAL
- en: Learning visibility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are lots of great data science algorithms that one can use to solve problems
    in different domains, but the key component that makes the learning process visible
    is having enough data. You might ask how much data is needed for the learning
    process to be visible and worth doing. As a rule of thumb, researchers and machine
    learning practitioners agree that you need to have data samples at least 10 times
    the number of **degrees of freedom** in your model.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in the case of linear models, the degree of freedom represents
    the number of features that you have in your dataset. If you have 50 explanatory
    features in your data, then you need at least 500 data samples/observations in
    your data.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking the rule of thumb
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In practice, you can get away with this rule and do learning with less than
    10 times the number of features in your data; this mostly happens if your model
    is simple and you are using something called **regularization** (addressed in
    the next chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Jake Vanderplas wrote an article ([https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/](https://jakevdp.github.io/blog/2015/07/06/model-complexity-myth/))
    to show that one can learn even if the data has more parameters than examples.
    To demonstrate this, he used regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the most important tools that machine learning practitioners
    use in order to make sense of their data and get the learning algorithm to get
    the most out of their data.
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering was the first and commonly used tool in data science; it's
    a must-have component in any data science pipeline. The purpose of this tool is
    to make better representations for your data and increase the predictive power
    of your model.
  prefs: []
  type: TYPE_NORMAL
- en: We saw how a large number of features can be problematic and lead to worse classifier
    performance. We also saw that there is an optimal number of features that should
    be used to get the maximum model performance, and this optimal number of features
    is a function of the number of data samples/observations you got.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently, we introduced one of the most powerful tools, which is bias-variance
    decomposition. This tool is widely used to test how good the model is over the
    test set.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we went through learning visibility, which answers the question of
    how much data we should need in order to get in business and do machine learning.
    The rule of thumb showed that we need data samples/observations at least 10 times
    the number of features in your data. However, this rule of thumb can be broken
    by using another tool called regularization, which will be addressed in more detail
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next up, we are going to continue to increase our data science tools that we
    can use to drive meaningful analytics from our data, and face some daily problems
    of applying machine learning.
  prefs: []
  type: TYPE_NORMAL
