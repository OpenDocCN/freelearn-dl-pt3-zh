["```\n$ unzip news-headlines.tsv.zip\nArchive:  news-headlines.tsv.zip\n  inflating: news-headlines.tsv \n```", "```\n$ head -3 news-headlines.tsv\nThere Were 2 Mass Shootings In Texas Last Week, But Only 1 On TV there were 2 mass shootings in texas last week, but only 1 on tv\nWill Smith Joins Diplo And Nicky Jam For The 2018 World Cup's Official Song will smith joins diplo and nicky jam for the 2018 world cup's official song\nHugh Grant Marries For The First Time At Age 57 hugh grant marries for the first time at age 57 \n```", "```\nchars = sorted(set(\"abcdefghijklmnopqrstuvwxyz0123456789 -,;.!?:'''/\\|_@#$%ˆ&*˜'+-=()[]{}' ABCDEFGHIJKLMNOPQRSTUVWXYZ\"))\nchars = list(chars)\nEOS = '<EOS>'\nUNK = \"<UNK>\"\nPAD = \"<PAD>\"      # need to move mask to '0'index for Embedding layer\nchars.append(UNK)\nchars.append(EOS)  # end of sentence\nchars.insert(0, PAD)  # now padding should get index of 0 \n```", "```\n# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(chars)}\nidx2char = np.array(chars)\ndef char_idx(c):\n    # takes a character and returns an index\n    # if character is not in list, returns the unknown token\n    if c in chars:\n        return char2idx[c]\n\n    return char2idx[UNK] \n```", "```\ndata = []     # load into this list of lists \nMAX_LEN = 75  # maximum length of a headline \nwith open(\"news-headlines.tsv\", \"r\") as file:\n    lines = csv.reader(file, delimiter='\\t')\n    for line in lines:\n        hdln = line[0]\n        cnvrtd = [char_idx(c) for c in hdln[:-1]]  \n        if len(cnvrtd) >= MAX_LEN:\n            cnvrtd = cnvrtd[0:MAX_LEN-1]\n            cnvrtd.append(char2idx[EOS])\n        else:\n            cnvrtd.append(char2idx[EOS])\n            # add padding tokens\n            remain = MAX_LEN - len(cnvrtd)\n            if remain > 0:\n                for i in range(remain):\n                    cnvrtd.append(char2idx[PAD])\n        data.append(cnvrtd)\nprint(\"**** Data file loaded ****\") \n```", "```\n# now convert to numpy array\nnp_data = np.array(data)\n# for training, we use one character shifted data\nnp_data_in = np_data[:, :-1]\nnp_data_out = np_data[:, 1:] \n```", "```\n# Create TF dataset\nx = tf.data.Dataset.from_tensor_slices((np_data_in, np_data_out)) \n```", "```\n# Length of the vocabulary in chars\nvocab_size = len(chars)\n# The embedding dimension\nembedding_dim = 256\n# Number of RNN units\nrnn_units = 1024\n# batch size\nBATCH_SIZE=256 \n```", "```\n# create tf.DataSet\nx_train = x.shuffle(100000, reshuffle_each_iteration=True).batch(BATCH_SIZE, drop_remainder=True) \n```", "```\n# define the model\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              mask_zero=True,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model \n```", "```\nmodel = build_model(\n                  vocab_size = vocab_size,\n                  embedding_dim=embedding_dim,\n                  rnn_units=rnn_units,\n                  batch_size=BATCH_SIZE)\nprint(\"**** Model Instantiated ****\")\nprint(model.summary()) \n```", "```\n**** Model Instantiated ****\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #\n=================================================================\nembedding (Embedding)        (256, None, 256)          24576\n_________________________________________________________________\ngru (GRU)                    (256, None, 1024)         3938304\n_________________________________________________________________\ndropout (Dropout)            (256, None, 1024)         0\n_________________________________________________________________\ndense (Dense)                (256, None, 96)           98400\n=================================================================\nTotal params: 4,061,280\nTrainable params: 4,061,280\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\nmodel.compile(optimizer = 'adam', loss = loss) \n```", "```\n# Setup checkpoints \n# dynamically build folder names\ndt = datetime.datetime.today().strftime(\"%Y-%b-%d-%H-%M-%S\")\n# Directory where the checkpoints will be saved\ncheckpoint_dir = './training_checkpoints/'+dt\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True) \n```", "```\nprint(\"**** Start Training ****\")\nEPOCHS=25\nstart = time.time()\nhistory = model.fit(x_train, epochs=EPOCHS, \n                    callbacks=[checkpoint_callback])\nprint(\"**** End Training ****\")\nprint(\"Training time: \", time.time()- start) \n```", "```\n# Plot accuracies\nlossplot = \"loss-\" + dt + \".png\"\nplt.plot(history.history['loss'])\nplt.title('model loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.savefig(lossplot)\nprint(\"Saved loss to: \", lossplot) \n```", "```\n$ nohup python rnn-train.py > training.log & \n```", "```\nlr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n  0.001,\n  decay_steps=STEPS_PER_EPOCH*(EPOCHS/10),\n  decay_rate=2,\n  staircase=False) \n```", "```\noptimizer = tf.keras.optimizers.Adam(lr_schedule) \n```", "```\nclass LearningRateScheduler(tf.keras.callbacks.Callback):\n  \"\"\"Learning rate scheduler which decays the learning rate\"\"\"\n  def __init__(self, init_lr, decay, steps, start_epoch):\n    super().__init__()\n    self.init_lr = init_lr          # initial learning rate\n    self.decay = decay              # how sharply to decay\n    self.steps = steps              # total number of steps of decay\n    self.start_epoch = start_epoch  # which epoch to start decaying\n  def on_epoch_begin(self, epoch, logs=None):\n    if not hasattr(self.model.optimizer, 'lr'):\n      raise ValueError('Optimizer must have a \"lr\" attribute.')\n    # Get the current learning rate\n    lr = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n    if(epoch >= self.start_epoch):\n        # Get the scheduled learning rate.\n        scheduled_lr = self.init_lr / (1 + self.decay * (epoch / self.steps))\n        # Set the new learning rate\n        tf.keras.backend.set_value(self.model.optimizer.lr, \n                                     scheduled_lr)\n    print('\\nEpoch %05d: Learning rate is %6.4f.' % (epoch, scheduled_lr)) \n```", "```\nprint(\"**** Start Training ****\")\nEPOCHS=150\nlr_decay = LearningRateScheduler(0.001, 4., EPOCHS, 10)\nstart = time.time()\nhistory = model.fit(x_train, epochs=EPOCHS,\n                    callbacks=[checkpoint_callback, lr_decay])\nprint(\"**** End Training ****\")\nprint(\"Training time: \", time.time()- start)\nprint(\"Checkpoint directory: \", checkpoint_dir) \n```", "```\n...\nEpoch 8/150\n2434/2434 [==================] - 249s 102ms/step - loss: 0.9055\nEpoch 9/150\n2434/2434 [==================] - 249s 102ms/step - loss: 0.9052\nEpoch 10/150\n2434/2434 [==================] - 249s 102ms/step - loss: `0.9064`\nEpoch 00010: Learning rate is 0.00078947.\nEpoch 11/150\n2434/2434 [==================] - 249s 102ms/step - loss: `0.8949`\nEpoch 00011: Learning rate is 0.00077320.\nEpoch 12/150\n2434/2434 [==================] - 249s 102ms/step - loss: 0.8888\n...\nEpoch 00149: Learning rate is 0.00020107.\nEpoch 150/150\n2434/2434 [==================] - 249s 102ms/step - loss: `0.7667`\n**** End Training ****\nTraining time:  37361.16723680496\nCheckpoint directory:  ./training_checkpoints/2021-Jan-01-09-55-03\nSaved loss to:  loss-2021-Jan-01-09-55-03.png \n```", "```\n# Length of the vocabulary in chars\nvocab_size = len(chars)\n# The embedding dimension\nembedding_dim = 256\n# Number of RNN units\nrnn_units = 1024\n# Batch size\nBATCH_SIZE=1 \n```", "```\n# this one is without padding masking or dropout layer\ndef build_gen_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.GRU(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model\ngen_model = build_gen_model(vocab_size, embedding_dim, rnn_units, \n                            BATCH_SIZE) \n```", "```\ncheckpoint_dir = './training_checkpoints/**<YOUR-CHECKPOINT-DIR>'** \ngen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\ngen_model.build(tf.TensorShape([1, None])) \n```", "```\ndef generate_text(model, start_string, temperature=0.7, num_generate=75):\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n  # Empty string to store our results\n  text_generated = []\n  # Here batch size == 1\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n      predictions = tf.squeeze(predictions, 0)\n      # using a categorical distribution to predict the \n      # word returned by the model\n      predictions = predictions / temperature\n      predicted_id = tf.random.categorical(predictions, \n                               num_samples=1)[-1,0].numpy()\n      # We pass the predicted word as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n      # lets break is <EOS> token is generated\n      # if idx2char[predicted_id] == EOS:\n      # break #end of a sentence reached, let's stop\n  return (start_string + ''.join(text_generated)) \n```", "```\nprint(generate_text(gen_model, start_string=u\"Google\")) \n```", "```\nGoogle plans to release the Xbox One vs. Samsung Galaxy Gea<EOS><PAD>ote on Mother's Day \n```", "```\nprint(generate_text(gen_model, start_string=u\"Lets meet tom\", \n                    temperature=0.7, num_generate=10)) \n```", "```\nLets meet tomorrow to t \n```", "```\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\ngpt2tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n# add the EOS token as PAD token to avoid warnings\ngpt2 = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", \n                          pad_token_id=gpt2tokenizer.eos_token_id) \n```", "```\n# encode context the generation is conditioned on\ninput_ids = gpt2tokenizer.encode('Robotics is the domain of ', return_tensors='tf')\n# generate text until the output length \n# (which includes the context length) reaches 50\ngreedy_output = gpt2.generate(input_ids, max_length=50)\nprint(\"Output:\\n\" + 50 * '-')\nprint(gpt2tokenizer.decode(greedy_output[0], skip_special_tokens=True)) \n```", "```\nOutput:\n-----------------------------------------------------------\nRobotics is the domain of the United States Government.\nThe United States Government is the primary source of information on the use of drones in the United States.\nThe United States Government is the primary source of information on the use of drones \n```", "```\n# BEAM SEARCH\n# activate beam search and early_stopping\nbeam_output = gpt2.generate(\n    input_ids, \n    max_length=50, \n    num_beams=5, \n    early_stopping=True\n)\nprint(\"Output:\\n\" + 50 * '-')\nprint(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True)) \n```", "```\nOutput:\n--------------------------------------------------\nRobotics is the domain of science and technology. It is the domain of science and technology. It is the domain of science and technology. It is the domain of science and technology. It is the domain of science and technology. It is the domain \n```", "```\n# set no_repeat_ngram_size to 2\nbeam_output = gpt2.generate(\n    input_ids, \n    max_length=50, \n    num_beams=5, \n    no_repeat_ngram_size=3, \n    early_stopping=True\n)\nprint(\"Output:\\n\" + 50 * '-')\nprint(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True)) \n```", "```\nOutput:\n--------------------------------------------------\nRobotics is the domain of science and technology.\nIn this article, we will look at some of the most important aspects of robotics and how they can be used to improve the lives of people around the world. We will also take a look \n```", "```\n# Returning multiple beams\nbeam_outputs = gpt2.generate(\n    input_ids, \n    max_length=50, \n    num_beams=7, \n    no_repeat_ngram_size=3, \n    num_return_sequences=3,  \n    early_stopping=True,\n    temperature=0.7\n)\nprint(\"Output:\\n\" + 50 * '-')\nfor i, beam_output in enumerate(beam_outputs):\n  print(\"\\n{}: {}\".format(i, \n                   gpt2tokenizer.decode(beam_output,\n                          skip_special_tokens=True))) \n```", "```\nOutput:\n--------------------------------------------------\n0: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, Canada, Australia, New Zealand, and the European Union.\n1: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, France, Germany, Italy, Japan, and the European Union.\n2: Robotics is the domain of the U.S. Department of Homeland Security. The agency is responsible for the security of the United States and its allies, including the United Kingdom, Canada, Australia, New Zealand, the European Union, and the United\nThe text generated is very similar but differs near the end. Also, note that temperature is available to control the creativity of the generated text. \n```", "```\n# Top-K sampling\ntf.random.set_seed(42)  # for reproducible results\nbeam_output = gpt2.generate(\n    input_ids, \n    max_length=50, \n    do_sample=True, \n    top_k=25,\n    temperature=2\n)\nprint(\"Output:\\n\" + 50 * '-')\nprint(gpt2tokenizer.decode(beam_output[0], skip_special_tokens=True)) \n```", "```\nOutput:\n--------------------------------------------------\nRobotics is the domain of people with multiple careers working with robotics systems. The purpose of Robotics & Machine Learning in Science and engineering research is not necessarily different for any given research type because the results would be much more diverse.\nOur team uses \n```"]