- en: '*Chapter 3*: Implementing Advanced RL Algorithms'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provides short and crisp recipes to implement advanced **Reinforcement
    Learning** (**RL**) algorithms and agents from scratch using **TensorFlow 2.x**.
    It includes recipes to build **Deep-Q-Networks** (**DQN**), **Double and Dueling
    Deep Q-Networks** (**DDQN**, **DDDQN**), **Deep Recurrent Q-Networks** (**DRQN**),
    **Asynchronous Advantage Actor-Critic** (**A3C**), **Proximal Policy Optimization**
    (**PPO**), and **Deep Deterministic Policy Gradients** (**DDPG**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following recipes are discussed in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Deep Q-Learning algorithm, DQN, and Double-DQN agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Dueling DQN agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Dueling Double DQN algorithm and DDDQN agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Deep Recurrent Q-Learning algorithm and DRQN agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Asynchronous Advantage Actor-Critic algorithm and A3C agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Proximal Policy Optimization algorithm and PPO agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the Deep Deterministic Policy Gradient algorithm and DDPG agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04
    and should work with later versions of Ubuntu if Python 3.6+ is available. With
    Python 3.6+ installed along with the necessary Python packages as listed before
    the start of each of the recipes, the code should run fine on Windows and Mac
    OS X too. It is advised to create and use a Python virtual environment named `tf2rl-cookbook`
    to install the packages and run the code in this book. Miniconda or Anaconda installation
    for Python virtual environment management is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete code for each recipe in each chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook](https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Deep Q-Learning algorithm, DQN, and Double-DQN agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DQN agent uses a deep neural network to learn the Q-value function. DQN has
    shown itself to be a powerful algorithm for discrete action-space environments
    and problems and is considered to be a notable milestone in the history of deep
    reinforcement learning when DQN mastered Atari Games.
  prefs: []
  type: TYPE_NORMAL
- en: The Double-DQN agent uses two identical deep neural networks that are updated
    differently and so hold different weights. The second neural network is a copy
    of the main neural network from some time in the past (typically from the last
    episode).
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this recipe, you will have implemented a complete DQN and Double-DQN
    agent from scratch using TensorFlow 2.x that is ready to be trained in any discrete
    action-space RL environment.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now we can begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DQN agent comprises a few components, namely, the `DQN` class, the `Agent`
    class, and the `train` method. Perform the following steps to implement each of
    these components from scratch to build a complete DQN agent using TensorFlow 2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create an argument parser to handle configuration inputs to the
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now create a Tensorboard logger to log useful statistics during the
    agent''s training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s implement a `ReplayBuffer` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s now time to implement the DQN class that defines the deep neural network
    in TensorFlow 2.x:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get the prediction and action from the DQN, let''s implement the `predict`
    and `get_action` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the other components implemented, we can begin implementing our `Agent`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The crux of the Deep Q-learning algorithm is the q-learning update and experience
    replay. Let''s implement that next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next crucial step is to implement the `train` function to train the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s create the main function to start training the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To train the DQN agent in the default environment (`CartPole-v0`), execute
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space
    environment using the command-line arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, to implement the Double DQN agent, we must modify the `replay_experience`
    method to use Double Q-learning''s update step, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, to train the Double DQN agent, save and run the script with the updated
    `replay_experience` method or use the script provided as part of the source code
    for this book:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Updates to the weights in the DQN are performed as per the following Q learning
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B15074_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/Formula_B15074_03_002.png) is the change in the parameters (weights)
    of the DQN, s is the current state, a is the current action, s' is the next state,
    w represents the weights of the DQN, ![](img/Formula_B15074_03_003.png) is the
    discount factor, ![](img/Formula_B15074_03_004.png) is the learning rate, and
    ![](img/Formula_B15074_03_005.png) represents the Q-value for the given state
    (s) and action (a) predicted by the DQN with a weight ![](img/Formula_B15074_03_006.png).
  prefs: []
  type: TYPE_NORMAL
- en: To understand the difference between the DQN agent and the Double-DQN agent,
    compare the `replay_experience` method in step 8 (DQN) and step 13 (Double DQN).
    You will notice that the key difference lies in calculating the `next_q_values`.
    The DQN agent uses the maximum of the predicted Q-values (which can be an overestimation),
    whereas the Double DQN agent uses the predicted Q-value using two distinct neural
    Q networks. This is done in Double DQN to avoid the problem of overestimating
    the Q-values by the DQN agent.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Dueling DQN agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Dueling DQN agent explicitly estimates two quantities through a modified
    network architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: State values, V(*s*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Advantage values, A(*s*, *a*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The state value estimates the value of being in state s, and the advantage value
    represents the advantage of taking action *a* in state *s*. This key idea of explicitly
    and separately estimating the two quantities enables the Dueling DQN to perform
    better in comparison to DQN. This recipe will walk you through the steps to implement
    a Dueling DQN agent from scratch using TensorFlow 2.x.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now we can begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Dueling DQN agent comprises a few components, namely, the `DuelingDQN`
    class, the `Agent` class, and the `train` method. Perform the following steps
    to implement each of these components from scratch to build a complete Dueling
    DQN agent using TensorFlow 2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a first step, let''s create an argument parser to handle command-line configuration
    inputs to the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To log useful statistics during the agent''s training, let''s create a TensorBoard
    logger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s implement a `ReplayBuffer` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s time to implement the DuelingDQN class that defines the deep neural network
    in TensorFlow 2.x:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get the prediction and action from the Dueling DQN, let''s implement the
    `predict`, `get_action`, and `train` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now begin implementing our `Agent` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The crux of the Dueling Deep Q-learning algorithm is the q-learning update
    and experience replay. Let''s implement that next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next crucial step is to implement the `train` function to train the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s create the main function to start training the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To train the Dueling DQN agent in the default environment (`CartPole-v0`),
    execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space
    environment using the command-line arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let's see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Dueling-DQN agent differs from the DQN agent in terms of the neural network
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The differences are summarized in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – DQN and Dueling-DQN compared ](img/B15074_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – DQN and Dueling-DQN compared
  prefs: []
  type: TYPE_NORMAL
- en: The DQN (top half of the diagram) has a linear architecture and predicts a single
    quantity (Q(s, a)), whereas the Dueling-DQN has a bifurcation in the last layer
    and predicts multiple quantities.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Dueling Double DQN algorithm and DDDQN agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Dueling Double DQN** (**DDDQN**) combines the benefits of both Double Q-learning
    and Dueling architecture. Double Q-learning corrects DQN from overestimating the
    action values. The Dueling architecture uses a modified architecture to separately
    learn the state value function (V) and the advantage function (A). This explicit
    separation allows the algorithm to learn faster, especially when there are many
    actions to choose from and when the actions are very similar to each other. The
    dueling architecture enables the agent to learn even when only one action in a
    state has been taken, as it can update and estimate the state value function,
    unlike the DQN agent, which cannot learn from actions that were not taken yet.
    By the end of this recipe, you will have a complete implementation of the DDDQN
    agent.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The DDDQN agent combines the ideas in DQN, Double DQN and the Dueling DQN.
    Perform the following steps to implement each of these components from scratch
    to build a complete Dueling Double DQN agent using TensorFlow 2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create an argument parser to handle configuration inputs to the
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s create a Tensorboard logger to log useful statistics during the
    agent''s training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s implement a `ReplayBuffer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s now time to implement the Dueling DQN class that defines the neural network
    as per the dueling architecture to which we will add Double DQN updates in later
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get the prediction and action from the Dueling DQN, let''s implement the
    `predict` and `get_action` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the other components implemented, we can begin implementing our `Agent`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The main elements in the Dueling Double Deep Q-learning algorithm are the Q-learning
    update and experience replay. Let''s implement that next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next crucial step is to implement the `train` function to train the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s create the main function to start training the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To train the DQN agent in the default environment (`CartPole-v0`), execute
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also train the Dueling Double DQN agent in any OpenAI Gym-compatible
    discrete action-space environment using the command-line arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Dueling Double DQN architecture combines the advancements introduced by
    the Double DQN and Dueling architectures together.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Deep Recurrent Q-Learning algorithm and DRQN agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DRQN uses a recurrent neural network to learn the Q-value function. DRQN is
    more suited for reinforcement learning in environments with partial observability.
    The recurrent network layers in the DRQN allow the agent to learn by integrating
    information from a temporal sequence of observations. For example, DRQN agents
    can infer the velocity of moving objects in the environment without any changes
    to their inputs (for example, no frame stacking is required). By the end of this
    recipe, you will have a complete DRQN agent ready to be trained in an RL environment
    of your choice.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Dueling Double DQN agent combines the ideas in DQN, Double DQN, and the
    Dueling DQN. Perform the following steps to implement each of these components
    from scratch to build a complete DRQN agent using TensorFlow 2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create an argument parser to handle configuration inputs to the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s log useful statistics during the agent''s training using Tensorboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s implement a `ReplayBuffer`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It''s now time to implement the DRQN class that defines the deep neural network
    using TensorFlow 2.x:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get the prediction and action from the DRQN, let''s implement the `predict`
    and `get_action` methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the other components implemented, we can begin implementing our `Agent`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In addition to the `train` method in the DRQN class that we implemented in
    step 6, the crux of the deep recurrent Q-learning algorithm is the q-learning
    update and experience replay. Let''s implement that next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since the DRQN agent uses recurrent states, let''s implement the `update_states`
    method to update the recurrent state of the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next crucial step is to implement the `train` function to train the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s create the main training loop for the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To train the DRQN agent in the default environment (`CartPole-v0`), execute
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also train the DQN agent in any OpenAI Gym-compatible discrete action-space
    environment using the command-line arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DRQN agent uses an LSTM layer, which adds a recurrent learning capability
    to the agent. The LSTM layer is added to the agent's network in step 5 of the
    recipe. The other steps in the recipe have similar components as the DQN agent.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Asynchronous Advantage Actor-Critic algorithm and A3C agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The A3C algorithm builds upon the Actor-Critic class of algorithms by using
    a neural network to approximate the actor (and critic). The actor learns the policy
    function using a deep neural network, while the critic estimates the value function.
    The asynchronous nature of the algorithm allows the agent to learn from different
    parts of the state space, allowing parallel learning and faster convergence. Unlike
    DQN agents, which use an experience replay memory, the A3C agent uses multiple
    workers to gather more samples for learning. By the end of this recipe, you will
    have a complete script to train an A3C agent for any continuous action valued
    environment of your choice!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Now we can begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will implement an **Asynchronous, Advantage Actor-Critic** (**A3C**) algorithm
    by making use of Python''s multiprocessing and multithreading capabilities. The
    following steps will help you to implement a complete A3C agent from scratch using
    TensorFlow 2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create an argument parser to handle configuration inputs to the
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now create a Tensorboard logger to log useful statistics during the
    agent''s training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To have a count of the global episode number, let''s define a global variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now focus on implementing the `Actor` class, which will contain a neural
    network-based policy to act in the environments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get an action from the actor given a state, let''s define the `get_action`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, to compute the loss, we need to calculate the log of the policy (probability)
    density function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now use the `log_pdf` method to compute the actor loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the final step in the `Actor` class implementation, let''s define the `train`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the `Actor` class defined, we can move on to define the `Critic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s define the `train` method and a `compute_loss` method to train
    the critic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is time to implement the `A3CWorker` class based on Python''s Thread interface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will be using **n-step Temporal Difference (TD)** learning updates. Therefore,
    let''s define a method to calculate the n-step TD target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will also need to calculate the advantage values. The advantage value in
    its simplest form is easy to implement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will split the implementation of the `train` method into the following two
    steps. First, let''s implement the outer loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will complete the `train` method implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The run method for the `A3CWorker` thread will simply be the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s implement the `Agent` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The A3C agent makes use of several concurrent workers. In order to update each
    of the workers to update the A3C agent, the following code is necessary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With that, our A3C agent implementation is complete, and we are ready to define
    our main function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In simple terms, the crux of the A3C algorithm can be summarized in the following
    sequence of steps for each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Updating steps in the A3C agent learning iteration ](img/B15074_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Updating steps in the A3C agent learning iteration
  prefs: []
  type: TYPE_NORMAL
- en: The steps repeat again from top to bottom for the next iteration and so on until
    convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Proximal Policy Optimization algorithm and PPO agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Proximal Policy Optimization** (**PPO**) algorithm builds upon the work
    of **Trust Region Policy Optimization** (**TRPO**) to constrain the new policy
    to be within a trust region from the old policy. PPO simplifies the implementation
    of this core idea by using a clipped surrogate objective function that is easier
    to implement, yet quite powerful and efficient. It is one of the most widely used
    RL algorithms, especially for continuous control problems. By the end of this
    recipe, you will have built a PPO agent that you can train in your RL environment
    of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to get started.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will help you to implement a complete PPO agent from scratch
    using TensorFlow 2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create an argument parser to handle configuration inputs to the
    script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s create a Tensorboard logger to log useful statistics during the
    agent''s training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now focus on implementing the `Actor` class, which will contain a neural
    network-based policy to act:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get an action from the actor given a state, let''s define the `get_action`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, to compute the loss, we need to calculate the log of the policy (probability)
    density function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now use the `log_pdf` method to compute the actor loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the final step in the `Actor` class implementation, let''s define the `train`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the `Actor` class defined, we can move on to define the `Critic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s define the `train` method and a `compute_loss` method to train
    the critic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is now time to implement the PPO `Agent` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will be using the **Generalized Advantage Estimates** (**GAE**). Let''s
    implement a method to calculate the GAE target values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now split the implementation of the `train` method. First, let''s implement
    the outer loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will start the inner loop (per episode) implementation and
    finish it in the next couple of steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will use the value predictions made by the PPO algorithm to
    prepare for the policy update process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will implement the PPO algorithm''s policy update steps. These
    happen inside the inner loop whenever enough of an agent''s trajectory information
    is available in the form of sampled experience batches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the final step of the `train` method, we will reset the intermediate variables
    and print a summary of the episode reward obtained by the agent:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, our PPO agent implementation is complete, and we are ready to define
    our main function to start training!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The PPO algorithm uses clipping to form a surrogate loss function, and uses
    multiple epochs of **Stochastic Gradient Decent/Ascent** (**SGD**) optimization
    per the policy update. The clipping introduced by PPO reduces the effective change
    that can be applied to the policy, thereby improving the stability of the policy
    while learning.
  prefs: []
  type: TYPE_NORMAL
- en: The PPO agent uses actor(s) to collect samples from the environment using the
    latest policy parameters. The loop defined in step 15 of the recipe samples a
    mini-batch of experience and trains the network for n epochs (passed as the `--epoch`
    argument to the script) using the clipped surrogate objective function. The process
    is then repeated with new samples of experiences.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Deep Deterministic Policy Gradient algorithm and DDPG agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deterministic Policy Gradient (DPG)** is a type of Actor-Critic RL algorithm
    that uses two neural networks: one for estimating the action value function, and
    the other for estimating the optimal target policy. The **Deep Deterministic Policy
    Gradient** (**DDPG**) agent builds upon the idea of DPG and is quite efficient
    compared to vanilla Actor-Critic agents due to the use of deterministic action
    policies. By completing this recipe, you will have access to a powerful agent
    that can be trained efficiently in a variety of RL environments.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To complete this recipe, you will first need to activate the `tf2rl-cookbook`
    Conda Python virtual environment and `pip install -r requirements.txt`. If the
    following import statements run without issues, you are ready to get started!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Now we can begin.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will help you to implement a complete DDPG agent from scratch
    using TensorFlow 2.x:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s first create an argument parser to handle command-line configuration
    inputs to the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s create a Tensorboard logger to log useful statistics during the agent''s
    training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now implement an experience replay memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now focus on implementing the `Actor` class, which will contain a neural
    network-based policy to act:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To get an action from the actor given a state, let''s define the `get_action`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll implement a predict function to return the predictions made by
    the actor network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the final step in the `Actor` class implementation, let''s define the `train`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With the `Actor` class defined, we can move on to define the `Critic` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will be implementing a method to calculate the gradients of
    the Q function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a convenience method, let''s also define a `predict` function to return
    the critic network''s prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s define the `train` method and a `compute_loss` method to train
    the critic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It is now time to implement the DDPG `Agent` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s now implement the `update_target` method to update the actor and critic
    network''s weights with that of the respective target networks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let''s implement a helper method to calculate the TD targets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The purpose of the Deterministic Policy Gradient algorithm is to add noise
    to the actions sampled from the deterministic policy. Let''s use the **Ornstein-Uhlenback**
    (**OU**) process to generate noise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we will use experience replay to update the actor and critic:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With all the components we have implemented, we are now ready to put them together
    in the `train` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With that, our DDPG agent implementation is complete, and we are ready to define
    our main function to start training!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DDPG agent estimates two quantities – the Q-value function and the optimal
    policy. DDPG combines the ideas introduced in DQN and DPG. DDPG uses a policy
    gradient update rule in addition to the ideas introduced in DQN, as can be seen
    in the update steps defined in step 14.
  prefs: []
  type: TYPE_NORMAL
