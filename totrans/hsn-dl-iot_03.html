<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Learning Architectures for IoT</h1>
                </header>
            
            <article>
                
<p>In the era of the <strong>Internet of Things</strong> (<strong>IoT</strong>), an enormous amount of sensory data for a wide range of fields and applications is being generated and collected from numerous sensing devices. Applying analytics over such data streams to discover new information, predict future insights, and make controlled decisions, is a challenging task, which makes IoT a worthy paradigm for business intelligence and quality-of-life improving technology. However, analytics on IoT—enabled devices requires a platform consisting of <strong>machine learning</strong> (<strong>ML</strong>) and <strong>deep learning</strong> (<strong>DL</strong>) frameworks, a software stack, and hardware (for example, a <strong>Graphical Processing Unit</strong> (<strong>GPU</strong>) and <strong>Tensor Processing Unit</strong> (<strong>TPU</strong>)).</p>
<p>In this chapter, we will discuss some basic concepts of DL architectures and platforms, which will be used in all subsequent chapters. We will start with a brief introduction to ML. Then, we will move onto DL, which is a branch of ML based on a set of algorithms that attempts to model high-level abstractions in data. We will briefly discuss some of the most well-known and widely used neural network architectures. Then, we will look at various features of DL frameworks and libraries that can be used for developing DL applications on IoT-enabled devices. Briefly, the following topics will be covered:</p>
<ul>
<li>A soft introduction to ML</li>
<li>Artificial neural networks</li>
<li>Deep neural network architectures</li>
<li>DL frameworks</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A soft introduction to ML</h1>
                </header>
            
            <article>
                
<p>ML approaches are based on a set of statistical and mathematical algorithms carrying out tasks such as classification, regression analysis, concept learning, predictive modeling, clustering, and mining of useful patterns. Using ML, we aim to improve the whole learning process automatically so that we may not need complete human interactions, or so that we can at least reduce the level of such interactions as much as possible.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working principle of a learning algorithm</h1>
                </header>
            
            <article>
                
<p class="mce-root">Tom M. Mitchell explained what learning really means from a computer science perspective:</p>
<div class="mce-root packt_quote">"A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E."</div>
<p class="mce-root">Based on this definition, we can conclude that a computer program or machine can do the following:</p>
<ul>
<li class="mce-root">Learn from data and histories</li>
<li class="mce-root">Improve with experience</li>
<li class="mce-root">Iteratively enhance a model that can be used to predict outcomes of questions</li>
</ul>
<p class="mce-root">Since the preceding points are at the core of predictive analytics, almost every ML algorithm we use can be treated as an optimization problem. This is about finding parameters that minimize an objective function; for example, a weighted sum of two terms such as a cost function and regularization. Typically, an objective function has two components:</p>
<ul>
<li class="mce-root">A regularizer that controls the complexity of the model</li>
<li class="mce-root">The loss that measures the error of the model on the training data</li>
</ul>
<p class="mce-root">On the other hand, the regularization parameter defines the trade-off between minimizing the training error and the model's complexity in an effort to avoid overfitting problems. Now, if both of these components are convex, then their sum is also convex. So, when using an ML algorithm, the goal is to obtain the best hyperparameters of a function that return the minimum error when making predictions. Therefore, by using a convex optimization technique, we can minimize the function until it converges toward the minimum error.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">Given that a problem is convex, it is usually easier to analyze the asymptotic behavior of the algorithm, which shows how fast it converges as the model observes more and more training data. The task of ML is to train a model so that it can recognize complex patterns from the given input data and can make decisions in an automated way. Thus, making predictions is all about testing the model against new (that is, unobserved) data and evaluating the performance of the model itself. However, in the process as a whole, and for making the predictive model a successful one, data acts as the first-class citizen in all ML tasks. In reality, the data that we feed to our ML systems must be made up of mathematical objects, such as vectors, so that they can consume such data.</p>
<p class="mce-root">Depending on the available data and feature types, the performance of your predictive model can vacillate dramatically. Therefore, selecting the right features is one of the most important steps before the model evaluation takes place. This is called <strong>feature engineering</strong>, where the domain knowledge pertaining to the data is used to create only selective or useful features that help prepare the feature vectors to be used so that an ML algorithm works.</p>
<p class="mce-root">For example, comparing hotels is quite difficult unless we already have a personal experience of staying in multiple hotels. However, with the help of an ML model, which is already trained with quality features out of thousands of reviews and features (for example, how many stars does a hotel have, the size of the room, the location, and room service, and so on), it is pretty feasible now. We'll see several examples throughout the chapters. However, before developing such an ML model, knowing a number of ML concepts is also important.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General ML rule of thumb</h1>
                </header>
            
            <article>
                
<p class="mce-root">The general ML rule of thumb is that the more data there is, the better the predictive model. However, having more features often creates a mess, to the extent that the performance degrades drastically, especially if the dataset is multidimensional. The entire learning process requires input datasets that can be split into three types (or are already provided as such):</p>
<ul>
<li class="mce-root">A <strong>training set</strong> is the knowledge base coming from historical or live data that is used to fit the parameters of the ML algorithm. During the training phase, the ML model utilizes the training set to find optimal weights of the network and reach the objective function by minimizing the training error. Here, the backpropagation rule, or an optimization algorithm, is used to train the model, but all the hyperparameters are needed to be set before the learning process starts.</li>
<li class="mce-root">A <strong>validation set</strong> is a set of examples used to tune the parameters of an ML model. It ensures that the model is trained well and generalizes toward avoiding overfitting. Some ML practitioners refer to it as a development set, or dev set as well.</li>
<li class="mce-root">A <strong>test set</strong> is used for evaluating the performance of the trained model on unseen data. This step is also referred to as <strong>model inferencing</strong>. After assessing the final model on the test set (that is, when we're fully satisfied with the model's performance), we do not have to tune the model any further, but the trained model can be deployed in a production-ready environment.</li>
</ul>
<p class="mce-root">A common practice is splitting the input data (after the necessary preprocessing and feature engineering) into 60% for training, 10% for validation, and 20% for testing, but it really depends on use cases. Sometimes, we also need to perform upsampling or downsampling on the data, based on the availability and quality of the datasets. This rule of thumb of learning on different types of training sets can differ across ML tasks, as we will cover in the next section. However, before that, let's take a quick look at a few common phenomena in ML.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">General issues in ML models</h1>
                </header>
            
            <article>
                
<p>When we use this input data for training, validation, and testing, usually, the learning algorithms cannot learn 100% accurately, which involves training, validation, and test error (or loss). There are two types of errors that you may encounter in an ML model:</p>
<ul>
<li>Irreducible error</li>
<li>Reducible error</li>
</ul>
<p>The irreducible error cannot be reduced even with the most robust and sophisticated model. However, the reducible error, which has two components, called bias and variance, can be reduced. Therefore, to understand the model (that is, prediction errors), we need to focus on bias and variance only. Bias means how far the predicted values are from the actual values. Usually, if the average predicted values are very different from the actual values (labels), then the bias is higher.</p>
<p>An ML model will have a high bias because it can't model the relationship between input and output variables (can't capture the complexity of data well) and becomes very simple. Thus, an overly simple model with high variance causes underfitting of the data. The following diagram gives some high-level insights, and also shows what a just-right fit model should look like:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4e475e9e-0922-45e1-8a36-afdca4dc93cd.png" style="width:67.42em;height:12.83em;"/></p>
<p class="CDPAlignLeft CDPAlign">Variance signifies the variability between the predicted values and the actual values (how scattered they are). If the model has a high training error as well as the validation error or test error being the same as the training error, the model has a high bias. On the other hand, if the model has a low training error but has a high validation or high test error, the model has a high variance. An ML model usually performs very well on the training set, but doesn't work well on the test set (because of high error rates). Ultimately, it results in an underfit model. We can recap the overfitting and underfitting once more:</p>
<ul>
<li class="CDPAlignLeft CDPAlign"><strong>Underfitting</strong>: If your training and validation errors are both relatively equal and very high, then your model is most likely underfitting your training data.</li>
<li class="CDPAlignLeft CDPAlign"><strong>Overfitting</strong>: If your training error is low and your validation error is high, then your model is most likely overfitting your training data. The just-right fit model learns very well and performs better on unseen data too.</li>
</ul>
<div class="CDPAlignLeft CDPAlign packt_tip">Bias-variance trade-off: the high bias and high variance issue is often called bias-variance trade-off, because a model cannot be too complex or too simple at the same time. Ideally, we strive for the best model that has both low bias and low variance.</div>
<p class="CDPAlignLeft CDPAlign">Now we know the basic working principle of an ML algorithm. However, based on problem type and the method used to solve a problem, ML tasks can be different; for example, supervised learning, unsupervised learning, and reinforcement learning. We'll discuss these learning tasks in more detail in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ML tasks</h1>
                </header>
            
            <article>
                
<p>Although every ML problem is more or less an optimization problem, the way in which they are solved can vary. In fact, learning tasks can be categorized into three types: supervised learning, unsupervised learning, and reinforcement learning.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Supervised learning</h1>
                </header>
            
            <article>
                
<p>Supervised learning is the simplest and most well-known automatic learning task. It is based on a number of predefined examples, in which the category to which each of the inputs should belong is already known, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7da0267f-7bf5-4811-bf54-5a19375ab274.png" style="width:51.25em;height:22.25em;"/></p>
<p>The preceding diagram shows a typical workflow of supervised learning. An actor (for example, a data scientist or data engineer) performs the <strong>extraction</strong>, <strong>transformation</strong>, <strong>and load</strong> (<strong>ETL</strong>) and the necessary feature engineering (including feature extraction, selection, and so on) to get the appropriate data with features and labels so that they can be fed into the model. Then, they split the data into training, development, and test sets. The training set is used to train an ML model, the validation set is used to validate the training against the overfitting problem and regularization, and then the actor would evaluate the model's performance on the test set (that is, unseen data).</p>
<p>However, if the performance is not satisfactory, the actor can perform additional tuning to get the best model based on hyperparameter optimization. Finally, they will deploy the best model in a production-ready environment. In the overall life cycle, there might be many actors involved (for example, a data engineer, data scientist, or an ML engineer), performing each step independently or collaboratively. The supervised learning context includes classification and regression tasks; classification is used to predict which class a data point is a part of (discrete value). It is also used for predicting the label of the class attribute. The following diagram summarizes these steps in a nutshell:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3ec8d6c2-8f14-485a-a124-40e74f65de64.png" style="width:53.75em;height:17.75em;"/></p>
<p>On the other hand, regression is used to predict continuous values and make a numeric prediction of the class attribute. In the context of supervised learning, the learning process required for the input dataset is split randomly into three sets; for example, 60% for the training set, 10% for the validation set, and the remaining 30% for the testing set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Unsupervised learning</h1>
                </header>
            
            <article>
                
<p>How would you summarize and group a dataset if the labels were not given? You'll probably try to answer this question by finding the underlying structure of a dataset and measuring the statistical properties, such as the frequency distribution, mean, and standard deviation. If the question is how would you effectively represent data in a compressed format, you'll probably reply saying that you'll use some software for doing the compression, although you might have no idea how that software would do it. The following diagram shows the typical workflow of an unsupervised learning task:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/cf93f6bf-f0b4-486d-aae2-4e13e70a2012.png" style="width:57.58em;height:18.25em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>These are precisely two of the main goals of unsupervised learning, which is largely a data-driven process. We call this type of learning unsupervised because you will have to deal with unlabeled data. The following quote comes from Yann LeCun, director of AI research (source: <em>Predictive Learning</em>, NIPS 2016, Yann LeCun, Facebook Research):</p>
<div class="packt_quote"><br/>
"Most human and animal learning is unsupervised learning. If intelligence was a cake, unsupervised learning would be the cake, supervised learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake. We know how to make the icing and the cherry, but we don't know how to make the cake. We need to solve the unsupervised learning problem before we can even think of getting to true AI."</div>
<p>A few most widely used unsupervised learning tasks include the following:</p>
<ul>
<li><strong>Clustering</strong>: Grouping data points based on similarity (or statistical properties), for example, a company such as Airbnb often groups its apartments and houses into neighborhoods so that customers can navigate the listed ones more easily</li>
<li><strong>Dimensionality reduction</strong>: Compressing the data with the structure and statistical properties preserved as much as possible, for example, often, the number of dimensions of the dataset needs to be reduced for the modelling and visualization</li>
<li><strong>Anomaly detection</strong>: Useful in several applications, such as identification of credit card fraud detection, identifying faulty pieces of hardware in an industrial engineering process, and identifying outliers in large-scale datasets</li>
<li><strong>Association rule mining</strong>: Often used in market basket analysis, for example, asking which items are bought together frequently<strong><br/></strong></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p class="mce-root">Reinforcement learning is an artificial intelligence approach that focuses on the learning of the system through its interactions with the environment. In reinforcement learning, the system's parameters are adapted based on the feedback obtained from the environment, which, in turn, provides feedback on the decisions made by the system. The following diagram shows a person making decisions in order to arrive at their destination. Let's take an example of the route you take from home to work:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-862 image-border" src="assets/78d85aba-6e21-4d7d-9256-878b8542d641.png" style="width:30.17em;height:18.92em;"/></p>
<p>We can take a look at one more example in terms of a system modeling a chess player. In order to improve its performance, the system utilizes the result of its previous moves; such a system is said to be a system learning with reinforcement. In this case, you take the same route to work every day. However, out of the blue one day, you get curious and decide to try a different route with a view to finding the shortest path. Similarly, based on your experience and the time taken with the different route, you'll decide whether you should take that specific route more often. We can take a look at one more example in terms of a system modeling a chess player.</p>
<p>So far, we have learned the basic working principles of ML and different learning tasks. Let's have a look at each learning task with some example use cases in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning types with applications</h1>
                </header>
            
            <article>
                
<p>We have seen the basic working principles of ML algorithms, and we have seen what the basic ML tasks are, and how they formulate domain-specific problems. However, each of these learning tasks can be solved using different algorithms. The following diagram provides a glimpse into this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4352c6ad-bcbf-4f21-9866-b0a6537c871e.png" style="width:51.33em;height:25.67em;"/></p>
<p>However, the preceding diagram lists only a few use cases and applications using different ML tasks. In practice, ML is used in numerous use cases and applications. We will try to cover a few of them throughout this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Delving into DL</h1>
                </header>
            
            <article>
                
<p>Simple ML methods that were used in the normal-size data analysis are no longer effective and should be replaced by more robust ML methods. Although classical ML techniques allow researchers to identify groups or clusters of related variables, the accuracy and effectiveness of these methods diminish with large and multidimensional data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How did DL take ML to the next level?</h1>
                </header>
            
            <article>
                
<p>Simple ML methods used in small-scale data analysis are not effective when dealing with large and high-dimensional datasets. However, deep learning (DL), which is a branch of ML based on a set of algorithms that attempt to model high-level abstractions in data, can handle this issue. Ian Goodfellow defined DL in his book "<em>Deep Learning</em>, MIT Press, 2016" as follows:</p>
<div class="packt_quote">"Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as a nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones."</div>
<p>Similar to the ML model, a DL model also takes in an input, <kbd>X</kbd>, and learns high-level abstractions or patterns from it to predict an output of <kbd>Y</kbd>. For example, based on the stock prices of the past week, a DL model can predict the stock price for the next day. When performing training on such historical stock data, a DL model tries to minimize the difference between the prediction and the actual values. This way, a DL model tries to generalize to inputs that it hasn't seen before and makes predictions on test data.<br/>
Now, you might be wondering, if an ML model can do the same tasks, why do we need DL for this? Well, DL models tend to perform well with large amounts of data, whereas old ML models stop improving after a certain point. The core concept of DL, inspired by the structure and function of the brain, is called <strong>artificial neural networks</strong> (<strong>ANNs</strong>).</p>
<p>Being at the core of DL, ANNs help you to learn the associations between sets of inputs and outputs in order to make more robust and accurate predictions. However, DL is not only limited to ANNs; there have been many theoretical advances, software stacks, and hardware improvements that bring DL to the masses. Let's look at an example in which we want to develop a predictive analytics model, such as an animal recognizer, where our system has to resolve two problems:</p>
<ul>
<li>To classify whether an image represents a cat or a dog</li>
<li>To cluster images of dogs and cats.</li>
</ul>
<p>If we solve the first problem using a typical ML method, we must define the facial features (ears, eyes, whiskers, and so on) and write a method to identify which features (typically non-linear) are more important when classifying a particular animal. However, at the same time, we cannot address the second problem because classical ML algorithms for clustering images (such as k-means) cannot handle nonlinear features. Take a look at the following diagram, which shows a workflow that we would follow to classify if the given image is of a cat:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6774fd8b-9b85-47e3-af7e-f9b28f882e9f.png" style="width:70.83em;height:21.08em;"/></p>
<p>DL algorithms take these two problems one step further, and the most important features will be extracted automatically after determining which features are the most important for classification or clustering. In contrast, when using a classical ML algorithm, we would have to provide the features manually. A DL algorithm takes more sophisticated steps instead. For example, first, it identifies the edges that are the most relevant when clustering cats or dogs. It then tries to find various combinations of shapes and edges hierarchically, which is called ETL.</p>
<p>Then, after several iterations, it carries out the hierarchical identification of complex concepts and features. Following that, based on the features identified, the DL algorithm will decide which of the features are most significant for classifying the animal. This step is known as feature extraction. Finally, it takes out the label column and performs unsupervised training using <strong>autoencoders</strong> (<strong>AEs</strong>) to extract the latent features to be redistributed to k-means for clustering. Then, the <strong>clustering assignment hardening loss</strong> (<strong>CAH loss</strong>) and reconstruction loss are jointly optimized toward an optimal clustering assignment.</p>
<p>However, in practice, a DL algorithm is fed with a raw image representation, which doesn't see an image as we see it because it only knows the position of each pixel and its color. The image is divided into various layers of analysis. For example, at a lower level, there is the software analysis—a grid of a few pixels with the task of detecting a type of color or various nuances. If it finds something, it informs the next level, which, at this point, checks whether or not that given color belongs to a larger form, such as a line. The process continues to the upper levels until the algorithm understands what is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6be27318-8090-42f0-8d8c-005086271f80.png" style="width:35.67em;height:13.67em;"/></p>
<p class="CDPAlignLeft CDPAlign">Although a dog versus a cat is an example of a very simple classifier, software that's capable of doing these types of things is now widespread and is found in systems for recognizing faces, or in those for searching an image on Google, for example. This kind of software is based on DL algorithms. By contrast, if we are using a linear ML algorithm we cannot build such applications, since these algorithms are incapable of handling non-linear image features.</p>
<p>Also, using ML approaches, we typically only handle a few hyperparameters. However, when neural networks are brought into the mix, things become too complex. In each layer, there are millions or even billions of hyperparameters to tune—so many that the cost function becomes non-convex. Another reason for this is that the activation functions that are used in hidden layers are non-linear, so the cost is non-convex. We will discuss this phenomenon in more detail in later chapters, but let's take a quick look at ANNs.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Artificial neural networks</h1>
                </header>
            
            <article>
                
<p>ANNs, which are inspired by how a human brain works, form the core of DL and its true realization. Today's revolution around DL would not have been possible without ANNs. Thus, to understand DL, we need to understand how neural networks work.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ANN and the human brain</h1>
                </header>
            
            <article>
                
<p>ANNs represent one aspect of the human nervous system, and how the nervous system consists of a number of neurons that communicate with each other using axons. The receptors receive the stimuli either internally or from the external world. Then, they pass this information to the biological neurons for further processing. There are a number of dendrites, in addition to another long extension called the axon. Toward the axon's extremities, there are minuscule structures called synaptic terminals, which are used to connect one neuron to the dendrites of other neurons. Biological neurons receive short electrical impulses called signals from other neurons, and, in response, they trigger their own signals.</p>
<p>We can, therefore, summarize that the neuron comprises a cell body (also known as the <strong>soma</strong>), one or more dendrites for receiving signals from other neurons, and an axon for carrying out the signals that are generated by the neurons. A neuron is in an active state when it is sending signals to other neurons. However, when it is receiving signals from other neurons, it is in an inactive state. In an idle state, a neuron accumulates all the signals that are received before reaching a certain activation threshold. This whole process motivated researchers to test out ANNs. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A brief history of ANNs</h1>
                </header>
            
            <article>
                
<p>Inspired by the working principles of biological neurons, Warren McCulloch and Walter Pitts proposed the first artificial neuron model, in 1943, in terms of a computational model of nervous activity. This simple model of a biological neuron, also known as an <strong>artificial neuron</strong> (<strong>AN</strong>), has one or more binary (on/off) inputs and one output only. An AN simply activates its output when more than a certain number of its inputs are active.</p>
<p>The example sounds too trivial, but even with such a simplified model, it is possible to build a network of ANs. Nevertheless, these networks can be combined to compute complex logical expressions too. This simplified model inspired John von Neumann, Marvin Minsky, Frank Rosenblatt, and many others to come up with another model called a <strong>perceptron,</strong> back in 1957. The perceptron is one of the simplest ANN architectures we have seen in the last 60 years. It is based on a slightly different AN called a <strong>Linear Threshold Unit</strong> (<strong>LTU</strong>). The only difference is that the inputs and outputs are now numbers instead of binary on/off values. Each input connection is associated with a weight. The LTU computes a weighted sum of its inputs, then applies a step function (which resembles the action of an activation function) to that sum, and outputs the result.</p>
<p>One of the downsides of a perceptron is that its decision boundary is linear. Therefore, they are incapable of learning complex patterns. They are also incapable of solving some simple problems, such as <strong>Exclusive OR</strong> (<strong>XOR</strong>). However, later on, the limitations of perceptrons were somewhat eliminated by stacking multiple perceptrons, called <strong>MLP</strong>. So, the most significant progress in ANNs and DL can be described in the following timeline. We have already discussed how the artificial neurons and perceptrons provided the base in 1943 and 1958, respectively. I<span>n 1969, <span class="st">Marvin <em>Minsky</em> and Seymour <em>Papert</em></span> formulated the</span> XOR as a linearly non-separable problem, and later, in 1974, <span class="st">Paul <em>Werbos</em></span> demonstrated the backpropagation algorithm for training the perceptron.</p>
<p class="mce-root">However, the most significant advancement happened in 1982, when John Hopfield proposed the Hopfield Network. Then, one of the godfathers of the neural network and DL—Hinton and his team—proposed the Boltzmann Machine in 1985. However, in 1986 Geoffrey Hinton successfully trained the MLP and Jordan M.I. proposed RNNs. In the same year, Paul Smolensky also proposed the improved version of the Boltzmann Machine, called the <strong>Restricted Boltzmann Machine</strong> (<strong>RBM</strong>). Then, in 1990, Lecun et al. proposed LeNet, which is a deep neural network architecture. For a brief glimpse, refer to the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-863 image-border" src="assets/cb122a6b-b05d-4691-8b20-edad562ebfa2.png" style="width:162.50em;height:45.00em;"/></p>
<p class="mce-root"/>
<p>The most significant year of the 90's era was 1997, when Jordan et al. proposed a <strong>recurrent neural network</strong> (<strong>RNN</strong>). In the same year, Schuster et al. proposed the improved version of <strong>long-short term memory</strong> (<strong>LSTM</strong>) and the improved version of the original RNN called bidirectional RNN.</p>
<p>Despite significant advances in computing, from 1997 to 2005, we did not experience much advancement. Then, in 2006, Hinton struck again when, he and his team proposed a <strong>deep belief network</strong> (<strong>DBN</strong>) by stacking multiple RBMs. Then in 2012, Hinton invented the dropout that significantly improved the regularization and overfitting in the deep neural network. After that, Ian Goodfellow et al. introduced the GANs—a significant milestone in image recognition. In 2017, Hinton proposed CapsNet to overcome the limitation of regular CNNs, and this is so far one of the most remarkable <span>milestones. We will discuss these architectures later in this chapter.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How does an ANN learn?</h1>
                </header>
            
            <article>
                
<p class="mce-root">Based on the concept of biological neurons, the term and idea of ANNs arose. Similar to biological neurons, the artificial neuron consists of the following:</p>
<ul>
<li class="mce-root">One or more incoming connections that aggregate signals from neurons</li>
<li class="mce-root">One or more output connections for carrying the signal to the other neurons</li>
<li class="mce-root">An activation function, which determines the numerical value of the output signal</li>
</ul>
<p class="mce-root">Besides the state of a neuron, synaptic weight is considered, which influences the connection within the network. Each weight has a numerical value indicated by <em>W<sub>ij</sub></em>, which is the synaptic weight connecting neuron <em>i</em> to neuron <em>j</em>. Now, for each neuron <em>i</em>, an input<br/>
vector can be defined by <em>x<sub>i</sub> = (x<sub>1</sub>,x<sub>2</sub>,...x<sub>n</sub>)</em>, and a weight vector can be defined by <em>w<sub>i</sub><span> </span>= (w<sub>i1</sub>,x<sub>i2</sub>,...x<sub>in</sub>)</em>. Now, depending on the position of a neuron, the weights and the output function determine the behavior of an individual neuron. Then, during forward propagation, each unit in the hidden layer gets the following signal:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/239e1275-0719-40b1-9689-eb3ea9bcf6a1.png" style="width:15.75em;height:2.92em;"/></p>
<p>Nevertheless, among the weights, there is also a special type of weight called a bias unit, <em>b</em>. Technically, bias units aren't connected to any previous layer, so they don't have true activity. But still, the bias <em>b</em> value allows the neural network to shift the activation function to the left or right. By taking the bias unit into consideration, the modified network output is formulated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/dc603183-bf05-4ad0-b110-7847a8084d82.png" style="width:16.92em;height:3.00em;"/></p>
<p>The preceding equation signifies that each hidden unit gets the sum of inputs, multiplied by the corresponding weight—this is known as the <strong>Summing junction</strong>. Then, the resultant output in the <strong>Summing junction</strong> is passed through the activation function, which squashes the output, as depicted in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2401dc9f-11f6-42e3-90e0-f31f4f14bfb7.png" style="width:29.83em;height:15.00em;"/></p>
<p>A practical neural network architecture, however, is composed of input, hidden, and output layers that are composed of nodes that make up a network structure. It still follows the working principle of an artificial neuron model, as shown in the preceding diagram. The input layer only accepts numeric data, such as features in real numbers, and images with pixel values. The following diagram shows a neural network architecture for solving a multiclass classification (that is, 10 classes) problem based on a data having 784 features:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e3fd76b9-b477-4275-8b19-8217f4a4483a.png" style="width:40.17em;height:23.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A neural network with one input layer, three hidden layers, and an output layer</div>
<p>Here, the hidden layers perform most of the computation to learn the patterns, and the network evaluates how accurate its prediction is compared to the actual output using a special mathematical function called the loss function. It could be a complex one or a very simple mean squared error, which can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bea114d3-df9b-4ac8-98b1-41acf87a2dc2.png" style="width:13.17em;height:3.33em;"/></p>
<p>In the preceding equation, <img class="fm-editor-equation" src="assets/4058e15e-01a0-4db0-9f89-4de3196c76f3.png" style="width:0.83em;height:1.17em;"/> is the prediction made by the network, while <em>Y</em> represents the actual or expected output. Finally, when the error is no longer being reduced, the neural network converges and makes a prediction through the output layer.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a neural network</h1>
                </header>
            
            <article>
                
<p>The learning process for a neural network is configured as an iterative process of the optimization of the weights. The weights are updated in each epoch. Once the training starts, the aim is to generate predictions by minimizing the loss function. The performance of the network is then evaluated on the test set. We already know about the simple concept of an artificial neuron. However, generating only some artificial signals is not enough to learn a complex task. As such, a commonly used supervised learning algorithm is the backpropagation algorithm, which is very often used to train a complex ANN.</p>
<p>Ultimately, training such a neural network is an optimization problem, too, in which we try to minimize the error by adjusting network weights and biases iteratively, by using backpropagation through <strong>gradient descent</strong> (<strong>GD</strong>). This approach forces the network to backtrack through all its layers to update the weights and biases across nodes in the opposite direction of the loss function.</p>
<p>However, this process using GD does not guarantee that the global minimum is reached. The presence of hidden units and the non-linearity of the output function means that the behavior of the error is very complex and has many local minima. This backpropagation step is typically performed thousands or millions of times, using many training batches, until the model parameters converge to values that minimize the cost function. The training process ends when the error on the validation set begins to increase, because this could mark the beginning of a phase of overfitting:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-864 image-border" src="assets/964eadcf-5c58-41d9-92a5-2122d399085d.png" style="width:24.67em;height:17.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Searching for the minimum for the error function E, we move in the direction in which the gradient G of E is minimal</div>
<p>The downside of using GD is that it takes too long to converge, which makes it impossible to meet the demand of handling large-scale training data. Therefore, a faster GD, called <strong>stochastic gradient descent</strong> (<strong>SGD</strong>) was proposed, which is also a widely used optimizer in DNN training. In SGD, we use only one training sample per iteration from the training set to update the network parameters, which is a stochastic approximation of the true cost gradient.</p>
<div class="packt_infobox"><br/>
There are other advanced optimizers nowadays such as Adam, RMSProp, ADAGrad, and Momentum. Each of them is either a direct or indirect optimized version of SGD.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Weight and bias initialization</h1>
                </header>
            
            <article>
                
<p>Now, here's a tricky question: how do we initialize the weights? Well, if we initialize all the weights to the same value (for example, 0 or 1), each hidden neuron will get the same signal. Let's try to break it down:</p>
<ul>
<li>If all weights are initialized to 1, then each unit gets a signal equal to the sum of the inputs.</li>
<li>If all weights are 0, which is even worse, then every neuron in a hidden layer will get zero signal.</li>
</ul>
<p>For network weight initialization, Xavier initialization is used widely. It is similar to random initialization, but often turns out to work much better, since it can identify the rate of initialization depending on the total number of input and output neurons by default. You may be wondering whether you can get rid of random initialization while training a regular DNN.</p>
<p>Well, recently, some researchers have been talking about random orthogonal matrix initializations that perform better than just any random initialization for training DNNs. When it comes to initializing the biases, we can initialize them to zero. But setting the biases to a small constant value, such as 0.01 for all biases, ensures that all <strong>rectified linear units</strong> (<strong>ReLU</strong>) can propagate a gradient. However, it neither performs well nor shows consistent improvement. Therefore, sticking with zero is recommended.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Activation functions</h1>
                </header>
            
            <article>
                
<p>To allow a neural network to learn complex decision boundaries, we apply a non-linear activation function to some of its layers. Commonly used functions include Tanh, ReLU, softmax, and variants of these. More technically, each neuron receives a signal of the weighted sum of the synaptic weights and the activation values of the neurons that are connected as input. One of the most widely used functions for this purpose is the so-called sigmoid logistic function, which is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d18ee45f-3d01-4f2c-9ce3-8ffc88c9d079.png" style="width:8.42em;height:2.67em;"/></p>
<p class="mce-root">The domain of this function includes all real numbers, and the co-domain is (0, 1). This means that any value obtained as an output from a neuron (as per the calculation of its activation state) will always be between zero and one. The Sigmoid function, as<br/>
represented in the following diagram, provides an interpretation of the saturation rate of a neuron, from not being active (equal to 0) to complete saturation, which occurs at a predetermined maximum value (equal to 1):</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/09c9bd66-be56-4e39-8322-365a9339cfcc.png" style="width:25.00em;height:17.17em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Sigmoid versus Tanh activation function</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">On the other hand, a hyperbolic tangent, or <strong>Tanh</strong>, is another form of activation function. <strong>Tanh</strong> flattens a real-valued number between <strong>-1</strong> and <strong>1</strong>. The preceding graph shows the difference between the <strong>Tanh</strong> and <strong>Sigmoid</strong> activation functions. In particular, mathematically,  speaking the <em>tanh</em> activation function can be expressed as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a31a5802-f491-4761-a390-73b2cbc778c3.png" style="width:9.75em;height:1.25em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign">In general, in the last level of a <strong>feedforward neural network</strong> (<strong>FFNN</strong>), the softmax function is applied as the decision boundary. This is a common case, especially when solving a classification problem. The softmax function is used for the probability distribution over the possible classes in a multiclass classification problem. To conclude, choosing proper activation functions and network weight initializations are two problems that make a network perform at its best and help to obtain good training. Now that we know the brief history of neural networks, let's deepdive into different architectures in the next section, which will give us an idea of their usage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural network architectures</h1>
                </header>
            
            <article>
                
<p>Up to now, numerous neural network architectures have been proposed and are in use. However, more or less all of them are based on a few core neural network architectures. We can categorize DL architectures into four groups:</p>
<ul>
<li>Deep neural networks</li>
<li>Convolutional neural networks</li>
<li>Recurrent neural networks</li>
<li>Emergent architectures</li>
</ul>
<p>However, DNNs, CNNs, and RNNs have many improved variants. Although most of the variants are proposed or developed for solving domain-specific research problems, the basic working principles still follow the original DNN, CNN, and RNN architectures. The following subsections will give you a brief introduction to these architectures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deep neural networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">DNNs are neural networks that have a complex and deeper architecture with a large number of neurons in each layer, and many connections between them. Although DNN refers to a very deep network, for simplicity, we consider MLP, <strong>stacked autoencoder </strong>(<strong>SAE</strong>), and <strong>deep belief networks</strong> (<strong>DBNs</strong>) as DNN architectures. These architectures mostly work as an FFNN, meaning information propagates from input to output layers.</p>
<p class="mce-root">Multiple perceptrons are stacked together as MLPs, where layers are connected as a directed graph. Fundamentally, an MLP is one of the most simple FFNNs since it has three layers: an input layer, a hidden layer, and an output layer. This way, the signal propagates one way, from the input layer to the hidden layers to the output layer, as shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2b984dc6-a80d-4037-adc9-dc61d4be1b5a.png" style="width:35.75em;height:20.92em;"/></p>
<p>Autoencoders and RBMs are the basic building blocks for SAEs and DBNs, respectively. Unlike MLP, which is an FFNN that's trained in a supervised way, both SAEs and DBNs are trained in two phases: unsupervised pretraining and supervised fine-tuning. In unsupervised pretraining, layers are stacked in order and trained in a layer-wise manner with used unlabeled data.</p>
<p>In supervised fine-tuning, an output classifier layer is stacked and the complete neural network is optimized by retraining with labeled data. One problem with MLP is that it often overfits the data, so it doesn't generalize well. To overcome this issue, DBN was proposed by Hinton et al. It uses a greedy, layer-by-layer, pretraining algorithm. DBNs are composed of a visible layer and multiple hidden unit layers. The building blocks of a DBN are RBMs, as shown in the following diagram, where several RBMs are stacked one after another:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/6a7b0c86-7732-461b-93b4-3512d208d1b8.png" style="width:32.92em;height:20.67em;"/></p>
<p>The top two layers have undirected, symmetric connections in-between, but the lower layers have directed connections from the preceding layer. Despite numerous successes, DBNs are now being replaced with AEs.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoencoders</h1>
                </header>
            
            <article>
                
<p>AEs are also special types of neural networks that learn automatically from the input data. AEs consist of two components: the encoder and the decoder. The encoder compresses the input into a latent-space representation. Then, the decoder part tries to reconstruct the original input data from this representation:</p>
<ul>
<li><strong>Encoder</strong>: Encodes or compresses the input into a latent-space representation using a function known as <em>h = f(x)</em></li>
<li><strong>Decoder</strong>: Decodes or reconstructs the input from the latent space representation using a function known as <em>r = g(h)</em></li>
</ul>
<p>So, an AE can be described by a function of<em><span> </span>g(f(x)) = 0</em>, where we want 0 as close to the original input of <em>x</em>. The following diagram shows how an AE typically works:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bcbf8bbf-53d7-4d16-9fdc-0bf5612e3e25.png" style="width:29.83em;height:9.00em;"/></p>
<p class="CDPAlignLeft CDPAlign">AEs are very useful for data denoising and dimensionality reduction for data visualization because they can learn data projections called representations more effectively than PCA.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional neural networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">CNNs have achieved much and have been widely adopted in computer vision (for example, image recognition). In CNN networks, the connection schemes are significantly different compared to an MLP or DBN. A few of the convolutional layers are connected in a cascade style. Each layer is backed up by an ReLU layer, a pooling layer, additional convolutional layers (+ReLU), and another pooling layer, which is followed by a fully connected layer and a softmax layer. The following diagram is a schematic of the architecture of a CNN that's used for facial recognition, which takes facial images as input and predicts emotions such as anger, disgust, fear, happy, and sad:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2a4a34db-3397-44cb-8825-78eb3053eae0.png" style="width:53.92em;height:17.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">A schematic architecture of a CNN used for facial recognition</div>
<div>
<p>Importantly, DNNs have no prior knowledge of how the pixels are organized because they do not know that nearby pixels are close. CNNs embed this prior knowledge using lower layers by using feature maps in small areas of the image, while the higher layers combine lower-level features into larger features.</p>
<p>This setting works well with most of the natural images, giving CNN a decisive head start over DNNs. The output from each convolutional layer is a set of objects, called feature maps, that are generated by a single kernel filter. Then, the feature maps can be used to define a new input to the next layer. Each neuron in a CNN network produces an output, followed by an activation threshold, which is proportional to the input and not bound.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Recurrent neural networks</h1>
                </header>
            
            <article>
                
<p>In RNNs, connections between units form a directed cycle. The RNN architecture was originally conceived by Hochreiter and Schmidhuber in 1997. RNN architectures have standard MLPs, plus added loops so that they can exploit the powerful nonlinear mapping capabilities of the MLP. They also have some form of memory. The following diagram shows a very basic RNN that has an input layer, two recurrent layers, and an output layer:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/77b89365-780e-4461-a58b-6e3c4e492336.png" style="width:35.83em;height:15.75em;"/></p>
<p>However, this basic RNN suffers from gradient vanishing and the exploding problem, and cannot model long-term dependencies. These architectures include LSTM, <strong>gated recurrent units</strong> (<strong>GRUs</strong>), bidirectional-LSTM, and other variants. Consequently, LSTM and GRU can overcome the drawbacks of regular RNNs: the gradient vanishing/exploding problem and long-short term dependency.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Emergent architectures</h1>
                </header>
            
            <article>
                
<p>Many other emergent DL architectures have been suggested, such as <strong>Deep SpatioTemporal Neural Networks</strong> (<strong>DST-NNs</strong>), <strong>Multi-Dimensional Recurrent Neural Networks</strong> (<strong>MD-RNNs</strong>), and <strong>Convolutional AutoEncoders</strong> (<strong>CAEs</strong>). Nevertheless, there are a few more emerging networks, such as <strong>CapsNets</strong> (which is an improved version of a CNN, designed to remove the drawbacks of regular CNNs), RNN for image recognition, and <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>) for simple image generation. Apart from these, factorization machines for personalization and deep reinforcement learning are also being used widely.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Residual neural networks</h1>
                </header>
            
            <article>
                
<p>Since there are sometimes millions and millions of hyperparameters and other practical aspects, it's really difficult to train deeper neural networks. To overcome this limitation, Kaiming H. et al. ( <a href="https://arxiv.org/abs/1512.03385v1">https://arxiv.org/abs/1512.03385v1</a>) proposed a residual learning framework to ease the training of networks that are substantially deeper than those used previously.</p>
<p>They also explicitly reformulated the layers as learning residual functions with reference to the layer inputs, instead of learning non-referenced functions. This way, these residual networks are easier to optimize and can gain accuracy from considerably increased depth. The downside is that building a network by simply stacking residual blocks inevitably limits the optimization ability. To overcome this limitation, Ke Zhang et al. also proposed using a multilevel residual network (<a href="https://arxiv.org/abs/1608.02908">https://arxiv.org/abs/1608.02908</a>). </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generative adversarial networks</h1>
                </header>
            
            <article>
                
<p>GANs are deep neural net architectures that consist of two networks pitted against each other (hence the name <em>adversarial</em>). Ian Goodfellow et al. introduced GANs in a paper (see more at <a href="https://arxiv.org/abs/1406.2661v1">https://arxiv.org/abs/1406.2661v1</a>). In GANs, the two main components are the <strong>generator and discriminator</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4920409b-74a3-482e-a9f8-638542cdeab9.png" style="width:54.00em;height:25.50em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Working principle of generative adversarial networks</div>
<p>In a GAN architecture, a generator and a discriminator are pitted against each other—hence the name, adversarial:</p>
<ul>
<li>The generator tries to generate data samples out of a specific probability distribution and is very similar to the actual object.</li>
<li>The discriminator will judge whether its input is coming from the original training set or from the generator part.</li>
</ul>
<p>Many DL practitioners think that GANs were one of the most important advancements because GANs can be used to mimic any distribution of data, and, based on the data distribution, they can be taught to create robot artist images, super-resolution images, text-to-image synthesis, music, speech, and more. For example, because of the concept of adversarial training, Facebook's AI research director, Yann LeCun, suggested that GANs are the most interesting idea in the last 10 years of ML.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Capsule networks</h1>
                </header>
            
            <article>
                
<p class="mce-root">In CNNs, each layer understands an image at a much more granular level through a slow receptive field or max pooling operations. If the images have rotation, tilt, or very different shapes or orientation, CNNs fail to extract such spatial information and show very poor performance at image processing tasks. Even the pooling operations in CNNs cannot be much help against such positional invariance. This issue in CNNs has led us to the recent advancement of CapsNet through the paper entitled <em>Dynamic Routing Between Capsules</em> (see more at <a href="https://arxiv.org/abs/1710.09829">https://arxiv.org/abs/1710.09829</a>) by Geoffrey Hinton et al:</p>
<div class="mce-root packt_quote">"A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity, such as an object or an object part."</div>
<p class="mce-root">Unlike a regular DNN, where we keep on adding layers, in CapsNet, the idea is to add more layers inside a single layer. This way, a CapsNet is a nested set of neural layers. In CapsNet, the vector inputs and outputs of a capsule are computed using the routing algorithm used in physics, which iteratively transfers information and processes the <strong>self-consistent field</strong> (<strong>SCF</strong>) procedure:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/14df1eb2-37d6-4291-b0c9-453194b85145.png" style="width:64.08em;height:18.08em;"/></p>
<p>The preceding diagram shows a schematic diagram of a simple three-layer CapsNet. The length of the activity vector of each capsule in the DigiCaps layer indicates the presence of an instance of each class, which is used to calculate the loss. Now that we have learned about the working principles of neural networks and the different neural network architectures, implementing something hands-on would be great. However, before that, let's take a look at some popular DL libraries and frameworks, which come with the implementation of these network architectures.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Neural networks for clustering analysis</h1>
                </header>
            
            <article>
                
<p>Several variants of k-means have been proposed to address issues with higher-dimensional input spaces. However, they are fundamentally limited to linear embedding. Hence, we cannot model non-linear relationships. Nevertheless, fine-tuning in these approaches is based on only cluster assignment hardening loss (see later in this section). Therefore, a fine-grained clustering accuracy cannot be achieved. Since the quality of the clustering results is dependent on the data distribution, deep architecture can help the model learn mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Several approaches have been proposed over the last few years, trying to use the representational power of deep neural networks for preprocessing clustering inputs.</p>
<p>A few notable approaches include deep embedded clustering, deep clustering networks, discriminatively boosted clustering, clustering CNNs, deep embedding networks, convolutional deep embedded clustering, and joint unsupervised learning of deep representation for images. Other approaches include DL with non-parametric clustering, CNN-based joint clustering and representation learning with feature drift compensation, learning latent representations in neural networks for clustering, clustering using convolutional neural networks, and deep clustering with convolutional autoencoder embedding.</p>
<p>Most of these approaches follow more or less the same principle: that is, representation learning using a deep architecture to transform the inputs into a latent representation and using these representations as input for a specific clustering method. Such deep architectures include MLP, CNN, DBN, GAN, and variational autoencoders. The following diagram shows an example of how to improve the clustering performance of a DEC network using convolutional autoencoders and optimizing both reconstruction and CAH losses jointly. The latent space out of the encoder layer is fed to K-means for soft clustering assignment. Blurred genetic variants signify the existence of reconstruction errors:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-865 image-border" src="assets/2d1aa2d3-1e6d-407c-9349-98e368b48591.png" style="width:162.50em;height:61.00em;"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">DL based clustering (source: Karim et al., Recurrent Deep Embedding Networks for Genotype Clustering and Ethnicity Prediction, arXiv:1805.12218)</div>
<p>In summary, in these approaches, there are three important steps involved—extracting cluster-friendly deep features using deep architectures, combining clustering and non-clustering losses, and, finally, network updates to optimize clustering and non-clustering losses jointly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL frameworks and cloud platforms for IoT</h1>
                </header>
            
            <article>
                
<p>There are several popular DL frameworks. Each of them comes with some pros and cons. Some of them are desktop-based, and some of them are cloud-based platforms, where you can deploy/run your DL applications. However, most of the libraries that are released under an open license help when people are using graphics processors, which can ultimately help in speeding up the learning process. Such frameworks and libraries include TensorFlow, PyTorch, Keras, Deeplearning4j, H2O, and the <strong>Microsoft Cognitive Toolkit</strong> (<strong>CNTK</strong>). Even a few years back, other implementations, including Theano, Caffee, and Neon, were used widely. However, these are now obsolete.</p>
<p class="mce-root"/>
<p><strong>Deeplearning4j</strong> (<strong>DL4J</strong>) is one of the first commercial-grade, open source, distributed DL libraries that was built for Java and Scala. This also provides integrated support for Hadoop and Spark. DL4J is built for use in business environments on distributed GPUs and CPUs. DL4J aims to be cutting-edge and <em>Plug and Play</em>, with more convention than configuration, which allows for fast prototyping for non-researchers. Its numerous libraries can be integrated with DL4J and will make your JVM experience easier, regardless of whether you are developing your ML application in Java or Scala. Similar to NumPy for JVM, ND4J comes up with basic operations of linear algebra (matrix creation, addition, and multiplication). However, ND4S is a scientific computing library for<br/>
linear algebra and matrix manipulation. It also provides n-dimensional arrays for JVM-based languages. The following diagram shows last year's Google Trends, illustrating how popular TensorFlow is:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/be613e47-6b26-477a-a6ca-d92333a53d42.png" style="width:73.17em;height:16.83em;"/></p>
<p>As well as these frameworks, Chainer is a powerful, flexible, and intuitive DL framework, which supports CUDA computation. It only requires a few lines of code to leverage a GPU. It also runs on multiple GPUs with little effort. Most importantly, Chainer supports various network architectures, including feed-forward nets, convnets, recurrent nets, and recursive nets. It also supports per-batch architectures. One more interesting feature in Chainer is that it supports forward computation, by which any control flow statements of Python can be included without lacking the ability of backpropagation. It makes code intuitive and easy to debug.</p>
<p class="mce-root">The DL framework power scores 2018 also shows that TensorFlow, Keras, and PyTorch are far ahead of other frameworks (see <a href="https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a">https://towardsdatascience.com/deep-learning-framework-power-scores-2018-23607ddf297a</a>). Scores were calculated based on usage, popularity, and interest in DL frameworks through the following sources. Apart from the preceding libraries, there are some recent initiatives for DL in the cloud. The idea is to bring DL capability to big data with billions of data points and high-dimensional data. For example, <strong>Amazon Web Services</strong> (<strong>AWS</strong>), Microsoft Azure, Google Cloud Platform, and <strong>NVIDIA GPU Cloud</strong> (<strong>NGC</strong>) all offer machine and DL services that are native to their public clouds.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">In October 2017, AWS released <strong>Deep Learning AMIs</strong> (<strong>DLAMIs</strong>) for <strong>Amazon Elastic Compute Cloud</strong> (<strong>Amazon EC2</strong>) P3 instances. These AMIs come preinstalled with DL frameworks, such as TensorFlow, Gluon, and Apache MXNet, which are optimized for the NVIDIA Volta V100 GPUs within Amazon EC2 P3 instances. The DL service currently offers three types of AMIs: Conda AMI, Base AMI, and AMI with source code.The CNTK is Azure's open source DL service. Similar to the AWS offering, it focuses on tools that can help developers build and deploy DL applications. Azure also provides a model gallery that includes resources, such as code samples, to help enterprises get started with the service.</p>
<p class="mce-root">On the other hand, NGC empowers AI scientists and researchers with GPU-accelerated containers (see <a href="https://www.nvidia.com/en-us/data-center/gpu-cloud-computing/">https://www. nvidia. com/en-us/data-center/gpu-cloud-computing/</a>). The NGC features containerized DL frameworks, such as TensorFlow, PyTorch, MXNet, and more that are tuned, tested, and certified by NVIDIA to run on the latest NVIDIA GPUs on participating cloud-service providers. Nevertheless, there are also third-party services available through their respective marketplaces.</p>
<p><strong>When it comes to cloud-based IoT system-development markets, currently it forks into three obvious routes:</strong> off-the-shelf platforms (for example, AWS IoT Core, Azure IoT Suite, and Google Cloud IoT Core), which trade off vendor lock-in and higher-end volume pricing against cost-effective scalability and shorter lead times; reasonably well-established MQTT configurations over the Linux stack (example: Eclipse Mosquitto); and the more exotic emerging protocols and products (for example, Nabto's P2P protocol) that are developing enough uptake, interest, and community investment to stake a claim for strong market presence in the future.</p>
<p>As a DL framework, Chainer Neural Network is a great choice for all devices powered by Intel Atom, NVIDIA Jetson TX2, and Raspberry Pi. Therefore, using Chainer, we don't need to build and configure the ML framework for our devices from scratch. It provides prebuilt packages for three popular ML frameworks, including TensorFlow, Apache MXNet, and Chainer. Chainer works in a similar fashion, which depends on a library on the Greengrass and a set of model files generated using Amazon SageMaker and/or stored directly in an Amazon S3 bucket. From Amazon SageMaker or Amazon S3, the ML models can be deployed to AWS Greengrass to be used as a local resource for ML inference. Conceptually, AWS IoT Core functions as the managing plane for deploying ML inference to the edge.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced a number of fundamental DL themes. We started our journey with a basic, but comprehensive, introduction to ML. Then, we gradually moved on to DL and different neural architectures. We then had a brief overview of the most important DL frameworks that can be utilized to develop DL-based applications for IoT-enabled devices.</p>
<p>IoT applications, such as smart home, smart city, and smart healthcare, heavily rely on video or image data processing for decision making. In the next chapter, we will cover DL-based image processing for IoT applications, including image recognition, classification, and object detection. Additionally, we will cover hands-on video data processing in IoT applications.</p>


            </article>

            
        </section>
    </body></html>