["```\nn_sentences = 250000\n# Loading English sentences\noriginal_en_sentences = []\nwith open(os.path.join('data', 'train.en'), 'r', encoding='utf-8') as en_file:\n    for i,row in enumerate(en_file):\n        if i >= n_sentences: break\n        original_en_sentences.append(row.strip().split(\" \"))\n\n# Loading German sentences\noriginal_de_sentences = []\nwith open(os.path.join('data', 'train.de'), 'r', encoding='utf-8') as de_file:\n    for i, row in enumerate(de_file):\n        if i >= n_sentences: break\n        original_de_sentences.append(row.strip().split(\" \")) \n```", "```\nEnglish: a fire restant repair cement for fire places , ovens , open fireplaces etc . \nGerman: feuerfester Reparaturkitt für Feuerungsanlagen , Öfen , offene Feuerstellen etc.\nEnglish: Construction and repair of highways and ... \nGerman: Der Bau und die Reparatur der Autostraßen ...\nEnglish: An announcement must be commercial character . \nGerman: die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen . \n```", "```\nen_sentences = [[\"<s>\"]+sent+[\"</s>\"] for sent in original_en_sentences]\nde_sentences = [[\"<s>\"]+sent+[\"</s>\"] for sent in original_de_sentences] \n```", "```\nEnglish: <s> a fire restant repair cement for fire places , ovens , open fireplaces etc . </s> \nGerman: <s> feuerfester Reparaturkitt für Feuerungsanlagen , Öfen , offene Feuerstellen etc. </s>\nEnglish: <s> Construction and repair of highways and ... </s> \nGerman: <s> Der Bau und die Reparatur der Autostraßen ... </s>\nEnglish: <s> An announcement must be commercial character . </s> \nGerman: <s> die Mitteilungen sollen den geschäftlichen kommerziellen Charakter tragen . </s> \n```", "```\nfrom sklearn.model_selection import train_test_split\ntrain_en_sentences, valid_test_en_sentences, train_de_sentences, valid_test_de_sentences = train_test_split(\n    np.array(en_sentences), np.array(de_sentences), test_size=0.2\n)\nvalid_en_sentences, valid_de_sentences, test_en_sentences, test_de_sentences = train_test_split(\n    valid_test_en_sentences, valid_test_de_sentences, test_size=0.5) \n```", "```\npd.Series(train_en_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95]) \n```", "```\nSequence lengths (English)\ncount    40000.000000\nmean        25.162625\nstd         13.857748\nmin          6.000000\n5%           9.000000\n50%         22.000000\n95%         53.000000\nmax        100.000000\ndtype: float64 \n```", "```\npd.Series(train_de_sentences).str.len().describe(percentiles=[0.05, 0.5, 0.95]) \n```", "```\nSequence lengths (German)\ncount    40000.000000\nmean        22.882550\nstd         12.574325\nmin          6.000000\n5%           9.000000\n50%         20.000000\n95%         47.000000\nmax        100.000000\ndtype: float64 \n```", "```\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\ntrain_en_sentences_padded = pad_sequences(train_en_sentences, maxlen=n_en_seq_length, value=unk_token, dtype=object, truncating='post', padding='post')\nvalid_en_sentences_padded = pad_sequences(valid_en_sentences, maxlen=n_en_seq_length, value=unk_token, dtype=object, truncating='post', padding='post')\ntest_en_sentences_padded = pad_sequences(test_en_sentences, maxlen=n_en_seq_length, value=unk_token, dtype=object, truncating='post', padding='post')\ntrain_de_sentences_padded = pad_sequences(train_de_sentences, maxlen=n_de_seq_length, value=unk_token, dtype=object, truncating='post', padding='post')\nvalid_de_sentences_padded = pad_sequences(valid_de_sentences, maxlen=n_de_seq_length, value=unk_token, dtype=object, truncating='post', padding='post')\ntest_de_sentences_padded = pad_sequences(test_de_sentences, maxlen=n_de_seq_length, value=unk_token, dtype=object, truncating='post', padding='post') \n```", "```\nn_vocab = 25000 + 1 \n```", "```\nen_vocabulary = []\nwith open(os.path.join('data', 'vocab.50K.en'), 'r', encoding='utf-8') as en_file:\n    for ri, row in enumerate(en_file):\n        if ri  >= n_vocab: break\n\n        en_vocabulary.append(row.strip()) \n```", "```\nde_vocabulary = []\nwith open(os.path.join('data', 'vocab.50K.de'), 'r', encoding='utf-8') as de_file:\n    for ri, row in enumerate(de_file):\n        if ri >= n_vocab: break\n\n        de_vocabulary.append(row.strip()) \n```", "```\nen_unk_token = en_vocabulary.pop(0)\nde_unk_token = de_vocabulary.pop(0) \n```", "```\nen_lookup_layer = tf.keras.layers.StringLookup(\n    vocabulary=en_vocabulary, oov_token=en_unk_token, \n    mask_token=pad_token, pad_to_max_tokens=False\n) \n```", "```\nde_lookup_layer = tf.keras.layers.StringLookup(\n    vocabulary=de_vocabulary, oov_token=de_unk_token, \n    mask_token=pad_token, pad_to_max_tokens=False\n) \n```", "```\nencoder_input = tf.keras.layers.Input(shape=(n_en_seq_length,), dtype=tf.string) \n```", "```\nencoder_wid_out = en_lookup_layer(encoder_input) \n```", "```\nen_full_vocab_size = len(en_lookup_layer.get_vocabulary())\nencoder_emb_out = tf.keras.layers.Embedding(en_full_vocab_size, 128, mask_zero=True)(encoder_wid_out) \n```", "```\nencoder_gru_out, encoder_gru_last_state = tf.keras.layers.GRU(256, return_sequences=True, return_state=True)(encoder_emb_out) \n```", "```\nencoder = tf.keras.models.Model(inputs=encoder_input, outputs=encoder_gru_out) \n```", "```\ndecoder_input = tf.keras.layers.Input(shape=(n_de_seq_length-1,), dtype=tf.string) \n```", "```\ndecoder_wid_out = de_lookup_layer(decoder_input) \n```", "```\nde_full_vocab_size = len(de_lookup_layer.get_vocabulary())\ndecoder_emb_out = tf.keras.layers.Embedding(de_full_vocab_size, 128, mask_zero=True)(decoder_wid_out) \n```", "```\ndecoder_gru_out = tf.keras.layers.GRU(256, return_sequences=True)(decoder_emb_out, initial_state=encoder_gru_last_state) \n```", "```\nclass BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super().__init__()\n        # Weights to compute Bahdanau attention\n        self.Wa = tf.keras.layers.Dense(units, use_bias=False)\n        self.Ua = tf.keras.layers.Dense(units, use_bias=False)\n        self.attention = \n        tf.keras.layers.AdditiveAttention(use_scale=True)\n    def call(self, query, key, value, mask, \n    return_attention_scores=False):\n        # Compute 'Wa.ht'.\n        wa_query = self.Wa(query)\n        # Compute 'Ua.hs'.\n        ua_key = self.Ua(key)\n        # Compute masks\n        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n        value_mask = mask\n        # Compute the attention\n        context_vector, attention_weights = self.attention(\n            inputs = [wa_query, value, ua_key],\n            mask=[query_mask, value_mask, value_mask],\n            return_attention_scores = True,\n        )\n\n        if not return_attention_scores:\n            return context_vector\n        else:\n            return context_vector, attention_weights \n```", "```\ndef call(self, query, key, value, mask, return_attention_scores=False):\n        # Compute 'Wa.ht'\n        wa_query = self.Wa(query)\n        # Compute 'Ua.hs'\n        ua_key = self.Ua(key)\n        # Compute masks\n        query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n        value_mask = mask\n        # Compute the attention\n        context_vector, attention_weights = self.attention(\n            inputs = [wa_query, value, ua_key],\n            mask=[query_mask, value_mask, value_mask],\n            return_attention_scores = True,\n        )\n\n        if not return_attention_scores:\n            return context_vector\n        else:\n            return context_vector, attention_weights \n```", "```\nself.attention = tf.keras.layers.AdditiveAttention(use_scale=True) \n```", "```\ndecoder_attn_out, attn_weights = BahdanauAttention(256)(\n    query=decoder_gru_out, key=encoder_gru_out, value=encoder_gru_out,\n    mask=(encoder_wid_out != 0),\n    return_attention_scores=True\n) \n```", "```\ncontext_and_rnn_output = tf.keras.layers.Concatenate(axis=-1)([decoder_attn_out, decoder_gru_out]) \n```", "```\n# Final prediction layer (size of the vocabulary)\ndecoder_out = tf.keras.layers.Dense(full_de_vocab_size, activation='softmax')(context_and_rnn_output) \n```", "```\nseq2seq_model = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input], outputs=decoder_out)\nseq2seq_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics='accuracy') \n```", "```\nattention_visualizer = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]) \n```", "```\ndef prepare_data(de_lookup_layer, train_xy, valid_xy, test_xy):\n    \"\"\" Create a data dictionary from the dataframes containing data \n    \"\"\"\n\n    data_dict = {}\n    for label, data_xy in zip(['train', 'valid', 'test'], [train_xy, \n    valid_xy, test_xy]):\n\n        data_x, data_y = data_xy\n        en_inputs = data_x\n        de_inputs = data_y[:,:-1]\n        de_labels = de_lookup_layer(data_y[:,1:]).numpy()\n        data_dict[label] = {'encoder_inputs': en_inputs, \n        'decoder_inputs': de_inputs, 'decoder_labels': de_labels}\n\n    return data_dict \n```", "```\ndef shuffle_data(en_inputs, de_inputs, de_labels, shuffle_inds=None): \n    \"\"\" Shuffle the data randomly (but all of inputs and labels at \n    ones)\"\"\"\n\n    if shuffle_inds is None:\n        # If shuffle_inds are not passed create a shuffling \n        automatically\n        shuffle_inds = \n        np.random.permutation(np.arange(en_inputs.shape[0]))\n    else:\n        # Shuffle the provided shuffle_inds\n        shuffle_inds = np.random.permutation(shuffle_inds)\n\n    # Return shuffled data\n    return (en_inputs[shuffle_inds], de_inputs[shuffle_inds], \n    de_labels[shuffle_inds]), shuffle_inds \n```", "```\nDef train_model(model, en_lookup_layer, de_lookup_layer, train_xy, valid_xy, test_xy, epochs, batch_size, shuffle=True, predict_bleu_at_training=False):\n    \"\"\" Training the model and evaluating on validation/test sets \"\"\"\n\n    # Define the metric\n    bleu_metric = BLEUMetric(de_vocabulary)\n    # Define the data\n    data_dict = prepare_data(de_lookup_layer, train_xy, valid_xy, \n    test_xy)\n    shuffle_inds = None\n\n    for epoch in range(epochs):\n        # Reset metric logs every epoch\n        if predict_bleu_at_training:\n            blue_log = []\n        accuracy_log = []\n        loss_log = []\n        # ========================================================== #\n        #                     Train Phase                            #\n        # ========================================================== #\n        # Shuffle data at the beginning of every epoch\n        if shuffle:\n            (en_inputs_raw,de_inputs_raw,de_labels), shuffle_inds  = \n            shuffle_data(\n                data_dict['train']['encoder_inputs'],\n                data_dict['train']['decoder_inputs'],\n                data_dict['train']['decoder_labels'],\n                shuffle_inds\n            )\n        else:\n            (en_inputs_raw,de_inputs_raw,de_labels)  = (\n                data_dict['train']['encoder_inputs'],\n                data_dict['train']['decoder_inputs'],\n                data_dict['train']['decoder_labels'],\n            )\n        # Get the number of training batches\n        n_train_batches = en_inputs_raw.shape[0]//batch_size\n\n        prev_loss = None\n        # Train one batch at a time\n        for i in range(n_train_batches):\n            # Status update\n            print(\"Training batch {}/{}\".format(i+1, n_train_batches), \n            end='\\r')\n            # Get a batch of inputs (english and german sequences)\n            x = [en_inputs_raw[i*batch_size:(i+1)*batch_size], \n            de_inputs_raw[i*batch_size:(i+1)*batch_size]]\n            # Get a batch of targets (german sequences offset by 1)\n            y = de_labels[i*batch_size:(i+1)*batch_size]\n\n            loss, accuracy = model.evaluate(x, y, verbose=0)\n\n            # Check if any samples are causing NaNs\n            check_for_nans(loss, model, en_lookup_layer, \n            de_lookup_layer)\n\n            # Train for a single step\n            model.train_on_batch(x, y)    \n\n            # Update the epoch's log records of the metrics\n            loss_log.append(loss)\n            accuracy_log.append(accuracy)\n\n            if predict_bleu_at_training:\n                # Get the final prediction to compute BLEU\n                pred_y = model.predict(x)\n                bleu_log.append(bleu_metric.calculate_bleu_from_\n                predictions(y, pred_y))\n\n        print(\"\")\n        print(\"\\nEpoch {}/{}\".format(epoch+1, epochs))\n        if predict_bleu_at_training:\n            print(f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: \n            {np.mean(accuracy_log)} - bleu: {np.mean(bleu_log)}\")\n        else:\n            print(f\"\\t(train) loss: {np.mean(loss_log)} - accuracy: \n            {np.mean(accuracy_log)}\")\n        # ========================================================== #\n        #                     Validation Phase                       #\n        # ========================================================== #\n\n        val_en_inputs = data_dict['valid']['encoder_inputs']\n        val_de_inputs = data_dict['valid']['decoder_inputs']\n        val_de_labels = data_dict['valid']['decoder_labels']\n\n        val_loss, val_accuracy, val_bleu = evaluate_model(\n            model, de_lookup_layer, val_en_inputs, val_de_inputs, \n            val_de_labels, batch_size\n        )\n\n        # Print the evaluation metrics of each epoch\n        print(\"\\t(valid) loss: {} - accuracy: {} - bleu: \n        {}\".format(val_loss, val_accuracy, val_bleu))\n\n    # ============================================================== #\n    #                      Test Phase                                #\n    # ============================================================== #\n\n    test_en_inputs = data_dict['test']['encoder_inputs']\n    test_de_inputs = data_dict['test']['decoder_inputs']\n    test_de_labels = data_dict['test']['decoder_labels']\n\n    test_loss, test_accuracy, test_bleu = evaluate_model(\n            model, de_lookup_layer, test_en_inputs, test_de_inputs, \n            test_de_labels, batch_size\n    )\n\n    print(\"\\n(test) loss: {} - accuracy: {} - bleu: \n    {}\".format(test_loss, test_accuracy, test_bleu)) \n```", "```\nResource exhausted: OOM when allocating tensor with ... \n```", "```\nattention_visualizer = tf.keras.models.Model(inputs=[encoder.inputs, decoder_input], outputs=[attn_weights, decoder_out]) \n```", "```\ndef get_attention_matrix_for_sampled_data(attention_model, target_lookup_layer, test_xy, n_samples=5):\n\n    test_x, test_y = test_xy\n\n    rand_ids = np.random.randint(0, len(test_xy[0]), \n    size=(n_samples,))\n    results = []\n\n    for rid in rand_ids:\n        en_input = test_x[rid:rid+1]\n        de_input = test_y[rid:rid+1,:-1]\n\n        attn_weights, predictions = attention_model.predict([en_input, \n        de_input])\n        predicted_word_ids = np.argmax(predictions, axis=-1).ravel()\n        predicted_words = [target_lookup_layer.get_vocabulary()[wid] \n        for wid in predicted_word_ids]\n\n        clean_en_input = []\n        en_start_i = 0\n        for i, w in enumerate(en_input.ravel()):\n            if w=='<pad>': \n                en_start_i = i+1\n                continue\n\n            clean_en_input.append(w)\n            if w=='</s>': break\n        clean_predicted_words = []\n        for w in predicted_words:\n            clean_predicted_words.append(w)\n            if w=='</s>': break\n\n        results.append(\n            {\n                \"attention_weights\": attn_weights[\n                0,:len(clean_predicted_words),en_start_i:en_start_\n                i+len(clean_en_input)\n                ], \n                \"input_words\": clean_en_input,  \n                \"predicted_words\": clean_predicted_words\n            }\n        )\n\n    return results \n```"]