<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Convolutional Neural Networks</h1>
                </header>
            
            <article>
                
<p class="calibre2">In data science, a <strong class="calibre13">convolutional neural network</strong> (<strong class="calibre13">CNN</strong>) is specific kind of deep learning architecture that uses the convolution operation to extract relevant explanatory features for the input image. CNN layers are connected as a feed-forward neural network while using this convolution operation to mimic how the human brain functions while trying to recognize objects. Individual cortical neurons respond to stimuli in a restricted region of space known as the receptive field. In particular, biomedical imaging problems could be challenging sometimes, but in this chapter, we'll see how to use CNN in order to discover patterns in this image.</p>
<p class="calibre2">The following topics will be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">The convolution operation</li>
<li class="calibre8">Motivation</li>
<li class="calibre8">Different layers of CNNs</li>
<li class="calibre8">CNN basic example: MNIST digit classification</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The convolution operation</h1>
                </header>
            
            <article>
                
<p class="calibre2">CNNs are widely used in the area of computer vision and they outperform most of the traditional computer vision techniques that we have been using. CNNs combine the famous convolution operation and neural networks, hence the name convolutional neural network. So, before diving into the neural network aspect of CNNs, we are going to introduce the convolution operation and see how it works.</p>
<p class="calibre2">The main purpose of the convolution operation is to extract information or features from an image. Any image could be considered as a matrix of values and a specific group of values in this matrix will form a feature. The purpose of the convolution operation is to scan this matrix and try to extract relevant or explanatory features for that image. For example, consider a 5 by 5 image whose corresponding intensity or pixel values are shown as zeros and ones:</p>
<div class="CDPAlignCenter"><img src="assets/1c7834a6-2ae2-474e-ad2b-6d51782c9525.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 9.1: Matrix of pixel values</div>
<p class="calibre2">And consider the following 3 x 3 matrix:</p>
<div class="CDPAlignCenter"><img src="assets/d9763afb-ab0d-4025-b397-defa753e1707.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 9.2: Matrix of pixel values</div>
<p class="calibre2">We can convolve the 5 x 5 image using a 3 x 3 one as follows:</p>
<div class="CDPAlignCenter"><img src="assets/03dbfb76-d1b6-4116-96b5-1f20a7da595f.png" class="calibre96"/></div>
<div class="CDPAlignCenter"><img src="assets/bc1d23c0-2eaa-4a9b-bed7-3760beeafefd.png" class="calibre97"/></div>
<div class="CDPAlignCenter"><img src="assets/02ab2c65-cad7-4eb1-a4cb-58461dd8083c.png" class="calibre98"/></div>
<div class="CDPAlignCenter"><img src="assets/6d592c61-4681-4347-9da7-ab30c8857ad7.png" class="calibre99"/></div>
<div class="CDPAlignCenter"><img src="assets/27277f69-a023-44dd-bc20-9d4258e910a8.png" class="calibre100"/></div>
<div class="CDPAlignCenter"><img src="assets/bfe47520-e728-47a6-8970-26a08c2ad7e2.png" class="calibre101"/></div>
<div class="CDPAlignCenter"><img src="assets/2bacdd70-eb6a-4a11-b8ff-c5d1b70511d3.png" class="calibre102"/></div>
<div class="CDPAlignCenter"><img src="assets/d4cb9f3d-e099-49b0-852f-1d6d8b1205c2.png" class="calibre103"/></div>
<div class="CDPAlignCenter"><img src="assets/3a6b2050-5870-4bd4-9f29-ebbb1a88e937.png" class="calibre104"/></div>
<div class="CDPAlignCenter1">Figure 9.3: The convolution operation. The output matrix is called a convolved feature or feature map</div>
<p class="calibre2">The preceding figure could be summarized as follows. In order to convolve the original 5 by 5 image using the 3 x 3 convolution kernel, we need to do the following:</p>
<ul class="calibre7">
<li class="calibre8">Scan the original green image using the orange matrix and each time move by only 1 pixel (stride)</li>
<li class="calibre8">For every position of the orange image, we do element-wise multiplication between the orange matrix and the corresponding pixel values in the green matrix</li>
<li class="calibre8">Add the results of these element-wise multiplication operations together to get a single integer which will form a single value in the output pink matrix</li>
</ul>
<div class="packtinfobox">
<p class="calibre9">As you can see from the preceding figure, the orange 3 by 3 matrix only operates on one part of the original green image at a time in each move (stride), or it only sees a part at a time.</p>
</div>
<p class="calibre2">So, let's put the previous explanation in the context of CNN terminology:</p>
<ul class="calibre7">
<li class="calibre8">The orange 3 x 3 matrix is called a <strong class="calibre1">kernel</strong>, <strong class="calibre1">feature detector</strong>, or <strong class="calibre1">filter</strong></li>
<li class="calibre8">The output pink matrix that contain the results of the element-wise multiplications is called the <strong class="calibre1">feature map</strong></li>
</ul>
<p class="calibre2">Because of the fact that we are getting the feature map based on the element-wise multiplication between the kernel and the corresponding pixels in the original input image, changing the values of the kernel or the filter will give different feature maps each time.</p>
<p class="calibre2">So, we might think that we need to figure out the values of the feature detectors ourselves during the training of the convolution neural networks, but this is not the case here. CNNs figure out these numbers during the learning process. So, if we have more filters, it means that we can extract more features from the image.</p>
<p class="calibre2">Before jumping to the next section, let's introduce some terminology that is usually used in the context of CNNs:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1">Stride</strong>: We mentioned this term briefly earlier. In general, stride is the number of pixels by which we move our feature detector or filter over the pixels of the input matrix. For example, stride 1 means moving the filter one pixel at a time while convolving the input image and stride 2 means moving the filter two pixels at a time while convolving the input image. The more stride we have, the smaller the generated feature maps are.</li>
<li class="calibre8"><strong class="calibre1">Zero-padding</strong>: If we wanted to include the border pixels of the input image, then part of our filter will be outside the input image. Zero-padding solves this problem by padding the input matrix with zeros around the borders.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Motivation</h1>
                </header>
            
            <article>
                
<p class="calibre2">Traditional computer vision techniques were used to perform most computer vision tasks, such as object detection and segmentation. The performance of these traditional computer vision techniques was good but it was never close to being usable in real time, for example by autonomous cars. In 2012, Alex Krizhevsky introduced CNNs, which made a breakthrough on the ImageNet competition by enhancing the object classification error from 26% to 15%. CNNs have been widely used since then and different variations have been discovered. It has even outperformed the human classification error over the ImageNet competition, as shown in the following diagram:</p>
<div class="CDPAlignCenter"><img src="assets/20ae5e5f-425e-44c5-9001-b1b25ca200d6.png" class="calibre105"/></div>
<div class="CDPAlignCenter1">Figure 9.4: Classification error over time with human level error marked in red</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of CNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">Since the breakthrough the CNNs achieved in different domains of computer vision and even natural language processing, most companies have integrated this deep learning solution into their computer vision echo system. For example, Google uses this architecture for its image search engine, and Facebook uses it for doing automatic tagging and more:</p>
<div class="CDPAlignCenter"><img src="assets/4d938629-dc53-44fa-b7fe-7185e92f773d.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 9.5: A typical CNN general architecture for object recognition</div>
<p class="calibre2">CNNs achieved this breakthrough because of their architecture, which intuitively uses the convolution operation to extract features from the images. Later on, you will see that it's very similar to the way the human brain works. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Different layers of CNNs</h1>
                </header>
            
            <article>
                
<p class="calibre2">A typical CNN architecture consists of multiple layers that do different tasks, as shown in the preceding diagram. In this section, we are going to go through them in detail and will see the benefits of having all of them connected in a special way to make such a breakthrough in computer vision.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Input layer</h1>
                </header>
            
            <article>
                
<p class="calibre2">This is the first layer in any CNN architecture. All the subsequent convolution and pooling layers expect the input to be in a specific format. The input variables will tensors, that has the following shape:</p>
<pre class="calibre21">[batch_size, image_width, image_height, channels]</pre>
<p class="calibre2">Here:</p>
<ul class="calibre7">
<li class="calibre8"><kbd class="calibre12">batch_size</kbd> is a random sample from the original training set that's used during applying stochastic gradient descent.</li>
<li class="calibre8"><kbd class="calibre12">image_width</kbd> is the width of the input images to the network.</li>
<li class="calibre8"><kbd class="calibre12">image_height</kbd> is the height of the input images to the network.</li>
<li class="calibre8"><kbd class="calibre12">channels</kbd> are the number of color channels of the input images. This number could be 3 for RGB images or 1 for binary images.</li>
</ul>
<p class="calibre2">For example, consider our famous MNIST dataset. Let's say we are going to perform digit classification using CNNs using this dataset. </p>
<p class="calibre2">If the dataset is composed of monochrome 28 x 28 pixel images like the MNIST dataset, then the desired shape for our input layer is as follows:</p>
<pre class="calibre21">[batch_size, 28, 28, 1].</pre>
<p class="calibre2">To change the shape of our input features, we can do the following reshaping operation:</p>
<pre class="calibre21">input_layer = tf.reshape(features["x"], [-1, 28, 28, 1])</pre>
<div class="packtinfobox">As you can see, we have specified the batch size to be -1, which means that this number should be determined dynamically based on the input values in the features. By doing this, we will be able to fine-tune our CNN model by controlling the batch size.</div>
<p class="calibre2">As an example for the reshape operation, suppose that we divided our input samples into batches of five and our feature <kbd class="calibre12">["x"]</kbd> array will hold 3,920 <kbd class="calibre12">values()</kbd> of the input images, where each value of this array corresponds to a pixel in an image. For this case, the input layer will have the following shape:</p>
<pre class="calibre21">[5, 28, 28, 1]</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolution step</h1>
                </header>
            
            <article>
                
<p class="calibre2">As mentioned earlier, the convolution step got its name from the convolution operation. The main purpose of having these convolution steps is to extract features from the input images and then feed them to a linear classifier.</p>
<p class="calibre2">In natural images, features could be anywhere in the image. For example, edges could be in the middle or at the corner of the images, so the whole idea of stacking a bunch of convolution steps is to be able to detect these features anywhere in the image.</p>
<p class="calibre2">It's very easy to define a convolution step in TensorFlow. For example, if we wanted to apply 20 filters each of size 5 by 5 to the input layer with a ReLU activation function, then we can use the following line of code to do that:</p>
<pre class="calibre21">conv_layer1 = tf.layers.conv2d(<br class="title-page-name"/> inputs=input_layer,<br class="title-page-name"/> filters=20,<br class="title-page-name"/> kernel_size=[5, 5],<br class="title-page-name"/> padding="same",<br class="title-page-name"/> activation=tf.nn.relu)</pre>
<p class="calibre2">The first parameter for this <kbd class="calibre12">conv2d</kbd> function is the input layer that we have defined in the preceding code, which has the appropriate shape, and the second argument is the filters argument which specifies the number of filters to be applied to the image where the higher the number of filters, the more features are extracted from the input image. The third parameter is the <kbd class="calibre12">kernel_size</kbd>, which represents the size of the filter or the feature detector. The padding parameters specifies where we use <kbd class="calibre12">"same"</kbd> here to introduce zero-padding to the corner pixels of the input image. The last argument specifies the activation function that should be used for the output of the convolution operation.</p>
<p class="calibre2">So, in our MNIST example, the input tensor will have the following shape:</p>
<pre class="calibre21">[batch_size, 28, 28, 1]</pre>
<p class="calibre2">And the output tensor for this convolution step will have the following shape:</p>
<pre class="calibre21">[batch_size, 28, 28, 20]</pre>
<p class="calibre2">The output tensor has the same dimensions as the input images, but now we have 20 channels that represent applying the 20 filters to the input image.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing non-linearity</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the convolution step, we talked about feeding the output of the convolution step to a ReLU activation function to introduce non-linearity:</p>
<div class="CDPAlignCenter"><img src="assets/655f87c8-366b-4717-a6bc-8b55c6f1c4c4.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 9.6: ReLU activation function</div>
<p class="calibre2">The ReLU activation function replaces all the negative pixel values with zeros and the whole purpose of feeding the output of the convolution step to this activation function is to introduce non-linearity in the output image because this will be useful for the training process as the data that we are using is usually non-linear. To clearly understand the benefit of ReLU activation function, have a look at the following figure, which shows the row output of the convolution step and the rectified version of it:</p>
<div class="CDPAlignCenter"><img src="assets/fdcf199b-f53c-4b9f-ac13-201474a7f2f4.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 9.7: The result of applying ReLU to the input feature map</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The pooling step</h1>
                </header>
            
            <article>
                
<p class="calibre2">One of the important steps for our learning process is the pooling step, which is sometimes called the subsampling or downsampling step. This step is mainly for reducing the dimensionality of the output of the convolution step (feature map). The advantage of this pooling step is reducing the size of the feature map while keeping the important information in the newly reduced version.</p>
<p class="calibre2">The following figure shows this step by scanning the image with a 2 by 2 filter and stride 2 while applying the max operation. This kind of pooling operation is called <strong class="calibre13">max pool</strong>:</p>
<div class="CDPAlignCenter"><img src="assets/a1278534-f054-4594-b1f5-9e534c5d0c2d.png" class="calibre106"/></div>
<div class="CDPAlignCenter1">Figure 9.8: An example of a max pooling operation on a rectified feature map (obtained after convolution and ReLU operation) by using a 2 x 2 window (source: http://textminingonline.com/wp-content/uploads/2016/10/max_polling-300x256.png)</div>
<p class="calibre2">We can connect the output of the convolution step to the pooling layer by using the following line of code:</p>
<pre class="calibre21">pool_layer1 = tf.layers.max_pooling2d(inputs=conv_layer1, pool_size=[2, 2], strides=2)</pre>
<p class="calibre2">The pooling layer receives the input from the convolution step with the following shape:</p>
<pre class="calibre21">[batch_size, image_width, image_height, channels]</pre>
<p class="calibre2">For example, in our digit classification task, the input to the pooling layer will have the following shape:</p>
<pre class="calibre21">[batch_size, 28, 28, 20]</pre>
<p class="calibre2"> The output of the pooling operation will have the following shape:</p>
<pre class="calibre21">[batch_size, 14, 14, 20]</pre>
<p class="calibre2">In this example, we have reduced the size of the output of the convolution step by 50%. This step is very useful because it keeps only the important information and it also reduces the model's complexity and hence avoids overfitting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fully connected layer</h1>
                </header>
            
            <article>
                
<p class="calibre2">After stacking up a bunch of convolution and pooling steps, we follow them with a fully connected layer where we feed the extracted high-level features that we got from the input image to this fully connected layer to use them and do the actual classification based on these features:</p>
<div class="CDPAlignCenter"><img src="assets/49cba1b5-f082-4e6a-87b4-a24e8cbcad72.png" class="calibre107"/></div>
<div class="CDPAlignCenter1">Figure 9.9: Fully connected layer -each node is connected to every other node in the adjacent layer</div>
<p class="calibre2">For example, in the case of the digit classification task, we can follow the convolution and pooling step with a fully connected layer that has 1,024 neurons and ReLU activation to perform the actual classification. This fully connected layer accepts the input in the following format:</p>
<pre class="calibre21">[batch_size, features]</pre>
<p class="calibre2">So, we need to reshape or flatten our input feature map from <kbd class="calibre12">pool_layer2</kbd> to match this format. We can use the following line of code to reshape the output:</p>
<pre class="calibre21">pool1_flat = tf.reshape(pool_layer1, [-1, 14 * 14 * 20])</pre>
<p class="calibre2">In this reshape function, we have used <kbd class="calibre12">-1</kbd> to indicate that the batch size will be dynamically determined and each example from the <kbd class="calibre12">pool_layer1</kbd> output will have a width of <kbd class="calibre12">14</kbd> and a height of <kbd class="calibre12">14</kbd> with <kbd class="calibre12">20</kbd> channels each.</p>
<p class="calibre2">So the final output of this reshape operation will be as follows:</p>
<pre class="calibre21"> [batch_size, 3136]</pre>
<p class="calibre2">Finally, we can use the <kbd class="calibre12">dense()</kbd> function of TensorFlow to define our fully connected layer with the required number of neurons (units) and the final activation function:</p>
<pre class="calibre21">dense_layer = tf.layers.dense(inputs=pool1_flat, units=1024, activation=tf.nn.relu)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Logits layer</h1>
                </header>
            
            <article>
                
<p class="calibre2">Finally, we need the logits layer, which will take the output of the fully connected layer and then produce the raw prediction values. For example, in the case of the digit classification, the output will be a tensor of 10 values, where each value represents the score of one class from 0-9. So, let's define this logit layer for the digit classification example, where we need 10 outputs only, and with linear activation, which is the default for the <kbd class="calibre12">dense()</kbd> function of TensorFlow:</p>
<pre class="calibre21">logits_layer = tf.layers.dense(inputs=dense_layer, units=10)</pre>
<div class="CDPAlignCenter"><img src="assets/983d2783-0a86-4586-acac-3d52882d8cd0.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 9.10: Training the ConvNet</div>
<p class="calibre2">The final output of this logits layer will be a tensor of the following shape:</p>
<pre class="calibre21">[batch_size, 10]</pre>
<p class="calibre2">As mentioned previously, the logits layer of the model will return the raw predictions our our batch. But we need to convert these values to interpretable format:</p>
<ul class="calibre7">
<li class="calibre8">The predicted class for the input sample 0-9.</li>
<li class="calibre8">The scores or probabilities for each possible class. For example, the probability that the sample is 0, is 1, and so on.</li>
</ul>
<div class="CDPAlignCenter"><img src="assets/d26f9bb1-357c-445c-932f-b8af5ec6d556.jpeg" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 9.11: A visualization of the different layers of a CNN (source: http://cs231n.github.io/assets/cnn/convnet.jpeg)</div>
<p class="calibre2">So, our predicted class will be the one that has the highest value in the 10 probabilities. We can get this value by using the <kbd class="calibre12">argmax</kbd> function as follows:</p>
<pre class="calibre21">tf.argmax(input=logits_layer, axis=1)</pre>
<p class="calibre2">Remember that the <kbd class="calibre12">logits_layer</kbd> has the following shape:</p>
<pre class="calibre21">[batch_size, 10]</pre>
<p class="calibre2">So, we need to find the max values along our predictions, which is the dimension that has an index of 1.</p>
<p class="calibre2">Finally, we can get our next value, which represents the probabilities of each target class, by applying <kbd class="calibre12">softmax</kbd> activation to the output of the <kbd class="calibre12">logits_layer</kbd>, which will squash each value to be between 0 and 1:</p>
<pre class="calibre21">tf.nn.softmax(logits_layer, name="softmax_tensor")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNN basic example – MNIST digit classification</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we will do a complete example of implementing a CNN for digit classification using the MNIST dataset. We will build a simple model of two convolution layers and fully connected layers.<br class="calibre20"/></p>
<p class="calibre2">Let's start off by importing the libraries that will be needed for this implementation:</p>
<div class="title-page-name">
<pre class="calibre21"><span>%</span><span>matplotlib</span> <span>inline</span>
<span>import</span> <span>matplotlib.pyplot</span> <span>as</span> <span>plt</span>
<span>import</span> <span>tensorflow</span> <span>as</span> <span>tf</span>
<span>import</span> <span>numpy</span> <span>as</span> <span>np</span>
<span>from</span> <span>sklearn.metrics</span> <span>import</span> <span>confusion_matrix</span>
<span>import</span> <span>math</span></pre></div>
<p class="calibre2">Next, we will use TensorFlow helper functions to download and preprocess the MNIST dataset as follows:</p>
<pre class="calibre21"><span>from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/>mnist_data = input_data.read_data_sets('data/MNIST/', one_hot=True)</span></pre>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">Output:<br class="title-page-name"/>Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.<br class="title-page-name"/>Extracting data/MNIST/train-images-idx3-ubyte.gz<br class="title-page-name"/>Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.<br class="title-page-name"/>Extracting data/MNIST/train-labels-idx1-ubyte.gz<br class="title-page-name"/>Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.<br class="title-page-name"/>Extracting data/MNIST/t10k-images-idx3-ubyte.gz<br class="title-page-name"/>Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.<br class="title-page-name"/>Extracting data/MNIST/t10k-labels-idx1-ubyte.gz</pre></div>
</div>
</div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">The dataset is split into three disjoint sets: training, validation, and testing. So, let's print the number of images in each set:</p>
</div>
</div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21"><span>print("- Number of images in the training set:\t\t{}".format(len(mnist_data.train.labels)))<br class="title-page-name"/>print("- Number of images in the test set:\t\t{}".format(len(mnist_data.test.labels)))<br class="title-page-name"/>print("- Number of images in the validation set:\t{}".format(len(mnist_data.validation.labels)))</span></pre></div>
</div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">- Number of images in the training set: 55000<br class="title-page-name"/>- Number of images in the test set: 10000<br class="title-page-name"/>- Number of images in the validation set: 5000</pre></div>
</div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">The actual labels of the images are stored in a one-hot encoding format, so we have an array of 10 values of zeros except for the index of the class that this image represents. For later use, we need to get the class numbers of the dataset as integers:</p>
<pre class="calibre21">mnist_data.test.cls_integer = np.argmax(mnist_data.test.labels, axis=1)</pre></div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">Let's define some known variables to be used later in our implementation:</p>
</div>
</div>
<pre class="calibre21"><span># Default size for the input monocrome images of MNIST<br class="title-page-name"/>image_size = 28<br class="title-page-name"/><br class="title-page-name"/># Each image is stored as vector of this size.<br class="title-page-name"/>image_size_flat = image_size * image_size<br class="title-page-name"/><br class="title-page-name"/># The shape of each image<br class="title-page-name"/>image_shape = (image_size, image_size)<br class="title-page-name"/><br class="title-page-name"/># All the images in the mnist dataset are stored as a monocrome with only 1 channel<br class="title-page-name"/>num_channels = 1<br class="title-page-name"/><br class="title-page-name"/># Number of classes in the MNIST dataset from 0 till 9 which is 10<br class="title-page-name"/>num_classes = 10</span></pre>
<p class="calibre2">Next, we need to define a helper function to plot some images from the dataset. This helper function will plot the images in a grid of nine subplots:</p>
<pre class="calibre21"><span>def plot_imgs(imgs, cls_actual, cls_predicted=None):<br class="title-page-name"/>    assert len(imgs) == len(cls_actual) == 9<br class="title-page-name"/>    <br class="title-page-name"/>    # create a figure with 9 subplots to plot the images.<br class="title-page-name"/>    fig, axes = plt.subplots(3, 3)<br class="title-page-name"/>    fig.subplots_adjust(hspace=0.3, wspace=0.3)<br class="title-page-name"/><br class="title-page-name"/>    for i, ax in enumerate(axes.flat):<br class="title-page-name"/>        # plot the image at the ith index<br class="title-page-name"/>        ax.imshow(imgs[i].reshape(image_shape), cmap='binary')<br class="title-page-name"/><br class="title-page-name"/>        # labeling the images with the actual and predicted classes.<br class="title-page-name"/>        if cls_predicted is None:<br class="title-page-name"/>            xlabel = "True: {0}".format(cls_actual[i])<br class="title-page-name"/>        else:<br class="title-page-name"/>            xlabel = "True: {0}, Pred: {1}".format(cls_actual[i], cls_predicted[i])<br class="title-page-name"/><br class="title-page-name"/>        # Remove ticks from the plot.<br class="title-page-name"/>        ax.set_xticks([])<br class="title-page-name"/>        ax.set_yticks([])<br class="title-page-name"/>        <br class="title-page-name"/>        # Show the classes as the label on the x-axis.<br class="title-page-name"/>        ax.set_xlabel(xlabel)<br class="title-page-name"/>        <br class="title-page-name"/>    <br class="title-page-name"/>    plt.show()</span></pre>
<p class="calibre2">Let's plot some images from the test set and see what it looks like:</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21"><span># Visualizing 9 images form the test set.<br class="title-page-name"/>imgs = mnist_data.test.images[0:9]<br class="title-page-name"/><br class="title-page-name"/># getting the actual classes of these 9 images<br class="title-page-name"/>cls_actual = mnist_data.test.cls_integer[0:9]<br class="title-page-name"/><br class="title-page-name"/>#plotting the images<br class="title-page-name"/>plot_imgs(imgs=imgs, cls_actual=cls_actual)</span></pre></div>
</div>
</div>
<p class="calibre2">Here is the output:</p>
<div class="CDPAlignCenter"><img src="assets/2f03662a-f7fb-4e0d-a0cb-d2f1da0cbfc0.png" class="calibre108"/></div>
<div class="CDPAlignCenter1">Figure 9.12: A visualization of some examples from the MNIST dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">Now, it's time to build the core of the model. The computational graph includes all the layers we mentioned earlier in this chapter. We'll start by defining some functions that will be used to define variables of a specific shape and randomly initialize them:</p>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21"><span>def</span> <span>new_weights</span><span>(</span><span>shape</span><span>):</span>
    <span>return</span> <span>tf</span><span>.</span><span>Variable</span><span>(</span><span>tf</span><span>.</span><span>truncated_normal</span><span>(</span><span>shape</span><span>,</span> <span>stddev</span><span>=</span><span>0.05</span><span>))</span></pre></div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21"><span>def</span> <span>new_biases</span><span>(</span><span>length</span><span>):</span>
    <span>return</span> <span>tf</span><span>.</span><span>Variable</span><span>(</span><span>tf</span><span>.</span><span>constant</span><span>(</span><span>0.05</span><span>,</span> <span>shape</span><span>=</span><span>[</span><span>length</span><span>]))</span></pre></div>
</div>
</div>
<p class="calibre2">Now, let's define the function that will be responsible for creating a new convolution layer based on some input layer, input channels, filter size, number of filters, and whether to use pooling parameters <span class="calibre10">or not</span><span class="calibre10">:</span></p>
<pre class="calibre21"><span>def conv_layer(input, # the output of the previous layer.<br class="title-page-name"/>                   input_channels, <br class="title-page-name"/>                   filter_size, <br class="title-page-name"/>                   filters, <br class="title-page-name"/>                   use_pooling=True): # Use 2x2 max-pooling.<br class="title-page-name"/><br class="title-page-name"/>    # preparing the accepted shape of the input Tensor.<br class="title-page-name"/>    shape = [filter_size, filter_size, input_channels, filters]<br class="title-page-name"/><br class="title-page-name"/>    # Create weights which means filters with the given shape.<br class="title-page-name"/>    filters_weights = new_weights(shape=shape)<br class="title-page-name"/><br class="title-page-name"/>    # Create new biases, one for each filter.<br class="title-page-name"/>    filters_biases = new_biases(length=filters)<br class="title-page-name"/><br class="title-page-name"/>    # Calling the conve2d function as we explained above, were the strides parameter<br class="title-page-name"/>    # has four values the first one for the image number and the last 1 for the input image channel<br class="title-page-name"/>    # the middle ones represents how many pixels the filter should move with in the x and y axis<br class="title-page-name"/>    conv_layer = tf.nn.conv2d(input=input,<br class="title-page-name"/>                         filter=filters_weights,<br class="title-page-name"/>                         strides=[1, 1, 1, 1],<br class="title-page-name"/>                         padding='SAME')<br class="title-page-name"/><br class="title-page-name"/>    # Adding the biase to the output of the conv_layer.<br class="title-page-name"/>    conv_layer += filters_biases<br class="title-page-name"/><br class="title-page-name"/>    # Use pooling to down-sample the image resolution?<br class="title-page-name"/>    if use_pooling:<br class="title-page-name"/>        # reduce the output feature map by max_pool layer<br class="title-page-name"/>        pool_layer = tf.nn.max_pool(value=conv_layer,<br class="title-page-name"/>                               ksize=[1, 2, 2, 1],<br class="title-page-name"/>                               strides=[1, 2, 2, 1],<br class="title-page-name"/>                               padding='SAME')<br class="title-page-name"/><br class="title-page-name"/>    # feeding the output to a ReLU activation function.<br class="title-page-name"/>    relu_layer = tf.nn.relu(pool_layer)<br class="title-page-name"/><br class="title-page-name"/>  <br class="title-page-name"/>    # return the final results after applying relu and the filter weights<br class="title-page-name"/>    return relu_layer, filters_weights</span></pre>
<p class="calibre2">As we mentioned previously, the pooling layer produces a 4D tensor. We need to flatten this 4D tensor to a 2D one to be fed to the fully connected layer:</p>
<pre class="calibre21"><span>def flatten_layer(layer):<br class="title-page-name"/>    # Get the shape of layer.<br class="title-page-name"/>    shape = layer.get_shape()<br class="title-page-name"/><br class="title-page-name"/>    # We need to flatten the layer which has the shape of The shape [num_images, image_height, image_width, num_channels]<br class="title-page-name"/>    # so that it has the shape of [batch_size, num_features] where number_features is image_height * image_width * num_channels<br class="title-page-name"/><br class="title-page-name"/>    number_features = shape[1:4].num_elements()<br class="title-page-name"/>    <br class="title-page-name"/>    # Reshaping that to be fed to the fully connected layer<br class="title-page-name"/>    flatten_layer = tf.reshape(layer, [-1, number_features])<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>    # Return both the flattened layer and the number of features.<br class="title-page-name"/>    return flatten_layer, number_features</span></pre>
<div class="title-page-name">
<p class="calibre2"><span class="calibre10">This function creates a fully connected layer which assumes that the input is a 2D tensor:</span></p>
</div>
<pre class="calibre21"><span>def fc_layer(input, # the flatten output.<br class="title-page-name"/>                 num_inputs, # Number of inputs from previous layer<br class="title-page-name"/>                 num_outputs, # Number of outputs<br class="title-page-name"/>                 use_relu=True): # Use ReLU on the output to remove negative values<br class="title-page-name"/><br class="title-page-name"/>    # Creating the weights for the neurons of this fc_layer<br class="title-page-name"/>    fc_weights = new_weights(shape=[num_inputs, num_outputs])<br class="title-page-name"/>    fc_biases = new_biases(length=num_outputs)<br class="title-page-name"/><br class="title-page-name"/>    # Calculate the layer values by doing matrix multiplication of<br class="title-page-name"/>    # the input values and fc_weights, and then add the fc_bias-values.<br class="title-page-name"/>    fc_layer = tf.matmul(input, fc_weights) + fc_biases<br class="title-page-name"/><br class="title-page-name"/>    # if use RelU parameter is true<br class="title-page-name"/>    if use_relu:<br class="title-page-name"/>        relu_layer = tf.nn.relu(fc_layer)<br class="title-page-name"/>        return relu_layer<br class="title-page-name"/><br class="title-page-name"/>    return fc_layer</span></pre>
<p class="calibre2">Before building the network, let's define a placeholder for the input images where the first dimension is <kbd class="calibre12">None</kbd> to represent an arbitrary number of images:</p>
<pre class="calibre21">input_values = tf.placeholder(tf.float32, shape=[None, image_size_flat], name='input_values')</pre>
<p class="calibre2">As we mentioned previously, the convolution step expects the input images to be in the shape of a 4D tensor. So, we need to reshape the input images to be in the following shape:</p>
<pre class="calibre21">[num_images, image_height, image_width, num_channels]</pre>
<p class="calibre2">So, let's reshape the input values to match this format:</p>
<pre class="calibre21">input_image = tf.reshape(input_values, [-1, image_size, image_size, num_channels])</pre>
<p class="calibre2">Next, we need to define another placeholder for the actual class values, which will in one-hot encoding format:</p>
<pre class="calibre21">y_actual = tf.placeholder(tf.float32, shape=[None, num_classes], name='y_actual')</pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">Also, we need to define a placeholder to hold the integer values of the actual class:</p>
</div>
</div>
<pre class="calibre21">y_actual_cls_integer = tf.argmax(y_actual, axis=1)</pre>
<p class="calibre2">So, let's start off by building the first CNN:</p>
<pre class="calibre21"><span>conv_layer_1, conv1_weights = \<br class="title-page-name"/>        conv_layer(input=input_image,<br class="title-page-name"/>                   input_channels=num_channels,<br class="title-page-name"/>                   filter_size=filter_size_1,<br class="title-page-name"/>                   filters=filters_1,<br class="title-page-name"/>                   use_pooling=True)</span></pre>
<p class="calibre2">Let's check the shape of the output tensor that will be produced by the first convolution layer:</p>
<pre class="calibre21">conv_layer_1</pre>
<div class="title-page-name">
<pre class="calibre21">Output:<br class="title-page-name"/>&lt;tf.Tensor 'Relu:0' shape=(?, 14, 14, 16) dtype=float32&gt;</pre></div>
<p class="calibre2">Next, we will create the second convolution network and feed the output of the first one to it:</p>
<pre class="calibre21"><span>conv_layer_2, conv2_weights = \<br class="title-page-name"/>         conv_layer(input=conv_layer_1,<br class="title-page-name"/>                   input_channels=filters_1,<br class="title-page-name"/>                   filter_size=filter_size_2,<br class="title-page-name"/>                   filters=filters_2,<br class="title-page-name"/>                   use_pooling=True)</span></pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">Also, we need to double-check the shape of the output tensor of the second convolution layer. The shape should be <kbd class="calibre12">(?, 7, 7, 36)</kbd>, where the <kbd class="calibre12">?</kbd> mark means an arbitrary number of images.</p>
</div>
<p class="calibre2">Next, we need to flatten the 4D tensor to match the expected format for the fully connected layer, which is a 2D tensor:</p>
</div>
<pre class="calibre21">flatten_layer, number_features = flatten_layer(conv_layer_2)</pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">We need to double-check the shape of the output tensor of the flattened layer:</p>
</div>
</div>
<pre class="calibre21">flatten_layer</pre>
<div class="title-page-name">
<pre class="calibre21">Output:<br class="title-page-name"/>&lt;tf.Tensor 'Reshape_1:0' shape=(?, 1764) dtype=float32&gt;</pre></div>
<p class="calibre2">Next, we will create a fully connected layer and feed the output of the flattened layer to it. We will also feed the output of the fully connected layer to a ReLU activation function before feeding it to the second fully connected layer:</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21"><span>fc_layer_1 = fc_layer(input=flatten_layer,<br class="title-page-name"/>                         num_inputs=number_features,<br class="title-page-name"/>                         num_outputs=fc_num_neurons,<br class="title-page-name"/>                         use_relu=True)</span></pre></div>
</div>
</div>
<p class="calibre2">Let's double-check the shape of the output tensor of the first fully connected layer:</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">fc_layer_1</pre></div>
</div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">Output:<br class="title-page-name"/>&lt;tf.Tensor 'Relu_2:0' shape=(?, 128) dtype=float32&gt;</pre></div>
</div>
</div>
</div>
<p class="calibre2">Next, we need to add another fully connected layer, which will take the output of the first fully connected layer and produce an array of size 10 for each image that represents the scores for each target class being the correct one:</p>
<pre class="calibre21"><span>fc_layer_2 = fc_layer(input=fc_layer_1,<br class="title-page-name"/>                         num_inputs=fc_num_neurons,<br class="title-page-name"/>                         num_outputs=num_classes,<br class="title-page-name"/>                         use_relu=False)</span></pre>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">fc_layer_2</pre></div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">Output:<br class="title-page-name"/>&lt;tf.Tensor 'add_3:0' shape=(?, 10) dtype=float32&gt;</pre></div>
</div>
</div>
</div>
<p class="calibre2">Next, we'll normalize these scores from the second fully connected layer and feed it to a <kbd class="calibre12">softmax</kbd> activation function, which will squash the values to be between 0 and 1:</p>
<pre class="calibre21">y_predicted = tf.nn.softmax(fc_layer_2)</pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">Then, we need to choose the target class that has the highest probability by using the <kbd class="calibre12">argmax</kbd> function of TensorFlow:</p>
</div>
</div>
<pre class="calibre21">y_predicted_cls_integer = tf.argmax(y_predicted, axis=1)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cost function</h1>
                </header>
            
            <article>
                
<p class="calibre2">Next, we need to define our performance measure, which is the cross-entropy. The value of the cross-entropy will be 0 if the predicted class is correct:</p>
<pre class="calibre21"><span>cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=fc_layer_2,<br class="title-page-name"/>                                                        labels=y_actual)</span></pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">Next, we need to average all the cross-entropy values that we got from the previous step to be able to get a single performance measure over the test set:</p>
<pre class="calibre21">model_cost = tf.reduce_mean(cross_entropy)</pre></div>
</div>
<p class="calibre2">Now, we have a cost function that needs to be optimized/minimized, so we will be using <kbd class="calibre12">AdamOptimizer</kbd>, which is an optimization method like gradient descent but a bit more advanced:</p>
<pre class="calibre21">model_optimizer = tf.train.AdamOptimizer(learning_rate=1e-4).minimize(model_cost)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performance measures</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">For showing the output, let's define a variable to check whether the predicted class is equal to the true one:</p>
</div>
<pre class="calibre21">model_correct_prediction = tf.equal(y_predicted_cls_integer, y_actual_cls_integer)</pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">Calculate the model accuracy by casting the boolean values then averaging them to sum the correctly classified ones:</p>
</div>
</div>
<pre class="calibre21">model_accuracy = tf.reduce_mean(tf.cast(model_correct_prediction, tf.float32))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's kick off the training process by creating a session variable that will be responsible for executing the computational graph that we defined earlier:</p>
<pre class="calibre21"><span>session</span> <span>=</span> <span>tf</span><span>.</span><span>Session</span><span>()</span></pre>
<p class="calibre2">Also, we need to initialize the variables that we have defined so far:</p>
<pre class="calibre21"><span>session</span><span>.</span><span>run</span><span>(</span><span>tf</span><span>.</span><span>global_variables_initializer</span><span>())</span></pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">We are going to feed the images in batches to avoid an out-of-memory error:</p>
</div>
</div>
<pre class="calibre21"><span>train_batch_size</span> <span>=</span> <span>64</span></pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">Before kicking the training process, we are going to define a helper function that will perform the optimization process by iterating through the training batches:</p>
</div>
</div>
<pre class="calibre21"><span># number of optimization iterations performed so far<br class="title-page-name"/>total_iterations = 0<br class="title-page-name"/><br class="title-page-name"/>def optimize(num_iterations):<br class="title-page-name"/>    # Update globally the total number of iterations performed so far.<br class="title-page-name"/>    global total_iterations<br class="title-page-name"/><br class="title-page-name"/>    <br class="title-page-name"/>    for i in range(total_iterations,<br class="title-page-name"/>                   total_iterations + num_iterations):<br class="title-page-name"/><br class="title-page-name"/>        # Generating a random batch for the training process<br class="title-page-name"/>        # input_batch now contains a bunch of images from the training set and<br class="title-page-name"/>        # y_actual_batch are the actual labels for the images in the input batch.<br class="title-page-name"/>        input_batch, y_actual_batch = mnist_data.train.next_batch(train_batch_size)<br class="title-page-name"/><br class="title-page-name"/>        # Putting the previous values in a dict format for Tensorflow to automatically assign them to the input<br class="title-page-name"/>        # placeholders that we defined above<br class="title-page-name"/>        feed_dict = {input_values: input_batch,<br class="title-page-name"/>                           y_actual: y_actual_batch}<br class="title-page-name"/><br class="title-page-name"/>        # Next up, we run the model optimizer on this batch of images<br class="title-page-name"/>        session.run(model_optimizer, feed_dict=feed_dict)<br class="title-page-name"/><br class="title-page-name"/>        # Print the training status every 100 iterations.<br class="title-page-name"/>        if i % 100 == 0:<br class="title-page-name"/>            # measuring the accuracy over the training set.<br class="title-page-name"/>            acc_training_set = session.run(model_accuracy, feed_dict=feed_dict)<br class="title-page-name"/>            <br class="title-page-name"/>            #Printing the accuracy over the training set<br class="title-page-name"/>            print("Iteration: {0:&gt;6}, Accuracy Over the training set: {1:&gt;6.1%}".format(i + 1, acc_training_set))<br class="title-page-name"/><br class="title-page-name"/>    # Update the number of iterations performed so far<br class="title-page-name"/>    total_iterations += num_iterations</span></pre>
<div class="title-page-name">
<div class="title-page-name">
<p class="calibre2">And we'll define some helper functions to help us visualize the results of the model and to see which images are misclassified by the model:</p>
</div>
</div>
<pre class="calibre21"><span>def plot_errors(cls_predicted, correct):<br class="title-page-name"/>   <br class="title-page-name"/>    # cls_predicted is an array of the predicted class number of each image in the test set.<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>    # Extracting the incorrect images.<br class="title-page-name"/>    incorrect = (correct == False)<br class="title-page-name"/>    <br class="title-page-name"/>    # Get the images from the test-set that have been<br class="title-page-name"/>    # incorrectly classified.<br class="title-page-name"/>    images = mnist_data.test.images[incorrect]<br class="title-page-name"/>    <br class="title-page-name"/>    # Get the predicted classes for those incorrect images.<br class="title-page-name"/>    cls_pred = cls_predicted[incorrect]<br class="title-page-name"/><br class="title-page-name"/>    # Get the actual classes for those incorrect images.<br class="title-page-name"/>    cls_true = mnist_data.test.cls_integer[incorrect]<br class="title-page-name"/>    <br class="title-page-name"/>    # Plot 9 of these images<br class="title-page-name"/>    plot_imgs(imgs=imgs[0:9],<br class="title-page-name"/>                cls_actual=cls_actual[0:9],<br class="title-page-name"/>                cls_predicted=cls_predicted[0:9])</span></pre>
<p class="calibre2">We can also plot the confusion matrix of the predicted results compared to the actual true classes:</p>
<pre class="calibre21">def plot_confusionMatrix(cls_predicted):<br class="title-page-name"/> <br class="title-page-name"/> # cls_predicted is an array of the predicted class number of each image in the test set.<br class="title-page-name"/><br class="title-page-name"/> # Get the actual classes for the test-set.<br class="title-page-name"/> cls_actual = mnist_data.test.cls_integer<br class="title-page-name"/> <br class="title-page-name"/> # Generate the confusion matrix using sklearn.<br class="title-page-name"/> conf_matrix = confusion_matrix(y_true=cls_actual,<br class="title-page-name"/> y_pred=cls_predicted)<br class="title-page-name"/><br class="title-page-name"/> # Print the matrix.<br class="title-page-name"/> print(conf_matrix)<br class="title-page-name"/><br class="title-page-name"/> # visualizing the confusion matrix.<br class="title-page-name"/> plt.matshow(conf_matrix)<br class="title-page-name"/><br class="title-page-name"/> plt.colorbar()<br class="title-page-name"/> tick_marks = np.arange(num_classes)<br class="title-page-name"/> plt.xticks(tick_marks, range(num_classes))<br class="title-page-name"/> plt.yticks(tick_marks, range(num_classes))<br class="title-page-name"/> plt.xlabel('Predicted class')<br class="title-page-name"/> plt.ylabel('True class')<br class="title-page-name"/> <br class="title-page-name"/> # Showing the plot<br class="title-page-name"/> plt.show()</pre>
<p class="calibre2">Finally, we are going to define a helper function to help us measure the accuracy of the trained model over the test set:</p>
<pre class="calibre21"><span># measuring the accuracy of the trained model over the test set by splitting it into small batches<br class="title-page-name"/>test_batch_size = 256<br class="title-page-name"/><br class="title-page-name"/>def test_accuracy(show_errors=False,<br class="title-page-name"/>                        show_confusionMatrix=False):<br class="title-page-name"/><br class="title-page-name"/>    #number of test images <br class="title-page-name"/>    number_test = len(mnist_data.test.images)<br class="title-page-name"/><br class="title-page-name"/>    # define an array of zeros for the predicted classes of the test set which<br class="title-page-name"/>    # will be measured in mini batches and stored it.<br class="title-page-name"/>    cls_predicted = np.zeros(shape=number_test, dtype=np.int)<br class="title-page-name"/><br class="title-page-name"/>    # measuring the predicted classes for the testing batches.<br class="title-page-name"/> <br class="title-page-name"/>    # Starting by the batch at index 0.<br class="title-page-name"/>    i = 0<br class="title-page-name"/><br class="title-page-name"/>    while i &lt; number_test:<br class="title-page-name"/>        # The ending index for the next batch to be processed is j.<br class="title-page-name"/>        j = min(i + test_batch_size, number_test)<br class="title-page-name"/><br class="title-page-name"/>        # Getting all the images form the test set between the start and end indices<br class="title-page-name"/>        input_images = mnist_data.test.images[i:j, :]<br class="title-page-name"/><br class="title-page-name"/>        # Get the acutal labels for those images.<br class="title-page-name"/>        actual_labels = mnist_data.test.labels[i:j, :]<br class="title-page-name"/><br class="title-page-name"/>        # Create a feed-dict with the corresponding values for the input placeholder values<br class="title-page-name"/>        feed_dict = {input_values: input_images,<br class="title-page-name"/>                     y_actual: actual_labels}<br class="title-page-name"/><br class="title-page-name"/>    <br class="title-page-name"/>        cls_predicted[i:j] = session.run(y_predicted_cls_integer, feed_dict=feed_dict)<br class="title-page-name"/><br class="title-page-name"/>        # Setting the start of the next batch to be the end of the one that we just processed j<br class="title-page-name"/>        i = j<br class="title-page-name"/><br class="title-page-name"/>    # Get the actual class numbers of the test images.<br class="title-page-name"/>    cls_actual = mnist_data.test.cls_integer<br class="title-page-name"/><br class="title-page-name"/>    # Check if the model predictions are correct or not<br class="title-page-name"/>    correct = (cls_actual == cls_predicted)<br class="title-page-name"/><br class="title-page-name"/>    # Summing up the correct examples<br class="title-page-name"/>    correct_number_images = correct.sum()<br class="title-page-name"/><br class="title-page-name"/>    # measuring the accuracy by dividing the correclty classified ones with total number of images in the test set.<br class="title-page-name"/>    testset_accuracy = float(correct_number_images) / number_test<br class="title-page-name"/><br class="title-page-name"/>    # showing the accuracy.<br class="title-page-name"/>    print("Accuracy on Test-Set: {0:.1%} ({1} / {2})".format(testset_accuracy, correct_number_images, number_test))<br class="title-page-name"/><br class="title-page-name"/>    # showing some examples form the incorrect ones.<br class="title-page-name"/>    if show_errors:<br class="title-page-name"/>        print("Example errors:")<br class="title-page-name"/>        plot_errors(cls_predicted=cls_predicted, correct=correct)<br class="title-page-name"/><br class="title-page-name"/>    # Showing the confusion matrix of the test set predictions<br class="title-page-name"/>    if show_confusionMatrix:<br class="title-page-name"/>        print("Confusion Matrix:")<br class="title-page-name"/>        plot_confusionMatrix(cls_predicted=cls_predicted)</span></pre>
<p class="calibre2">Let's print the accuracy of the created model over the test set without doing any optimization:</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">test_accuracy()</pre></div>
</div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">Output:<br class="title-page-name"/>Accuracy on Test-Set: 4.1% (410 / 10000)</pre></div>
</div>
</div>
</div>
<p class="calibre2">Let's get a sense of the optimization process actually enhancing the model capability to classify images to their correct class by running the optimization process for one iteration:</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">optimize(num_iterations=1)<br class="title-page-name"/>Output:<br class="title-page-name"/>Iteration: 1, Accuracy Over the training set: 4.7%<br class="title-page-name"/>test_accuracy()<br class="title-page-name"/>Output<br class="title-page-name"/>Accuracy on Test-Set: 4.4% (437 / 10000)</pre></div>
</div>
</div>
</div>
</div>
<p class="calibre2">Now, let's get down to business and kick off a long optimization process of 10,000 iterations:</p>
<pre class="calibre21">optimize(num_iterations=9999) #We have already performed 1 iteration.</pre>
<p class="calibre2">At the end of the output, you should be getting something very close to the following output:</p>
<pre class="calibre21">Iteration: 7301, Accuracy Over the training set: 96.9%<br class="title-page-name"/>Iteration: 7401, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 7501, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 7601, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 7701, Accuracy Over the training set: 96.9%<br class="title-page-name"/>Iteration: 7801, Accuracy Over the training set: 96.9%<br class="title-page-name"/>Iteration: 7901, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 8001, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 8101, Accuracy Over the training set: 96.9%<br class="title-page-name"/>Iteration: 8201, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 8301, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 8401, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 8501, Accuracy Over the training set: 96.9%<br class="title-page-name"/>Iteration: 8601, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 8701, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 8801, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 8901, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 9001, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 9101, Accuracy Over the training set: 96.9%<br class="title-page-name"/>Iteration: 9201, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 9301, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 9401, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 9501, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 9601, Accuracy Over the training set: 98.4%<br class="title-page-name"/>Iteration: 9701, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 9801, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 9901, Accuracy Over the training set: 100.0%<br class="title-page-name"/>Iteration: 10001, Accuracy Over the training set: 98.4%</pre>
<p class="calibre2">Now, let's check how the model will generalize over the test:</p>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21"><span>test_accuracy(show_errors=True,<br class="title-page-name"/>                    show_confusionMatrix=True)</span></pre></div>
</div>
</div>
</div>
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">Output:<br class="title-page-name"/>Accuracy on Test-Set: 92.8% (9281 / 10000)<br class="title-page-name"/>Example errors:<br class="title-page-name"/><br class="title-page-name"/></pre></div>
</div>
</div>
</div>
<div class="CDPAlignCenter"><img src="assets/cea9722b-34a1-4a22-a216-9083a559579a.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 9.13: Accuracy over the test</div>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">Confusion Matrix:
[[ 971    0    2    2    0    4    0    1    0    0]
 [   0 1110    4    2    1    2    3    0   13    0]
 [  12    2  949   15   16    3    4   17   14    0]
 [   5    3   14  932    0   34    0   13    6    3]
 [   1    2    3    0  931    1    8    2    3   31]
 [  12    1    4   13    3  852    2    1    3    1]
 [  21    4    5    2   18   34  871    1    2    0]
 [   1   10   26    5    5    0    0  943    2   36]
 [  16    5   10   27   16   48    5   13  815   19]
 [  12    5    5   11   38   10    0   18    3  907]]</pre>
<p class="calibre2">The following is the output:</p>
</div>
</div>
<p class="calibre2"/>
<div class="CDPAlignCenter"><img src="assets/71ec6e79-46a4-4511-b063-c5985c75d61d.png" class="calibre31"/></div>
<div class="title-page-name">
<div class="CDPAlignCenter1">Figure 9.14: Confusion matrix of the test set.</div>
</div>
<p class="calibre2">It was interesting that we actually got almost 93% accuracy over the test while using a basic convolution network. This implementation and the results show you what a simple convolution network can do.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we have covered the intuition and the technical details of how CNNs work. we also had a look at how to implement a basic architecture of a CNN in TensorFlow.</p>
<p class="calibre2">In the next chapter we'll demonstrate more advanced architectures that could be used for detecting objects in one of the <span class="calibre10">image datasets</span><span class="calibre10"> widely used by data scientists. We'll also see the beauty of CNNs and how they come to mimic human understanding of objects by first realizing the basic features of objects and then building up more advanced semantic features on them to come up with a classification for them. Although this process happens very quickly in our minds, it is what actually happens when we recognize objects.</span></p>


            </article>

            
        </section>
    </body></html>