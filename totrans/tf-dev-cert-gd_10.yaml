- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introduction to Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Today, we are faced with a staggering amount of text data coming at us from
    all directions, from social media platforms to email communications, and from
    text messages to online reviews. This exponential growth in text data has led
    to a rapid growth in the development of text-based applications powered by advanced
    deep learning techniques, used to unlock insights from text data. We find ourselves
    in the dawn of a transformative era, powered by tech giants such as Google and
    Microsoft and revolutionary start-ups such as OpenAI and Anthropic. These visionaries
    are leading the charge in building powerful solutions capable of solving a myriad
    of text-based challenges, such as summarizing large volumes of documents, extracting
    sentiments from online platforms, and generating text for blog posts – the list
    of uses is endless.
  prefs: []
  type: TYPE_NORMAL
- en: Real-world text data can be messy; it could be riddled with unwanted information
    such as punctuation marks, special characters, and common words that may not contribute
    significantly to the text’s meaning. Hence, we will kick off this chapter by looking
    at some basic text preprocessing steps to help transform text data into a more
    digestible form in preparation for modeling. Again, you may wonder, how do these
    machines learn to understand text? How do they make sense of words and sentences,
    or even grasp their semantic meaning or the context in which words are used? In
    this chapter, we will journey through the fundamentals of **natural language processing**
    (**NLP**). We will explore concepts such as tokenization, which deals with how
    we segment text into individual words or terms (tokens). We will also explore
    the concept of word embeddings – here, we will see how they enable models to capture
    the meaning, context, and relationship between words.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we will put together all we have learned in this chapter to build a sentiment
    analysis model, using the Yelp Polarity dataset to classify customer reviews.
    As an interactive activity, we will examine how to visualize word embeddings in
    TensorFlow; this can be useful in gaining a snapshot of how our model understands
    and represents different words. We will also explore various techniques to improve
    the performance of our sentiment analysis classifier. By the end of this chapter,
    you will have a good foundational understanding of how to preprocess and model
    text data, as well as the skills required to tackle real-world NLP problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Text preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a sentiment classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embedding visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model improvement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP is an exciting and evolving field that lies at the intersection of computer
    science and linguistics. It empowers computers with the ability to understand,
    analyze, interpret, and generate text data. However, working with text data presents
    a unique set of challenges, one that differs from the tabular and image data we
    worked with in the earlier sections of this book. *Figure 10**.1* gives us a high-level
    overview of some of the inherent challenges that text data presents. Let’s drill
    into them and see what and how they present issues to us when building deep learning
    models with text data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – The challenges presented by text data](img/B18118_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – The challenges presented by text data
  prefs: []
  type: TYPE_NORMAL
- en: Text data in its natural form is unstructured, and this is just the beginning
    of the uniqueness of this interesting type of data we will work with in this chapter.
    Let's illustrate some of the issues by looking at these two sentences – “*The
    house next to ours is beautiful*” versus “*Our neighbor’s house is one everyone
    in this area admires*.” Both phrases have a similar sentiment, yet they have different
    structures and varying lengths. To humans, this lack of structure and varying
    length is not a challenge, but when we work with deep learning models, this could
    pose a challenge. To address these challenges, we can consider ideas such as tokenization,
    which refers to the splitting of text data into smaller units called tokens. These
    tokens could be used to represent words, sub-words, or individual characters.
    To handle the varying length of text data in preparation for modeling, we will
    reuse an old trick that we applied when working with image data with CNNs – padding.
    By padding the sentences, we can ensure that our data (such as sentences or paragraphs)
    is of the same length. This uniformity makes our data more digestible for our
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we may come across words with multiple meanings and decoding the meaning
    of these words largely depends on the context in which they are used. For example,
    if we have a sentence reading “*I will be at the bank*,” without additional context,
    it is difficult to tell whether *bank* refers to a financial bank or a riverbank.
    Words such as this add an extra layer of complexity when modeling text data. To
    handle this issue, we need to apply techniques that capture the essence of words
    and their surrounding words. A good example of such a technique is word embeddings.
    Word embeddings are powerful vector representations that can be used to capture
    the semantic meaning of words, by enabling words with a similar meaning or context
    to have similar representations.
  prefs: []
  type: TYPE_NORMAL
- en: Other issues we could face when working with text data are typos, spelling variations,
    and noise. To tackle these issues, we can use noise-filtering techniques to filter
    out URLs, special characters, and other irrelevant entities when collecting data
    online. Let’s say we have a sample sentence – “*Max loves to play chess at the
    London country club, and he is the best golfer on our street*.” When we examine
    this sentence, we see that it contains common words such as *and*, *is*, and *the*.
    Although these words are needed for linguistic coherence, in some instances, they
    may not add semantic value. If this is the case, then we may want to remove these
    words to reduce the dimensionality of our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve covered some foundational ideas around the challenges of text
    data, let’s see how to preprocess text data by looking at how we can extract and
    clean text data on machine learning from Wikipedia. Here, we will see how to apply
    TensorFlow to perform techniques such as tokenization, padding, and using word
    embeddings to extract meaning from text. To access the sample data, use this link:
    [https://en.wikipedia.org/wiki/Machine_learning](https://en.wikipedia.org/wiki/Machine_learning).
    Let’s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use these libraries to effectively fetch, preprocess, and tokenize web data,
    preparing it for modeling with neural networks. When we want to access data from
    the internet, the `requests` library can prove to be a useful tool, enabling us
    to streamline the process of retrieving information from web pages by making requests
    to web servers and fetching web data. The collected data is often in the HTML
    format, which isn’t in the best shape for us to feed into our models. This is
    where `BeautifulSoup` (an intuitive tool for parsing HTML and XML) comes into
    the picture, enabling us to easily navigate and access the content we need. To
    perform string manipulation, text cleaning, or extracting patterns, we can use
    the `re` module. We also import the `Tokenizer` class from TensorFlow’s Keras
    API, which enables us to perform tokenization, thus converting our data into a
    model-friendly format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we assign our variable to the web page we want to scrape; in this case,
    we are interested in scraping data from the Wikipedia page on machine learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use the `GET` method to retrieve data from the web server. The web server
    replies with the status code that tells us whether the request was a success or
    failure. It also returns other metadata along with the HTML content of the web
    page – in our case, the Wikipedia page. We save the server’s response to the `GET`
    request in the `response` object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We use the `BeautifulSoup` class to do the heavy lifting of parsing the HTML
    content, which we access by using `response.content`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we convert the raw HTML contents contained in `response.content` into
    a digestible format by specifying `html.parser` and storing the result in the
    `soup` variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s extract all the text contents in a paragraph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use `soup.find_all('p')` to extract all the paragraphs stored in the `soup`
    variable. Then, we apply the `join` method to combine them into a body of text
    in which each paragraph is repeated by a space, and then we store this text in
    the `passage` variable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is the removal of stopwords from our data. Stopwords are common
    words that may not contain useful information in a certain use case. Hence, we
    may want to remove them to help reduce the dimensionality of our data, especially
    for tasks where these high-frequency words offer little importance, such as information
    retrieval or document clustering. Here, it may be wise to remove stopwords to
    enable faster convergence and produce better categorization. Examples of stopwords
    are words such as “and,” “the,” “in,” and “is”:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have defined a list of stopwords. This way, we have the flexibility of adding
    words of our choice to this list. This approach can be useful when working on
    domain-specific projects in which you may want to extend your stopword list.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step might not always be beneficial. In some NLP tasks, stopwords might
    contain useful information. For example, in text generation or machine translation,
    a model needs to generate/translate stopwords to produce coherent sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s convert the entire passage into lowercase. We do this to ensure words
    with the same semantic meaning are not represented differently – for example,
    “DOG” and “dog.” By ensuring all our data is in lowercase, we introduce uniformity
    to our dataset, removing the possibility of duplicate representation of the same
    word. To convert our text to lowercase, we use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we run the code, it converts all our text data to lowercase.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Converting a body of text to lowercase isn’t always the best solution. In fact,
    in some use cases, such as sentiment analysis, converting to lowercase may lead
    to information loss because capital letters are usually used to express strong
    emotions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s remove the HTML tags, special characters, and stopwords from the
    passage we gathered from Wikipedia. To do this, we‘ll use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the first line of code, we remove the HTML tags, after which we remove unwanted
    special characters in our data. Then, we pass the passage through a stopword filter
    to check and remove words that are in the stopword list, after which we combine
    the remaining words into a passage, separated by spaces between them. We print
    the first 500 characters to get an idea of what our processed text looks like.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s print the first 500 characters and compare that with the web page shown
    in *Figure 10**.2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When we compare the output with the web page, we can see that our text is all
    in lowercase and all the stopwords, special characters, and HTML tags have been
    removed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2 – A screenshot of the Wikipedia page on machine learning](img/B18118_10_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – A screenshot of the Wikipedia page on machine learning
  prefs: []
  type: TYPE_NORMAL
- en: Here, we explored some simple steps in preparing text data for modeling with
    neural networks. Now, let’s extend our learning by exploring tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have looked at some important ideas about how to preprocess real-world text
    data. Our next step is to map out a strategy to prepare our text model. To do
    this, let’s begin by examining the concept of tokenization, which entails breaking
    sentences into smaller units called tokens. Tokenization can be implemented at
    character, sub-word, word, or even sentence level. It is common to see a lot of
    word-level tokenizers; however, the choice of tokenizer to use largely depends
    on the use case.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how we can apply tokenization to a sentence with a sample. Let’s say
    we have this sentence – “*I like playing chess in my leisure time*.” Applying
    word-level tokenization will give us output such as `["i", "like", "playing",
    "chess", "in", "my", "leisure", "time"]`, while if we decide to use character
    level tokenization, we will have output such as `['I', ' ', 'l', 'i', 'k', 'e',
    ' ', 'p', 'l', 'a', 'y', 'i', 'n', 'g', ' ', 'c', 'h', 'e', 's', 's', ' ', 'i',
    'n', ' ', 'm', 'y', ' ', 'l', 'e', 'i', 's', 'u', 'r', 'e', ' ', 't', 'i', 'm',
    'e']`. In word-level tokenization, we split across each word, while in character-level
    tokenization, we split on each character. You can also see that wide spaces within
    the sentence are included when we use character-level tokenization. For subword
    and sentence-level tokenization, we split into subwords and sentences respectively.
    Now, let’s use TensorFlow to implement word-level and character-level tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: Word-level tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s see how to perform word-level tokenization with TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by importing the `Tokenizer` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we create a variable called text to hold our sample sentence (`"Machine
    learning is fascinating. It is a field full of challenges!"`). We create an instance
    of the `Tokenizer` class to handle the tokenization of our sample sentence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can pass several parameters into the `tokenizer` class, depending on our
    use case. For instance, we can set the maximum number of words we want to keep
    by using `num_words`. Also, we may want to convert our entire text into lowercase;
    we can do this with the `tokenizer` class. However, if we don’t specify these
    parameters, TensorFlow will apply the default parameters. Then, we use the `fit_on_text`
    method to fit the tokenizer on our text data. The `fit_on_text` method goes through
    the input text and creates a vocabulary made up of unique words. It also counts
    the number of occurrences of each word in our input text data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To view the mapping of words to integer values, we use the `word_index` property
    of our `tokenizer` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we print out the result, we can see that `word_index` returns a dictionary
    of key-value pairs, where each key-value pair corresponds to a unique word and
    its assigned integer index in the tokenizer’s vocabulary:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see that the exclamation mark in our sample sentence is gone and the
    word `'is'` is listed only once. Also, you can see that our indexing begins at
    `1` and not `0`, because `0` is reserved as a special token, which we will encounter
    shortly. Now, let’s also examine how to perform character-level tokenization.
  prefs: []
  type: TYPE_NORMAL
- en: Character-level tokenization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In character-level tokenization, we split the text on each character within
    our sample text. To do this with TensorFlow, we slightly modify the code we used
    for word-level tokenization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we set the `char_level` argument to `True` when we create our `tokenizer`
    instance. When we do this, we can see that only unique characters in our text
    will be treated as separate tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: Note that every unique character is represented in this scenario, including
    whitespaces (`' '`) with a token value of `1`, full stops (`'.'`) with a token
    value of `15`, and exclamations (`'!'`) with a token value of `19`.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of tokenization we talked about is subwords. Subwords involve breaking
    down words into commonly occurring groups of characters – for example, “unhappiness”
    might be tokenized into [“un”, “happiness”]. Once the text is tokenized, each
    token can be transformed into a numerical representation, using one of the encoding
    methods that we will discuss in this chapter. Now, let’s look at another concept
    called sequencing.
  prefs: []
  type: TYPE_NORMAL
- en: Sequencing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The order in which words are used in a sentence is crucial to understanding
    the meaning they convey; sequencing is the process of converting sentences or
    a group of words or tokens into their numerical representations, preserving the
    sequential order of words when building NLP applications using neural networks.
    In TensorFlow, we can use the `texts_to_sequences` function to convert our tokenized
    text into a sequence of integers. From the output of our word-level tokenization
    step, we now know that our sample sentence (`"Machine learning is fascinating.
    It is a field full of challenges!"`) can be represented by a list of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'By converting text into sequences, we translate human-readable text into a
    machine-readable format while preserving the order in which words occur. When
    we print the result, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: The output printed is the sequence of integers that represent the original text.
    In many real-world scenarios, we will have to handle sentences of varying lengths.
    While it is not a problem for humans to understand sentences irrespective of their
    length, neural networks require us to put our data in a defined type of input
    format. In the image classification section of this book, we used a fixed width
    and height when passing image data as input; with text data, we have to ensure
    that all our sentences are of the same length. To do this, let’s return to a concept
    we discussed in [*Chapter 7*](B18118_07.xhtml#_idTextAnchor146)*,* *Image Classification
    with Convolutional Neural Networks,* padding, and see how we can leverage it to
    resolve the issue of varying sentence lengths.
  prefs: []
  type: TYPE_NORMAL
- en: Padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 7*](B18118_07.xhtml#_idTextAnchor146), *Image Classification with
    Convolutional Neural Networks,* we introduced the concept of padding when we discussed
    CNNs. In the context of NLP, padding is the process of adding elements to a sequence
    to ensure that it attains a desired length. To do this in TensorFlow, we use the
    `pad_sequences` function from the `keras` preprocessing module. Let’s use an example
    to explain the application of padding to text data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we have the following four sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we perform word-level tokenization and sequencing, the output will look
    like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the length of our returned sequences varies with the second
    sentence longer than the other sentences. Let’s resolve this issue using padding
    next.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We import `pad_sequences` from the TensorFlow Keras preprocessing module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `pad_sequences` function takes various parameters – here, we will discuss
    a few important ones, such as `sequences`, `maxlen`, `truncating`, and `padding`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s start passing sequences as the only parameter and observe what the result
    looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we use the `pad_sequence` function, it ensures all the sentences are the
    same length as our longest sentence. To achieve this, a special token (`0`) is
    used to pad the shorter sentences until they are of the same length as the longest
    sentence. The special token (`0`) does not carry any meaning, and models are built
    to ignore them during training and inference:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the output, we see that every other sentence has zeros added to it until
    its length is the same length as our longest sentence (sentence two), which has
    the longest sequence. Note that all the zeros are added at the beginning of each
    sentence. This scenario is known as `padding=post` parameter:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we print the result, we get the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, we can see that the zeros are added at the end of shorter sentences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Another useful parameter is `maxlen`. It is used to specify the maximum length
    for all sequences we want to keep. In this case, any sequence greater than the
    specified `maxlen` will be truncated. To see how `maxlen` works, let’s add another
    sentence to our list of sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We take our new list of sentences and perform tokenization and sequencing on
    them. Then, we pad the numerical representations to ensure that our input data
    is of the same length, and to ensure that our special (`0`) tokens are added at
    the end of a sentence, we set `padding` to `post`. When we implement these steps,
    our output looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: From the output, we can see that one sentence is quite long, and the other sentences
    are largely made up of numerical representations with many zeros. In this case,
    it’s clear that our longest sentence is an outlier, since all the other sentences
    are much smaller. This can skew our model’s learning process and also increase
    the computation resource required to model our data, especially when we work with
    large datasets or limited computation resources. To fix this situation, we apply
    the `maxlen` parameter. It is important to use a good `max_length` value; otherwise,
    we could lose important information in our data due to truncation. It is a good
    idea to make the maximum length long enough to capture useful information without
    adding much noise.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s see how to apply `maxlen` in our example. We start by setting the `max_length`
    variable to `10`. This means it will take only a maximum of 10 tokens. We pass
    the `maxlen` parameter and print our padded sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our result produces a much shorter sequence:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Note that our longest sentence has been truncated at the beginning of the sequence.
    What if we want to truncate the sentence at the end? How do we achieve this? To
    do this, we introduce another parameter called `truncating` and set it to `post`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our result will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We now have all our sequences padded and truncating done at the end of the sentence.
    Now, what if we train our model to classify text using these five sentences, and
    we want to make a prediction on a new sentence (“*I love playing chess*”)? Remember
    from our training sentences that our model will have tokens to represent “I” and
    “love.” However, it has no way of knowing or representing “playing” and “Chess.”
    This presents us with another problem. Let’s look at how to solve this.
  prefs: []
  type: TYPE_NORMAL
- en: Out of vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have seen how to prepare our data, moving from a sequence of text
    data that makes up sentences to numerical representations to train our models.
    Now, let’s say we build a text classification model, which we train using the
    five sentences in our sentence list. Of course, this is a hypothetical situation,
    which would hold true even when we train with a massive amount of text data, as
    we will eventually come across words that our model has not seen before in training,
    such as in this scenario with our sample test sentence (“*I love playing chess*”).
    This means we must prepare our model to handle words that are not present in our
    predefined vocabulary. To fix this issue, we use `oov_token="<OOV>"` argument
    during the tokenization process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we fit the tokenizer on the training sentences, converting the
    sentences to sequences of integers, and then we print out the word index:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can see that the `"<OOV>"` string is chosen to represent these OOV
    words and has a value of `1`. This token will take care of any unknown words that
    the model comes across. Let’s see this in action with our sample test sentence:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: 'We pass in our test sentence (`"I love playing chess"`), which contains words
    our model has not seen before, and then use the `texts_to_sequences` method to
    convert the test sentence into a sequence. Because we fit the tokenizer on the
    training sentences, it will replace each word in the test sentence with its corresponding
    numerical representation from the word index. However, the words “playing” and
    “chess,” which were not present in our training sentences, will be replaced with
    the index of the special OOV token; hence, the `print` statement returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Here, the token value of `1` is used for the words `playing` and `chess`. Using
    the OOV token is a common practice in NLP to handle words that are not present
    in the training data but may appear in the test or real-world data.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have our text data as a numerical representation. We have also preserved
    the sequence in which words occur; however, we need to find a way to capture the
    semantic meaning of the words and their relationships with each other. To do this,
    we use word embeddings. Let’s discuss word embeddings next.
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A significant landmark in the field of NLP is the use of word embeddings. With
    word embeddings, we are able to solve many complex modern-day text-based problems.
    **Word embeddings** are a type of word representation that allows words with similar
    meanings to have a similar representation, with the ability to capture the context
    in which a word is used. Along with the context, word embedding is also able to
    capture the semantic and syntactic similarity between words and how a word relates
    to other words. This allows ML models to generalize better when using word embedding
    in comparison to instances where words are used as standalone input.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies such as one-hot encoding prove to be inefficient, as it builds a
    sparse representation of words largely made up of zeros. This happens because
    the more words we have in our vocabulary, the greater the number of zeros we will
    have in our resulting vector when we apply one-hot encoding. Conversely, word
    embedding is a dense vector representation in a continuous space that can capture
    the meaning, context, and relationship between words using dense and low-dimensional
    vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine the following sample sentences and see how word embedding works:'
  prefs: []
  type: TYPE_NORMAL
- en: She enjoys reading books.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: He likes reading newspapers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are eating grapes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We start by tokenizing each sentence and apply sequencing to transform each
    sentence into a sequence of integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[1, 2, 3, 4]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[5, 6, 3, 7]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[8, 9, 10, 11]'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe that with our returned sequence, we have successfully captured the order
    in which the words that make up each sentence occur. However, this approach fails
    to take into consideration the meaning of words or the relationship between words.
    For example, the words “enjoy” and “likes” both portray positive sentiments in
    sentence 1 and sentence 2, while both sentences have “reading” as their common
    action. When we design deep learning models, we want them to be aware that “books”
    and “newspapers” are more closely related and differ from words such as “grapes”
    and “eating,” as shown in *Figure 10**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Word embedding](img/B18118_10_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Word embedding
  prefs: []
  type: TYPE_NORMAL
- en: Word embeddings empower our models to capture relationships between words, thus
    enhancing our ability to build better-performing models. We have explored some
    foundational ideas around text preprocessing and data preparation, taking our
    text data from words to numerical representations, capturing both sequencing and
    the underlying relationships between words used in language. Let’s now put together
    everything we have learned and build a sentiment analysis model, using the Yelp
    Polarity dataset. We will start by training our own word embedding from scratch,
    after which we will apply pretrained word embeddings to our use case.
  prefs: []
  type: TYPE_NORMAL
- en: The Yelp Polarity dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this experiment, we will work with the Yelp Polarity dataset. This dataset
    is made up of a training size of 560,000 reviews and 38,000 reviews for testing,
    with each entry consisting of a text-based review and a label (positive – 1 and
    negative – 0). The data was drawn from customer reviews of restaurants, hair salons,
    locksmiths, and so on. This dataset presents some real challenges – for example,
    the reviews are made up of text with varying lengths, from short reviews to very
    long reviews. Also, the data contains the use of slang and different dialects.
    The dataset is available at this link: [https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews](https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start building our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by loading the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We import the necessary libraries to load, split, preprocess, and visualize
    word embeddings, and model our dataset with TensorFlow for our sentiment analysis
    use case.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we load the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use the `tf.load` function to fetch datasets from TensorFlow datasets. Here,
    we specify our dataset, which is the Yelp Polarity reviews dataset. We also split
    our data into training and testing sets. We shuffle our data by setting shuffle
    to `True`, and we set `with_info=True` to ensure we can retrieve metadata of the
    dataset, which can be accessed using the `dataset_info` variable. We also set
    `as_supervised=True`; when we do this, it returns a tuple made up of the input
    and target rather than a dictionary. This way we can directly use the dataset
    with the `fit` method to train our models. We now have our training dataset as
    `train_dataset` and our testing set as `test_dataset`; both datasets are `tf.data.Dataset`
    objects. Let’s proceed with some quick data exploration before we build our sentiment
    analysis models on our training data and evaluate them on the testing data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s write some functions to enable us to explore our dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use the `get_reviews` function to examine reviews from either the training
    or testing sets. This function displays the specified number of reviews and their
    corresponding labels; by default, it displays the first five reviews. However,
    we can set this parameter to any number we want. The second function is the `dataset_insight`
    function – this function performs several analyses, such as extracting the shortest,
    longest, and average length of reviews. It also generates the total count of positive
    and negative reviews in the dataset. Because we are working with a large dataset,
    we set `dataset_insight` to explore the first 2,000 samples. If you increase the
    number of samples, it will take a long time to analyze the data. We pass the total
    positive and negative count of reviews into the `plot_reviews` function to give
    us a graphical distribution of the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s check out the first seven reviews in our training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we run the code, it returns the top seven reviews. Also, we only return
    the first 100 characters of each review for brevity:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s examine some important statistics about our training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When we run the code, it returns the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s plot the distributions of our sampled training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we call the `plot_reviews` function and pass in the total number of positive
    and negative reviews from our sampled training data. When we run the code, we
    get the plot shown in *Figure 10**.4*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4 – A distribution of reviews from our sampled training data](img/B18118_10_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – A distribution of reviews from our sampled training data
  prefs: []
  type: TYPE_NORMAL
- en: From the sampled training dataset, we see that our reviews are finely balanced.
    Therefore, we can proceed to train our model on this dataset. Let’s do that now.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the key parameters for the tokenization, sequencing, and training
    processes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We set our vocabulary size to 10,000\. This means the tokenizer will focus on
    the top 10,000 words in our dataset. When selecting the vocabulary size, there
    is a need to strike a balance between computational efficiency and capturing word
    diversity present within the dataset under consideration. If we increase the vocab
    size, we are likely to capture more nuances that can enrich our model’s understanding,
    but this will require more computational resources for training. Also, if we reduce
    the vocab size, training will be much faster; however, we will only capture a
    small portion of the linguistic variations present in our dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we set the embedding dimension to 16\. This means each word will be represented
    by a 16-dimensional vector. The choice of embedding dimension is usually based
    on empirical testing. Here, our choice of embedding dimension was based on computational
    efficiency. If we use higher dimensions such as 64 or 128, we are likely to capture
    more nuanced relationships between words; however, we will need more computational
    resources for training. When working with large datasets, you may wish to use
    higher dimensions for better performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We set our max length to 132 words; we use this based on the average word length
    we got during our exploration of the first 2,000 reviews in our data. Reviews
    longer than 132 words will be truncated after the first 132 words are selected.
    Our choice of maximum length is to ensure we strike a decent compromise between
    computational efficiency and capturing the most important aspects of most of the
    reviews in our dataset. We set truncation and `padding` to `post`; this ensures
    that longer sentences are cut off at the end of a sequence and shorter sentences
    are padded with zeros at the end of the sequence. The key assumption here is that
    most of the important information we will find in a customer’s review is likely
    to be found at the beginning part of the review.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we set the OOV token to cater to OOV words that may occur in the test
    set but which the model did not see during training. Setting this parameter prevents
    our model from running into errors when handling unseen words. We also set the
    number of epochs that our model will train for to 10\. Although we use 10 to test
    out our model, you may wish to train for longer and perhaps use callbacks to monitor
    the model’s performance during training on a validation set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With all our parameters defined, we can now instantiate our `Tokenizer` class,
    passing in `num_words` and `oov_token` as arguments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To reduce the processing time, we will make use of the 20,000 samples for training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we train our model with the first 20,000 samples from the Yelp Polarity
    training dataset. We collect these reviews and their corresponding labels, and
    since the data is in the form of bytes, we use UTF-8 encoding to decode the string,
    after which we append the text and their labels to their respective lists. We
    convert the list of labels for easy manipulation using NumPy. After this, we fit
    the tokenizer on our selected training data and convert the text to a sequence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For testing purposes, we take 8,000 samples. The set of steps we carry out
    here is quite similar to those on the training set; however, we do not fit on
    text on the test set. This step is only for training purposes to help the neural
    network learn the word-to-index mapping in the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE230]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We take the first 8,000 samples from our test dataset. It is important to use
    the same tokenizer that was used to fit our training data here. This ensures that
    the word index mapping learned by the tokenizer during training is applied to
    the test set, and words not learned in the training set are replaced with the
    OOV token.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The next step is to pad and truncate the sequences of integers representing
    the texts in the training and testing sets, ensuring that they all have the same
    length:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE243]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output returned, `train_padded` and `test_padded`, is NumPy arrays of shape
    (`num_sequences` and `maxlen`). Now, every sequence that makes up these arrays
    is of the same length.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We want to set up a validation set, which will help us track how our modeling
    process is going. To do this, we can use the `train_test_split` function from
    scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE244]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE245]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE246]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE247]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we split the data into training and validation sets, with 20 percent set
    as the validation set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s proceed to build our sentiment analysis model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE248]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE249]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE250]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE251]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE252]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE253]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE254]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE255]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE256]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We build our model with TensorFlow’s Keras API. Note that we have a new layer
    called the embedding layer, which is used to represent words in a dense vector
    space. This layer takes in the vocabulary size, the embedding dimension, and the
    max length as its parameters. In this experiment, we are training our word embeddings
    as a part of our model. We can also train this layer independently for the purpose
    of learning word embeddings. This can come in handy when we intend to use the
    same word embeddings across multiple models. In [*Chapter 11*](B18118_11.xhtml#_idTextAnchor267),
    *NLP with TensorFlow,* we will see how to apply a pretrained embedding layer from
    TensorFlow Hub.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When we pass in a two-dimensional tensor of shape (`batch_size`, `input_length`),
    where each sample is an integer sequence, the embedding layer returns a three-dimensional
    tensor of shape (`batch_size`, `input_length`, `embedding_dim`). At the start
    of training, embedding vectors are randomly initialized. As we train the model,
    these vectors are adjusted, ensuring words with a similar context are clustered
    closely together within the embedding space. Instead of using discrete values,
    word embeddings make use of continuous values that our model can use to discern
    patterns and model intricate relationships with the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `GlobalAveragePooling1D` layer is applied to reduce the dimensionality of
    our data; it applies an average pooling operation. For example, if we apply `GlobalAveragePooling1D`
    to a sequence of words, it will return a summarized, single vector as output that
    can be fed into our fully connected layers for classification. Because we are
    performing binary classification, we use one neuron in our output layer and a
    sigmoid as our activation function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we compile and fit our model. We pass in the loss as `binary_crossentropy`
    for the compilation step. We use Adam as our optimizer, and for our classification
    metric, we use accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE257]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE258]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE259]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We fit the model for 10 epochs using our training data (`train_padded`) and
    labels (`train_labels`) and use the validation data to track our experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE260]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE261]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE262]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE263]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We report the results from the last 5 epochs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE264]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE265]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE266]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE267]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE268]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE269]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE270]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE271]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE272]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE273]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model reaches an accuracy of 0.9786 on training and a validation accuracy
    of 0.8783\. This tells us there is an element of overfitting. Let’s see how our
    model will do on unseen data. To do this, let’s evaluate the model with our test
    data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We use the `evaluate` function to evaluate the trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE274]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE275]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE276]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE277]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We pass in the test data (`test_padded`) and test labels (`test_labels`) and
    print out the loss and accuracy. The model reached an accuracy of 0.8783 on the
    test set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is good practice to plot the loss and accuracy curves during training and
    validation, as it provides us with valuable insights into the learning process
    of the model and its performance. To do this, let’s construct a function called
    `plot_history`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE278]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE279]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE280]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE281]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE282]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE283]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE284]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE285]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE286]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE287]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE288]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE289]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE290]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE291]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE292]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE293]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE294]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE295]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE296]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE297]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE298]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE299]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function takes in the `history` object and returns to us both the loss
    and accuracy curves. The `plot_history` function will create a figure with two
    subplots – the subplot on the left shows the training and validation accuracy
    per epoch, and the subplot on the right shows the training and validation loss
    per epoch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.5 – The loss and accuracy curves](img/B18118_10_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – The loss and accuracy curves
  prefs: []
  type: TYPE_NORMAL
- en: From the plots in *Figure 10**.5*, we can see that the model’s accuracy on training
    increases per epoch; however, the validation accuracy begins to fall slightly
    around the end of the first epoch. The training loss also falls steadily per epoch,
    while the validation loss rises steadily over each epoch, thus indicating overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we explore ways to fix overfitting in our case study, let’s try four
    new sentences and see how our model fares on them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE300]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE301]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE302]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE303]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE304]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this example, we have the sentiments of each sentence indicated for reference.
    Let’s see how our trained model will perform on each of these new sentences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE305]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE306]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE307]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE308]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE309]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE310]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE311]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE312]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE313]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE314]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE315]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE316]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE317]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE318]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE319]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE320]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE321]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE322]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s print out the sequence corresponding to each sentence, along with the
    sentiment the model predicted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE323]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE324]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE325]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE326]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE327]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE328]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE329]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE330]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE331]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE332]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE333]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE334]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE335]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our sentiment analysis model was able to effectively predict the results correctly.
    What about if we want to visualize embedd[ings? TensorFlow has an embeddin](https://projector.tensorflow.org)g
    projector, which can be accessed at [https://projector.tensorflow.org](https://projector.tensorflow.org).
  prefs: []
  type: TYPE_NORMAL
- en: Embedding visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we wish to visualize word embeddings from our trained model, we will need
    to extract the learned embeddings from the embedding layer and load them into
    the embedding projector provided by TensorBoard. Let’s examine how we can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Extract the embedding layer weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE336]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE337]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE338]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE339]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE340]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The first step is to retrieve the learned weights from our embedding layer after
    training. Next, we obtain the word index mapping that was generated during the
    tokenization process. If we apply a `print` statement, we can see the vocabulary
    size and the embedding dimension.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, save the weights and vocabulary to disk. The TensorFlow Projector reads
    these file types and uses them to plot the vectors in 3D space, so we can visualize
    them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE341]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE342]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE343]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE344]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE345]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE346]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE347]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE348]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE349]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The next step is to save the embedding vectors and the vocabulary (words) as
    two separate `vecs.tsv` and `meta.tsv`, respectively. When we run this code block,
    we see that we have two new files in our Google Colab notebook, as shown in *Figure
    10**.6*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.6 – A screenshot showing the meta and vecs files](img/B18118_10_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – A screenshot showing the meta and vecs files
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the files locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE350]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE351]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE352]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE353]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE354]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE355]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To download the required files from Google Colab to our local machine, run this
    code block. Note that you need to move these files from your server to your local
    machine if you work in a cloud environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[Visualize the embeddings. Open](https://projector.tensorflow.org/) the embedding
    projector using this link: [https://projector.tensorflow.org/](https://projector.tensorflow.org/).
    Then, you will have to click on the load button to load the `vectors.tsv` and
    `metadata.tsv` files you downloaded to your local machine. Once you successfully
    upload the files, the word embeddings will appear in 3D, as illustrated in *Figure
    10**.7*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.7 – A snapshot of word embeddings](img/B18118_10_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – A snapshot of word embe[ddings](https://www.tensorflow.org/text/guide/word_embeddings?hl=en)
  prefs: []
  type: TYPE_NORMAL
- en: '[To learn more about embedding visualization, see th](https://www.tensorflow.org/text/guide/word_embeddings?hl=en)e
    documentation: [https://www.tensorflow.org/text/guide/word_embeddings?hl=en](https://www.tensorflow.org/text/guide/word_embeddings?hl=en).
    We have now seen how to visualize word embeddings. Now, let’s try to improve the
    performance of our model.'
  prefs: []
  type: TYPE_NORMAL
- en: Improving the performance of the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, we discussed some factors that we should consider as we designed our
    baseline architecture for sentiment analysis in this chapter. Also, in [*Chapter
    8*](B18118_08.xhtml#_idTextAnchor186), *Handling Overfitting,* we explored some
    foundational concepts to mitigate against overfitting. There, we looked at ideas
    such as early stopping and dropout regularization. To curb overfitting, let’s
    begin by tuning some of our model’s hyperparameters. To do this, let’s construct
    a function called `sentiment_model`. This function takes in three parameters –
    `vocab_size`, `embedding_dim`, and the size of the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the size of the vocabulary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One hyperparameter we may consider changing is the size of the vocabulary.
    Increasing the vocabulary size empowers the model to learn more unique words from
    our dataset. Let’s see how this will impact the performance of our base model.
    Here, we adjust `vocab_size` from `10000` to `20000`, while keeping the other
    hyperparameters constant:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE356]'
  prefs: []
  type: TYPE_PRE
- en: The model reaches a test accuracy of 0.8749 compared to 0.8783, which was achieved
    by our base model. Here, increasing `vocab_size` had no positive impact on the
    performance of our model.
  prefs: []
  type: TYPE_NORMAL
- en: When we use a larger vocabulary size, our model will learn more unique words,
    which could be a good idea, depending on the dataset and the use case. On the
    downside, more parameters and computational resource is required for us to efficiently
    train our model. Again, there is a greater risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In light of these issues, it is important to strike the right balance by ensuring
    we have a large enough `vocab_size` to capture the nuances in our data, without
    introducing the risk of overfitting at the same time. One strategy is to set a
    minimum frequency threshold, such that rare words that may lead to overfitting
    are excluded from our vocabulary. Another idea we can try is to adjust the dimensions
    of the embedding. Let’s discuss that next.
  prefs: []
  type: TYPE_NORMAL
- en: Adjusting the embedding dimension
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The embedding dimension refers to the size of the vector space in which words
    are represented. A high-dimension embedding has the ability to capture more nuanced
    relationships between words. However, it also increases the model complexity and
    may lead to overfitting, especially when working with small datasets. Let’s adjust
    `embedding_dim` from `16` to `32` while keeping other parameters constant and
    see what the impact will be on our experiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE357]'
  prefs: []
  type: TYPE_PRE
- en: In 10 epochs, our new model with a larger embedding dimension reached an accuracy
    of 0.8720 on the test set. This falls short of our baseline model. Here, we see
    firsthand that an increase in the embedding dimension doesn’t always guarantee
    a better-performing model. When the embedding dimension is too small, it may fail
    to capture important relationships in our data. Conversely, an oversized embedding
    will lead to increased computation requirements and a greater risk of overfitting.
    It is important to note that a small embedding dimension suffices for simpler
    tasks or smaller datasets, while a larger embedding is an excellent choice for
    a large dataset. A pragmatic approach is to begin with a smaller embedding and
    gradually increase its size, while keeping an eye on the model’s performance during
    each iteration. Usually, the performance will improve, and at some point, diminishing
    returns will set in. When this happens, we stop training. Now, we can collect
    more data, increase the number of samples, and see what our results look like.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting more data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186), *Handling Overfitting,*
    we explored this option when handling overfitting. Collecting more data samples
    enables us to have a more diverse set of examples that our model can learn from.
    However, this process can be time-consuming. Also, more data may not help in cases
    where it is noisy or irrelevant. For our case study, let’s increase the training
    data size from 20,000 samples to 40,000 samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE358]'
  prefs: []
  type: TYPE_PRE
- en: After 10 epochs of training, we see that our model’s performance improves to
    0.8726 on our test set. Collecting more data can be a good strategy, as it may
    provide our model with a more diverse dataset; however, it didn’t work in this
    instance. So, let’s move on to other ideas; this time, let’s try dropout regularization.
  prefs: []
  type: TYPE_NORMAL
- en: Dropout regularization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In [*Chapter 8*](B18118_08.xhtml#_idTextAnchor186), *Handling Overfitting,*
    we discussed dropout regularization, where we randomly dropped out a percentage
    of neurons from our model during training to break co-dependence among neurons
    in our model. Since we are dealing with a case of overfitting, let’s try out this
    technique. To implement dropout in our model, we can add a dropout layer, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE359]'
  prefs: []
  type: TYPE_PRE
- en: Here, we set our dropout rate to 50 percent, meaning we turn off half of the
    neurons during training. Our new model achieves an accuracy of 0.8830, which is
    marginally better than our baseline model. Dropout can help to enhance the robustness
    of our model by preventing co-dependence between neurons in it. However, we must
    apply dropout with caution. If we drop out too many neurons, our model becomes
    too simple and begins to underfit because it is unable to capture the underlying
    patterns in our data. Also, if we apply a low dropout value to our model, we may
    not achieve the desired regularization effect we hope for. It is a good idea to
    experiment with different dropout values to find the best balance between model
    complexity and generalization. Now, let’s try out different optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Trying a different optimizer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While Adam is a good general-purpose optimizer, you might find that a different
    optimizer, such as SGD or RMSprop, works better for your specific task. Different
    optimizers might work better, depending on the task at hand. Let’s try out RMSprop
    for our use case and see how it fares:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE360]'
  prefs: []
  type: TYPE_PRE
- en: We achieved a test accuracy of 0.8920, beating our baseline model, using RMSprop
    as our optimizer. When choosing an appropriate optimizer, it is important to assess
    the key properties of your use case. For example, when working with large datasets,
    SGD is a more suitable choice than batch gradient descent, as SGD’s use of mini-batches
    reduces the computational cost. This attribute is useful when working with limited
    computation resources. It’s worth noting that if we have too many small batches
    while using SGD, it could lead to noisy updates; on the flip side, very large
    batch sizes could increase the computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: Adam is an excellent default optimizer for many deep learning use cases, as
    it combines the advantages of RMSprop and Momentum; however, when we deal with
    simple convex problems such as linear regression, SGD proves to be a better choice,
    due to Adam’s overcompensation in these scenarios. With this, we draw the curtain
    on this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the foundations of NLP. We began by looking at
    how to handle real-world text data, and we explored some preprocessing ideas,
    using tools such as Beautiful Soup, requests, and regular expressions. Then, we
    unpacked various ideas, such as tokenization, sequencing, and the use of word
    embedding to transform text data into vector representations, which not only preserved
    the sequential order of text data but also captured the relationships between
    words. We took a step further by building a sentiment analysis classifier using
    the Yelp Polarity dataset from the TensorFlow dataset. Finally, we performed a
    series of experiments with different hyperparameters in a bid to improve our base
    model’s performance and overcome overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce **Recurrent Neural Networks** (**RNNs**)
    and see how they do things differently from the DNN we used in this chapter. We
    will put RNNs to the test as we will build a new classifier with them. We will
    also take things a step further by experimenting with pretrained embeddings, and
    finally, we will round off the NLP section by generating text in a fun exercise,
    using a dataset of children's stories. See you there.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s test what we have learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using the test notebook, load the IMDB dataset from TFDS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a different embedding dimension and evaluate the model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a different vocabulary size and evaluate the model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add more layers and evaluate the model on the test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use your best model to make predictions on the sample sentences given.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, you can check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Kapoor, A., Gulli, A. and Pal, S. (2020) *Deep Learning with TensorFlow and
    Keras, Third Edition*. Packt Publishing Ltd.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Twitter Sentiment Classification using Distant Supervision* by Go et al. (2009)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Embedding Projector: Interactive Visualization and Interpretation of Embeddings*
    by Smilkov et al. (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural
    Networks for Sentence Classification* by Zhang et al. (2016)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
