["```\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nimport numpy as np\nimport pandas as pd\nimdb_train, ds_info = tfds.load(name=\"imdb_reviews\",\n                      split=\"train\", \n                      with_info=True, as_supervised=True)\nimdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", \n                      as_supervised=True) \n```", "```\n# Use the default tokenizer settings\ntokenizer = tfds.features.text.Tokenizer()\nvocabulary_set = set()\nMAX_TOKENS = 0\nfor example, label in imdb_train:\n  some_tokens = tokenizer.tokenize(example.numpy())\n  if MAX_TOKENS < len(some_tokens):\n            MAX_TOKENS = len(some_tokens)\n  vocabulary_set.update(some_tokens) \n```", "```\nimdb_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set,\n                                                   **lowercase=****True**,\n                                                   tokenizer=tokenizer)\nvocab_size = imdb_encoder.vocab_size\nprint(vocab_size, MAX_TOKENS) \n```", "```\n93931 2525 \n```", "```\n# transformation functions to be used with the dataset\nfrom tensorflow.keras.preprocessing import sequence\ndef encode_pad_transform(sample):\n    encoded = imdb_encoder.encode(sample.numpy())\n    pad = sequence.pad_sequences([encoded], padding='post', \n                                 maxlen=150)\n    return np.array(pad[0], dtype=np.int64)  \ndef encode_tf_fn(sample, label):\n    encoded = tf.py_function(encode_pad_transform, \n                                       inp=[sample], \n                                       Tout=(tf.int64))\n    encoded.set_shape([None])\n    label.set_shape([])\n    return encoded, label \n```", "```\nencoded_train = imdb_train.map(encode_tf_fn,\n                      num_parallel_calls=tf.data.experimental.AUTOTUNE)\nencoded_test = imdb_test.map(encode_tf_fn,\n                      num_parallel_calls=tf.data.experimental.AUTOTUNE) \n```", "```\n# Download the GloVe embeddings\n!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip\nArchive:  glove.6B.zip\n  inflating: glove.6B.50d.txt        \n  inflating: glove.6B.100d.txt       \n  inflating: glove.6B.200d.txt       \n  inflating: glove.6B.300d.txt \n```", "```\ndict_w2v = {}\nwith open('glove.6B.50d.txt', \"r\") as file:\n    for line in file:\n        tokens = line.split()\n        word = tokens[0]\n        vector = np.array(tokens[1:], dtype=np.float32)\n        if vector.shape[0] == 50:\n            dict_w2v[word] = vector\n        else:\n            print(\"There was an issue with \" + word)\n# let's check the vocabulary size\nprint(\"Dictionary Size: \", len(dict_w2v)) \n```", "```\nDictionary Size:  400000 \n```", "```\nembedding_dim = 50\nembedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim)) \n```", "```\nunk_cnt = 0\nunk_set = set()\nfor word in imdb_encoder.tokens:\n    embedding_vector = dict_w2v.get(word)\n    if embedding_vector is not None:\n        tkn_id = imdb_encoder.encode(word)[0]\n        embedding_matrix[tkn_id] = embedding_vector\n    else:\n        unk_cnt += 1\n        unk_set.add(word)\n# Print how many weren't found\nprint(\"Total unknown words: \", unk_cnt) \n```", "```\nTotal unknown words:  14553 \n```", "```\n# Length of the vocabulary in chars\nvocab_size = imdb_encoder.vocab_size # len(chars)\n# Number of RNN units\nrnn_units = 64\n# batch size\nBATCH_SIZE=100 \n```", "```\nfrom tensorflow.keras.layers import Embedding, LSTM, \\\n                                    Bidirectional, Dense\n\ndef build_model_bilstm(vocab_size, embedding_dim, \n                       rnn_units, batch_size, **train_emb=****False**):\n  model = tf.keras.Sequential([\n    Embedding(vocab_size, embedding_dim, mask_zero=True,\n              **weights=[embedding_matrix], trainable=train_emb**),\n   Bidirectional(LSTM(rnn_units, return_sequences=True, \n                                      dropout=0.5)),\n   Bidirectional(LSTM(rnn_units, dropout=0.25)),\n   Dense(1, activation='sigmoid')\n  ])\n  return model \n```", "```\nmodel_fe = build_model_bilstm(\n  vocab_size = vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE)\nmodel_fe.summary() \n```", "```\nModel: \"sequential_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_5 (Embedding)      (None, None, 50)          4696550   \n_________________________________________________________________\nbidirectional_6 (Bidirection (None, None, 128)         58880     \n_________________________________________________________________\nbidirectional_7 (Bidirection (None, 128)               98816     \n_________________________________________________________________\ndense_5 (Dense)              (None, 1)                 129       \n=================================================================\nTotal params: 4,854,375\nTrainable params: 157,825\nNon-trainable params: 4,696,550\n_________________________________________________________________ \n```", "```\nmodel_fe.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy', 'Precision', 'Recall']) \n```", "```\n# Prefetch for performance\nencoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)\nmodel_fe.fit(encoded_train_batched, epochs=10) \n```", "```\nEpoch 1/10\n250/250 [==============================] - 28s 113ms/step - loss: 0.5896 - accuracy: 0.6841 - Precision: 0.6831 - Recall: 0.6870\nEpoch 2/10\n250/250 [==============================] - 17s 70ms/step - loss: 0.5160 - accuracy: 0.7448 - Precision: 0.7496 - Recall: 0.7354\n...\nEpoch 9/10\n250/250 [==============================] - 17s 70ms/step - loss: 0.4108 - accuracy: 0.8121 - Precision: 0.8126 - Recall: 0.8112\nEpoch 10/10\n250/250 [==============================] - 17s 70ms/step - loss: 0.4061 - accuracy: 0.8136 - Precision: 0.8147 - Recall: 0.8118 \n```", "```\nmodel_fe.evaluate(encoded_test.batch(BATCH_SIZE)) \n```", "```\n250/Unknown - 21s 85ms/step - loss: 0.3999 - accuracy: 0.8282 - Precision: 0.7845 - Recall: 0.9050 \n```", "```\nmodel_ft = build_model_bilstm(\n  vocab_size=vocab_size,\n  embedding_dim=embedding_dim,\n  rnn_units=rnn_units,\n  batch_size=BATCH_SIZE,\n  train_emb=True)\nmodel_ft.summary() \n```", "```\nmodel_ft.compile(loss='binary_crossentropy', \n             optimizer='adam', \n             metrics=['accuracy', 'Precision', 'Recall'])\nmodel_ft.fit(encoded_train_batched, epochs=10) \n```", "```\nEpoch 1/10\n250/250 [==============================] - 35s 139ms/step - loss: 0.5432 - accuracy: 0.7140 - Precision: 0.7153 - Recall: 0.7111\nEpoch 2/10\n250/250 [==============================] - 24s 96ms/step - loss: 0.3942 - accuracy: 0.8234 - Precision: 0.8274 - Recall: 0.8171\n...\nEpoch 9/10\n250/250 [==============================] - 24s 97ms/step - loss: 0.1303 - accuracy: 0.9521 - Precision: 0.9530 - Recall: 0.9511\nEpoch 10/10\n250/250 [==============================] - 24s 96ms/step - loss: 0.1132 - accuracy: 0.9580 - Precision: 0.9583 - Recall: 0.9576 \n```", "```\nmodel_ft.evaluate(encoded_test.batch(BATCH_SIZE)) \n```", "```\n250/Unknown - 22s 87ms/step - loss: 0.4624 - accuracy: 0.8710 - Precision: 0.8789 - Recall: 0.8605 \n```", "```\nThis was an absolutely terrible movie. Don't be `lured` in by Christopher Walken or Michael Ironside. \n```", "```\n[CLS] This was an absolutely terrible movie . Don' t be `lure ##d` in by Christopher Walk ##en or Michael Iron ##side . [SEP] \n```", "```\n!pip install transformers==3.0.2 \n```", "```\nfrom transformers import BertTokenizer\nbert_name = 'bert-base-cased'\ntokenizer = BertTokenizer.from_pretrained(bert_name, \n                                          add_special_tokens=True, \n                                          do_lower_case=False,\n                                          max_length=150,\n                                          pad_to_max_length=True) \n```", "```\ntokenizer.encode_plus(\" Don't be lured\", add_special_tokens=True, \n                      max_length=9,\n                      pad_to_max_length=True, \n                      return_attention_mask=True, \n                      return_token_type_ids=True) \n```", "```\n{'input_ids': [101, 1790, 112, 189, 1129, 19615, 1181, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0]} \n```", "```\ntokenizer.encode_plus(\" Don't be\",\" lured\", add_special_tokens=True, \n                      max_length=10,\n                      pad_to_max_length=True, \n                      return_attention_mask=True, \n                      return_token_type_ids=True) \n```", "```\n{'input_ids': [101, 1790, 112, 189, 1129, **102**, 19615, 1181, **102**, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, **1, 1, 1**, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]} \n```", "```\ndef bert_encoder(review):\n    txt = review.numpy().decode('utf-8')\n    encoded = tokenizer.encode_plus(txt, add_special_tokens=True, \n                                    max_length=150, \n                                    pad_to_max_length=True, \n                                    return_attention_mask=True, \n                                    return_token_type_ids=True)\n    return encoded['input_ids'], encoded['token_type_ids'], \\\n           encoded['attention_mask'] \n```", "```\nbert_train = [bert_encoder(r) for r, l in imdb_train]\nbert_lbl = [l for r, l in imdb_train]\nbert_train = np.array(bert_train)\nbert_lbl = tf.keras.utils.to_categorical(bert_lbl, num_classes=2) \n```", "```\n# create training and validation splits\nfrom sklearn.model_selection import train_test_split\nx_train, x_val, y_train, y_val = train_test_split(bert_train, \n                                         bert_lbl,\n                                         test_size=0.2, \n                                         random_state=42)\nprint(x_train.shape, y_train.shape) \n```", "```\n(20000, 3, 150) (20000, 2) \n```", "```\ntr_reviews, tr_segments, tr_masks = np.split(x_train, 3, axis=1)\nval_reviews, val_segments, val_masks = np.split(x_val, 3, axis=1)\ntr_reviews = tr_reviews.squeeze()\ntr_segments = tr_segments.squeeze()\ntr_masks = tr_masks.squeeze()\nval_reviews = val_reviews.squeeze()\nval_segments = val_segments.squeeze()\nval_masks = val_masks.squeeze() \n```", "```\ndef example_to_features(input_ids,attention_masks,token_type_ids,y):\n  return {\"input_ids\": input_ids,\n          \"attention_mask\": attention_masks,\n          \"token_type_ids\": token_type_ids},y\ntrain_ds = tf.data.Dataset.from_tensor_slices((tr_reviews, \ntr_masks, tr_segments, y_train)).\\\n            map(example_to_features).shuffle(100).batch(16)\nvalid_ds = tf.data.Dataset.from_tensor_slices((val_reviews, \nval_masks, val_segments, y_val)).\\\n            map(example_to_features).shuffle(100).batch(16) \n```", "```\nfrom transformers import TFBertForSequenceClassification\nbert_model = TFBertForSequenceClassification.from_pretrained(bert_name) \n```", "```\noptimizer = tf.keras.optimizers.Aadam(learning_rate=2e-5)\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\nbert_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) \n```", "```\nbert_model.summary() \n```", "```\nModel: \"tf_bert_for_sequence_classification_7\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nbert (TFBertMainLayer)       multiple                  108310272 \n_________________________________________________________________\ndropout_303 (Dropout)        multiple                  0         \n_________________________________________________________________\nclassifier (Dense)           multiple                  1538      \n=================================================================\nTotal params: 108,311,810\nTrainable params: 108,311,810\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\nprint(\"Fine-tuning BERT on IMDB\")\nbert_history = bert_model.fit(train_ds, epochs=3, \n                              validation_data=valid_ds) \n```", "```\nFine-tuning BERT on IMDB\nTrain for 1250 steps, validate for 313 steps\nEpoch 1/3\n1250/1250 [==============================] - 480s 384ms/step - loss: 0.3567 - accuracy: 0.8320 - val_loss: 0.2654 - val_accuracy: 0.8813\nEpoch 2/3\n1250/1250 [==============================] - 469s 375ms/step - loss: 0.2009 - accuracy: 0.9188 - val_loss: 0.3571 - val_accuracy: 0.8576\nEpoch 3/3\n1250/1250 [==============================] - 470s 376ms/step - loss: 0.1056 - accuracy: 0.9613 - val_loss: 0.3387 - val_accuracy: 0.8883 \n```", "```\n# prep data for testing\nbert_test = [bert_encoder(r) for r,l in imdb_test]\nbert_tst_lbl = [l for r, l in imdb_test]\nbert_test2 = np.array(bert_test)\nbert_tst_lbl2 = tf.keras.utils.to_categorical (bert_tst_lbl,                                                num_classes=2)\nts_reviews, ts_segments, ts_masks = np.split(bert_test2, 3, axis=1)\nts_reviews = ts_reviews.squeeze()\nts_segments = ts_segments.squeeze()\nts_masks = ts_masks.squeeze()\ntest_ds = tf.data.Dataset.from_tensor_slices((ts_reviews, \n                    ts_masks, ts_segments, bert_tst_lbl2)).\\\n            map(example_to_features).shuffle(100).batch(16) \n```", "```\nbert_model.evaluate(test_ds) \n```", "```\n1563/1563 [==============================] - 202s 129ms/step - loss: 0.3647 - accuracy: 0.8799\n[0.3646871318983454, 0.8799] \n```", "```\nfrom transformers import TFBertModel\nbert_name = 'bert-base-cased'\nbert = TFBertModel.from_pretrained(bert_name) \nbert.summary() \n```", "```\nModel: \"tf_bert_model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nbert (TFBertMainLayer)       multiple                  108310272 \n=================================================================\nTotal params: 108,310,272\nTrainable params: 108,310,272\nNon-trainable params: 0\n_________________________________________________________________ \n```", "```\nmax_seq_len = 150\ninp_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"input_ids\")\natt_mask = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"attention_mask\")\nseg_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=\"token_type_ids\") \n```", "```\ntrain_ds.element_spec \n```", "```\n({'input_ids': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None),\n  'attention_mask': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None),\n  'token_type_ids': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None)},\n TensorSpec(shape=(None, 2), dtype=tf.float32, name=None)) \n```", "```\ninp_dict = {\"input_ids\": inp_ids,\n            \"attention_mask\": att_mask,\n            \"token_type_ids\": seg_ids}\noutputs = bert(inp_dict)\n# let's see the output structure\noutputs \n```", "```\n(<tf.Tensor 'tf_bert_model_3/Identity:0' shape=(None, 150, 768) dtype=float32>,\n <tf.Tensor 'tf_bert_model_3/Identity_1:0' shape=(None, 768) dtype=float32>) \n```", "```\nx = tf.keras.layers.Dropout(0.2)(outputs[1])\nx = tf.keras.layers.Dense(200, activation='relu')(x)\nx = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense(2, activation='sigmoid')(x)\ncustom_model = tf.keras.models.Model(inputs=inp_dict, outputs=x) \n```", "```\noptimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\nloss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\ncustom_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) \n```", "```\ncustom_model.summary() \n```", "```\nprint(\"Custom Model: Fine-tuning BERT on IMDB\")\ncustom_history = custom_model.fit(train_ds, epochs=3, \n                                  validation_data=valid_ds) \n```", "```\nCustom Model: Fine-tuning BERT on IMDB\nTrain for 1250 steps, validate for 313 steps\nEpoch 1/3\n1250/1250 [==============================] - 477s 381ms/step - loss: 0.5912 - accuracy: 0.8069 - val_loss: 0.6009 - val_accuracy: 0.8020\nEpoch 2/3\n1250/1250 [==============================] - 469s 375ms/step - loss: 0.5696 - accuracy: 0.8570 - val_loss: 0.5643 - val_accuracy: 0.8646\nEpoch 3/3\n1250/1250 [==============================] - 470s 376ms/step - loss: 0.5559 - accuracy: 0.8883 - val_loss: 0.5647 - val_accuracy: 0.8669 \n```", "```\ncustom_model.evaluate(test_ds) \n```", "```\n1563/1563 [==============================] - 201s 128ms/step - loss: 0.5667 - accuracy: 0.8629 \n```", "```\nbert.trainable = False                  # don't train BERT any more\noptimizer = tf.keras.optimizers.Adam()  # standard learning rate\ncustom_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy']) \n```", "```\ncustom_model.summary() \n```", "```\nprint(\"Custom Model: Keep training custom model on IMDB\")\ncustom_history = custom_model.fit(train_ds, epochs=10, \n                                  validation_data=valid_ds) \n```", "```\ncustom_model.evaluate(test_ds) \n```", "```\n1563/1563 [==============================] - 195s 125ms/step - loss: 0.5657 - accuracy: 0.8696 \n```"]