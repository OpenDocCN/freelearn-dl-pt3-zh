- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter introduces **Reinforcement Learning** (**RL**)—the least explored
    and yet most promising learning paradigm. Reinforcement learning is very different
    from the supervised and unsupervised learning models we covered in earlier chapters.
    Starting from a clean slate (that is, having no prior information), the RL agent
    can go through multiple stages of trial and error, and learn to achieve a goal,
    all the while the only input being the feedback from the environment. The research
    in RL by OpenAI seems to suggest that continuous competition can be a cause for
    the evolution of intelligence. Many deep learning practitioners believe that RL
    will play an important role in the big AI dream: **Artificial General Intelligence**
    (**AGI**). This chapter will delve into different RL algorithms. The following
    topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: What RL is and its lingo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn how to use the OpenAI Gym interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Q-Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp11](https://packt.link/dltfchp11).
  prefs: []
  type: TYPE_NORMAL
- en: An introduction to RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'What is common between a baby learning to walk, birds learning to fly, and
    an RL agent learning to play an Atari game? Well, all three involve:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Trial and error**: The child (or the bird) tries various ways, fails many
    times, and succeeds in some ways before it can really walk (or fly). The RL agent
    plays many games, winning some and losing many, before it can become reliably
    successful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Goal**: The child has the goal to walk, the bird to fly, and the RL agent
    to win the game.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interaction with the environment**: The only feedback they have is from their
    environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, the first questions that arise are what is RL, and how is it different from
    supervised and unsupervised learning? Anyone who owns a pet knows that the best
    strategy to train a pet is rewarding it for desirable behavior and disciplining
    it for bad behavior. RL, also called **learning with a critic**, is a learning
    paradigm where the agent learns in the same manner. The agent here corresponds
    to our network (program); it can perform a set of **actions** (**a**), which brings
    about a change in the **state** (**s**) of the environment, and, in turn, the
    agent receives a reward or punishment from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the case of training a dog to fetch a ball: here, the
    dog is our agent, the voluntary muscle movements that the dog makes are the actions,
    and the ground (as well as the person and ball) is the environment; the dog perceives
    our reaction to its action in terms of giving it a treat as a reward. RL can be
    defined as a computational approach to goal-directed learning and decision making,
    from interaction with the environment, under some idealized conditions. The agent
    can sense the state of the environment, and the agent can perform specific well-defined
    actions on the environment. This causes two things: first, a change in the state
    of the environment, and second, a reward is generated (under ideal conditions).
    This cycle continues, and in theory the agent learns how to more frequently generate
    a reward over time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B18331_11_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.1: Reinforcement learning: interaction between agent and environment'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike supervised learning, the agent is not presented with any training examples;
    it does not know what the correct action is.
  prefs: []
  type: TYPE_NORMAL
- en: And unlike unsupervised learning, the agent’s goal is not to find some inherent
    structure in the input (the learning may find some structure, but that isn’t the
    goal); instead, its only goal is to maximize the rewards (in the long run) and
    reduce the punishments.
  prefs: []
  type: TYPE_NORMAL
- en: RL lingo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before learning various RL algorithms, it is important we understand a few
    important terms. We will illustrate the terms with the help of two examples, first
    a robot in a maze, and second an agent controlling the wheels of a **Self-Driving
    Car** (**SDC**). The two RL agents are shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing diagram  Description automatically generated](img/B18331_11_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.2: State for a robot trying to find a path in a maze (LHS). State
    for an agent trying to control the steering wheel of a self-driving car (RHS)'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11.2* shows the two examples we will be considering. Let us start with
    the terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**State**, *S*: State is the set of tokens (or representations) that can define
    all of the possible states the environment can be in. It can be continuous or
    discrete. In the case of the robot finding its path through a maze, the state
    can be represented by a 4×4 matrix, with elements indicating whether that block
    is empty, occupied, or blocked. A block with a value of 1 means it is occupied
    by the robot, 0 means it is empty, and *X* represents that the block is impassable.
    Each element in this array, *S*, can have one of these three discrete values,
    so the state is discrete in nature. Next, consider the agent controlling the steering
    wheel of a self-driving car. The agent takes as input the front-view image. The
    image contains continuous valued pixels, so here the state is continuous.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Action**, *A(S)*: Actions are the set of all possible things that the agent
    can do in a particular state. The set of possible actions, *A*, depends on the
    present state, *S*. Actions may or may not result in a change of state. Like states,
    they can be discrete or continuous. The robot finding a path in the maze can perform
    five discrete actions [**up**, **down**, **left**, **right**, **no change**].
    The SDC agent, on the other hand, can rotate the steering wheel at a continuous
    range of angles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward** *R(S,A,S’)*: Rewards are a scalar value returned by the environment
    based on the agent’s action(s). Here *S* is the present state and *S’* is the
    state of the environment after action *A* is taken. It is determined by the goal;
    the agent gets a higher reward if the action brings it near the goal, and a low
    (or even negative) reward otherwise. How we define a reward is totally up to us—in
    the case of the maze, we can define the reward as the Euclidean distance between
    the agent’s current position and goal. The SDC agent reward can be that the car
    is on the road (positive reward) or off the road (negative reward).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Policy** ![](img/B18331_11_001.png): Policy defines a mapping between each
    state and the action to take in that state. The policy can be *deterministic*,
    that is, for each state, there is a well-defined policy. In the case of the maze
    robot, a policy can be that if the top block is empty, move up. The policy can
    also be *stochastic*, that is, where an action is taken by some probability. It
    can be implemented as a simple look-up table, or it can be a function dependent
    on the present state. The policy is the core of the RL agent. In this chapter,
    we’ll learn about different algorithms that help the agent to learn the policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Return** *G*[t]: This is the discounted sum of all future rewards starting
    from the current time, mathematically defined as:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18331_11_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Here *R*[t] is the reward at time *t* and ![](img/B18331_11_003.png) is the
    discount factor; its value lies between 0 and 1\. The discount factor determines
    how important future rewards are in deciding the policy. If it is near zero, the
    agent gives importance to the immediate rewards. A high discount factor, however,
    means the agent is looking far into the future. It may give up immediate reward
    in favor of high future rewards, just as in the game chess, you may sacrifice
    a pawn to later checkmate the opponent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value function** *V(S)*: This defines the “goodness” of a state in the long
    run. It can be thought of as the total amount of reward the agent can expect to
    accumulate over time, starting from the state, *S*. You can think of it as long-term
    good, as opposed to an immediate but short-lived good. What do you think is more
    important, maximizing the immediate reward or the value function? You probably
    guessed right: just as in chess, we sometimes lose a pawn to win the game a few
    steps later, and so the agent should try to maximize the value function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normally, the value is defined either as the **state-value function** ![](img/B18331_11_004.png)
    or the **action-value function** ![](img/B18331_11_005.png), where ![](img/B18331_11_006.png)
    is the policy followed. The state-value function is the expected return from the
    state *S* after following policy ![](img/B18331_11_006.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18331_11_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here *E* is the expectation, and *S*[t]*=s* is the state at time *t*. The action-value
    function is the expected return from the state *S*, taking an action *A=a* and
    following the policy ![](img/B18331_11_006.png):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B18331_11_010.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Model of the environment**: This is an optional element. It mimics the behavior
    of the environment, and it contains the physics of the environment; in other words,
    it indicates how the environment will behave. The model of the environment is
    defined by the transition probability to the next state. This is an optional component;
    we can have **model-free** reinforcement learning as well where the transition
    probability is not needed to define the RL process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In RL, we assume that the state of the environment follows the **Markov property**,
    that is, each state is dependent solely on the preceding state, the action taken
    from the action space, and the corresponding reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, if *S*^t^(+1) is the state of the environment at time *t+1*, then
    it is a function of *S*^t state at time *t*, *A*^t is the action taken at time
    *t*, and *R*^t is the corresponding reward received at time *t*, no prior history
    is needed. If *P(S*^t^(+1)|*S*^t*)* is the transition probability, mathematically
    the Markov property can be written as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_011.png)'
  prefs: []
  type: TYPE_IMG
- en: And thus, RL can be assumed to be a **Markov Decision Process** (**MDP**).
  prefs: []
  type: TYPE_NORMAL
- en: Deep reinforcement learning algorithms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The basic idea of **Deep Reinforcement Learning** (**DRL**) is that we can
    use a deep neural network to approximate either the policy function or the value
    function. In this chapter, we will be studying some popular DRL algorithms. These
    algorithms can be classified into two classes, depending upon what they approximate:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Value-based methods**: In these methods, the algorithms take the action that
    maximizes the value function. The agent here learns to predict how good a given
    state or action would be. An example of the value-based method is the Deep Q-Network.
    Consider, for example, our robot in a maze: assuming that the value of each state
    is the negative of the number of steps needed to go from that box to the goal,
    then, at each time step, the agent will choose the action that takes it to a state
    with optimal value, as in the following diagram. So, starting from a value of
    **-6**, it’ll move to **-5**, **-4**, **-3**, **-2**, **-1**, and eventually reach
    the goal with the value **0**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18331_11_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.3: Demo value function values for the maze-finding robot'
  prefs: []
  type: TYPE_NORMAL
- en: '**Policy-based methods**: In these methods, the algorithms predict the optimal
    policy (the one that maximizes the expected return), without maintaining the value
    function estimates. The aim is to find the optimal policy, instead of the optimal
    action. An example of the policy-based method is policy gradients. Here, we approximate
    the policy function, which allows us to map each state to the best corresponding
    action. One advantage of policy-based methods over value-based is that we can
    use them even for continuous action spaces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides the algorithms approximating either policy or value, there are a few
    questions we need to answer to make reinforcement learning work.
  prefs: []
  type: TYPE_NORMAL
- en: How does the agent choose its actions, especially when untrained?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When the agent starts learning, it has no idea what the best way in which to
    determine an action is, or which action will provide the best *Q* value. So how
    do we go about it? We take a leaf out of nature’s book. Like bees and ants, the
    agent makes a balance between exploring new actions and exploiting learned ones.
    Initially, when the agent starts, it has no idea which action among the possible
    actions is better, so it makes random choices, but as it learns, it starts making
    use of the learned policy. This is called the **exploration vs exploitation**
    [2] tradeoff. Using exploration, the agent gathers more information, and later
    exploits the gathered information to make the best decision.
  prefs: []
  type: TYPE_NORMAL
- en: How does the agent maintain a balance between exploration and exploitation?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are various strategies; one of the most employed is the **epsilon-greedy**
    (![](img/B18331_11_012.png)) policy. Here, the agent explores unceasingly, and
    depending upon the value of ![](img/B18331_11_013.png), at each step the agent
    selects a random action with probability ![](img/B18331_11_014.png), and with
    probability ![](img/B18331_11_015.png) selects an action that maximizes the value
    function. Normally, the value of ![](img/B18331_11_014.png) decreases asymptotically.
    In Python the ![](img/B18331_11_012.png) policy can be implemented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: where `model` is the deep neural network approximating the value/policy function,
    `a` is the action chosen from the action space of size `action_size`, and `s`
    is the state. Another way to perform exploration is to use noise; researchers
    have experimented with both Gaussian and Ornstein-Uhlenbeck noise with success.
  prefs: []
  type: TYPE_NORMAL
- en: How to deal with the highly correlated input state space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The input to our RL model is the present state of the environment. Each action
    results in some change in the environment; however, the correlation between two
    consecutive states is very high. Now if we make our network learn based on the
    sequential states, the high correlation between consecutive inputs results in
    what is known as **catastrophic forgetting**. To mitigate the effect of catastrophic
    forgetting, in 2018, David Isele and Akansel Cosgun proposed the **experience
    replay** method.
  prefs: []
  type: TYPE_NORMAL
- en: 'In simplest terms, the learning algorithm first stores the MDP tuple—state,
    action, reward, and next state *<S, A, R, S’>*—in a buffer/memory. Once a significant
    amount of memory is built, a batch is selected randomly to train the agent. The
    memory is continuously refreshed with new additions and old deletions. The use
    of experience replay provides three benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it allows the same experience to be potentially used in many weight updates,
    hence increasing data efficiency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, the random selection of batches of experience removes the correlations
    between consecutive states presented to the network for training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Third, it stops any unwanted feedback loops that may arise and cause the network
    to get stuck in local minima or diverge.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A modified version of experience replay is the **Prioritized Experience Replay**
    (**PER**). Introduced in 2015 by Tom Schaul et al. [4], it derives from the idea
    that not all experiences (or, you might say, attempts) are equally important.
    Some attempts are better lessons than others. Thus, instead of selecting the experiences
    randomly, it will be much more efficient to assign higher priority to more educational
    experiences in selection for training. In the Schaul paper, it was proposed that
    experiences in which the difference between the prediction and target is high
    should be given priority, as the agent could learn a lot in these cases.
  prefs: []
  type: TYPE_NORMAL
- en: How to deal with the problem of moving targets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike supervised learning, the target is not previously known in RL. With a
    moving target, the agent tries to maximize the expected return, but the maximum
    value goes on changing as the agent learns. In essence, this is like trying to
    catch a butterfly yet each time you approach it, it moves to a new location. The
    major reason to have a moving target is that the same networks are used to estimate
    the action and the target values, and this can cause oscillations in learning.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to this was proposed by the DeepMind team in their 2015 paper, titled
    *Human-level Control through Deep Reinforcement Learning*, published in Nature.
    The solution is that now, instead of a moving target, the agent has short-term
    fixed targets. The agent now maintains two networks, both are exactly the same
    in architecture, one called the local network, which is used at each step to estimate
    the present action, and one the target network, which is used to get the target
    value. However, both networks have their own set of weights. At each time step,
    the local network learns in the direction such that its estimate and target are
    near to each other. After some number of time steps, the target network weights
    are updated. The update can be a **hard update**, where the weights of the local
    network are copied completely to the target network after *N* time steps, or it
    can be a **soft update**, in which the target network slowly (by a factor of Tau
    ![](img/B18331_11_018.png)) moves its weight toward the local network.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement success in recent years
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the last few years, DRL has been successfully used in a variety of tasks,
    especially in game playing and robotics. Let us acquaint ourselves with some success
    stories of RL before learning its algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**AlphaGo Zero**: Developed by Google’s DeepMind team, the AlphaGo Zero paper
    *Mastering the game of Go without any human knowledge* starts from an absolutely
    blank slate (**tabula rasa**). The AlphaGo Zero uses one neural network to approximate
    both the move probabilities and value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This neural network takes as an input the raw board representation. It uses
    a Monte Carlo tree search guided by the neural network to select the moves. The
    reinforcement learning algorithm incorporates a look-ahead search inside the training
    loop. It was trained for 40 days using a 40-block residual CNN and, over the course
    of training, it played about 29 million games (a big number!). The neural network
    was optimized on Google Cloud using TensorFlow, with 64 GPU workers and 19 CPU
    parameter servers. You can access the paper here: [https://www.nature.com/articles/nature24270](https://www.nature.com/articles/nature24270).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AI-controlled sailplanes**: Microsoft has developed a controller system that
    can run on many different autopilot hardware platforms, such as Pixhawk and Raspberry
    Pi 3\. It can keep the sailplane in the air without using a motor, by autonomously
    finding and catching rides on naturally occurring thermals. The controller helps
    the sailplane to operate on its own by detecting and using these thermals to travel
    without the aid of a motor or a person. They implemented it as a partially observable
    Markov decision process. They employed Bayesian reinforcement learning and used
    the Monte Carlo tree search to search for the best action. They’ve divided the
    whole system into level planners—a high-level planner that makes a decision based
    on experience and a low-level planner that uses Bayesian reinforcement learning
    to detect and latch onto thermals in real time. You can see the sailplane in action
    at Microsoft News: [https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/](https://news.microsoft.com/features/science-mimics-nature-microsoft-researchers-test-ai-controlled-soaring-machine/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locomotion behavior**: In the paper *Emergence of Locomotion Behaviours in
    Rich Environments* ([https://arxiv.org/pdf/1707.02286.pdf](https://arxiv.org/pdf/1707.02286.pdf)),
    DeepMind researchers provided the agents with rich and diverse environments. The
    environments presented a spectrum of challenges at different levels of difficulty.
    The agent was provided with difficulties in increasing order; this led the agent
    to learn sophisticated locomotion skills without performing any reward engineering
    (that is, designing special reward functions).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data center cooling using reinforcement learning**: Data centers are workhorses
    of the present digital/internet revolution. With their large servers and networking
    devices, they facilitate data storage, data transfer, and the processing of information
    over the internet. Data centers account for about ~1.5% of all global energy consumption
    and if nothing is done about it, the consumption will only increase. DeepMind,
    along with Google Research in 2016, employed reinforcement learning models to
    reduce the energy consumption of their data centers by 40%. Using the historical
    data collected from the sensors within the data center, they trained a deep neural
    network to predict future energy efficiency and propose optimal action. You can
    read the details of the models and approach in the paper *Data center cooling
    using model-predictive control* ([https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf](https://proceedings.neurips.cc/paper/2018/file/059fdcd96baeb75112f09fa1dcc740cc-Paper.pdf)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controlling nuclear fusion plasma**: A recent (2022) and interesting application
    of RL is in controlling nuclear fusion plasma with the help of reinforcement learning.
    The results are published in a Nature paper: *Magnetic control of tokamak plasmas
    through reinforcement learning*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is really amazing to see how the DRL agent, without any implicit knowledge,
    learns to perform, and even beat, humans – in many specialized tasks. In the coming
    sections, we will explore these fabulous DRL algorithms and see them play games
    with almost human efficiency within a few thousand epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Simulation environments for RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned earlier, **trial and error** is an important component of any RL
    algorithm. Therefore, it makes sense to train our RL agent firstly in a simulated
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Today there exists a large number of platforms that can be used for the creation
    of an environment. Some popular ones are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenAI Gym**: This contains a collection of environments that we can use
    to train our RL agents. In this chapter, we’ll be using the OpenAI Gym interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unity ML-Agents SDK**: It allows developers to transform games and simulations
    created using the Unity editor into environments where intelligent agents can
    be trained using DRL, evolutionary strategies, or other machine learning methods
    through a simple-to-use Python API. It works with TensorFlow and provides the
    ability to train intelligent agents for 2D/3D and VR/AR games. You can learn more
    about it here: [https://github.com/Unity-Technologies/ml-agents](https://github.com/Unity-Technologies/ml-agents).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gazebo**: In Gazebo, we can build three-dimensional worlds with physics-based
    simulation. The `gym-gazebo` toolkit uses Gazebo along with the **Robot Operating
    System** (**ROS**) and the OpenAI Gym interface and can be used to train RL agents.
    To find out more about this, you can refer to the white paper: [https://arxiv.org/abs/1608.05742](https://arxiv.org/abs/1608.05742).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blender learning environment**: This is a Python interface for the Blender
    game engine, and it also works with OpenAI Gym. It has at its base Blender: a
    free 3D modeling software with an integrated game engine. This provides an easy-to-use,
    powerful set of tools for creating games. It provides an interface to the Blender
    game engine, and the games themselves are designed in Blender. We can then create
    a custom virtual environment to train an RL agent on a specific problem ([https://github.com/LouisFoucard/gym-blender](https://github.com/LouisFoucard/gym-blender)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Malmo**: Built by the Microsoft team, Malmo is a platform for AI experimentation
    and research built on top of Minecraft. It provides a simple API for creating
    tasks and missions. You can learn more about Project Malmo here: [https://www.microsoft.com/en-us/research/project/project-malmo/](https://www.microsoft.com/en-us/research/project/project-malmo/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An introduction to OpenAI Gym
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be using OpenAI Gym to provide an environment for our agent. OpenAI
    Gym is an open source toolkit to develop and compare RL algorithms. It contains
    a variety of simulated environments that can be used to train agents and develop
    new RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing to do is install OpenAI Gym. The following command will install
    the minimal `gym` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to install all (free) `gym` modules, add `[all]` after it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The MuJoCo environment requires a purchasing license. For Atari-based games,
    you will need to install Atari dependencies (Box2D and ROM):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenAI Gym provides a variety of environments, from simple text-based to three-dimensional
    games. The environments supported can be grouped as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Algorithms**: Contains environments that involve performing computations
    such as addition. While we can easily perform the computations on a computer,
    what makes these problems interesting as RL problems is that the agent learns
    these tasks purely by example.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Atari**: This environment provides a wide variety of classic Atari/arcade
    games.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Box2D**: Contains robotics tasks in two dimensions such as a car racing agent
    or bipedal robot walk.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classic control**: This contains the classical control theory problems, such
    as balancing a cart pole.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MuJoCo**: This is proprietary (you can get a one-month free trial). It supports
    various robot simulation tasks. The environment includes a physics engine; hence,
    it’s used for training robotic tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Robotics**: This environment also uses the physics engine of MuJoCo. It simulates
    goal-based tasks for fetch and shadow-hand robots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Toy text**: A simple text-based environment—very good for beginners.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can get a complete list of environments from the Gym website: [https://gym.openai.com](https://gym.openai.com).
    To find a list of all available environments in your installation, you can use
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'At the time of writing this book, it resulted in 859, that is, there are 859
    different environments present in the `gym` module. Let us see more details of
    these environments. Each environment is created by using the `make` function.
    Associated with each environment is a unique ID, its observation space, its action
    space, and a default reward range. Gym allows you to access them through dot notation,
    as shown in the following code. We go through all the environments in the `envall`
    list and note down its unique ID, which is used to create the environment using
    the `make` method, its observation space, reward range, and the action space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 11.4* shows a random sample from the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table  Description automatically generated](img/B18331_11_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.4: Random list of environments available in OpenAI Gym'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use these commands to find out details about any environment in Gym.
    For example, the following code prints details of the MountainCar environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The core interface provided by OpenAI Gym is the unified environment interface.
    The agent can interact with the environment using three basic methods, that is,
    `reset`, `step`, and `render`. The `reset` method resets the environment and returns
    the observation. The `step` method steps the environment by one time step and
    returns `new_obs`, `reward`, `done`, and `info`. The `render` method renders one
    frame of the environment, like popping a window. Let us try and view some different
    environments and view their initial frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Physics Engine** | **Classic Control** | **Atari** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![Icon  Description automatically generated](img/B18331_11_05.png) | ![Chart,
    box and whisker chart  Description automatically generated](img/B18331_11_06.png)
    | ![A picture containing text  Description automatically generated](img/B18331_11_07.png)
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11.1: Different environments of OpenAI Gym and their initial state'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code uses Matplotlib to display the environment; alternatively,
    you can directly use the `render` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the Breakout environment in *Figure 11.5*; the `render` function
    pops up the environment window:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_11_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.5: Initial state of the Breakout environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use `env.observation_space` and `env.action_space` to find out more
    about the state space and action space for the Breakout game. The results show
    the state consists of a three-channel image of size 210 × 160, and the action
    space is discrete with four possible actions. Once you are done, do not forget
    to close OpenAI using:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Random agent playing Breakout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s have some fun and play the Breakout game. When I first played the game,
    I had no idea of the rules or how to play, so I randomly chose the control buttons.
    Our novice agent will do the same; it will choose the actions randomly from the
    action space. Gym provides a function called `sample()`, which chooses a random
    action from the action space – we will be using this function. Also, we can save
    a replay of the game, to view it later. There are two ways to save the play, one
    using Matplotlib and another using an OpenAI Gym Monitor wrapper. Let us first
    see the Matplotlib method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first import the necessary modules; we will only need `gym` and `matplotlib`
    for now, as the agent will be playing random moves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the Gym environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will run the game, one step at a time, choosing a random action, either
    for 300 steps or until the game is finished (whichever is earlier). The environment
    state (observation) space is saved at each step in the list `frames`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now comes the part of combining all the frames into a GIF image using Matplotlib
    Animation. We create an image object, patch, and then define a function that sets
    image data to a particular frame index. The function is used by the Matplotlib
    `Animation` class to create an animation, which we finally save in the file `random_agent.gif`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The code above will generate a GIF image. Below are some screen grabs from
    the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_11_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.6: Some screenshots from the saved GIF image'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are familiar with OpenAI Gym, we’ll move on to wrappers—which you
    can use to create your own custom environments.
  prefs: []
  type: TYPE_NORMAL
- en: Wrappers in Gym
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gym provides various wrappers for us to modify the existing environment. For
    example, if you have image-based inputs with the RGB intensity value lying between
    0 and 255, but the RL agent you use is a neural network, which works best if the
    input is in the range 0-1, you can use the Gym wrapper class to preprocess the
    state space. Below we define a wrapper that concatenates observations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we need to change the default `reset` function, `step` function,
    and observation function `_get_obs`. We also need to modify the default observation
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see how it works. If you take the `"BreakoutNoFrameskip-v4"` environment,
    then the initial observation space is 210 x 160 x 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'And now if you use the wrapper we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that now a dimension is added—it has four frames, with each frame
    of size 210 x 160 x 3\. You can use a wrapper to modify the rewards as well. In
    this case, you use the superclass `RewardWrapper`. Below is sample code that can
    clip the reward to lie within the range [-10, 10]:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us try using it in the CartPole environment, which has the reward range
    ![](img/B18331_11_019.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Another useful application of wrappers is when you want to save the state space
    as an agent is learning. Normally, an RL agent requires lots of steps for proper
    training, and as a result, it is not feasible to store the state space at each
    step. Instead, we can choose to store after every 500th step (or any other number
    you wish) in the preceding algorithm. OpenAI Gym provides the `Wrapper Monitor`
    class to save the game as a video. To do so, we need to first import wrappers,
    then create the environment, and finally use `Monitor`.
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, it will store the video of 1, 8, 27, 64, (episode numbers with
    perfect cubes), and so on, and then every 1,000th episode; each training, by default,
    is saved in one folder. The code to do this is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: For `Monitor` to work, we require FFmpeg support. We may need to install it
    depending upon our OS, if it is missing.
  prefs: []
  type: TYPE_NORMAL
- en: This will save the videos in `.mp4` format in the folder `recording`. An important
    thing to note here is that you have to set the `force=True` option if you want
    to use the same folder for the next training session.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to train your agent on Google Colab, you will need to add the following
    drivers to be able to visualize the Gym output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'After installing the Python virtual display, you need to start it—Gym uses
    the virtual display to set observations. The following code can help you in starting
    a display of size 600 x 400:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And to be able to play around with Atari games, use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Deep Q-networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep Q-Networks**, **DQNs** for short, are deep learning neural networks
    designed to approximate the Q-function (value-state function). They are one of
    the most popular value-based reinforcement learning algorithms. The model was
    proposed by Google’s DeepMind in NeurIPS 2013, in the paper entitled *Playing
    Atari with Deep Reinforcement Learning*. The most important contribution of this
    paper was that they used the raw state space directly as input to the network;
    the input features were not hand-crafted as done in earlier RL implementations.
    Also, they could train the agent with exactly the same architecture to play different
    Atari games and obtain state-of-the-art results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This model is an extension of the simple Q-learning algorithm. In Q-learning
    algorithms, a Q-table is maintained as a cheat sheet. After each action, the Q-table
    is updated using the Bellman equation [5]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_020.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_11_021.png) is the learning rate, and its value lies in the
    range [0,1]. The first term represents the component of the old *Q* value and
    the second term the target *Q* value. Q-learning is good if the number of states
    and the number of possible actions is small, but for large state spaces and action
    spaces, Q-learning is simply not scalable. A better alternative would be to use
    a deep neural network as a function approximator, approximating the target Q-function
    for each possible action. The weights of the deep neural network in this case
    store the Q-table information. There is a separate output unit for each possible
    action. The network takes the state as its input and returns the predicted target
    *Q* value for all possible actions. The question arises: how do we train this
    network, and what should be the loss function? Well, since our network has to
    predict the target *Q* value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'the loss function should try and reduce the difference between the *Q* value
    predicted, *Q*[predicted], and the target *Q*, *Q*[target]. We can do this by
    defining the loss function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_023.png)'
  prefs: []
  type: TYPE_IMG
- en: where *W* is the training parameters of our deep *Q* network, learned using
    gradient descent, such that the loss function is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the general architecture of a DQN. The network takes the *n*-dimensional
    state as input and outputs the *Q* value of each possible action in the *m*-dimensional
    action space. Each layer (including the input) can be a convolutional layer (if
    we are taking the raw pixels as input, convolutional layers make more sense) or
    a dense layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing icon  Description automatically generated](img/B18331_11_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.7: The figure shows a simple DQN network, the input layer taking
    State vector S, and the output predicting Q for all possible actions for the state'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will try training a DQN. Our agent task will be to stabilize
    a pole on a cart. The agent can move the cart left or right to maintain balance.
  prefs: []
  type: TYPE_NORMAL
- en: DQN for CartPole
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CartPole is a classic OpenAI problem with continuous state space and discrete
    action space. In it, a pole is attached by an un-actuated joint to a cart; the
    cart moves along a frictionless track. The goal is to keep the pole standing on
    the cart by moving the cart left or right. A reward of +1 is given for each time
    step the pole is standing. Once the pole is more than 15 degrees from the vertical,
    or the cart moves beyond 2.4 units from the center, the game is over:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Box and whisker chart  Description automatically generated](img/B18331_11_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.8: A screenshot from the CartPole Gym environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can check the leaderboard of OpenAI Gym for some cool entries for the CartPole
    environment: [https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0](https://github.com/openai/gym/wiki/Leaderboard#cartpole-v0).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with importing the necessary modules. We require `gym`, obviously,
    to provide us with the CartPole environment, and `tensorflow` to build our DQN
    network. Besides these, we need the `random` and `numpy` modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the global values for the maximum episodes for which we will be training
    the agent (`EPOCHS`), the threshold value when we consider the environment solved
    (`THRESHOLD`), and a bool to indicate if we want to record the training or not
    (`MONITOR`). Please note that as per the official OpenAI documentation, the CartPole
    environment is considered solved when the agent is able to maintain the pole in
    the vertical position for 195 time steps (ticks). In the following code, for the
    sake of time, we have reduced the `THRESHOLD` to 45:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let us build our DQN. We declare a class `DQN` and in its `__init__()`
    function declare all the hyperparameters and our model. We are also creating the
    environment inside the `DQN` class. As you can see, the class is quite general,
    and you can use it to train any Gym environment whose state space information
    can be encompassed in a 1D array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The DQN that we have built is a three-layered perceptron; in the following
    output, you can see the model summary. We use the Adam optimizer with learning
    rate decay:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The variable list `self.memory` will contain our experience replay buffer.
    We need to add a method for saving the *<S,A,R,S’>* tuple into the memory and
    a method to get random samples from it in batches to train the agent. We perform
    these two functions by defining the class methods `remember` and `replay`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Our agent will use the **epsilon-greedy policy** when choosing the action.
    This is implemented in the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we write a method to train the agent. We define two lists to keep track
    of the scores. First, we fill the experience replay buffer and then we choose
    some samples from it to train the agent and hope that the agent will slowly learn
    to do better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that all necessary functions are done, we just need one more helper function
    to reshape the state of the CartPole environment so that the input to the model
    is in the correct shape. The state of the environment is described by four continuous
    variables: cart position ([-2.4-2.4]), cart velocity, pole angle ([-41.8o-41.8o]),
    and pole velocity :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now instantiate our agent for the CartPole environment and train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s plot the average reward as the agent learns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 11.9* shows the agent being trained on my system. The agent was able
    to achieve our set threshold of 45 in 254 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`![Chart  Description automatically generated](img/B18331_11_12.png)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.9: Average agent reward plot'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the training is done, you can close the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: You can see that starting with no information about how to balance the pole,
    the agent, using a DQN, is able to balance the pole for more and more time (on
    average) as it learns. Starting from a blank slate, the agent is able to build
    information/knowledge to fulfill the required goal. Remarkable!
  prefs: []
  type: TYPE_NORMAL
- en: DQN to play a game of Atari
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the preceding section, we trained a DQN to balance a pole in CartPole. It
    was a simple problem, and thus we could solve it using a perceptron model. But
    imagine if the environment state was just the CartPole visual as we humans see
    it. With raw pixel values as the input state space, our previous DQN will not
    work. What we need is a convolutional neural network. Next, we build one based
    on the seminal paper on DQNs, *Playing Atari with Deep Reinforcement Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the code will be similar to the DQN for CartPole, but there will be
    significant changes in the DQN network itself, and how we preprocess the state
    that we obtain from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us see the change in the way state space is processed. *Figure 11.10*
    shows one of the Atari games, Breakout:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_11_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.10: A screenshot of the Atari game, Breakout'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you look at the image, not all of it contains relevant information:
    the top part has redundant information about the score, the bottom part has unnecessary
    blank space, and the image is colored. To reduce the burden on our model, it is
    best to remove the unnecessary information, so we crop the image, convert it to
    grayscale, and make it a square of size 84 × 84 (as in the paper). Here is the
    code to preprocess the input raw pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Another important issue is that just by looking at the image at one time step,
    how can the agent know whether the ball is going up or down? One way could be
    to use LSTM along with a CNN to keep a record of the past and hence the ball movement.
    The paper, however, used a simple technique. Instead of a single state frame,
    it concatenated the state space for the past four time steps together as one input
    to the CNN; that is, the network sees four past frames of the environment as its
    input. The following is the code for combining the present and previous states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The model was defined in the `__init__` function. We modify the function to
    now have a CNN with an input of (84 × 84 × 4) representing four state frames each
    of size 84 × 84:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, we will need to make a minor change in the `train` function. We will
    need to call the new `preprocess` function, along with the `combine_images` function
    to ensure that four frames are concatenated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: That’s all. We can now train the agent for playing Breakout. The complete code
    is available on GitHub repository ([https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition/tree/main/Chapter_11](https://github.com/PacktPublishing/Deep-Learning-with-TensorFlow-and-Keras-3rd-edition/tree/main/Chapter_11))
    of this chapter in the file `DQN_Atari_v2.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: DQN variants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After the unprecedented success of DQNs, the interest in RL increased and many
    new RL algorithms came into being. Next, we see some of the algorithms that are
    based on DQNs. They all use DQNs as the base and build upon it.
  prefs: []
  type: TYPE_NORMAL
- en: Double DQN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In DQNs, the agent uses the same *Q* value to both select and evaluate an action.
    This can cause a maximization bias in learning. For example, let us consider that
    for a state, *S*, all possible actions have true *Q* values of zero. Now, our
    DQN estimates will have some values above and some values below zero, and since
    we are choosing the action with the maximum *Q* value and later evaluating the
    *Q* value of each action using the same (maximized) estimated value function,
    we are overestimating *Q*—or in other words, our agent is over-optimistic. This
    can lead to unstable training and a low-quality policy. To deal with this issue,
    Hasselt et al. from DeepMind proposed the Double DQN algorithm in their paper
    *Deep Reinforcement Learning with Double Q-Learning*. In Double DQN, we have two
    Q-networks with the same architecture but different weights. One of the Q-networks
    is used to determine the action using the epsilon-greedy policy and the other
    is used to determine its value (Q-target).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you recall in DQNs, the Q-target was given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the action *A* was selected using the same DQN, *Q(S,A; W)*, where *W*
    is the training parameters of the network; that is, we are writing the *Q* value
    function along with its training parameter to emphasize the difference between
    vanilla DQNs and Double DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In Double DQN, the equation for the target will now change. Now, the DQN *Q(S,A;W)*
    is used for determining the action and the DQN *Q(S,A;W’)* is used for calculating
    the target (notice the different weights). So, the preceding equation will change
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_026.png)'
  prefs: []
  type: TYPE_IMG
- en: This simple change reduces the overestimation and helps us to train the agent
    faster and more reliably.
  prefs: []
  type: TYPE_NORMAL
- en: Dueling DQN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This architecture was proposed by Wang et al. in their paper *Dueling Network
    Architectures for Deep Reinforcement Learning* in 2015\. Like the DQN and Double
    DQN, it is also a model-free algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Dueling DQN decouples the Q-function into the value function and advantage
    function. The value function, which we discussed earlier, represents the value
    of the state independent of any action. The advantage function, on the other hand,
    provides a relative measure of the utility (advantage/goodness) of action *A*
    in the state *S*. The Dueling DQN uses convolutional networks in the initial layers
    to extract the features from raw pixels. However, in the later stages, it is separated
    into two different networks, one approximating the value and another approximating
    the advantage. This ensures that the network produces separate estimates for the
    value function and the advantage function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_027.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B18331_10_024.png) is an array of the training parameters of the
    shared convolutional network (it is shared by both *V* and *A*), and ![](img/B18331_11_021.png)
    and ![](img/B18331_11_030.png) are the training parameters for the *Advantage*
    and *Value* estimator networks. Later, the two networks are recombined using an
    aggregating layer to estimate the *Q* value.
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 11.11*, you can see the architecture of Dueling DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface  Description automatically generated](img/B18331_11_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.11: Visualizing the architecture of a Dueling DQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be wondering, what is the advantage of doing all of this? Why decompose
    *Q* if we will just be putting it back together? Well, decoupling the value and
    advantage functions allows us to know which states are valuable, without having
    to take into account the effect of each action for each state. There are many
    states that, irrespective of the action taken, are good or bad states: for example,
    having breakfast with your loved ones in a good resort is always a good state,
    and being admitted to a hospital emergency ward is always a bad state. Thus, separating
    value and advantage allows one to get a more robust approximation of the value
    function. Next, you can see a figure from the paper highlighting how in the Atari
    game Enduro, the value network learns to pay attention to the road, and the advantage
    network learns to pay attention only when there are cars immediately in front,
    so as to avoid a collision:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B18331_11_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.12: In the Atari game Enduro, the value network learns to pay attention
    to the road (red spot), and the advantage network focuses only when other vehicles
    are immediately in front. Image source: https://arxiv.org/pdf/1511.06581.pdf'
  prefs: []
  type: TYPE_NORMAL
- en: 'The aggregate layer is implemented in a manner that allows one to recover both
    *V* and *A* from the given *Q*. This is achieved by enforcing that the advantage
    function estimator has zero advantage at the chosen action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_11_031.png)'
  prefs: []
  type: TYPE_IMG
- en: In the paper, Wang et al. reported that the network is more stable if the max
    operation is replaced by the average operation. This is so because the speed of
    change in advantage is now the same as the change in average, instead of the optimal
    (max) value.
  prefs: []
  type: TYPE_NORMAL
- en: Rainbow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Rainbow is the current state-of-the-art DQN variant. Technically, to call it
    a DQN variant would be wrong. In essence, it is an ensemble of many DQN variants
    combined together into a single algorithm. It modifies the distributional RL [6]
    loss to multi-step loss and combines it with Double DQN using a greedy action.
    Quoting from the paper:'
  prefs: []
  type: TYPE_NORMAL
- en: The network architecture is a dueling network architecture adapted for use with
    return distributions. The network has a shared representation ![](img/B18331_11_032.png),
    which is then fed into a value stream ![](img/B18331_11_033.png) with *N*[atoms]
    outputs, and into an advantage stream ![](img/B18331_11_034.png) with *N*[atoms]×*N*[actions]
    outputs, where a1ξ(fξ(s), a)will denote the output corresponding to atom i and
    action a. For each atom *z*[i], the value and advantage streams are aggregated,
    as in Dueling DQN, and then passed through a softmax layer to obtain the normalised
    parametric distributions used to estimate the returns’ distributions.
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Rainbow combines six different RL algorithms:'
  prefs: []
  type: TYPE_NORMAL
- en: N-step returns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributional state-action value learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dueling networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noisy networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritized experience replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Till now, we’ve considered value-based reinforcement learning algorithms. In
    the next section, we will learn about policy-based reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Deep deterministic policy gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The DQN and its variants have been very successful in solving problems where
    the state space is continuous and action space is discrete. For example, in Atari
    games, the input space consists of raw pixels, but actions are discrete—[**up**,
    **down**, **left**, **right**, **no-op**]. How do we solve a problem with continuous
    action space? For instance, say an RL agent driving a car needs to turn its wheels:
    this action has a continuous action space.'
  prefs: []
  type: TYPE_NORMAL
- en: One way to handle this situation is by discretizing the action space and continuing
    with a DQN or its variants. However, a better solution would be to use a policy
    gradient algorithm. In policy gradient methods, the policy ![](img/B18331_11_035.png)
    is approximated directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'A neural network is used to approximate the policy; in the simplest form, the
    neural network learns a policy for selecting actions that maximize the rewards
    by adjusting its weights using the steepest gradient ascent, hence the name: policy
    gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will focus on the **Deep Deterministic Policy Gradient**
    (**DDPG**) algorithm, another successful RL algorithm by Google’s DeepMind in
    2015\. DDPG is implemented using two networks; one called the actor network and
    the other called the critic network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actor network approximates the optimal policy deterministically, that is,
    it outputs the most preferred action for any given input state. In essence, the
    actor is learning. The critic on the other hand evaluates the optimal action value
    function using the actor’s most preferred action. Before going further, let us
    contrast this with the DQN algorithm that we discussed in the preceding section.
    In *Figure 11.13*, you can see the general architecture of DDPG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B18331_11_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11.13: Architecture of the DDPG model'
  prefs: []
  type: TYPE_NORMAL
- en: On the left-hand side of *Figure 11.13* is the critic network, it takes as input
    the state vector, *S*, and action taken, *A*. The output of the network is the
    *Q* value for that state and action. The right-hand figure shows the actor network.
    It takes as input the state vector, S, and predicts the optimum action, A, to
    be taken. In the figure, we have shown both the actor and critic to be of four
    layers. This is only for demonstration purposes.
  prefs: []
  type: TYPE_NORMAL
- en: The actor network outputs the most preferred action; the critic takes as input
    both the input state and action taken and evaluates its *Q* value. To train the
    critic network, we follow the same procedure as with a DQN; that is, we try to
    minimize the difference between the estimated *Q* value and the target *Q* value.
    The gradient of the *Q* value over actions is then propagated back to train the
    actor network. So, if the critic is good enough, it will force the actor to choose
    actions with optimal value functions.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Reinforcement learning has in recent years seen a lot of progress. To summarize
    all of that in a single chapter is not possible. However, in this chapter, we
    focused on the recent successful RL algorithms. The chapter started by introducing
    the important concepts in the RL field, its challenges, and the solutions to move
    forward. Next, we delved into two important RL algorithms: the DQN and DDPG algorithms.
    Toward the end of this chapter, we covered important topics in the field of deep
    learning.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move on to applying what we have learned to production.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MIT Technology Review covers OpenAI experiments on reinforcement learning:
    [https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/](https://www.technologyreview.com/s/614325/open-ai-algorithms-learned-tool-use-and-cooperation-after-hide-and-seek-games/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Coggan, Melanie. (2014). *Exploration and Exploitation in Reinforcement Learning*.
    Research supervised by Prof. Doina Precup, CRA-W DMP Project at McGill University.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lin, Long-Ji. (1993). *Reinforcement learning for robots using neural networks*.
    No. CMU-CS-93-103\. Carnegie-Mellon University Pittsburgh PA School of Computer
    Science.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Schaul, Tom, John Quan, Ioannis Antonoglou, and David Silver. (2015). *Prioritized
    Experience Replay*. arXiv preprint arXiv:1511.05952
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sutton R., Barto A. *Chapter 4, Reinforcement Learning*. MIT Press: [https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dabney W., Rowland M., Bellemare M G., and Munos R. (2018). *Distributional
    Reinforcement Learning with Quantile Regression*. In Thirty-Second AAAI Conference
    on Artificial Intelligence.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hessel, M., Modayil, J., Van Hasselt, H., Schaul, T., Ostrovski, G., Dabney,
    W., Horgan, D., Piot, B., Azar, M., and Silver, D. (2018). *Rainbow: Combining
    improvements in Deep Reinforcement Learning*. In Thirty-Second AAAI Conference
    on Artificial Intelligence.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Details about different environments can be obtained from [https://www.gymlibrary.ml/](https://www.gymlibrary.ml/)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Wiki pages are maintained for some environments at [https://github.com/openai/gym/wiki](https://github.com/openai/gym/wiki)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Details regarding installation instructions and dependencies can be obtained
    from [https://github.com/openai/gym](https://github.com/openai/gym)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Link to the paper by DeepMind, *Asynchronous Methods for Deep Reinforcement
    Learning*: [https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is a blog post by Andrej Karpathy on reinforcement learning: [http://karpathy.github.io/2016/05/31/rl/](http://karpathy.github.io/2016/05/31/rl/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Glorot X. and Bengio Y. (2010). *Understanding the difficulty of training deep
    feedforward neural networks*. Proceedings of the Thirteenth International Conference
    on Artificial Intelligence and Statistics: [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'A good read on why RL is still hard to crack: [https://www.alexirpan.com/2018/02/14/rl-hard.xhtml](https://www.alexirpan.com/2018/02/14/rl-hard.xhtml)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y.,
    ... & Wierstra, D. (2015). *Continuous control with deep reinforcement learning.
    arXiv preprint arXiv:1509.02971*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1831217224278819687.png)'
  prefs: []
  type: TYPE_IMG
