["```\nclass PixelNorm(Layer):\n    def __init__(self, epsilon=1e-8):\n        super(PixelNorm, self).__init__()\n        self.epsilon = epsilon\n    def call(self, input_tensor):\n        return input_tensor / tf.math.sqrt(                        \t                            tf.reduce_mean(input_tensor**2,         \t                            axis=-1, keepdims=True) + \t\t                            self.epsilon)\n```", "```\nclass MinibatchStd(Layer):\n    def __init__(self, group_size=4, epsilon=1e-8):\n        super(MinibatchStd, self).__init__()\n        self.epsilon = epsilon\n        self.group_size = group_size\n    def call(self, input_tensor):\n        n, h, w, c = input_tensor.shape\n        x = tf.reshape(input_tensor, [self.group_size,  \t\t\t\t -1, h, w, c])\n        group_mean, group_var = tf.nn.moments(x,  \t\t\t\t\t\t\taxes=(0), \t\t\t\t\t\t\tkeepdims=False)\n        group_std = tf.sqrt(group_var + self.epsilon)\n        avg_std = tf.reduce_mean(group_std, axis=[1,2,3], \t\t\t\t\t\tkeepdims=True)\n        x = tf.tile(avg_std, [self.group_size, h, w, 1])\n        return tf.concat([input_tensor, x], axis=-1)\n```", "```\nclass Conv2D(layers.Layer):\n    def build(self, input_shape):\n        self.in_channels = input_shape[-1]\n        fan_in = self.kernel*self.kernel*self.in_channels\n        self.scale = tf.sqrt(self.gain/fan_in)\n    def call(self, inputs):\n        x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1],  \t\t\t  \t\t[0, 0]], mode='REFLECT') \\ \t\t\t\tif self.pad else inputs \n        output = tf.nn.conv2d(x, self.scale*self.w, \t\t\t\t\t   strides=1,  \t\t\t\t\t   padding=\"SAME\") + self.b\n        return output\n```", "```\nclass Dense(layers.Layer):\n    def __init__(self, units, gain=2, **kwargs):\n        super(Dense, self).__init__(kwargs)\n        self.units = units\n        self.gain = gain\n    def build(self, input_shape):\n        self.in_channels = input_shape[-1]\n        initializer = \\ \t\t\ttf.keras.initializers.RandomNormal( \t\t\t\t\t\tmean=0., stddev=1.)        \n        self.w = self.add_weight(shape=[self.in_channels,\n                                        self.units],\n                                initializer=initializer,\n                                trainable=True,\n \t\t\t\t\t     name='kernel')\n        self.b = self.add_weight(shape=(self.units,),\n                                initializer='zeros',\n                                trainable=True, \n \t\t\t\t\t\tname='bias')\n        fan_in = self.in_channels\n        self.scale = tf.sqrt(self.gain/fan_in)\n    def call(self, inputs):\n        output = tf.matmul(inputs,  \t\t\t\t\tself.scale*self.w) + self.b\n        return output\n```", "```\ndef build_generator_base(self, input_shape):\n    input_tensor = Input(shape=input_shape)\n    x = PixelNorm()(input_tensor)\n    x = Dense(8192, gain=1./8)(x)\n    x = Reshape((4, 4, 512))(x)\n    x = LeakyReLU(0.2)(x)        \n    x = PixelNorm()(x)\n    x = Conv2D(512, 3, name='gen_4x4_conv1')(x)\n    x = LeakyReLU(0.2)(x)\n    x = PixelNorm()(x)\n    return Model(input_tensor, x, \n                 name='generator_base')\n```", "```\nself.log2_res_to_filter_size = {\n    0: 512,\n    1: 512,\n    2: 512, # 4x4\n    3: 512, # 8x8\n    4: 512, # 16x16\n    5: 512, # 32x32\n    6: 256, # 64x64\n    7: 128, # 128x128\n    8: 64,  # 256x256\n    9: 32,  # 512x512\n    10: 16} # 1024x1024\n```", "```\ndef build_generator_block(self, log2_res, input_shape):\n    res = 2**log2_res\n    res_name = f'{res}x{res}'\n    filter_n = self.log2_res_to_filter_size[log2_res]\n    input_tensor = Input(shape=input_shape)\n    x = UpSampling2D((2,2))(input_tensor)\n    x = Conv2D(filter_n, 3,  \t\t    name=f'gen_{res_name}_conv1')(x)\n    x = PixelNorm()(LeakyReLU(0.2)(x))\n    x = Conv2D(filter_n, 3, \t\t\tname=f'gen_{res_name}_conv2')(x)\n    x = PixelNorm()(LeakyReLU(0.2)(x))\n    return Model(input_tensor, x, \n                 name=f'genblock_{res}_x_{res}')\n```", "```\ndef build_discriminator_base(self, input_shape):\n    input_tensor = Input(shape=input_shape)\n    x = MinibatchStd()(input_tensor)\n    x = Conv2D(512, 3, name='gen_4x4_conv1')(x)\n    x = LeakyReLU(0.2)(x)\n    x = Flatten()(x)\n    x = Dense(512, name='gen_4x4_dense1')(x)\n    x = LeakyReLU(0.2)(x)\n    x = Dense(1, name='gen_4x4_dense2')(x)\n    return Model(input_tensor, x, \n                 name='discriminator_base')\n```", "```\ndef build_discriminator_block(self, log2_res, input_shape):\n    filter_n = self.log2_res_to_filter_size[log2_res]        \n    input_tensor = Input(shape=input_shape)\n    x = Conv2D(filter_n, 3)(input_tensor)\n    x = LeakyReLU(0.2)(x)\n    filter_n = self.log2_res_to_filter_size[log2_res-1]        \n    x = Conv2D(filter_n, 3)(x)\n    x = LeakyReLU(0.2)(x)\n    x = AveragePooling2D((2,2))(x)\n    res = 2**log2_res\n    return Model(input_tensor, x, \n                 name=f'disc_block_{res}_x_{res}')\n```", "```\ndef build_to_rgb(self, res, filter_n): \n    return Sequential([Input(shape=(res, res, filter_n)),\n                       Conv2D(3, 1, gain=1, \n \t\t\t\t activation='tanh')])\ndef build_from_rgb(self, res, filter_n): \n    return Sequential([Input(shape=(res, res, 3)),\n                       Conv2D(filter_n, 1),\n                       LeakyReLU(0.2)])\n```", "```\nclass FadeIn(Layer):\n    @tf.function\n    def call(self, input_alpha, a, b):\n        alpha = tf.reduce_mean(input_alpha)\n        y = alpha * a + (1\\. - alpha) * b\n        return y\n```", "```\ndef grow_generator(self, log2_res):\n    res = 2**log2_res\n    alpha = Input(shape=(1))\n    x = self.generator_blocks[2].input\n    for i in range(2, log2_res):            \n        x = self.generator_blocks[i](x)\n    old_rgb = self.to_rgb[log2_res-1](x)\n    old_rgb = UpSampling2D((2,2))(old_rgb)\n    x = self.generator_blocks[log2_res](x)\n    new_rgb = self.to_rgb[log2_res](x)\n    rgb = FadeIn()(alpha, new_rgb, old_rgb)\n    self.generator = Model([self.generator_blocks[2].input,\n \t\t\t\t\talpha], rgb, \n                               name=f'generator_{res}_x_{res}')\n```", "```\ndef grow_discriminator(self, log2_res):\n    res = 2**log2_res \n    input_image = Input(shape=(res, res, 3))\n    alpha = Input(shape=(1))\n    x = self.from_rgb[log2_res](input_image)\n    x = self.discriminator_blocks[log2_res](x)        \n    downsized_image = AveragePooling2D((2,2))(input_image)\n    y = self.from_rgb[log2_res-1](downsized_image)\n    x = FadeIn()(alpha, x, y)\n    for i in range (log2_res-1, 1, -1):\n        x = self.discriminator_blocks[i](x)\n    self.discriminator =  Model([input_image, alpha], x,\n                 name=f'discriminator_{res}_x_{res}')\n```", "```\ndef grow_model(self, log2_res):\n    self.grow_generator(log2_res)\n    self.grow_discriminator(log2_res)\n    self.discriminator.trainable = False\n    latent_input = Input(shape=(self.z_dim))\n    alpha_input = Input(shape=(1))\n    fake_image = self.generator([latent_input, alpha_input])\n    pred = self.discriminator([fake_image, alpha_input])\n    self.model = Model(inputs=[latent_input, alpha_input],\n                      outputs=pred)\n    self.model.compile(loss=wasserstein_loss, \n                       optimizer=Adam(**self.opt_init))\n    self.optimizer_discriminator = Adam(**self.opt_init)\n```", "```\n# drift loss\nall_pred = tf.concat([pred_fake, pred_real], axis=0)\ndrift_factor = 0.001\ndrift_loss = drift_factor * tf.reduce_mean(all_pred**2)\n```", "```\ndef build_mapping(self):\n    # Mapping Network\n    z = Input(shape=(self.z_dim))\n    w = PixelNorm()(z)\n    for i in range(8):\n        w = Dense(512, lrmul=0.01)(w)\n        w = LeakyReLU(0.2)(w)\n    w = tf.tile(tf.expand_dims(w, 0), (8,1,1))            \n    self.mapping = Model(z, w, name='mapping')\n```", "```\nclass AddNoise(Layer):\n    def build(self, input_shape):\n        n, h, w, c = input_shape[0]\n        initializer = \\ \t\ttf.keras.initializers.RandomNormal( \t\t\t\t\t\tmean=0., stddev=1.)\n        self.B = self.add_weight(shape=[1, 1, 1, c],\n                                initializer=initializer,\n                                trainable=True, \n \t\t\t\t\t\tname='kernel')\n    def call(self, inputs):\n        x, noise = inputs\n        output = x + self.B * noise\n        return output\n```", "```\nclass AdaIN(Layer):\n    def __init__(self, gain=1, **kwargs):\n        super(AdaIN, self).__init__(kwargs)\n        self.gain = gain\n    def build(self, input_shapes):\n        x_shape = input_shapes[0]\n        w_shape = input_shapes[1]\n        self.w_channels = w_shape[-1]\n        self.x_channels = x_shape[-1]\n        self.dense_1 = Dense(self.x_channels, gain=1)\n        self.dense_2 = Dense(self.x_channels, gain=1)\n    def call(self, inputs):\n        x, w = inputs\n        ys = tf.reshape(self.dense_1(w), (-1, 1, 1,\n \t\t\t\t\tself.x_channels))\n        yb = tf.reshape(self.dense_2(w), (-1, 1, 1, \n \t\t\t\t\tself.x_channels))\n        output = ys*x + yb\n        return output\n```", "```\ndef build_generator_block(self, log2_res, input_shape):\n    res = int(2**log2_res)\n    res_name = f'{res}x{res}'\n    filter_n = self.log2_res_to_filter_size[log2_res]\n    input_tensor = Input(shape=input_shape)\n    x = input_tensor\n    w = Input(shape=512)\n    noise = Input(shape=(res, res, 1))\n    if log2_res > 2:\n        x = UpSampling2D((2,2))(x)\n        x = Conv2D(filter_n, 3,  \t\t\t   name=f'gen_{res_name}_conv1')(x)\n    x = AddNoise()([x, noise])\n    x = PixelNorm()(LeakyReLU(0.2)(x))\n    x = AdaIN()([x, w])\n    # ADD NOISE\n    x = Conv2D(filter_n, 3, \n \t\t\tname=f'gen_{res_name}_conv2')(x)\n    x = AddNoise()([x, noise])    \n    x = PixelNorm()(LeakyReLU(0.2)(x))\n    x = AdaIN()([x, w])\n    return Model([input_tensor, x, noise], x, \n                 name=f'genblock_{res}_x_{res}')\n```"]