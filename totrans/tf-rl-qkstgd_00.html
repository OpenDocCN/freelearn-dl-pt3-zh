<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Preface</h1>
                </header>
            
            <article>
                
<p class="mce-root">This book provides a summary of several different <strong>reinforcement learning</strong> (<strong>RL</strong>) algorithms, including the theory involved in the algorithms as well as coding them using Python and TensorFlow. Specifically, the algorithms covered in this book are Q-learning, SARSA, DQN, DDPG, A3C, TRPO, and PPO. The applications of these RL algorithms include computer games from OpenAI Gym and autonomous driving using the TORCS racing car simulator.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Who this book is for</h1>
                </header>
            
            <article>
                
<p>This book is designed for <strong>machine learning</strong> (<strong>ML</strong>) practitioners interested in learning RL. It will help ML engineers, data scientists, and graduate students. A basic knowledge of ML, and experience of coding in Python and TensorFlow, is expected of the reader in order to be able to complete this book <span>successfully</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What this book covers</h1>
                </header>
            
            <article>
                
<p><a href="eabadc99-98c0-4419-81fc-533264228962.xhtml" target="_blank">Chapter 1</a>, <em>Up and Running with Reinforcement Learning</em>, provides an overview of the basic concepts of RL, such as an agent, an environment, and the relationship between them. It also covers topics such as reward functions, discounted rewards, and value and advantage functions. The reader will also get familiar with the Bellman equation, on-policy and off-policy algorithms, as well as model-free and model-based RL algorithms.</p>
<p><a href="72e58e29-a6e5-46b4-9d3e-d1baf4dc57f5.xhtml" target="_blank">Chapter 2</a>, <em>Temporal Difference, SARSA, and Q-learning</em><span>, </span>introduces the reader to temporal difference learning, SARSA, and Q-learning. It also summarizes how to code these algorithms in Python, and to train and test them on two classical RL problems – GridWorld and Cliff Walking.</p>
<p><a href="1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml" target="_blank">Chapter 3</a>, <em>Deep Q-Network</em>, introduces the reader to the first deep RL algorithm of the book, DQN. It will also discuss how to code this in Python and TensorFlow. The code will then be used to train an RL agent to play <em>Atari Breakout</em>. </p>
<p><a href="ac0b4811-f9fc-474c-b1d6-3f8a43dc018c.xhtml" target="_blank"/><a href="ac0b4811-f9fc-474c-b1d6-3f8a43dc018c.xhtml" target="_blank">Chapter 4</a>, <em>Double DQN, Dueling Architectures, and Rainbow</em>, builds on the previous chapter and extends it to double DQN. It also discusses dueling network architectures that involve value and advantage streams. These extensions will be coded in Python and TensorFlow, and will be used to train RL agents to play <em>Atari Breakout</em>. Finally, Google's dopamine code will be introduced and will be used to train a Rainbow DQN agent.</p>
<p><a href="c74c5dbc-8070-48da-889b-c6b357e07b92.xhtml" target="_blank">Chapter 5</a>, <em>Deep Deterministic Policy Gradient</em>, is the first actor-critic algorithm of the book as well as the first RL algorithm for continuous control. It introduces policy gradients to the reader and discusses how to use it to train the policy for the actor. The chapter will code this algorithm using Python and TensorFlow and use it to train an agent to play the inverted pendulum problem.</p>
<p><a href="f9b2e0f4-90ce-4070-b030-b47339ca1aa8.xhtml" target="_blank">Chapter 6</a>, <em>Asynchronous Methods – A3C and A2C</em>, introduces the reader to the A3C algorithm, which is an asynchronous RL algorithm where one master processor will update the policy network, and multiple worker processors will use it to collect experience samples, which will be used to compute the policy gradients, and then passed on to the master processor. Also in this chapter, A3C will be used to train RL agents to play OpenAI Gym's <em>CartPole</em> and <em>LunarLander</em>. Finally, A2C is also briefly introduced.</p>
<p><a href="7f55a061-06a5-4f69-ab05-4eff75c2dacd.xhtml" target="_blank">Chapter 7</a>, <em>Trust Region Policy Optimization and Proximal Policy Optimization</em>, discusses two RL algorithms based on the policy distribution ratio—TRPO and PPO. This chapter also discusses how to code PPO using Python and TensorFlow, and will use it to train an RL agent to solve the MountainCar problem in OpenAI Gym.</p>
<p><a href="f4774497-444a-4174-b7d4-a8ef21928320.xhtml" target="_blank">Chapter 8</a>, <em>Deep RL Applied to Autonomous Driving</em>, introduces the reader to the TORCS racing car simulator, coding the DDPG algorithm for training an agent to drive a car autonomously. The code files for this chapter also include the PPO algorithm for the same TORCS problem, and is provided as an exercise for the reader. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">To get the most out of this book</h1>
                </header>
            
            <article>
                
<p>The reader is expected to have a good knowledge of ML algorithms, such as deep neural networks, convolutional neural networks, stochastic gradient descent, and Adam optimization. <span>The reader is also expected to have hands-on coding experience in Python and TensorFlow.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the example code files</h1>
                </header>
            
            <article>
                
<p>You can download the example code files for this book from your account at <a href="http://www.packt.com" target="_blank">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="http://www.packt.com/support" target="_blank">www.packt.com/support</a> and register to have the files emailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register at <a href="http://www.packt.com" target="_blank">www.packt.com</a>.</li>
<li>Select the <span class="packt_screen">SUPPORT</span> tab.</li>
<li>Click on <span class="packt_screen">Code Downloads &amp; Errata</span>.</li>
<li>Enter the name of the book in the <span class="packt_screen">Search</span> box and follow the onscreen instructions.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR/7-Zip for Windows</li>
<li>Zipeg/iZip/UnRarX for Mac</li>
<li>7-Zip/PeaZip for Linux</li>
</ul>
<p><span>The code bundle for the book is also hosted on GitHub at</span><span> </span><strong><span class="Object"><a href="https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide" target="_blank">https://github.com/PacktPublishing/TensorFlow-Reinforcement-Learning-Quick-Start-Guide</a></span></strong><span>. </span><span>In case there's an update to the code, it will be updated on the existing GitHub repository.</span></p>
<p><span>We also have other code bundles from our rich catalog of books and videos available at</span><span> </span><strong><span class="Object"><a href="https://github.com/PacktPublishing/" target="_blank">https://github.com/PacktPublishing/</a></span></strong><span>. Check them out!</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the color images</h1>
                </header>
            
            <article>
                
<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="http://www.packtpub.com/sites/default/files/downloads/9781789533583_ColorImages.pdf" target="_blank">http://www.packtpub.com/sites/default/files/downloads/9781789533583_ColorImages.pdf</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conventions used</h1>
                </header>
            
            <article>
                
<p>There are a number of text conventions used throughout this book.</p>
<p><kbd>CodeInText</kbd>: <span>Indicates c</span>ode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. <span>Here is an example:</span> "Mount the downloaded <kbd>WebStorm-10*.dmg</kbd> disk image file as another disk in your system."</p>
<p>A block of code is set as follows:</p>
<pre class="mce-root">import numpy as np <br/>import sys <br/>import matplotlib.pyplot as plt</pre>
<p>When we wish to draw your attention to a particular part of a code block, the relevant lines or items are set in bold:</p>
<pre class="mce-root">def random_action():<br/>    # a = 0 : top/north<br/>    # a = 1 : right/east<br/>    # a = 2 : bottom/south<br/>    # a = 3 : left/west<br/>    <strong>a = np.random.randint(nact)</strong><br/>    return a</pre>
<p>Any command-line input or output is written as follows:</p>
<pre><strong><span class="n">sudo</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="n">python</span><span class="o">-</span><span class="n">numpy</span> <span class="n">python</span><span class="o">-</span><span class="n">scipy</span> <span class="n">python</span><span class="o">-</span><span class="n">matplotlib</span></strong></pre>
<p><strong>Bold</strong>: Indicates a new term, an important word, or w<span>ords that you see on screen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "Select <span class="packt_screen">System info</span> from the <span class="packt_screen">Administration</span> panel.</span><span>"</span></p>
<div class="packt_infobox">Warnings or important notes appear like this.</div>
<div class="packt_tip">Tips and tricks appear like this.</div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Get in touch</h1>
                </header>
            
            <article>
                
<p>Feedback from our readers is always welcome.</p>
<p class="mce-root"><strong>General feedback</strong>: If you have questions about any aspect of this book, <span>mention the book title in the subject of your message and</span> email us at <kbd><span>customercare@packtpub.com</span></kbd>.</p>
<p><strong>Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="http://www.packt.com/submit-errata" target="_blank">www.packt.com/submit-errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p><strong>Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd>copyright@packt.com</kbd> with a link to the material.</p>
<p class="mce-root"><strong>If you are interested in becoming an author</strong>: If there is a topic that you have expertise in, and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" target="_blank">authors.packtpub.com</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviews</h1>
                </header>
            
            <article>
                
<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p>For more information about Packt, please visit <a href="http://www.packt.com/" target="_blank">packt.com</a>.<a href="https://www.packtpub.com/" target="_blank"/></p>


            </article>

            
        </section>
    </body></html>