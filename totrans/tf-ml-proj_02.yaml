- en: Using Machine Learning to Detect Exoplanets in Outer Space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we shall learn how to detect exoplanets in outer space using
    ensemble methods that are based on decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees are a family of non-parametric supervised learning methods. In
    a decision tree algorithm, the data is divided into two partitions by using a
    simple rule. The rule is applied again and again to further partition the data,
    thus forming a tree of decisions.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble methods combine the learning from multiple learning algorithms to improve
    predictions and reduce errors. These ensembles are differentiated on the basis
    of what kind of learners they use and how they structure those learns in the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: The two most popular ensemble methods based on decision trees are known as gradient
    boosted trees and random forests.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a decision tree?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why we need ensembles?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree-based ensemble methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree-based ensembles in TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a TensorFlow boosted tree model for exoplanet detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code from this chapter is available in Jupyter Notebook as `ch-02_Detecting_Explonaets_in_Outer_Space.ipynb`
    in the code bundle.
  prefs: []
  type: TYPE_NORMAL
- en: What is a decision tree?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are a family of non-parametric supervised learning methods. In
    the decision tree algorithm, we start with the complete dataset and split it into
    two partitions based on a simple rule. The splitting continues until a specified
    criterion is met. The nodes at which the split is made are called interior nodes
    and the final endpoints are called terminal or leaf nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, let us look at the following tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3bba725c-c1a9-4574-bec6-5b2b904469b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we are assuming that the exoplanet data has only two properties: **flux.1**
    and **flux.2**. First, we make a decision if **flux.1 > 400** and then divide
    the data into two partitions. Then we divide the data again based on **flux.2**
    feature, and that division decides whether the planet is an exoplanet or not.
    How did we decide that condition **flux.1 > 400**? We did not. This was just to demonstrate
    a decision tree. During the training phase, that''s what the model learns – the
    parameters of conditions that divide the data into partitions.'
  prefs: []
  type: TYPE_NORMAL
- en: For classification problems, the decision tree has leaf nodes that shows the
    result as the discrete classification of the data and for regression problems,
    the leaf nodes show the results as a predicted number. Decision trees, thus, are
    also popularly known as **Classification and Regression Trees** (**CART**).
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need ensembles?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are prone to overfitting training data and suffer from high variance,
    thus, providing poor predictions from new unseen data. However, using an ensemble
    of decision trees helps alleviate the shortcoming of using a single decision tree
    model. In an ensemble, many weak learners come together to create a strong learner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Among the many ways that we can combine decision trees to make ensembles, the
    two methods that have been popular due to their performance for predictive modeling
    are:'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting (also known as gradient tree boosting)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Random decision trees (also known as random forests)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decision tree-based ensemble methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section let us explore briefly two kinds of ensemble methods for decision
    trees: random forests and gradient boosting.'
  prefs: []
  type: TYPE_NORMAL
- en: Random forests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Random forests is a technique where you construct multiple trees, and then use
    those trees to learn the classification and regression models, but the results
    are aggregated from the trees to produce a final result.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1ba7f1c9-51bf-4f83-b967-7041953cbb9c.png)'
  prefs: []
  type: TYPE_IMG
- en: Random forests are an ensemble of random, uncorrelated, and fully-grown decision
    trees. The decision trees used in the random forest model are fully grown, thus,
    having low bias and high variance. The trees are uncorrelated in nature, which
    results in a maximum decrease in the variance. By uncorrelated, we imply that
    each decision tree in the random forest is given a randomly selected subset of
    features and a randomly selected subset of the dataset for the selected features.
  prefs: []
  type: TYPE_NORMAL
- en: The original paper describing random forests is available at the following link: [https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: The random forest technique does not reduce bias and as a result, has a slightly
    higher bias as compared to the individual trees in the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests were invented by Leo Breiman and have been trademarked by Leo
    Breiman and Adele Cutler. More information is available at the following link: [https://www.stat.berkeley.edu/~breiman/RandomForests](https://www.stat.berkeley.edu/~breiman/RandomForests).
  prefs: []
  type: TYPE_NORMAL
- en: Intuitively, in the random forest model, a large number of decision trees are
    trained on different samples of data, that either fit or overfit. By averaging
    the individual decision trees, overfitting cancels out.
  prefs: []
  type: TYPE_NORMAL
- en: Random forests seem similar to bagging, aka bootstrap aggregating, but they
    are different. In bagging, a random sample with replacement is selected to train
    every tree in the ensemble. The tree is trained on all the features. In random
    forests, the features are also sampled randomly, and at each candidate that is
    split, a subset of features is used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: For predicting values in case of regression problems, the random forest model
    averages the predictions from individual decision trees. For predicting classes
    in case of a classification problem, the random forest model takes a majority
    vote from the results of individual decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting explanation of random forests can be found at the following link: [https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/](https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/)
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient boosted trees are an ensemble of shallow trees (or weak learners).
    The shallow decision trees could be as small as a tree with just two leaves (also
    known as decision stump). The boosting methods help in reducing bias mainly but
    also help reduce variance slightly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Original papers by Breiman and Friedman who developed the idea of gradient
    boosting are available at following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Prediction Games and Arcing Algorithms* by Breiman, L at [https://www.stat.berkeley.edu/~breiman/games.pdf](https://www.stat.berkeley.edu/~breiman/games.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Arcing The Edge *by Breiman, L at [http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf](http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Greedy Function Approximation: A Gradient Boosting Machine* by Friedman, J. H.
    at [http://statweb.stanford.edu/~jhf/ftp/trebst.pdf](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Stochastic Gradient Boosting* by Friedman, J. H. at [https://statweb.stanford.edu/~jhf/ftp/stobst.pdf](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Intuitively, in the gradient boosting model, the decision trees in the ensemble
    are trained in several iterations as shown in the following image. A new decision
    tree is added at each iteration. Every additional decision tree is trained to
    improve the trained ensemble model in previous iterations. This is different from
    the random forest model where each decision tree is trained independently from
    the other decision trees in the ensemble.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a5d0ce4-b378-4921-8df5-c88b5ae2297b.png)'
  prefs: []
  type: TYPE_IMG
- en: The gradient boosting model has lesser number of trees as compared to the random
    forests model but ends up with a very large number of hyperparameters that need
    to be tuned to get a decent gradient boosting model.
  prefs: []
  type: TYPE_NORMAL
- en: An interesting explanation of gradient boosting can be found at the following
    link: [http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/).
  prefs: []
  type: TYPE_NORMAL
- en: Decision tree-based ensembles in TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we shall use the gradient boosted trees and random forest implementation
    as pre-made estimators in TensorFlow from the Google TensorFlow team. Let us learn
    the details of their implementation in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: TensorForest Estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorForest is a highly scalable implementation of random forests built by
    combining a variety of online HoeffdingTree algorithms with the extremely randomized
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google published the details of the TensorForest implementation in the following
    paper: *TensorForest: Scalable Random Forests on TensorFlow* by Thomas Colthurst,
    D. Sculley, Gibert Hendry, Zack Nado, presented at Machine Learning Systems Workshop
    at the Conference on **Neural Information Processing Systems** (**NIPS**) 2016\.
    The paper is available at the following link: [https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE).'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorForest estimators are used to implementing the following algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: More details of this algorithm implementation can be found in the TensorForest
    paper.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow boosted trees estimator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow Boosted Trees** (**TFBT**) is an improved scalable ensemble model
    built on top of generic gradient boosting trees.'
  prefs: []
  type: TYPE_NORMAL
- en: Google published the details of the TensorFlow boosted trees implementation
    in the following paper: *A scalable TensorFlow based framework for gradient boosting* by
    Natalia Ponomareva, Soroush Radpour, Gilbert Hendry, Salem Haykal, Thomas Colthurst,
    Petr Mitrichev, Alexander Grushetsky, presented at the European Conference on
    Machine Learning and Principles and Practice of Knowledge Discovery in Databases
    (ECML PKDD) 2017\. The paper is available at the following link: [http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf](http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf)[.](http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient boosting algorithm is implemented by various libraries such as
    `sklearn`, `MLLib`, and `XGBoost`. TensorFlow''s implementation is different from
    these implementations as described in the following table extracted from the TFBT
    research paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bce85b46-1f54-4c08-a435-80f5f5acee49.png)'
  prefs: []
  type: TYPE_IMG
- en: TFBT Research Paper from Google
  prefs: []
  type: TYPE_NORMAL
- en: The TFBT model can be extended by writing custom loss functions in TensorFlow.
    The differentiation for these custom loss functions is automatically provided
    by TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting exoplanets in outer space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the project explained in this chapter, we use the *Kepler labeled time series
    data* from Kaggle:[ https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data/home](https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data/home).
    This dataset is derived mainly from the Campaign 3 observations of the mission
    by NASA's Kepler space telescope.
  prefs: []
  type: TYPE_NORMAL
- en: In the dataset, column 1 values are the labels and columns 2 to 3198 values
    are the flux values over time. The training set has 5087 data points, 37 confirmed
    exoplanets, and 5050 non-exoplanet stars. The test set has 570 data points, 5
    confirmed exoplanets, and 565 non-exoplanet stars.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will carry out the following steps to download, and then preprocess our
    data to create the train and test datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Download the dataset using the Kaggle API. The following code will be used
    for the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The folder contains the following two files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Link the folder `datasets` to our home folder so we can access it from the
    `~/datasets/kaggle-kepler` path and then we define the folder path and list the
    contents of the folder through the Notebook to confirm if we have access to the
    data files through the Notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The ZIP file is just a leftover of the download process because the Kaggle API
    begins by downloading the ZIP file and then proceeds to unzip the contents in
    the same folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then read the two `.csv` data files in the `pandas` DataFrames named `train` and `test` respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The first five lines of the `training` and `test data` look similar to the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The training and test datasets have labels in the first column and 3197 features
    in the next columns. Now let us split the training and test data into labels and
    features with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we subtract `1` from the labels, since the TFBT estimator
    assumes labels starting with numerical zero while the features in the datasets
    are numbers 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the label and feature vectors for training and test data, let
    us build the boosted tree models.
  prefs: []
  type: TYPE_NORMAL
- en: Building a TFBT model for exoplanet detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we shall build the gradient boosted trees model for detecting
    exoplanets using the Kepler dataset. Let us follow these steps in the Jupyter
    Notebook to build and train the exoplanet finder model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will save the names of all the features in a vector with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then bucketize the feature columns into two buckets around the mean
    since the TFBT estimator only takes bucketed features with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we only have numeric bucketized features and no other kinds of features,
    we store them in the `all_features` variable with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then define the batch size and create a function that will provide
    inputs from the label and feature vectors created from the training data. For
    creating this function we use a convenience function `tf.estimator.inputs.pandas_input_fn()` provided
    by TensorFlow. We will use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, we will create another data input function that would be used to
    evaluate the model from the test features and label vectors and name it `eval_input_fn` using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define the number of trees to be created as `100` and the number of
    steps to be used for training as `100`. We also define the `BoostedTreeClassifier` as
    the `estimator` using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Since we are doing classification, hence we use the `BoostedTreesClassifier`,
    for regression problems where a value needs to be predicted, TensorFlow also has
    an `estimator` named `BoostedTreesRegressor`.
  prefs: []
  type: TYPE_NORMAL
- en: One of the parameters provided to the `estimator` function is `model_dir` that
    defines where the trained model would be stored. The estimators are built such
    that they look for the model in that folder in further invocations for using them
    for inference and prediction. We name the folder as `tfbtmodel` to save the model.
  prefs: []
  type: TYPE_NORMAL
- en: We have used the minimum number of models to define the `BoostedTreesClassifier`.
    Please look up the definition of this estimator in the TensorFlow API documentation
    to find various other parameters that can be provided to further customize the
    estimator.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output in the Jupyter Notebook describes the classifier estimator
    and its various settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Post this, we will train the model using the `train_input_fn` function that
    provides the exoplanets input data using 100 steps with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The Jupyter Notebook shows the following output to indicate the training in
    progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the `eval_input_fn` that provides batches from the `test` dataset to evaluate
    the model with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The Jupyter Notebook shows the following output as the progress of the evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that during the evaluation the estimator loads the parameters saved in
    the checkpoint file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the evaluation are stored in the `results` collection. Let us
    print each item in the `results` collection using the `for` loop in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The Notebook shows the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: It is observed that we achieve an accuracy of almost 99% with the first model
    itself. This is because the estimators are prewritten with several optimizations
    and we did not need to set various values of hyperparameters ourselves. For some
    datasets, the default hyperparameter values in the estimators will work out of
    the box, but for other datasets, you will have to play with various inputs to
    the estimators.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what a decision tree is and two broad classes of
    creating ensembles from the decision trees. The ensembles we took a look at were
    random forests and gradient boosting trees.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about the Kepler dataset from Kaggle competitions. We used the
    Kepler dataset to build an exoplanet detection model using TensorFlow's prebuilt
    estimator for gradient boosting trees known as the `BoostedTreesClassifier`. The
    `BoostedTreesClassifier` estimator is part of the machine learning toolkit recently
    released by the TensorFlow team. As for now, the TensorFlow team is working on
    releasing prebuilt estimators based on **support vector machine** (**SVM**) and
    extreme random forests as part of the `tf.estimators` API.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we shall learn how to use TensorFlow in the browser using
    the `TensorFlow.js` API for sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How is gradient boosting different from random forests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you improve the performance of random forests?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can you improve the performance of gradient boosting trees?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to ensure that gradient boosting trees and random forests do not overfit?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modify the model in this chapter with different parameters such as the number
    of trees, batch size, number of epochs and number of steps and observe their effect
    on training time and different levels of accuracy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[https://www.tensorflow.org/tutorials/](https://www.tensorflow.org/tutorials/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.stat.berkeley.edu/~breiman/RandomForests](https://www.stat.berkeley.edu/~breiman/RandomForests)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf](https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/](https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/](http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.stat.berkeley.edu/~breiman/games.pdf](https://www.stat.berkeley.edu/~breiman/games.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf](http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://statweb.stanford.edu/~jhf/ftp/trebst.pdf](http://statweb.stanford.edu/~jhf/ftp/trebst.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://statweb.stanford.edu/~jhf/ftp/stobst.pdf](https://statweb.stanford.edu/~jhf/ftp/stobst.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE](https://docs.google.com/viewer?a=v&pid=sites&srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf](http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
