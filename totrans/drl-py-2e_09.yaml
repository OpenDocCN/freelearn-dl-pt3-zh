- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Q Network and Its Variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, let's get started with one of the most popular **Deep Reinforcement
    Learning** (**DRL**) algorithms called **Deep Q Network** (**DQN**). Understanding
    DQN is very important as many of the state-of-the-art DRL algorithms are based
    on DQN. The DQN algorithm was first proposed by researchers at Google's DeepMind
    in 2013 in the paper *Playing Atari with Deep Reinforcement Learning.* They described
    the DQN architecture and explained why it was so effective at playing Atari games
    with human-level accuracy. We begin the chapter by learning what exactly a deep
    Q network is, and how it is used in reinforcement learning. Next, we will deep
    dive into the algorithm of DQN. Then we will learn how to implement DQN to play Atari
    games.
  prefs: []
  type: TYPE_NORMAL
- en: After learning about DQN, we will cover several variants of DQN, such as double
    DQN, DQN with prioritized experience replay, dueling DQN, and the deep recurrent
    Q network in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is DQN?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DQN algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing Atari games with DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DQN with prioritized experience replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dueling DQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The deep recurrent Q network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is DQN?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The objective of reinforcement learning is to find the optimal policy, that
    is, the policy that gives us the maximum return (the sum of rewards of the episode).
    In order to compute the policy, first we compute the Q function. Once we have
    the Q function, then we extract the policy by selecting an action in each state
    that has the maximum Q value. For instance, let''s suppose we have two states
    **A** and **B** and our action space consists of two actions; let the actions
    be *up* and *down*. So, in order to find which action to perform in state **A**
    and **B**, first we compute the Q value of all state-action pairs, as *Table 9.1*
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 9.1: Q-value of state-action pairs'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the Q value of all state-action pairs, then we select the action
    in each state that has the maximum Q value. So, we select the action *up* in state
    **A** and *down* in state **B** as they have the maximum Q value. We improve the
    Q function on every iteration and once we have the optimal Q function, then we
    can extract the optimal policy from it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s revisit our grid world environment, as shown in *Figure 9.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Grid world environment'
  prefs: []
  type: TYPE_NORMAL
- en: We learned that in the grid world environment, the goal of our agent is to reach
    state **I** from state **A** without visiting the shaded states, and in each state,
    the agent has to perform one of the four actions—*up*, *down*, *left*, *right*.
  prefs: []
  type: TYPE_NORMAL
- en: To compute the policy, first we compute the Q values of all state-action pairs.
    Here, the number of states is 9 (**A** to **I**) and we have 4 actions in our
    action space, so our Q table will consist of 9 x 4 = 36 rows containing the Q
    values of all possible state-action pairs. Once we obtain the Q values, then we
    extract the policy by selecting the action in each state that has the maximum
    Q value. But is it a good approach to compute the Q value exhaustively for all
    state-action pairs? Let's explore this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose we have an environment where we have 1,000 states and 50 possible
    actions in each state. In this case, our Q table will consist of 1,000 x 50 =
    50,000 rows containing the Q values of all possible state-action pairs. In cases
    like this, where our environment consists of a large number of states and actions,
    it will be very expensive to compute the Q values of all possible state-action
    pairs in an exhaustive fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of computing Q values in this way, can we approximate them using any
    function approximator, such as a neural network? Yes! We can parameterize our
    Q function by a parameter ![](img/B15558_09_001.png) and compute the Q value where
    the parameter ![](img/B15558_09_002.png) is just the parameter of our neural network.
    So, we just feed the state of the environment to a neural network and it will
    return the Q value of all possible actions in that state. Once we obtain the Q
    values, then we can select the best action as the one that has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s consider our grid world environment. As *Figure 9.2* shows,
    we just feed state **D** as an input to the network and it returns the Q value
    of all actions in state **D**, which are *up*, *down*, *left*, and *right*, as
    output. Then, we select the action that has the maximum Q value. Since action
    *right* has a maximum Q value, we select action *right* in the state **D**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Deep Q network'
  prefs: []
  type: TYPE_NORMAL
- en: Since we are using a neural network to approximate the Q value, the neural network
    is called the Q network, and if we use a deep neural network to approximate the
    Q value, then the deep neural network is called a **deep Q network** (**DQN**).
  prefs: []
  type: TYPE_NORMAL
- en: We can denote our Q function by ![](img/B15558_09_003.png), where the parameter
    ![](img/B15558_09_004.png) in subscript indicates that our Q function is parameterized
    by ![](img/B15558_09_004.png), and ![](img/B15558_09_006.png) is just the parameter
    of our neural network.
  prefs: []
  type: TYPE_NORMAL
- en: We initialize the network parameter ![](img/B15558_09_004.png) with random values
    and approximate the Q function (Q values), but since we initialized ![](img/B15558_09_008.png)
    with random values, the approximated Q function will not be optimal. So, we train
    the network for several iterations by finding the optimal parameter ![](img/B15558_09_002.png).
    Once we find the optimal ![](img/B15558_09_004.png), we will have the optimal
    Q function. Then we can extract the optimal policy from the optimal Q function.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, but how can we train our network? What about the training data and the
    loss function? Is it a classification or regression task? Now that we have a basic
    understanding of how DQN works, in the next section, we will get into the details
    and address all these questions.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will understand how exactly DQN works. We learned that we
    use DQN to approximate the Q value of all the actions in the given input state.
    The Q value is just a continuous number, so we are essentially using our DQN to
    perform a regression task.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, what about the training data? We use a buffer called a replay buffer to
    collect the agent's experience and, based on this experience, we train our network.
    Let's explore the replay buffer in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Replay buffer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We know that the agent makes a transition from a state *s* to the next state
    ![](img/B15558_09_011.png) by performing some action *a*, and then receives a
    reward *r*. We can save this transition information ![](img/B15558_09_012.png)
    in a buffer called a replay buffer or experience replay. The replay buffer is
    usually denoted by ![](img/B15558_09_013.png). This transition information is
    basically the agent''s experience. We store the agent''s experience over several
    episodes in the replay buffer. The key idea of using the replay buffer to store
    the agent''s experience is that we can train our DQN with experience (transition)
    sampled from the buffer. A replay buffer is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Replay buffer'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps help us to understand how we store the transition information
    in the replay buffer ![](img/B15558_09_014.png):'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_09_015.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each episode perform *step 3*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make a transition, that is, perform an action *a* in the state *s*, move to
    the next state ![](img/B15558_09_016.png), and receive the reward *r*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the transition information ![](img/B15558_09_017.png) in the replay buffer
    ![](img/B15558_09_018.png).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As explained in the preceding steps, we collect the agent''s transition information
    over many episodes and save it in the replay buffer. To understand this clearly,
    let''s consider our favorite grid world environment. Let''s suppose we have the
    following two episodes/trajectories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Episode 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Trajectory 1'
  prefs: []
  type: TYPE_NORMAL
- en: '**Episode 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: Trajectory 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, this information will be stored in the replay buffer, as *Figure 9.6*
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: Replay buffer'
  prefs: []
  type: TYPE_NORMAL
- en: As *Figure 9.6* shows, we store the transition information by stacking it sequentially
    one after another. We train the network by sampling a minibatch of transitions
    from the replay buffer. Wait! There is a small issue here. Since we are stacking
    up the agent's experience (transition) one after another sequentially, the agent's
    experience will be highly correlated. For example, as shown in the preceding figure,
    transitions will be correlated with the rows above and below. If we train our
    network with this correlated experience then our neural network will easily overfit.
    So, to combat this, we sample a random minibatch of transitions from the replay
    buffer and train the network.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the replay buffer is of limited size, that is, a replay buffer will
    store only a fixed amount of the agent's experience. So, when the buffer is full
    we replace the old experience with new experience. A replay buffer is usually
    implemented as a queue structure (first in first out) rather than a list. So,
    if the buffer is full when new experience comes in, we remove the old experience
    and add the new experience into the buffer.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned that we train our network by randomly sampling a minibatch of
    experience from the buffer. But how exactly does the training happen? How does
    our network learn to approximate the optimal Q function using this minibatch of
    samples? This is exactly what we discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We learned that in DQN, our goal is to predict the Q value, which is just a
    continuous value. Thus, in DQN we basically perform a regression task. We generally
    use the **mean squared error** (**MSE**) as the loss function for the regression
    task. MSE can be defined as the average squared difference between the target
    value and the predicted value, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_019.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *y* is the target value, ![](img/B15558_09_020.png) is the predicted value,
    and *K* is the number of training samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to use MSE in the DQN and train the network. We can train
    our network by minimizing the MSE between the target Q value and predicted Q value.
    First, how can we obtain the target Q value? Our target Q value should be the
    optimal Q value so that we can train our network by minimizing the error between
    the optimal Q value and predicted Q value. But how can we compute the optimal
    Q value? This is where the Bellman equation helps us. In *Chapter 3*, *The Bellman
    Equation and Dynamic Programming*, we learned that the optimal Q value can be
    obtained using the Bellman optimality equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where ![](img/B15558_09_022.png) represents the immediate reward *r* that we
    obtain while performing an action *a* in state *s* and moving to the next state
    ![](img/B15558_09_016.png), so we can just denote ![](img/B15558_09_024.png) by
    *r*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_025.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding equation, we can remove the expectation. We will approximate
    the expectation by sampling *K* number of transitions from the replay buffer and
    taking the average value; we will learn more about this in a while.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, according to the Bellman optimality equation, the optimal Q value is
    just the sum of the reward and the discounted maximum Q value of the next state-action
    pair, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, we can define our loss as the difference between the target value (the
    optimal Q value) and the predicted value (the Q value predicted by the DQN) and
    express the loss function *L* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_027.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substituting equation (1) in the preceding equation, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_028.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that we compute the predicted Q value using the network parameterized
    by ![](img/B15558_09_029.png). How can we compute the target value? That is, we
    learned that the target value is the sum of the reward and the discounted maximum
    Q value of the next state-action pair. How do we compute the Q value of the next
    state-action pair?
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to the predicted Q value, we can compute the Q value of the next state-action
    pair in the target using the same DQN parameterized by ![](img/B15558_09_001.png).
    So, we can rewrite our loss function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_031.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown, both the target value and the predicted Q value are parameterized
    by ![](img/B15558_09_006.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of computing the loss as just the difference between the target Q value
    and the predicted Q value, we use MSE as our loss function. We learned that we
    store the agent''s experience in a buffer called a replay buffer. So, we randomly
    sample a minibatch of *K* number of transitions ![](img/B15558_09_012.png) from
    the replay buffer and train the network by minimizing the MSE, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Loss function of DQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, our loss function can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For simplicity of notation, we can denote the target value by *y* and rewrite
    the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Where ![](img/B15558_09_036.png). We have learned that the target value is
    just the sum of the reward and the discounted maximum Q value of the next state-action
    pair. But what if the next state ![](img/B15558_03_034.png) is a terminal state?
    If the next state ![](img/B15558_03_034.png) is terminal then we cannot compute
    the Q value as we don''t take any action in the terminal state, so in that case,
    the target value will be just the reward, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, our loss function is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_035.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We train our network by minimizing the loss function. We can minimize the loss
    function by finding the optimal parameter ![](img/B15558_09_002.png). So, we use
    gradient descent to find the optimal parameter ![](img/B15558_09_042.png). We
    compute the gradient of our loss function ![](img/B15558_09_043.png) and update
    our network parameter ![](img/B15558_09_044.png) as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_045.png)'
  prefs: []
  type: TYPE_IMG
- en: Target network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the last section, we learned that we train the network by minimizing the
    loss function, which is the MSE between the target value and the predicted value,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_034.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, there is a small issue with our loss function. We have learned that
    the target value is just the sum of the reward and the discounted maximum Q value
    of the next state-action pair. We compute this Q value of the next state-action
    pair in the target and predicted Q values using the same network parameterized
    by ![](img/B15558_09_001.png), as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_20.png)'
  prefs: []
  type: TYPE_IMG
- en: The problem is since the target and predicted value depend on the same parameter
    ![](img/B15558_09_048.png), this will cause instability in the MSE and the network
    learns poorly. It also causes a lot of divergence during training.
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand this with a simple example. We will take arbitrary numbers
    to make it easier to understand. We know that we try to minimize the difference
    between the target value and the predicted value. So, on every iteration, we compute the
    gradient of loss and update our network parameter ![](img/B15558_09_001.png) so
    that we can make our predicted value the same as the target value.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose in iteration 1, the target value is 13 and the predicted value
    is 11\. So, we update our parameter ![](img/B15558_09_002.png) to match the predicted
    value to the target value, which is 13\. But in the next iteration, the target
    value changes to 15 and the predicted value becomes 13 since we updated our network
    parameter ![](img/B15558_09_002.png). So, again we update our parameter ![](img/B15558_09_002.png)
    to match the predicted value to the target value, which is now 15\. But in the
    next iteration, the target value changes to 17 and the predicted value becomes
    15 since we updated our network parameter ![](img/B15558_09_053.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'As *Table 9.2* shows, on every iteration, the predicted value tries to be the
    same as the target value, which keeps on changing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 9.2: Target and predicted value'
  prefs: []
  type: TYPE_NORMAL
- en: This is because the predicted and target values both depend on the same parameter
    ![](img/B15558_09_054.png). If we update ![](img/B15558_09_054.png), then both
    the target and predicted values change. Thus, the predicted value keeps on trying
    to be the same as the target value, but the target value keeps on changing due
    to the update on the network parameter ![](img/B15558_09_056.png).
  prefs: []
  type: TYPE_NORMAL
- en: How can we avoid this? Can we freeze the target value for a while and compute
    only the predicted value so that our predicted value matches the target value?
    Yes! To do this, we introduce another neural network called a target network for
    computing the Q value of the next state-action pair in the target. The parameter
    of the target network is represented by ![](img/B15558_09_057.png). So, our main
    deep Q network, which is used for predicting Q values, learns the optimal parameter
    ![](img/B15558_09_054.png) using gradient descent. The target network is frozen
    for a while and then the target network parameter ![](img/B15558_09_059.png) is
    updated by just copying the main deep Q network parameter ![](img/B15558_09_054.png).
    Freezing the target network for a while and then updating its parameter ![](img/B15558_09_061.png)
    with the main network parameter ![](img/B15558_09_054.png) stabilizes the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, now our loss function can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_063.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, the Q value of the next state-action pair in the target is computed by
    the target network parameterized by ![](img/B15558_09_064.png), and the predicted
    Q value is computed by our main network parameterized by ![](img/B15558_09_065.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For notation simplicity, we can represent our target value by *y* and rewrite
    the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_035.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_09_067.png).
  prefs: []
  type: TYPE_NORMAL
- en: We have learned about several concepts concerning DQN, including the experience
    replay, the loss function, and the target network. In the next section, we will
    put all these concepts together and see how DQN works.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, we initialize the main network parameter ![](img/B15558_09_054.png) with
    random values. We learned that the target network parameter is just a copy of
    the main network. So, we initialize the target network parameter ![](img/B15558_09_069.png)
    by just copying the main network parameter ![](img/B15558_09_054.png). We also
    initialize the replay buffer ![](img/B15558_09_071.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, for each step in the episode, we feed the state of the environment to
    our network and it outputs the Q values of all possible actions in that state.
    Then, we select the action that has the maximum Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_072.png)'
  prefs: []
  type: TYPE_IMG
- en: If we only select the action that has the highest Q value, then we will not
    explore any new actions. So, to avoid this, we select actions using the epsilon-greedy
    policy. With the epsilon-greedy policy, we select a random action with probability
    epsilon and with probability 1-epsilon, we select the best action that has the
    maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, since we initialized our network parameter ![](img/B15558_09_056.png)
    with random values, the action we select by taking the maximum Q value will not
    be the optimal action. But that's okay, we simply perform the selected action,
    move to the next state, and obtain the reward. If the action is good then we will
    receive a positive reward, and if it is bad then the reward will be negative.
    We store all this transition information ![](img/B15558_09_074.png) in the replay
    buffer ![](img/B15558_09_075.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we randomly sample a minibatch of *K* transitions from the replay buffer
    and compute the loss. We have learned that our loss function is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_035.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *y*[i] is the target value, that is, ![](img/B15558_09_067.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the initial iterations, the loss will be very high since our network parameter
    ![](img/B15558_09_008.png) is just random values. To minimize the loss, we compute
    the gradients of the loss and update our network parameter ![](img/B15558_09_054.png)
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_045.png)'
  prefs: []
  type: TYPE_IMG
- en: We don't update the target network parameter ![](img/B15558_09_059.png) in every
    time step. We freeze the target network parameter ![](img/B15558_09_059.png) for
    several time steps and then we copy the main network parameter ![](img/B15558_09_054.png)
    to the target network parameter ![](img/B15558_09_084.png).
  prefs: []
  type: TYPE_NORMAL
- en: We keep repeating the preceding steps for several episodes to approximate the
    optimal Q value. Once we have the optimal Q value, we extract the optimal policy
    from them. To give us a more detailed understanding, the algorithm of DQN is given
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The DQN algorithm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The DQN algorithm is given in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_09_056.png) with random
    values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_09_086.png) by copying
    the main network parameter ![](img/B15558_09_087.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_09_088.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes, perform *step 5*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0, . . ., *T*-1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Observe the state *s* and select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select random action *a* and with probability
    1-epsilon, select the action ![](img/B15558_09_072.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action and move to the next state ![](img/B15558_03_021.png)
    and obtain the reward *r*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the transition information in the replay buffer ![](img/B15558_09_075.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_09_092.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target value, that is, ![](img/B15558_09_067.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss, ![](img/B15558_09_035.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the gradients of the loss and update the main network parameter ![](img/B15558_09_054.png)
    using gradient descent: ![](img/B15558_09_096.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Freeze the target network parameter ![](img/B15558_09_084.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_098.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have understood how DQN works, in this next section, we will learn
    how to implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Playing Atari games using DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Atari 2600 is a popular video game console from a game company called Atari.
    The Atari game console provides several popular games, such as Pong, Space Invaders,
    Ms. Pac-Man, Breakout, Centipede, and many more. In this section, we will learn
    how to build a DQN for playing the Atari games. First, let's explore the architecture
    of the DQN for playing the Atari games.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture of the DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the Atari environment, the image of the game screen is the state of the environment.
    So, we just feed the image of the game screen as input to the DQN and it returns
    the Q values of all the actions in the state. Since we are dealing with images,
    instead of using a vanilla deep neural network for approximating the Q value,
    we can use a **convolutional neural network (CNN)** since it is very effective
    for handling images.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, now our DQN is a CNN. We feed the image of the game screen (the game state)
    as input to the CNN, and it outputs the Q values of all the actions in the state.
  prefs: []
  type: TYPE_NORMAL
- en: 'As *Figure 9.8* shows, given the image of the game screen, the convolutional
    layers extract features from the image and produce a feature map. Next, we flatten
    the feature map and feed the flattened feature map as input to the feedforward
    network. The feedforward network takes this flattened feature map as input and
    returns the Q values of all the actions in the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Architecture of a DQN'
  prefs: []
  type: TYPE_NORMAL
- en: Note that we don't perform a pooling operation. A pooling operation is useful
    when we perform tasks such as object detection, image classification, and so on
    where we don't consider the position of the object in the image and we just want
    to know whether the desired object is present in the image. For example, if we
    want to identify whether there is a dog in an image, we only look for whether
    a dog is present in the image and we don't check the position of the dog in the
    image. Thus, in this case, a pooling operation is used to identify whether there
    is a dog in the image irrespective of the position of the dog.
  prefs: []
  type: TYPE_NORMAL
- en: But in our setting, a pooling operation should not be performed because to understand
    the current game state, the position is very important. For example, in the Pong,
    we just don't want to classify if there is a ball on the game screen. We want
    to know the position of the ball so that we can make a better action. Thus, we
    don't include the pooling operation in our DQN architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood the architecture of the DQN to play Atari games,
    in the next section, we will start implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting hands-on with the DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s implement the DQN to play the Ms Pacman game. First, let''s import the
    necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create the Ms Pacman game environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the state size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the number of actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Preprocess the game screen
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned that we feed the game state (an image of the game screen) as input
    to the DQN, which is a CNN, and it outputs the Q values of all the actions in
    the state. However, directly feeding the raw game screen image is not efficient,
    since the raw game screen size will be 210 x 160 x 3\. This will be computationally
    expensive.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, we preprocess the game screen and then feed the preprocessed
    game screen to the DQN. First, we crop and resize the game screen image, convert
    the image to grayscale, normalize it, and then reshape the image to 88 x 80 x
    1\. Next, we feed this preprocessed game screen image as input to the CNN, which
    returns the Q values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s define a function called `preprocess_state`, which takes the game
    state (image of the game screen) as an input and returns the preprocessed game
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Crop and resize the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the image to grayscale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Improve the image contrast:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Reshape and return the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Defining the DQN class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s define the class called DQN where we will implement the DQN algorithm.
    For a clear understanding, let''s look into the code line by line. You can also
    access the complete code from the GitHub repository of the book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Defining the init method
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: First, let's define the `init` method
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the state size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the action size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the discount factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the epsilon value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the update rate at which we want to update the target network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the main network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the target network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the weights of the main network to the target network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Building the DQN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s build the DQN. We have learned that to play Atari games, we use
    a CNN as the DQN, which takes the image of the game screen as an input and returns
    the Q values. We define the DQN with three convolutional layers. The convolutional
    layers extract the features from the image and output the feature maps, and then
    we flatten the feature map obtained by the convolutional layers and feed the flattened
    feature maps to the feedforward network (the fully connected layer), which returns
    the Q values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the first convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the second convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the third convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Flatten the feature maps obtained as a result of the third convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Feed the flattened maps to the fully connected layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model with loss as MSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Return the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Storing the transition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We have learned that we train the DQN by randomly sampling a minibatch of transitions
    from the replay buffer. So, we define a function called `store_transition`, which
    stores the transition information in the replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Defining the epsilon-greedy policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We learned that in DQN, to take care of exploration-exploitation trade-off,
    we select action using the epsilon-greedy policy. So, now we define the function
    called `epsilon_greedy` for selecting an action using the epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Define the training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now let''s define a function called `train` for the training network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample a minibatch of transitions from the replay buffer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the target value using the target network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the predicted value using the main network and store the predicted
    value in the `Q_values`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the target value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the main network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Updating the target network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s define the function called `update_target_network` for updating
    the target network weights by copying from the main network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Training the DQN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s train the network. First, let''s set the number of episodes we
    want to train the network for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the number of time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the batch size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of past game screens we want to consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the DQN class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Set done to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `time_step`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'For each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Set `Return` to `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Preprocess the game screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'For each step in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the target network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Preprocess the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the transition information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the current state to the next state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the return value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'If the episode is done, then print the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'If the number of transitions in the replay buffer is greater than the batch
    size, then train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'By rendering the environment, we can also observe how the agent learns to play
    the game over a series of episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: DQN agent learning to play'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how DQNs work and how to build a DQN to play Atari
    games, in the next section, we will learn an interesting variant of DQN called
    the double DQN.
  prefs: []
  type: TYPE_NORMAL
- en: The double DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have learned that in DQN, the target value is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_099.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the problems with a DQN is that it tends to overestimate the Q value
    of the next state-action pair in the target:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_22.png)'
  prefs: []
  type: TYPE_IMG
- en: This overestimation is due to the presence of the max operator. Let's see how
    this overestimation happens with an example. Suppose we are in a state ![](img/B15558_09_100.png)
    and we have three actions *a*[1], *a*[2], and *a*[3]. Assume *a*[3] is the optimal
    action in the state ![](img/B15558_09_101.png). When we estimate the Q values
    of all the actions in state ![](img/B15558_03_004.png), the estimated Q value
    will have some noise and differ from the actual value. Say, due to the noise,
    action *a*[2] will get a higher Q value than the optimal action *a*[3].
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the target value is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_103.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, if we select the best action as the one that has the maximum value then
    we will end up selecting the action *a*[2] instead of optimal action *a*[3], as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_104.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, how can we get rid of this overestimation? We can get rid of this overestimation
    by just modifying our target value computation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_105.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can observe, now we have two Q functions in our target value computation.
    One Q function parameterized by the main network parameter ![](img/B15558_09_106.png)
    is used for action selection, and the other Q function parameterized by the target
    network parameter ![](img/B15558_09_107.png) is used for Q value computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand the preceding equation by breaking it down into two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Action selection**: First, we compute the Q values of all the next state-action
    pairs using the main network parameterized by ![](img/B15558_09_087.png), and
    then we select action ![](img/B15558_09_109.png), which has the maximum Q value:![](img/B15558_09_23.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Q value computation**: Once we have selected action ![](img/B15558_09_110.png),
    then we compute the Q value using the target network parameterized by ![](img/B15558_09_111.png)
    for the selected action ![](img/B15558_09_112.png):![](img/B15558_09_24.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s understand this with an example. Let''s suppose state ![](img/B15558_03_021.png)
    is *E*, then we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_114.png)'
  prefs: []
  type: TYPE_IMG
- en: 'First, we compute the Q values of all actions in state *E* using the main network
    parameterized by ![](img/B15558_09_054.png), and then we select the action that
    has the maximum Q value. Let''s suppose the action that has the maximum Q value
    is *right*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we can compute the Q value using the target network parameterized by ![](img/B15558_09_061.png)
    with the action selected by the main network, which is *right*. Thus, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_117.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Still not clear? The difference between how we compute the target value in
    DQN and double DQN is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Difference between a DQN and double DQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we learned that in a double DQN, we compute the target value using two
    Q functions. One Q function parameterized by the main network parameter ![](img/B15558_09_118.png)
    used for selecting the action that has the maximum Q value, and the other Q function
    parameterized by target network parameter ![](img/B15558_09_059.png) computes
    the Q value using the action selected by the main network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_105.png)'
  prefs: []
  type: TYPE_IMG
- en: Apart from target value computation, double DQN works exactly the same as DQN.
    To give us more clarity, the algorithm of double DQN is given in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The double DQN algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The algorithm of double DQN is shown here. As we can see, except the target
    value computation (the bold step), the rest of the steps are exactly the same
    as in the DQN:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the main network parameter ![](img/B15558_09_087.png) with random
    values
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the target network parameter ![](img/B15558_09_086.png) by copying
    the main network parameter ![](img/B15558_09_123.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the replay buffer ![](img/B15558_09_124.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For *N* number of episodes repeat *step 5*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step in the episode, that is, for *t* = 0, . . . , *T*-1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Observe the state *s* and select an action using the epsilon-greedy policy,
    that is, with probability epsilon, select a random action *a* with probability
    1-epsilon; select the action: ![](img/B15558_09_072.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Perform the selected action, move to the next state ![](img/B15558_09_126.png),
    and obtain the reward *r*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store the transition information in the replay buffer ![](img/B15558_09_075.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly sample a minibatch of *K* transitions from the replay buffer ![](img/B15558_09_128.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Compute the target value, that is,** ![](img/B15558_09_129.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the loss, ![](img/B15558_09_035.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the gradients of the loss and update the main network parameter ![](img/B15558_09_054.png)
    using gradient descent: ![](img/B15558_09_132.png)'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Freeze the target network parameter ![](img/B15558_09_133.png) for several time
    steps and then update it by just copying the main network parameter ![](img/B15558_09_087.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have learned how the double DQN works, in the next section, we will
    learn about an interesting variant of DQN called DQN with prioritized experience
    replay.
  prefs: []
  type: TYPE_NORMAL
- en: DQN with prioritized experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned that in DQN, we randomly sample a minibatch of *K* transitions from
    the replay buffer and train the network. Instead of doing this, can we assign
    some priority to each transition in the replay buffer and sample the transitions
    that had high priority for learning?
  prefs: []
  type: TYPE_NORMAL
- en: Yes, but first, why do we need to assign priority for the transition, and how
    can we decide which transition should be given more priority than the others?
    Let's explore this more in detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'The TD error ![](img/B15558_09_135.png) is the difference between the target
    value and the predicted value, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_136.png)'
  prefs: []
  type: TYPE_IMG
- en: A transition that has a high TD error implies that the transition is not correct,
    and so we need to learn more about that transition to minimize the error. A transition
    that has a low TD error implies that the transition is already good. We can always
    learn more from our mistakes rather than only focusing on what we are already
    good at, right? Similarly, we can learn more from the transitions that have a
    high TD error than those that have a low TD error. Thus, we can assign more priority
    to the transitions that have a high TD error and less priority to transitions
    that have a low TD error.
  prefs: []
  type: TYPE_NORMAL
- en: 'We know that the transition information consists of ![](img/B15558_09_137.png),
    and along with this information, we also add priority *p* and store the transition
    with the priority in our replay buffer as ![](img/B15558_09_138.png). The following
    figure shows the replay buffer containing transitions along with the priority:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: Prioritized replay buffer'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how to prioritize our transitions using the
    TD error based on two different types of prioritization methods.
  prefs: []
  type: TYPE_NORMAL
- en: Types of prioritization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can prioritize our transition using the following two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Proportional prioritization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rank-based prioritization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Proportional prioritization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We learned that the transition can be prioritized using the TD error, so the
    priority *p* of the transition *i* will be just its TD error:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_139.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Note that we take the absolute value of the TD error as a priority to keep
    the priority positive. Okay, what about a transition that has a TD error of zero?
    Say we have a transition *i* and its TD error is 0, then the priority of the transition
    *i* will just be 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_140.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But setting the priority of the transition to zero is not desirable, and if
    we set the priority of a transition to zero then that particular transition will
    not be used in our training at all. So, to avoid this issue, we will add a small
    value called epsilon to our TD error. So, even if the TD error is zero, we will
    still have a small priority due to the epsilon. To be more precise, adding an
    epsilon to the TD error guarantees that there will be no transition with zero
    priority. Thus, our priority can be modified as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_141.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of having the priority as a raw number, we can convert it into a probability
    so that we will have priorities ranging from 0 to 1\. We can convert the priority
    to a probability as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_142.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation calculates the probability *P* of the transition *i*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Can we also control the amount of prioritization? That is, instead of sampling
    only the prioritized transition, can we also take a random transition? Yes! To
    do this, we introduce a new parameter called ![](img/B15558_09_143.png) and rewrite
    our equation as follows. When the value of ![](img/B15558_07_025.png) is high,
    say 1, then we sample only the transitions that have high priority and when the
    value of ![](img/B15558_07_025.png) is low, say 0, then we sample a random transition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_146.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we have learned how to assign priority to a transition using the proportional
    prioritization method. In the next section, we will learn another prioritization
    method called rank-based prioritization.
  prefs: []
  type: TYPE_NORMAL
- en: Rank-based prioritization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Rank-based prioritization is the simplest type of prioritization. Here, we assign
    priority based on the rank of a transition. What is the rank of a transition?
    The rank of a transition *i* can be defined as the location of the transition
    in the replay buffer where the transitions are sorted from high TD error to low
    TD error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we can define the priority of the transition *i* using rank as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_147.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Just as we learned in the previous section, we convert the priority into probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similar to what we learned in the previous section, we can add a parameter
     to control the amount of prioritization and express our final equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_146.png)'
  prefs: []
  type: TYPE_IMG
- en: Correcting the bias
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have learned how to prioritize the transitions using two methods—proportional
    prioritization and rank-based prioritization. But the problem with these methods
    is that we will be highly biased towards the samples that have high priority.
    That is, when we give more importance to samples that have a high TD error, it
    essentially means that we are learning only from a subset of samples that have
    a high TD error.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, what''s the issue with this? It will lead to the problem of overfitting,
    and our agent will be highly biased to those transitions that have a high TD error.
    To combat this, we use importance weights *w*. The importance weights help us
    to reduce the weights of transitions that have occurred many times. The importance
    weight *w* of the transition *i* can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_150.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding expression, *N* denotes the length of our replay buffer and
    *P*(*i*) denotes the probability of the transition *i*. Okay, what's that parameter
    ![](img/B15558_09_151.png)? It controls the importance weight. We start off with
    small values of ![](img/B15558_09_152.png), from 0.4 and anneal it toward 1.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in this section, we have learned how to prioritize transitions in DQN
    with prioritized experience replay. In the next section, we will learn about another
    interesting variant of DQN called dueling DQN.
  prefs: []
  type: TYPE_NORMAL
- en: The dueling DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before going ahead, let''s learn about one of the most important functions
    in reinforcement learning, called the advantage function. The advantage function
    is defined as the difference between the Q function and the value function, and
    it is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_153.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Okay, but what''s the use of an advantage function? What does it signify? First,
    let''s recall the Q function and the value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Q function**: The Q function gives the expected return an agent would obtain
    starting from state *s*, performing action *a*, and following the policy ![](img/B15558_03_008.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Value function**: The value function gives the expected return an agent would obtain
    starting from state *s* and following the policy ![](img/B15558_04_032.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now if we think intuitively, what's the difference between the Q function and
    the value function? The Q function gives us the value of a state-action pair,
    while the value function gives the value of a state irrespective of the action.
    Now, the difference between the Q function and the value function tells us how
    good the action *a* is compared to the average actions in state *s*.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the advantage function tells us that in state *s*, how good the action
    *a* is compared to the average actions. Now that we have understood what the advantage
    function is, let's see why and how we can make use of it in the DQN.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dueling DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have learned that in a DQN, we feed the state as input and our network computes
    the Q value for all actions in that state. Instead of computing the Q values in
    this way, can we compute the Q values using the advantage function? We have learned
    that the advantage function is given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_153.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can rewrite the preceding equation as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_157.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see from the preceding equation, we can compute the Q value just by
    adding the value function and the advantage function together. Wait! Why do we
    have to do this? What's wrong with computing the Q value directly?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we are in some state *s* and we have 20 possible actions to
    perform in this state. Computing the Q values of all these 20 actions in state
    *s* is not going to be useful because most of the actions will not have any effect
    on the state, and also most of the actions will have a similar Q value. What do
    we mean by that? Let''s understand this with the grid world environment shown
    in *Figure 9.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Grid world environment'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the agent is in state **A**. In this case, what is the use of
    computing the Q value of the action *up* in state **A**? Moving *up* will have
    no effect in state **A**, and it's not going to take the agent anywhere. Similarly,
    think of an environment where our action space is huge, say 100\. In this case,
    most of the actions will not have any effect in the given state. Also, when the
    action space is large, most of the actions will have a similar Q value.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's talk about the value of a state. Note that not all the states are
    important for an agent. There could be a state that always gives a bad reward
    no matter what action we perform. In that case, it is not useful to compute the
    Q value of all possible actions in the state if we know that the state is always
    going to give us a bad reward.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to solve this we can compute the Q function as the sum of the value function
    and the advantage function. That is, with the value function, we can understand
    whether a state is valuable or not without computing the values of all actions
    in the state. And with the advantage function, we can understand whether an action
    is really good or it just gives us the same value as all the other actions.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic idea of dueling DQN, let's explore the architecture
    of dueling DQN in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a dueling DQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have learned that in a dueling DQN, the Q values can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_157.png)'
  prefs: []
  type: TYPE_IMG
- en: 'How can we design our neural network to emit Q values in this way? We can break
    the final layer of our network into two streams. The first stream computes the
    value function and the second stream computes the advantage function. Given any
    state as input, the value stream gives the value of a state, while the advantage
    stream gives the advantage of all possible actions in the given state. For instance,
    as *Figure 9.13* shows, we feed the game state (game screen) as an input to the
    network. The value stream computes the value of a state while the advantage stream
    computes the advantage values of all actions in the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: Architecture of a dueling DQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that we compute the Q value by adding the state value and the advantage
    value together, so we combine the value stream and the advantage stream using
    another layer called the aggregate layer, and compute the Q value as *Figure 9.14*
    shows. Thus, the value stream computes the state value, the advantage stream computes
    the advantage value, and the aggregate layer sums these streams and computes the
    Q value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: Architecture of a dueling DQN including an aggregate layer'
  prefs: []
  type: TYPE_NORMAL
- en: But there is a small issue here. Just summing the state value and advantage
    value in the aggregate layer and computing the Q value leads us to a problem of
    identifiability.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to combat this problem, we make the advantage function to have zero advantage
    for the selected action. We can achieve this by subtracting the average advantage
    value, that is, the average advantage of all actions in the action space, as shown
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_159.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_09_160.png) denotes the length of the action space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we can write our final equation for computing the Q value with parameters
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_161.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the preceding equation, ![](img/B15558_09_054.png) is the parameter of the
    convolutional network, ![](img/B15558_06_016.png) is the parameter of the value
    stream, and ![](img/B15558_09_143.png) is the parameter of the advantage stream.
    After computing the Q value, we can select the action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_165.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the only difference between a dueling DQN and DQN is that in a dueling
    DQN, instead of computing the Q values directly, we compute them by combining
    the state value and the advantage value.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explore another variant of DQN called deep recurrent
    Q network.
  prefs: []
  type: TYPE_NORMAL
- en: The deep recurrent Q network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**The deep recurrent Q network** (**DRQN**) is just the same as a DQN but with
    recurrent layers. But what''s the use of recurrent layers in DQN? To answer this
    question, first, let''s understand the problem called **Partially Observable Markov
    Decision Process** (**POMDP**).'
  prefs: []
  type: TYPE_NORMAL
- en: An environment is called a POMDP when we have a limited set of information available
    about the environment. So far, in the previous chapters, we have seen a fully
    observable MDP where we know all possible actions and states—although we might
    be unaware of transition and reward probabilities, we had complete knowledge of
    the environment. For example, in the frozen lake environment, we had complete
    knowledge of all the states and actions of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: But most real-world environments are only partially observable; we cannot see
    all the states. For instance, consider an agent learning to walk in a real-world
    environment. In this case, the agent will not have complete knowledge of the environment
    (the real world); it will have no information outside its view.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in a POMDP, states provide only partial information, but keeping the information
    about past states in the memory will help the agent to understand more about the
    nature of the environment and find the optimal policy. Thus, in POMDP, we need
    to retain information about past states in order to take the optimal action.
  prefs: []
  type: TYPE_NORMAL
- en: So, can we take advantage of recurrent neural networks to understand and retain
    information about the past states as long as it is required? Yes, the **Long Short-Term
    Memory Recurrent Neural Network** (**LSTM RNN**) is very useful for retaining,
    forgetting, and updating the information as required. So, we can use the LSTM
    layer in the DQN to retain information about the past states as long as it is
    required. Retaining information about the past states helps when we have the problem
    of POMDP.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of why we need DRQN and how it solves
    the problem of POMDP, in the next section, we will look into the architecture
    of DRQN.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a DRQN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '*Figure 9.15* shows the architecture of a DRQN. As we can see, it is similar
    to the DQN architecture except that it has an LSTM layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15: Architecture of a DRQN'
  prefs: []
  type: TYPE_NORMAL
- en: We pass the game screen as an input to the convolutional layer. The convolutional
    layer convolves the image and produces a feature map. The resulting feature map
    is then passed to the LSTM layer. The LSTM layer has memory to hold information.
    So, it retains information about important previous game states and updates its
    memory over time steps as required. Then, we feed the hidden state from the LSTM
    layer to the fully connected layer, which outputs the Q value.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.16* helps us to understand how exactly DRQN works. Let''s suppose
    we need to compute the Q value for the state *s*[t] and the action *a*[t]. Unlike
    DQN, we don''t just compute the Q value as Q(*s*[t], *a*[t]) directly. As we can
    see, along with the current state *s*[t] we also use the hidden state *h*[t] to
    compute the Q value. The reason for using the hidden state is that it holds information
    about the past game states in memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we are using the LSTM cells, the hidden state *h*[t] will consist of
    information about the past game states in the memory as long as it is required:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_09_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16: Architecture of DRQN'
  prefs: []
  type: TYPE_NORMAL
- en: Except for this change, DRQN works just like DQN. Wait. What about the replay
    buffer? In DQN, we learned that we store the transition information in the replay
    buffer and train our network by sampling a minibatch of experience. We also learned
    that the transition information is placed sequentially in the replay buffer one
    after another, so to avoid the correlated experience, we randomly sample a minibatch
    of experience from the replay buffer and train the network.
  prefs: []
  type: TYPE_NORMAL
- en: But in the case of a DRQN, we need sequential information so that our network
    can retain information from past game states. Thus we need sequential information
    but also we don't want to overfit the network by training with correlated experience.
    How can we achieve this?
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this, in a DRQN, we randomly sample a minibatch of episodes rather
    than a random minibatch of transitions. That is, we know that in the episode,
    we will have transition information that follows sequentially, so we take a random
    minibatch of episodes and in each episode we will have the transition information
    that is placed sequentially. So, in this way, we can accommodate both randomization
    and also the transition information that follows one another. This is called **bootstrapped
    sequential updates**.
  prefs: []
  type: TYPE_NORMAL
- en: After sampling the minibatch of episodes randomly, then we can train the DRQN
    just like we trained the DQN network by minimizing the MSE loss. To learn more,
    you can refer to the DRQN paper given in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by learning what deep Q networks are and how they are
    used to approximate the Q value. We learned that in a DQN, we use a buffer called
    the replay buffer to store the agent's experience. Then, we randomly sample a
    minibatch of experience from the replay buffer and train the network by minimizing
    the MSE. Moving on, we looked at the algorithm of DQN in more detail, and then
    we learned how to implement DQN to play Atari games.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we learned that the DQN overestimates the target value due to
    the max operator. So, we used double DQN, where we have two Q functions in our
    target value computation. One Q function parameterized by the main network parameter
    ![](img/B15558_09_042.png) is used for action selection, and the other Q function
    parameterized by the target network parameter ![](img/B15558_09_059.png) is used
    for Q value computation.
  prefs: []
  type: TYPE_NORMAL
- en: Going ahead, we learned about the DQN with prioritized experience replay, where
    the transition is prioritized based on the TD error. We explored two different
    types of prioritization methods called proportional prioritization and rank-based
    prioritization.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we learned about another interesting variant of DQN called dueling DQN.
    In dueling DQN, instead of computing Q values directly, we compute them using
    two streams called the value stream and the advantage stream.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we learned about DRQN and how they solve the problem
    of partially observable Markov decision processes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about another popular algorithm called policy
    gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s evaluate our understanding of DQN and its variants by answering the
    following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need a DQN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the replay buffer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need the target network?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does a double DQN differ from a DQN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we have to prioritize the transitions?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the advantage function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we need LSTM layers in a DRQN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, we can refer to the following papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Playing Atari with Deep Reinforcement Learning** by *Volodymyr Mnih, et al.*,
    [https://arxiv.org/pdf/1312.5602.pdf](https://arxiv.org/pdf/1312.5602.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Reinforcement Learning with Double Q-learning** by *Hado van Hasselt,
    Arthur Guez, David Silver*, [https://arxiv.org/pdf/1509.06461.pdf](https://arxiv.org/pdf/1509.06461.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prioritized Experience Replay** by *Tom Schaul, John Quan, Ioannis Antonoglou
    and David Silver*, [https://arxiv.org/pdf/1511.05952.pdf](https://arxiv.org/pdf/1511.05952.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dueling Network Architectures for Deep Reinforcement Learning** by *Ziyu
    Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, Nando de Freitas*,
    [https://arxiv.org/pdf/1511.06581.pdf](https://arxiv.org/pdf/1511.06581.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deep Recurrent Q-Learning for Partially Observable MDPs** by *Matthew Hausknecht
    and Peter Stone*, [https://arxiv.org/pdf/1507.06527.pdf](https://arxiv.org/pdf/1507.06527.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
