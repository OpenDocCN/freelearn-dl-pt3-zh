<html><head></head><body>
  <div id="_idContainer820">
    <h1 class="chapterNumber">6</h1>
    <h1 id="_idParaDest-149" class="chapterTitle">Case Study – The MAB Problem</h1>
    <p class="normal">So far in the previous chapters, we have learned the fundamental concepts of reinforcement learning and also several interesting reinforcement learning algorithms. We learned about a model-based method called dynamic programming and a model-free method called the Monte Carlo method, and then we learned about the temporal difference method, which combines the advantages of dynamic programming and the Monte Carlo method.</p>
    <p class="normal">In this chapter, we will learn about one of the classic problems in reinforcement learning called the <strong class="keyword">multi-armed bandit</strong> (<strong class="keyword">MAB</strong>) problem. We start the chapter by understanding the MAB<strong class="keyword"> </strong>problem, and then we will learn about several exploration strategies, called epsilon-greedy, softmax exploration, upper confidence bound, and Thompson sampling, for solving the MAB problem. Following this, we will learn how a MAB is useful in real-world use cases.</p>
    <p class="normal">Moving forward, we will understand how to find the best advertisement banner that is clicked on most frequently by users by framing it as a MAB problem. At the end of the chapter, we will learn about contextual bandits and how they are used in different use cases.</p>
    <p class="normal">In this chapter, we will learn about the following:</p>
    <ul>
      <li class="bullet">The MAB problem</li>
      <li class="bullet">The epsilon-greedy method</li>
      <li class="bullet">Softmax exploration</li>
      <li class="bullet">The upper confidence bound algorithm</li>
      <li class="bullet">The Thompson sampling algorithm</li>
      <li class="bullet">Applications of MAB</li>
      <li class="bullet">Finding the best advertisement banner using MAB</li>
      <li class="bullet">Contextual bandits</li>
    </ul>
    <h1 id="_idParaDest-150" class="title">The MAB problem</h1>
    <p class="normal">The MAB problem <a id="_idIndexMarker531"/>is one of the classic problems in reinforcement learning. A MAB is a slot machine where we pull the arm (lever) and get a payout (reward) based on some probability distribution. A single slot machine is called a <a id="_idIndexMarker532"/>one-armed bandit and when there are multiple slot <a id="_idIndexMarker533"/>machines it is called a MAB or <em class="italic">k</em>-armed bandit, where <em class="italic">k</em> denotes <a id="_idIndexMarker534"/>the number of slot machines. </p>
    <p class="normal"><em class="italic">Figure 6.1</em> shows a 3-armed bandit:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.1: 3-armed bandit slot machines</p>
    <p class="normal">Slot machines are one of the most popular games in the casino, where we pull the arm and get a reward. If we get 0 reward then we lose the game, and if we get +1 reward then we win the game. There can be several slot machines, and each slot machine is referred to as an arm. For instance, slot machine 1 is referred to as arm 1, slot machine 2 is referred to as arm 2, and so on. Thus, whenever we say arm <em class="italic">n</em>, it actually means that we are referring to slot machine <em class="italic">n</em>.</p>
    <p class="normal">Each arm has its own probability distribution indicating the probability of winning and losing the game. For example, let's suppose we have two arms. Let the probability of winning if we pull arm 1 (slot machine 1) be 0.7 and the probability of winning if we pull arm 2 (slot machine 2) be 0.5. </p>
    <p class="normal">Then, if we pull arm 1, 70% of the time we win the game and get the +1 reward, and if we pull arm 2, then 50% of the time we win the game and get the +1 reward.</p>
    <p class="normal">Thus, we can say that pulling arm 1 is desirable as it makes us win the game 70% of the time. However, this probability distribution of the arm (slot machine) will not be given to us. We need to find out which arm helps us to win the game most of the time and gives us a good reward.</p>
    <p class="normal">Okay, how can we find this?</p>
    <p class="normal">Say we pulled arm 1 once and received a +1 reward, and we pulled arm 2 once and received a 0 reward. Since arm 1 gives a +1 reward, we cannot come to the conclusion that arm 1 is the best arm immediately after pulling it only once. We need to pull both of the arms many times and compute the average reward we obtain from each of the arms, and then we can select the arm that gives the maximum average reward as the best arm.</p>
    <p class="normal">Let's denote <a id="_idIndexMarker535"/>the arm by <em class="italic">a</em> and define the average reward by pulling the arm <em class="italic">a</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_001.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">Where <em class="italic">Q</em>(<em class="italic">a</em>) denotes the average reward of arm <em class="italic">a</em>.</p>
    <p class="normal">The optimal arm <em class="italic">a</em>* is the one that gives us the maximum average reward, that is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_002.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">Okay, we have learned that the arm that gives the maximum average reward is the optimal arm. But how can we find this?</p>
    <p class="normal">We play the game for several rounds and we can pull only one arm in each round. Say in the first round we pull arm 1 and observe the reward, and in the second round we pull arm 2 and observe the reward. Similarly, in every round, we keep pulling arm 1 or arm 2 and observe the reward. After completing several rounds of the game, we compute the average reward of each of the arms, and then we select the arm that has the maximum average reward as the best arm.</p>
    <p class="normal">But this is not a good approach to find the best arm. Say we have 20 arms; if we keep pulling a different arm in each round, then in most of the rounds we will lose the game and get a 0 reward. Along with finding the best arm, our goal should be to minimize the cost of identifying the best arm, and this is usually referred to as regret. </p>
    <p class="normal">Thus, we need to find the best arm while minimizing regret. That is, we need to find the best arm, but we don't want to end up selecting the arms that make us lose the game in most of the rounds.</p>
    <p class="normal">So, should we explore a different arm in each round, or should we select only the arm that got us a good reward in the previous rounds? This leads to a situation called the exploration-exploitation dilemma, which we learned about in <em class="chapterRef">Chapter 4</em>, <em class="italic">Monte Carlo Methods</em>. So, to resolve this, we use the epsilon-greedy method and select the arm that got us a good reward in the previous rounds with probability 1-epsilon and select the random arm <a id="_idIndexMarker536"/>with probability epsilon. After completing several rounds, we select the best arm as the one that has the maximum average reward.</p>
    <p class="normal">Similar to the epsilon-greedy method, there are several different exploration strategies that help us to overcome the exploration-exploitation dilemma. In the upcoming section, we will learn more about several different exploration strategies in detail and how they help us to find the optimal arm, but first let's look at creating a bandit.</p>
    <h2 id="_idParaDest-151" class="title">Creating a bandit in the Gym</h2>
    <p class="normal">Before <a id="_idIndexMarker537"/>going ahead, let's learn how to create a bandit environment with the Gym toolkit. The Gym does not come with a prepackaged bandit environment. So, we need to create a <a id="_idIndexMarker538"/>bandit environment and integrate it with the Gym. Instead of creating the bandit environment from scratch, we will <a id="_idIndexMarker539"/>use the open-source version of the bandit environment provided by Jesse Cooper.</p>
    <p class="normal">First, let's clone the Gym bandits repository:</p>
    <pre class="programlisting code"><code class="hljs-code">git clone https://github.com/JKCooper2/gym-bandits
</code></pre>
    <p class="normal">Next, we can install it using <code class="Code-In-Text--PACKT-">pip</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">cd gym-bandits
pip install -e .
</code></pre>
    <p class="normal">After installation, we import <code class="Code-In-Text--PACKT-">gym_bandits</code> and also the <code class="Code-In-Text--PACKT-">gym</code> library:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym_bandits
<span class="hljs-keyword">import</span> gym
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">gym_bandits</code> provides several versions of the bandit environment. We can examine the different bandit versions at <a href="https://github.com/JKCooper2/gym-bandits"><span class="url">https://github.com/JKCooper2/gym-bandits</span></a>.</p>
    <p class="normal">Let's just create a simple 2-armed bandit whose environment ID is <code class="Code-In-Text--PACKT-">BanditTwoArmedHighLowFixed-v0</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"BanditTwoArmedHighLowFixed-v0"</span>)
</code></pre>
    <p class="normal">Since we created a 2-armed bandit, our action space will be <code class="Code-In-Text--PACKT-">2</code> (as there are two arms), as shown here:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.action_space.n)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-number">2</span>
</code></pre>
    <p class="normal">We can also check the probability distribution of the arm with:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.p_dist)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>]
</code></pre>
    <p class="normal">It <a id="_idIndexMarker540"/>indicates that, with arm 1, we win the game 80% of the time and with arm 2, we win <a id="_idIndexMarker541"/>the game 20% of the time. Our goal is to find out whether pulling arm 1 or arm 2 makes us win the game most of the time. </p>
    <p class="normal">Now that we have learned how to create bandit environments in the Gym, in the next section, we will explore different exploration strategies to solve the MAB problem and we will implement them with the Gym.</p>
    <h2 id="_idParaDest-152" class="title">Exploration strategies</h2>
    <p class="normal">At the <a id="_idIndexMarker542"/>beginning of the chapter, we learned <a id="_idIndexMarker543"/>about the exploration-exploitation dilemma in the MAB problem. To overcome this, we use different exploration strategies and find the best arm. The different exploration strategies are listed here:</p>
    <ul>
      <li class="bullet">Epsilon-greedy</li>
      <li class="bullet">Softmax exploration</li>
      <li class="bullet">Upper confidence bound</li>
      <li class="bullet">Thomson sampling</li>
    </ul>
    <p class="normal">Now, we <a id="_idIndexMarker544"/>will explore all of these exploration strategies <a id="_idIndexMarker545"/>in detail and implement them to find the best arm.</p>
    <h3 id="_idParaDest-153" class="title">Epsilon-greedy</h3>
    <p class="normal">We learned <a id="_idIndexMarker546"/>about the epsilon-greedy algorithm in the previous chapters. With epsilon-greedy, we select the best arm with a probability 1-epsilon <a id="_idIndexMarker547"/>and we select a random arm with a probability epsilon. Let's take a simple example and learn how to find the best arm with the epsilon-greedy method in more detail.</p>
    <p class="normal">Say we have two arms—arm 1 and arm 2. Suppose with arm 1 we win the game 80% of the time and with arm 2 we win the game 20% of the time. So, we can say that arm 1 is the best arm as it makes us win the game 80% of the time. Now, let's learn how to find this with the epsilon-greedy method.</p>
    <p class="normal">First, we initialize the <code class="Code-In-Text--PACKT-">count</code> (the number of times the arm is pulled), <code class="Code-In-Text--PACKT-">sum_rewards</code> (the sum of rewards obtained from pulling the arm), and <code class="Code-In-Text--PACKT-">Q</code> (the average reward obtained by pulling the arm), as <em class="italic">Table 6.1</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_02.png" alt=""/></figure>
    <p class="packt_figref">Table 6.1: Initialize the variables with zero</p>
    <p class="normal"><strong class="keyword">Round 1</strong>:</p>
    <p class="normal">Say, in round 1 of the game, we select a random arm with a probability epsilon, and suppose we randomly pull arm 1 and observe the reward. Let the reward obtained by pulling arm 1 be 1. So, we update our table with <code class="Code-In-Text--PACKT-">count</code> of arm 1 set to 1, and <code class="Code-In-Text--PACKT-">sum_rewards</code> of arm 1 set to 1, and thus the average reward <code class="Code-In-Text--PACKT-">Q</code> of arm 1 after round 1 is 1 as <em class="italic">Table 6.2</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_03.png" alt=""/></figure>
    <p class="packt_figref">Table 6.2: Results after round 1</p>
    <p class="normal"><strong class="keyword">Round 2</strong>:</p>
    <p class="normal">Say, in round 2, we select the best arm with a probability 1-epsilon. The best arm is the one that has the maximum average reward. So, we check our table to see which arm has the maximum <a id="_idIndexMarker548"/>average reward. Since arm 1 has the maximum average reward, we pull arm 1 and observe the reward and let the reward <a id="_idIndexMarker549"/>obtained from pulling arm 1 be 1. </p>
    <p class="normal">So, we update our table with <code class="Code-In-Text--PACKT-">count</code> of arm 1 to 2 and <code class="Code-In-Text--PACKT-">sum_rewards</code> of arm 1 to 2, and thus the average reward <code class="Code-In-Text--PACKT-">Q</code> of arm 1 after round 2 is 1 as <em class="italic">Table 6.3</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_04.png" alt=""/></figure>
    <p class="packt_figref">Table 6.3: Results after round 2</p>
    <p class="normal"><strong class="keyword">Round 3</strong>:</p>
    <p class="normal">Say, in round 3, we select a random arm with a probability epsilon. Suppose we randomly pull arm 2 and observe the reward. Let the reward obtained by pulling arm 2 be 0. So, we update our table with <code class="Code-In-Text--PACKT-">count</code> of arm 2 set to 1 and <code class="Code-In-Text--PACKT-">sum_rewards</code> of arm 2 set to 0, and thus the average reward <code class="Code-In-Text--PACKT-">Q</code> of arm 2 after round 3 is 0 as <em class="italic">Table 6.4</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_05.png" alt=""/></figure>
    <p class="packt_figref">Table 6.4: Results after round 3</p>
    <p class="normal"><strong class="keyword">Round 4</strong>:</p>
    <p class="normal">Say, in round 4, we select the best arm with a probability 1-epsilon. So, we pull arm 1 since it has the maximum average reward. Let the reward obtained by pulling arm 1 be 0 this time. Now, we update our table with <code class="Code-In-Text--PACKT-">count</code> of arm 1 to 3 and <code class="Code-In-Text--PACKT-">sum_rewards</code> of arm 2 to 2, and thus the average reward <code class="Code-In-Text--PACKT-">Q</code> of arm 1 after round 4 will be 0.66 as <em class="italic">Table 6.5</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_06.png" alt=""/></figure>
    <p class="packt_figref">Table 6.5: Results after round 4</p>
    <p class="normal">We repeat <a id="_idIndexMarker550"/>this process for several <a id="_idIndexMarker551"/>rounds; that is, for several rounds of the game, we pull the best arm with a probability 1-epsilon and we pull a random arm with probability epsilon. </p>
    <p class="normal"><em class="italic">Table 6.6</em> shows the updated table after 100 rounds of the game:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_07.png" alt=""/></figure>
    <p class="packt_figref">Table 6.6: Results after 100 rounds</p>
    <p class="normal">From <em class="italic">Table 6.6</em>, we can conclude that arm 1 is the best arm since it has the maximum average reward.</p>
    <h4 class="title">Implementing epsilon-greedy</h4>
    <p class="normal">Now, let's <a id="_idIndexMarker552"/>learn to implement the epsilon-greedy method to find the best arm. First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> gym_bandits
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">For better understanding, let's create the bandit with only two arms:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"BanditTwoArmedHighLowFixed-v0"</span>)
</code></pre>
    <p class="normal">Let's check the probability distribution of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.p_dist)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.2</span>]
</code></pre>
    <p class="normal">We can observe that with arm 1 we win the game with 80% probability and with arm 2 we win the game with 20% probability. Here, the best arm is arm 1, as with arm 1 we win the game with 80% probability. Now, let's see how to find this best arm using the epsilon-greedy method.</p>
    <p class="normal">First, let's <a id="_idIndexMarker553"/>initialize the variables.</p>
    <p class="normal">Initialize the <code class="Code-In-Text--PACKT-">count</code> for storing the number of times an arm is pulled:</p>
    <pre class="programlisting code"><code class="hljs-code">count = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">sum_rewards</code> for storing the sum of rewards of each arm:</p>
    <pre class="programlisting code"><code class="hljs-code">sum_rewards = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">Q</code> for storing the average reward of each arm:</p>
    <pre class="programlisting code"><code class="hljs-code">Q = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Set the number of rounds (iterations):</p>
    <pre class="programlisting code"><code class="hljs-code">num_rounds = <span class="hljs-number">100</span>
</code></pre>
    <p class="normal">Now, let's define the <code class="Code-In-Text--PACKT-">epsilon_greedy</code> function.</p>
    <p class="normal">First, we generate a random number from a uniform distribution. If the random number is less than epsilon, then we pull the random arm; else, we pull the best arm that has the maximum average reward, as shown here:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">epsilon_greedy</span><span class="hljs-function">(</span><span class="hljs-params">epsilon</span><span class="hljs-function">):</span>
    
    <span class="hljs-keyword">if</span> np.random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>) &lt; epsilon:
        <span class="hljs-keyword">return</span> env.action_space.sample()
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> np.argmax(Q)
</code></pre>
    <p class="normal">Now, let's play the game and try to find the best arm using the epsilon-greedy method.</p>
    <p class="normal">For each round:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_rounds):
</code></pre>
    <p class="normal">Select the arm based on the epsilon-greedy method:</p>
    <pre class="programlisting code"><code class="hljs-code">    arm = epsilon_greedy(epsilon=<span class="hljs-number">0.5</span>)
</code></pre>
    <p class="normal">Pull the arm and store the reward and next state information:</p>
    <pre class="programlisting code"><code class="hljs-code">    next_state, reward, done, info = env.step(arm)
</code></pre>
    <p class="normal">Increment <a id="_idIndexMarker554"/>the count of the arm by <code class="Code-In-Text--PACKT-">1</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    count[arm] += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Update the sum of rewards of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">    sum_rewards[arm]+=reward
</code></pre>
    <p class="normal">Update the average reward of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">    Q[arm] = sum_rewards[arm]/count[arm]
</code></pre>
    <p class="normal">After all the rounds, we look at the average reward obtained from each of the arms:</p>
    <pre class="programlisting code"><code class="hljs-code">print(Q)
</code></pre>
    <p class="normal">The preceding code will print something like this:</p>
    <pre class="programlisting code"><code class="hljs-code">[<span class="hljs-number">0.83783784</span> <span class="hljs-number">0.34615385</span>]
</code></pre>
    <p class="normal">Now, we can select the optimal arm as the one that has the maximum average reward:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_002.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">Since arm 1 has a higher average reward than arm 2, our optimal arm will be arm 1:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">'The optimal arm is arm {}'</span>.format(np.argmax(Q)+<span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">The <a id="_idIndexMarker555"/>preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">The optimal arm <span class="hljs-keyword">is</span> arm <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Thus, we have found the optimal arm using the epsilon-greedy method.</p>
    <h3 id="_idParaDest-154" class="title">Softmax exploration</h3>
    <p class="normal">Softmax exploration, also <a id="_idIndexMarker556"/>known as Boltzmann exploration, is another <a id="_idIndexMarker557"/>useful exploration strategy for finding the optimal arm.</p>
    <p class="normal">In the <a id="_idIndexMarker558"/>epsilon-greedy policy, we learned that we select the best arm with probability 1-epsilon and a random arm with probability epsilon. As you may have noticed, in the epsilon-greedy policy, all the non-best arms are explored equally. That is, all the non-best arms have a uniform probability of being selected. For example, say we have 4 arms and arm 1 is the best arm. Then we explore the non-best arms – [arm 2, arm 3, arm 4] – uniformly.</p>
    <p class="normal">Say arm 3 is never a good arm and it always gives a reward of 0. In this case, instead of exploring arm 3 again, we can spend more time exploring arm 2 and arm 4. But the problem with the epsilon-greedy method is that we explore all the non-best arms equally. So, all the non-best arms – [arm 2, arm 3, arm 4] – will be explored equally.</p>
    <p class="normal">To avoid this, if we can give priority to arm 2 and arm 4 over arm 3, then we can explore arm 2 and arm 4 more than arm 3.</p>
    <p class="normal">Okay, but how can we give priority to the arms? We can give priority to the arms by assigning a probability to all the arms based on the average reward <em class="italic">Q</em>. The arm that has the maximum average reward will have high probability, and all the non-best arms have a probability proportional to their average reward.</p>
    <p class="normal">For instance, as <em class="italic">Table 6.7</em> shows, arm 1 is the best arm as it has a high average reward <em class="italic">Q</em>. So, we assign a high probability to arm 1. Arms 2, 3, and 4 are the non-best arms, and we need to explore them. As we can observe, arm 3 has an average reward of 0. So, instead of selecting all the non-best arms uniformly, we give more priority to arms 2 and 4 than arm 3. So, the probability of arm 2 and 4 will be high compared to arm 3:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_08.png" alt=""/></figure>
    <p class="packt_figref">Table 6.7: Average reward for a 4-armed bandit</p>
    <p class="normal">Thus, in softmax <a id="_idIndexMarker559"/>exploration, we select the arms based on a probability. The probability of each arm is directly proportional to its average reward:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_004.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">But wait, the <a id="_idIndexMarker560"/>probabilities should sum to 1, right? The average reward (Q value) will not sum to 1. So, we convert them into probabilities with the softmax function, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_005.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">So, now the arm will be selected based on the probability. However, in the initial rounds we will not know the correct average reward of each arm, so selecting the arm based on the probability of average reward will be inaccurate in the initial rounds. To avoid this, we introduce a new parameter called <em class="italic">T</em>. <em class="italic">T</em> is called the temperature parameter. </p>
    <p class="normal">We can rewrite the preceding equation with the temperature <em class="italic">T</em>, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_006.png" alt="" style="height: 2.6em;"/></figure>
    <p class="normal">Okay, how will this <em class="italic">T</em> help us? When <em class="italic">T</em> is high, all the arms have an equal probability of being selected and when <em class="italic">T</em> is low, the arm that has the maximum average reward will have a high probability. So, we set <em class="italic">T</em> to a high number in the initial rounds, and after a series of rounds we reduce the value of <em class="italic">T</em>. This means that in the initial round we explore all the arms equally and after a series of rounds, we select the best arm that has a high probability.</p>
    <p class="normal">Let's understand this with a simple example. Say we have four arms, arm 1 to arm 4. Suppose we pull arm 1 and receive a reward of 1. Then the average reward of arm 1 will be 1 and <a id="_idIndexMarker561"/>the average reward of all other arms will be 0, as <em class="italic">Table 6.8</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_09.png" alt=""/></figure>
    <p class="packt_figref">Table 6.8: Average reward for each arm</p>
    <p class="normal">Now, if we <a id="_idIndexMarker562"/>convert the average reward to probabilities using the softmax function given in equation (1), then our probabilities look like the following:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_10.png" alt=""/></figure>
    <p class="packt_figref">Table 6.9: Probability of each arm</p>
    <p class="normal">As we can observe, we have a 47% probability for arm 1 and a 17% probability for all other arms. But we cannot assign a high probability to arm 1 by just pulling arm 1 once. So, we set <em class="italic">T</em> to a high number, say <em class="italic">T</em> = 30, and calculate the probabilities based on equation (2). Now our probabilities become:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_11.png" alt=""/></figure>
    <p class="packt_figref">Table 6.10: Probability of each arm with T=30</p>
    <p class="normal">As we can see, now <a id="_idIndexMarker563"/>all the arms have equal probabilities of being selected. Now we explore the arms based on this probability and over a series of rounds, the <em class="italic">T</em> value <a id="_idIndexMarker564"/>will be reduced, and we will have a high probability to the best arm. Let's suppose after some 30 rounds, the average reward of all the arms is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_12.png" alt=""/></figure>
    <p class="packt_figref">Table 6.11: Average reward for each arm after 30+ rounds</p>
    <p class="normal">We learned that the value of <em class="italic">T</em> is reduced over several rounds. Suppose the value of <em class="italic">T</em> is reduced and it is now 0.3 (<em class="italic">T</em>=0.3); then the probabilities will become:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_13.png" alt=""/></figure>
    <p class="packt_figref">Table 6.12: Probabilities for each arm with T now set to 0.3</p>
    <p class="normal">As we can see, arm 1 has a high probability compared to other arms. So, we select arm 1 as the best arm and explore the non-best arms – [arm 2, arm 3, arm 4] – based on their probabilities <a id="_idIndexMarker565"/>in the next rounds. </p>
    <p class="normal">Thus, in the <a id="_idIndexMarker566"/>initial round, we don't know which arm is the best arm. So instead of assigning a high probability to the arm based on the average reward, we assign an equal probability to all the arms in the initial round with a high value of <em class="italic">T</em> and over a series of rounds, we reduce the value of <em class="italic">T</em> and assign a high probability to the arm that has a high average reward. </p>
    <h4 class="title">Implementing softmax exploration</h4>
    <p class="normal">Now, let's <a id="_idIndexMarker567"/>learn how to implement softmax exploration to find the best arm. First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> gym_bandits
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">Let's take the same two-armed bandit we saw in the epsilon-greedy section: </p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"BanditTwoArmedHighLowFixed-v0"</span>)
</code></pre>
    <p class="normal">Now, let's initialize the variables.</p>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">count</code> for storing the number of times an arm is pulled:</p>
    <pre class="programlisting code"><code class="hljs-code">count = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">sum_rewards</code> for storing the sum of rewards of each arm:</p>
    <pre class="programlisting code"><code class="hljs-code">sum_rewards = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">Q</code> for storing the average reward of each arm:</p>
    <pre class="programlisting code"><code class="hljs-code">Q = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Set the number of rounds (iterations):</p>
    <pre class="programlisting code"><code class="hljs-code">num_rounds = <span class="hljs-number">100</span>
</code></pre>
    <p class="normal">Now, we define the softmax function with the temperature <em class="italic">T</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_007.png" alt="" style="height: 2.6em;"/></figure>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">softmax</span><span class="hljs-function">(</span><span class="hljs-params">T</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Compute the probability of each arm based on the preceding equation:</p>
    <pre class="programlisting code"><code class="hljs-code">    denom = sum([np.exp(i/T) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> Q])
    probs = [np.exp(i/T)/denom <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> Q]
</code></pre>
    <p class="normal">Select the <a id="_idIndexMarker568"/>arm based on the computed probability distribution of arms:</p>
    <pre class="programlisting code"><code class="hljs-code">    arm = np.random.choice(env.action_space.n, p=probs)
    
    <span class="hljs-keyword">return</span> arm
</code></pre>
    <p class="normal">Now, let's play the game and try to find the best arm using the softmax exploration method.</p>
    <p class="normal">Let's begin by setting the temperature <code class="Code-In-Text--PACKT-">T</code> to a high number, say, <code class="Code-In-Text--PACKT-">50</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">T = <span class="hljs-number">50</span>
</code></pre>
    <p class="normal">For each round:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_rounds):
</code></pre>
    <p class="normal">Select the arm based on the softmax exploration method:</p>
    <pre class="programlisting code"><code class="hljs-code">    arm = softmax(T)
</code></pre>
    <p class="normal">Pull the arm and store the reward and next state information:</p>
    <pre class="programlisting code"><code class="hljs-code">    next_state, reward, done, info = env.step(arm)
</code></pre>
    <p class="normal">Increment the count of the arm by 1:</p>
    <pre class="programlisting code"><code class="hljs-code">    count[arm] += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Update the sum of rewards of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">    sum_rewards[arm]+=reward
</code></pre>
    <p class="normal">Update the average reward of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">    Q[arm] = sum_rewards[arm]/count[arm]
</code></pre>
    <p class="normal">Reduce the temperature <code class="Code-In-Text--PACKT-">T</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    T = T*<span class="hljs-number">0.99</span>
</code></pre>
    <p class="normal">After all the rounds, we check the Q value, that is, the average reward of all the arms:</p>
    <pre class="programlisting code"><code class="hljs-code">print(Q)
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker569"/>code will print something like this:</p>
    <pre class="programlisting code"><code class="hljs-code">[<span class="hljs-number">0.77700348</span> <span class="hljs-number">0.1971831</span> ]
</code></pre>
    <p class="normal">As we can see, arm 1 has a higher average reward than arm 2, so we select arm 1 as the optimal arm:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">'The optimal arm is arm {}'</span>.format(np.argmax(Q)+<span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">The preceding code prints:</p>
    <pre class="programlisting code"><code class="hljs-code">The optimal arm <span class="hljs-keyword">is</span> arm <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Thus, we have found the optimal arm using the softmax exploration method.</p>
    <h3 id="_idParaDest-155" class="title">Upper confidence bound </h3>
    <p class="normal">In this section, we <a id="_idIndexMarker570"/>will explore another interesting algorithm called <strong class="keyword">upper confidence bound</strong> (<strong class="keyword">UCB</strong>) for handling the exploration-exploitation dilemma. The UCB algorithm is based on a principle called optimism <a id="_idIndexMarker571"/>in the face of uncertainty. Let's take a simple example and understand how exactly the UCB algorithm works.</p>
    <p class="normal">Suppose we have two arms – arm 1 and arm 2. Let's say we played the game for 20 rounds by pulling arm 1 and arm 2 randomly and found that the mean reward of arm 1 is 0.6 and the mean reward of arm 2 is 0.5. But how can we be sure that this mean reward is actually accurate? That is, how can we be sure that this mean reward represents the true mean (population mean)? This is where we use the confidence interval. </p>
    <p class="normal">The confidence interval denotes the interval within which the true value lies. So, in our setting, the confidence interval denotes the interval within which the true mean reward of the arm lies.</p>
    <p class="normal">For instance, from <em class="italic">Figure 6.2</em>, we can see that the confidence interval of arm 1 is 0.2 to 0.9, which indicates that the mean reward of arm 1 lies in the range of 0.2 to 0.9. 0.2 is the lower confidence bound and 0.9 is the upper confidence bound. Similarly, we can observe that the confidence interval of arm 2 is 0.5 to 0.7, which indicates that the mean reward <a id="_idIndexMarker572"/>of arm 2 lies in the range of 0.5 to 0.7. where 0.5 is the lower confidence bound and 0.7 is the upper confidence bound:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.2: Confidence intervals for arms 1 and 2</p>
    <p class="normal">Okay, from <em class="italic">Figure 6.2</em>, we can see the confidence intervals of arm 1 and arm 2. Now, how can we make a decision? That is, how can we decide whether to pull arm 1 or arm 2? If we look closely, we can see that the confidence interval of arm 1 is large and the confidence interval of arm 2 is small.</p>
    <p class="normal">When the confidence interval is large, we are uncertain about the mean value. Since the confidence interval of arm 1 is large (0.2 to 0.9), we are not sure what reward we would obtain by pulling arm 1 because the average reward varies from as low as 0.2 to as high as 0.9. So, there is a lot of uncertainty in arm 1 and we are not sure whether arm 1 gives a high reward or a low reward.</p>
    <p class="normal">When the confidence interval is small, then we are certain about the mean value. Since the confidence interval of arm 2 is small (0.5 to 0.7), we can be sure that we will get a good reward by pulling arm 2 as our average reward is in the range of 0.5 to 0.7.</p>
    <p class="normal">But what is the reason for the confidence interval of arm 2 being small and the confidence interval of arm 1 being large? At the beginning of the section, we learned that we played the game for 20 rounds by pulling arm 1 and arm 2 randomly and computed the mean reward of arm 1 and arm 2. Say arm 2 has been pulled 15 times and arm 1 has been pulled only 5 times. Since arm 2 has been pulled many times, the confidence interval of arm 2 is small and it denotes a certain mean reward. Since arm 1 has been pulled fewer times, the confidence interval of the arm is large and it denotes an uncertain mean reward. Thus, it indicates that arm 2 has been explored a lot more than arm 1.</p>
    <p class="normal">Okay, coming <a id="_idIndexMarker573"/>back to our question, should we pull arm 1 or arm 2? In UCB, we always select the arm that has a high upper confidence bound, so in our example, we select arm 1 since it has a high upper confidence bound of 0.9. But why <a id="_idIndexMarker574"/>do we have to select the arm that has the highest upper confidence bound? Selecting the arm with the highest upper bound helps us to select the arm that gives the maximum reward.</p>
    <p class="normal">But there is a small catch here. When the confidence interval is large, we will not be sure about the mean reward. For instance, in our example, we select arm 1 since it has a high upper confidence bound of 0.9; however, since the confidence interval of arm 1 is large, our mean reward could be anywhere from 0.2 to 0.9, and so we can even get a low reward. But that's okay, we still select arm 1 as it promotes exploration. When the arm is explored well, then the confidence interval gets smaller.</p>
    <p class="normal">As we play the game for several rounds by selecting the arm that has a high UCB, our confidence interval of both arms will get narrower and denote a more accurate mean value. For instance, as we can see in <em class="italic">Figure 6.3</em>, after playing the game for several rounds, the confidence interval of both the arms becomes small and denotes a more accurate mean value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.3: Confidence intervals for arms 1 and 2 after several rounds</p>
    <p class="normal">From <em class="italic">Figure 6.3</em>, we can see that the confidence interval of both arms is small and we have a more accurate mean, and since in UCB we select arm that has the highest UCB, we select arm 2 as the best arm. </p>
    <p class="normal">Thus, in UCB, we <a id="_idIndexMarker575"/>always select the arm that has the highest upper confidence bound. In the initial rounds, we may not select the best arm as the <a id="_idIndexMarker576"/>confidence interval of the arms will be large in the initial round. But over a series of rounds, the confidence interval gets smaller and we select the best arm.</p>
    <p class="normal">Let <em class="italic">N</em>(<em class="italic">a</em>) be the number of times arm <em class="italic">a</em> was pulled and <em class="italic">t</em> be the total number of rounds, then the upper confidence bound of arm <em class="italic">a</em> can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_008.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">We select the arm that has the highest upper confidence bound as the best arm:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_009.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">The <a id="_idIndexMarker577"/>algorithm of UCB is given as follows:</p>
    <ol>
      <li class="numbered">Select <a id="_idIndexMarker578"/>the arm whose upper confidence bound is high</li>
      <li class="numbered">Pull the arm and receive a reward</li>
      <li class="numbered">Update the arm's mean reward and confidence interval</li>
      <li class="numbered">Repeat <em class="italic">steps 1</em> to <em class="italic">3</em> for several rounds</li>
    </ol>
    <h4 class="title">Implementing UCB</h4>
    <p class="normal">Now, let's <a id="_idIndexMarker579"/>learn how to implement the UCB algorithm to find the best arm.</p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> gym_bandits
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">Let's create the same two-armed bandit we saw in the previous section:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"BanditTwoArmedHighLowFixed-v0"</span>)
</code></pre>
    <p class="normal">Now, let's initialize the variables.</p>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">count</code> for storing the number of times an arm is pulled:</p>
    <pre class="programlisting code"><code class="hljs-code">count = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">sum_rewards</code> for storing the sum of rewards of each arm:</p>
    <pre class="programlisting code"><code class="hljs-code">sum_rewards = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">Q</code> for storing the average reward of each arm:</p>
    <pre class="programlisting code"><code class="hljs-code">Q = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Set the number of rounds (iterations):</p>
    <pre class="programlisting code"><code class="hljs-code">num_rounds = <span class="hljs-number">100</span>
</code></pre>
    <p class="normal">Now, we <a id="_idIndexMarker580"/>define the <strong class="keyword">UCB function</strong>, which returns the best arm as the one that has the highest UCB:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">UCB</span><span class="hljs-function">(</span><span class="hljs-params">i</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Initialize the <code class="Code-In-Text--PACKT-">numpy</code> array for storing the UCB of all the arms:</p>
    <pre class="programlisting code"><code class="hljs-code">    ucb = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Before computing the UCB, we explore all the arms at least once, so for the first 2 rounds, we directly select the arm corresponding to the round number:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> i &lt; <span class="hljs-number">2</span>:
        <span class="hljs-keyword">return</span> i
</code></pre>
    <p class="normal">If the round is greater than 2, then we compute the UCB of all the arms as specified in equation (3) and return the arm that has the highest UCB:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">for</span> arm <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>):
            ucb[arm] = Q[arm] + np.sqrt((<span class="hljs-number">2</span>*np.log(sum(count))) / count[arm])
        <span class="hljs-keyword">return</span> (np.argmax(ucb))
</code></pre>
    <p class="normal">Now, let's <a id="_idIndexMarker581"/>play the game and try to find the best arm using the UCB method.</p>
    <p class="normal">For each round:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_rounds):
</code></pre>
    <p class="normal">Select the arm based on the UCB method:</p>
    <pre class="programlisting code"><code class="hljs-code">    arm = UCB(i)
</code></pre>
    <p class="normal">Pull the arm and store the reward and next state information:</p>
    <pre class="programlisting code"><code class="hljs-code">    next_state, reward, done, info = env.step(arm) 
</code></pre>
    <p class="normal">Increment the count of the arm by <code class="Code-In-Text--PACKT-">1</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">    count[arm] += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Update the sum of rewards of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">    sum_rewards[arm]+=reward
</code></pre>
    <p class="normal">Update the average reward of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">    Q[arm] = sum_rewards[arm]/count[arm]
</code></pre>
    <p class="normal">After all <a id="_idIndexMarker582"/>the rounds, we can select the optimal arm as the one that has the maximum average reward:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">'The optimal arm is arm {}'</span>.format(np.argmax(Q)+<span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">The optimal arm <span class="hljs-keyword">is</span> arm <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Thus, we found the optimal arm using the UCB method.</p>
    <h3 id="_idParaDest-156" class="title">Thompson sampling</h3>
    <p class="normal"><strong class="keyword">Thompson sampling</strong> (<strong class="keyword">TS</strong>) is another <a id="_idIndexMarker583"/>interesting exploration <a id="_idIndexMarker584"/>strategy to overcome the exploration-exploitation dilemma and it is based on a beta distribution. So, before diving into Thompson sampling, let's first understand the beta distribution. The beta distribution is a probability distribution function and it is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_010.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_06_011.png" alt="" style="height: 2.51em;"/> and <img src="../Images/B15558_06_012.png" alt="" style="height: 1.11em;"/> is the gamma function.</p>
    <p class="normal">The shape of the distribution is controlled by the two parameters <img src="../Images/B15558_06_013.png" alt="" style="height: 0.93em;"/> and <img src="../Images/B15558_06_014.png" alt="" style="height: 1.11em;"/>. When the values of <img src="../Images/B15558_05_055.png" alt="" style="height: 0.93em;"/> and <img src="../Images/B15558_06_016.png" alt="" style="height: 1.11em;"/> are the same, then we will have a symmetric distribution. </p>
    <p class="normal">For instance, as <em class="italic">Figure 6.4</em> shows, since the value of <img src="../Images/B15558_05_055.png" alt="" style="height: 0.93em;"/> and <img src="../Images/B15558_06_018.png" alt="" style="height: 1.11em;"/> is equal to two we have a symmetric distribution:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.4: Symmetric beta distribution</p>
    <p class="normal">When <a id="_idIndexMarker585"/>the value of <img src="../Images/B15558_06_019.png" alt="" style="height: 0.93em;"/> is higher than <img src="../Images/B15558_06_020.png" alt="" style="height: 1.11em;"/> then we will have a probability <a id="_idIndexMarker586"/>closer to 1 than 0. For instance, as <em class="italic">Figure 6.5</em> shows, since the value of <img src="../Images/B15558_06_021.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_06_022.png" alt="" style="height: 1.11em;"/>, we have a high probability closer to 1 than 0:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.5: Beta distribution where <img src="../Images/B15558_06_023.png" alt="" style="height: 1.11em;"/></p>
    <p class="normal">When the <a id="_idIndexMarker587"/>value of <img src="../Images/B15558_06_024.png" alt="" style="height: 1.11em;"/> is higher than <img src="../Images/B15558_05_055.png" alt="" style="height: 0.93em;"/> then we will have a high probability <a id="_idIndexMarker588"/>closer to 0 than 1. For instance, as shown in the following plot, since the value of <img src="../Images/B15558_06_026.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_06_027.png" alt="" style="height: 1.11em;"/>, we have a high probability closer to 0 than 1:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.6: Gamma distribution where <img src="../Images/B15558_06_028.png" alt="" style="height: 1.11em;"/></p>
    <p class="normal">Now that we have a basic idea of the beta distribution, let's explore how Thompson sampling works and how it uses the beta distribution. Understanding the true distribution <a id="_idIndexMarker589"/>of each arm is very important because once we know <a id="_idIndexMarker590"/>the true distribution of the arm, then we can easily understand whether the arm will give us a good reward; that is, we can understand whether pulling the arm will help us to win the game. For example, let's say we have two arms – arm 1 and arm 2. <em class="italic">Figure 6.7</em> shows the true distribution of the two arms:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.7: True distributions for arms 1 and 2</p>
    <p class="normal">From <em class="italic">Figure 6.7</em>, we can see that it is better to pull arm 1 than arm 2 because arm 1 has a high probability close to 1, but arm 2 has a high probability close to 0. So, if we pull arm 1, we get a reward of 1 and win the game, but if we pull arm 2 we get a reward of 0 and lose the game. Thus, once we know the true distribution of the arms then we can understand which arm is the best arm.</p>
    <p class="normal">But how <a id="_idIndexMarker591"/>can we learn the true distribution of arm 1 and arm 2? This is <a id="_idIndexMarker592"/>where we use the Thompson sampling method. Thompson sampling is a probabilistic method and it is based on a prior distribution.</p>
    <p class="normal">First, we take <em class="italic">n</em> samples from arm 1 and arm 2 and compute their distribution. However, in the initial iterations, the computed distributions of arm 1 and arm 2 will not be the same as the true distribution, and so we will call this the prior distribution. As <em class="italic">Figure 6.8</em> shows, we have the prior distribution of arm 1 and arm 2, and it varies from the true distribution:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_20.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.8: Prior distributions for arms 1 and 2</p>
    <p class="normal">But over a series of iterations, we learn the true distribution of arm 1 and arm 2 and, as <em class="italic">Figure 6.9 </em>shows, the prior distributions of the arms look the same as the true distribution after a series of iterations:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_21.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.9: The prior distributions move closer to the true distributions</p>
    <p class="normal">Once we <a id="_idIndexMarker593"/>have learned the true distributions of all the arms, then we can easily select the best arm. Okay, but how exactly do we learn the true distribution? Let's explore this in more detail.</p>
    <p class="normal">Here, we use <a id="_idIndexMarker594"/>the beta distribution as a prior distribution. Say we have two arms, so we will have two beta distributions (prior distributions), and we initialize both <img src="../Images/B15558_05_055.png" alt="" style="height: 0.93em;"/> and <img src="../Images/B15558_06_030.png" alt="" style="height: 1.11em;"/> to the same value, say 3, as <em class="italic">Figure 6.10</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_22.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.10: Initialized prior distributions for arms 1 and 2 look the same</p>
    <p class="normal">As we can see, since we initialized alpha and beta to the same value, the beta distributions of arm 1 and arm 2 look the same.</p>
    <p class="normal">In the first round, we just randomly sample a value from these two distributions and select the arm that has the maximum sampled value. Let's say the sampled value of arm 1 is high, so in this case, we pull arm 1. Say we win the game by pulling arm 1, then we update the distribution of arm 1 by incrementing the alpha value of the distribution by 1; that is, we update <a id="_idIndexMarker595"/>the alpha value as <img src="../Images/B15558_06_031.png" alt="" style="height: 1.11em;"/>. As <em class="italic">Figure 6.11</em> shows, the <a id="_idIndexMarker596"/>alpha value of the distribution of arm 1 is incremented, and as we can see, arm 1's beta distribution has slightly high probability closer to 1 compared to arm 2:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_23.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.11: Prior distributions for arms 1 and 2 after round 1</p>
    <p class="normal">In the next round, we again sample a value randomly from these two distributions and select the arm that has the maximum sampled value. Suppose, in this round as well, we got the maximum sampled value from arm 1. Then we pull the arm 1 again. Say we win the game by pulling arm 1, then we update the distribution of arm 1 by updating the alpha value to <img src="../Images/B15558_06_031.png" alt="" style="height: 1.11em;"/>. As <em class="italic">Figure 6.12</em> shows, the alpha value of arm 1's distribution is incremented, and arm 1's beta distribution has a slightly high probability close to 1:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_24.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.12: Prior distributions for arms 1 and 2 after round 2</p>
    <p class="normal">Similarly, in the next round, we again randomly sample a value from these distributions and pull the arm that has the maximum value. Say this time we got the maximum value from arm 2, so we pull arm 2 and play the game. Suppose we lose the game by pulling arm 2. Then we <a id="_idIndexMarker597"/>update the distribution of arm 2 by updating <a id="_idIndexMarker598"/>the beta value as <img src="../Images/B15558_06_033.png" alt="" style="height: 1.11em;"/>. As <em class="italic">Figure 6.13</em> shows, the beta value of arm 2's distribution is incremented and the beta distribution of arm 2 has a slightly high probability close to 0:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_25.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.13: Prior distributions for arms 1 and 2 after round 3</p>
    <p class="normal">Again, in the next round, we randomly sample a value from the beta distribution of arm 1 and arm 2. Say the sampled value of arm 2 is high, so we pull arm 2. Say we lose the game again by pulling arm 2. Then we update the distribution of arm 2 by updating the beta value as <img src="../Images/B15558_06_034.png" alt="" style="height: 1.11em;"/>. As <em class="italic">Figure 6.14</em> shows, the beta value of arm 2's distribution is incremented by 1 and also arm 2's beta distribution has a slightly high probability close to 0:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_26.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.14: Prior distributions for arms 1 and 2 after round 4</p>
    <p class="normal">Okay, so, did you notice what we are doing here? We are essentially increasing the alpha value <a id="_idIndexMarker599"/>of the distribution of the arm if we win the game by <a id="_idIndexMarker600"/>pulling that arm, else we increase the beta value. If we do this repeatedly for several rounds, then we can learn the true distribution of the arm. Say after several rounds, our distribution will look like <em class="italic">Figure 6.15</em>. As we can see, the distributions of both arms resemble the true distributions:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_27.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.15: Prior distributions for arms 1 and 2 after several rounds</p>
    <p class="normal">Now if we sample a value from each of these distributions, then the sampled value will always be high from arm 1 and we always pull arm 1 and win the game. </p>
    <p class="normal">The steps <a id="_idIndexMarker601"/>involved in the Thomson sampling method are <a id="_idIndexMarker602"/>given here:</p>
    <ol>
      <li class="numbered" value="1">Initialize the beta distribution with alpha and beta set to equal values for all <em class="italic">k</em> arms</li>
      <li class="numbered">Sample a value from the beta distribution of all<em class="italic"> k</em> arms</li>
      <li class="numbered">Pull the arm whose sampled value is high</li>
      <li class="numbered">If we win the game, then update the alpha value of the distribution to <img src="../Images/B15558_06_035.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">If we lose the game, then update the beta value of the distribution to <img src="../Images/B15558_06_034.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Repeat <em class="italic">steps 2</em> to <em class="italic">5</em> for many rounds</li>
    </ol>
    <h4 class="title">Implementing Thompson sampling</h4>
    <p class="normal">Now, let's <a id="_idIndexMarker603"/>learn how to implement the Thompson sampling method to find the best arm.</p>
    <p class="normal">First, let's import the necessary libraries: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> gym_bandits
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">For better understanding, let's create the same two-armed bandit we saw in the previous section: </p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"BanditTwoArmedHighLowFixed-v0"</span>)
</code></pre>
    <p class="normal">Now, let's initialize the variables.</p>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">count</code> for storing the number of times an arm is pulled:</p>
    <pre class="programlisting code"><code class="hljs-code">count = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">sum_rewards</code> for storing the sum of rewards of each arm:</p>
    <pre class="programlisting code"><code class="hljs-code">sum_rewards = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">Q</code> for storing the average reward of each arm:</p>
    <pre class="programlisting code"><code class="hljs-code">Q = np.zeros(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize the alpha value as <code class="Code-In-Text--PACKT-">1</code> for both arms:</p>
    <pre class="programlisting code"><code class="hljs-code">alpha = np.ones(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Initialize the beta value as <code class="Code-In-Text--PACKT-">1</code> for both arms:</p>
    <pre class="programlisting code"><code class="hljs-code">beta = np.ones(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Set the number of rounds (iterations):</p>
    <pre class="programlisting code"><code class="hljs-code">num_rounds = <span class="hljs-number">100</span>
</code></pre>
    <p class="normal">Now, let's define the <code class="Code-In-Text--PACKT-">thompson_sampling</code> function.</p>
    <p class="normal">As the <a id="_idIndexMarker604"/>following code shows, we randomly sample values from the beta distributions of both arms and return the arm that has the maximum sampled value: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">thompson_sampling</span><span class="hljs-function">(</span><span class="hljs-params">alpha,beta</span><span class="hljs-function">):</span>
    
    samples = [np.random.beta(alpha[i]+<span class="hljs-number">1</span>,beta[i]+<span class="hljs-number">1</span>) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">2</span>)]
    <span class="hljs-keyword">return</span> np.argmax(samples)
</code></pre>
    <p class="normal">Now, let's play the game and try to find the best arm using the Thompson sampling method.</p>
    <p class="normal">For each round:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_rounds):
</code></pre>
    <p class="normal">Select the arm based on the Thompson sampling method:</p>
    <pre class="programlisting code"><code class="hljs-code">    arm = thompson_sampling(alpha,beta)
</code></pre>
    <p class="normal">Pull the arm and store the reward and next state information:</p>
    <pre class="programlisting code"><code class="hljs-code">    next_state, reward, done, info = env.step(arm) 
</code></pre>
    <p class="normal">Increment the count of the arm by 1:</p>
    <pre class="programlisting code"><code class="hljs-code">    count[arm] += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Update the sum of rewards of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">    sum_rewards[arm]+=reward
</code></pre>
    <p class="normal">Update the average reward of the arm:</p>
    <pre class="programlisting code"><code class="hljs-code">    Q[arm] = sum_rewards[arm]/count[arm]
</code></pre>
    <p class="normal">If we <a id="_idIndexMarker605"/>win the game, that is, if the reward is equal to 1, then we update the value of alpha to <img src="../Images/B15558_06_037.png" alt="" style="height: 1.11em;"/>, else we update the value of beta to <img src="../Images/B15558_06_033.png" alt="" style="height: 1.11em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> reward==<span class="hljs-number">1</span>:
        alpha[arm] = alpha[arm] + <span class="hljs-number">1</span>
    <span class="hljs-keyword">else</span>:
        beta[arm] = beta[arm] + <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">After all the rounds, we can select the optimal arm as the one that has the highest average reward:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">'The optimal arm is arm {}'</span>.format(np.argmax(Q)+<span class="hljs-number">1</span>))
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">The optimal arm <span class="hljs-keyword">is</span> arm <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Thus, we found the optimal arm using the Thompson sampling method. </p>
    <h1 id="_idParaDest-157" class="title">Applications of MAB</h1>
    <p class="normal">So far, we have <a id="_idIndexMarker606"/>learned about the MAB problem and how can we solve it using various exploration strategies. But our goal is not to just use these algorithms for playing slot machines. We can apply the various exploration strategies to several different use cases.</p>
    <p class="normal">For instance, bandits can be used as an alternative to AB testing. AB testing is one of the most commonly used classic methods of testing. Say we have two versions of the landing page of our website. Suppose we want to know which version of the landing page is most liked by the users. In this case, we conduct AB testing to understand which version of the landing page is most liked by the users. So, we show version 1 of the landing page to a particular set of users and version 2 of the landing page to other set of users. Then we measure several metrics, such as click-through rate, average time spent on the website, and so on, to understand which version of the landing page is most liked by the users. Once we understand which version of the landing page is most liked by the users, then we will start showing that version to all the users. </p>
    <p class="normal">Thus, in AB testing, we schedule a separate time for exploration and exploitation. That is, AB testing has two different dedicated periods for exploration and exploitation. But the problem <a id="_idIndexMarker607"/>with AB testing is that it will incur high regret. We can minimize the regret using the various exploration strategies that we have used to solve the MAB problem. So, instead of performing complete exploration and exploitation separately, we can perform exploration and exploitation simultaneously in an adaptive fashion with the various exploration strategies we learned in the previous sections. </p>
    <p class="normal">Bandits are widely used for website optimization, maximizing conversion rates, online advertisements, campaigning, and so on. </p>
    <h1 id="_idParaDest-158" class="title">Finding the best advertisement banner using bandits</h1>
    <p class="normal">In this section, let's see how to find the best advertisement banner using bandits. Suppose <a id="_idIndexMarker608"/>we are running a website and we <a id="_idIndexMarker609"/>have five different banners for a single advertisement on our website, and say we want to figure out which advertisement banner is most liked by the users.</p>
    <p class="normal">We can frame this problem as a MAB problem. The five advertisement banners represent the five arms of the bandit, and we assign +1 reward if the user clicks the advertisement and 0 reward if the user does not click the advertisement. So, to find out which advertisement banner is most clicked by the users, that is, which advertisement banner can give us the maximum reward, we can use various exploration strategies. In this section, let's just use an epsilon-greedy method to find the best advertisement banner.</p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
%matplotlib inline
plt.style.use('ggplot')
</code></pre>
    <h2 id="_idParaDest-159" class="title">Creating a dataset </h2>
    <p class="normal">Now, let's <a id="_idIndexMarker610"/>create a dataset. We generate a dataset with five columns denoting the five advertisement banners, and we generate 100,000 rows, where the values in the rows will be either 0 or 1, indicating whether the advertisement banner has been clicked (1) or not clicked (0) by the user:</p>
    <pre class="programlisting code"><code class="hljs-code">df = pd.DataFrame()
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">5</span>):
    df[<span class="hljs-string">'Banner_type_'</span>+str(i)] = np.random.randint(<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">100000</span>)
</code></pre>
    <p class="normal">Let's look at the first few rows of our dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">df.head()
</code></pre>
    <p class="normal">The preceding code will print the following. As we can see, we have the five advertisement banners (0 to 4) and the rows consisting of values of 0 or 1, indicating whether the banner has been clicked (1) or not clicked (0).</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_28.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.14: Clicks per banner</p>
    <h2 id="_idParaDest-160" class="title">Initialize the variables </h2>
    <p class="normal">Now, let's <a id="_idIndexMarker611"/>initialize some of the important variables.</p>
    <p class="normal">Set the number of iterations:</p>
    <pre class="programlisting code"><code class="hljs-code">num_iterations = <span class="hljs-number">100000</span>
</code></pre>
    <p class="normal">Define the number of banners:</p>
    <pre class="programlisting code"><code class="hljs-code">num_banner = <span class="hljs-number">5</span>
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">count</code> for storing the number of times the banner was clicked:</p>
    <pre class="programlisting code"><code class="hljs-code">count = np.zeros(num_banner)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">sum_rewards</code> for storing the sum of rewards obtained from each banner: </p>
    <pre class="programlisting code"><code class="hljs-code">sum_rewards = np.zeros(num_banner)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">Q</code> for storing the mean reward of each banner:</p>
    <pre class="programlisting code"><code class="hljs-code">Q = np.zeros(num_banner)
</code></pre>
    <p class="normal">Define a list for storing the selected banners:</p>
    <pre class="programlisting code"><code class="hljs-code">banner_selected = []
</code></pre>
    <h2 id="_idParaDest-161" class="title">Define the epsilon-greedy method</h2>
    <p class="normal">Now, let's define <a id="_idIndexMarker612"/>the epsilon-greedy method. We generate a random value from a uniform distribution. If the random value is less than epsilon, then we select the random banner; else, we select the best banner that has the maximum average reward:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">epsilon_greedy_policy</span><span class="hljs-function">(</span><span class="hljs-params">epsilon</span><span class="hljs-function">):</span>
    
    <span class="hljs-keyword">if</span> np.random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>) &lt; epsilon:
        <span class="hljs-keyword">return</span> np.random.choice(num_banner)
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> np.argmax(Q)
</code></pre>
    <h2 id="_idParaDest-162" class="title">Run the bandit test</h2>
    <p class="normal">Now, we run <a id="_idIndexMarker613"/>the epsilon-greedy policy to find out which advertisement banner is the best.</p>
    <p class="normal">For each iteration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
</code></pre>
    <p class="normal">Select the banner using the epsilon-greedy policy:</p>
    <pre class="programlisting code"><code class="hljs-code">    banner = epsilon_greedy_policy(<span class="hljs-number">0.5</span>)
</code></pre>
    <p class="normal">Get the reward of the banner:</p>
    <pre class="programlisting code"><code class="hljs-code">    reward = df.values[i, banner]
</code></pre>
    <p class="normal">Increment the counter:</p>
    <pre class="programlisting code"><code class="hljs-code">    count[banner] += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Store the sum of rewards:</p>
    <pre class="programlisting code"><code class="hljs-code">    sum_rewards[banner]+=reward
</code></pre>
    <p class="normal">Compute the average reward:</p>
    <pre class="programlisting code"><code class="hljs-code">    Q[banner] = sum_rewards[banner]/count[banner]
</code></pre>
    <p class="normal">Store the banner to the banner selected list:</p>
    <pre class="programlisting code"><code class="hljs-code">    banner_selected.append(banner)
</code></pre>
    <p class="normal">After all the rounds, we can select the best banner as the one that has the maximum average reward:</p>
    <pre class="programlisting code"><code class="hljs-code">print( <span class="hljs-string">'The best banner is banner {}'</span>.format(np.argmax(Q)))
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">The best banner <span class="hljs-keyword">is</span> banner <span class="hljs-number">2</span>
</code></pre>
    <p class="normal">We can also plot and see which banner is selected the most often:</p>
    <pre class="programlisting code"><code class="hljs-code">ax = sns.countplot(banner_selected)
ax.set(xlabel=<span class="hljs-string">'Banner'</span>, ylabel=<span class="hljs-string">'Count'</span>)
plt.show()
</code></pre>
    <p class="normal">The preceding code will plot the following. As we can see, banner 2 is selected most often:</p>
    <figure class="mediaobject"><img src="../Images/B15558_06_29.png" alt=""/></figure>
    <p class="packt_figref">Figure 6.15: Banner 2 is the best advertisement banner</p>
    <p class="normal">Thus, we <a id="_idIndexMarker614"/>have learned how to find the best advertisement banner by framing our problem as a MAB problem.</p>
    <h1 id="_idParaDest-163" class="title">Contextual bandits</h1>
    <p class="normal">We just learned how to use bandits to find the best advertisement banner for the users. But the <a id="_idIndexMarker615"/>banner preference varies from user to user. That is, user A likes banner 1, but user B might like banner 3, and so on. Each user has their own preferences. So, we have to personalize advertisement banners according to each user. How can we do that? This is where we use contextual bandits. </p>
    <p class="normal">In the MAB problem, we just perform the action and receive a reward. But with contextual bandits, we take actions based on the state of the environment and the state holds the context.</p>
    <p class="normal">For instance, in the advertisement banner example, the state specifies the user behavior and we will take action (show the banner) according to the state (user behavior) that will result in the maximum reward (ad clicks).</p>
    <p class="normal">Contextual bandits are widely used for personalizing content according to the user's behavior. They are <a id="_idIndexMarker616"/>also used to solve the cold-start problems faced by recommendation systems. Netflix uses contextual bandits for personalizing the artwork for TV shows according to user behavior.</p>
    <h1 id="_idParaDest-164" class="title">Summary</h1>
    <p class="normal">We started off the chapter by understanding what the MAB problem is and how it can be solved using several exploration strategies. We first learned about the epsilon-greedy method, where we select a random arm with a probability epsilon and select the best arm with a probability 1-epsilon. Next, we learned about the softmax exploration method, where we select the arm based on the probability distribution, and the probability of each arm is proportional to the average reward.</p>
    <p class="normal">Following this, we learned about the UCB algorithm, where we select the arm that has the highest upper confidence bound. Then, we explored the Thomspon sampling method, where we learned the distributions of the arms based on the beta distribution.</p>
    <p class="normal">Moving forward, we learned how MAB can be used as an alternative to AB testing and how can we find the best advertisement banner by framing the problem as a MAB problem. At the end of the chapter, we also had an overview of contextual bandits.</p>
    <p class="normal">In the next chapter, we will learn about several interesting deep learning algorithms that are essential for deep reinforcement learning. </p>
    <h1 id="_idParaDest-165" class="title">Questions</h1>
    <p class="normal">Let's evaluate the knowledge we gained in this chapter by answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">What is a MAB problem?</li>
      <li class="numbered">How does the epsilon-greedy policy select an arm?</li>
      <li class="numbered">What is the significance of <em class="italic">T</em> in softmax exploration?</li>
      <li class="numbered">How do we compute the upper confidence bound?</li>
      <li class="numbered">What happens when the value of alpha is higher than the value of beta in the beta distribution?</li>
      <li class="numbered">What are the steps involved in Thompson sampling?</li>
      <li class="numbered">What are contextual bandits?</li>
    </ol>
    <h1 id="_idParaDest-166" class="title">Further reading</h1>
    <p class="normal">For more information, check out these interesting resources:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Introduction to Multi-Armed Bandits</strong> by <em class="italic">Aleksandrs Slivkins</em>, <a href="https://arxiv.org/pdf/1904.07272.pdf"><span class="url">https://arxiv.org/pdf/1904.07272.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">A Survey on Practical Applications of Multi-Armed and Contextual Bandits</strong> by <em class="italic">Djallel Bouneffouf, Irina Rish</em>, <a href="https://arxiv.org/pdf/1904.10040.pdf"><span class="url">https://arxiv.org/pdf/1904.10040.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Collaborative Filtering Bandits</strong> by <em class="italic">Shuai Li, Alexandros Karatzoglou, Claudio Gentile</em>, <a href="https://arxiv.org/pdf/1502.03473.pdf"><span class="url">https://arxiv.org/pdf/1502.03473.pdf</span></a></li>
    </ul>
  </div>
</body></html>