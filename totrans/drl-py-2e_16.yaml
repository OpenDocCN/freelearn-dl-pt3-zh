- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning with Stable Baselines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned various deep **reinforcement learning** (**RL**) algorithms.
    Wouldn't it be nice if we had a library to easily implement a deep RL algorithm?
    Yes! There are various libraries available to easily build a deep RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: One such popular deep RL library is OpenAI Baselines. OpenAI Baselines provides
    an efficient implementation of many deep RL algorithms, which makes them easier
    to use. However, OpenAI Baselines does not provide good documentation. So, we
    will look at the fork of OpenAI Baselines called **Stable Baselines**.
  prefs: []
  type: TYPE_NORMAL
- en: Stable Baselines is an improved implementation of OpenAI Baselines. Stable Baselines
    is easier to use and it also includes state-of-the-art deep RL algorithms along
    with several useful features. We can use Stable Baselines for quickly prototyping
    the RL model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start off the chapter by installing Stable Baselines, and then we will
    learn how to create our first agent using the library. Next, we will learn about
    vectorized environments. Going further, we will learn to implement several deep
    RL algorithms using Stable Baselines along with exploring various functionalities
    of baselines.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing Stable Baselines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating our first agent with Stable Baselines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiprocessing with vectorized environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing Atari games with DQN and its variants
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lunar lander using A2C
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Swinging up a pendulum using DDPG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an agent to walk using TRPO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing GAIL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin the chapter by installing Stable Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Stable Baselines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First, let''s install the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Several deep RL algorithms require MPI to run, so, let''s install MPI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can install Stable Baselines through `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Note that currently, Stable Baselines works only with TensorFlow version 1.x.
    So, make sure you are running the Stable Baselines experiment with TensorFlow
    1.x.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have installed Stable Baselines, let's see how to create our first
    agent using it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our first agent with Stable Baselines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's build our first deep RL algorithm using Stable Baselines. Let's create
    a simple agent using a **Deep Q Network** (**DQN**) for the mountain car climbing
    task. We know that in the mountain car climbing task, a car is placed between
    two mountains and the goal of the agent is to drive up the mountain on the right.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import `gym` and `DQN` from `stable_baselines`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a mountain car environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s instantiate our agent. As we can observe in the following code,
    we are passing `MlpPolicy`, which implies that our network is a multilayer perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s train the agent by specifying the number of time steps we want
    to train:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: That's it. Building a DQN agent and training it is that simple.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the trained agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can also evaluate the trained agent by looking at the mean rewards using
    `evaluate_policy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following code, `agent` is the trained agent, `agent.get_env()` gets
    the environment we trained our agent in, and `n_eval_episodes` represents the
    number of episodes we need to evaluate our agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Storing and loading the trained agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With Stable Baselines, we can also save and load our trained agent to and from
    disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can save the agent as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After saving, we can load the agent as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Viewing the trained agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After training, we can also have a look at how our trained agent performs in
    the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'For 5,000 steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Predict the action to perform in the given state using our trained agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the predicted action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Update `state` to the current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can see how our trained agent performs in the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: Agent learning to climb mountain'
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s look at the final code combining everything we learned so far:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have a basic idea of how to use Stable Baselines, let's explore
    it in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorized environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the very interesting and useful features of Stable Baselines is that
    we can train our agent in multiple independent environments either in separate
    processes (using **SubprocVecEnv**) or in the same process (using **DummyVecEnv**).
  prefs: []
  type: TYPE_NORMAL
- en: For example, say we are training our agent in a cart pole balancing environment
    – instead of training our agent only in a single cart pole balancing environment,
    we can train our agent in the multiple cart pole balancing environments.
  prefs: []
  type: TYPE_NORMAL
- en: We generally train our agent in a single environment per step but now we can
    train our agent in multiple environments per step. This helps our agent to learn
    more quickly. Now, our state, action, reward, and done will be in the form of
    a vector since we are training our agent in multiple environments. So, we call
    this a vectorized environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two types of vectorized environment offered by Stable Baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: SubprocVecEnv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DummyVecEnv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SubprocVecEnv
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the subproc vectorized environment, we run each environment in a **separate**
    process (taking advantage of multiprocessing). Now, let's see how to create the
    subproc vectorized environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import `SubprocVecEnv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we create a function called `make_env` for initializing our environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create the subproc vectorized environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: DummyVecEnv
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the dummy vectorized environment, we run each environment in sequence on
    the current Python process. It does not support multiprocessing. Now, let's see
    how to create the dummy vectorized environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import `DummyVecEnv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can create the dummy vectorized environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have learned to train the agent in multiple independent environments
    using vectorized environments, in the next section, we will see how to integrate
    custom environments into Stable Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating custom environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can also use Stable Baselines to train an agent in our own environment. While
    creating our own environment, we need to make sure that our custom environment
    follows the Gym interface. That is, our environment should include methods such
    as `step`, `reset`, `render`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose the name of our custom environment is `CustomEnv`. First, we instantiate
    our custom environment as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can train our agent in the custom environment as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: That's it. In the next section, let's learn how to play Atari games using a
    DQN and its variants.
  prefs: []
  type: TYPE_NORMAL
- en: Playing Atari games with a DQN and its variants
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to create a DQN to play Atari games with Stable Baselines.
    First, let''s import the necessary modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we are dealing with Atari games, we can use a convolutional neural network
    instead of a vanilla neural network. So, we use `CnnPolicy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We learned that we preprocess the game screen before feeding it to the agent.
    With Stable Baselines, we don''t have to preprocess manually; instead, we can
    make use of the `make_atari` module, which takes care of preprocessing the game
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create an Atari game environment. Let''s create the Ice Hockey
    game environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the agent, we can have a look at how our trained agent performs
    in the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code displays how our trained agent plays the ice hockey game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: Agent playing the Ice Hockey game'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing DQN variants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We just learned how to implement DQN using Stable Baselines. Now, let's see
    how to implement the variants of DQN, such as double DQN, DQN with prioritized
    experience replay, and dueling DQN. Implementing DQN variants is very simple with
    Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define our keyword arguments as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, while instantiating our agent, we just need to pass the keyword arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can train the agent as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Now we have the dueling double DQN with prioritized experience replay.
    In the next section, we will learn how to play the lunar lander game using the
    **Advantage Actor-Critic Algorithm** (**A2C**).
  prefs: []
  type: TYPE_NORMAL
- en: Lunar lander using A2C
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s learn how to implement A2C with Stable Baselines for the lunar landing
    task. In the lunar lander environment, our agent drives the space vehicle, and
    the goal of the agent is to land correctly on the landing pad. If our agent (lander)
    lands away from the landing pad, then it loses the reward, and the episode will
    get terminated if the agent crashes or comes to rest. The action space of the
    environment includes four discrete actions, which are: do nothing, fire left orientation
    engine, fire main engine, and fire right orientation engine. Now, let''s see how
    to train the agent using A2C to correctly land on the landing pad.'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the lunar lander environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use the dummy vectorized environment. We learned that in the dummy vectorized
    environment, we run each environment in the same process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, we can evaluate our agent by looking at the mean rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also have a look at how our trained agent performs in the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will show how well our trained agent lands on the landing
    pad:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: Agent playing the lunar lander game'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we learned how to create an A2C using Stable Baselines.
    Instead of using the default network, can we customize the network architecture?
    Yes! With Stable Baselines, we can also use our own custom architecture. Let's
    see how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the feedforward policy (feedforward network):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We can instantiate the agent with the custom policy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can train the agent as usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: That's it. Similarly, we can create our own custom network. In the next section,
    let's learn how to perform the *inverted pendulum swing-up* task using the **Deep
    Deterministic Policy Gradient (DDPG)** algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Swinging up a pendulum using DDPG
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s learn how to implement the DDPG for the inverted pendulum swing-up task
    using Stable Baselines. First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the pendulum environment using Gym:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the number of actions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that in DDPG, instead of selecting the action directly, we add some
    noise using the Ornstein-Uhlenbeck process to ensure exploration. So, we create
    the action noise as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: After training the agent, we can also look at how our trained agent swings up
    the pendulum by rendering the environment. Can we also look at the computational
    graph of DDPG? Yes! In the next section, we will learn how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Viewing the computational graph in TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With Stable Baselines, it is easier to view the computational graph of our
    model in TensorBoard. In order to that, we just need to pass the directory where
    we will store our log files while instantiating the agent, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can train the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, open the terminal and type the following command to run TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can observe, we can now see the computational graph of the DDPG model
    (agent):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: Computational graph of DDPG'
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 16.4*, we can understand how the DDPG computational graph is generated
    just as we learned in *Chapter 12*, *Learning DDPG, TD3, and SAC*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s expand and look into the model node for more clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.5: Computational graph of DDPG'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe from *Figure 16.5*, our model includes the policy (actor)
    and Q (critic) network.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to use Stable Baselines to implement DDPG for the
    inverted pendulum swing-up task, in the next section we will learn how to implement
    TRPO using Stable Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Training an agent to walk using TRPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's learn how to train the agent to walk using **Trust Region
    Policy Optimization** (**TRPO**).Let's use the MuJoCo environment for training
    the agent. **MuJoCo** stands for **Multi-Joint dynamics with Contact** and is
    one of the most popular simulators used for training agents to perform continuous
    control tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Note that MuJoCo is a proprietary physics engine, so we need to acquire a license
    to use it. Also, MuJoCo offers a free 30-day trial period. Installing MuJoCo requires
    a specific set of steps. So, in the next section, we will see how to install the
    MuJoCo environment.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the MuJoCo environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, in your home directory, create a new hidden folder called `.mujoco`.
    Next, go to the MuJoCo website ([https://www.roboti.us/](https://www.roboti.us/))
    and download MuJoCo according to your operating system. As shown in *Figure 16.6*,
    MuJoCo provides support for Windows, Linux, and macOS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.6: Different MuJoCo versions'
  prefs: []
  type: TYPE_NORMAL
- en: If you are using Linux, then you can download the zip file named `mujoco200
    linux`. After downloading the zip file, unzip the file and rename it to `mujoco200`.
    Now, copy the `mujoco200` folder and place the folder inside the `.mujoco` folder
    in your home directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'As *Figure 16.7* shows, now in our home directory, we have a `.mujoco` folder,
    and inside the `.mujoco` folder, we have a `mujoco200` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: Installing MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to obtain a trial license. First, go to [https://www.roboti.us/license.html](https://www.roboti.us/license.html)
    and register for the trial license, as *Figure 16.8* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: Registering for the trial license'
  prefs: []
  type: TYPE_NORMAL
- en: To register, we also need the computer id. As *Figure 16.8* shows, to the right
    of the **Computer id** field, we have the name of different platforms. Now, just
    click on your operating system and you will obtain the relevant executable `getid`
    file. For instance, if you are using Linux, then you will obtain a file named
    `getid_linux`.
  prefs: []
  type: TYPE_NORMAL
- en: 'After downloading the `getid_linux` file, run the following command on your
    terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: The preceding command will display your computer id. After getting the computer
    id, fill in the form and register to obtain a license. Once you click on the **Submit**
    button, you will get an email from Roboti LLC Licensing.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the email, download the file named `mjkey.txt`. Next, place the `mjkey.txt`
    file in the `.mujoco` folder. As *Figure 16.9* shows, now our `.mujoco` hidden
    folder contains the `mjkey.txt` file and a folder named `mujoco200`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.9: Installing MuJoCo'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, open your terminal and run the following command to edit the `bashrc`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Copy the following line to the `bashrc` file and make sure to replace the username
    text with your own username:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, save the file and exit the nano editor. Now, run the following command
    on your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Well done! We are almost there. Now, clone the MuJoCo GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Navigate to the `mujoco-py` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, install MuJoCo:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'To test the successful installation of MuJoCo, let''s run a Humanoid agent
    by taking a random action in the environment. So, create the following Python
    file named `mujoco_test.py` with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, open the terminal and run the Python file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will render the `Humanoid` environment as *Figure 16.10
    shows*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.10: Humanoid environment'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully installed MuJoCo, let's start implementing TRPO
    to train our agent to walk in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing TRPO
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a vectorized `Humanoid` environment using `DummyVecEnv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the states (observations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'After training the agent, we can see how our trained agent learned to walk
    by rendering the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the whole code used in this section in a Python file called `trpo.py`
    and then open the terminal and run the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how our trained agent learned to walk in *Figure 16.11*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.11: Agent learning to walk using TRPO'
  prefs: []
  type: TYPE_NORMAL
- en: Always use the terminal to run the program that uses the MuJoCo environment.
  prefs: []
  type: TYPE_NORMAL
- en: That's it. In the next section, we will learn how to record our trained agent's
    actions as a video.
  prefs: []
  type: TYPE_NORMAL
- en: Recording the video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we trained our agent to learn to walk using TRPO. Can
    we also record a video of our trained agent? Yes! With Stable Baselines, we can
    easily record a video of our agent using the `VecVideoRecorder` module.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that to record the video, we need the `ffmpeg` package installed in our
    machine. If it is not installed, then install it using the following set of commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s import the `VecVideoRecorder` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function called `record_video` for recording the video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the video recorder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Select actions in the environment using our trained agent where the number
    of time steps is set to the video length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it! Now, let''s call our `record_video` function. Note that we are
    passing the environment name, our trained agent, the length of the video, and
    the name of our video file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will have a new file called `Humanoid_walk_TRPO-step-0-to-step-500.mp4`
    in the `videos` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.12: Recorded video'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we can record our trained agent's action. In the next section,
    we will learn how to implement PPO using Stable Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Training a cheetah bot to run using PPO
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, let''s learn how to train the 2D cheetah bot to run using
    **Proximal Policy Optimization** (**PPO**). First, import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a vectorized environment using `DummyVecEnv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'After training, we can see how our trained cheetah bot learned to run by rendering
    the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Save the whole code used in this section in a Python file called `ppo.py` and
    then open the terminal and run the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how our trained cheetah bot learned to run, as *Figure 16.13* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.13: 2D cheetah bot learning to run'
  prefs: []
  type: TYPE_NORMAL
- en: Making a GIF of a trained agent
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we learned how to train the cheetah bot to run using
    PPO. Can we also create a GIF file of our trained agent? Yes! Let's see how to
    do that.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the list for storing images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment, where `agent` is the agent
    we trained in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment and get the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'For every step in the environment, save the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the GIF file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will have a new file called `HalfCheetah.gif`, as *Figure 16.14* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_16_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.14: GIF of the trained agent'
  prefs: []
  type: TYPE_NORMAL
- en: In this way, we can obtain a GIF of our trained agent. In the next section,
    we will learn how to implement GAIL using Stable Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing GAIL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's explore how to implement **Generative Adversarial Imitation
    Learning** (**GAIL**) with Stable Baselines. In *Chapter 15*, *Imitation Learning
    and Inverse RL*, we learned that we use the generator to generate the state-action
    pair in a way that the discriminator is not able to distinguish whether the state-action
    pair is generated using the expert policy or the agent policy. We train the generator
    to generate a policy similar to an expert policy using TRPO, while the discriminator
    is a classifier and it is optimized using Adam.
  prefs: []
  type: TYPE_NORMAL
- en: To implement GAIL, we need expert trajectories so that our generator learns
    to mimic the expert trajectory. Okay, so how can we obtain the expert trajectory?
    First, we use the TD3 algorithm to generate expert trajectories and then create
    an expert dataset. Then, using this expert dataset, we train our GAIL agent. Note
    that instead of using TD3, we can also use any other algorithm for generating
    expert trajectories.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the TD3 agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the expert trajectories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the expert dataset using the expert trajectories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the GAIL agent with the expert dataset (expert trajectories):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the GAIL agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: After training, we can also render the environment and see how our trained agent
    performs in the environment. That's it, implementing GAIL using Stable Baselines
    is that simple.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by understanding what Stable Baselines is and how to
    install it. Then, we learned to create our first agent with Stable Baselines using
    a DQN. We also learned how to save and load an agent. Next, we learned how to
    create multiple independent environments using vectorization. We also learned
    about two types of vectorized environment called SubprocVecEnv and DummyVecEnv.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that in SubprocVecEnv, we run each environment in a different process,
    whereas in DummyVecEnv, we run each environment in the same process.
  prefs: []
  type: TYPE_NORMAL
- en: Later, we learned how to implement a DQN and its variants to play Atari games
    using Stable Baselines. Next, we learned how to implement A2C and also how to
    create a custom policy network. Moving on, we learned how to implement DDPG and
    also how to view the computational graph in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: Going further, we learned how to set up the MuJoCo environment and how to train
    an agent to walk using TRPO. We also learned how to record a video of a trained
    agent. Next, we learned how to implement PPO and how to make a GIF of a trained
    agent. At the end of the chapter, we learned how to implement generative adversarial
    imitation learning using Stable Baselines.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s put our knowledge of Stable Baselines to the test. Try answering the
    following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Stable Baselines?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you store and load a trained agent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a vectorized environment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between the SubprocVecEnv and DummyVecEnv environments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you visualize a computational graph in TensorBoard?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you record a video of a trained agent?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, check the following resource:'
  prefs: []
  type: TYPE_NORMAL
- en: Check out the Stable Baselines documentation, available at [https://stable-baselines.readthedocs.io/en/master/index.html](https://stable-baselines.readthedocs.io/en/master/index.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
