["```\nimport gym\nimport numpy as np \n```", "```\nenv = gym.make('FrozenLake-v0') \n```", "```\nenv.render() \n```", "```\ndef value_iteration(env): \n```", "```\n num_iterations = 1000 \n```", "```\n threshold = 1e-20 \n```", "```\n gamma = 1.0 \n```", "```\n value_table = np.zeros(env.observation_space.n) \n```", "```\n for i in range(num_iterations): \n```", "```\n updated_value_table = np.copy(value_table) \n```", "```\n for s in range(env.observation_space.n): \n```", "```\n Q_values = [sum([prob*(r + gamma * updated_value_table[s_])\n                             for prob, s_, r, _ in env.P[s][a]]) \n                                   for a in range(env.action_space.n)] \n```", "```\n value_table[s] = max(Q_values) \n```", "```\n if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n             break\n\n    return value_table \nvalue_iteration function is shown to provide more clarity:\n```", "```\ndef value_iteration(env):\n\n    num_iterations = 1000\n    threshold = 1e-20\n    gamma = 1.0    \n\n    value_table = np.zeros(env.observation_space.n)\n\n    for i in range(num_iterations):\n        updated_value_table = np.copy(value_table) \n\n        for s in range(env.observation_space.n):\n\n            Q_values = [sum([prob*(r + gamma * updated_value_table[s_])\n                             for prob, s_, r, _ in env.P[s][a]])\n                                   for a in range(env.action_space.n)]\n\n            value_table[s] = max(Q_values)\n\n        if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n             break\n\n    return value_table \n```", "```\ndef extract_policy(value_table): \n```", "```\n gamma = 1.0 \n```", "```\n policy = np.zeros(env.observation_space.n) \n```", "```\n for s in range(env.observation_space.n): \n```", "```\n Q_values = [sum([prob*(r + gamma * value_table[s_])\n                             for prob, s_, r, _ in env.P[s][a]])\n                                   for a in range(env.action_space.n)] \n```", "```\n policy[s] = np.argmax(np.array(Q_values))\n\n    return policy \nextract_policy function is shown here to give us more clarity: \n```", "```\ndef extract_policy(value_table):\n    gamma = 1.0\n\n    policy = np.zeros(env.observation_space.n) \n\n    for s in range(env.observation_space.n):\n\n        Q_values = [sum([prob*(r + gamma * value_table[s_])\n                             for prob, s_, r, _ in env.P[s][a]]) \n                                   for a in range(env.action_space.n)]\n\n        policy[s] = np.argmax(np.array(Q_values)) \n\n    return policy \n```", "```\noptimal_value_function = value_iteration(env) \n```", "```\noptimal_policy = extract_policy(optimal_value_function) \n```", "```\nprint(optimal_policy) \n```", "```\n[0\\. 3\\. 3\\. 3\\. 0\\. 0\\. 0\\. 0\\. 3\\. 1\\. 0\\. 0\\. 0\\. 2\\. 1\\. 0.] \n```", "```\npolicy = random_policy\nvalue_function = compute_value_function(policy) \n```", "```\nnew_policy = extract_policy(value_function) \n```", "```\npolicy = new_policy \nvalue_function = compute_value_function(policy) \n```", "```\npolicy = random_policy\nfor i in range(num_iterations): \n    value_function = compute_value_function(policy)\n    new_policy = extract_policy(value_function)\n    if value_function = optimal:\n        break\n    else:\n        policy = new_policy \n```", "```\npolicy = random_policy\nfor i in range(num_iterations): \n    value_function = compute_value_function(policy)\n    new_policy = extract_policy(value_function)\n**if** **policy == new_policy:**\n        break\n    else:\n        policy = new_policy \n```", "```\nimport gym\nimport numpy as np \n```", "```\nenv = gym.make('FrozenLake-v0') \n```", "```\ndef compute_value_function(policy): \n```", "```\n num_iterations = 1000 \n```", "```\n threshold = 1e-20 \n```", "```\n gamma = 1.0 \n```", "```\n value_table = np.zeros(env.observation_space.n) \n```", "```\n for i in range(num_iterations): \n```", "```\n updated_value_table = np.copy(value_table) \n```", "```\n for s in range(env.observation_space.n): \n```", "```\n a = policy[s] \n```", "```\n value_table[s] = sum(\n                [prob * (r + gamma * updated_value_table[s_])\n                    for prob, s_, r, _ in env.P[s][a]]) \n```", "```\n if (np.sum(np.fabs(updated_value_table - value_table)) <= threshold):\n             break\n\n    return value_table \n```", "```\ndef extract_policy(value_table):\n\n    gamma = 1.0\n    policy = np.zeros(env.observation_space.n) \n    for s in range(env.observation_space.n):\n\n        Q_values = [sum([prob*(r + gamma * value_table[s_])\n                             for prob, s_, r, _ in env.P[s][a]]) \n                                   for a in range(env.action_space.n)] \n\n        policy[s] = np.argmax(np.array(Q_values)) \n\n    return policy \n```", "```\ndef policy_iteration(env): \n```", "```\n num_iterations = 1000 \n```", "```\n policy = np.zeros(env.observation_space.n) \n```", "```\n for i in range(num_iterations): \n```", "```\n value_function = compute_value_function(policy) \n```", "```\n new_policy = extract_policy(value_function) \n```", "```\n if (np.all(policy == new_policy)):\n            break \n```", "```\n policy = new_policy\n\n    return policy \n```", "```\noptimal_policy = policy_iteration(env) \n```", "```\nprint(optimal_policy) \n```", "```\narray([0., 3., 3., 3., 0., 0., 0., 0., 3., 1., 0., 0., 0., 2., 1., 0.]) \n```"]