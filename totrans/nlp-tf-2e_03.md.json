["```\ndef download_data(url, data_dir):\n    \"\"\"Download a file if not present, and make sure it's the right\n    size.\"\"\"\n\n    os.makedirs(data_dir, exist_ok=True)\n    file_path = os.path.join(data_dir, 'bbc-fulltext.zip')\n\n    if not os.path.exists(file_path):\n        print('Downloading file...')\n        filename, _ = urlretrieve(url, file_path)\n    else:\n        print(\"File already exists\")\n\n    extract_path = os.path.join(data_dir, 'bbc')\n    if not os.path.exists(extract_path):\n\n        with zipfile.ZipFile(\n            os.path.join(data_dir, 'bbc-fulltext.zip'),\n        'r'\n        ) as zipf:\n            zipf.extractall(data_dir)\n\n    else:\n        print(\"bbc-fulltext.zip has already been extracted\") \n```", "```\nurl = 'http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip'\ndownload_data(url, 'data') \n```", "```\ndef read_data(data_dir):\n    news_stories = []\n    print(\"Reading files\")\n    for root, dirs, files in os.walk(data_dir):\n        for fi, f in enumerate(files):\n            if 'README' in f:\n                continue\n            print(\".\"*fi, f, end='\\r')\n            with open(os.path.join(root, f), encoding='latin-1') as f:\n                story = []\n                for row in f:               \n                    story.append(row.strip())\n                story = ' '.join(story)                        \n                news_stories.append(story)                \n    print(f\"\\nDetected {len(news_stories)} stories\")\n    return news_stories \n```", "```\nnews_stories = read_data(os.path.join('data', 'bbc'))\nprint(f\"{sum([len(story.split(' ')) for story in news_stories])} words found in the total news set\")\nprint('Example words (start): ',news_stories[0][:50])\nprint('Example words (end): ',news_stories[-1][-50:]) \n```", "```\nReading files\n............. 361.txt\nDetected 2225 stories\n865163 words found in the total news set\nExample words (start):  Windows worm travels with Tetris  Users are being \nExample words (end):  is years at Stradey as \"the best time of my life.\" \n```", "```\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(\n    num_words=None,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_'{|}~\\t\\n',\n    lower=True,\nsplit=' ' \n) \n```", "```\ntokenizer.fit_on_texts(news_stories) \n```", "```\nn_vocab = len(tokenizer.word_index.items())+1\nprint(f\"Vocabulary size: {n_vocab}\")\nprint(\"\\nWords at the top\")\nprint('\\t', dict(list(tokenizer.word_index.items())[:10]))\nprint(\"\\nWords at the bottom\")\nprint('\\t', dict(list(tokenizer.word_index.items())[-10:])) \n```", "```\nVocabulary size: 32361\nWords at the top\n    {'the': 1, 'to': 2, 'of': 3, 'and': 4, 'a': 5, 'in': 6, 'for': 7, 'is': 8, 'that': 9, 'on': 10}\nWords at the bottom\n    {'counsellor': 32351, \"'frag'\": 32352, 'relasing': 32353, \"'real'\": 32354, 'hrs': 32355, 'enviroment': 32356, 'trifling': 32357, '24hours': 32358, 'ahhhh': 32359, 'lol': 32360} \n```", "```\nfrom tensorflow.keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(\n    num_words=15000,\n    filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_'{|}~\\t\\n',\n    lower=True, split=' ', oov_token='',    \n)\ntokenizer.fit_on_texts(news_stories) \n```", "```\nprint(f\"Original: {news_stories[0][:100]}\") \n```", "```\nprint(f\"Sequence IDs: {tokenizer.texts_to_sequences([news_stories[0][:100]])[0]}\") \n```", "```\nOriginal: Ad sales boost Time Warner profit  Quarterly profits at US media giant TimeWarner jumped 76% to $1.1\nSequence IDs: [4223, 187, 716, 66, 3596, 1050, 3938, 626, 21, 49, 303, 717, 8263, 2972, 5321, 3, 108, 108] \n```", "```\nnews_sequences = tokenizer.texts_to_sequences(news_stories) \n```", "```\nsample_word_ids = news_sequences[0][:5]\nsample_phrase = ' '.join([tokenizer.index_word[wid] for wid in sample_word_ids])\nprint(f\"Sample phrase: {sample_phrase}\")\nprint(f\"Sample word IDs: {sample_word_ids }\\n\") \n```", "```\nSample phrase: ad sales boost time warner\nSample word IDs: [4223, 187, 716, 66, 3596] \n```", "```\nwindow_size = 1 # How many words to consider left and right. \n```", "```\ninputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n    sequence=sample_word_ids, \n    vocabulary_size=n_vocab, \n    window_size=window_size, \n    negative_samples=1.0, \n    shuffle=False,\n    categorical=False, \n    sampling_table=None, \n    seed=None\n) \n```", "```\nprint(\"Sample skip-grams\")\nfor inp, lbl in zip(inputs, labels):\n    print(f\"\\tInput: {inp} ({[tokenizer.index_word[wi] for wi in inp]}) /\n    Label: {lbl}\") \n```", "```\nSample skip-grams\n    Input: [4223, 187] (['ad', 'sales']) / Label: 1\n    Input: [187, 4223] (['sales', 'ad']) / Label: 1\n    Input: [187, 716] (['sales', 'boost']) / Label: 1\n    Input: [716, 187] (['boost', 'sales']) / Label: 1\n    Input: [716, 66] (['boost', 'time']) / Label: 1\n    Input: [66, 716] (['time', 'boost']) / Label: 1\n    Input: [66, 3596] (['time', 'warner']) / Label: 1\n    Input: [3596, 66] (['warner', 'time']) / Label: 1\n    Input: [716, 9685] (['boost', \"kenya's\"]) / Label: 0\n    Input: [3596, 12251] (['warner', 'rear']) / Label: 0\n    Input: [4223, 3325] (['ad', 'racing']) / Label: 0\n    Input: [66, 7978] (['time', 'certificate']) / Label: 0\n    Input: [716, 12756] (['boost', 'crushing']) / Label: 0\n    Input: [66, 14543] (['time', 'touchy']) / Label: 0\n    Input: [187, 3786] (['sales', '9m']) / Label: 0\n    Input: [187, 3917] (['sales', 'doherty']) / Label: 0 \n```", "```\ninputs, labels = tf.keras.preprocessing.sequence.skipgrams(\n    sample_phrase_word_ids, \n    vocabulary_size=len(tokenizer.word_index.items())+1, \n    window_size=window_size, \n    negative_samples=0, \n    shuffle=False    \n)\ninputs, labels = np.array(inputs), np.array(labels) \n```", "```\nnegative_sampling_candidates, true_expected_count, sampled_expected_count = tf.random.log_uniform_candidate_sampler(\n    true_classes=inputs[:1, 1:], # [b, 1] sized tensor\n    num_true=1, # number of true words per example\n    num_sampled=10,\n    unique=True,\n    range_max=n_vocab,            \n    name=\"negative_sampling\"\n) \n```", "```\nPositive sample: [[187]]\nNegative samples: [   1   10 9744 3062  139    5   14   78 1402  115]\ntrue_expected_count: [[0.00660027]]\nsampled_expected_count: [4.0367463e-01 1.0333969e-01 1.2804421e-04 4.0727769e-04 8.8460185e-03\n 1.7628242e-01 7.7631921e-02 1.5584969e-02 8.8879210e-04 1.0659459e-02] \n```", "```\ndef skip_gram_data_generator(sequences, window_size, batch_size, negative_samples, vocab_size, seed=None): \n```", "```\n rand_sequence_ids = np.arange(len(sequences))\n    np.random.shuffle(rand_sequence_ids) \n```", "```\n for si in rand_sequence_ids:\n\n        positive_skip_grams, _ = \n        tf.keras.preprocessing.sequence.skipgrams(\n            sequences[si], \n            vocabulary_size=vocab_size, \n            window_size=window_size, \n            negative_samples=0.0, \n            shuffle=False,\n            sampling_table=sampling_table,\n            seed=seed\n        ) \n```", "```\nsampling_table = tf.keras.preprocessing.sequence.make_sampling_table(\n    n_vocab, sampling_factor=1e-05\n) \n```", "```\n targets, contexts, labels = [], [], []\n\n        for target_word, context_word in positive_skip_grams:\n            context_class = tf.expand_dims(tf.constant([context_word], \n            dtype=\"int64\"), 1)\n\n            negative_sampling_candidates, _, _ = \n            tf.random.log_uniform_candidate_sampler(\n              true_classes=context_class,\n              num_true=1,\n              num_sampled=negative_samples,\n              unique=True,\n              range_max=vocab_size,\n              name=\"negative_sampling\")\n            # Build context and label vectors (for one target word)\n            context = tf.concat(\n                [tf.constant([context_word], dtype='int64'), \n                negative_sampling_candidates],\n                axis=0\n            )\n            label = tf.constant([1] + [0]*negative_samples, \n            dtype=\"int64\")\n            # Append each element from the training example to global\n            # lists.\n            targets.extend([target_word]*(negative_samples+1))\n            contexts.append(context)\n            labels.append(label) \n```", "```\n contexts, targets, labels = np.concatenate(contexts), \n        np.array(targets), np.concatenate(labels)\n\n        # If seed is not provided generate a random one\n        if not seed:\n            seed = random.randint(0, 10e6)\n        np.random.seed(seed)\n        np.random.shuffle(contexts)\n        np.random.seed(seed)\n        np.random.shuffle(targets)\n        np.random.seed(seed)\n        np.random.shuffle(labels) \n```", "```\n for eg_id_start in range(0, contexts.shape[0], batch_size): \n            yield (\n                targets[eg_id_start: min(eg_id_start+batch_size, \n                inputs.shape[0])], \n                contexts[eg_id_start: min(eg_id_start+batch_size, \n                inputs.shape[0])]\n            ), labels[eg_id_start: min(eg_id_start+batch_size, \n               inputs.shape[0])] \n```", "```\nbatch_size = 4096 # Data points in a single batch\nembedding_size = 128 # Dimension of the embedding vector.\nwindow_size=1 # We use a window size of 1 on either side of target word\nnegative_samples = 4 # Number of negative samples generated per example\nepochs = 5 # Number of epochs to train for\n# We pick a random validation set to sample nearest neighbors\nvalid_size = 16 # Random set of words to evaluate similarity on.\n# We sample valid datapoints randomly from a large window without always\n# being deterministic\nvalid_window = 250\n# When selecting valid examples, we select some of the most frequent words # as well as some moderately rare words as well\nnp.random.seed(54321)\nrandom.seed(54321)\nvalid_term_ids = np.array(random.sample(range(valid_window), valid_size))\nvalid_term_ids = np.append(\n    valid_term_ids, random.sample(range(1000, 1000+valid_window), \n    valid_size),\n    axis=0\n) \n```", "```\nimport tensorflow.keras.backend as K\nK.clear_session() \n```", "```\n# Inputs - skipgrams() function outputs target, context in that order\ninput_1 = tf.keras.layers.Input(shape=(), name='target')\ninput_2 = tf.keras.layers.Input(shape=(), name='context') \n```", "```\n# Two embeddings layers are used one for the context and one for the\n# target\ntarget_embedding_layer = tf.keras.layers.Embedding(\n    input_dim=n_vocab, output_dim=embedding_size, \n    name='target_embedding'\n)\ncontext_embedding_layer = tf.keras.layers.Embedding(\n    input_dim=n_vocab, output_dim=embedding_size, \n    name='context_embedding'\n) \n```", "```\n# Lookup outputs of the embedding layers\ntarget_out = target_embedding_layer(input_1)\ncontext_out = context_embedding_layer(input_2) \n```", "```\n# Computing the dot product between the two \nout = tf.keras.layers.Dot(axes=-1)([context_out, target_out]) \n```", "```\n# Defining the model\nskip_gram_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='skip_gram_model') \n```", "```\n# Compiling the model\nskip_gram_model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), optimizer='adam', metrics=['accuracy']) \n```", "```\nskip_gram_model.summary() \n```", "```\nModel: \"skip_gram_model\"\n________________________________________________________________________\nLayer (type)                    Output Shape     Param   # Connected to\n========================================================================\ncontext (InputLayer)            [(None,)]        0         \n_______________________________________________________________________\ntarget (InputLayer)             [(None,)]        0         \n_______________________________________________________________________\ncontext_embedding (Embedding)   (None, 128)     1920128    context[0][0]\n_______________________________________________________________________\ntarget_embedding (Embedding)    (None, 128)     1920128    target[0][0]\n_______________________________________________________________________\ndot (Dot)                        (None, 1)      0  context_embedding[0][0]\n                                                   target_embedding[0][0]\n=======================================================================\nTotal params: 3,840,256\nTrainable params: 3,840,256\nNon-trainable params: 0\n________________________________________________________________________ \n```", "```\nclass ValidationCallback(tf.keras.callbacks.Callback):\n\n    def __init__(self, valid_term_ids, model_with_embeddings, tokenizer):\n\n        self.valid_term_ids = valid_term_ids\n        self.model_with_embeddings = model_with_embeddings\n        self.tokenizer = tokenizer\n\n        super().__init__()\n\n    def on_epoch_end(self, epoch, logs=None):\n        \"\"\" Validation logic \"\"\"\n        # We will use context embeddings to get the most similar words\n        # Other strategies include: using target embeddings, mean\n        # embeddings after avaraging context/target\n        embedding_weights = \n        self.model_with_embeddings.get_layer(\n            \"context_embedding\"\n        ).get_weights()[0]\n        normalized_embeddings = embedding_weights / \n        np.sqrt(np.sum(embedding_weights**2, axis=1, keepdims=True))\n\n        # Get the embeddings corresponding to valid_term_ids\n        valid_embeddings = normalized_embeddings[self.valid_term_ids, \n        :]\n\n        # Compute the similarity between valid_term_ids and all the\n        # embeddings\n        # V x d (d x D) => V x D\n        top_k = 5 # Top k items will be displayed\n        similarity = np.dot(valid_embeddings, normalized_embeddings.T)\n\n        # Invert similarity matrix to negative\n        # Ignore the first one because that would be the same word as the\n        # probe word\n        similarity_top_k = np.argsort(-similarity, axis=1)[:, 1: \n        top_k+1]\n\n        # Print the output\n        for i, term_id in enumerate(valid_term_ids):\n            similar_word_str = ', '.join([self.tokenizer.index_word[j] \n            for j in similarity_top_k[i, :] if j > 1])\n            print(f\"{self.tokenizer.index_word[term_id]}: \n            {similar_word_str }\")\n        print('\\n') \n```", "```\nskipgram_validation_callback = ValidationCallback(valid_term_ids, skip_gram_model, tokenizer)\nfor ei in range(epochs):\n\n    print(f\"Epoch: {ei+1}/{epochs} started\")\n\n    news_skip_gram_gen = skip_gram_data_generator(\n        news_sequences, window_size, batch_size, negative_samples, \n        n_vocab\n    )\n\n    skip_gram_model.fit(\n        news_skip_gram_gen, epochs=1, \n        callbacks=skipgram_validation_callback,\n    ) \n```", "```\nEpoch: 5/5 ended\n2233/2233 [==============================] - 146s 65ms/step - loss: 0.4842 - accuracy: 0.8056\nmonths: days, weeks, years, detained, meaning\nwere: are, was, now, davidson, widened\nmr: resignation, scott, tony, stead, article\nchampions: premier, pottage, kampala, danielli, dominique\nbusinesses: medium, port, 2002's, tackling, doug\npositive: electorate, proposal, bolz, visitors', strengthen\npop: 'me', style, lacks, tourism, tuesdays \n```", "```\nbatch_size = 4096 # Data points in a single batch\nembedding_size = 128 # Dimension of the embedding vector.\nwindow_size=1 # We use a window size of 1 on either side of target word\nepochs = 5 # Number of epochs to train for\nnegative_samples = 4 # Number of negative samples generated per example\n# We pick a random validation set to sample nearest neighbors\nvalid_size = 16 # Random set of words to evaluate similarity on.\n# We sample valid datapoints randomly from a large window without always\n# being deterministic\nvalid_window = 250\n# When selecting valid examples, we select some of the most frequent words\n# as well as some moderately rare words as well\nnp.random.seed(54321)\nrandom.seed(54321)\nvalid_term_ids = np.array(random.sample(range(valid_window), valid_size))\nvalid_term_ids = np.append(\n    valid_term_ids, random.sample(range(1000, 1000+valid_window), \n    valid_size),\n    axis=0\n) \n```", "```\nimport tensorflow.keras.backend as K\nK.clear_session() \n```", "```\n# Inputs\ninput_1 = tf.keras.layers.Input(shape=())\ninput_2 = tf.keras.layers.Input(shape=(window_size*2,)) \n```", "```\ncontext_embedding_layer = tf.keras.layers.Embedding(\n    input_dim=n_vocab+1, output_dim=embedding_size, \n    name='context_embedding'\n)\ntarget_embedding_layer = tf.keras.layers.Embedding(\n    input_dim=n_vocab+1, output_dim=embedding_size, \n    name='target_embedding'\n)\ncontext_out = context_embedding_layer(input_2)\ntarget_out = target_embedding_layer(input_1) \n```", "```\nmean_context_out = tf.keras.layers.Lambda(lambda x: tf.reduce_mean(x, axis=1))(context_out) \n```", "```\nout = tf.keras.layers.Dot(axes=-1)([context_out, target_out]) \n```", "```\ncbow_model = tf.keras.models.Model(inputs=[input_1, input_2], outputs=out, name='cbow_model') \n```", "```\ncbow_model.compile(\n    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), \n    optimizer='adam', \n    metrics=['accuracy']\n) \n```", "```\ncbow_validation_callback = ValidationCallback(valid_term_ids, cbow_model, tokenizer) \n```", "```\nfor ei in range(epochs):\n    print(f\"Epoch: {ei+1}/{epochs} started\")\n    news_cbow_gen = cbow_data_generator(\n        news_sequences, \n        window_size, \n        batch_size, \n        negative_samples\n    )\n    cbow_model.fit(\n        news_cbow_gen, \n        epochs=1, \n        callbacks=cbow_validation_callback,\n    ) \n```", "```\nmonths: years, days, weeks, minutes, seasons\nyou: we, they, i, don't, we'll\nwere: are, aren't, have, because, need\nmusic: terrestrial, cameras, casual, divide, camera\nalso: already, previously, recently, rarely, reportedly\nbest: supporting, actress, category, fiction, contenders\nhim: them, me, themselves, won't, censors\nmr: tony, gordon, resignation, cherie, jack\n5bn: 5m, 7bn, 4bn, 8bn, 8m\nchampions: premier, rugby, appearances, irish, midfielder\ndeutsche: austria, austria's, butcher, violence, 1989\nfiles: movies, collections, vast, habit, ballad\npop: fiction, veteran, scrubs, wars, commonwealth \n```"]