- en: Deep Deterministic Policy Gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In earlier chapters, you saw the use of **reinforcement learning** (**RL**)
    to solve discrete action problems, such as those that arise in Atari games. We
    will now build on this to tackle continuous, real-valued action problems. Continuous
    control problems are copious—for example, the motor torque of a robotic arm; the
    steering, acceleration, and braking of an autonomous car; the wheeled robotic
    motion on terrain; and the roll, pitch, and yaw controls of a drone. For these
    problems, we train neural networks in an RL setting to output real-valued actions.
  prefs: []
  type: TYPE_NORMAL
- en: Many continuous control algorithms involve two neural networks—one referred
    to as the **actor** (policy-based), and the other as the **critic** (value-based)—and
    therefore, this family of algorithms is referred to as **Actor-Critic algorithms**.
    The role of the actor is to learn a good policy that can predict good actions
    for a given state. The role of the critic is to ascertain whether the actor undertook
    a good action, and to provide feedback that serves as the learning signal for
    the actor. This is akin to a student-teacher or employee-boss relationship, wherein
    the student or employee undertakes a task or work, and the role of the teacher
    or boss is to provide feedback on the quality of the action performed.
  prefs: []
  type: TYPE_NORMAL
- en: The foundation of continuous control RL is through what is called the **policy
    gradient**, which is an estimate of how much a neural network's weights should
    be altered so as to maximize the long-term cumulative discounted rewards. Specifically,
    it uses the **chain rule** and it is an estimate of the gradient that needs to
    be back-propagated into the actor network for the policy to improve. It is evaluated
    as an average over a mini-batch of samples. We will cover these topics in this
    chapter. In particular, we will cover an algorithm called the **Deep Deterministic
    Policy Gradient** (**DDPG**), which is a state-of-the-art RL algorithm for continuous
    control.
  prefs: []
  type: TYPE_NORMAL
- en: Continuous control has many real-world applications. For instance, continuous
    control can involve the evaluation of the steering, acceleration, and braking
    of an autonomous car. It can also be applied to determine the torques required
    for the actuator motors of a robot. Or, it can be used in biomedical applications,
    where the control could be to determine the muscle movements for a humanoid locomotion.
    Thus, continuous control problem applications abound.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Actor-Critic algorithms and policy gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DDPG
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training and testing DDPG on Pendulum-v0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to successfully complete this chapter, the following items are required:'
  prefs: []
  type: TYPE_NORMAL
- en: Python (2 and above)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (version 1.4 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A computer with at least 8 GB of RAM (higher than this is even better!)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Actor-Critic algorithms and policy gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover what Actor-Critic algorithms are. You will also
    see what policy gradients are and how they are useful to Actor-Critic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: How do students learn at school? Students normally make a lot of mistakes as
    they learn. When they do well at learning a task, their teacher provides positive
    feedback. On the other hand, if students do poorly at a task, the teacher provides
    negative feedback. This feedback serves as the learning signal for the student
    to get better at their tasks. This is the crux of Actor-Critic algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a summary of the steps involved:'
  prefs: []
  type: TYPE_NORMAL
- en: We will have two neural networks—one referred to as the actor, and the other
    as the critic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The actor is like the student, as we described previously, and takes an action
    at a given state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The critic is like the teacher, as we described previously, and provides feedback
    for the actor to learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike a teacher in a school, the critic network should also be trained from
    scratch, which makes the problem challenging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The policy gradient is used to train the actor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The L2 norm on the Bellman update is used to train the critic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The **policy gradient** is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd6043e6-9bab-4949-b906-f96405cf58c5.png)'
  prefs: []
  type: TYPE_IMG
- en: '*J* is the long-term reward function that needs to be maximized, *θ* is the
    policy neural network parameters, *N* is the mini-batch size, *Q(s,a)* is the
    state-action value function, and *π* is the policy. In other words, we compute
    the gradient of the state-action value function with respect to actions and the
    gradient of the policy with respect to the network parameters, multiply them,
    and take an average of them over *N* samples of data from a mini-batch. We can
    then use this policy gradient in a gradient ascent setting to update the policy
    parameters. Note that it is essentially a chain rule of calculus that is being
    used to evaluate the policy gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: Deep Deterministic Policy Gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now delve into the DDPG algorithm, which is a state-of-the-art RL algorithm
    for continuous control. It was originally published by Google DeepMind in 2016
    and has gained a lot of interest in the community, with several new variants proposed
    thereafter. As was the case in DQN, DDPG also uses target networks for stability.
    It also uses a replay buffer to reuse past data, and therefore, it is an off-policy
    RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: The `ddpg.py` file is the main file from which we start the training and testing.
    It will call the training or testing functions, which are present in `TrainOrTest.py`.
    The `AandC.py` file has the TensorFlow code for the actor and the critic networks.
    Finally, `replay_buffer.py` stores the samples in a replay buffer by using a deque
    data structure. We will train the DDPG to learn to hold an inverted pendulum vertically,
    using OpenAI Gym's Pendulum-v0, which has three states and one continuous action,
    which is the torque to be applied to hold the pendulum as vertically inverted.
  prefs: []
  type: TYPE_NORMAL
- en: Coding ddpg.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will first code the `ddpg.py` file. The steps that are involved are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now summarize the DDPG code:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing the required packages**: We will import the required packages and
    other Python files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining the** **train()** **function**: We will define the `train()` function.
    This takes the argument parser object, `args`. We create a TensorFlow session
    as `sess`. The name of the environment is used to make a Gym environment stored
    in the `env` object. We also set the random number of seeds and the maximum number
    of steps for an episode of the environment. We also set the state and action dimensions
    in `state_dim` and `action_dim`, which take the values of `3` and `1`, respectively,
    for the Pendulum-v0 problem. We then create actor and critic objects, which are
    instances of the `ActorNetwork` class and the `CriticNetwork` class, respectively,
    which will be described later, in the `AandC.py file`. We then call the `trainDDPG()`
    function, which will start the training of the RL agent.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, we save the TensorFlow model by using `tf.train.Saver()` and `saver.save()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining the** **test()** **function**: The `test()` function is defined
    next. This will be used once we have finished the training and want to test how
    well our agent is performing. The code is as follows for the `test()` function
    and is very similar to `train()`. We will restore the saved model from `train()` by
    using `tf.train.Saver()` and `saver.restore()`. We call the `testDDPG()` function
    to test the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining the** **main** **function**: Finally, the `main` function is as
    follows. We define an argument parser by using Python''s `argparse`. The learning
    rates for the actor and critic are specified, including the discount factor, `gamma`,
    and the target network exponential average parameter, `tau`. The buffer size,
    mini-batch size, and number of episodes are also specified in the argument parser.
    The environment that we are interested in is Pendulum-v0, and this is also specified
    in the argument parser.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Calling the** **train()** **or** **test()** **function, as appropriate**:
    The mode for running this code is train or test, and it will call the appropriate
    eponymous function, which we defined previously:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: That's it for `ddpg.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Coding AandC.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will specify the `ActorNetwork` class and the `CriticNetwork` class in `AandC.py`.
    The steps involved are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Importing packages**: First, we import the packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '**Define initializers for the weights and biases**: Next, we define the weights
    and biases initializers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining the** **ActorNetwork** **class**: The `ActorNetwork` class is specified
    as follows. First, it receives parameters as arguments in the `__init__` constructor.
    We then call `create_actor_network()`, which will return `inputs`, `out`, and
    `scaled_out` objects. The actor model parameters are stored in `self.network_params`
    by calling TensorFlow''s `tf.trainable_variables()`. We replicate the same for
    the actor''s target network, as well. Note that the target network is required
    for stability reasons; it is identical to the actor in neural network architecture,
    albeit the parameters gradually change. The target network parameters are collected
    and stored in `self.target_network_params` by calling `tf.trainable_variables()` again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining** **self.update_target_network_params**: Next, we define `self.update_target_network_params`,
    which will weigh the current actor network parameters with `tau` and the target
    network''s parameters with `1-tau`, and add them to store them as a TensorFlow
    operation. We are thus gradually updating the target network''s model parameters.
    Note the use of `tf.multiply()` for multiplying the weights with `tau` (or `1-tau`,
    as the case may be). We then create a TensorFlow placeholder called `action_gradient`
    to store the gradient of *Q*, with respect to the action, which is to be supplied
    by the critic. We also use `tf.gradients()` to compute the gradient of the output
    of the policy network with respect to the network parameters. Note that we then
    divide by the `batch_size`, in order to average the summation over a mini-batch.
    This gives us the averaged policy gradient, which we can then use to update the
    actor network parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '**Using Adam optimization**: We use Adam optimization to apply the policy gradients,
    in order to optimize the actor''s policy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining the** **create_actor_network()** **function**: We now define the
    `create_actor_network()` function. We will use a neural network with two layers,
    with `400` and `300` neurons, respectively. The weights are initialized by using
    **Xavier initialization**, and the biases are zeros to begin with. We use the `relu`
    activation function, and also batch normalization, for stability. The final output
    layer has weights initialized with a uniform distribution and a `tanh` activation
    function in order to keep it bounded. For the Pendulum-v0 problem, the actions
    are bounded in the range [*-2,2*], and since `tanh` is bounded in the range [*-1,1*],
    we need to multiply the output by two to scale accordingly; this is done by using
    `tf.multiply()`, where `action_bound = 2` for the inverted pendulum problem:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '**Define the actor functions**: Finally, we have the remaining functions that
    are required to complete the `ActorNetwork` class. We will define `train()`, which
    will run a session on `self.optimize`; the `predict()` function runs a session
    on `self.scaled_out`, that is, the output of the `ActorNetwork`; the `predict_target()`
    function will run a session on `self.target_scaled_out`, that is, the output action
    of the actor''s target network. Then, `update_target_network()` will run a session
    on `self.update_target_network_params`, which will perform the weighted average
    of the network parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Finally, the `get_num_trainable_vars()` function returns a count of the number
    of trainable variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining** **CriticNetwork class**: We will now define the `CriticNetwork`
    class. Similar to `ActorNetwork`, we receive the model hyperparameters as arguments.
    We then call the `create_critic_network()` function, which will return `inputs`,
    `action`, and `out`. We also create the target network for the critic by calling
    `create_critic_network()` again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '**Critic target network**: Similar to the actor''s target, the critic''s target
    network is also updated by using weighted averaging. We then create a TensorFlow
    placeholder called `predicted_q_value`, which is the target value. We then define
    the L2 norm in `self.loss`, which is the quadratic error on the Bellman residual.
    Note that `self.out` is the *Q(s,a)* that we saw earlier, and `predicted_q_value`
    is the *r + γQ(s'',a'')* in the Bellman equation. Again, we use the Adam optimizer
    to minimize this L2 loss function. We then evaluate the gradient of *Q(s,a)* with
    respect to the actions by calling `tf.gradients()`, and we store this in `self.action_grads`.
    This gradient is used later in the computation of the policy gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining create_critic_network()**: Next, we will define the `create_critic_network()`
    function. The critic network is also similar to the actor in architecture, except
    that it takes both the states and the actions as input. There are two hidden layers,
    with `400` and `300` neurons, respectively. The last output layer has only one
    neuron, that is, is the *Q(s,a)* state-action value function. Note that the last
    layer has no activation function, as `Q(s,a)` is, in theory, unbounded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the functions required to complete the `CriticNetwork` are as follows.
    These are similar to the `ActorNetwork`, so we do not elaborate further for brevity.
    One difference, however, is the `action_gradients()` function, which is the gradient
    of *Q(s,a)* with respect to the actions, which is computed by the critic and supplied
    to the actor, to be used in the evaluation of the policy gradients:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: That's it for `AandC.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Coding TrainOrTest.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `trainDDPG()` and `testDDPG()` functions that we used earlier will now
    be defined in `TrainOrTest.py`. The steps that are involved are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Import packages and functions**: The `TrainOrTest.py` file starts with the
    importing of the packages and other Python files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '**Define the trainDDPG() function**: Next, we define the `trainDDPG()` function.
    First, we initialize all of the networks by calling a `sess.run()` on `tf.global_variables_initializer()`.
    Then, we initialize the target network weights and the replay buffer. Then, we
    start the main loop over the training episodes. Inside of this loop, we reset
    the environment (Pendulum-v0, in our case) and also start the loop over time steps
    for each episode (recall that each episode has a `max_episode_len` number of time
    steps).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The actor''s policy is sampled to obtain the action for the current state.
    We feed this action into `env.step()`, which takes one time step of this action
    and, in the process, moves to the next state, `s2`. The environment also gives
    this a reward, `r`, and information on whether the episode is terminated is stored
    in the Boolean variable `terminal`. We add the tuple (`state`, `action`, `reward`,
    `terminal`, `new state`) to the replay buffer for sampling later and for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '**Sample a mini-batch of data from the replay buffer**: Once we have more than
    the mini-batch size of samples in the replay buffer, we sample a mini-batch of
    data from the buffer. For the subsequent state, `s2`, we use the critic''s target
    network to compute the target *Q* value and store it in `target_q`. Note the use
    of the critic''s target and not the critic—this is done for stability reasons.
    We then use the Bellman equation to evaluate the target, `y_i`, which is computed
    as *r + γ Q* for non-Terminal time steps and as *r* for Terminal steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '**Use the preceding to train the actor and critic**: We then train the critic
    for one step on the mini-batch by calling `critic.train()`. Then, we compute the
    gradient of *Q* with respect to the action by calling `critic.action_gradients()` and
    we store it in `grads`; note that this action gradient is used to compute the
    policy gradient, as we mentioned previously. We then train the actor for one step
    by calling `actor.train()` and passing `grads` as an argument, along with the
    state that we sampled from the replay buffer. Finally, we update the actor and
    critic target networks by calling the appropriate functions for the actor and
    critic objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The new state, `s2`, is assigned to the current state, `s`, as we proceed to
    the next time step. If the episode has terminated, we print the episode reward
    and other observations on the screen, and we write them into a text file called
    `pendulum.txt` for later analysis. We also break out of the inner `for` loop,
    as the episode has terminated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '**Defining testDDPG()**: This concludes the `trainDDPG()` function. We will
    now present the `testDDPG()` function that is used to test how well our model
    is performing. The `testDDPG()` function is more or less the same as `trainDDPG()`,
    except that we do not have a replay buffer and we do not train the neural networks.
    Like before, we have two `for` loops—the outer one for episodes, and the inner
    loop over time steps for each episode. We sample actions from the trained actor''s
    policy by using `actor.predict()` and use it to evolve the environment by using
    `env.step()`. Finally, we terminate the episode if `terminal == True`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This concludes `TrainOrTest.py`.
  prefs: []
  type: TYPE_NORMAL
- en: Coding replay_buffer.py
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the deque data structure for storing our replay buffer. The steps
    that are involved are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Import the packages**: First, we import the required packages.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Define the ReplayBuffer class**:We then define the `ReplayBuffer` class,
    with the arguments passed to the `__init__()` constructor. The `self.buffer =
    deque()` function is the instance of the data structure to store the data in a
    queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '**Define** **the add** **and** **size** **functions**: We then define the `add()`
    function to add the experience as a tuple (`state`, `action`, `reward`, `terminal`,
    `new state`). The `self.count` function keeps a count of the number of samples
    we have in the replay buffer. If this count is less than the replay buffer size
    (`self.buffer_size`), we append the current experience to the buffer and increment
    the count. On the other hand, if the count is equal to (or greater than) the buffer
    size, we discard the old samples from the buffer by calling `popleft()`, which
    is a built-in function of deque. Then, we add the experience to the replay buffer;
    the count need not be incremented, as we discarded one old data sample in the
    replay buffer and replaced it with the new data sample or experience, so the total
    number of samples in the buffer remains the same. We also define the `size()`
    function to obtain the current size of the replay buffer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '**Define** **the sample_batch** **and** **clear** **functions**: Next, we define
    the `sample_batch()` function to sample a `batch_size` number of samples from
    the replay buffer. If the count of the number of samples in the buffer is less
    than the `batch_size`, we sample count the number of samples from the buffer.
    Otherwise, we sample the `batch_size` number of samples from the replay buffer.
    Then, we convert these samples to `NumPy` arrays and return them. Lastly, the
    `clear()` function is used to completely clear the reply buffer and make it empty:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: That concludes the code for the DDPG. We will now test it.
  prefs: []
  type: TYPE_NORMAL
- en: Training and testing the DDPG on Pendulum-v0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now train the preceding DDPG code on Pendulum-v0\. To train the DDPG
    agent, simply type the following in the command line at the same level as the
    rest of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This will start the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training is complete, you can also test the trained DDPG agent, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot the episodic rewards during training by using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The plot is presented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b31b8d7-d68d-4efe-9a3c-65afec2fa8e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Plot showing the episode rewards during training for the Pendulum-v0
    problem, using the DDPG'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the DDPG agent has learned the problem very well. The maximum
    rewards are slightly negative, and this is the best for this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to our first continuous actions RL algorithm,
    DDPG, which also happens to be the first Actor-Critic algorithm in this book.
    DDPG is an off-policy algorithm, as it uses a replay buffer. We also covered the
    use of policy gradients to update the actor, and the use of the L2 norm to update
    the critic. Thus, we have two different neural networks. The actor learns the
    policy and the critic learns to evaluate the actor's policy, thereby providing
    a learning signal to the actor. You saw how to compute the gradient of the state-action
    value, *Q(s,a)*, with respect to the action, and also the gradient of the policy,
    both of which are combined to evaluate the policy gradient, which is then used
    to update the actor. We trained the DDPG on the inverted pendulum problem, and
    the agent learned it very well.
  prefs: []
  type: TYPE_NORMAL
- en: We have come a long way in this chapter. You have learned about Actor-Critic
    algorithms and how to code your first continuous control RL algorithm. In the
    next chapter, you will learn about the **A3C algorithm**, which is an on-policy
    deep RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Is the DDPG an on-policy or off-policy algorithm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We used the same neural network architectures for both the actor and the critic.
    Is this required, or can we choose different neural network architectures for
    the actor and the critic?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Can we use the DDPG for Atari Breakout?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why are the biases of the neural networks initialized to small positive values?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is left as an exercise: Can you modify the code in this chapter to train
    an agent to learn InvertedDoublePendulum-v2, which is more challenging than the
    Pendulum-v0 that you saw in this chapter?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is another exercise: Vary the neural network architecture and check whether
    the agent can learn the Pendulum-v0 problem. For instance, keep decreasing the
    number of neurons in the first hidden layer with the values 400, 100, 25, 10,
    5, and 1, and check how the agent performs for the different number of neurons
    in the first hidden layer. If the number of neurons is too small, it can lead
    to information bottlenecks, where the input of the network is not sufficiently
    represented; that is, the information is lost as we go deeper into the neural
    network. Do you observe this effect?'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Continuous control with deep reinforcement learning*, by *Timothy P. Lillicrap*,
    *Jonathan J. Hunt*, *Alexander Pritzel*, *Nicolas Heess*, *Tom Erez*, *Yuval Tassa*,
    *David Silver*, and *Daan Wierstra*, original DDPG paper from *DeepMind*, arXiv:1509.02971:
    [https://arxiv.org/abs/1509.02971](https://arxiv.org/abs/1509.02971)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
