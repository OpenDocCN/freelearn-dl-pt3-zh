<html><head></head><body>
		<div id="_idContainer082">
			<h1 id="_idParaDest-55"><em class="italic"><a id="_idTextAnchor060"/>Chapter 3</em>: Generative Adversarial Network </h1>
			<p><strong class="bold">Generative Adversarial Network</strong>, more commonly known as <strong class="bold">GANs</strong>, are currently the most prominent method in image and video generation. As the inventor of the convolutional neural network, Dr. Yann LeCun, said in 2016, <em class="italic">"...it is the most interesting idea in the last 10 years in machine learning."</em> The images generated using GANs are superior, in terms of realism, to other competing technologies and things have advanced tremendously since their invention in 2014 by then graduate student Ian Goodfellow. </p>
			<p>In this chapter, we will first learn about the fundamentals of GANs and build a DCGAN to generate Fashion MNIST. We'll learn about the challenges in training GANs. Finally, we will learn how to build a WGAN and its variant, WGAN-GP, to resolve many of the challenges involved in generating faces.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Understanding the fundamentals of GANs</li>
				<li>Building a Deep Convolutional GAN (DCGAN)</li>
				<li>Challenges in training GANs</li>
				<li>Building a Wasserstein GAN (WGAN)</li>
			</ul>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor061"/>Technical requirements</h1>
			<p>The Jupyter notebooks and code can be found here:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter03">https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter03</a></p>
			<p>The notebooks used in the chapter are as follows:</p>
			<ul>
				<li><strong class="source-inline">ch3_dcgan.ipynb</strong></li>
				<li><strong class="source-inline">ch3_mode_collapse</strong></li>
				<li><strong class="source-inline">ch3_wgan_fashion_mnist.ipynb</strong></li>
				<li><strong class="source-inline">ch3_wgan_gp_fashion_mnist.ipynb</strong></li>
				<li><strong class="source-inline">ch3_wgan_gp_celeb_a.ipynb</strong></li>
			</ul>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor062"/>Understanding the fundamentals of GANs</h1>
			<p>The purpose of generative models is to learn a data distribution and to sample from it to generate <a id="_idIndexMarker138"/>new data. With the models that we looked at in the previous chapters, namely PixelCNN and VAE, their generative <a id="_idIndexMarker139"/>part gets to look at the image distribution during training. Thus, they are known as <strong class="bold">explicit density models</strong>. In contrast, the generative part in a GAN never gets to look at the images directly; rather, it is only told whether <a id="_idIndexMarker140"/>the generated images look real or fake. For this reason, GANs are categorized as <strong class="bold">implicit density models</strong>. </p>
			<p>We could use an analogy to compare the explicit and implicit models. Let's say an art student, G, was given a collection of Picasso paintings and asked to learn how to draw fake Picasso paintings. The student can look at the collections as they learn to paint, so that is an explicit model. In a different scenario, we ask student G to forge Picasso paintings, but we don't show them any paintings and they don't know what a Picasso painting looks like. The only way they learn is from the feedback they get from student D, who is learning to spot fake Picasso paintings. The feedback is simple – the painting is either <em class="italic">fake</em> or <em class="italic">real</em>. That is our implicit density GAN model. </p>
			<p>Perhaps one day they painted a twisted face by chance and learned from the feedback that it looked like a real Picasso painting. Then they start to draw in that style to fool student D. Students G and D are <a id="_idIndexMarker141"/>the two networks in a GAN, known as the <strong class="bold">generator</strong> and <strong class="bold">discriminator</strong>. This is the biggest difference in the network architecture <a id="_idIndexMarker142"/>compared with other generative models. </p>
			<p>We will start this chapter by learning about the GAN building blocks, followed by the losses. The original GAN does not have reconstruction loss, which is another thing that sets it apart from other algorithms. Then, we will create custom training steps for a GAN, and we'll be ready to train our first GAN.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor063"/>The architecture of a GAN</h2>
			<p>The word <em class="italic">adversarial</em> in Generative Adversarial Network means <em class="italic">involving opposition or disagreement</em> according to the dictionary definition. There are two networks, known as the <a id="_idIndexMarker143"/>generator and discriminator, that compete with each other. The generator, as the name implies, generates <em class="italic">fake</em> images; while the discriminator will look at the generated images to decide whether they are real or fake. Each network is trying to win the game – the discriminator wants to correctly identify every real and fake image and the generator wants to fool the discriminator into believing the fake images generated by it are real. The following diagram shows the architecture of a GAN:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B14538_03_01.jpg" alt="Figure 3.1 – Architecture of a GAN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Architecture of a GAN</p>
			<p>The GAN architecture bears some resemblance to a VAE (see <a href="B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a>, <em class="italic">Variational Autoencoder</em>). In fact, you could rearrange the blocks in a VAE block diagram and add some lines and switches to produce this GAN block diagram. If a VAE was made up of two separate networks, we could think of:</p>
			<ul>
				<li>The GAN's generator as the VAE's decoder </li>
				<li>The GAN's discriminator as the VAE's encoder</li>
			</ul>
			<p>The generator converts low-dimensional and simple distributions into high-dimensional images with a complex distribution, just like a decoder does. In fact, they are identical; we could <a id="_idIndexMarker144"/>simply copy and paste the decoder code and rename it as the generator, and vice versa, and it would just work. The input to the generator is usually samples from a normal distribution, despite some using uniform distribution. </p>
			<p>We send real and fake images to the discriminator in different minibatches. Real images are those from the dataset while fake images are generated by the generator. The discriminator outputs a single value probability of whether the input is real or fake. It is a binary classifier and we could implement it using a CNN. Technically, the discriminator serves a different purpose than the encoder but they both reduce the dimensionality of their inputs. </p>
			<p>Well, it turns out having two networks in a model is not that scary after all. The generator and discriminator are our old friends in disguise and under new names. We already know how to build those models, therefore let's not worry about the details of constructing them now. In fact, the original GAN paper used only a multilayer perceptron, which is made up of some basic dense layers.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor064"/>Value functions</h2>
			<p>The value <a id="_idIndexMarker145"/>function captures <a id="_idIndexMarker146"/>the fundamentals of how a GAN works. The equation is as follows:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/Formula_03_001.jpg" alt=""/>
				</div>
			</div>
			<p>Here:</p>
			<ul>
				<li><em class="italic">D</em> stands for discriminator.</li>
				<li><em class="italic">G</em> is the generator. </li>
				<li><em class="italic">x</em> is input data and <em class="italic">z</em> is a latent variable.</li>
			</ul>
			<p>We will <a id="_idIndexMarker147"/>also use the same notation in the code. This is the function that the generator tries to minimize while the discriminator wants to maximize it.</p>
			<p>When you understand it, the code implementation will be a lot easier and will make a lot of sense. Furthermore, much of our later discussion about the challenges of GANs and <a id="_idIndexMarker148"/>improvements to it revolves around the loss function. Therefore, it is well worth your time studying it. The GAN loss <a id="_idIndexMarker149"/>function is also known as <strong class="bold">adversarial loss</strong> in some literature. It looks rather complex now, but I'll break it down and show you step by step how it can be converted into simple loss functions that we can implement.</p>
			<h3>Discriminator loss</h3>
			<p>The first <a id="_idIndexMarker150"/>right-hand term of the value function is the value to <a id="_idIndexMarker151"/>classify a real image correctly. From the left-hand term, we know the discriminator wants to maximize it. <strong class="bold">Expectation</strong> is a mathematical term that is the sum of the weighted <a id="_idIndexMarker152"/>average of every sample of a random variable. In this equation, the weight is the probability of data, and the variable is the log of the discriminator output as follows:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/Formula_03_002.jpg" alt=""/>
				</div>
			</div>
			<p>In a minibatch of size <em class="italic">N</em>, <em class="italic">p(x)</em> is <em class="italic">1/N</em>. This is because <em class="italic">x</em> is a single image. Instead of trying to maximize it, we can change the sign to minus and try to minimize it instead. This can be done <a id="_idIndexMarker153"/>with the help of the following equation, called the <strong class="bold">log loss</strong>:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/Formula_03_003.jpg" alt=""/>
				</div>
			</div>
			<p>Here:</p>
			<ul>
				<li><em class="italic">y</em><span class="subscript">i</span> is the label, which is <em class="italic">1</em> for real images. </li>
				<li><em class="italic">p(y</em><span class="subscript">i</span><em class="italic">)</em> is the probability of the sample being real.</li>
			</ul>
			<p>The second <a id="_idIndexMarker154"/>right-hand term of the value function is about <a id="_idIndexMarker155"/>fake images; <em class="italic">z</em> is random noise and <em class="italic">G(z)</em> is generated fake images. <em class="italic">D(G(z))</em> is the discriminator's confidence score of how likely the image is to be real. If we use a label of <em class="italic">0</em> for fake images, we can use the same method to cast it into the following equation:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/Formula_03_004.jpg" alt=""/>
				</div>
			</div>
			<p>Now, putting everything together, we have our discriminator loss function, which is binary cross-entropy loss:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/Formula_03_005.jpg" alt=""/>
				</div>
			</div>
			<p>The following code shows how to implement the discriminator loss. You can find the code in Jupyter notebook <strong class="source-inline">ch3_dcgan.ipynb</strong>:</p>
			<p class="source-code">import tf.keras.losses.binary_crossentropy as bce</p>
			<p class="source-code">def discriminator_loss(pred_fake, pred_real):</p>
			<p class="source-code">    real_loss = bce(tf.ones_like(pred_real), pred_real)</p>
			<p class="source-code">    fake_loss = bce(tf.zeros_like(pred_fake), pred_fake)</p>
			<p class="source-code">    d_loss = 0.5 *(real_loss + fake_loss)</p>
			<p class="source-code">    return d_loss</p>
			<p>In our <a id="_idIndexMarker156"/>training, we do a forward pass on real and fake images separately <a id="_idIndexMarker157"/>using the same minibatch size. Therefore, we compute the binary cross-entropy loss for them separately and take the average as the loss.</p>
			<h3>Generator loss</h3>
			<p>The generator <a id="_idIndexMarker158"/>is only involved when the model is evaluating <a id="_idIndexMarker159"/>fake images, thus we only need to look at the second right-hand term of the value function and simplify it to this:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/Formula_03_006.jpg" alt=""/>
				</div>
			</div>
			<p>At the beginning of the training, the generator is not good at generating images, therefore the discriminator is confident in classifying it as <em class="italic">0</em> all the time, making <em class="italic">D(G(z))</em> always <em class="italic">0</em>, and so is <em class="italic">log (1 – 0)</em>. When the error in the model output is always <em class="italic">0</em>, then there is no gradient to backpropagate. As a result, the generator's weights are not updated, and the <a id="_idIndexMarker160"/>generator is not learning. This phenomenon is known as <strong class="bold">saturating gradient</strong> due to there being almost no gradient in the discriminator's sigmoid output. To avoid this problem, the equation is cast from minimizing <em class="italic">1-D(G(z))</em> to maximizing <em class="italic">D(G(z))</em> as follows:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/Formula_03_007.jpg" alt=""/>
				</div>
			</div>
			<p>GANs that <a id="_idIndexMarker161"/>use this function are also known as <strong class="bold">Non-Saturating GANs (NS-GANs)</strong>. In fact, almost every implementation of <strong class="bold">Vanilla GAN</strong> uses this <a id="_idIndexMarker162"/>value function rather than the original GAN function. </p>
			<p class="callout-heading">Vanilla GAN</p>
			<p class="callout">The interest of researchers in GANs exploded soon after their invention and many researchers gave their GAN a name. Some tried to keep track of all the named GANs over the years, but the list got too long. Vanilla GAN is the name used to loosely refer to the first basic GAN without fancy flavors. Vanilla GAN is usually implemented with two or three hidden dense layers.</p>
			<p>We can derive <a id="_idIndexMarker163"/>the generator loss using the same mathematical steps for the discriminator, which will eventually lead to the same discriminator <a id="_idIndexMarker164"/>loss function except that labels of one is used for real images. It can be confusing to beginners as to why to use real labels for fake images. It will be clear if we derive the equation, or we can also understand it as we want to fool the discriminator into assuming that those generated images are real, thus we use the real labels. The code is as follows:</p>
			<p class="source-code">    def generator_loss(pred_fake):</p>
			<p class="source-code">        g_loss = bce(tf.ones_like(pred_fake), pred_fake)</p>
			<p class="source-code">        return g_loss</p>
			<p>Congratulations, you have turned the most complex equation in a GAN into simple binary cross-entropy loss and implemented it in a few lines of code! Now let's look at the GAN training pipeline.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor065"/>GAN training steps</h2>
			<p>To train a <a id="_idIndexMarker165"/>conventional neural network in TensorFlow or any other high-level machine learning framework, we specify the model, loss function, optimizer, and then call <strong class="source-inline">model.fit()</strong>. TensorFlow will do all the work for us – we just sit there and wait for the loss to drop. Unfortunately, we cannot chain the generator and discriminator to be a single model, like we did for the VAE, and call <strong class="source-inline">model.fit()</strong> to train the GAN. </p>
			<p>Before delving into the GAN problem, let's take a pause and refresh ourselves on what happens underneath the hood when doing a single training step:</p>
			<ol>
				<li>Perform a forward pass to compute the loss.</li>
				<li>From the loss, backpropagate the gradients backward with respect to the variables (weights and biases). </li>
				<li>Then, it's the variables update step. The optimizer will scale the gradients and add them to the variables, which completes one training step. </li>
			</ol>
			<p>These are the generic training steps in a deep neural network. The various optimizers differ only in how they calculate the scaling factors. </p>
			<p>Now come <a id="_idIndexMarker166"/>back to the GAN and look at the flow of gradients. When we train with real images, only the discriminator is involved – the network input is a real image and the output is a label of <em class="italic">1</em>. The generator plays no role here and therefore we can't use <strong class="source-inline">model.fit()</strong>. However, we could still fit the model using the discriminator only, that is, <strong class="source-inline">D.fit()</strong> so that it is not the blocking issue. The problem arises when we use fake images and the gradients backpropagate to the generator via the discriminator. So, what is the problem? Let's take the generator loss and discriminator loss for the fake image and put them side by side:</p>
			<p class="source-code">g_loss = bce(tf.ones_like(pred_fake), pred_fake) </p>
			<p class="source-code"># generator</p>
			<p class="source-code">fake_loss = bce(tf.zeros_like(pred_fake), pred_fake) </p>
			<p class="source-code"># generator</p>
			<p>If you try to spot the difference between them, then you'll find that their labels are opposite signs! This means, using generator loss to train the entire model will make the discriminator move in the opposite direction and not learn to discriminate. This is counterproductive and we don't want to have an untrained discriminator that will discourage the generator from learning. For this reason, we must train the generator and discriminator separately. We will freeze the discriminator's variables when training the generator. </p>
			<p>There are two ways to design a GAN training pipeline. One is to use the high-level Keras model, which needs less code and therefore looks more elegant. We'll only need to define the model once, and call <strong class="source-inline">train_on_batch()</strong> to perform all the steps, including the forward pass, backpropagation, and weights update. However, it is less flexible when it comes to implementing more complex loss functions. </p>
			<p>The other <a id="_idIndexMarker167"/>method is to use low-level code so we can control every step. For our first GAN, we will use a low-level <a id="_idIndexMarker168"/>custom training step function from the official TensorFlow GAN tutorial (<a href="https://www.tensorflow.org/tutorials/generative/dcgan">https://www.tensorflow.org/tutorials/generative/dcgan</a>), as shown in the following code:</p>
			<p class="source-code">def train_step(g_input, real_input):</p>
			<p class="source-code">    with tf.GradientTape() as g_tape,\</p>
			<p class="source-code">         tf.GradientTape() as d_tape:</p>
			<p class="source-code">        # Forward pass</p>
			<p class="source-code">        fake_input = G(g_input)</p>
			<p class="source-code">        pred_fake = D(fake_input)</p>
			<p class="source-code">        pred_real = D(real_input)   </p>
			<p class="source-code">        # Calculate losses</p>
			<p class="source-code">        d_loss = discriminator_loss(pred_fake, pred_real)</p>
			<p class="source-code">        g_loss = generator_loss(pred_fake)</p>
			<p><strong class="source-inline">tf.GradientTape()</strong> is used to record the gradients of a single pass. You may have seen another API, <strong class="source-inline">tf.Gradient()</strong>, that has a similar function, but the latter does not work in TensorFlow eager execution. We will see how the three procedural steps mentioned previously get implemented in <strong class="source-inline">train_step()</strong>. The preceding code snippet shows the first step to carry out a forward pass to calculate the losses. </p>
			<p>The second step is to calculate the gradient of the generator and discriminator from their respective losses using a tape gradient:</p>
			<p class="source-code">        gradient_g = g_tape.gradient(g_loss,\ 						G.trainable_variables)</p>
			<p class="source-code">        gradient_d = d_tape.gradient(d_loss,\ 						D.trainable_variables)</p>
			<p>The third and final step is to use the optimizer to apply the gradients to the variables:</p>
			<p class="source-code">        G_optimizer.apply_gradients(zip(gradient_g, 					self.G.trainable_variables))</p>
			<p class="source-code">        D_optimizer.apply_gradients(zip(gradient_d, 					self.D.trainable_variables))</p>
			<p>You have <a id="_idIndexMarker169"/>now learned everything you need to train a GAN. What is left to be done is to set up the input pipeline, generator, and discriminator, and we will go over that in the coming section.</p>
			<p class="callout-heading">Custom model fit</p>
			<p class="callout">After TensorFlow 2.2, it is now possible to create a custom <strong class="source-inline">train_step()</strong> for a Keras model without re-writing the entire training pipeline. Then, we can use <strong class="source-inline">model.fit()</strong> in the usual way. This will also enable the use of multiple GPUs for training. Unfortunately, this new feature was not released in time to make it into the code in this book. However, do check out the TensorFlow tutorial at <a href="https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit">https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit</a> and feel free to <a id="_idIndexMarker170"/>modify the GAN's code to use a custom model fit.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor066"/>Building a Deep Convolutional GAN (DCGAN)</h1>
			<p>Although Vanilla GAN has proven itself as a generative model, it suffers from a few training <a id="_idIndexMarker171"/>problems. One of them is the difficulty in scaling networks to make them deeper in order to increase their capacities. The <a id="_idIndexMarker172"/>authors of <strong class="bold">DCGAN</strong> incorporated a few recent advancements in CNNs at that time to make networks deeper and stabilize the training. These include the removal of the <strong class="bold">maxpool</strong> layer, replacing <a id="_idIndexMarker173"/>it with strided convolutions for downsampling, and the removal of fully connected layers. This has since become the standard way of designing a new CNN. </p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor067"/>Architecture guidelines</h2>
			<p>DCGAN is not strictly a fixed neural network that has layers pre-defined with a fixed set of parameters <a id="_idIndexMarker174"/>such as kernel size and the number of layers. Instead, it is more like architecture design guidelines. The use of batch normalization, activation, and upsampling in DCGAN has influenced the development of GANs. We will therefore look into them more, which should provide guidance in designing our own GAN. </p>
			<h3>Batch normalization</h3>
			<p><strong class="bold">Batch normalization</strong> is informally called <strong class="bold">batchnorm</strong> within the machine learning community. In the early days of deep neural network training, a layer updated its weights <a id="_idIndexMarker175"/>after backpropagation to produce outputs that are closer to the targets. However, the weights of <a id="_idIndexMarker176"/>the subsequent layers have also changed, so it is like a moving goal, and this makes the training of deep networks difficult. Batchnorm solves this by normalizing the input to every layer to have zero mean and unity variance, hence stabilizing the training. These are operations that happen within batchnorm:</p>
			<ul>
				<li>Calculate the mean <em class="italic">µ</em> and standard deviation <em class="italic">σ</em> of tensor <em class="italic">x</em> in a minibatch for every channel (hence the name <em class="italic">batch</em> normalization).</li>
				<li>Normalize the tensor: <em class="italic">x' = (x – µ) / σ</em>.</li>
				<li>Perform an affine transformation: <em class="italic">y = α * x' + β</em>, where <em class="italic">α</em> and <em class="italic">β</em> are trainable variables.</li>
			</ul>
			<p>In a DCGAN, batchnorm is added to both the generator and discriminator, except for the first layer of the discriminator and the last layer of the generator. One thing to note is that newer researches show that batchnorm is not the best normalization technique to use for image generation as it removes some of the important information. We will look at other normalization techniques in later chapters, but we will keep using batchnorm in our GAN until then. One thing that we should know is that in order to use batchnorm, we will have to use a large minibatch, otherwise, the batch statistics can vary greatly from batch to batch and make the training unstable.</p>
			<h3>Activations</h3>
			<p>The <a id="_idIndexMarker177"/>following figure shows the activations <a id="_idIndexMarker178"/>that we will use in DCGAN:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B14538_03_02.jpg" alt="Figure 3.2 – ReLU and leaky ReLU are used in intermediate layers of the generator and discriminator&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – ReLU and leaky ReLU are used in intermediate layers of the generator and discriminator</p>
			<p>As the discriminator's job is to be a binary classifier, we use sigmoid to squeeze the output to within the range of <em class="italic">0</em> (fake) and <em class="italic">1</em> (real). On the other hand, the generator's output uses <em class="italic">tanh</em>, which bounds the images between <em class="italic">-1</em> and <em class="italic">+1</em>. Therefore, we will need to scale our images to this range in the preprocessing step.</p>
			<p>For intermediate layers, the generator uses ReLU in all layers, but the discriminator uses leaky ReLU instead. In standard ReLU, the activation increases linearly with positive input but is zero for all negative input values. This limits the gradient flow when it is negative and thus the generator does not receive gradients in order to update its weights and learn. Leaky ReLU alleviates that problem by allowing small gradients to flow when the activation is negative. </p>
			<p>As we can see in the preceding figure, for input above and equal to <em class="italic">0</em>, it is identical to ReLU where <a id="_idIndexMarker179"/>the output equals input with a slope of <em class="italic">1</em>. For input below <em class="italic">0</em>, the <a id="_idIndexMarker180"/>output is scaled to <em class="italic">0.2</em> of the input. The default slope of leaky ReLU in TensorFlow is <em class="italic">0.3</em> while the DCGAN uses <em class="italic">0.2</em>. It is just a hyperparameter and you are free to try any other values.</p>
			<h3>Upsampling</h3>
			<p>In DCGAN, upsampling in <a id="_idIndexMarker181"/>the generator <a id="_idIndexMarker182"/>is performed using the transpose convolutional layer. However, it has been shown that this will produce a checkerboard pattern in the generated image, especially in images with strong colors. As a result, we replace it with <strong class="source-inline">UpSampling2D</strong>, which performs conventional image resizing methods by using bilinear interpolation.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor068"/>Building a DCGAN for Fashion-MNIST</h2>
			<p>The Jupyter notebook for this exercise is <strong class="source-inline">ch3_dcgan.ipynb</strong>.</p>
			<p>MNIST has <a id="_idIndexMarker183"/>been used in <a id="_idIndexMarker184"/>many introductory machine learning tutorials and we are all familiar with it. With recent advancements in machine learning, this dataset began to look a bit trivial for deep learning. As a result, a new dataset, Fashion-MNIST, has been created as a direct drop-in replacement for the MNIST dataset. It has exactly the same number of training and test examples, 28x28 grayscale images of 10 classes. This is what we will train our DCGAN with.</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B14538_03_03.jpg" alt="Figure 3.3 – Examples of images from the Fashion-MNIST dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Examples of images from the Fashion-MNIST dataset</p>
			<h3>Generator</h3>
			<p>The design <a id="_idIndexMarker185"/>of the generator can be broken into two parts:</p>
			<ul>
				<li>Convert the 1D latent vector into a 3D activation map.</li>
				<li>Double the activation map's spatial resolution until it matches the target image.</li>
			</ul>
			<p>The first thing to do is to work out the number of upsampling stages. As the images have a shape of 28x28, we can use two upsampling stages to increase the dimension from 7-&gt;14-&gt;28. </p>
			<p>For simple data, we can use one convolution layer per upsampling stage, but we could also use more layers. This method is similar to CNNs in that you have several convolution layers working on the same spatial resolution before downsampling.</p>
			<p>Next, we will decide the channel numbers of the first convolutional layer. Let's say we use [512, 256, 128, 1], where the last channel number is the image channel number. With this information, we know the neuron numbers in first dense layer to be <strong class="source-inline">7 x 7 x 512</strong>. The <strong class="source-inline">7x7</strong> is the spatial resolution we worked out and <strong class="source-inline">512</strong> is the filter number in the first convolutional layer. After the dense layer, we reshape it to <strong class="source-inline">(7,7,512)</strong> so it can be fed <a id="_idIndexMarker186"/>into a convolutional layer. Then, we only need to define the filter number of convolutional layers and add the batchnorm and ReLU, as shown in the following code: </p>
			<p class="source-code">def Generator(self, z_dim): </p>
			<p class="source-code">        model = tf.keras.Sequential(name='Generator') </p>
			<p class="source-code">        model.add(layers.Input(shape=[z_dim])) </p>
			<p class="source-code">        model.add(layers.Dense(7*7*512))        </p>
			<p class="source-code">        model.add(layers.BatchNormalization(momentum=0.9)) </p>
			<p class="source-code">        model.add(layers.LeakyReLU())</p>
			<p class="source-code">        model.add(layers.Reshape((7,7,512))) </p>
			<p class="source-code">        model.add(layers.UpSampling2D((2,2), 				    interpolation="bilinear"))</p>
			<p class="source-code">        model.add(layers.Conv2D(256, 3, padding='same')) </p>
			<p class="source-code">        model.add(layers.BatchNormalization(momentum=0.9)) </p>
			<p class="source-code">        model.add(layers.LeakyReLU())         </p>
			<p class="source-code">        model.add(layers.UpSampling2D((2,2), 				    interpolation="bilinear"))        </p>
			<p class="source-code">        model.add(layers.Conv2D(128, 3, padding='same')) </p>
			<p class="source-code">        model.add(layers.LeakyReLU())</p>
			<p class="source-code">        model.add(layers.Conv2D(image_shape[-1], 3,  			padding='same', activation='tanh')) </p>
			<p class="source-code">    return model     </p>
			<p>The model <a id="_idIndexMarker187"/>summary for the generator is as follows:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B14538_03_04.jpg" alt="Figure 3.4 – DCGAN generator model summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – DCGAN generator model summary</p>
			<p>The generator's <a id="_idIndexMarker188"/>model summary shows the activation map shapes that are doubling in spatial resolution <strong class="source-inline">(7×7 to 14×14 to 28×28)</strong> while halving in the channel numbers <strong class="source-inline">(512 to 256 to 128)</strong>.</p>
			<h3>Discriminator</h3>
			<p>The design of <a id="_idIndexMarker189"/>the discriminator is straightforward, just like a simple classifier CNN but with leaky ReLU as activation. As a matter of fact, the discriminator architecture was not even mentioned in the DCGAN paper. As a rule of thumb, the discriminator should have fewer or an equal number of layers as the generator, so it doesn't overpower the generator to stop the latter from learning. The following is the code to create the discriminator:</p>
			<p class="source-code">def Discriminator(self, input_shape): </p>
			<p class="source-code">    model = tf.keras.Sequential(name='Discriminator') </p>
			<p class="source-code">    model.add(layers.Input(shape=input_shape)) </p>
			<p class="source-code">    model.add(layers.Conv2D(32, 3, strides=(2,2),  					 padding='same'))</p>
			<p class="source-code">    model.add(layers.BatchNormalization(momentum=0.9))</p>
			<p class="source-code">    model.add(layers.ReLU()) </p>
			<p class="source-code">    model.add(layers.Conv2D(64, 3, strides=(2,2), 					 padding='same')) </p>
			<p class="source-code">    model.add(layers.BatchNormalization(momentum=0.9)) </p>
			<p class="source-code">    model.add(layers.ReLU())</p>
			<p class="source-code">    model.add(layers.Flatten()) </p>
			<p class="source-code">    model.add(layers.Dense(1, activation='sigmoid')) </p>
			<p class="source-code">    return model</p>
			<p>The model <a id="_idIndexMarker190"/>summary of the discriminator, which is a simple CNN classifier, is shown as follows:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B14538_03_05.jpg" alt="Figure 3.5 – DCGAN discriminator model summary &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – DCGAN discriminator model summary </p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor069"/>Training our DCGAN</h2>
			<p>Now we <a id="_idIndexMarker191"/>can start training our first GAN. The following diagram shows the samples generated during different steps in the training:  </p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B14538_03_06.jpg" alt="Figure 3.6 – Generated images during DCGAN training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Generated images during DCGAN training</p>
			<p>The first row of samples is generated right after network weight initialization and before any training steps. As we can see, they are just some random noise. As training progresses, the generated images become better. However, the generator loss is higher than when it was only generating random noise. </p>
			<p>The loss is not an absolute measurement of generated image quality; it merely provides relative terms to compare the performance of the generator relative to the discriminator and vice versa. The generator loss was low simply because the discriminator <a id="_idIndexMarker192"/>had not learned to do its job well. This is one of the challenges of a GAN where the loss does not give sufficient information about the model's quality.</p>
			<p>The following graphs show the discriminator loss and generator loss during training:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B14538_03_07.jpg" alt="Figure 3.7 – Discriminator and generator training losses&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7 – Discriminator and generator training losses</p>
			<p>We can see that the equilibrium achieved in the first 1,000 steps and the loss remain roughly stable after that. However, the loss isn't definitive in gauging when to stop training. For now, we can save the weights every few epochs and eyeball to select the one that <a id="_idIndexMarker193"/>generates the best-looking images!</p>
			<p>In theory, the global optimal for the discriminator is achieved when <em class="italic">pdiscriminator = pdata</em>. In other words, if <em class="italic">pdata = 0.5</em> as half of the data is real and half is fake, then <em class="italic">pdiscriminator = 0.5</em> will mean it can no longer distinguish between the two classes and the prediction is no better than flipping a coin. </p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor070"/>Challenges in training GANs</h1>
			<p>GANs are <a id="_idIndexMarker194"/>notoriously difficult to train. We'll discuss some of the main challenges in training a GAN.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor071"/>Uninformative loss and metrics</h2>
			<p>When training a CNN for classification or detection tasks, we can look at the shape of the loss <a id="_idIndexMarker195"/>plots to tell whether the network has converged or is overfitting and we'll know when to stop training. Then the metrics will correlate with the loss. For example, classification accuracy is normally the highest when the loss is the lowest. However, we can't do the same with GAN loss, as it doesn't have a minimum but fluctuates around some constant values after training for a while. We also could not correlate the generated image quality with the loss. A few metrics were invented to address this in the early days of GANs and <a id="_idIndexMarker196"/>one of them is the <strong class="bold">inception score.</strong> </p>
			<p>A classification CNN known as <strong class="bold">inception</strong> is used to predict the confidence score of an image belonging <a id="_idIndexMarker197"/>to one of 1,000 categories in the <strong class="source-inline">ImageNet</strong> dataset. If high confidence is recorded for a class, it is more likely to be a real image. There is another <a id="_idIndexMarker198"/>metric known as the <strong class="bold">Fréchet inception distance</strong>, which measures the variety of generated images. These metrics are normally used only in academic papers to make a comparison with other models (so they can claim their models are superior), so we will not cover them in detail in this book. Human visual inspection is still the most reliable way of assessing the quality of generated images.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor072"/>Instability</h2>
			<p>GANs are <a id="_idIndexMarker199"/>extremely sensitive to any change in hyperparameters, including learning rate and filter kernel size. Even after a lot of hyperparameter tuning, and the correct architecture, there are instances while retraining the model where the following can occur: </p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B14538_03_08.jpg" alt="Figure 3.8 – Generator stuck in local minima&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8 – Generator stuck in local minima</p>
			<p>If the network weights are unfortunately randomly initialized to some bad values, the generator could get stuck in some bad local minima and may never recover, while the discriminator keeps improving. As a result, the generator gives up and produces only nonsensical images. This is <a id="_idIndexMarker200"/>also known as <strong class="bold">convergence failure</strong>, where the losses fail to converge. We'll need to stop the training, re-initialize the network, and restart the training. This is also the reason why I haven't chosen a more complex dataset such as CelebA to introduce GANs, but don't worry, we'll get there before the chapter ends. </p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor073"/>Vanishing gradient</h2>
			<p>One reason for instability is the vanishing gradient of the generator. As we've already mentioned, when <a id="_idIndexMarker201"/>we train the generator, the gradient will flow through the discriminator. If the discriminator is confident that the images are fake, then there will be little or even zero gradient to backpropagate <a id="_idIndexMarker202"/>to the generator. The following points are some of the methods of mitigation:</p>
			<ul>
				<li>Reformulating the value function from minimizing log <em class="italic">(1-D(G(z))</em> to maximizing log <em class="italic">D(G(z))</em>, which we already did. In practice, this alone is still not enough.</li>
				<li>Using activation functions that allow more gradients to flow, such as leaky ReLU.</li>
				<li>Balancing between the generator and the discriminator by reducing the discriminator's network capacity or increasing the training steps for the generator.  </li>
				<li>Using <em class="italic">one-sided label smoothing,</em> where the label of the real image is decreased from <em class="italic">1</em> to, say, <em class="italic">0.9</em> to reduce the discriminator's confidence. </li>
			</ul>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor074"/>Mode collapse</h2>
			<p><strong class="bold">Mode collapse</strong> happens when the generator is producing images that look like each other. This is <a id="_idIndexMarker203"/>not to be confused with convergence failure where the GAN produces only garbage images. </p>
			<p>Mode collapse <a id="_idIndexMarker204"/>can happen even when the generated images look great but are limited to small subsets of classes (inter-class mode collapse) or a few of the same images within the class (intra-class mode collapse). We can demonstrate mode collapse by training a Vanilla GAN on a mixture of two Gaussian distributions, which you could run in the <strong class="source-inline">ch3_mode_collapse</strong> notebook. </p>
			<p>The following figure shows the shape of the generated samples during training taking the form of two Gaussian blobs. One sample is round and the other is elliptical:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B14538_03_09.jpg" alt="Figure 3.9 – The top figure is the real samples. The bottom figures show generated samples in two different epochs during training"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9 – The top figure is the real samples. The bottom figures show generated samples in two different epochs during training</p>
			<p>As the Vanilla GAN trains, the generated samples can look like one of two modes in a minibatch <a id="_idIndexMarker205"/>but never two modes at the same time. For Fashion-MNIST, it may be that the generator is producing shoes that look the same every time, regardless. After all, the objective of the generator is to produce realistic-looking images, and it is not penalized for showing the same shoes every time as long as the discriminator deems the images to be real. As proven in the original GAN <a id="_idIndexMarker206"/>paper mathematically, after the discriminator achieved optimality, the generator will work toward optimizing for <strong class="bold">Jensen-Shannon divergence</strong> (<strong class="bold">JSD</strong>). </p>
			<p>For our purposes, we only need to know that JSD is a symmetrical version of <strong class="bold">Kullback-Leibler divergence</strong> (<strong class="bold">KLD</strong>) with an upper bound of <em class="italic">log(2)</em> rather than an infinite upper bound. Unfortunately, JSD is also the cause of mode collapse, as can be illustrated in the following figure:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B14538_03_10.jpg" alt="Figure 3.10 – A standard Gaussian distribution fit on data drawn from a mixture of Gaussians by minimizing KLD, MMD, and JSD (Source: L. Theis et al, 2016, &quot;A Note On The Evaluation of Generative Models,&quot; https://arxiv.org/abs/1511.01844)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10 – A standard Gaussian distribution fit on data drawn from a mixture of Gaussians by minimizing KLD, MMD, and JSD (Source: L. Theis et al, 2016, "A Note On The Evaluation of Generative Models," https://arxiv.org/abs/1511.01844)</p>
			<p>We will <a id="_idIndexMarker207"/>not talk about <strong class="bold">maximum mean discrepancy</strong> (<strong class="bold">MMD</strong>), which is not used in a GAN. The data is two Gaussian distributions <a id="_idIndexMarker208"/>where one has more mass density than the other. A single Gaussian is fitted on the data. In other words, we try to estimate one best mean and standard deviation to describe the two type of Gaussian distribution. With KLD, we see that although the fitted Gaussian leans toward the bigger Gaussian blob, it still provides some coverage to the smaller Gaussian blob. This is not the case for JSD, where it is fitted to only the most prominent Gaussian blob. This explains mode collapse in a GAN – when the probability of some particular generated images is high, when these few modes are <em class="italic">locked</em> by the optimizer.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor075"/>Building a Wasserstein GAN</h1>
			<p>Many have <a id="_idIndexMarker209"/>attempted to solve the instability of GAN training by using heuristic approaches such as trying different network architectures, hyperparameters, and optimizers. One major breakthrough happened in 2016 with the introduction of <strong class="bold">Wasserstein GAN (WGAN)</strong>. </p>
			<p>WGAN alleviates <a id="_idIndexMarker210"/>or even eliminates many of the GAN challenges we've discussed altogether. It no longer requires careful design of network architecture nor careful balancing of the discriminator and the generator. The mode collapse problem is also reduced drastically.  </p>
			<p>The biggest fundamental improvement from the original GAN is the change of the loss function. The theory is that if the two distributions are disjointed, JSD will no longer be continuous, hence not differentiable, resulting in a zero gradient. WGAN solves this by using a new <a id="_idIndexMarker211"/>loss function that is continuous and differentiable everywhere! </p>
			<p>The notebook for this exercise is <strong class="source-inline">ch3_wgan_fashion_mnist.ipynb</strong>.</p>
			<p class="callout-heading">Tips</p>
			<p class="callout">It is alright to not learn how to implement the code in this section, particularly WGAN-GP, which is more complex. Although theoretically superior, we could still train GANs stably using a simpler loss function with carefully designed model architecture and hyperparameters. However, you should try to understand the term Lipschitz constraint as it was used in the development of several advanced techniques, which we will cover in later chapters.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor076"/>Understanding Wasserstein loss</h2>
			<p>Let's remind <a id="_idIndexMarker212"/>ourselves of the non-saturating value function:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/Formula_03_008.jpg" alt=""/>
				</div>
			</div>
			<p>WGAN uses a new loss <a id="_idIndexMarker213"/>function known as the <strong class="bold">Earth mover's distance</strong> or just Wasserstein distance. It measures the distance or the effort needed to transform one distribution into another. Mathematically, it is the minimum distance for every joint distribution between real and generated images, which is intractable, with some mathematical assumptions that are outside the scope of this book, and the value function becomes:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/Formula_03_009.jpg" alt=""/>
				</div>
			</div>
			<p>Now, let's <a id="_idIndexMarker214"/>compare the preceding equation with NS loss and use that to derive the loss function. The most prominent change is that the <em class="italic">log()</em> is gone, and another is the sign of the fake image term changes. The loss function of the first term is therefore:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/Formula_03_010.jpg" alt=""/>
				</div>
			</div>
			<p>This is the average of the discriminator output, multiplied by <em class="italic">-1</em>. We can also generalize it by using <em class="italic">y</em><span class="subscript">i</span> as labels where <em class="italic">+1 is for real images</em>, and <em class="italic">-1 is for fake images</em>. Thus, we can implement Wasserstein loss as a TensorFlow Keras custom loss function as follows:</p>
			<p class="source-code">    def wasserstein_loss(self, y_true, y_pred):</p>
			<p class="source-code">        w_loss = -tf.reduce_mean(y_true*y_pred)</p>
			<p class="source-code">        return w_loss</p>
			<p>As this loss function is no longer binary cross-entropy, the discriminator's objective is no longer classifying or discriminating between real and fake images. Instead, it aims to maximize the score for real images with respect to fake images. For this reason, in WGAN, the discriminator is given a new name of <strong class="bold">critic</strong>. </p>
			<p>The generator and discriminator architecture stays the same. The only change is that the sigmoid is removed from the discriminator's output. Therefore, the critic's prediction is unbounded and can be very large positive and negative values. This is put in check by <a id="_idIndexMarker215"/>implementing the <strong class="bold">1-Lipschitz</strong> constraint. </p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor077"/>Implementing the 1-Lipschitz constraint</h2>
			<p>The mathematical <a id="_idIndexMarker216"/>assumption mentioned in Wasserstein <a id="_idIndexMarker217"/>loss is the <strong class="bold">1-Lipschitz function</strong>. We say the critic <em class="italic">D(x)</em> is <em class="italic">1-Lipschitz</em> if it satisfies the following inequality:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/Formula_03_011.jpg" alt=""/>
				</div>
			</div>
			<p>For two images, <em class="italic">x</em><span class="subscript">1</span> and <em class="italic">x</em><span class="subscript">2</span>, their absolute critic's output difference must be smaller or equal to their average pixel-wise absolute difference. In other words, the critic's outputs should not differ too much for different images – be it real images or fakes. When WGAN was invented, the authors could not think of a proper implementation to enforce inequality. Therefore, they came up with a hack, which is to clip the critic's weights to some small values. By doing that, the layers' outputs and eventually the critics' outputs are capped to some small values. In the WGAN paper, the weights are clipped to the range of <em class="italic">[-0.01, 0.01]</em>.</p>
			<p>Weight clipping can be implemented in two ways. One way is to write a custom constraint function and use that in instantiating a new layer as follows:</p>
			<p class="source-code">class WeightsClip(tf.keras.constraints.Constraint):</p>
			<p class="source-code">    def __init__(self, min_value=-0.01, max_value=0.01):</p>
			<p class="source-code">        self.min_value = min_value</p>
			<p class="source-code">        self.max_value = max_value</p>
			<p class="source-code">    def __call__(self, w):</p>
			<p class="source-code">        return tf.clip_by_value(w, self.min, 						self.max_value)</p>
			<p>We can then pass the function to layers that accept constraint functions as follows:</p>
			<p class="source-code">model = tf.keras.Sequential(name='critics')        </p>
			<p class="source-code">model.add(Conv2D(16, 3, strides=2, padding='same', </p>
			<p class="source-code">                  kernel_constraint=WeightsClip(),</p>
			<p class="source-code">                  bias_constraint=WeightsClip()))</p>
			<p class="source-code">model.add(BatchNormalization(</p>
			<p class="source-code"> 			 beta_constraint=WeightsClip(),</p>
			<p class="source-code"> 		 gamma_constraint=WeightsClip()))</p>
			<p>However, adding the constraint code in every layer creation can make the code look bloated. As we <a id="_idIndexMarker218"/>don't need to cherry-pick which layer to clip, we can use a loop to read the weights and clips and write them back as follows:</p>
			<p class="source-code">for layer in critic.layers:</p>
			<p class="source-code">    weights = layer.get_weights() </p>
			<p class="source-code">    weights = [tf.clip_by_value(w, -0.01, 0.01) for  			w in weights]</p>
			<p class="source-code">    layer.set_weights(weights)</p>
			<p>This is the method we use in the code example.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor078"/>Restructuring training steps</h2>
			<p>In the original GAN theory, the discriminator is supposed to be trained optimally before the generator. That was not possible in practice due to the vanishing gradient of the generator <a id="_idIndexMarker219"/>as the discriminator gets better. Now, with the Wasserstein loss function, the gradient is derivable everywhere and we don't have to worry about the critic being too good to compare with the generator. </p>
			<p>Therefore, in WGAN, the critic is trained for five steps for every one training step for the generator. In order to do this, we will split the critic training step into a separate function, which we can then loop through multiple times:</p>
			<p class="source-code">for _ in range(self.n_critic):</p>
			<p class="source-code">    real_images = next(data_generator)</p>
			<p class="source-code">    critic_loss = self.train_critic(real_images, 						    batch_size)</p>
			<p>We will also need to rework the generator training step. In our DCGAN code, we use two models – the generator and discriminator. To train the generator, we also use gradient tape to update the weights. All these are rather cumbersome. There is another way of implementing the training step for the generator by merging the two models into one as follows:</p>
			<p class="source-code">self.critic = self.build_critic()</p>
			<p class="source-code">self.critic.trainable = False</p>
			<p class="source-code">self.generator = self.build_generator()</p>
			<p class="source-code">critic_output = self.critic(self.generator.output)</p>
			<p class="source-code">self.model = Model(self.generator.input, critic_output)</p>
			<p class="source-code">self.model.compile(loss = self.wasserstein_loss,  			  optimizer = RMSprop(3e-4))</p>
			<p class="source-code">self.critic.trainable = True</p>
			<p>In the preceding code, we freeze the critic layers by setting <strong class="source-inline">trainable=False</strong>, and we chain <a id="_idIndexMarker220"/>that to the generator to create a new model and compile it. After that, we can set the critic to be trainable again, which will not affect the model that we have already compiled. </p>
			<p>We use the <strong class="source-inline">train_on_batch()</strong> API to perform a single training step that will automatically do the forward pass, loss calculation, backpropagation, and weights update:</p>
			<p class="source-code">g_loss = self.model.train_on_batch(g_input,  						  real_labels)</p>
			<p>For this exercise, we resize the image shape to 32x32 so we can use deeper layers in the generator to upscale the image. The WGAN generator and discriminator architecture are shown in the following model summaries:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B14538_03_11.jpg" alt="Figure 3.11 – Model summary of the WGAN's generator&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11 – Model summary of the WGAN's generator</p>
			<p>The <a id="_idIndexMarker221"/>generator architecture follows the usual design with decreasing channel numbers as the feature map's size doubles. The following is the model summary of the WGAN's critic:</p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B14538_03_12.jpg" alt="Figure 3.12 – Model summary of the WGAN's critic&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12 – Model summary of the WGAN's critic</p>
			<p>Despite <a id="_idIndexMarker222"/>the improvement over DCGAN, I found it difficult to train a WGAN and the image quality produced is no more superior than DCGAN. We'll now implement a WGAN variant that trains faster and produces sharper images.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor079"/>Implementing gradient penalty (WGAN-GP)</h2>
			<p>Weight clipping is not an ideal way to enforce a Lipschitz constraint, as acknowledged by the WGAN authors. There are two drawbacks: capacity underuse and exploding/vanishing gradients. As we limit the weights, we also limit the critic's ability to learn. It was <a id="_idIndexMarker223"/>found that weight clipping forces the network to learn only simple functions. Therefore, the neural network's capacity becomes underused. </p>
			<p>Secondly, the clipping values require careful tuning. If set too high, the gradients will explode, hence violating the Lipschitz constraint. If set too low, gradients will vanish as we move the network back.  Also, the weight clipping will push the gradients to the two limits, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B14538_03_13.jpg" alt="Figure 3.13 – Left: Weight clipping pushes weights toward two values. Right: Gradients produced by gradient penalty. Source: I. Gulrajani et al, 2017, Improved Training of Wasserstein GANs"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13 – Left: Weight clipping pushes weights toward two values. Right: Gradients produced by gradient penalty. Source: I. Gulrajani et al, 2017, Improved Training of Wasserstein GANs</p>
			<p>As a result, <strong class="bold">gradient penalty</strong> (<strong class="bold">GP</strong>) is proposed <a id="_idIndexMarker224"/>to replace weight clipping to enforce the Lipschitz constraint as follows:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/Formula_03_012.jpg" alt=""/>
				</div>
			</div>
			<p>We will look at each of the variables in the equation and implement them in the code. The Jupyter notebook for this exercise is <strong class="source-inline">ch3_wgan_gp_fashion_mnist.ipynb</strong>.</p>
			<p>We normally use <em class="italic">x</em> to denote a real image, but there is now an <img src="image/Formula_03_013.png" alt=""/> in the equation. This <img src="image/Formula_03_014.png" alt=""/> is pointwise interpolation between a real image and a fake image. The ratio of the images, or the epsilon, is drawn from a uniform distribution of <em class="italic">[0,1]</em>:</p>
			<p class="source-code">epsilon = tf.random.uniform((batch_size,1,1,1))</p>
			<p class="source-code">interpolates = epsilon*real_images + \ 					(1-epsilon)*fake_images</p>
			<p>There is <a id="_idIndexMarker225"/>mathematical proof that the <em class="italic">"optimal critic contains straight lines with gradient norm 1 connecting coupled points from Pr and Pg",</em> as quoted from the WGAN-GP paper <em class="italic">Improved Training of Wasserstein GANs</em> (<a href="https://arxiv.org/pdf/1704.00028.pdf">https://arxiv.org/pdf/1704.00028.pdf</a>). For our purposes, we can understand it as the gradient comes from the mixture of both real and fake images and we don't need to calculate the penalty for real and fake images separately. </p>
			<p>The term <img src="image/Formula_03_015.png" alt=""/> is the gradient of the critic's output with respect to the interpolation. We can again use gradient tape to get the gradient:</p>
			<p class="source-code">with tf.GradientTape() as gradient_tape:</p>
			<p class="source-code"> 	gradient_tape.watch(interpolates) </p>
			<p class="source-code">	critic_interpolates = self.critic(interpolates)</p>
			<p class="source-code"> 	gradient_d = gradient_tape.gradient( 						 critic_interpolates,  						 [interpolates])</p>
			<p>The next step is to calculate the L2-norm: </p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/Formula_03_016.jpg" alt=""/>
				</div>
			</div>
			<p>We square <a id="_idIndexMarker226"/>every value, add them together, then do a square root as follows:</p>
			<p class="source-code">grad_loss = tf.square(grad)</p>
			<p class="source-code">grad_loss = tf.reduce_sum(grad_loss, 				    axis=np.arange(1, 						len(grad)loss.shape)))</p>
			<p class="source-code">graid_loss = tf.sqrt(grad_loss)</p>
			<p>When doing <strong class="source-inline">tf.reduce_sum()</strong>, we exclude the first dimension in the axis as that dimension is the batch size. The penalty aims to bring the gradient norm close to <strong class="source-inline">1</strong>, and this is the last step to calculate the gradient loss:</p>
			<p class="source-code">grad_loss = tf.reduce_mean(tf.square(grad_loss - 1))</p>
			<p>The lambda in the equation is the ratio of the gradient penalty to other critic losses and is set to 10 in the paper. Now we add all the critic losses and gradient penalty to backpropagate and update weights:</p>
			<p class="source-code">total_loss = loss_real + loss_fake + LAMBDA * grad_loss</p>
			<p class="source-code">gradients = total_tape.gradient(total_loss, 						self.critic.variables)</p>
			<p class="source-code">self.optimizer_critic.apply_gradients(zip(gradients, 						self.critic.variables))</p>
			<p>That is everything you'll need to add to the WGAN to make it WGAN-GP. There are two things to remove though:</p>
			<ul>
				<li>Weight clipping</li>
				<li>Batch normalization in the critic</li>
			</ul>
			<p>The gradient penalty is to penalize the norm of the critic's gradient with respect to each input independently. However, batch normalization changes the gradients with the batch statistics. To avoid this problem, batch normalization was removed from the critic and it was found that it still works well. This has since become a common practice in GANs. </p>
			<p>The critic <a id="_idIndexMarker227"/>architecture is the same as WGAN, less the batch normalization:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B14538_03_14.jpg" alt="Figure 3.14 – Model summary of WGAN-GP&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14 – Model summary of WGAN-GP</p>
			<p>The following are the samples generated by a trained WGAN-GP:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B14538_03_15.jpg" alt="Figure 3.15 – Samples generated by WGAN-GP&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15 – Samples generated by WGAN-GP</p>
			<p>They look <a id="_idIndexMarker228"/>sharp and pretty, much like samples from the Fashion-MNIST dataset. The training was very stable and converged quickly! Next, we will put WGAN-GP to the test by training it on CelebA!</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor080"/>Tweaking WGAN-GP for CelebA</h2>
			<p>We will make some small tweaks to WGAN-GP to train on the CelebA dataset. First, as we will <a id="_idIndexMarker229"/>use a larger image size of 64 compared to 32 previously, we will need to add another stage of upsampling. Then we replace the <a id="_idIndexMarker230"/>batch normalization with <strong class="bold">layer normalization</strong> as suggested <a id="_idIndexMarker231"/>by the WGAN-GP authors. The following figure shows different types of normalization for tensors with a dimension of <strong class="bold">(N, H, W, C)</strong> where the notations stand for batch size, height, width, and channel respectively:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B14538_03_16.jpg" alt="Figure 3.16 – Different types of normalizations used in deep learning. (Source: Y. Wu, K. He, 2018, Group Normalization)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.16 – Different types of normalizations used in deep learning. (Source: Y. Wu, K. He, 2018, Group Normalization)</p>
			<p>Batch normalization calculates statistics across <strong class="bold">(N, H, W)</strong> to produce one statistic for each channel. In contrast, layer normalization calculates statistics across all tensors within <a id="_idIndexMarker232"/>one sample, that is, <strong class="bold">(H,W,C)</strong> and therefore <a id="_idIndexMarker233"/>does not correlate between samples and hence works better for image generation. It is a drop-in replacement for batch normalization where we replace the word <em class="italic">Batch</em> with <em class="italic">Layer</em>:</p>
			<p class="source-code">model.add(layers.BatchNormalization())</p>
			<p class="source-code">model.add(layers.LayerNormalization())</p>
			<p>The Jupyter notebook for this exercise is <strong class="source-inline">ch3_wgan_gp_celeb_a.ipynb</strong>. The following are the images generated by our WGAN-GP. Although the training time of WGAN-GP is longer due to the additional step to do gradient penalty, the training is able to converge faster:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B14538_03_17.jpg" alt="Figure 3.17 – Celebrity faces generated by WGAN-GP&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17 – Celebrity faces generated by WGAN-GP</p>
			<p>They don't look quite perfect compared to the VAE, partly because there wasn't reconstruction <a id="_idIndexMarker234"/>loss to make sure the facial <a id="_idIndexMarker235"/>features stay in the places they belong. Nonetheless, this encourages the GAN to be more imaginative and, as a result, more varieties of faces were generated. I also did not notice mode collapse. WGAN-GP is a milestone to achieve the training stability of a GAN. Many subsequent GANs use Wasserstein loss and gradient penalty, and that includes the Progressive GAN, to generate high-resolution images, which we will talk about in detail in <a href="B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136"><em class="italic">Chapter 7</em></a><em class="italic">, High Fidelity Face Generation</em>.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor081"/>Summary</h1>
			<p>We have definitely learned a lot in this chapter. We started by learning about the theory and loss functions of GANs, and how to translate the mathematical value function into the code implementation of binary cross-entropy loss. We implemented DCGAN with convolutional layers, batch normalization layers, and leaky ReLU to make the networks go deeper. However, there are still challenges in training GANs, which include instability and being prone to mode collapse due to Jensen-Shannon divergence. </p>
			<p>Many of these problems were solved by WGAN with Wasserstein distance, weight clipping, and the removal of the sigmoid at the critic's output. Finally, WGAN-GP introduces gradient penalty to properly enforce the 1-Lipztschitz constraint and give us a framework for stable GAN training. We then replaced batch normalization with layer normalization to train on the CelebA dataset successfully to generate a good variety of faces.</p>
			<p>This concludes part 1 of the book. Well done to you for making it this far! By now, you have learned about using different families of generative models to generate image<a id="_idTextAnchor082"/>s. That includes autoregressive models like PixelCNN in <a href="B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a><em class="italic">, Getting Started with Image Generation Using TensorFlow</em>, in <a href="B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 2</em></a><em class="italic">, Variational Autoencoder</em> and GANs in this chapter. You are now familiar with the concept of distribution, loss functions, and how to construct neural networks for image generation. </p>
			<p>With this solid foundation, we will explore some interesting applications in part 2 of the book, where we will also get to learn about some advanced techniques and cool applications. In the next chapter, we will learn how to perform image-to-image translation with GANs.</p>
		</div>
	</body></html>