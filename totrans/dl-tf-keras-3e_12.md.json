["```\npip install tensorflow-probability \n```", "```\nimport matplotlib.pyplot as plt\nimport tensorflow_probability as tfp\nimport functools, inspect, sys \n```", "```\ntfd = tfp.distributions\ndistribution_class =  tfp.distributions.Distribution\ndistributions = [name for name, obj in inspect.getmembers(tfd)\n                if inspect.isclass(obj) and issubclass(obj, distribution_class)]\nprint(distributions) \n```", "```\n['Autoregressive', 'BatchBroadcast', 'BatchConcat', 'BatchReshape', 'Bates', 'Bernoulli', 'Beta', 'BetaBinomial', 'BetaQuotient', 'Binomial', 'Blockwise', 'Categorical', 'Cauchy', 'Chi', 'Chi2', 'CholeskyLKJ', 'ContinuousBernoulli', 'DeterminantalPointProcess', 'Deterministic', 'Dirichlet', 'DirichletMultinomial', 'Distribution', 'DoublesidedMaxwell', 'Empirical', 'ExpGamma', 'ExpInverseGamma', 'ExpRelaxedOneHotCategorical', 'Exponential', 'ExponentiallyModifiedGaussian', 'FiniteDiscrete', 'Gamma', 'GammaGamma', 'GaussianProcess', 'GaussianProcessRegressionModel', 'GeneralizedExtremeValue', 'GeneralizedNormal', 'GeneralizedPareto', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'HalfStudentT', 'HiddenMarkovModel', 'Horseshoe', 'Independent', 'InverseGamma', 'InverseGaussian', 'JohnsonSU', 'JointDistribution', 'JointDistributionCoroutine', 'JointDistributionCoroutineAutoBatched', 'JointDistributionNamed', 'JointDistributionNamedAutoBatched', 'JointDistributionSequential', 'JointDistributionSequentialAutoBatched', 'Kumaraswamy', 'LKJ', 'LambertWDistribution', 'LambertWNormal', 'Laplace', 'LinearGaussianStateSpaceModel', 'LogLogistic', 'LogNormal', 'Logistic', 'LogitNormal', 'MarkovChain', 'Masked', 'MatrixNormalLinearOperator', 'MatrixTLinearOperator', 'Mixture', 'MixtureSameFamily', 'Moyal', 'Multinomial', 'MultivariateNormalDiag', 'MultivariateNormalDiagPlusLowRank', 'MultivariateNormalDiagPlusLowRankCovariance', 'MultivariateNormalFullCovariance', 'MultivariateNormalLinearOperator', 'MultivariateNormalTriL', 'MultivariateStudentTLinearOperator', 'NegativeBinomial', 'Normal', 'NormalInverseGaussian', 'OneHotCategorical', 'OrderedLogistic', 'PERT', 'Pareto', 'PixelCNN', 'PlackettLuce', 'Poisson', 'PoissonLogNormalQuadratureCompound', 'PowerSpherical', 'ProbitBernoulli', 'QuantizedDistribution', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'Sample', 'SigmoidBeta', 'SinhArcsinh', 'Skellam', 'SphericalUniform', 'StoppingRatioLogistic', 'StudentT', 'StudentTProcess', 'StudentTProcessRegressionModel', 'TransformedDistribution', 'Triangular', 'TruncatedCauchy', 'TruncatedNormal', 'Uniform', 'VariationalGaussianProcess', 'VectorDeterministic', 'VonMises', 'VonMisesFisher', 'Weibull', 'WishartLinearOperator', 'WishartTriL', 'Zipf'] \n```", "```\nnormal = tfd.Normal(loc=0., scale=1.) \nN samples and plots them:\n```", "```\ndef plot_normal(N):\n  samples = normal.sample(N)\n  sns.distplot(samples)\n  plt.title(f\"Normal Distribution with zero mean, and 1 std. dev {N} samples\")\n  plt.show() \n```", "```\nnormal = tfd.Normal(loc=0., scale=1.) \n```", "```\nprint(normal) \n```", "```\n>>> tfp.distributions.Normal(\"Normal\", batch_shape=[], event_shape=[], dtype=float32) \n```", "```\nnormal_2 = tfd.Normal(loc=[0., 0.], scale=[1., 3.])\nprint(normal_2) \n```", "```\n>>> tfp.distributions.Normal(\"Normal\", batch_shape=[2], event_shape=[], dtype=float32) \n```", "```\nnormal_3 = tfd.MultivariateNormalDiag(loc = [[1.0, 0.3]])\nprint(normal_3) \n```", "```\n>>> tfp.distributions.MultivariateNormalDiag(\"MultivariateNormalDiag\", batch_shape=[1], event_shape=[2], dtype=float32) \n```", "```\ncoin_flip = tfd.Bernoulli(probs=0.5, dtype=tf.int32) \n```", "```\ncoin_flip_data = coin_flip.sample(2000) \n```", "```\nplt.hist(coin_flip_data) \n```", "```\ncoin_flip.prob(0) ## Probability of tail \n```", "```\n>>> <tf.Tensor: shape=(), dtype=float32, numpy=0.5> \n```", "```\nbias_coin_flip = tfd.Bernoulli(probs=0.8, dtype=tf.int32) \n```", "```\nbias_coin_flip_data = bias_coin_flip.sample(2000) \n```", "```\nplt.hist(bias_coin_flip_data) \n```", "```\nbias_coin_flip.prob(0) ## Probability of tail \n```", "```\n>>> <tf.Tensor: shape=(), dtype=float32, numpy=0.19999999> \n```", "```\ntwo_bias_coins_flip = tfd.Bernoulli(probs=[0.8, 0.6], dtype=tf.int32) \n```", "```\ntwo_bias_coins_flip_data = two_bias_coins_flip.sample(2000) \n```", "```\nplt.hist(two_bias_coins_flip_data[:,0], alpha=0.8, label='Coin 1')\nplt.hist(two_bias_coins_flip_data[:,1], alpha=0.5, label='Coin 2')\nplt.legend(loc='center') \n```", "```\ntemperature = tfd.Normal(loc=35, scale = 4) \n```", "```\ntemperature_data = temperature.sample(1000) \n```", "```\nsns.displot(temperature_data, kde= True) \n```", "```\ntemperature.mean() \n```", "```\n# output\n>>> <tf.Tensor: shape=(), dtype=float32, numpy=35.0> \n```", "```\ntemperature.stddev() \n```", "```\n# output\n>>> <tf.Tensor: shape=(), dtype=float32, numpy=4.0> \n```", "```\ntf.math.reduce_mean(temperature_data) \n```", "```\n# output\n>>> <tf.Tensor: shape=(), dtype=float32, numpy=35.00873> \n```", "```\ntf.math.reduce_std(temperature_data) \n```", "```\n# output\n>>> <tf.Tensor: shape=(), dtype=float32, numpy=3.9290223> \n```", "```\nweather = tfd.MultivariateNormalDiag(loc = [35, 56], scale_diag=[4, 15])\nweather_data = weather.sample(1000)\nplt.scatter(weather_data[:, 0], weather_data[:, 1], color='blue', alpha=0.4)\nplt.xlabel(\"Temperature Degree Celsius\")\nplt.ylabel(\"Humidity %\") \n```", "```\nRoot = tfd.JointDistributionCoroutine.Root\ndef model():\n  # generate the distribution for cloudy weather\n  cloudy = yield Root(tfd.Bernoulli(probs=0.2, dtype=tf.int32))\n  # define sprinkler probability table\n  sprinkler_prob = [0.5, 0.1]\n  sprinkler_prob = tf.gather(sprinkler_prob, cloudy)\n  sprinkler = yield tfd.Bernoulli(probs=sprinkler_prob, dtype=tf.int32)\n  # define rain probability table\n  raining_prob = [0.1, 0.8]\n  raining_prob = tf.gather(raining_prob, cloudy)\n  raining = yield tfd.Bernoulli(probs=raining_prob, dtype=tf.int32)\n  #Conditional Probability table for wet grass\n  grass_wet_prob = [[0.0, 0.8],\n                    [0.9, 0.99]]\n  grass_wet_prob = tf.gather_nd(grass_wet_prob, _stack(sprinkler, raining))\n  grass_wet = yield tfd.Bernoulli(probs=grass_wet_prob, dtype=tf.int32) \n```", "```\ndef _conform(ts):\n  \"\"\"Broadcast all arguments to a common shape.\"\"\"\n  shape = functools.reduce(\n      tf.broadcast_static_shape, [a.shape for a in ts])\n  return [tf.broadcast_to(a, shape) for a in ts]\ndef _stack(*ts):\n  return tf.stack(_conform(ts), axis=-1) \n```", "```\nd = marginalize.MarginalizableJointDistributionCoroutine(model) \n```", "```\nobservations = ['marginalize', # We don't know the cloudy state\n                'tabulate', # We want to know the probability of rain\n                'marginalize', # We don't know the sprinkler state.\n                1]             # We observed a wet lawn. \n```", "```\np = tf.exp(d.marginalized_log_prob(observations))\np = p / tf.reduce_sum(p) \n```", "```\nobservations = ['marginalize',  \n                'marginalize', \n                'tabulate',  \n                1] \n```", "```\nobservations = ['marginalize',  \n                 0,\n                 0, \n                'tabulate'] \n```", "```\ndef create_dataset(n, x_range):\n    x_uniform_dist = tfd.Uniform(low=x_range[0], high=x_range[1])\n    x = x_uniform_dist.sample(n).numpy() [:, np.newaxis] \n    y_true = 2.7*x+3\n    eps_uniform_dist = tfd.Normal(loc=0, scale=1)\n    eps = eps_uniform_dist.sample(n).numpy() [:, np.newaxis] *0.74*x\n    y = y_true + eps\n    return x, y, y_true \n```", "```\nx_train, y_train, y_true = create_dataset(2000, [-10, 10])\nx_val, y_val, _ = create_dataset(500, [-10, 10]) \n```", "```\n# Model Architecture\nmodel = Sequential([Dense(1, input_shape=(1,))])\n# Compile \nmodel.compile(loss='mse', optimizer='adam')\n# Fit\nmodel.fit(x_train, y_train, epochs=100, verbose=1) \n```", "```\nmodel = Sequential([Dense(2, input_shape = (1,)),\n    tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t[..., :1], scale=0.3+tf.math.abs(t[...,1:])))\n]) \n```", "```\n# Define negative loglikelihood loss function\ndef neg_loglik(y_true, y_pred):\n    return -y_pred.log_prob(y_true) \n```", "```\nmodel.compile(loss=neg_loglik, optimizer='adam')\n# Fit\nmodel.fit(x_train, y_train, epochs=500, verbose=1) \n```", "```\n# Summary Statistics\ny_mean = model(x_test).mean()\ny_std = model(x_test).stddev() \n```", "```\nfig = plt.figure(figsize = (20, 10))\nplt.scatter(x_train, y_train, marker='+', label='Training Data', alpha=0.5)\nplt.plot(x_train, y_true, color='k', label='Ground Truth')\nplt.plot(x_test, y_mean, color='r', label='Predicted Mean')\nplt.fill_between(np.squeeze(x_test), np.squeeze(y_mean+1*y_std), np.squeeze(y_mean-1*y_std),  alpha=0.6, label='Aleatory Uncertainty (1SD)')\nplt.fill_between(np.squeeze(x_test), np.squeeze(y_mean+2*y_std), np.squeeze(y_mean-2*y_std),  alpha=0.4, label='Aleatory Uncertainty (2SD)')\nplt.title('Aleatory Uncertainty')\nplt.xlabel('$x$')\nplt.ylabel('$y$')\nplt.legend()\nplt.show() \n```", "```\nmodel = Sequential([\n  tfp.layers.DenseVariational(1, posterior_mean_field, prior_trainable, kl_weight=1/x_train.shape[0]),\n  tfp.layers.DistributionLambda(lambda t: tfd.Normal(loc=t, scale=1)),\n]) \n```", "```\nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik) \n```", "```\nmodel.fit(x_train, y_train, epochs=100, verbose=1) \n```"]