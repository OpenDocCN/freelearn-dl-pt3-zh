["```\nInitialize  function with random weight \nInitialize  function with random weight \nInitialize empty replay memory \n\nfor  do Initialize environment \n    for  do\n        > Collect observation from the env:\n\n        > Store the transition in the replay buffer:\n\n        > Update the model using (5.4):\n        Sample a random minibatch  from \n\n        Perform a step of GD on  on \n        > Update target network:\n        Every C steps   \n\n    end for \nend for\n```", "```\nenv = gym.make('Pong-v0')\nenv = ScaledFloatFrame(env)\n```", "```\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n```", "```\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env):\n        \"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"\n        gym.ObservationWrapper.__init__(self, env)\n        self.width = 84\n        self.height = 84\n        self.observation_space = spaces.Box(low=0, high=255,\n                shape=(self.height, self.width, 1), dtype=np.uint8)\n    def observation(self, frame):\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n        return frame[:, :, None]\n```", "```\ndef make_env(env_name, fire=True, frames_num=2, noop_num=30, skip_frames=True):\n    env = gym.make(env_name)\n    if skip_frames:\n        env = MaxAndSkipEnv(env) # Return only every `skip`-th frame\n    if fire:\n        env = FireResetEnv(env) # Fire at the beginning\n    env = NoopResetEnv(env, noop_max=noop_num)\n    env = WarpFrame(env) # Reshape image\n    env = FrameStack(env, frames_num) # Stack last 4 frames\n    return env\n```", "```\nimport numpy as np\nimport tensorflow as tf\nimport gym\nfrom datetime import datetime\nfrom collections import deque\nimport time\nimport sys\n\nfrom atari_wrappers import make_env\n```", "```\ndef cnn(x):\n    x = tf.layers.conv2d(x, filters=16, kernel_size=8, strides=4, padding='valid', activation='relu') \n    x = tf.layers.conv2d(x, filters=32, kernel_size=4, strides=2, padding='valid', activation='relu') \n    return tf.layers.conv2d(x, filters=32, kernel_size=3, strides=1, padding='valid', activation='relu') \n\ndef fnn(x, hidden_layers, output_layer, activation=tf.nn.relu, last_activation=None):\n    for l in hidden_layers:\n        x = tf.layers.dense(x, units=l, activation=activation)\n    return tf.layers.dense(x, units=output_layer, activation=last_activation)\n\n```", "```\ndef qnet(x, hidden_layers, output_size, fnn_activation=tf.nn.relu, last_activation=None):\n    x = cnn(x)\n    x = tf.layers.flatten(x)\n    return fnn(x, hidden_layers, output_size, fnn_activation, last_activation)\n```", "```\nclass ExperienceBuffer():\n\n    def __init__(self, buffer_size):\n        self.obs_buf = deque(maxlen=buffer_size)\n        self.rew_buf = deque(maxlen=buffer_size)\n        self.act_buf = deque(maxlen=buffer_size)\n        self.obs2_buf = deque(maxlen=buffer_size)\n        self.done_buf = deque(maxlen=buffer_size)\n\n    def add(self, obs, rew, act, obs2, done):\n        self.obs_buf.append(obs)\n        self.rew_buf.append(rew)\n        self.act_buf.append(act)\n        self.obs2_buf.append(obs2)\n        self.done_buf.append(done)\n```", "```\n    def sample_minibatch(self, batch_size):\n        mb_indices = np.random.randint(len(self.obs_buf), size=batch_size)\n\n        mb_obs = scale_frames([self.obs_buf[i] for i in mb_indices])\n        mb_rew = [self.rew_buf[i] for i in mb_indices]\n        mb_act = [self.act_buf[i] for i in mb_indices]\n        mb_obs2 = scale_frames([self.obs2_buf[i] for i in mb_indices])\n        mb_done = [self.done_buf[i] for i in mb_indices]\n\n        return mb_obs, mb_rew, mb_act, mb_obs2, mb_done\n```", "```\n    def __len__(self):\n        return len(self.obs_buf)\n```", "```\ndef DQN(env_name, hidden_sizes=[32], lr=1e-2, num_epochs=2000, buffer_size=100000, discount=0.99, update_target_net=1000, batch_size=64, update_freq=4, frames_num=2, min_buffer_size=5000, test_frequency=20, start_explor=1, end_explor=0.1, explor_steps=100000):\n\n    env = make_env(env_name, frames_num=frames_num, skip_frames=True, noop_num=20)\n    env_test = make_env(env_name, frames_num=frames_num, skip_frames=True, noop_num=20)\n    env_test = gym.wrappers.Monitor(env_test, \"VIDEOS/TEST_VIDEOS\"+env_name+str(current_milli_time()),force=True, video_callable=lambda x: x%20==0)\n\n    obs_dim = env.observation_space.shape\n    act_dim = env.action_space.n \n```", "```\n    tf.reset_default_graph()\n    obs_ph = tf.placeholder(shape=(None, obs_dim[0], obs_dim[1], obs_dim[2]), dtype=tf.float32, name='obs')\n    act_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='act')\n    y_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='y')\n```", "```\n    with tf.variable_scope('target_network'):\n        target_qv = qnet(obs_ph, hidden_sizes, act_dim)\n    target_vars = tf.trainable_variables()\n\n    with tf.variable_scope('online_network'):\n        online_qv = qnet(obs_ph, hidden_sizes, act_dim)\n    train_vars = tf.trainable_variables()\n\n    update_target = [train_vars[i].assign(train_vars[i+len(target_vars)]) for i in range(len(train_vars) - len(target_vars))]\n    update_target_op = tf.group(*update_target)\n```", "```\n    act_onehot = tf.one_hot(act_ph, depth=act_dim)\n    q_values = tf.reduce_sum(act_onehot * online_qv, axis=1)\n    v_loss = tf.reduce_mean((y_ph - q_values)**2)\n```", "```\n    v_opt = tf.train.AdamOptimizer(lr).minimize(v_loss)\n```", "```\n    now = datetime.now()\n    clock_time = \"{}_{}.{}.{}\".format(now.day, now.hour, now.minute, int(now.second))\n\n    mr_v = tf.Variable(0.0)\n    ml_v = tf.Variable(0.0)\n\n    tf.summary.scalar('v_loss', v_loss)\n    tf.summary.scalar('Q-value', tf.reduce_mean(q_values))\n    tf.summary.histogram('Q-values', q_values)\n\n    scalar_summary = tf.summary.merge_all()\n    reward_summary = tf.summary.scalar('test_rew', mr_v)\n    mean_loss_summary = tf.summary.scalar('mean_loss', ml_v)\n\n    hyp_str = \"-lr_{}-upTN_{}-upF_{}-frms_{}\".format(lr, update_target_net, update_freq, frames_num)\n    file_writer = tf.summary.FileWriter('log_dir/'+env_name+'/DQN_'+clock_time+'_'+hyp_str, tf.get_default_graph())\n```", "```\n    def agent_op(o):\n        o = scale_frames(o)\n        return sess.run(online_qv, feed_dict={obs_ph:[o]})\n\n```", "```\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n\n    step_count = 0\n    last_update_loss = []\n    ep_time = current_milli_time()\n    batch_rew = []\n\n    obs = env.reset()\n```", "```\n    obs = env.reset()\n\n    buffer = ExperienceBuffer(buffer_size)\n\n    sess.run(update_target_op)\n\n    eps = start_explor\n    eps_decay = (start_explor - end_explor) / explor_steps\n```", "```\n    for ep in range(num_epochs):\n        g_rew = 0\n        done = False\n\n        while not done:\n            act = eps_greedy(np.squeeze(agent_op(obs)), eps=eps)\n            obs2, rew, done, _ = env.step(act)\n            buffer.add(obs, rew, act, obs2, done)\n\n            obs = obs2\n            g_rew += rew\n            step_count += 1\n```", "```\n            if eps > end_explor:\n                eps -= eps_decay\n\n            if len(buffer) > min_buffer_size and (step_count % update_freq == 0):\n                mb_obs, mb_rew, mb_act, mb_obs2, mb_done = buffer.sample_minibatch(batch_size)\n                mb_trg_qv = sess.run(target_qv, feed_dict={obs_ph:mb_obs2})\n                y_r = q_target_values(mb_rew, mb_done, mb_trg_qv, discount) # Compute the target values\n                train_summary, train_loss, _ = sess.run([scalar_summary, v_loss, v_opt], feed_dict={obs_ph:mb_obs, y_ph:y_r, act_ph: mb_act})\n\n                file_writer.add_summary(train_summary, step_count)\n                last_update_loss.append(train_loss)\n\n            if (len(buffer) > min_buffer_size) and (step_count % update_target_net) == 0:\n                _, train_summary = sess.run([update_target_op, mean_loss_summary], feed_dict={ml_v:np.mean(last_update_loss)})\n                file_writer.add_summary(train_summary, step_count)\n                last_update_loss = []\n```", "```\n            if done:\n                obs = env.reset()\n                batch_rew.append(g_rew)\n                g_rew = 0\n        if ep % test_frequency == 0:\n            test_rw = test_agent(env_test, agent_op, num_games=10)\n            test_summary = sess.run(reward_summary, feed_dict={mr_v: np.mean(test_rw)})\n            file_writer.add_summary(test_summary, step_count)\n            print('Ep:%4d Rew:%4.2f, Eps:%2.2f -- Step:%5d -- Test:%4.2f %4.2f' % (ep,np.mean(batch_rew), eps, step_count, np.mean(test_rw), np.std(test_rw))\n            batch_rew = []\n    file_writer.close()\n    env.close()\n    env_test.close()\n```", "```\nif __name__ == '__main__':\n    DQN('PongNoFrameskip-v4', hidden_sizes=[128], lr=2e-4, buffer_size=100000, update_target_net=1000, batch_size=32, update_freq=2, frames_num=2, min_buffer_size=10000)\n\n```", "```\ntensorboard --logdir .\n```", "```\nmb_trg_qv = sess.run(target_qv, feed_dict={obs_ph:mb_obs2})\ny_r = q_target_values(mb_rew, mb_done, mb_trg_qv, discount)\n```", "```\nmb_onl_qv, mb_trg_qv = sess.run([online_qv,target_qv], feed_dict={obs_ph:mb_obs2})\ny_r = double_q_target_values(mb_rew, mb_done, mb_trg_qv, mb_onl_qv, discount)\n```", "```\ndef dueling_qnet(x, hidden_layers, output_size, fnn_activation=tf.nn.relu, last_activation=None):\n    x = cnn(x)\n    x = tf.layers.flatten(x)\n    qf = fnn(x, hidden_layers, 1, fnn_activation, last_activation)\n    aaqf = fnn(x, hidden_layers, output_size, fnn_activation, last_activation)\n    return qf + aaqf - tf.reduce_mean(aaqf)\n```"]