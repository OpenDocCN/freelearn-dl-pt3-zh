- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monte Carlo Methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to compute the optimal policy using
    two interesting dynamic programming methods called value and policy iteration.
    Dynamic programming is a model-based method and it requires the model dynamics
    of the environment to compute the value and Q functions in order to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: But let's suppose we don't have the model dynamics of the environment. In that
    case, how do we compute the value and Q functions? Here is where we use model-free
    methods. Model-free methods do not require the model dynamics of the environment
    to compute the value and Q functions in order to find the optimal policy. One
    such popular model-free method is the **Monte Carlo** (**MC**) method.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin the chapter by understanding what the MC method is, then we will
    look into two important types of tasks in reinforcement learning called prediction
    and control tasks. Later, we will learn how the Monte Carlo method is used in
    reinforcement learning and how it is beneficial compared to the dynamic programming
    method we learned about in the previous chapter. Moving forward, we will understand
    what the MC prediction method is and the different types of MC prediction methods.
    We will also learn how to train an agent to play blackjack with the MC prediction
    method.
  prefs: []
  type: TYPE_NORMAL
- en: Going ahead, we will learn about the Monte Carlo control method and different
    types of Monte Carlo control methods. Following this, we will learn how to train
    an agent to play blackjack with the MC control method.
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize, in this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Monte Carlo method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction and control tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Monte Carlo prediction method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing blackjack with the MC prediction method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Monte Carlo control method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Playing blackjack with the MC control method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Monte Carlo method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before understanding how the Monte Carlo method is useful in reinforcement learning,
    first, let's understand what the Monte Carlo method is and how it works. The Monte
    Carlo method is a statistical technique used to find an approximate solution through
    sampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, the Monte Carlo method approximates the expectation of a random
    variable by sampling, and when the sample size is greater, the approximation will
    be better. Let''s suppose we have a random variable *X* and say we need to compute
    the expected value of *X*; that is *E(X)*, then we can compute it by taking the
    sum of the values of *X* multiplied by their respective probabilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But instead of computing the expectation like this, can we approximate it with
    the Monte Carlo method? Yes! We can estimate the expected value of *X* by just
    sampling the values of *X* for some *N* times and compute the average value of
    *X* as the expected value of *X* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_002.png)'
  prefs: []
  type: TYPE_IMG
- en: When *N* is larger our approximation will be better. Thus, with the Monte Carlo
    method, we can approximate the solution through sampling and our approximation
    will be better when the sample size is large.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming sections, we will learn how exactly the Monte Carlo method is
    used in reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction and control tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In reinforcement learning, we perform two important tasks, and they are:'
  prefs: []
  type: TYPE_NORMAL
- en: The prediction task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The control task
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the prediction task, a policy ![](img/B15558_03_172.png) is given as an input
    and we try to predict the value function or Q function using the given policy.
    But what is the use of doing this? Our goal is to evaluate the given policy. That
    is, we need to determine whether the given policy is good or bad. How can we determine
    that? If the agent obtains a good return using the given policy then we can say
    that our policy is good. Thus, to evaluate the given policy, we need to understand
    what the return the agent would obtain if it uses the given policy. To obtain
    the return, we predict the value function or Q function using the given policy.
  prefs: []
  type: TYPE_NORMAL
- en: That is, we learned that the value function or value of a state denotes the
    expected return an agent would obtain starting from that state following some
    policy ![](img/B15558_03_140.png). Thus, by predicting the value function using
    the given policy ![](img/B15558_03_084.png), we can understand what the expected
    return the agent would obtain in each state if it uses the given policy ![](img/B15558_03_050.png).
    If the return is good then we can say that the given policy is good.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we learned that the Q function or Q value denotes the expected return
    the agent would obtain starting from the state *s* and an action *a* following
    the policy ![](img/B15558_03_082.png). Thus, predicting the Q function using the
    given policy ![](img/B15558_03_008.png), we can understand what the expected return
    the agent would obtain in each state-action pair if it uses the given policy.
    If the return is good then we can say that the given policy is good.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can evaluate the given policy ![](img/B15558_03_008.png) by computing
    the value and Q functions.
  prefs: []
  type: TYPE_NORMAL
- en: Note that, in the prediction task, we don't make any change to the given input
    policy. We keep the given policy as fixed and predict the value function or Q
    function using the given policy and obtain the expected return. Based on the expected
    return, we evaluate the given policy.
  prefs: []
  type: TYPE_NORMAL
- en: Control task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike the prediction task, in the control task, we will not be given any policy
    as an input. In the control task, our goal is to find the optimal policy. So,
    we will start off by initializing a random policy and we try to find the optimal
    policy iteratively. That is, we try to find an optimal policy that gives the maximum
    return.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in a nutshell, in the prediction task, we evaluate the given input policy
    by predicting the value function or Q function, which helps us to understand the
    expected return an agent would get if it uses the given policy, while in the control
    task our goal is to find the optimal policy and we will not be given any policy
    as input; so we will start off by initializing a random policy and we try to find
    the optimal policy iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood what prediction and control tasks are, in the next
    section, we will learn how to use the Monte Carlo method for performing the prediction
    and control tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to use the Monte Carlo method to perform
    the prediction task. We have learned that in the prediction task, we will be given
    a policy and we predict the value function or Q function using the given policy
    to evaluate it. First, we will learn how to predict the value function using the
    given policy with the Monte Carlo method. Later, we will look into predicting
    the Q function using the given policy. Alright, let's get started with the section.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need the Monte Carlo method for predicting the value function of the
    given policy? Why can't we predict the value function using the dynamic programming
    methods we learned about in the previous chapter? We learned that in order to
    compute the value function using the dynamic programming method, we need to know
    the model dynamics (transition probability), and when we don't know the model
    dynamics, we use the model-free methods.
  prefs: []
  type: TYPE_NORMAL
- en: The Monte Carlo method is a model-free method, meaning that it doesn't require
    the model dynamics to compute the value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s recap the definition of the value function. The value function
    or the value of the state *s* can be defined as the expected return the agent
    would obtain starting from the state *s* and following the policy ![](img/B15558_04_010.png).
    It can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_011.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, how can we estimate the value of the state (value function) using the
    Monte Carlo method? At the beginning of the chapter, we learned that the Monte
    Carlo method approximates the expected value of a random variable by sampling,
    and when the sample size is greater, the approximation will be better. Can we
    leverage this concept of the Monte Carlo method to predict the value of a state?
    Yes!
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to approximate the value of the state using the Monte Carlo method,
    we sample episodes (trajectories) following the given policy ![](img/B15558_04_012.png)
    for some *N* times and then we compute the value of the state as the average return
    of a state across the sampled episodes, and it can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_013.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding equation, we can understand that the value of a state *s*
    can be approximated by computing the average return of the state *s* across some
    *N* episodes. Our approximation will be better when *N* is higher.
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, in the Monte Carlo prediction method, we approximate the value
    of a state by taking the average return of a state across *N* episodes instead
    of taking the expected return.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, let''s get a better understanding of how the Monte Carlo method estimates
    the value of a state (value function) with an example. Let''s take our favorite
    grid world environment we covered in *Chapter 1*, *Fundamentals of Reinforcement
    Learning*, as shown in *Figure 4.1*. Our goal is to reach the state **I** from
    the state **A** without visiting the shaded states, and the agent receives +1
    reward when it visits the unshaded states and -1 reward when it visits the shaded
    states:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: Grid world environment'
  prefs: []
  type: TYPE_NORMAL
- en: Let's say we have a stochastic policy ![](img/B15558_03_050.png). Let's suppose,
    in state **A,** our stochastic policy ![](img/B15558_03_140.png) selects action
    *down* 80% of time and action *right* 20% of the time, and it selects action *right*
    in states **D** and **E** and action *down* in states **B** and **F** 100% of
    the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we generate an episode ![](img/B15558_04_016.png) using our given stochastic
    policy ![](img/B15558_03_140.png) as *Figure 4.2* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: Episode ![](img/B15558_04_018.png)'
  prefs: []
  type: TYPE_NORMAL
- en: For a better understanding, let's focus only on state **A**. Let's now compute
    the return of state **A**. The return of a state is the sum of the rewards of
    the trajectory starting from that state. Thus, the return of state **A** is computed
    as *R*[1](*A*) = 1+1+1+1 = 4 where the subscript 1 in *R*[1] indicates the return
    from episode 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we generate another episode ![](img/B15558_04_019.png) using the same given
    stochastic policy ![](img/B15558_03_084.png) as *Figure 4.3* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Episode ![](img/B15558_04_019.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now compute the return of state **A**. The return of state **A** is *R*[2](*A*)
    = -1+1+1+1 = 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we generate another episode ![](img/B15558_04_022.png) using the same given
    stochastic policy ![](img/B15558_03_082.png) as *Figure 4.4* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Episode ![](img/B15558_04_024.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Let's now compute the return of state **A**. The return of state **A** is *R*[3](*A*)
    = 1+1+1+1 = 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we generated three episodes and computed the return of state **A** in
    all three episodes. Now, how can we compute the value of the state **A**? We learned
    that in the Monte Carlo method, the value of a state can be approximated by computing
    the average return of the state across some *N* episodes (trajectories):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We need to compute the value of state **A**, so we can compute it by just taking
    the average return of the state **A** across the *N* episodes as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We generated three episodes, thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_027.png)![](img/B15558_04_028.png)![](img/B15558_04_029.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, the value of state **A** is 3.3\. Similarly, we can compute the value
    of all other states by just taking the average return of the state across the
    three episodes.
  prefs: []
  type: TYPE_NORMAL
- en: For easier understanding, in the preceding example, we only generated three
    episodes. In order to find a better and more accurate estimate of the value of
    the state, we should generate many episodes (not just three) and compute the average
    return of the state as the value of the state.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in the Monte Carlo prediction method, to predict the value of a state
    (value function) using the given input policy ![](img/B15558_03_139.png), we generate
    some *N* episodes using the given policy and then we compute the value of a state
    as the average return of the state across these *N* episodes.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while computing the return of the state, we can also include the discount
    factor and compute the discounted return, but for simplicity let's not include
    the discount factor.
  prefs: []
  type: TYPE_NORMAL
- en: Now, that we have a basic understanding of how the Monte Carlo prediction method
    predicts the value function of the given policy, let's look into more detail by
    understanding the algorithm of the Monte Carlo prediction method in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: MC prediction algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Monte Carlo prediction algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*) be the sum of return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_03_140.png) is given as input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using the policy ![](img/B15558_04_032.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return of state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of state *s*[t] as total_returns(*s*[t]) = total_return(*s*[t])
    + R(*s*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_033.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The preceding algorithm implies that the value of the state is just the average
    return of the state across several episodes.
  prefs: []
  type: TYPE_NORMAL
- en: To get a better understanding of how exactly the preceding algorithm works,
    let's take a simple example and compute the value of each state manually. Say
    we need to compute the value of three states *s*[0], *s*[1], and *s*[2]. We know
    that we obtain a reward when we transition from one state to another. Thus, the
    reward for the final state will be 0 as we don't make any transitions from the
    final state. Hence, the value of the final state *s*[2] will be zero. Now, we
    need to find the value of two states *s*[0] and *s*[1].
  prefs: []
  type: TYPE_NORMAL
- en: The upcoming sections are explained with manual calculations, for a better understanding,
    follow along with a pen and paper.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the total_returns(*s*) and *N*(*s*) for all the states to zero as
    *Table 4.1* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 4.1: Initial values'
  prefs: []
  type: TYPE_NORMAL
- en: Say we are given a stochastic policy ![](img/B15558_03_082.png); in state *s*[0]
    our stochastic policy selects the action 0 for 50% of the time and action 1 for
    50% of the time, and it selects action 1 in state *s*[1] for 100% of the time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2: Iteration 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generate an episode using the given input policy ![](img/B15558_03_038.png),
    as *Figure 4.5* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Generating an episode using the given policy ![](img/B15558_03_084.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Store all rewards obtained in the episode in the list called rewards. Thus,
    rewards = [1, 1].
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the return of the state *s*[0] (sum of rewards from *s*[0]):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the total return of the state *s*[0] in our table as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the number of times the state *s*[0] is visited in our table as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_039.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s compute the return of the state *s*[1] (sum of rewards from *s*[1]):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the total return of the state *s*[1] in our table as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_041.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the number of times the state *s*[1] is visited in our table as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_042.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our updated table, after iteration 1, is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 4.2: Updated table after the first iteration'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 2:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we generate another episode using the same given policy ![](img/B15558_03_140.png)
    as *Figure 4.6* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Generating an episode using the given policy ![](img/B15558_03_172.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Store all rewards obtained in the episode in the list called rewards. Thus,
    rewards = [3, 1].
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we compute the return of the state *s*[0] (sum of rewards from *s*[0]):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_045.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the total return of the state *s*[0] in our table as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_046.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the number of times the state *s*[0] is visited in our table as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_047.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s compute the return of the state *s*[1] (sum of rewards from *s*[1]):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the return of the state *s*[1] in our table as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Update the number of times the state is visited:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_050.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Our updated table after the second iteration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 4.3: Updated table after the second iteration'
  prefs: []
  type: TYPE_NORMAL
- en: Since we are computing manually, for simplicity, let's stop at two iterations;
    that is, we just generate only two episodes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3:**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can compute the value of the state as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_051.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_052.png)![](img/B15558_04_053.png)'
  prefs: []
  type: TYPE_IMG
- en: Thus, we computed the value of the state by just taking the average return across
    multiple episodes. Note that in the preceding example, for our manual calculation,
    we just generated two episodes, but for a better estimation of the value of the
    state, we generate several episodes and then we compute the average return across
    those episodes (not just 2).
  prefs: []
  type: TYPE_NORMAL
- en: Types of MC prediction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We just learned how the Monte Carlo prediction algorithm works. We can categorize
    the Monte Carlo prediction algorithm into two types:'
  prefs: []
  type: TYPE_NORMAL
- en: First-visit Monte Carlo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Every-visit Monte Carlo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First-visit Monte Carlo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned that in the MC prediction method, we estimate the value of the state
    by just taking the average return of the state across multiple episodes. We know
    that in each episode a state can be visited multiple times. In the first-visit
    Monte Carlo method, if the same state is visited again in the same episode, we
    don't compute the return for that state again. For example, consider a case where
    an agent is playing snakes and ladders. If the agent lands on a snake, then there
    is a good chance that the agent will return to a state that it had visited earlier.
    So, when the agent revisits the same state, we don't compute the return for that
    state for the second time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the algorithm of first-visit MC; as the point in bold says,
    we compute the return for the state *s*[t] only if it is occurring for the first
    time in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*) be the sum of return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_04_054.png) is given as input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using the policy ![](img/B15558_03_084.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**If** **the state s**[t] **is occurring for the first time in the episode:**'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the return of the state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state *s*[t] as total_return(*s*[t]) = total_return(*s*[t])
    + R(*s*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_056.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Every-visit Monte Carlo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As you might have guessed, every-visit Monte Carlo is just the opposite of
    first-visit Monte Carlo. Here, we compute the return every time a state is visited
    in the episode. The algorithm of every-visit Monte Carlo is the same as the one
    we saw earlier at the beginning of this section and it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*) be the sum of the return of a state across several episodes
    and *N*(*s*) be the counter, that is, the number of times a state is visited across
    several episodes. Initialize total_return(*s*) and *N*(*s*) as zero for all the
    states. The policy ![](img/B15558_04_012.png) is given as input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using the policy ![](img/B15558_03_140.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return of the state *s*[t] as *R*(*s*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state *s*[t] as total_return(*s*[t]) = total_return(*s*[t])
    + R(*s*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t]) = *N*(*s*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the value of a state by just taking the average, that is:![](img/B15558_04_056.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember that the only difference between the first-visit MC and every-visit
    MC methods is that in the first-visit MC method, we compute the return for a state
    only for its first time of occurrence in the episode but in the every-visit MC
    method, the return of the state is computed every time the state is visited in
    an episode. We can choose between first-visit MC and every-visit MC based on the
    problem that we are trying to solve.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how the Monte Carlo prediction method predicts the
    value function of the given policy, in the next section, we will learn how to
    implement the Monte Carlo prediction method.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the Monte Carlo prediction method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you love playing card games then this section is definitely going to be interesting
    for you. In this section, we will learn how to play blackjack with the Monte Carlo
    prediction method. Before diving in, let's understand how the blackjack game works
    and its rules.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the blackjack game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Blackjack, also known as **21**, is one of the most popular card games. The
    game consists of a player and a dealer. The goal of the player is to have the
    value of the sum of all their cards be 21 or a larger value than the sum of the
    dealer's cards while not exceeding 21\. If one of these criteria is met then the
    player wins the game; else the dealer wins the game. Let's understand this in
    more detail.
  prefs: []
  type: TYPE_NORMAL
- en: The values of the cards **Jack (J)**, **King (K)**, and **Queen (Q)** will be
    considered as 10\. The value of the **Ace (A)** can be 1 or 11, depending on the
    player's choice. That is, the player can decide whether the value of an **Ace**
    should be 1 or 11 during the game. The value of the rest of the cards (**2** **to**
    **10**) is just their face value. For instance, the value of the card **2** will
    be 2, the value of the card **3** will be 3, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that the game consists of a player and a dealer. There can be many
    players at a time but only one dealer. All the players compete with the dealer
    and not with other players. Let's consider a case where there is only one player
    and a dealer. Let's understand blackjack by playing the game along with different
    cases. Let's suppose we are the player and we are competing with the dealer.
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 1: When the player wins the game**'
  prefs: []
  type: TYPE_NORMAL
- en: Initially, a player is given two cards. Both of these cards are face up, that
    is, both of the player's cards are visible to the dealer. Similarly, the dealer
    is also given two cards. But one of the dealer's cards is face up, and the other
    is face down. That is, the dealer shows only one of their cards.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in *Figure 4.7*, the player has two cards (both face up) and
    the dealer also has two cards (only one face up):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: The player has 20, and the dealer has 2 with one card face down'
  prefs: []
  type: TYPE_NORMAL
- en: Now, the player performs either of the two actions, which are **Hit** and **Stand**.
    If we (the player) perform the action **hit,** then we get one more card. If we
    perform **stand,** then it implies that we don't need any more cards and tells
    the dealer to show all their cards. Whoever has a sum of cards value equal to
    21 or a larger value than the other player but not exceeding 21 wins the game.
  prefs: []
  type: TYPE_NORMAL
- en: We learned that the value of **J**, **K**, and **Q** is 10\. As shown in *Figure
    4.7*, we have cards **J** and **K**, which sums to 20 (10+10). Thus, the total
    value our cards is already a large number and it didn't exceed 21\. So we **stand**,
    and this action tells the dealer to show their cards. As we can observe in *Figure
    4.8*, the dealer has now shown all their cards and the total value of the dealer's
    cards is 12 and the total value of our (the player's) cards is 20, which is larger
    and also didn't exceed 21, so we win the game.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: The player wins!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 2: When the player loses the game**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.9* shows we have two cards and the dealer also has two cards and
    only one of the dealer''s card is visible to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.9: The player has 13, and the dealer has 7 with one card face down'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to decide whether we should (perform the action) hit or stand.
    *Figure 4.9* shows we have two cards, **K** and **3**, which sums to 13 (10+3).
    Let''s be a little optimistic and hope that the total value of the dealer''s cards
    will not be greater than ours. So we **stand,** and this action tells the dealer
    to show their cards. As we can observe in *Figure 4.10*, the sum of the dealer''s
    card is 17, but ours is only 13, so we lose the game. That is, the dealer has
    got a larger value than us, and it did not exceed 21, so the dealer wins the game,
    and we lose:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.10: The dealer wins!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 3: When the player goes bust**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.11* shows we have two cards and the dealer also has two cards but
    only one of the dealer''s cards is visible to us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.11: The player has 8, and the dealer has 10 with one card face down'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we have to decide whether we should (perform the action) hit or stand.
    We learned that the goal of the game is to have a sum of cards value of 21, or
    a larger value than the dealer while not exceeding 21\. Right now, the total value
    of our cards is just 3+5 = 8\. Thus, we (perform the action) **hit** so that we
    can make our sum value larger. After we **hit,** we receive a new card as shown
    in *Figure 4.12*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.12: The player has 18, and the dealer has 10 with one card face down'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can observe, we got a new card. Now, the total value of our cards is
    3+5+10 = 18\. Again, we need to decide whether we should (perform the action)
    hit or stand. Let''s be a little greedy and (perform the action) **hit** so that
    we can make our sum value a little larger. As shown in *Figure 4.13*, we **hit**
    and received one more card but now the total value of our cards becomes 3+5+10+10
    = 28, which exceeds 21, and this is called a **bust** and we lose the game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.13: The player goes bust!'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 4: Useable Ace**'
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that the value of the **Ace** can be either 1 or 11, and the player
    can decide the value of the **ace** during the game. Let''s learn how this works.
    As *Figure 4.14* shows, we have been given two cards and the dealer also has two
    cards and only one of the dealer''s cards is face up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.14: The player has 10, and the dealer has 5 with one card face down'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the total value of our cards is 5+5 = 10\. Thus, we **hit** so
    that we can make our sum value larger. As *Figure 4.15* shows, after performing
    the hit action we received a new card, which is an **Ace**. Now, we can decide
    the value of the **Ace** to be either 1 or 11\. If we consider the value of **Ace**
    to be 1, then the total value of our cards will be 5+5+1 = 11\. But if we consider
    the value of the **Ace** to be 11, then the total value of our cards will be 5+5+11
    = 21\. In this case, we consider the value of our **Ace** to be 11 so that our
    sum value becomes 21.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we set the value of the **Ace** to be 11 and win the game, and in this
    case, the **Ace** is called the usable **Ace** since it helped us to win the game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.15: The player uses the **Ace** as 11 and wins the game'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 5: Non-usable Ace**'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.16* shows we have two cards and the dealer has two cards with one
    face up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.16: The player has 13, and the dealer has 10 with one card face down'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can observe, the total value of our cards is 13 (10+3). We (perform the
    action) **hit** so that we can make our sum value a little larger:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: The player has to use the **Ace** as a 1 else they go bust'
  prefs: []
  type: TYPE_NORMAL
- en: As *Figure 4.17* shows, we **hit** and received a new card, which is an **Ace**.
    Now we can decide the value of **Ace** to be 1 or 11\. If we choose 11, then our
    sum value becomes 10+3+11 = 23\. As we can observe, when we set our ace to 11,
    then our sum value exceeds 21, and we lose the game. Thus, instead of choosing
    **Ace** = 11, we set the **Ace** value to be 1; so our sum value becomes 10+3+1
    = 14.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we need to decide whether we should (perform the action) hit or stand.
    Let's say we stand hoping that the dealer sum value will be lower than ours. As
    *Figure 4.18* shows, after performing the stand action, both of the dealer's cards
    are shown, and the sum of the dealer's card is 20, but ours is just 14, and so
    we lose the game, and in this case, the **Ace** is called a **non-usable** **Ace**
    since it did not help us to win the game.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: The player has 14, and the dealer has 20 and wins'
  prefs: []
  type: TYPE_NORMAL
- en: '**Case 6: When the game is a draw**'
  prefs: []
  type: TYPE_NORMAL
- en: If both the player and the dealer's sum of cards value is the same, say 20,
    then the game is called a draw.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have understood how to play blackjack, let's implement the Monte
    Carlo prediction method in the blackjack game. But before going ahead, first,
    let's learn how the blackjack environment is designed in Gym.
  prefs: []
  type: TYPE_NORMAL
- en: The blackjack environment in the Gym library
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Import the Gym library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The environment id of blackjack is `Blackjack-v0`. So, we can create the blackjack
    game using the `make` function as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s look at the state of the blackjack environment; we can just reset
    our environment and look at the initial state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that every time we run the preceding code, we might get a different result,
    as the initial state is randomly initialized. The preceding code will print something
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As we can observe, our state is represented as a tuple, but what does this mean?
    We learned that in the blackjack game, we will be given two cards and we also
    get to see one of the dealer's cards. Thus, `15` implies that the value of the
    sum of our cards, `9` implies the face value of one of the dealer's cards, `True`
    implies that we have a usable ace, and it will be `False` if we don't have a usable
    ace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in the blackjack environment the state is represented as a tuple consisting
    of three values:'
  prefs: []
  type: TYPE_NORMAL
- en: The value of the sum of our cards
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The face value of one of the dealer's card
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Boolean value—`True` if we have a useable ace and `False` if we don't have a
    useable ace
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let''s look at the action space of our blackjack environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can observe, it implies that we have two actions in our action space,
    which are 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: The action **stand** is represented by 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The action **hit** is represented by 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Okay, what about the reward? The reward will be assigned as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**+1.0** reward if we win the game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**-1.0** reward if we lose the game'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**0** reward if the game is a draw'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have understood how the blackjack environment is designed in Gym,
    let's start implementing the MC prediction method in the blackjack game. First,
    we will look at every-visit MC and then we will learn how to implement first-visit
    MC prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Every-visit MC prediction with the blackjack game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To understand this section clearly, you should recap the every-visit Monte
    Carlo method we learned earlier. Let''s now understand how to implement every-visit
    MC prediction with the blackjack game step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a blackjack environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Defining a policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We learned that in the prediction method, we will be given an input policy and
    we predict the value function of the given input policy. So, now, we first define
    a policy function that acts as an input policy. That is, we define the input policy
    whose value function will be predicted in the upcoming steps.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the following code, our policy function takes the state as an input
    and if the **state[0]**, the sum of our cards, value, is greater than 19, then
    it will return action **0** (stand), else it will return action **1** (hit):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We defined an optimal policy: it makes more sense to perform an action 0 (stand)
    when our sum value is already greater than 19\. That is, when the sum value is
    greater than 19 we don''t have to perform a 1 (hit) action and receive a new card,
    which may cause us to lose the game or bust.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s generate an initial state by resetting the environment
    as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose the preceding code prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can notice, `state[0] = 20`; that is, the value of the sum of our cards
    is 20, so in this case, our policy will return the action 0 (stand) as the following
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have defined the policy, in the next sections, we will predict the
    value function (state values) of this policy.
  prefs: []
  type: TYPE_NORMAL
- en: Generating an episode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Next, we generate an episode using the given policy, so we define a function
    called `generate_episode`, which takes the policy as an input and generates the
    episode using the given policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s set the number of time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'For a clear understanding, let''s look into the function line by line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a list called `episode` for storing the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then for each time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action according to the given policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the action and store the next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the state, action, and reward into our episode list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If the next state is a final state then break the loop, else update the next
    state to the current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at what the output of our `generate_episode` function looks
    like. Note that we generate an episode using the policy we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As we can observe our output is in the form of **[(state, action, reward)]**.
    As shown previously, we have two states in our episode. We performed action 1
    (hit) in the state `(10, 2, False)` and received a 0 reward, and we performed
    action 0 (stand) in the state `(20, 2, False)` and received a reward of 1.0.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to generate an episode using the given policy,
    next, we will look at how to compute the value of the state (value function) using
    the every-visit MC method.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the value function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We learned that in order to predict the value function, we generate several
    episodes using the given policy and compute the value of the state as an average
    return across several episodes. Let's see how to implement that.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we define the `total_return` and `N` as a dictionary for storing the
    total return and the number of times the state is visited across episodes respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of iterations, that is, the number of episodes, we want to generate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for every iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate the episode using the given policy; that is, generate an episode using
    the policy function we defined earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Store all the states, actions, and rewards obtained from the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, for each step in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the return `R` of the state as the sum of rewards, *R*(*s*[t]) = sum(rewards[t:]):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the `total_return` of the state as total_return(*s*[t]) = total_return(*s*[t])
    + R(*s*[t]):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the number of times the state is visited in the episode as *N*(*s*[t])
    = *N*(*s*[t]) + 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: After computing the `total_return` and `N` we can just convert them into a pandas
    data frame for a better understanding. Note that this is just to give a clear
    understanding of the algorithm; we don't necessarily have to convert to the pandas
    data frame, we can also implement this efficiently just by using the dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Convert the `total_returns` dictionary into a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the counter `N` dictionary into a data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Merge the two data frames on states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Look at the first few rows of the data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will display the following. As we can observe, we have the
    total return and the number of times the state is visited:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: The total return and the number of times a state has been visited'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can compute the value of the state as the average return:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_056.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the first few rows of the data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will display something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: The value is calculated as the average of the return of each state'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, we now have the value of the state, which is just the average
    of a return of the state across several episodes. Thus, we have successfully predicted
    the value function of the given policy using the every-visit MC method.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, let's check the value of some states and understand how accurately our
    value function is estimated according to the given policy. Recall that when we
    started off, to generate episodes, we used the optimal policy, which selects the
    action 0 (stand) when the sum value is greater than 19 and the action 1 (hit)
    when the sum value is lower than 19\.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s evaluate the value of the state `(21,9,False)`, as we can observe, the
    value of the sum of our cards is already 21 and so this is a good state and should
    have a high value. Let''s see what our estimated value of the state is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding will print something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: As we can observe, the value of the state is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s check the value of the state `(5,8,False)`. As we can notice, the
    value of the sum of our cards is just 5 and even the one dealer''s single card
    has a high value, 8; in this case, the value of the state should be lower. Let''s
    see what our estimated value of the state is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: As we can notice, the value of the state is lower.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we learned how to predict the value function of the given policy using
    the every-visit MC prediction method. In the next section, we will look at how
    to compute the value of the state using the first-visit MC method.
  prefs: []
  type: TYPE_NORMAL
- en: First-visit MC prediction with the blackjack game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Predicting the value function using the first-visit MC method is exactly the
    same as how we predicted the value function using the every-visit MC method, except
    that here we compute the return of a state only for its first time of occurrence
    in the episode. The code for first-visit MC is the same as what we have seen in
    every-visit MC except here, we compute the return only for its first time of occurrence
    as shown in the following highlighted code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: You can obtain the complete code from the GitHub repo of the book and you will
    get results similar to what we saw in the every-visit MC section.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we learned how to predict the value function of the given policy using
    the first-visit and every-visit MC methods.
  prefs: []
  type: TYPE_NORMAL
- en: Incremental mean updates
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In both first-visit MC and every-visit MC, we estimate the value of a state
    as an average (arithmetic mean) return of the state across several episodes as
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_056.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Instead of using the arithmetic mean to approximate the value of the state,
    we can also use the incremental mean, and it is expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_062.png)![](img/B15558_04_063.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But why do we need incremental mean? Consider our environment as non-stationary.
    In that case, we don''t have to take the return of the state from all the episodes
    and compute the average. As the environment is non-stationary we can ignore returns
    from earlier episodes and use only the returns from the latest episodes for computing
    the average. Thus, we can compute the value of the state using the incremental
    mean as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_064.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_04_065.png) and *R*[t] is the return of the state *s*[t].
  prefs: []
  type: TYPE_NORMAL
- en: MC prediction (Q function)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have learned how to predict the value function of the given policy
    using the Monte Carlo method. In this section, we will see how to predict the
    Q function of the given policy using the Monte Carlo method.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting the Q function of the given policy using the MC method is exactly
    the same as how we predicted the value function in the previous section except
    that here we use the return of the state-action pair, whereas in the case of the
    value function we used the return of the state. That is, just like we approximated
    the value of a state (value function) by computing the average return of the state
    across several episodes, we can also approximate the value of a state-action pair
    (Q function) by computing the average return of the state-action pair across several
    episodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we generate several episodes using the given policy ![](img/B15558_03_140.png),
    then, we calculate the total_return(*s*, *a*), the sum of the return of the state-action
    pair across several episodes, and also we calculate *N*(*s*, *a*), the number
    of times the state-action pair is visited across several episodes. Then we compute
    the Q function or Q value as the average return of the state-action pair as shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_067.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For instance, let consider a small example. Say we have two states *s*[0] and
    *s*[1] and we have two possible actions 0 and 1\. Now, we compute total_return(*s*,
    *a*) and *N*(*s*, *a*). Let''s say our table after computation looks like *Table
    4.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 4.4: The result of two actions in two states'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have this, we can compute the Q value by just taking the average, that
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_067.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Thus, we can compute the Q value for all state-action pairs as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_069.png)![](img/B15558_04_070.png)![](img/B15558_04_071.png)![](img/B15558_04_072.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The algorithm for predicting the Q function using the Monte Carlo method is
    as follows. As we can see, it is exactly the same as how we predicted the value function
    using the return of the state except that here we predict the Q function using
    the return of a state-action pair:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero. The policy ![](img/B15558_03_185.png)
    is given as input
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using policy ![](img/B15558_03_084.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute return for the state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state-action pair, total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q function (Q value) by just taking the average, that is:![](img/B15558_04_067.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recall that in the MC prediction of the value function, we learned two types
    of MC—first-visit MC and every-visit MC. In first-visit MC, we compute the return
    of the state only for the first time the state is visited in the episode and in
    every-visit MC we compute the return of the state every time the state is visited
    in the episode.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, in the MC prediction of the Q function, we have two types of MC—first-visit
    MC and every-visit MC. In first-visit MC, we compute the return of the state-action
    pair only for the first time the state-action pair is visited in the episode and in every-visit
    MC we compute the return of the state-action pair every time the state-action
    pair is visited in the episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the previous section, instead of using the arithmetic mean,
    we can also use the incremental mean. We learned that the value of a state can
    be computed using the incremental mean as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_064.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can also compute the Q value using the incremental mean as shown
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_077.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we have learned how to perform the prediction task using the Monte
    Carlo method, in the next section, we will learn how to perform the control task
    using the Monte Carlo method.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo control
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the control task, our goal is to find the optimal policy. Unlike the prediction
    task, here, we will not be given any policy as an input. So, we will begin by
    initializing a random policy, and then we try to find the optimal policy iteratively.
    That is, we try to find an optimal policy that gives the maximum return. In this
    section, we will learn how to perform the control task to find the optimal policy
    using the Monte Carlo method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, we learned that in the control task our goal is to find the optimal policy.
    First, how can we compute a policy? We learned that the policy can be extracted
    from the Q function. That is, if we have a Q function, then we can extract policy
    by selecting an action in each state that has the maximum Q value as the following
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_078.png)'
  prefs: []
  type: TYPE_IMG
- en: So, to compute a policy, we need to compute the Q function. But how can we compute
    the Q function? We can compute the Q function similarly to what we learned in
    the MC prediction method. That is, in the MC prediction method, we learned that
    when given a policy, we can generate several episodes using that policy and compute
    the Q function (Q value) as the average return of the state-action pair across
    several episodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can perform the same step here to compute the Q function. But in the control
    method, we are not given any policy as input. So, we will initialize a random
    policy, and then we compute the Q function using the random policy. That is, just
    like we learned in the prediction method, we generate several episodes using our
    random policy. Then we compute the Q function (Q value) as the average return
    of a state-action pair across several episodes as the following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_067.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s suppose after computing the Q function as the average return of the
    state-action pair, our Q function looks like *Table 4.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 4.5: The Q table'
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding Q function, we can extract a new policy by selecting an action
    in each state that has the maximum Q value. That is, ![](img/B15558_04_078.png).
    Thus, our new policy selects action 0 in state *s*[0] and action 1 in state *s*[1]
    as it has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: However, this new policy will not be an optimal policy because this new policy
    is extracted from the Q function, which is computed using the random policy. That
    is, we initialized a random policy and generated several episodes using the random
    policy, then we computed the Q function by taking the average return of the state-action
    pair across several episodes. Thus, we are using the random policy to compute
    the Q function and so the new policy extracted from the Q function will not be
    an optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'But now that we have extracted a new policy from the Q function, we can use
    this new policy to generate episodes in the next iteration and compute the new
    Q function. Then, from this new Q function, we extract a new policy. We repeat
    these steps iteratively until we find the optimal policy. This is explained clearly
    in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 1**—Let ![](img/B15558_04_081.png) be the random policy. We use
    this random policy to generate an episode, and then we compute the Q function
    ![](img/B15558_04_082.png) by taking the average return of the state-action pair.
    Then, from this Q function ![](img/B15558_04_083.png), we extract a new policy
    ![](img/B15558_04_084.png). This new policy ![](img/B15558_04_085.png) will not
    be an optimal policy since it is extracted from the Q function, which is computed
    using the random policy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 2—**So, we use the new policy ![](img/B15558_04_086.png) derived
    from the previous iteration to generate an episode and compute the new Q function
    ![](img/B15558_04_087.png) as average return of a state-action pair. Then, from
    this Q function ![](img/B15558_04_088.png), we extract a new policy ![](img/B15558_03_158.png).
    If the policy ![](img/B15558_03_158.png) is optimal we stop, else we go to iteration
    3.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Iteration 3—**Now, we use the new policy ![](img/B15558_03_159.png) derived
    from the previous iteration to generate an episode and compute the new Q function
    ![](img/B15558_04_092.png). Then, from this Q function ![](img/B15558_04_092.png),
    we extract a new policy ![](img/B15558_04_094.png). If ![](img/B15558_04_094.png)
    is optimal we stop, else we go to the next iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat this process for several iterations until we find the optimal policy
    ![](img/B15558_04_096.png) as shown in *Figure 4.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_097.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.21: The path to finding the optimal policy'
  prefs: []
  type: TYPE_NORMAL
- en: This step is called policy evaluation and improvement and is similar to the
    policy iteration method we covered in *Chapter 3*, *The Bellman Equation and Dynamic
    Programming*. Policy evaluation implies that at each step we evaluate the policy.
    Policy improvement implies that at each step we are improving the policy by taking
    the maximum Q value. Note that here, we select the policy in a greedy manner meaning
    that we are selecting policy ![](img/B15558_03_055.png) by just taking the maximum
    Q value, and so we can call our policy a greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of how the MC control method works, in
    the next section, we will look into the algorithm of the MC control method and
    learn about it in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: MC control algorithm
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following steps show the Monte Carlo control algorithm. As we can observe,
    unlike the MC prediction method, here, we will not be given any policy. So, we
    start off by initializing the random policy and use the random policy to generate
    an episode in the first iteration. Then, we will compute the Q function (Q value)
    as the average return of the state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the Q function, we extract a new policy by selecting an action
    in each state that has the maximum Q value. In the next iteration, we use the
    extracted new policy to generate an episode and compute the new Q function (Q
    value) as the average return of the state-action pair. We repeat these steps for
    many iterations to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: One more thing, we need to observe that just as we learned in the first-visit
    MC prediction method, here, we compute the return of the state-action pair only
    for the first time a state-action pair is visited in the episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a better understanding, we can compare the MC control algorithm with the
    MC prediction of the Q function. One difference we can observe is that, here,
    we compute the Q function in each iteration. But if you notice, in the MC prediction
    of the Q function, we compute the Q function after all the iterations. The reason
    for computing the Q function in every iteration here is that we need the Q function
    to extract the new policy so that we can use the extracted new policy in the next
    iteration to generate an episode:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_04_099.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using policy ![](img/B15558_04_099.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state-action pair as, total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q value by just taking the average, that is,![](img/B15558_04_101.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the new updated policy ![](img/B15558_04_032.png) using the Q function:![](img/B15558_04_078.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From the preceding algorithm, we can observe that we generate an episode using
    the policy ![](img/B15558_04_054.png). Then for each step in the episode, we compute
    the return of state-action pair and compute the Q function *Q*(*s*[t], *a*[t])
    as an average return, then from this Q function, we extract a new policy ![](img/B15558_03_038.png).
    We repeat this step iteratively to find the optimal policy ![](img/B15558_03_172.png).
    Thus, we learned how to perform the control task using the Monte Carlo method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can classify the control methods into two types:'
  prefs: []
  type: TYPE_NORMAL
- en: On-policy control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Off-policy control
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**On-policy control—**In the on-policy control method, the agent behaves using
    one policy and also tries to improve the same policy. That is, in the on-policy
    method, we generate episodes using one policy and also improve the same policy
    iteratively to find the optimal policy. For instance, the MC control method, which
    we just learned above, can be called on-policy MC control as we are generating
    episodes using a policy ![](img/B15558_03_139.png), and we also try to improve
    the same policy ![](img/B15558_03_185.png) on every iteration to compute the optimal
    policy.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Off-policy control—**In the off-policy control method, the agent behaves
    using one policy *b* and tries to improve a different policy *![](img/B15558_04_099.png)*.
    That is, in the off-policy method, we generate episodes using one policy and we
    try to improve the different policy iteratively to find the optimal policy.'
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how exactly the preceding two control methods work in detail in
    the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: On-policy Monte Carlo control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two types of on-policy Monte Carlo control methods:'
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo exploring starts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo with the epsilon-greedy policy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monte Carlo exploring starts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have already learned how the Monte Carlo control method works. One thing
    we may want to take into account is exploration. There can be several actions
    in a state: some actions will be optimal, while others won''t. To understand whether
    an action is optimal or not, the agent has to explore by performing that action.
    If the agent never explores a particular action in a state, then it will never
    know whether it is a good action or not. So, how can we solve this? That is, how
    can we ensure enough exploration? Here is where Monte Carlo exploring starts helps
    us.'
  prefs: []
  type: TYPE_NORMAL
- en: In the MC exploring starts method, we set all state-action pairs to a non-zero
    probability for being the initial state-action pair. So before generating an episode,
    first, we choose the initial state-action pair randomly and then we generate the
    episode starting from this initial state-action pair following the policy ![](img/B15558_03_055.png).
    Then, in every iteration, our policy will be updated as a greedy policy (selecting
    the max Q value; see the next section on *Monte Carlo with the epsilon-greedy
    policy* for more details).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following steps show the algorithm of MC control exploring starts. It is
    essentially the same as what we learned earlier for the MC control algorithm section,
    except that here, we select an initial state-action pair and generate episodes
    starting from this initial state-action pair as shown in the bold point:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_03_084.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Select the initial state s****0** **and initial action a****0** **randomly
    such that all state-action pairs have a probability greater than 0**'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode from the selected initial state *s*[0] and action *a*[0]
    using policy ![](img/B15558_03_084.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all the rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q value by just taking the average, that is,![](img/B15558_04_101.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the updated policy ![](img/B15558_03_084.png) using the Q function:![](img/B15558_04_078.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the major drawbacks of the exploring starts method is that it is not
    applicable to every environment. That is, we can't just randomly choose any state-action
    pair as an initial state-action pair because in some environments there can be
    only one state-action pair that can act as an initial state-action pair. So we
    can't randomly select the state-action pair as the initial state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: For example, suppose we are training an agent to play a car racing game; we
    can't start the episode in a random position as the initial state and a random
    action as the initial action because we have a fixed single starting state and
    action as the initial state and action.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, to overcome the problem in exploring starts, in the next section, we will
    learn about the Monte Carlo control method with a new type of policy called the
    epsilon-greedy policy.
  prefs: []
  type: TYPE_NORMAL
- en: Monte Carlo with the epsilon-greedy policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before going ahead, first, let us understand what the epsilon-greedy policy
    is as it is ubiquitous in reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s learn what a greedy policy is. A greedy policy is one that selects
    the best action available at the moment. For instance, let''s say we are in some
    state **A** and we have four possible actions in the state. Let the actions be
    *up, down, left,* and *right*. But let''s suppose our agent has explored only
    two actions, *up* and *right*, in the state **A**; the Q value of actions *up*
    and *right* in the state **A** are shown in *Table 4.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 4.6: The agent has only explored two actions in state A'
  prefs: []
  type: TYPE_NORMAL
- en: We learned that the greedy policy selects the best action available at the moment.
    So the greedy policy checks the Q table and selects the action that has the maximum
    Q value in state **A**. As we can see, the action *up* has the maximum Q value.
    So our greedy policy selects the action *up* in state **A**.
  prefs: []
  type: TYPE_NORMAL
- en: But one problem with the greedy policy is that it never explores the other possible
    actions; instead, it always picks the best action available at the moment. In
    the preceding example, the greedy policy always selects the action *up*. But there
    could be other actions in state **A** that might be more optimal than the action
    *up* that the agent has not explored yet. That is, we still have two more actions—*down*
    and *left*—in state **A** that the agent has not explored yet, and they might
    be more optimal than the action *up*.
  prefs: []
  type: TYPE_NORMAL
- en: So, now the question is whether the agent should explore all the other actions
    in the state and select the best action as the one that has the maximum Q value
    or exploit the best action out of already-explored actions. This is called an
    **exploration-exploitation dilemma.**
  prefs: []
  type: TYPE_NORMAL
- en: Say there are many routes from our work to home and we have explored only two
    routes so far. Thus, to reach home, we can select the route that takes us home
    most quickly out of the two routes we have explored. However, there are still
    many other routes that we have not explored yet that might be even better than
    our current optimal route. The question is whether we should explore new routes
    (exploration) or whether we should always use our current optimal route (exploitation).
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this dilemma, we introduce a new policy called the epsilon-greedy policy.
    Here, all actions are tried with a non-zero probability (epsilon). With a probability
    epsilon, we explore different actions randomly and with a probability 1-epsilon,
    we choose an action that has the maximum Q value. That is, with a probability
    epsilon, we select a random action (exploration) and with a probability 1-epsilon
    we select the best action (exploitation).
  prefs: []
  type: TYPE_NORMAL
- en: In the epsilon-greedy policy, if we set the value of epsilon to 0, then it becomes
    a greedy policy (only exploitation), and when we set the value of epsilon to 1,
    then we will always end up doing only the exploration. So, the value of epsilon
    has to be chosen optimally between 0 and 1\.
  prefs: []
  type: TYPE_NORMAL
- en: Say we set epsilon = 0.5; then we will generate a random number from the uniform
    distribution and if the random number is less than epsilon (0.5), then we select
    a random action (exploration), but if the random number is greater than or equal
    to epsilon then we select the best action, that is, the action that has the maximum
    Q value (exploitation).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, in this way, we explore actions that we haven''t seen before with the probability
    epsilon and select the best actions out of the explored actions with the probability
    1-epsilon. As *Figure 4.22* shows, if the random number we generated from the
    uniform distribution is less than epsilon, then we choose a random action. If
    the random number is greater than or equal to epsilon, then we choose the best
    action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.22: Epsilon-greedy policy'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following snippet shows the Python code for the epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have understood what an epsilon-greedy policy is, and how it is
    used to solve the exploration-exploitation dilemma, in the next section, we will
    look at how to use the epsilon-greedy policy in the Monte Carlo control method.
  prefs: []
  type: TYPE_NORMAL
- en: The MC control algorithm with the epsilon-greedy policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The algorithm of Monte Carlo control with the epsilon-greedy policy is essentially
    the same as the MC control algorithm we learned earlier except that here we select
    actions based on the epsilon-greedy policy to avoid the exploration-exploitation
    dilemma. The following steps show the algorithm of Monte Carlo with the epsilon-greedy
    policy:'
  prefs: []
  type: TYPE_NORMAL
- en: Let total_return(*s*, *a*) be the sum of the return of a state-action pair across
    several episodes and *N*(*s*, *a*) be the number of times a state-action pair
    is visited across several episodes. Initialize total_return(*s*, *a*) and *N*(*s*,
    *a*) for all state-action pairs to zero and initialize a random policy ![](img/B15558_03_055.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using policy ![](img/B15558_04_117.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Store all rewards obtained in the episode in the list called rewards
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If (*s*[t], *a*[t]) is occurring for the first time in the episode:'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
- en: Compute the return of a state-action pair, *R*(*s*[t], *a*[t]) = sum(rewards[t:])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t])
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the counter as *N*(*s*[t], *a*[t]) = *N*(*s*[t], *a*[t]) + 1
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q value by just taking the average, that is,![](img/B15558_04_101.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the updated policy ![](img/B15558_04_119.png) using the Q function.
    Let ![](img/B15558_04_120.png). The policy ![](img/B15558_04_054.png) selects
    the best action ![](img/B15558_04_122.png) with probability ![](img/B15558_04_123.png)
    and random action with probability ![](img/B15558_04_124.png)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can observe, in every iteration, we generate the episode using the policy
    ![](img/B15558_04_125.png) and also we try to improve the same policy ![](img/B15558_03_140.png)
    in every iteration to compute the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing on-policy MC control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's learn how to implement the MC control method with the epsilon-greedy
    policy to play the blackjack game; that is, we will see how can we use the MC
    control method to find the optimal policy in the blackjack game.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a blackjack environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the dictionary for storing the Q values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the dictionary for storing the total return of the state-action
    pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the dictionary for storing the count of the number of times a state-action
    pair is visited:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Define the epsilon-greedy policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'We learned that we select actions based on the epsilon-greedy policy, so we
    define a function called `epsilon_greedy_policy`, which takes the state and Q
    value as an input and returns the action to be performed in the given state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the epsilon value to 0.5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Sample a random value from the uniform distribution; if the sampled value is
    less than epsilon then we select a random action, else we select the best action
    that has the maximum Q value as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: Generating an episode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now, let's generate an episode using the epsilon-greedy policy. We define a
    function called `generate_episode`, which takes the Q value as an input and returns
    the episode.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s set the number of time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s define the function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a list for storing the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state using the `reset` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Then for each time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the action according to the epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action and store the next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the state, action, and reward in the episode list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'If the next state is the final state then break the loop, else update the next
    state to the current state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Computing the optimal policy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to compute the optimal policy. First, let''s set the
    number of iterations, that is, the number of episodes, we want to generate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'For each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: We learned that in the on-policy control method, we will not be given any policy
    as an input. So, we initialize a random policy in the first iteration and improve
    the policy iteratively by computing the Q value. Since we extract the policy from
    the Q function, we don't have to explicitly define the policy. As the Q value
    improves, the policy also improves implicitly. That is, in the first iteration,
    we generate the episode by extracting the policy (epsilon-greedy) from the initialized
    Q function. Over a series of iterations, we will find the optimal Q function,
    and hence we also find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here we pass our initialized Q function to generate an episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Get all the state-action pairs in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Store all the rewards obtained in the episode in the rewards list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'For each step in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'If the state-action pair is occurring for the first time in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the return `R` of the state-action pair as the sum of rewards, *R*(*s*[t],
    *a*[t]) = sum(rewards[t:]):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the total return of the state-action pair as total_return(*s*[t], *a*[t])
    = total_return(*s*[t], *a*[t]) + R(*s*[t], *a*[t]):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the number of times the state-action pair is visited as *N*(*s*[t],
    *a*[t]) = *N*(*s*[t], *a*[t]) + 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: Compute the Q value by just taking the average, that is,
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_127.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Thus on every iteration, the Q value improves and so does the policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'After all the iterations, we can have a look at the Q value of each state-action
    pair in the pandas data frame for more clarity. First, let''s convert the Q value
    dictionary into a pandas data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the first few rows of the data frame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B15558_04_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.23: The Q values of the state-action pairs'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, we have the Q values for all the state-action pairs. Now
    we can extract the policy by selecting the action that has the maximum Q value
    in each state. For instance, say we are in the state `(21,8, True)`. Now, should
    we perform action 0 (stand) or action 1 (hit)? It makes more sense to perform
    action 0 (stand) here, since the value of the sum of our cards is already 21,
    and if we perform action 1 (hit) our game will lead to a bust.
  prefs: []
  type: TYPE_NORMAL
- en: Note that due to stochasticity, you might get different results than those shown
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the Q values of all the actions in this state, `(21,8, True)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.24: The Q values of the state (21,8, True)'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, we have a maximum Q value for action 0 (stand) compared to
    action 1 (hit). So, we perform action 0 in the state `(21,8, True)`. Similarly,
    in this way, we can extract the policy by selecting the action in each state that
    has the maximum Q value.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about an off-policy control method that uses
    two different policies.
  prefs: []
  type: TYPE_NORMAL
- en: Off-policy Monte Carlo control
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Off-policy Monte Carlo is another interesting Monte Carlo control method. In
    the off-policy method, we use two policies called the behavior policy and the
    target policy. As the name suggests, we behave (generate episodes) using the behavior
    policy and we try to improve the other policy called the target policy.
  prefs: []
  type: TYPE_NORMAL
- en: In the on-policy method, we generate an episode using the policy ![](img/B15558_03_140.png)
    and we improve the same policy ![](img/B15558_04_129.png) iteratively to find
    the optimal policy. But in the off-policy method, we generate an episode using
    a policy called the behavior policy *b* and we try to iteratively improve a different
    policy called the target policy ![](img/B15558_04_130.png).
  prefs: []
  type: TYPE_NORMAL
- en: That is, in the on-policy method, we learned that the agent generates an episode
    using the policy ![](img/B15558_03_008.png). Then for each step in the episode,
    we compute the return of the state-action pair and compute the Q function *Q*(*s*[t],
    *a*[t]) as an average return, then from this Q function, we extract a new policy
    ![](img/B15558_03_055.png). We repeat this step iteratively to find the optimal
    policy ![](img/B15558_04_032.png).
  prefs: []
  type: TYPE_NORMAL
- en: But in the off-policy method, the agent generates an episode using a policy
    called the behavior policy *b*. Then for each step in the episode, we compute
    the return of the state-action pair and compute the Q function *Q*(*s*[t], *a*[t])
    as an average return, then from this Q function, we extract a new policy called
    the target policy ![](img/B15558_03_084.png). We repeat this step iteratively
    to find the optimal target policy ![](img/B15558_04_032.png).
  prefs: []
  type: TYPE_NORMAL
- en: The behavior policy will usually be set to the epsilon-greedy policy and thus
    the agent explores the environment with the epsilon-greedy policy and generates
    an episode. Unlike the behavior policy, the target policy is set to be the greedy
    policy and so the target policy will always select the best action in each state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now understand how the off-policy Monte Carlo method works exactly.
    First, we will initialize the Q function with random values. Then we generate
    an episode using the behavior policy, which is the epsilon-greedy policy. That
    is, from the Q function we select the best action (the action that has the max
    Q value) with probability 1-epsilon and we select the random action with probability
    epsilon. Then for each step in the episode, we compute the return of the state-action
    pair and compute the Q function *Q*(*s*[t], *a*[t]) as an average return. Instead
    of using the arithmetic mean to compute the Q function, we can use the incremental
    mean. We can compute the Q function using the incremental mean as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_077.png)'
  prefs: []
  type: TYPE_IMG
- en: 'After computing the Q function, we extract the target policy ![](img/B15558_03_140.png)
    by selecting an action in each state that has the maximum Q value as shown as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_138.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The algorithm is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the Q function *Q*(*s*, *a*) with random values, set the behavior
    policy *b* to be epsilon-greedy, and also set the target policy ![](img/B15558_03_055.png)
    to be greedy policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using the behavior policy *b*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize return *R* to 0
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode, *t* = *T*-1, *T*-2,…, 0:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return as *R* = *R*+*r*[t][+1]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the Q value as ![](img/B15558_04_077.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target policy ![](img/B15558_04_138.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the target policy ![](img/B15558_04_142.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can observe from the preceding algorithm, first we set the Q values of
    all the state-action pairs to random values and then we generate an episode using
    the behavior policy. Then on each step of the episode, we compute the updated
    Q function (Q values) using the incremental mean and then we extract the target
    policy from the updated Q function. As we can notice, on every iteration, the
    Q function is constantly improving and since we are extracting the target policy
    from the Q function, our target policy will also be improving on every iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Also, note that since it is an off-policy method, the episode is generated using
    the behavior policy and we try to improve the target policy.
  prefs: []
  type: TYPE_NORMAL
- en: But wait! There is a small issue here. Since we are finding the target policy
    ![](img/B15558_04_142.png) from the Q function, which is computed based on the
    episodes generated by a different policy called the behavior policy, our target
    policy will be inaccurate. This is because the distribution of the behavior policy
    and the target policy will be different. So, to correct this, we introduce a new
    technique called **importance sampling**. This is a technique for estimating the
    values of one distribution when given samples from another.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us say we want to compute the expectation of a function *f*(*x*) where
    the value of *x* is sampled from the distribution *p*(*x*) that is, ![](img/B15558_04_143.png);
    then we can write:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_144.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the importance sampling method, we estimate the expectation using a different
    distribution *q*(*x*); that is, instead of sampling *x* from *p*(*x*) we use a
    different distribution *q*(*x*) as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_145.png)'
  prefs: []
  type: TYPE_IMG
- en: The ratio ![](img/B15558_04_146.png) is called the importance sampling ratio
    or importance correction.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how does importance sampling help us? We learned that with importance
    sampling, we can estimate the value of one distribution by sampling from another
    using the importance sampling ratio. In off-policy control, we can estimate the
    target policy with the samples (episodes) from the behavior policy using the importance
    sampling ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 'Importance sampling has two types:'
  prefs: []
  type: TYPE_NORMAL
- en: Ordinary importance sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weighted importance sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In ordinary importance sampling, the importance sampling ratio will be the ratio
    of the target policy to the behavior policy ![](img/B15558_04_147.png) and in
    weighted importance sampling, the importance sampling ratio will be the weighted
    ratio of the target policy to the behavior policy ![](img/B15558_04_148.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now understand how we use weighted importance sampling in the off-policy
    Monte Carlo method. Let *W* be the weight and *C*(*s*[t], *a*[t]) denote the cumulative
    sum of weights across all the episodes. We learned that we compute the Q function
    (Q values) using the incremental mean as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_077.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we slightly modify our Q function computation with the weighted importance
    sampling as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_04_150.png)'
  prefs: []
  type: TYPE_IMG
- en: The algorithm of the off-policy Monte Carlo method is shown next. First, we
    generate an episode using the behavior policy and then we initialize return *R*
    to 0 and the weight *W* to 1\. Then on every step of the episode, we compute the
    return and update the cumulative weight as *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t])
    + *W*. After updating the cumulative weights, we update the Q value as ![](img/B15558_04_150.png).
  prefs: []
  type: TYPE_NORMAL
- en: From the Q value, we extract the target policy as ![](img/B15558_04_138.png).
    When the action *a*[t] given by the behavior policy and the target policy is not
    the same then we break the loop and generate the next episode; else we update
    the weight as ![](img/B15558_04_153.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The complete algorithm of the off-policy Monte Carlo method is explained in
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the Q function *Q*(*s*, *a*) with random values, set the behavior
    policy *b* to be epsilon-greedy, and target policy ![](img/B15558_03_084.png)
    to be greedy policy and initialize the cumulative weights as *C*(*s*, *a*) = 0
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For *M* number of episodes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generate an episode using the behavior policy *b*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize return *R* to 0 and weight *W* to 1
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each step *t* in the episode, *t* = *T*-1, *T*-2,…, 0:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the return as *R* = *R*+*r*[t][+1]
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the cumulative weights *C*(*s*[t], *a*[t]) = *C*(*s*[t], *a*[t]) + *W*
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the Q value as ![](img/B15558_04_150.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the target policy ![](img/B15558_04_138.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If ![](img/B15558_04_157.png) then break
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weight as ![](img/B15558_04_153.png)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Return the target policy ![](img/B15558_03_139.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Is the MC method applicable to all tasks?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned that Monte Carlo is a model-free method, and so it doesn't require
    the model dynamics of the environment to compute the value and Q function in order
    to find the optimal policy. The Monte Carlo method computes the value function
    and Q function by just taking the average return of the state and the average
    return of the state-action pair, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: But one issue with the Monte Carlo method is that it is applicable only to episodic
    tasks. We learned that in the Monte Carlo method, we compute the value of the
    state by taking the average return of the state and the return is the sum of rewards
    of the episode. But when there is no episode, that is, if our task is a continuous
    task (non-episodic task), then we cannot apply the Monte Carlo method.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how do we compute the value of the state where we have a continuous task
    and also where we don't know the model dynamics of the environment? Here is where
    we use another interesting model-free method called temporal difference learning.
    In the next chapter, we will learn exactly how temporal difference learning works.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by understanding what the Monte Carlo method is. We learned
    that in the Monte Carlo method, we approximate the expectation of a random variable
    by sampling, and when the sample size is greater, the approximation will be better.
    Then we learned about the prediction and control tasks. In the prediction task,
    we evaluate the given policy by predicting the value function or Q function, which
    helps us to understand the expected return an agent would get if it uses the given
    policy. In the control task, our goal is to find the optimal policy, and we will
    not be given any policy as input, so we start by initializing a random policy
    and we try to find the optimal policy iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we learned how to use the Monte Carlo method to perform the
    prediction task. We learned that the value of a state and the value of a state-action
    pair can be computed by just taking the average return of the state and an average
    return of state-action pair across several episodes, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned about the first-visit MC and every-visit MC methods. In first-visit
    MC, we compute the return only for the first time the state is visited in the
    episode, and in every-visit MC, we compute the return every time the state is
    visited in the episode.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we explored how to perform a control task using the Monte Carlo
    method. We learned about two different types of control methods—on-policy and
    off-policy control.
  prefs: []
  type: TYPE_NORMAL
- en: In the on-policy method, we generate episodes using one policy and also improve
    the same policy iteratively to find the optimal policy. We first learned about
    the Monte Carlo control exploring starts method where we set all the state-action
    pairs to a non-zero probability to ensure exploration. Later, we learned about
    Monte Carlo control with an epsilon-greedy policy where we select a random action
    (exploration) with probability epsilon, and with probability 1-epsilon we select
    the best action (exploitation).
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the chapter, we discussed the off-policy Monte Carlo control method
    where we use two different policies called the behavior policy, for generating
    the episode, and the target policy, for finding the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assess our knowledge of the Monte Carlo methods by answering the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the Monte Carlo method?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the Monte Carlo method preferred over dynamic programming?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do prediction tasks differ from control tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the MC prediction method predict the value function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the difference between first-visit MC and every-visit MC?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why do we use incremental mean updates?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does on-policy control differ from off-policy control?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the epsilon-greedy policy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
