<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Face Generation and Handling Missing Labels</h1>
                </header>
            
            <article>
                
<p class="calibre2">The list of interesting applications that we can use GANs for is endless. In this chapter, we are going to demonstrate another promising application of GANs, which is face generation based on the CelebA database. We'll also demonstrate how to use GANs for semi-supervised learning setups where we've got a poorly labeled dataset with some missing labels.</p>
<p class="calibre2">The following topics will be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">Face generation</li>
<li class="calibre8">Semi-supervised learning with generative adversarial networks (GANs)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Face generation</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we mentioned in the previous chapter, the Generator and Discriminator consist of a <strong class="calibre13">Deconvolutional Network</strong> (<strong class="calibre13">DNN</strong>: <a href="https://www.quora.com/How-does-a-deconvolutional-neural-network-work" target="_blank" class="calibre11">https://www.quora.com/How-does-a-deconvolutional-neural-network-work</a>) and <strong class="calibre13">Convolutional Neural Network</strong> (<strong class="calibre13">CNN</strong>: <a href="http://cs231n.github.io/convolutional-networks/" target="_blank" class="calibre11">http://cs231n.github.io/convolutional-networks/</a>):</p>
<ul class="calibre7">
<li class="calibre8">CNN is a a type of neural network that encodes hundreds of pixels of an image into a vector of small dimensions (z), which is a summary of the image</li>
<li class="calibre8">DNN is a network that learns some filters to recover the original image from z</li>
</ul>
<p class="calibre2">Also, the discriminator will output one or zero to indicate whether the input image is from the actual dataset or generated by the generator. On the other side, the generator will try to replicate <span class="calibre10">images </span>similar to the original dataset based on the latent space z, which might follow a Gaussian distribution. So, the goal of the discriminator is to correctly discriminate between the real images, and the goal of the generator is to learn the distribution of the original dataset and hence fool the discriminator so that it makes a wrong decision.</p>
<p class="calibre2">In this section, we'll try to teach the generator to learn human face image distribution so that it can generate realistic faces.</p>
<p class="calibre2">Generating human-like faces is crucial for most graphics companies, who are always looking for new faces for their applications, and it gives us a clue of how artificial intelligence is really close to achieving realism in generating artificial faces.</p>
<p class="calibre2">In this example, we'll be using the CelebA dataset. The CelebFaces Attributes Dataset (CelebA)  is a large-scale face attributes dataset with about 200K celebrity images, each with 40 attribute annotations. There are lots of pose variations covered by the dataset, as well as background clutter, so CelebA is very diverse and well annotated. it includes:</p>
<ul class="calibre7">
<li class="calibre8">10,177 identities</li>
<li class="calibre8">202,599 face images</li>
<li class="calibre8">Five landmark locations and 40 binary attribute annotations per image</li>
</ul>
<p class="calibre2">We can use this dataset for many computer vision applications other than face generation, such as face recognition and localization, or face attribute detection.</p>
<p class="calibre2">This figure shows how the generator error, or learning human face distribution, gets close to realism during the training process:</p>
<div class="CDPAlignCenter"><img src="assets/26446f87-864b-4267-95b8-a4980fd70a11.png" class="calibre177"/></div>
<div class="CDPAlignCenter1">Figure 1: GANs for generating new faces from a celebrity images dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting the data</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we will define some helper functions that will help us to download the CelebA dataset. We'll start off by importing their required packages for this implementation:</p>
<pre class="calibre21">import math<br class="title-page-name"/>import os<br class="title-page-name"/>import hashlib<br class="title-page-name"/>from urllib.request import urlretrieve<br class="title-page-name"/>import zipfile<br class="title-page-name"/>import gzip<br class="title-page-name"/>import shutil<br class="title-page-name"/><br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from PIL import Image<br class="title-page-name"/>from tqdm import tqdm<br class="title-page-name"/>import utils<br class="title-page-name"/><br class="title-page-name"/>import tensorflow as tf</pre>
<p class="calibre2">Next up, we are going to use the utils script to download the dataset:<br class="calibre20"/></p>
<pre class="calibre21">#Downloading celebA dataset<br class="title-page-name"/>celebA_data_dir = 'input'<br class="title-page-name"/>utils.download_extract('celeba', celebA_data_dir)</pre>
<pre class="calibre21">Output:<br class="title-page-name"/><br class="title-page-name"/>Downloading celeba: 1.44GB [00:21, 66.6MB/s] <br class="title-page-name"/>Extracting celeba...</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the Data</h1>
                </header>
            
            <article>
                
<p class="calibre2">The CelebA dataset contains over 200k annotated celebrity images. Since we are going to use GANs to generate similar images, it is worth looking at a bunch of images from the dataset and see how they look. In this section, we are going to define some helper functions for visualizing a bunch of images from the CelebA dataset.</p>
<p class="calibre2">Now, let's use the <kbd class="calibre12">utils</kbd> script to display some images from the dataset:</p>
<pre class="calibre21">#number of images to display<br class="title-page-name"/>num_images_to_show = <span>25<br class="title-page-name"/></span><span><br class="title-page-name"/></span>celebA_images = utils.get_batch(glob(os.path.join(celebA_data_dir, <span>'img_align_celeba/*.jpg'</span>))[:num_images_to_show], <span>28</span>,<br class="title-page-name"/>                                <span>28</span>, <span>'RGB'</span>)<br class="title-page-name"/>pyplot.imshow(utils.images_square_grid(celebA_images, <span>'RGB'</span>))</pre>
<pre class="calibre21">Output:</pre>
<div class="CDPAlignCenter"><img src="assets/1972d296-3172-438f-8dd0-4b700ba8c968.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 2: Plotting a sample of images from CelebA dataset</div>
<p class="calibre2">The main focus of this computer vision task is to use GANs for generating images similar two the ones in the celebrity dataset, so we'll need to focus on the face part of the images. To focus on the face part of an image, we are going to remove the parts of the image that don't include a celebrity face.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, let's start off by building the core of our implementation, which is the computational graph; it will mainly include the following components:</p>
<ul class="calibre7">
<li class="calibre8">Model inputs</li>
<li class="calibre8">Discriminator</li>
<li class="calibre8">Generator</li>
<li class="calibre8">Model losses</li>
<li class="calibre8">Model optimizer</li>
<li class="calibre8">Training the model</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model inputs</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to implement a helper function that we'll define the model input placeholders which will responsible for feeding the data the the computational graph.</p>
<p class="calibre2">The functions should be able to create three main placeholders:</p>
<ul class="calibre7">
<li class="calibre8">Actual input images from the dataset which will have the dimensions of (batch size, input image width, input image height, number of channels)</li>
<li class="calibre8">The latent space Z, which will be used by the generator for generating fake images</li>
<li class="calibre8">Learning rate placeholder</li>
</ul>
<p class="calibre2">The helper function will return a tuple of these three input placeholders. So, let's go ahead and define this function:</p>
<pre class="calibre21"><span># defining the model inputs<br class="title-page-name"/></span><span>def </span>inputs(img_width, img_height, img_channels, latent_space_z_dim):<br class="title-page-name"/>    true_inputs = tf.placeholder(tf.float32, (<span>None</span>, img_width, img_height, img_channels),<br class="title-page-name"/>                                 <span>'true_inputs'</span>)<br class="title-page-name"/>    l_space_inputs = tf.placeholder(tf.float32, (<span>None</span>, latent_space_z_dim), <span>'l_space_inputs'</span>)<br class="title-page-name"/>    model_learning_rate = tf.placeholder(tf.float32, <span>name</span>=<span>'model_learning_rate'</span>)<br class="title-page-name"/><br class="title-page-name"/>    <span>return </span>true_inputs, l_space_inputs, model_learning_rate</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discriminator</h1>
                </header>
            
            <article>
                
<p class="calibre2"> </p>
<p class="calibre2">Next up, we need to implement the discriminator part of the network, which will be used to judge whether the incoming input is coming from the real dataset or generated by the generator. Again, we'll use the TensorFlow feature of <kbd class="calibre12">tf.variable_scope</kbd> to prefix some variables with discriminator so that we can retrieve and reuse them.</p>
<p class="calibre2">So, let's define the function which will return the binary output of the discriminator as well as the logit values:</p>
<pre class="calibre21"><span># Defining the discriminator function<br class="title-page-name"/></span><span>def </span>discriminator(input_imgs, reuse=<span>False</span>):<br class="title-page-name"/>    <span># using variable_scope to reuse variables<br class="title-page-name"/></span><span>    </span><span>with </span>tf.variable_scope(<span>'discriminator'</span>, <span>reuse</span>=reuse):<br class="title-page-name"/>        <span># leaky relu parameter<br class="title-page-name"/></span><span>        </span>leaky_param_alpha = <span>0.2<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>        </span><span># defining the layers<br class="title-page-name"/></span><span>        </span>conv_layer_1 = tf.layers.conv2d(input_imgs, <span>64</span>, <span>5</span>, <span>2</span>, <span>'same'</span>)<br class="title-page-name"/>        leaky_relu_output = tf.maximum(leaky_param_alpha * conv_layer_1, conv_layer_1)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_2 = tf.layers.conv2d(leaky_relu_output, <span>128</span>, <span>5</span>, <span>2</span>, <span>'same'</span>)<br class="title-page-name"/>        normalized_output = tf.layers.batch_normalization(conv_layer_2, <span>training</span>=<span>True</span>)<br class="title-page-name"/>        leay_relu_output = tf.maximum(leaky_param_alpha * normalized_output, normalized_output)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_3 = tf.layers.conv2d(leay_relu_output, <span>256</span>, <span>5</span>, <span>2</span>, <span>'same'</span>)<br class="title-page-name"/>        normalized_output = tf.layers.batch_normalization(conv_layer_3, <span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_relu_output = tf.maximum(leaky_param_alpha * normalized_output, normalized_output)<br class="title-page-name"/><br class="title-page-name"/>        <span># reshaping the output for the logits to be 2D tensor<br class="title-page-name"/></span><span>        </span>flattened_output = tf.reshape(leaky_relu_output, (-<span>1</span>, <span>4 </span>* <span>4 </span>* <span>256</span>))<br class="title-page-name"/>        logits_layer = tf.layers.dense(flattened_output, <span>1</span>)<br class="title-page-name"/>        output = tf.sigmoid(logits_layer)<br class="title-page-name"/><br class="title-page-name"/>    <span>return </span>output, logits_layer</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generator</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, it's time to implement the second part of the network that will be trying to replicate the original input images using the latent space <kbd class="calibre12">z</kbd>. We'll be using <kbd class="calibre12">tf.variable_scope</kbd> for this function as well.</p>
<p class="calibre2">So, let's define the function which will return a generated image by the generator:</p>
<pre class="calibre21">def generator(z_latent_space, output_channel_dim, is_train=True):<br class="title-page-name"/><br class="title-page-name"/>    <br class="title-page-name"/>    with tf.variable_scope('generator', reuse=not is_train):<br class="title-page-name"/>        <br class="title-page-name"/>        #leaky relu parameter<br class="title-page-name"/>        leaky_param_alpha = 0.2<br class="title-page-name"/>    <br class="title-page-name"/>        fully_connected_layer = tf.layers.dense(z_latent_space, 2*2*512)<br class="title-page-name"/>        <br class="title-page-name"/>        #reshaping the output back to 4D tensor to match the accepted format for convolution layer<br class="title-page-name"/>        reshaped_output = tf.reshape(fully_connected_layer, (-1, 2, 2, 512))<br class="title-page-name"/>        normalized_output = tf.layers.batch_normalization(reshaped_output, training=is_train)<br class="title-page-name"/>        leaky_relu_output = tf.maximum(leaky_param_alpha * normalized_output, normalized_output)<br class="title-page-name"/>    <br class="title-page-name"/>        conv_layer_1 = tf.layers.conv2d_transpose(leaky_relu_output, 256, 5, 2, 'valid')<br class="title-page-name"/>        normalized_output = tf.layers.batch_normalization(conv_layer_1, training=is_train)<br class="title-page-name"/>        leaky_relu_output = tf.maximum(leaky_param_alpha * normalized_output, normalized_output)<br class="title-page-name"/>    <br class="title-page-name"/>        conv_layer_2 = tf.layers.conv2d_transpose(leaky_relu_output, 128, 5, 2, 'same')<br class="title-page-name"/>        normalized_output = tf.layers.batch_normalization(conv_layer_2, training=is_train)<br class="title-page-name"/>        leaky_relu_output = tf.maximum(leaky_param_alpha * normalized_output, normalized_output)<br class="title-page-name"/>    <br class="title-page-name"/>        logits_layer = tf.layers.conv2d_transpose(leaky_relu_output, output_channel_dim, 5, 2, 'same')<br class="title-page-name"/>        output = tf.tanh(logits_layer)<br class="title-page-name"/>    <br class="title-page-name"/>        return output</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model losses</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now comes the tricky part, which we covered in the previous chapter, which is to calculate the losses of the discriminator and the generator.</p>
<p class="calibre2">So, let's define this function, which will make use of the <kbd class="calibre12">generator</kbd> and <kbd class="calibre12">discriminator</kbd> functions that were defined previously:</p>
<pre class="calibre21"><span># Define the error for the discriminator and generator<br class="title-page-name"/></span><span>def </span>model_losses(input_actual, input_latent_z, out_channel_dim):<br class="title-page-name"/>    <span># building the generator part<br class="title-page-name"/></span><span>    </span>gen_model = generator(input_latent_z, out_channel_dim)<br class="title-page-name"/>    disc_model_true, disc_logits_true = discriminator(input_actual)<br class="title-page-name"/>    disc_model_fake, disc_logits_fake = discriminator(gen_model, <span>reuse</span>=<span>True</span>)<br class="title-page-name"/><br class="title-page-name"/>    disc_loss_true = tf.reduce_mean(<br class="title-page-name"/>        tf.nn.sigmoid_cross_entropy_with_logits(<span>logits</span>=disc_logits_true, <span>labels</span>=tf.ones_like(disc_model_true)))<br class="title-page-name"/><br class="title-page-name"/>    disc_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(<br class="title-page-name"/>        <span>logits</span>=disc_logits_fake, <span>labels</span>=tf.zeros_like(disc_model_fake)))<br class="title-page-name"/><br class="title-page-name"/>    gen_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(<br class="title-page-name"/>        <span>logits</span>=disc_logits_fake, <span>labels</span>=tf.ones_like(disc_model_fake)))<br class="title-page-name"/><br class="title-page-name"/>    disc_loss = disc_loss_true + disc_loss_fake<br class="title-page-name"/><br class="title-page-name"/>    <span>return </span>disc_loss, gen_loss</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model optimizer</h1>
                </header>
            
            <article>
                
<p class="calibre2">Finally, before training our model, we need to implement the optimization criteria for this task. We will use the naming conventions that we used previously to retrieve the trainable parameters for the discriminator and the generator and train them:</p>
<pre class="calibre21"><span># specifying the optimization criteria<br class="title-page-name"/></span><span>def </span>model_optimizer(disc_loss, gen_loss, learning_rate, beta1):<br class="title-page-name"/>    trainable_vars = tf.trainable_variables()<br class="title-page-name"/>    disc_vars = [var <span>for </span>var <span>in </span>trainable_vars <span>if </span>var.name.startswith(<span>'discriminator'</span>)]<br class="title-page-name"/>    gen_vars = [var <span>for </span>var <span>in </span>trainable_vars <span>if </span>var.name.startswith(<span>'generator'</span>)]<br class="title-page-name"/><br class="title-page-name"/>    disc_train_opt = tf.train.AdamOptimizer(<br class="title-page-name"/>        learning_rate, <span>beta1</span>=beta1).minimize(disc_loss, <span>var_list</span>=disc_vars)<br class="title-page-name"/><br class="title-page-name"/>    update_operations = tf.get_collection(tf.GraphKeys.UPDATE_OPS)<br class="title-page-name"/>    gen_updates = [opt <span>for </span>opt <span>in </span>update_operations <span>if </span>opt.name.startswith(<span>'generator'</span>)]<br class="title-page-name"/><br class="title-page-name"/>    <span>with </span>tf.control_dependencies(gen_updates):<br class="title-page-name"/>        gen_train_opt = tf.train.AdamOptimizer(<br class="title-page-name"/>            learning_rate, beta1).minimize(gen_loss, <span>var_list</span>=gen_vars)<br class="title-page-name"/><br class="title-page-name"/>    <span>return </span>disc_train_opt, gen_train_opt</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, it's time to train the model and see how the generator will be able to fool, to some extent, the discriminator, by generating <span class="calibre10">images </span>very close to the original CelebA dataset.</p>
<p class="calibre2">But first, let's define a helper function that will display some generated images by the generator:</p>
<pre class="calibre21"><span># define a function to visualize some generated images from the generator<br class="title-page-name"/></span><span>def </span>show_generator_output(sess, num_images, input_latent_z, output_channel_dim, img_mode):<br class="title-page-name"/>    cmap = <span>None if </span>img_mode == <span>'RGB' </span><span>else </span><span>'gray'<br class="title-page-name"/></span><span>    </span>latent_space_z_dim = input_latent_z.get_shape().as_list()[-<span>1</span>]<br class="title-page-name"/>    examples_z = np.random.uniform(-<span>1</span>, <span>1</span>, <span>size</span>=[num_images, latent_space_z_dim])<br class="title-page-name"/><br class="title-page-name"/>    examples = sess.run(<br class="title-page-name"/>        generator(input_latent_z, output_channel_dim, <span>False</span>),<br class="title-page-name"/>        <span>feed_dict</span>={input_latent_z: examples_z})<br class="title-page-name"/><br class="title-page-name"/>    images_grid = utils.images_square_grid(examples, img_mode)<br class="title-page-name"/>    pyplot.imshow(images_grid, <span>cmap</span>=cmap)<br class="title-page-name"/>    pyplot.show()</pre>
<p class="calibre2">Then, we will use the helper functions that we have defined before to build the model inputs, loss, and optimization criteria. We stack them together and start training our model based on the CelebA dataset:</p>
<pre class="calibre21"><span>def </span>model_train(num_epocs, train_batch_size, z_dim, learning_rate, beta1, get_batches, input_data_shape, data_img_mode):<br class="title-page-name"/>    _, image_width, image_height, image_channels = input_data_shape<br class="title-page-name"/><br class="title-page-name"/>    actual_input, z_input, leaningRate = inputs(<br class="title-page-name"/>        image_width, image_height, image_channels, z_dim)<br class="title-page-name"/><br class="title-page-name"/>    disc_loss, gen_loss = model_losses(actual_input, z_input, image_channels)<br class="title-page-name"/><br class="title-page-name"/>    disc_opt, gen_opt = model_optimizer(disc_loss, gen_loss, learning_rate, beta1)<br class="title-page-name"/><br class="title-page-name"/>    steps = <span>0<br class="title-page-name"/></span><span>    </span>print_every = <span>50<br class="title-page-name"/></span><span>    </span>show_every = <span>100<br class="title-page-name"/></span><span>    </span>model_loss = []<br class="title-page-name"/>    num_images = <span>25<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>    </span><span>with </span>tf.Session() <span>as </span>sess:<br class="title-page-name"/><br class="title-page-name"/>        <span># initializing all the variables<br class="title-page-name"/></span><span>        </span>sess.run(tf.global_variables_initializer())<br class="title-page-name"/><br class="title-page-name"/>        <span>for </span>epoch_i <span>in </span><span>range</span>(num_epocs):<br class="title-page-name"/>            <span>for </span>batch_images <span>in </span>get_batches(train_batch_size):<br class="title-page-name"/><br class="title-page-name"/>                steps += <span>1<br class="title-page-name"/></span><span>                </span>batch_images *= <span>2.0<br class="title-page-name"/></span><span>                </span>z_sample = np.random.uniform(-<span>1</span>, <span>1</span>, (train_batch_size, z_dim))<br class="title-page-name"/><br class="title-page-name"/>                _ = sess.run(disc_opt, <span>feed_dict</span>={<br class="title-page-name"/>                    actual_input: batch_images, z_input: z_sample, leaningRate: learning_rate})<br class="title-page-name"/>                _ = sess.run(gen_opt, <span>feed_dict</span>={<br class="title-page-name"/>                    z_input: z_sample, leaningRate: learning_rate})<br class="title-page-name"/><br class="title-page-name"/>                <span>if </span>steps % print_every == <span>0</span>:<br class="title-page-name"/>                    train_loss_disc = disc_loss.eval({z_input: z_sample, actual_input: batch_images})<br class="title-page-name"/>                    train_loss_gen = gen_loss.eval({z_input: z_sample})<br class="title-page-name"/><br class="title-page-name"/>                    <span>print</span>(<span>"Epoch {}/{}..."</span>.format(epoch_i + <span>1</span>, num_epocs),<br class="title-page-name"/>                          <span>"Discriminator Loss: {:.4f}..."</span>.format(train_loss_disc),<br class="title-page-name"/>                          <span>"Generator Loss: {:.4f}"</span>.format(train_loss_gen))<br class="title-page-name"/>                    model_loss.append((train_loss_disc, train_loss_gen))<br class="title-page-name"/><br class="title-page-name"/>                <span>if </span>steps % show_every == <span>0</span>:<br class="title-page-name"/>                    show_generator_output(sess, num_images, z_input, image_channels, data_img_mode)</pre>
<p class="calibre2">Kick off the training process, which might take some time depending on your host machine specs:</p>
<pre class="calibre21"><span># Training the model on CelebA dataset<br class="title-page-name"/></span>train_batch_size = <span>64<br class="title-page-name"/></span>z_dim = <span>100<br class="title-page-name"/></span>learning_rate = <span>0.002<br class="title-page-name"/></span>beta1 = <span>0.5<br class="title-page-name"/></span><span><br class="title-page-name"/></span>num_epochs = <span>1<br class="title-page-name"/></span><span><br class="title-page-name"/></span>celeba_dataset = utils.Dataset(<span>'celeba'</span>, glob(os.path.join(data_dir, <span>'img_align_celeba/*.jpg'</span>)))<br class="title-page-name"/><span>with </span>tf.Graph().as_default():<br class="title-page-name"/>    model_train(num_epochs, train_batch_size, z_dim, learning_rate, beta1, celeba_dataset.get_batches,<br class="title-page-name"/>                celeba_dataset.shape, celeba_dataset.image_mode)</pre>
<p class="calibre2">Output:</p>
<pre class="calibre21"><br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 0.9118... Generator Loss: 12.2238<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 0.6119... Generator Loss: 3.2168<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 0.5383... Generator Loss: 2.8054<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 1.4381... Generator Loss: 0.4672<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 0.7815... Generator Loss: 14.8220<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 0.6435... Generator Loss: 9.2591<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 1.5661... Generator Loss: 10.4747<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 1.5407... Generator Loss: 0.5811<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 0.6470... Generator Loss: 2.9002<br class="title-page-name"/> Epoch 1/1... Discriminator Loss: 0.5671... Generator Loss: 2.0700</pre>
<div class="CDPAlignCenter"><img src="assets/2d5b7d29-193a-4b1d-898d-8f1a4345ca3e.png" class="calibre31"/></div>
<div class="packtfigref2">Figure 3: Sample generated output at this point of training</div>
<pre class="calibre21">Epoch 1/1... Discriminator Loss: 0.7950... Generator Loss: 1.5818<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 1.2417... Generator Loss: 0.7094<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 1.1786... Generator Loss: 1.0948<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 1.0427... Generator Loss: 2.8878<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 0.8409... Generator Loss: 2.6785<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 0.8557... Generator Loss: 1.7706<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 0.8241... Generator Loss: 1.2898<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 0.8590... Generator Loss: 1.8217<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 1.1694... Generator Loss: 0.8490<br class="title-page-name"/>Epoch 1/1... Discriminator Loss: 0.9984... Generator Loss: 1.0042</pre>
<div class="CDPAlignCenter"><img src="assets/5b21242d-61d1-45ba-90be-a766f2d984fd.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 4: Sample generated output at this point of training</div>
<p class="calibre2">After some time of training, you should get something like this:</p>
<div class="CDPAlignCenter"><img src="assets/e78748d2-24a1-4507-9a53-c202659576b0.png" class="calibre31"/></div>
<div class="title-page-name">
<div class="CDPAlignCenter1">Figure 5: Sample generated output at this point of training</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Semi-supervised learning with Generative Adversarial Networks (GANs)</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">With that in mind, semi-supervised learning is a technique in which both labeled and unlabeled data is used to train a classifier.</p>
<p class="calibre2">This type of classifier takes a tiny portion of labeled data and a much larger amount of unlabeled data (from the same domain). The goal is to combine these sources of data to train a <strong class="calibre13">Deep Convolution Neural Network</strong> (<strong class="calibre13">DCNN</strong>) to learn an inferred function capable of mapping a new datapoint to its desirable outcome.</p>
<p class="calibre2">In this frontier, we present a GAN model to classify street view house numbers using a very small labeled training set. In fact, the model uses roughly 1.3% of the original SVHN training labels i.e. 1000 (one thousand) labeled examples. We use some of the techniques described in the paper <em class="calibre19">Improved Techniques for Training GANs from OpenAI</em> (<a href="https://arxiv.org/abs/1606.03498" target="_blank" class="calibre11">https://arxiv.org/abs/1606.03498</a>).</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Intuition</h1>
                </header>
            
            <article>
                
<p class="calibre2">When building a GAN for generating images, we trained both   the generator and the discriminator at the same time. After training, we can discard the discriminator because we only used it for training the generator.</p>
<div class="CDPAlignCenter"><img src="assets/7b223ba3-8c25-4477-9a17-f04fa287296b.png" class="calibre178"/></div>
<div class="CDPAlignCenter1">Figure 6: Semi-supervised learning GAN architecture for an 11 class classification problem</div>
<div class="title-page-name">
<p class="calibre2">In semi-supervised learning, we need to transform the discriminator into a multi-class classifier. This new model has to be able to generalize well on the test set, even though we do not have many labeled examples for training. Additionally, this time, by the end of training, we can actually throw away the generator. Note that the roles changed. Now, the generator is only used for helping the discriminator during training. Putting it differently, the generator acts as a different source of information from which the discriminator gets raw, unlabeled training data. As we will see, this unlabelled data is key to improving the discriminator's performance. Also, for a regular image generation GAN, the discriminator has only one role. Compute the probability of whether its inputs are real or not — let's call it the GAN problem.</p>
<p class="calibre2">However, to turn the discriminator into a semi-supervised classifier, besides the GAN problem, the discriminator also has to learn the probabilities of each of the original dataset classes. In other words, for each input image, the discriminator has to learn the probabilities of it being a one, two, three, and so on.</p>
<p class="calibre2">Recall that for an image generation GAN discriminator, we have a single sigmoid unit output. This value represents the probability of an input image being real (value close to 1), or fake (value near 0). In other words, from the discriminator's point of view, values close to 1 mean that the samples are likely to come from the training set. Likewise, value near 0 mean a higher chance that the samples come from the generator network. By using this probability, the discriminator is able to send a signal back to the generator. This signal allows the generator to adapt its parameters during training, making it possible to improve its capabilities of creating realistic images.</p>
<p class="calibre2">We have to convert the discriminator (from the previous GAN) into an 11 class classifier. To do that, we can turn its sigmoid output into a softmax with 11 class outputs, the first 10 for the individual class probabilities of the SVHN dataset (zero to nine), and the 11th class for all the fake images that come from the generator.</p>
</div>
<div class="packtinfobox">Note that if we set the 11th class probability to 0, then the sum of the first 10 probabilities represents the same probability computed using the sigmoid function.</div>
<p class="calibre2">Finally, we need to set up the losses in such a way that the discriminator can do both:</p>
<ul class="calibre7">
<li class="calibre8">Help the generator learn to produce realistic images. To do that, we have to instruct the discriminator to distinguish between real and fake samples.</li>
<li class="calibre8">Use the generator’s images, along with the labeled and unlabeled training data, to help classify the dataset.</li>
</ul>
<p class="calibre2">To summarize, the discriminator has three different sources of training data:</p>
<ul class="calibre7">
<li class="calibre8">Real images with labels. These are image label pairs like in any regular supervised classification problem.</li>
<li class="calibre8">Real images without labels. For those, the classifier only learns that these images are real.</li>
<li class="calibre8">Images from the generator. To use these ones, the discriminator learns to classify as fake.</li>
</ul>
<p class="calibre2">The combination of these different sources of data will make the classifier able to learn from a broader perspective. That, in turn, allows the model to perform inference much more precisely than it would be if only using the 1,000 labeled examples for training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis and preprocessing</h1>
                </header>
            
            <article>
                
<p class="calibre2">For this task, we will be using SVHN dataset, which is an abbreviation for Street View House Numbers by Stanford (<a href="http://ufldl.stanford.edu/housenumbers/" target="_blank" class="calibre11">http://ufldl.stanford.edu/housenumbers/</a>). So, let's start the implementation by importing the required packages for this implementation:</p>
<pre class="calibre21"># Lets start by loading the necessary libraries<br class="title-page-name"/>%matplotlib inline<br class="title-page-name"/><br class="title-page-name"/>import pickle as pkl<br class="title-page-name"/>import time<br class="title-page-name"/>import matplotlib.pyplot as plt<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>from scipy.io import loadmat<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import os<br class="title-page-name"/><br class="title-page-name"/></pre>
<p class="calibre2">Next up, we are going to define a helper class to download the SVHN dataset (remember that you need to manually create the <kbd class="calibre12">input_data_dir</kbd> first):</p>
<pre class="calibre21">from urllib.request import urlretrieve<br class="title-page-name"/>from os.path import isfile, isdir<br class="title-page-name"/>from tqdm import tqdm<br class="title-page-name"/>input_data_dir = 'input/'<br class="title-page-name"/><br class="title-page-name"/>input_data_dir = 'input/'<br class="title-page-name"/><br class="title-page-name"/>if not isdir(input_data_dir):<br class="title-page-name"/>    raise Exception("Data directory doesn't exist!")<br class="title-page-name"/><br class="title-page-name"/>class DLProgress(tqdm):<br class="title-page-name"/>    last_block = 0<br class="title-page-name"/><br class="title-page-name"/>    def hook(self, block_num=1, block_size=1, total_size=None):<br class="title-page-name"/>        self.total = total_size<br class="title-page-name"/>        self.update((block_num - self.last_block) * block_size)<br class="title-page-name"/>        self.last_block = block_num<br class="title-page-name"/><br class="title-page-name"/>if not isfile(input_data_dir + "train_32x32.mat"):<br class="title-page-name"/>    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Training Set') as pbar:<br class="title-page-name"/>        urlretrieve(<br class="title-page-name"/>            'http://ufldl.stanford.edu/housenumbers/train_32x32.mat',<br class="title-page-name"/>            input_data_dir + 'train_32x32.mat',<br class="title-page-name"/>            pbar.hook)<br class="title-page-name"/><br class="title-page-name"/>if not isfile(input_data_dir + "test_32x32.mat"):<br class="title-page-name"/>    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='SVHN Training Set') as pbar:<br class="title-page-name"/>        urlretrieve(<br class="title-page-name"/>            'http://ufldl.stanford.edu/housenumbers/test_32x32.mat',<br class="title-page-name"/>            input_data_dir + 'test_32x32.mat',<br class="title-page-name"/>            pbar.hook)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>train_data = loadmat(input_data_dir + 'train_32x32.mat')<br class="title-page-name"/>test_data = loadmat(input_data_dir + 'test_32x32.mat')</pre>
<pre class="calibre21">Output:</pre>
<pre class="calibre21">trainset shape: (32, 32, 3, 73257)<br class="title-page-name"/>testset shape: (32, 32, 3, 26032)</pre>
<p class="calibre2">Let's get a sense of what these images look like:</p>
<pre class="calibre21">indices = np.random.randint(0, train_data['X'].shape[3], size=36)<br class="title-page-name"/>fig, axes = plt.subplots(6, 6, sharex=True, sharey=True, figsize=(5,5),)<br class="title-page-name"/>for ii, ax in zip(indices, axes.flatten()):<br class="title-page-name"/>    ax.imshow(train_data['X'][:,:,:,ii], aspect='equal')<br class="title-page-name"/>    ax.xaxis.set_visible(False)<br class="title-page-name"/>    ax.yaxis.set_visible(False)<br class="title-page-name"/>plt.subplots_adjust(wspace=0, hspace=0)</pre>
<pre class="calibre21">Output:</pre>
<div class="CDPAlignCenter"><img src="assets/a75e45c4-fe45-4bf0-b1a6-52d1ddbdeef6.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 7: Sample images from the SVHN dataset.</div>
<div class="title-page-name">
<p class="calibre2">Next up, we need to scale our images to be between -1 and 1, and this will be necessary since we are going to use the <kbd class="calibre12">tanh()</kbd> function, which will squash the output values of the generator:</p>
<pre class="calibre21"># Scaling the input images<br class="title-page-name"/>def scale_images(image, feature_range=(-1, 1)):<br class="title-page-name"/>    # scale image to (0, 1)<br class="title-page-name"/>    image = ((image - image.min()) / (255 - image.min()))<br class="title-page-name"/><br class="title-page-name"/>    # scale the image to feature range<br class="title-page-name"/>    min, max = feature_range<br class="title-page-name"/>    image = image * (max - min) + min<br class="title-page-name"/>    return image</pre>
<pre class="calibre21">class Dataset:<br class="title-page-name"/>    def __init__(self, train_set, test_set, validation_frac=0.5, shuffle_data=True, scale_func=None):<br class="title-page-name"/>        split_ind = int(len(test_set['y']) * (1 - validation_frac))<br class="title-page-name"/>        self.test_input, self.valid_input = test_set['X'][:, :, :, :split_ind], test_set['X'][:, :, :, split_ind:]<br class="title-page-name"/>        self.test_target, self.valid_target = test_set['y'][:split_ind], test_set['y'][split_ind:]<br class="title-page-name"/>        self.train_input, self.train_target = train_set['X'], train_set['y']<br class="title-page-name"/><br class="title-page-name"/>        # The street house number dataset comes with lots of labels,<br class="title-page-name"/>        # but because we are going to do semi-supervised learning we are going to assume that we don't have all labels<br class="title-page-name"/>        # like, assume that we have only 1000<br class="title-page-name"/>        self.label_mask = np.zeros_like(self.train_target)<br class="title-page-name"/>        self.label_mask[0:1000] = 1<br class="title-page-name"/><br class="title-page-name"/>        self.train_input = np.rollaxis(self.train_input, 3)<br class="title-page-name"/>        self.valid_input = np.rollaxis(self.valid_input, 3)<br class="title-page-name"/>        self.test_input = np.rollaxis(self.test_input, 3)<br class="title-page-name"/><br class="title-page-name"/>        if scale_func is None:<br class="title-page-name"/>            self.scaler = scale_images<br class="title-page-name"/>        else:<br class="title-page-name"/>            self.scaler = scale_func<br class="title-page-name"/>        self.train_input = self.scaler(self.train_input)<br class="title-page-name"/>        self.valid_input = self.scaler(self.valid_input)<br class="title-page-name"/>        self.test_input = self.scaler(self.test_input)<br class="title-page-name"/>        self.shuffle = shuffle_data<br class="title-page-name"/><br class="title-page-name"/>    def batches(self, batch_size, which_set="train"):<br class="title-page-name"/>        input_name = which_set + "_input"<br class="title-page-name"/>        target_name = which_set + "_target"<br class="title-page-name"/><br class="title-page-name"/>        num_samples = len(getattr(dataset, target_name))<br class="title-page-name"/>        if self.shuffle:<br class="title-page-name"/>            indices = np.arange(num_samples)<br class="title-page-name"/>            np.random.shuffle(indices)<br class="title-page-name"/>            setattr(dataset, input_name, getattr(dataset, input_name)[indices])<br class="title-page-name"/>            setattr(dataset, target_name, getattr(dataset, target_name)[indices])<br class="title-page-name"/>            if which_set == "train":<br class="title-page-name"/>                dataset.label_mask = dataset.label_mask[indices]<br class="title-page-name"/><br class="title-page-name"/>        dataset_input = getattr(dataset, input_name)<br class="title-page-name"/>        dataset_target = getattr(dataset, target_name)<br class="title-page-name"/><br class="title-page-name"/>        for jj in range(0, num_samples, batch_size):<br class="title-page-name"/>            input_vals = dataset_input[jj:jj + batch_size]<br class="title-page-name"/>            target_vals = dataset_target[jj:jj + batch_size]<br class="title-page-name"/><br class="title-page-name"/>            if which_set == "train":<br class="title-page-name"/>                # including the label mask in case of training<br class="title-page-name"/>                # to pretend that we don't have all the labels<br class="title-page-name"/>                yield input_vals, target_vals, self.label_mask[jj:jj + batch_size]<br class="title-page-name"/>            else:<br class="title-page-name"/>                yield input_vals, target_vals</pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we will build all the bits and pieces that are necessary for our test, so let's start off by defining the inputs that will be used to feed data to the computational graph.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model inputs</h1>
                </header>
            
            <article>
                
<p class="calibre2">First off, we are going to define the model inputs function, which will create the model input placeholders to be used for feeding data to the computational model:</p>
<pre class="calibre21"># defining the model inputs<br class="title-page-name"/>def inputs(actual_dim, z_dim):<br class="title-page-name"/>    inputs_actual = tf.placeholder(tf.float32, (None, *actual_dim), name='input_actual')<br class="title-page-name"/>    inputs_latent_z = tf.placeholder(tf.float32, (None, z_dim), name='input_latent_z')<br class="title-page-name"/><br class="title-page-name"/>    target = tf.placeholder(tf.int32, (None), name='target')<br class="title-page-name"/>    label_mask = tf.placeholder(tf.int32, (None), name='label_mask')<br class="title-page-name"/><br class="title-page-name"/>    return inputs_actual, inputs_latent_z, target, label_mask</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generator</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to implement the first core part of the GAN network. The architecture and implementation of this part will follow the original DCGAN paper:</p>
<pre class="calibre21"><span>def </span><span>generator</span>(latent_z<span>, </span>output_image_dim<span>, </span>reuse_vars=<span>False</span><span>, </span>leaky_alpha=<span>0.2</span><span>, </span>is_training=<span>True</span><span>, </span>size_mult=<span>128</span>):<br class="title-page-name"/>    <span>with </span>tf.variable_scope(<span>'generator'</span><span>, </span><span>reuse</span>=reuse_vars):<br class="title-page-name"/>        <span># define a fully connected layer<br class="title-page-name"/></span><span>        </span>fully_conntected_1 = tf.layers.dense(latent_z<span>, </span><span>4 </span>* <span>4 </span>* size_mult * <span>4</span>)<br class="title-page-name"/><br class="title-page-name"/>        <span># Reshape it from 2D tensor to 4D tensor to be fed to the convolution neural network<br class="title-page-name"/></span><span>        </span>reshaped_out_1 = tf.reshape(fully_conntected_1<span>, </span>(-<span>1</span><span>, </span><span>4</span><span>, </span><span>4</span><span>, </span>size_mult * <span>4</span>))<br class="title-page-name"/>        batch_normalization_1 = tf.layers.batch_normalization(reshaped_out_1<span>, </span><span>training</span>=is_training)<br class="title-page-name"/>        leaky_output_1 = tf.maximum(leaky_alpha * batch_normalization_1<span>, </span>batch_normalization_1)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_1 = tf.layers.conv2d_transpose(leaky_output_1<span>, </span>size_mult * <span>2</span><span>, </span><span>5</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_2 = tf.layers.batch_normalization(conv_layer_1<span>, </span><span>training</span>=is_training)<br class="title-page-name"/>        leaky_output_2 = tf.maximum(leaky_alpha * batch_normalization_2<span>, </span>batch_normalization_2)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_2 = tf.layers.conv2d_transpose(leaky_output_2<span>, </span>size_mult<span>, </span><span>5</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_3 = tf.layers.batch_normalization(conv_layer_2<span>, </span><span>training</span>=is_training)<br class="title-page-name"/>        leaky_output_3 = tf.maximum(leaky_alpha * batch_normalization_3<span>, </span>batch_normalization_3)<br class="title-page-name"/><br class="title-page-name"/>        <span># defining the output layer<br class="title-page-name"/></span><span>        </span>logits_layer = tf.layers.conv2d_transpose(leaky_output_3<span>, </span>output_image_dim<span>, </span><span>5</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/><br class="title-page-name"/>        output = tf.tanh(logits_layer)<br class="title-page-name"/><br class="title-page-name"/>        <span>return </span>output</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discriminator</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, it's time to build the second core piece of the GAN network, which is the discriminator. In previous implementations, we said that the discriminator will produce a binary output that represents whether the input image is from the real dataset (1) or it's generated by the generator (0). The scenario is different here, so the discriminator will now be a multi-class classifier.</p>
<p class="calibre2">Now, let's go ahead and build up the discriminator part of the architecture:</p>
<pre class="calibre21"><span># Defining the discriminator part of the network<br class="title-page-name"/></span><span>def </span><span>discriminator</span>(input_x<span>, </span>reuse_vars=<span>False</span><span>, </span>leaky_alpha=<span>0.2</span><span>, </span>drop_out_rate=<span>0.</span><span>, </span>num_classes=<span>10</span><span>, </span>size_mult=<span>64</span>):<br class="title-page-name"/>    <span>with </span>tf.variable_scope(<span>'discriminator'</span><span>, </span><span>reuse</span>=reuse_vars):<br class="title-page-name"/><br class="title-page-name"/>        <span># defining a dropout layer<br class="title-page-name"/></span><span>        drop_out_output </span>= tf.layers.dropout(input_x<span>, </span><span>rate</span>=drop_out_rate / <span>2.5</span>)<br class="title-page-name"/><br class="title-page-name"/>        <span># Defining the input layer for the discriminator which is 32x32x3<br class="title-page-name"/></span><span>        </span>conv_layer_3 = tf.layers.conv2d(input_x<span>, </span>size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        leaky_output_4 = tf.maximum(leaky_alpha * conv_layer_3<span>, </span>conv_layer_3)<br class="title-page-name"/>        leaky_output_4 = tf.layers.dropout(leaky_output_4<span>, </span><span>rate</span>=drop_out_rate)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_4 = tf.layers.conv2d(leaky_output_4<span>, </span>size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_4 = tf.layers.batch_normalization(conv_layer_4<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_5 = tf.maximum(leaky_alpha * batch_normalization_4<span>, </span>batch_normalization_4)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_5 = tf.layers.conv2d(leaky_output_5<span>, </span>size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_5 = tf.layers.batch_normalization(conv_layer_5<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_6 = tf.maximum(leaky_alpha * batch_normalization_5<span>, </span>batch_normalization_5)<br class="title-page-name"/>        leaky_output_6 = tf.layers.dropout(leaky_output_6<span>, </span><span>rate</span>=drop_out_rate)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_6 = tf.layers.conv2d(leaky_output_6<span>, </span><span>2 </span>* size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>1</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_6 = tf.layers.batch_normalization(conv_layer_6<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_7 = tf.maximum(leaky_alpha * batch_normalization_6<span>, </span>batch_normalization_6)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_7 = tf.layers.conv2d(leaky_output_7<span>, </span><span>2 </span>* size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>1</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_7 = tf.layers.batch_normalization(conv_layer_7<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_8 = tf.maximum(leaky_alpha * batch_normalization_7<span>, </span>batch_normalization_7)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_8 = tf.layers.conv2d(leaky_output_8<span>, </span><span>2 </span>* size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_8 = tf.layers.batch_normalization(conv_layer_8<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_9 = tf.maximum(leaky_alpha * batch_normalization_8<span>, </span>batch_normalization_8)<br class="title-page-name"/>        leaky_output_9 = tf.layers.dropout(leaky_output_9<span>, </span><span>rate</span>=drop_out_rate)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_9 = tf.layers.conv2d(leaky_output_9<span>, </span><span>2 </span>* size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>1</span><span>, </span><span>padding</span>=<span>'valid'</span>)<br class="title-page-name"/><br class="title-page-name"/>        leaky_output_10 = tf.maximum(leaky_alpha * conv_layer_9<span>, </span>conv_layer_9)</pre>
<pre class="calibre21">...</pre>
<p class="calibre2">Instead of applying a fully connected layer at the end, we are going to perform so-called <strong class="calibre13">global average pooling</strong> (<strong class="calibre13">GAP</strong>), which takes the average over the spatial dimensions of a feature vector; this will produce a squashed tensor to only a single value:</p>
<pre class="calibre21">...</pre>
<pre class="calibre21"># Flatten it by global average pooling<br class="title-page-name"/>leaky_output_features = tf.reduce_mean(leaky_output_10, (1, 2))</pre>
<pre class="calibre21">...</pre>
<p class="calibre2">For example, suppose that after a stack of convolutions, we get an output tensor of shape:</p>
<pre class="calibre21">[BATCH_SIZE, 8, 8, NUM_CHANNELS] </pre>
<p class="calibre2">To apply global average pooling, we calculate the average value on the [8x8] tensor slice. This operation will result in a tensor which is the following shape:</p>
<pre class="calibre21"> [BATCH_SIZE, 1, 1, NUM_CHANNELS] </pre>
<p class="calibre2">That can be reshaped to:</p>
<pre class="calibre21">[BATCH_SIZE, NUM_CHANNELS].</pre>
<p class="calibre2">After applying the global average pooling, we add a fully connected layer that will output the final logits. These have the shape of:</p>
<p class="calibre2"><kbd class="calibre12">[BATCH_SIZE, NUM_CLASSES]</kbd></p>
<p class="calibre2">which will represent the scores for each class. To get these scores for probability, we are going to use the <kbd class="calibre12">softmax</kbd> activation function:</p>
<pre class="calibre21">...<br class="title-page-name"/># Get the probability that the input is real rather than fake<br class="title-page-name"/>softmax_output = tf.nn.softmax(classes_logits)s<br class="title-page-name"/>...</pre>
<p class="calibre2">And finally the discriminator function will look like this,</p>
<pre class="calibre21"><span># Defining the discriminator part of the network<br class="title-page-name"/></span><span>def </span><span>discriminator</span>(input_x<span>, </span>reuse_vars=<span>False</span><span>, </span>leaky_alpha=<span>0.2</span><span>, </span>drop_out_rate=<span>0.</span><span>, </span>num_classes=<span>10</span><span>, </span>size_mult=<span>64</span>):<br class="title-page-name"/>    <span>with </span>tf.variable_scope(<span>'discriminator'</span><span>, </span><span>reuse</span>=reuse_vars):<br class="title-page-name"/><br class="title-page-name"/>        <span># defining a dropout layer<br class="title-page-name"/></span><span>        drop_out_output </span>= tf.layers.dropout(input_x<span>, </span><span>rate</span>=drop_out_rate / <span>2.5</span>)<br class="title-page-name"/><br class="title-page-name"/>        <span># Defining the input layer for the discrminator which is 32x32x3<br class="title-page-name"/></span><span>        </span>conv_layer_3 = tf.layers.conv2d(input_x<span>, </span>size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        leaky_output_4 = tf.maximum(leaky_alpha * conv_layer_3<span>, </span>conv_layer_3)<br class="title-page-name"/>        leaky_output_4 = tf.layers.dropout(leaky_output_4<span>, </span><span>rate</span>=drop_out_rate)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_4 = tf.layers.conv2d(leaky_output_4<span>, </span>size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_4 = tf.layers.batch_normalization(conv_layer_4<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_5 = tf.maximum(leaky_alpha * batch_normalization_4<span>, </span>batch_normalization_4)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_5 = tf.layers.conv2d(leaky_output_5<span>, </span>size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_5 = tf.layers.batch_normalization(conv_layer_5<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_6 = tf.maximum(leaky_alpha * batch_normalization_5<span>, </span>batch_normalization_5)<br class="title-page-name"/>        leaky_output_6 = tf.layers.dropout(leaky_output_6<span>, </span><span>rate</span>=drop_out_rate)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_6 = tf.layers.conv2d(leaky_output_6<span>, </span><span>2 </span>* size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>1</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_6 = tf.layers.batch_normalization(conv_layer_6<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_7 = tf.maximum(leaky_alpha * batch_normalization_6<span>, </span>batch_normalization_6)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_7 = tf.layers.conv2d(leaky_output_7<span>, </span><span>2 </span>* size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>1</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_7 = tf.layers.batch_normalization(conv_layer_7<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_8 = tf.maximum(leaky_alpha * batch_normalization_7<span>, </span>batch_normalization_7)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_8 = tf.layers.conv2d(leaky_output_8<span>, </span><span>2 </span>* size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>2</span><span>, </span><span>padding</span>=<span>'same'</span>)<br class="title-page-name"/>        batch_normalization_8 = tf.layers.batch_normalization(conv_layer_8<span>, </span><span>training</span>=<span>True</span>)<br class="title-page-name"/>        leaky_output_9 = tf.maximum(leaky_alpha * batch_normalization_8<span>, </span>batch_normalization_8)<br class="title-page-name"/>        leaky_output_9 = tf.layers.dropout(leaky_output_9<span>, </span><span>rate</span>=drop_out_rate)<br class="title-page-name"/><br class="title-page-name"/>        conv_layer_9 = tf.layers.conv2d(leaky_output_9<span>, </span><span>2 </span>* size_mult<span>, </span><span>3</span><span>, </span><span>strides</span>=<span>1</span><span>, </span><span>padding</span>=<span>'valid'</span>)<br class="title-page-name"/><br class="title-page-name"/>        leaky_output_10 = tf.maximum(leaky_alpha * conv_layer_9<span>, </span>conv_layer_9)<br class="title-page-name"/><br class="title-page-name"/>        <span># Flatten it by global average pooling<br class="title-page-name"/></span><span>        </span>leaky_output_features = tf.reduce_mean(leaky_output_10<span>, </span>(<span>1</span><span>, </span><span>2</span>))<br class="title-page-name"/><br class="title-page-name"/>        <span># Set class_logits to be the inputs to a softmax distribution over the different classes<br class="title-page-name"/></span><span>        </span>classes_logits = tf.layers.dense(leaky_output_features<span>, </span>num_classes + extra_class)<br class="title-page-name"/><br class="title-page-name"/>        <span>if </span>extra_class:<br class="title-page-name"/>            actual_class_logits<span>, </span>fake_class_logits = tf.split(classes_logits<span>, </span>[num_classes<span>, </span><span>1</span>]<span>, </span><span>1</span>)<br class="title-page-name"/>            <span>assert </span>fake_class_logits.get_shape()[<span>1</span>] == <span>1</span><span>, </span>fake_class_logits.get_shape()<br class="title-page-name"/>            fake_class_logits = tf.squeeze(fake_class_logits)<br class="title-page-name"/>        <span>else</span>:<br class="title-page-name"/>            actual_class_logits = classes_logits<br class="title-page-name"/>            fake_class_logits = <span>0.<br class="title-page-name"/></span><span><br class="title-page-name"/></span><span>        </span>max_reduced = tf.reduce_max(actual_class_logits<span>, </span><span>1</span><span>, </span><span>keep_dims</span>=<span>True</span>)<br class="title-page-name"/>        stable_actual_class_logits = actual_class_logits - max_reduced<br class="title-page-name"/><br class="title-page-name"/>        gan_logits = tf.log(tf.reduce_sum(tf.exp(stable_actual_class_logits)<span>, </span><span>1</span>)) + tf.squeeze(<br class="title-page-name"/>            max_reduced) - fake_class_logits<br class="title-page-name"/><br class="title-page-name"/>        softmax_output = tf.nn.softmax(classes_logits)<br class="title-page-name"/><br class="title-page-name"/>        <span>return </span>softmax_output<span>, </span>classes_logits<span>, </span>gan_logits<span>, </span>leaky_output_features</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model losses</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now it's time to define the model losses. First off, the discriminator loss will be divided into two parts:</p>
<ul class="calibre7">
<li class="calibre8">One which will represent the GAN problem, which is the unsupervised loss</li>
<li class="calibre8">The second one will compute the individual actual class probabilities, which is the supervised loss</li>
</ul>
<p class="calibre2">For the discriminator's unsupervised loss, it has to discriminate between actual training images and the generated images by the generator.</p>
<p class="calibre2">As for a regular GAN, half of the time, the discriminator will get unlabeled images from the training set as an input and the other half, fake, unlabeled images from the generator.</p>
<p class="calibre2"> </p>
<p class="calibre2">For the second part of the discriminator loss, which is the supervised loss, we need to build upon the logits from the discriminator. So, we will use the softmax cross entropy since it's a multi classification problem.</p>
<p class="calibre2">As mentioned in the <em class="calibre19">Enhanced Techniques for Training GANs</em> paper, we should use feature matching for the generator loss. <span class="calibre10">As the authors describe: </span></p>
<div class="packtquote">"Feature matching is the concept of penalizing the mean absolute error between the average value of some set of features on the training data and the average values of that set of features on the generated samples. To do that, we take some set of statistics (the moments) from two different sources and force them to be similar. First, we take the average of the features extracted from the discriminator when a real training minibatch is being processed. Second, we compute the moments in the same way, but now for when a minibatch composed of fake images that come from the generator was being analyzed by the discriminator. Finally, with these two sets of moments, the generator loss is the mean absolute difference between them. In other words, as the paper emphasizes: We train the generator to match the expected values of the features on an intermediate layer of the discriminator."</div>
<p class="calibre2"> </p>
<p class="calibre2">And finally, the model loss function will look like this:</p>
<pre class="calibre21">def model_losses(input_actual, input_latent_z, output_dim, target, num_classes, label_mask, leaky_alpha=0.2,<br class="title-page-name"/>                     drop_out_rate=0.):<br class="title-page-name"/><br class="title-page-name"/>        # These numbers multiply the size of each layer of the generator and the discriminator,<br class="title-page-name"/>        # respectively. You can reduce them to run your code faster for debugging purposes.<br class="title-page-name"/>        gen_size_mult = 32<br class="title-page-name"/>        disc_size_mult = 64<br class="title-page-name"/><br class="title-page-name"/>        # Here we run the generator and the discriminator<br class="title-page-name"/>        gen_model = generator(input_latent_z, output_dim, leaky_alpha=leaky_alpha, size_mult=gen_size_mult)<br class="title-page-name"/>        disc_on_data = discriminator(input_actual, leaky_alpha=leaky_alpha, drop_out_rate=drop_out_rate,<br class="title-page-name"/>                                     size_mult=disc_size_mult)<br class="title-page-name"/>        disc_model_real, class_logits_on_data, gan_logits_on_data, data_features = disc_on_data<br class="title-page-name"/>        disc_on_samples = discriminator(gen_model, reuse_vars=True, leaky_alpha=leaky_alpha,<br class="title-page-name"/>                                        drop_out_rate=drop_out_rate, size_mult=disc_size_mult)<br class="title-page-name"/>        disc_model_fake, class_logits_on_samples, gan_logits_on_samples, sample_features = disc_on_samples<br class="title-page-name"/><br class="title-page-name"/>        # Here we compute `disc_loss`, the loss for the discriminator.<br class="title-page-name"/>        disc_loss_actual = tf.reduce_mean(<br class="title-page-name"/>            tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits_on_data,<br class="title-page-name"/>                                                    labels=tf.ones_like(gan_logits_on_data)))<br class="title-page-name"/>        disc_loss_fake = tf.reduce_mean(<br class="title-page-name"/>            tf.nn.sigmoid_cross_entropy_with_logits(logits=gan_logits_on_samples,<br class="title-page-name"/>                                                    labels=tf.zeros_like(gan_logits_on_samples)))<br class="title-page-name"/>        target = tf.squeeze(target)<br class="title-page-name"/>        classes_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=class_logits_on_data,<br class="title-page-name"/>                                                                        labels=tf.one_hot(target,<br class="title-page-name"/>                                                                                          num_classes + extra_class,<br class="title-page-name"/>                                                                                          dtype=tf.float32))<br class="title-page-name"/>        classes_cross_entropy = tf.squeeze(classes_cross_entropy)<br class="title-page-name"/>        label_m = tf.squeeze(tf.to_float(label_mask))<br class="title-page-name"/>        disc_loss_class = tf.reduce_sum(label_m * classes_cross_entropy) / tf.maximum(1., tf.reduce_sum(label_m))<br class="title-page-name"/>        disc_loss = disc_loss_class + disc_loss_actual + disc_loss_fake<br class="title-page-name"/><br class="title-page-name"/>        # Here we set `gen_loss` to the "feature matching" loss invented by Tim Salimans.<br class="title-page-name"/>        sampleMoments = tf.reduce_mean(sample_features, axis=0)<br class="title-page-name"/>        dataMoments = tf.reduce_mean(data_features, axis=0)<br class="title-page-name"/><br class="title-page-name"/>        gen_loss = tf.reduce_mean(tf.abs(dataMoments - sampleMoments))<br class="title-page-name"/><br class="title-page-name"/>        prediction_class = tf.cast(tf.argmax(class_logits_on_data, 1), tf.int32)<br class="title-page-name"/>        check_prediction = tf.equal(tf.squeeze(target), prediction_class)<br class="title-page-name"/>        correct = tf.reduce_sum(tf.to_float(check_prediction))<br class="title-page-name"/>        masked_correct = tf.reduce_sum(label_m * tf.to_float(check_prediction))<br class="title-page-name"/><br class="title-page-name"/>        return disc_loss, gen_loss, correct, masked_correct, gen_model</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model optimizer</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now, let's define the model optimizer, which is pretty much similar to the ones that we defined before:</p>
<pre class="calibre21">def model_optimizer(disc_loss, gen_loss, learning_rate, beta1):<br class="title-page-name"/><br class="title-page-name"/>        # Get weights and biases to update. Get them separately for the discriminator and the generator<br class="title-page-name"/>        trainable_vars = tf.trainable_variables()<br class="title-page-name"/>        disc_vars = [var for var in trainable_vars if var.name.startswith('discriminator')]<br class="title-page-name"/>        gen_vars = [var for var in trainable_vars if var.name.startswith('generator')]<br class="title-page-name"/>        for t in trainable_vars:<br class="title-page-name"/>            assert t in disc_vars or t in gen_vars<br class="title-page-name"/><br class="title-page-name"/>        # Minimize both gen and disc costs simultaneously<br class="title-page-name"/>        disc_train_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(disc_loss,<br class="title-page-name"/>                                                                                           var_list=disc_vars)<br class="title-page-name"/>        gen_train_optimizer = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(gen_loss, var_list=gen_vars)<br class="title-page-name"/>        shrink_learning_rate = tf.assign(learning_rate, learning_rate * 0.9)<br class="title-page-name"/><br class="title-page-name"/>        return disc_train_optimizer, gen_train_optimizer, shrink_learning_rate</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training</h1>
                </header>
            
            <article>
                
<p class="calibre2">Finally, let's go ahead and kick off the training process after putting it all together:</p>
<pre class="calibre21"><span>class </span><span>GAN</span>:<br class="title-page-name"/>        <span>def </span><span>__init__</span>(<span>self</span><span>, </span>real_size<span>, </span>z_size<span>, </span>learning_rate<span>, </span>num_classes=<span>10</span><span>, </span><span>alpha=0.2</span><span>, </span>beta1=<span>0.5</span>):<br class="title-page-name"/>            tf.reset_default_graph()<br class="title-page-name"/><br class="title-page-name"/>            <span>self</span>.learning_rate = tf.Variable(learning_rate<span>, </span><span>trainable</span>=<span>False</span>)<br class="title-page-name"/>            model_inputs = inputs(real_size<span>, </span>z_size)<br class="title-page-name"/>            <span>self</span>.input_actual<span>, </span><span>self</span>.input_latent_z<span>, </span><span>self</span>.target<span>, </span><span>self</span>.label_mask = model_inputs<br class="title-page-name"/>            <span>self</span>.drop_out_rate = tf.placeholder_with_default(<span>.5</span><span>, </span>()<span>, </span><span>"drop_out_rate"</span>)<br class="title-page-name"/><br class="title-page-name"/>            losses_results = model_losses(<span>self</span>.input_actual<span>, </span><span>self</span>.input_latent_z<span>,<br class="title-page-name"/></span><span>                                          </span>real_size[<span>2</span>]<span>, </span><span>self</span>.target<span>, </span>num_classes<span>,<br class="title-page-name"/></span><span>                                          </span><span>label_mask</span>=<span>self</span>.label_mask<span>,<br class="title-page-name"/></span><span>                                          </span><span>leaky_alpha</span>=<span>0.2</span><span>,<br class="title-page-name"/></span><span>                                          </span><span>drop_out_rate</span>=<span>self</span>.drop_out_rate)<br class="title-page-name"/>            <span>self</span>.disc_loss<span>, </span><span>self</span>.gen_loss<span>, </span><span>self</span>.correct<span>, </span><span>self</span>.masked_correct<span>, </span><span>self</span>.samples = losses_results<br class="title-page-name"/><br class="title-page-name"/>            <span>self</span>.disc_opt<span>, </span><span>self</span>.gen_opt<span>, </span><span>self</span>.shrink_learning_rate = model_optimizer(<span>self</span>.disc_loss<span>, </span><span>self</span>.gen_loss<span>,<br class="title-page-name"/></span><span>                                                                                     </span><span>self</span>.learning_rate<span>, </span>beta1)</pre>
<pre class="calibre21"><span>def </span><span>view_generated_samples</span>(epoch<span>, </span>samples<span>, </span>nrows<span>, </span>ncols<span>, </span>figsize=(<span>5</span><span>, </span><span>5</span>)):<br class="title-page-name"/>        fig<span>, </span>axes = plt.subplots(<span>figsize</span>=figsize<span>, </span><span>nrows</span>=nrows<span>, </span><span>ncols</span>=ncols<span>,<br class="title-page-name"/></span><span>                                 </span><span>sharey</span>=<span>True</span><span>, </span><span>sharex</span>=<span>True</span>)<br class="title-page-name"/>        <span>for </span>ax<span>, </span>img <span>in </span><span>zip</span>(axes.flatten()<span>, </span>samples[epoch]):<br class="title-page-name"/>            ax.axis(<span>'off'</span>)<br class="title-page-name"/>            img = ((img - img.min()) * <span>255 </span>/ (img.max() - img.min())).astype(np.uint8)<br class="title-page-name"/>            ax.set_adjustable(<span>'box-forced'</span>)<br class="title-page-name"/>            <span>im </span>= ax.imshow(img)<br class="title-page-name"/><br class="title-page-name"/>        plt.subplots_adjust(<span>wspace</span>=<span>0</span><span>, </span><span>hspace</span>=<span>0</span>)<br class="title-page-name"/>        <span>return </span>fig<span>, </span>axes</pre>
<pre class="calibre21"><span>def train(net, dataset, epochs, batch_size, figsize=(5, 5)):<br class="title-page-name"/><br class="title-page-name"/>        saver = tf.train.Saver()<br class="title-page-name"/>        sample_z = np.random.normal(0, 1, size=(50, latent_space_z_size))<br class="title-page-name"/><br class="title-page-name"/>        samples, train_accuracies, test_accuracies = [], [], []<br class="title-page-name"/>        steps = 0<br class="title-page-name"/><br class="title-page-name"/>        with tf.Session() as sess:<br class="title-page-name"/>            sess.run(tf.global_variables_initializer())<br class="title-page-name"/>            for e in range(epochs):<br class="title-page-name"/>                print("Epoch", e)<br class="title-page-name"/><br class="title-page-name"/>                num_samples = 0<br class="title-page-name"/>                num_correct_samples = 0<br class="title-page-name"/>                for x, y, label_mask in dataset.batches(batch_size):<br class="title-page-name"/>                    assert 'int' in str(y.dtype)<br class="title-page-name"/>                    steps += 1<br class="title-page-name"/>                    num_samples += label_mask.sum()<br class="title-page-name"/><br class="title-page-name"/>                    # Sample random noise for G<br class="title-page-name"/>                    batch_z = np.random.normal(0, 1, size=(batch_size, latent_space_z_size))<br class="title-page-name"/><br class="title-page-name"/>                    _, _, correct = sess.run([net.disc_opt, net.gen_opt, net.masked_correct],<br class="title-page-name"/>                                             feed_dict={net.input_actual: x, net.input_latent_z: batch_z,<br class="title-page-name"/>                                                        net.target: y, net.label_mask: label_mask})<br class="title-page-name"/>                    num_correct_samples += correct<br class="title-page-name"/><br class="title-page-name"/>                sess.run([net.shrink_learning_rate])<br class="title-page-name"/><br class="title-page-name"/>                training_accuracy = num_correct_samples / float(num_samples)<br class="title-page-name"/><br class="title-page-name"/>                print("\t\tClassifier train accuracy: ", training_accuracy)<br class="title-page-name"/><br class="title-page-name"/>                num_samples = 0<br class="title-page-name"/>                num_correct_samples = 0<br class="title-page-name"/><br class="title-page-name"/>                for x, y in dataset.batches(batch_size, which_set="test"):<br class="title-page-name"/>                    assert 'int' in str(y.dtype)<br class="title-page-name"/>                    num_samples += x.shape[0]<br class="title-page-name"/><br class="title-page-name"/>                    correct, = sess.run([net.correct], feed_dict={net.input_real: x,<br class="title-page-name"/>                                                                  net.y: y,<br class="title-page-name"/>                                                                  net.drop_rate: 0.})<br class="title-page-name"/>                    num_correct_samples += correct<br class="title-page-name"/><br class="title-page-name"/>                testing_accuracy = num_correct_samples / float(num_samples)<br class="title-page-name"/>                print("\t\tClassifier test accuracy", testing_accuracy)<br class="title-page-name"/><br class="title-page-name"/>                gen_samples = sess.run(<br class="title-page-name"/>                    net.samples,<br class="title-page-name"/>                    feed_dict={net.input_latent_z: sample_z})<br class="title-page-name"/>                samples.append(gen_samples)<br class="title-page-name"/>                _ = view_generated_samples(-1, samples, 5, 10, figsize=figsize)<br class="title-page-name"/>                plt.show()<br class="title-page-name"/><br class="title-page-name"/>                # Save history of accuracies to view after training<br class="title-page-name"/>                train_accuracies.append(training_accuracy)<br class="title-page-name"/>                test_accuracies.append(testing_accuracy)<br class="title-page-name"/><br class="title-page-name"/>            saver.save(sess, './checkpoints/generator.ckpt')<br class="title-page-name"/><br class="title-page-name"/>        with open('samples.pkl', 'wb') as f:<br class="title-page-name"/>            pkl.dump(samples, f)<br class="title-page-name"/><br class="title-page-name"/>        return train_accuracies, test_accuracies, samples</span></pre>
<p class="calibre2">Don't forget to create a directory called checkpoints:</p>
<pre class="calibre21">real_size = (32,32,3)<br class="title-page-name"/>latent_space_z_size = 100<br class="title-page-name"/>learning_rate = 0.0003<br class="title-page-name"/><br class="title-page-name"/>net = GAN(real_size, latent_space_z_size, learning_rate)</pre>
<pre class="calibre21"><span>dataset</span> = Dataset(train_data<span>, </span>test_data)<br class="title-page-name"/><br class="title-page-name"/>train_batch_size = <span>128<br class="title-page-name"/></span>num_epochs = <span>25<br class="title-page-name"/></span>train_accuracies<span>, </span>test_accuracies<span>, </span>samples = train(net<span>,<br class="title-page-name"/></span><span>                                                   </span><span>dataset</span><span>,<br class="title-page-name"/></span><span>                                                   </span>num_epochs<span>,<br class="title-page-name"/></span><span>                                                   </span>train_batch_size<span>,<br class="title-page-name"/></span><span>                                                   </span><span>figsize</span>=(<span>10</span><span>,</span><span>5</span>))</pre>
<p class="calibre2">Finally, at <kbd class="calibre12">Epoch 24</kbd>, you should get something close to this:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">Epoch 24
                Classifier train accuracy:  0.937
                Classifier test accuracy 0.67401659496
                Step time:  0.03694915771484375
                Epoch time:  26.15842580795288</pre></div>
</div>
<div class="CDPAlignCenter"><img src="assets/16442734-420c-4f3e-ad7e-f233a4280e9a.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 8: Sample images created by the generator network using the feature matching loss</div>
<div class="title-page-name">
<div class="CDPAlignCenter">
<div class="highlight">
<pre class="calibre21"><span>fig</span><span>,</span> <span>ax</span> <span>=</span> <span>plt</span><span>.</span><span>subplots</span><span>()</span>
<span>plt</span><span>.</span><span>plot</span><span>(</span><span>train_accuracies</span><span>,</span> <span>label</span><span>=</span><span>'Train'</span><span>,</span> <span>alpha</span><span>=</span><span>0.5</span><span>)</span>
<span>plt</span><span>.</span><span>plot</span><span>(</span><span>test_accuracies</span><span>,</span> <span>label</span><span>=</span><span>'Test'</span><span>,</span> <span>alpha</span><span>=</span><span>0.5</span><span>)</span>
<span>plt</span><span>.</span><span>title</span><span>(</span><span>"Accuracy"</span><span>)</span>
<span>plt</span><span>.</span><span>legend</span><span>()</span></pre></div>
<img src="assets/4e8ab204-3482-4502-ad4e-2610c537b0e0.png" class="calibre31"/><br class="title-page-name"/>
<div class="packtfigref">Figure 9: Train versus Test accuracy over the training process</div>
</div>
</div>
<p class="calibre2">Although feature matching loss performs well on the task of semi-supervised learning, the images produced by the generator are not as good as the ones created in the previous chapter. But this implementation was mainly introduced to demonstrate how <span class="calibre10">we </span>can use GANs for semi-supervised learning setups.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">Finally, many researchers consider unsupervised learning as the missing link in general AI systems. To overcome these obstacles, attempts to solve established problems using less labeled data is key. In this scenario, GANs pose a real alternative for learning complicated tasks with less labeled samples. Yet, the performance gap between supervised and semi-supervised learning is still far from being equal. We can certainly expect this gap to become shorter as new approaches come into play.</p>
<p class="calibre2"/>


            </article>

            
        </section>
    </body></html>