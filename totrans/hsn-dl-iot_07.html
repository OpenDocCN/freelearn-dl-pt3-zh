<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Indoor Localization in IoT</h1>
                </header>
            
            <article>
                
<p>Many IoT applications, such as indoor navigation and location-aware marketing by retailers, smart homes, smart campuses, and hospitals, rely on indoor localization. The input data generated from such applications generally comes from numerous sources such as infrared, ultrasound, Wi-Fi, RFID, ultrawideband, Bluetooth, and so on. </p>
<p>The communication fingerprint of those devices and technologies, such as Wi-Fi fingerprinting data, can be analyzed using DL models to predict the location of the device or user in indoor environments. In this chapter, we will discuss how DL techniques can be used for indoor localization in IoT applications in general with a hands-on example. Furthermore, we will discuss some deployment settings for indoor localization services in IoT environments. The following topics will be briefly covered in this chapter:</p>
<ul>
<li>Introducing indoor localization in IoT applications</li>
<li><strong>Deep learning</strong> (<strong>DL</strong>) for indoor localization in IoT applications</li>
<li>Example – indoor localization in Wi-Fi fingerprinting</li>
<li>Different deployment options for DL-based indoor localization</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of indoor localization</h1>
                </header>
            
            <article>
                
<p><span>With the rapid development of mobile internet, <strong>Location-Based</strong></span> <strong>S</strong><span><strong>ervices</strong> (<strong>LBS</strong>) in large public indoor places</span> is <span>becoming increasingly</span> <span>popular. In such an indoor location, the</span> <span><span class="ILfuVd"><strong>Received Signal Strength Indicator</strong> (<strong>RSSI</strong>) is often used as an estimated measure of the power level that an IoT device is receiving from <strong>Wireless Access Points</strong> (<strong>WAPs</strong>). However, when the distance from the source is increased, the signal gets weaker and the wireless data rates get slower, leading to a lower overall data throughput.<br/></span></span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Techniques for indoor localization</h1>
                </header>
            
            <article>
                
<p><span>Several indoor localization technologies have been proposed to date based on measuring techniques such as ultrasound, infrared, image, light, magnetic field, and wireless signals. For example, <strong>Bluetooth low energy</strong> (<strong>BLE</strong>)-based indoor localization has been attracting increasing interest because it is low-cost, low-power consumption, and has ubiquitous availability on almost every mobile device. On the other hand, the Wi-Fi localiza</span><span>tion system is based on the <strong>Channel State Information</strong> (<strong>CSI</strong>)</span> <span>of Wi-Fi signals.</span></p>
<p><span>Lately, DL approaches have been proposed in which DL models are</span> <span>used to learn the fingerprint patterns</span> of <span>high-dimensional CSI signals. Although, each Wi-Fi scan contains the signal strength measurements for APs available in its vicinity, only a subset of a total number of networks in the environment are observed.</span></p>
<p><span>Also, since those devices are low-end with very small processing power,</span> <span>the</span> <span>unpredictable weakening or strengthening combination used in those approaches affect the multi-path signals, which will break the relationship between the RSSI and the transmission distance, and consequently prove to be less effective.</span> <span>On the other hand, the f</span><span>ingerprinting approach doesn't rely on the recovery of distances but instead uses the measured RSSIs as spatial patterns only. It is thus less vulnerable to the multi-path effect.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fingerprinting</h1>
                </header>
            
            <article>
                
<p><span>The</span> <span>fingerprinting approaches used commonly have</span> <span>two phases: an offline phase and an online phase.</span></p>
<p><span>One phase uses a fingerprint database to construct position-dependent parameters, which are extracted from measured RSSIs' reference locations, known as <strong>offline phases</strong>. In a localization phase, which is also known as an <strong>online phase</strong>, the mapping of RSSI measurements is done to a reference location using the most relevant RSSI fingerprint from the database, which can be explained as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b86e0d50-48d9-41ad-bcad-3f123875d7ae.png" style="width:16.58em;height:1.25em;"/></p>
<p class="mce-root"/>
<p><span>In the preceding equation,</span> <img class="fm-editor-equation" src="assets/bab27896-3a38-49f3-8ec0-1677e6fc1bb6.png" style="width:0.83em;height:0.83em;"/> <span>is the total number of reference locations in the</span> <span>database, </span><span><img class="fm-editor-equation" src="assets/242d15f7-deb3-407d-b33c-ca60da767fea.png" style="width:0.92em;height:1.17em;"/></span> <span>denotes the fingerprint pattern of the </span><span><img class="fm-editor-equation" src="assets/b68d16c9-827a-41a5-a8cf-380580a3429b.png" style="width:1.17em;height:1.08em;"/></span> <span>reference</span> <span>location, and </span><span><img class="fm-editor-equation" src="assets/449d1379-8d2b-43d3-aab5-ecca314065e9.png" style="width:1.25em;height:1.08em;"/></span> <span>is the spatial coordinates of that reference</span> <span>location.</span> <span>The fingerprint pattern,</span> <span><img class="fm-editor-equation" src="assets/ab5fac86-a4b7-43f4-92b9-8b545d15ff87.png" style="width:0.42em;height:0.83em;"/>, can be raw RSSI values from mul</span><span>tiple beacon stations, or any other feature vectors extracted</span> <span>from the RSSIs, which can be expressed as follows: </span> <span><br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d6372765-89d2-45b0-8de8-c747c53e1a14.png" style="width:9.42em;height:1.42em;"/></p>
<p><span>However, the</span> <span>raw RSSI values are used as spatial patterns in existing fingerprinting systems. In the preceding equation, <em>m</em></span> <span>is the total number of BLE beacon stations or Wi-Fi APs, and </span><span><img class="fm-editor-equation" src="assets/44ce7c44-7826-44b9-85f5-cf7b91f88441.png" style="width:1.08em;height:1.08em;"/></span> <span>represents the measured RSSI value of the </span><span><img class="fm-editor-equation" src="assets/01e38a2a-24f4-455d-9388-1dc08b082661.png" style="width:1.50em;height:1.50em;"/></span> <span>station.<br/></span></p>
<p>Now, we roughly know what indoor localization is. In the next section, we'll see how machine learning and DL algorithms can be used to develop such an indoor localization system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DL-based indoor localization for IoT</h1>
                </header>
            
            <article>
                
<p>Now, if we want to develop a DL application and deploy low-end devices, such IoT devices won't be able to process them. In particular, handling very high-dimensional data would be a bottleneck. So, an outdoor localization problem can be solved with reasonable accuracy using a machine learning algorithm such as <strong>k-nearest neighbors</strong> (<strong>k-NNs</strong>) because the inclusion of GPS sensors in mobile devices means we now have more data at hand.</p>
<p>However, indoor localization is still an open research problem, mainly due to the loss of GPS signals in indoor environments, despite advanced indoor positioning technologies. Fortunately, by using DL techniques, we can solve this problem with reasonable accuracy, especially since using <strong>Autoencoders</strong> (<strong>AEs</strong>) and their representation capabilities can be a pretty good workaround and a viable option. In such a setting, we have two options:</p>
<ol>
<li>Add a fully connected layer and a softmax layer in front of the AE network, which will act as an end-to-end classifier.</li>
</ol>
<ol start="2">
<li>Use any other classification algorithms, such as logistic regression, k-NN, Random Forest, or a support vector machine for the location estimation (that is, classification), as shown in the following diagram:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0c323e9d-9947-4845-b736-6e662bc90b1f.png" style="width:66.83em;height:40.50em;"/></p>
<p>The idea is to use AEs for the representation learning so that the network can learn the features well. Then, the output of the encoder part can be used to initialize the weight of the classifier part. In the following section, we will discuss k-NN and AEs and see how they can be used to solve the indoor localization problem.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">K-nearest neighbor (k-NN) classifier</h1>
                </header>
            
            <article>
                
<p>The <span>k-NN</span> algorithm is a non-parametric method that can be trained using the fingerprinting data coming from IoT devices. This tries to classify the collected RSSI values from the gateways to one of the reference points and not to the coordinates. The input consists of the k-closest RSSI values and the output would be a class membership. An input sample is then classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k-nearest neighbors.</p>
<p>Technically, if the fingerprinting database consists of (<em>X, y</em>)—with <em>X</em> being the RSSI values and <em>y</em> being the set of already known locations—then k-NN first computes the distance <img class="fm-editor-equation" src="assets/b6016adb-5765-47b1-8f24-9983adfe5dfb.png" style="width:6.25em;height:1.33em;"/>, where <em>x</em> is the unknown sample. Then, it computes a set, <img class="fm-editor-equation" src="assets/90cfbfa6-1377-48ef-af6e-d3b0bad19a33.png" style="width:0.50em;height:0.75em;"/>, containing indices for the <em>k</em> smallest distances from <img class="fm-editor-equation" src="assets/d6f38274-2e83-4fe5-8ee9-d2f0c4d3b6e7.png" style="width:0.67em;height:0.83em;"/>. Then, the majority label for <img class="fm-editor-equation" src="assets/18562ef7-7264-488e-b212-421a0c37a3e7.png" style="width:1.00em;height:1.00em;"/> is returned, where <img class="fm-editor-equation" src="assets/d3ee66fb-7bd8-4e35-b3f9-2e7feaf938a4.png" style="width:2.25em;height:0.92em;"/>. In other words, using k-NN, the classification is performed by computing the similarity between the observed data and records in the training RSSI samples in the database. Ultimately, the grid cell with the highest occurrence in the first <em>k</em> most similar records is the estimated location, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/e7c21824-f316-41f0-99ef-9b1ceca414e5.png" style="width:28.83em;height:28.00em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Localization of IoT enabled devices using k-NN algorithm<br/></div>
<p>In the preceding diagram, for k=4, the Wi-Fi packet trace is classified as being in the grid c (green triangles) record, while it is classified as being in grid a (red rectangle) when <em>k=6</em>. So, k-NN can be thought of as a lazy learning approach, where the function is only approximated locally and all computation is deferred until classification occurs. The good thing about the k-NN algorithm is that it is robust against noisy data. In particular, the inverse square of the weighted distance is used as the distance measure. Nevertheless, it performs well if it's already trained on a large amount of training data.</p>
<p>There are possible drawbacks as well. For example, we need to determine the <span><em>K</em> </span>parameter value, which is the number of nearest neighbors. It performs quite differently based on the distance measure used. The computation cost using the k-NN algorithm is quite high since it is required to compute the distance of each sample in the training data. This becomes even worse in the case of very high-dimensional data. In the next section, we will use k-NN as an end-to-end classifier rather than using a neural network setting to provide a comparative analysis between an AE-based classifier and k-NN classifiers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AE classifier</h1>
                </header>
            
            <article>
                
<p>As described in <a href="7626c72a-c3b8-4707-96a5-88d524d9f3f7.xhtml">Chapter 2</a>, <em>Deep Learning Architectures for IoT</em>, AEs are special types of neural networks that learn automatically from the input data. AEs consists of two components: an encoder and a decoder. An encoder compresses the input into a latent-space representation. Then, the decoder part, tries to reconstruct the original input data from that representation:</p>
<ul>
<li><strong>Encoder</strong>: Encodes or compresses the input into a latent-space representation using a function known as <img class="fm-editor-equation" src="assets/b5bf675d-1f97-4cdd-a9c5-c5de096126fd.png" style="width:3.50em;height:1.17em;"/></li>
<li><strong>Decoder</strong>: Decodes or reconstructs the input from the latent space representation using a function known as <img class="fm-editor-equation" src="assets/643771fa-fed6-457b-b4c6-beb20bfbcf5d.png" style="width:3.58em;height:1.17em;"/></li>
</ul>
<p>So, an AE can be described by a function of <img class="fm-editor-equation" src="assets/5455f554-5bcc-42bd-8b3b-ec1070db6e05.png" style="width:5.17em;height:1.25em;"/>, where we want <em>0</em> to be as close as the original input of <em>x</em>. AEs are very useful for data denoising and dimensionality reduction for data visualization. AEs can learn data projections, called <strong>representations</strong>, more effectively than PCA. The following diagram shows the architecture of a denoising AE:<br/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f40044e9-4968-443e-8ca5-1771240ccac6.png" style="width:37.58em;height:31.50em;"/></p>
<p><span>So, once we have a fingerprinting database to hand, </span><span>AEs can be trained with the</span> <span>raw RSSI measurements and the trained network itself is</span> <span>used as the fingerprint pattern for a specific reference location.</span> <span>Since a deep network can be represented by the weight of</span> <span>each layer, the fingerprint pattern can be expressed</span> <span>as follows:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/468c9efa-9469-4607-a4a8-2b8f4f4defe4.png" style="width:19.17em;height:1.75em;"/></p>
<p><span>In the preceding equation, <em>l</em></span> <span>is the number of encoding hidden layers of an AE</span><span>, and </span><span><img class="fm-editor-equation" src="assets/5a91f9bf-9c2b-44bb-857a-a1e0f676c199.png" style="width:1.33em;height:0.92em;"/></span> <span>and </span><span><img class="fm-editor-equation" src="assets/35489ecc-25a5-4e0a-b4ae-4378f7665645.png" style="width:1.17em;height:0.92em;"/></span> <span>represent the weights of the</span> <img class="fm-editor-equation" src="assets/ef1ef057-8bed-40e9-87b9-8de470f3daae.png" style="width:1.33em;height:1.17em;"/> <span>encoding hidden layer and its decoding mirror layer, as shown in the following diagram:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8b2950f2-58fb-4dff-ad2f-7fa884d78a3a.png" style="width:35.83em;height:24.42em;"/></p>
<p><span>Then, we can use the output of the central hidden layers of the AE as the input to the fully connected softmax layer to predict the location, as shown in the preceding diagram. Now that we know how indoor localization works in a neural network or machine learning setting, we can now start a hands-on example using Wi-Fi fingerprinting.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Example – Indoor localization with Wi-Fi fingerprinting</h1>
                </header>
            
            <article>
                
<p>In this example, we will use a <strong>multi-building, multi-floor indoor localization</strong> database and stacked AEs to localize Wi-Fi fingerprinting. With some minimal effort, this application can be deployed to mobile robots to use Wi-Fi localization subsystems.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Describing the dataset</h1>
                </header>
            
            <article>
                
<p>The <kbd>UJIIndoorLoc</kbd> dataset is a multi-building, multi-floor indoor localization database designed to test an indoor positioning system relying on Wi-Fi fingerprinting. Automatic user localization consists of estimating the position of the user, such as the latitude, longitude, and altitude, collected from a mobile phone. The <kbd>UJIIndoorLoc</kbd> database covers three buildings of Universitat Jaume I with 4 or more floors and almost 110,000 square meters, measured in 2013 by means of more than 20 different users and 25 Android devices. The database consists of two CSV files:</p>
<ul>
<li><kbd>trainingData.csv</kbd>: 19,937 training/reference records</li>
<li><kbd>validationData.csv</kbd>: 1,111 validation/test records</li>
</ul>
<p>The 529 attributes contain Wi-Fi fingerprints and the coordinates where they were taken. Each Wi-Fi fingerprint can be characterized by the detected WAPs and the corresponding RSSI. The intensity values are represented as negative integer values ranging from 1,04 dBm (extremely poor signal) to 0 dBm. The positive 100 value is used to denote when a WAP was not detected. During the database creation, 520 different WAPs were detected. Thus, the Wi-Fi fingerprint is composed of 520 intensity values. The coordinates' latitude, longitude, floor, and <strong>BuildingID</strong> information are the attributes to be predicted. The following list gives a quick summary of the dataset:</p>
<ul>
<li><strong>Attribute 001 to 520 (that is, WAP001 to WAP520)</strong>: These are the intensity measurement values for the access points in which values are in—104 to 0 and +100. The 100 value signifies that WAP001 was not detected.</li>
<li><strong>Attribute 521 (Longitude)</strong>: Negative real values from 7,695.9,387,549,299,299,000 to -7299.786516730871000</li>
<li><strong>Attribute 522 (Latitude)</strong>: Positive real values from 4,864,745.7,450,159,714 to 4,865,017.3,646,842,018.</li>
<li><strong>Attribute 523 (Floor)</strong>: Altitude in floors inside the building. Integer values from 0 to 4.</li>
<li><strong>Attribute 524 (BuildingID)</strong>: ID to identify the building provided as categorical integer values from 0 to 2.</li>
<li><strong>Attribute 525 (SpaceID)</strong>: Internal ID number to identify the space, such as the office, the corridor, or the classroom.</li>
<li><strong>Attribute 526 (RelativePosition)</strong>: Relative position with respect to the space (1—inside, 2—outside, in front of the door).</li>
<li><strong>Attribute 527 (UserID)</strong>: User identifier.</li>
<li><strong>Attribute 528 (PhoneID)</strong>: Android device identifier (see the following).</li>
<li><strong>Attribute 529 (Timestamp)</strong>: UNIX time when the capture was taken.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Network construction</h1>
                </header>
            
            <article>
                
<p>The AE classifier we will be using will have an AE part consisting of an encoder and a decoder. The following AE architecture is used to determine the floor and building location where the Wi-Fi is located. The input to the AE are signal strengths detected in a scan. Then, one value for each visible network is considered an RSSI record. The output of a decoder is the reconstructed input from the reduced representation, as shown in the following diagram (source: <em>Low-effort place recognition with Wi-Fi fingerprints using deep learning</em>, <span>Michał N. et al.,</span> arXiv:1611.02049v1):</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/104e89b7-6a30-4173-bc33-ed0cbacab1e5.png" style="width:37.92em;height:21.50em;"/></p>
<div class="mce-root packt_figref CDPAlignCenter CDPAlign">The AE architecture for the feature space representation</div>
<p>The classifier part consists of two hidden layers; depending on the complexity of the problem, the number of neurons needs to be selected. When the unsupervised learning of the weights of AE is finished, the decoder part of the network is disconnected. Then fully-connected layers are typically placed after the output of the encoder by turning the whole network into a classifier. In the following diagram, the pre-trained encoder part is connected to the fully connected softmax layer (source: <em>Low-effort Place Recognition with Wi-Fi Fingerprints Using Deep Learning</em>, <span>Michał N. et al., </span>arXiv:1611.02049v1):</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/7efbdf3f-e1a6-4412-9f42-0003d051578a.png" style="width:40.17em;height:23.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The architecture of an AE classifier for classifying a building and its floor based on Wi-Fi scan input </div>
<p class="CDPAlignLeft CDPAlign">The final output layer is a softmax layer that outputs the probabilities of the current sample belonging to the analyzed classes. Now, without any further delay, let's start implementing the preceding networks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementation</h1>
                </header>
            
            <article>
                
<p>We will use Keras to wrap up this conceptualization. First, let's import the necessary packages and libraries, as follows:</p>
<pre><strong>import</strong> pandas as pd<br/><strong>import</strong> numpy as np<br/><strong>import</strong> tensorflow as tf<br/><strong>from</strong> sklearn.preprocessing <strong>import</strong> scale<br/><strong>from</strong> keras.models <strong>import</strong> Sequential<br/><strong>from</strong> keras.layers <strong>import</strong> Input, Dense, Flatten, Dropout, Embedding, BatchNormalization<br/><strong>from</strong> keras.layers.convolutional <strong>import</strong> Conv1D,MaxPooling1D<br/><strong>from</strong> keras.layers <strong>import</strong> LSTM<br/><strong>from</strong> keras.layers.merge <strong>import</strong> concatenate<br/><strong>from</strong> keras.layers <strong>import</strong> GaussianNoise<br/><strong>from</strong> pickle <strong>import</strong> load<br/><strong>from</strong> keras <strong>import</strong> optimizers<br/><strong>from</strong> sklearn.metrics <strong>import</strong> classification_report<br/><strong>from</strong> sklearn.metrics <strong>import</strong> confusion_matrix<br/><strong>from</strong> sklearn.metrics <strong>import</strong> precision_recall_curve<br/><strong>from</strong> sklearn.metrics <strong>import</strong> precision_recall_fscore_support<br/><strong>import</strong> pandas_profiling</pre>
<p>Once we have imported all the necessary <span>packages,</span><span> we can proceed to prepare the training set and test set, which can be used to train and evaluate the model, respectively.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploratory analysis</h1>
                </header>
            
            <article>
                
<p>The exploratory analysis of data using the <strong>Python pandas</strong> library provides many powerful features–no doubt. However, using <kbd>df.describe()</kbd>, <kbd>df.dtypes</kbd>, or using <kbd>df.isnull().sum()</kbd> and plotting them separately is always time-consuming. Sometimes, you won't even get the required information in a sophisticated way. In fact, you'll have to write extra lines of code to convert them into a presentable format. However, to make your life easier, you can now start using the <kbd>pandas_profiling</kbd> library (see <a href="https://github.com/pandas-profiling/pandas-profiling">https://github.com/pandas-profiling/pandas-profiling</a>). Just one line of code will give the information you need:</p>
<pre>pandas_profiling.ProfileReport(df)</pre>
<p>Surely, it would be worth using <kbd>pandas_profiling</kbd> to get a quick understanding of your data. Let's try it out! First, we read the training data by explicitly passing <kbd>header=0</kbd> to be able to replace the existing names:</p>
<pre>trainDF = pd.read_csv("trainingData.csv",header = 0)</pre>
<p>To retrieve the list of variables that are rejected due to high correlation, you can use the following command:</p>
<div class="highlight highlight-source-python">
<pre>profile <span class="pl-k">=</span> pandas_profiling.ProfileReport(trainDF) </pre>
<p>This will produce a report showing information on the dataset: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f81ce1d6-dc12-4b22-a14c-4b38ff6d3eff.png" style="width:34.75em;height:32.58em;"/></p>
<p><span>Let's look at the first few lines of the report. A</span>s we can see, we don't have any null values and all the variables are numeric, which is great. However, some features are less significant, being highly correlated with other variables (for example, 74 variables were rejected) and some of the variables are very skewed, giving a very wide distribution. Even our training d<span>ataset</span> <span>has 637 duplicate rows</span>. Rejected variables would not help the model learn well. Consequently, those can be dropped from the training data (this is optional, though). The list of such rejected variables can be collected using the <kbd>get_rejected_variables</kbd> method, as follows:</p>
<pre>rejected_variables = profile.get_rejected_variables(threshold=0.9)</pre></div>
<p>If you want to generate a HTML report file, save the profile to an object and use the <kbd>to_file</kbd> function as follows:</p>
<div class="highlight highlight-source-python">
<pre>profile.to_file(outputfile="Report.html")</pre>
<p>This will generate an <kbd>HTML</kbd> report containing the necessary information. Now that we know the data and variables, let's focus on the feature engineering steps by which we'll prepare the data required for training and testing.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing training and test sets</h1>
                </header>
            
            <article>
                
<p>First, we scale the data to center to the mean. Then, we perform component-wise scaling to unit variance. This will help our model to converge the training more quickly:</p>
<pre>featureDF = np.asarray(trainDF.iloc[:,0:520]) # First 520 features <br/>featureDF[featureDF == 100] = -110<br/>featureDF = (featureDF - featureDF.mean()) / featureDF.var()</pre>
<p>Then, we construct the true labels. We convert all the building IDs and building floors to strings:</p>
<pre>labelDF = np.asarray(trainDF["BUILDINGID"].map(str) + trainDF["FLOOR"].map(str)) <br/>labelDF = np.asarray(pd.get_dummies(labelDF))</pre>
<p>Then, let's try to create two variables: <kbd>train_x</kbd> and <kbd>train_y</kbd>. This will help to avoid confusion during the training evaluation:</p>
<pre>train_x = featureDF<br/>train_y = labelDF<br/>print(train_x.shape)<br/>print(train_x.shape[1])</pre>
<p>Now, similar to the training set, we prepare the test set as well:</p>
<pre>testDF = pd.read_csv("validationData.csv",header = 0)<br/>test_featureDF = np.asarray(testDF.iloc[:,0:520])<br/>test_featureDF[test_featureDF == 100] = -110<br/>test_x = (test_featureDF - test_featureDF.mean()) / test_featureDF.var()<br/>test_labelDF = np.asarray(testDF["BUILDINGID"].map(str) + testDF["FLOOR"].map(str))<br/>test_y = np.asarray(pd.get_dummies(test_labelDF))<br/>print(test_x.shape)<br/>print(test_y.shape[1])</pre>
<p>Once we have the training and the test sets ready, we can now proceed with creating an AE.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an AE</h1>
                </header>
            
            <article>
                
<p><span>Let's create separate encoder and decoder functions since you will be using encoder weights later on for classification purposes. First, we define some parameters, such as the number of epochs and the batch size. Also, we compute the shape of the input data and the number of classes that will be required to construct and train the AE:</span></p>
<pre>number_epochs = 100<br/>batch_size = 32<br/>input_size = train_x.shape[1] # 520<br/>num_classes = train_y.shape[1] # 13</pre>
<p>Then, we create the encoder part of the AE, which has three hidden layers:</p>
<pre><strong>def</strong> encoder():<br/>    model = Sequential()<br/>    model.add(Dense(256, input_dim=input_size, activation='relu', use_bias=True))<br/>    model.add(Dense(128, activation='relu', use_bias=True))<br/>    model.add(Dense(64, activation='relu', use_bias=True))    <br/>    return model</pre>
<p>Next, we create the decoder part of the AE, which has three hidden layers, followed by the <kbd>compile()</kbd> method:</p>
<pre><strong>def</strong> decoder(encoder):   <br/>    encoder.add(Dense(128, input_dim=64, activation='relu', use_bias=True))<br/>    encoder.add(Dense(256, activation='relu', use_bias=True))<br/>    encoder.add(Dense(input_size, activation='relu', use_bias=True))<br/>    encoder.compile(optimizer='adam', loss='mse')<br/>    <strong>return</strong> encoder </pre>
<p>Then, we stack them together to construct an AE:</p>
<pre>encoderModel = encoder() # Encoder<br/>auto_encoder = decoder(encoderModel) # The autoencoder <br/>auto_encoder.summary()</pre>
<p>Let's see the structure and a summary of the AE:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d99b49b5-34e4-402d-8d44-9f1def2179d7.png" style="width:34.75em;height:21.25em;"/></p>
<p>We can then train the AE with the training data for 100 iterations, where 10% of the training data is to be used for validation:</p>
<pre>auto_encoder.fit(train_x, train_x, epochs = 100, batch_size = batch_size, <br/>                 validation_split=0.1, verbose = 1)</pre>
<p class="mce-root">Since we set the <kbd>verbose =1</kbd> in the preceding code block, during training, you'll see the following logs:</p>
<pre>Train on 17943 samples, validate on 1994 samples
Epoch 1/100
17943/17943 [==============================] - 5s 269us/step - loss: 0.0109 - val_loss: 0.0071
Epoch 2/100
17943/17943 [==============================] - 4s 204us/step - loss: 0.0085 - val_loss: 0.0066
Epoch 3/100
17943/17943 [==============================] - 3s 185us/step - loss: 0.0081 - val_loss: 0.0062
Epoch 4/100
17943/17943 [==============================] - 4s 200us/step - loss: 0.0077 - val_loss: 0.0062<br/>Epoch 98/100
17943/17943 [==============================] - 6s 360us/step - loss: 0.0067 - val_loss: 0.0055<br/>.......
Epoch 99/100
17943/17943 [==============================] - 5s 271us/step - loss: 0.0067 - val_loss: 0.0055
Epoch 100/100
17943/17943 [==============================] - 7s 375us/step - loss: 0.0067 - val_loss: 0.0055</pre>
<p>Then, we take the output of the encoder network for both the training set and the test set as the latent features:</p>
<pre>X_train_re = encoderModel.predict(train_x)<br/>X_test_re = encoderModel.predict(test_x)</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an AE classifier</h1>
                </header>
            
            <article>
                
<p><span>Next, we will re-train the <kbd>auto_encoder</kbd> model by making the first three layers trainable as <kbd>True</kbd> instead of keeping them as <kbd>False</kbd>:</span></p>
<pre><strong>for</strong> layer in auto_encoder.layers[0:3]:<br/>    layer.trainable = True  </pre>
<p>Alternatively, we can pop off the first three layers as follows:</p>
<pre><strong>for</strong> i in range(number_of_layers_to_remove):<br/>    auto_encoder.pop()</pre>
<p>Then, we add fully connected layers in front, with the <kbd>BatchNormalization</kbd> layer is followed by the first dense layer. Then, we add another dense layer, followed by the <kbd>BatchNormalization</kbd> and <kbd>Dropout</kbd> layers. Then, we place another dense layer, followed by a <kbd>GaussionNoise</kbd> layer and a <kbd>Dropout</kbd> layer, before we finally have the softmax layer:</p>
<pre>auto_encoder.add(Dense(128, input_dim=64, activation='relu', use_bias=True)) <br/>auto_encoder.add(BatchNormalization())                     <br/>auto_encoder.add(Dense(64, activation='relu', kernel_initializer = 'he_normal', use_bias=True)) <br/>auto_encoder.add(BatchNormalization())<br/>auto_encoder.add(Dropout(0.2))    <br/>auto_encoder.add(Dense(32, activation='relu', kernel_initializer = 'he_normal', use_bias=True))<br/>auto_encoder.add(GaussianNoise(0.1))<br/>auto_encoder.add(Dropout(0.1))  <br/>auto_encoder.add(Dense(num_classes, activation = 'softmax', use_bias=True))</pre>
<p>Finally, we get the full AE classifier:</p>
<pre>full_model = autoEncoderClassifier(auto_encoder)</pre>
<p>The full code is given as follows:</p>
<pre><strong>def</strong> autoEncoderClassifier(auto_encoder):<br/>    for layer in auto_encoder.layers[0:3]:<br/>        layer.trainable = True        <br/><br/>    auto_encoder.add(Dense(128, input_dim=64, activation='relu', use_bias=True)) <br/>    auto_encoder.add(BatchNormalization())                     <br/>    auto_encoder.add(Dense(64, activation='relu', kernel_initializer = 'he_normal', use_bias=True)) <br/>    auto_encoder.add(BatchNormalization())<br/>    auto_encoder.add(Dropout(0.2))    <br/>    auto_encoder.add(Dense(32, activation='relu', kernel_initializer = 'he_normal', use_bias=True))<br/>    auto_encoder.add(GaussianNoise(0.1))<br/>    auto_encoder.add(Dropout(0.1))  <br/>    auto_encoder.add(Dense(num_classes, activation = 'softmax', use_bias=True))<br/>    return auto_encoder<br/><br/>full_model = autoEncoderClassifier(auto_encoder)</pre>
<p>Then, we compile the model before starting the training:</p>
<pre>full_model.compile(loss = 'categorical_crossentropy', optimizer = optimizers.adam(lr = 0.001), metrics = ['accuracy'])</pre>
<p>Now, we start fine-tuning the network in a supervised way:</p>
<pre>history = full_model.fit(X_train_re, train_y, epochs = 50, batch_size = 200, validation_split = 0.2, verbose = 1)</pre>
<p>Since we set <kbd>verbose =1</kbd> in the preceding code block, during training, you'll experience the following logs:</p>
<pre>Train on 15949 samples, validate on 3988 samples
Epoch 1/50
15949/15949 [==============================] - 10s 651us/step - loss: 0.9263 - acc: 0.7086 - val_loss: 1.4313 - val_acc: 0.5747
Epoch 2/50
15949/15949 [==============================] - 5s 289us/step - loss: 0.6103 - acc: 0.7749 - val_loss: 1.2776 - val_acc: 0.5619
Epoch 3/50
15949/15949 [==============================] - 5s 292us/step - loss: 0.5499 - acc: 0.7942 - val_loss: 1.3871 - val_acc: 0.5364<br/>.......<br/>Epoch 49/50
15949/15949 [==============================] - 5s 342us/step - loss: 1.3861 - acc: 0.4662 - val_loss: 1.8799 - val_acc: 0.2706
Epoch 50/50
15949/15949 [==============================] - 5s 308us/step - loss: 1.3735 - acc: 0.4805 - val_loss: 2.1081 - val_acc: 0.2199</pre>
<p>Now let's take a look at the training loss versus validation loss, which will help us to understand how the training went. This will also help us to establish whether our neural network has issues such as overfitting and underfitting:</p>
<pre><strong>import</strong> pandas as pd<br/><strong>import</strong> numpy as np<br/><strong>import</strong> matplotlib.pyplot as plt<br/>%matplotlib inline<br/><br/>plt.plot(history.history['acc'])<br/>plt.plot(history.history['val_acc'])<br/>plt.ylabel('Accuracy')<br/>plt.xlabel('Epochs')<br/>plt.legend(['Training loss', 'Validation loss'], loc='upper left')<br/>plt.show()</pre>
<p class="CDPAlignLeft CDPAlign">The preceding code block will plot the training loss and validation losses: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c2372d36-7447-4aca-9c0e-86565eeefeaf.png" style="width:30.92em;height:19.67em;"/></p>
<p><span>As seen in the preceding graph, the training losses across epochs are higher than the validation loss, which is a sign of overfitting.</span> We don't have enough training samples to train the neural network well. Some samples were even repeated in the dataset, which literally turned out to be trivial and redundant in the network. This was probably the reason adding the <strong>Dropout</strong> and <strong>Gaussian</strong> noise layers didn't help much. Anyway, we <span>can</span><span> </span><span>also save the trained model for future reuse, which we'll discuss in the next section.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Saving the trained model</h1>
                </header>
            
            <article>
                
<p>Now that we have the AE classifier fully trained, we can save it so that we can restore it from disk <span>later on</span><span>:</span></p>
<pre><strong>import</strong> os<br/><strong>from</strong> pickle <strong>import</strong> load<br/><strong>from</strong> keras.models <strong>import</strong> load_model<br/>os.environ["PATH"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'<br/><strong>from</strong> keras.utils.vis_utils <strong>import</strong> plot_model<br/><br/>plot_model(full_model, show_shapes=True, to_file='Localization.png')<br/># save the model<br/>full_model.save('model.h5')<br/># load the model<br/>model = load_model('model.h5') </pre>
<p>In the next section, we will evaluate the trained model on the test set, which we will discuss in the next subsection.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating the model</h1>
                </header>
            
            <article>
                
<p>Now that our model is fully trained, we can evaluate its performance on unseen data:</p>
<pre>results = full_model.evaluate(X_test_re, test_y)<br/>print('Test accuracy: ', results[1])</pre>
<p>The preceding lines of code will show the accuracy score, something like this:</p>
<pre>1111/1111 [==============================] - 0s 142us/step
Test accuracy: 0.8874887488748875</pre>
<p>Then, let's compute the performance metrics:</p>
<pre>predicted_classes = full_model.predict(test_x)<br/>pred_y = np.argmax(np.round(predicted_classes),axis=1)<br/>y = np.argmax(np.round(test_y),axis=1)<br/>p, r, f1, s = precision_recall_fscore_support(y, pred_y, average='weighted')<br/>print("Precision: " + str(p*100) + "%")<br/>print("Recall: " + str(r*100) + "%")<br/>print("F1-score: " + str(f1*100) + "%")</pre>
<p>The preceding code block will show the following output, giving an F1-score of 88%, approximately:</p>
<pre>Precision: 90.29611866225324%
Recall: 88.11881188118812%
F1-score: 88.17976604784566%</pre>
<p>Additionally, we can print the classification report to know the class-specific localization as well:</p>
<pre>print(classification_report(y, pred_y))</pre>
<p>The preceding line of code will produce the following output:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/e2f80ecc-424d-4628-9cd6-b442280fcb78.png" style="width:28.25em;height:19.83em;"/></div>
<p>Additionally, we will plot the confusion matrix:</p>
<pre>print(confusion_matrix(y, pred_y))</pre>
<p>The preceding line of code will produce the following confusion matrix:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/91024568-524d-40ff-9690-1f36e7a1242b.png" style="width:21.92em;height:19.08em;"/></p>
<p>As seen in the preceding confusion matrix, our AE classifier was mostly confused for class 11 and predicted as many as 39 samples to be classified in grid 12. However, we have still managed to get very good accuracy. Possible suggestions for improvements could be as follows:</p>
<ul>
<li>Training the network after removing the rejected variables</li>
<li>Training the network on more epochs</li>
<li>Performing hyperparameter tuning using grid search and cross-validation</li>
<li>Adding more layers to the network</li>
</ul>
<p>Once you find the optimized model trained on more data, giving stable, improved performance, it can be deployed in on IoT enabled device. We will discuss some possible deployment options in the next section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deployment techniques</h1>
                </header>
            
            <article>
                
<p>As we argued earlier, <span>each Wi-Fi scan contains the signal strength measurements for APs available in its vicinity, but only a subset of the total number of networks in the environment are observed. Many IoT devices, such as a mobile phone or a Raspberry Pi, are low-end with very little processing power. So, deploying such a DL model would be a challenging task.</span></p>
<p>Many solution providers and technology companies provide smart positioning services commercially. Using Wi-Fi fingerprinting from indoor and outdoor location data, the accurate tracking of devices is now possible. In most of these companies, the RSSI fingerprint positioning is used as the core technology. In such a setting, signals or messages that bear different sensitivity levels across RSSI values (which is of course subject to the proximity) can be picked up by gateways. Then, if there are <img class="fm-editor-equation" src="assets/c189cb14-792d-4335-8822-38f5e338c167.png" style="width:0.83em;height:0.92em;"/> gateways in a network, the RSSI values acquired from a particular indoor or outdoor location will form the RSSI fingerprint having <em>n</em> entries at that location, which is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f080db6c-cb77-425d-bedb-97be4ebdc152.png" style="width:22.50em;height:18.75em;"/></p>
<p>The preceding diagram corresponds to the following equation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/95c8924c-4289-48ba-887d-109ac1944279.png" style="width:21.75em;height:1.33em;"/></p>
<p>However, in cases with a large number of gateways (&gt; 4), the fingerprint could be distinctly unique within a certain range. One deployment technique could be using the trained model serving at the backend and serving it as an Android or iOS mobile application. The application then monitors the signals from the IoT devices already deployed in the indoor location, inserts them as RSSI values in the SQLite database and, based on the RSSI values, prepares the test set and sends a query to the pre-trained model to get the location.</p>
<p>The following diagram shows a schematic architecture outlining all the steps required for such a deployment:<br/></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/187ba3bf-40ca-4c00-8ed9-aef4555073f5.png" style="width:68.08em;height:28.92em;"/></p>
<p>In such a case, the trained model will serve as the transfer learning. Nevertheless, the trained model can be served as a web application using Flask or the DJango Python framework. Then, the RSSI values and signals from the IoT devices can be stored in a database to enrich the historical data. The location can subsequently be tracked using an Android or iOS application. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have discussed how indoor localization works for IoT enabled devices. In particular, we have seen how DL techniques can be used for indoor localization in IoT applications employing that data in general with a hands-on example. Furthermore, we have looked at some deployment settings of indoor localization services in IoT environments.</p>
<p>In <a href="958a0ed2-1d5f-4df1-8bfa-55d3c870d733.xhtml">Chapter 6</a>, <em>Physiological and Psychological State Detection in IoT</em>, we will discuss DL-based human physiological and psychological state detection techniques for IoT applications in general. Considering a real-world scenario, we will look at two IoT applications based on physiological and psychological state detection.</p>


            </article>

            
        </section>
    </body></html>