<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer109">&#13;
			<p id="_idParaDest-90" class="chapter-number"><a id="_idTextAnchor177"/>Chapter 6: </p>&#13;
			<h1 id="_idParaDest-91"><a id="_idTextAnchor178"/>Hyperparameter Tuning</h1>&#13;
			<p>In this chapter, we are going to start by looking at three different hyperparameter tuning algorithms—Hyperband, Bayesian optimization, and random search. These algorithms are implemented in the <strong class="source-inline">tf.keras</strong> API, which makes them relatively easy to understand. With this API, you now have access to simplified APIs for these complex and advanced algorithms that we will encounter in this chapter. We will learn how to implement these algorithms and use the best hyperparameters we can find to build and train an image classification model. We will also learn the details of its learning process in order to know which hyperparameters to search and optimize. We will start by getting and preparing the data, and then we'll apply our algorithm to it. Along the way, we will also try to understand key principles and the logic to implement user choices for these algorithms as user inputs, and we'll look at a template to submit tuning and training jobs in GCP Cloud TPU.</p>&#13;
			<p>In this chapter, we will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Delineating hyperparameter types</li>&#13;
				<li>Understanding the syntax and use of Keras Tuner</li>&#13;
				<li>Delineating hyperparameter search algorithms</li>&#13;
				<li>Submitting tuning jobs in a local environment</li>&#13;
				<li>Submitting tuning jobs in Google's AI Platform</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-92"><a id="_idTextAnchor179"/>Technical requirements</h1>&#13;
			<p>The entire code base for this chapter is in the following GitHub repository. Please clone it to your environment:</p>&#13;
			<p><a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_06/">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_06/</a></p>&#13;
			<p>This can be done through a command-line environment:</p>&#13;
			<p class="source-code">git clone https://github.com/PacktPublishing/learn-tensorflow-enterprise.git</p>&#13;
			<h1 id="_idParaDest-93"><a id="_idTextAnchor180"/>Delineating hyperparameter types</h1>&#13;
			<p>As we develop a model <a id="_idIndexMarker282"/>and its training process, we define variables and set their values to determine the training workflow and the model's structure. These values (such as the number of hidden nodes in a layer of a multilayer perceptron, or the selection of an optimizer and a loss function) are known as hyperparameters. These parameters are specified by the model creator. The performance of a machine learning model often depends on the model architecture and the hyperparameters selected during its training process. Finding a set of optimal hyperparameters for the model is not a trivial task. The simplest method to this task is by grid search, that is, building all possible combinations of hyperparameter values within a search space and then comparing the evaluation metrics across these combinations. While this is straightforward and thorough, it is a tedious process. We will see how the new <strong class="source-inline">tf.keras</strong> API implements three different search algorithms.</p>&#13;
			<p>There are two types of hyperparameters in the context of model training: </p>&#13;
			<ul>&#13;
				<li><strong class="bold">Model hyperparameters</strong>: These parameters are directly related to the structure of a model layer, such as the number of nodes in a layer.</li>&#13;
				<li><strong class="bold">Algorithm hyperparameters</strong>: These parameters are required to execute the learning algorithm, such as the learning rate in the loss function used during gradient descent, or the choice of loss function.</li>&#13;
			</ul>&#13;
			<p>The code modification required to scan through both types of hyperparameters would be very complicated if you want to use the grid search technique. This is where a more efficient and comprehensive framework would be very helpful for hyperparameter tuning. </p>&#13;
			<p>In this chapter, we are going to see the latest addition in hyperparameter tuning frameworks to the TensorFlow ecosystem. This framework is Keras Tuner. As the name suggests, it is for models developed with the TensorFlow 2.x Keras API. The minimum requirement for this framework is TensorFlow 2.0+ and Python 3.6. It is released as a part of the TensorFlow 2.3 <a id="_idIndexMarker283"/>distribution. If you are not yet using TensorFlow 2.3, then as long as you are using TensorFlow 2.x, Keras Tuner can be installed with the help of the following command:</p>&#13;
			<p class="source-code">pip install keras-tuner</p>&#13;
			<p>Once Keras Tuner is installed, you can load it in your Python code. </p>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">You need not put a dash <strong class="source-inline">-</strong> between <strong class="source-inline">keras-tuner</strong> while importing it. It will be imported like this: <strong class="source-inline">import kerastuner as kt</strong>.</p>&#13;
			<p>Keras Tuner is a distributable hyperparameter optimization framework that helps to define the search space for collections of hyperparameters. It also includes the following three built-in algorithms to help find the best hyperparameters:</p>&#13;
			<ul>&#13;
				<li>Hyperband</li>&#13;
				<li>Bayesian optimization</li>&#13;
				<li>Random search</li>&#13;
			</ul>&#13;
			<p>For Keras Tuner, whether it is model hyperparameters or algorithm hyperparameters, it makes no difference to the syntax or definition style of these hyperparameters. Therefore, you have great flexibility in choosing what hyperparameters to tune without complicated coding patterns or loops. </p>&#13;
			<p>The Keras Tuner framework makes it easy for us to modify our training script. While there are changes and refactoring involved, the API format and logical flow are very much consistent with <a id="_idIndexMarker284"/>Keras styles and implementations. Before we jump into the examples, let's spend some time understanding this framework and seeing how to extend our training code to accommodate it<a id="_idTextAnchor181"/>. </p>&#13;
			<h1 id="_idParaDest-94"><a id="_idTextAnchor182"/>Understanding the syntax and use of Keras Tuner </h1>&#13;
			<p>For the most <a id="_idIndexMarker285"/>part, as<a id="_idIndexMarker286"/> far as Keras Tuner is concerned, hyperparameters can be described by the following three data types: integers, floating points, and choices from a list of discrete values or objects. In the following sub-sections, we will take a closer look at how to use these data types to define hyperparameters in different parts of the model architecture and training workflow.</p>&#13;
			<h2 id="_idParaDest-95"><a id="_idTextAnchor183"/>Using hp.Int for hyperparameter definition</h2>&#13;
			<p>Keras Tuner<a id="_idIndexMarker287"/> defines a search space with a very simple<a id="_idIndexMarker288"/> and intuitive style. To define a set of possible number of nodes in a given layer, you typically would have a layer definition like the this:</p>&#13;
			<p class="source-code">tf.keras.layers.Dense(units = hp_units, activation = 'relu')</p>&#13;
			<p>In the preceding line of code, <strong class="source-inline">hp_units</strong> is the number of nodes in this layer. If you wish to subject <strong class="source-inline">hp_units</strong> to hyperparameter search, then you simply need to define the definition for this hyperparameter's search space. Here's an example: </p>&#13;
			<p class="source-code">hp = kt.HyperParameters()</p>&#13;
			<p class="source-code">hp_units = hp.Int('units', min_value = 64, max_value = 256, step = 16)</p>&#13;
			<p><strong class="source-inline">hp</strong> is the object that represents an instance of <strong class="source-inline">kerastuner</strong>. </p>&#13;
			<p>This is simply an <a id="_idIndexMarker289"/>array of integers between <strong class="source-inline">64</strong> and <strong class="source-inline">256</strong> at an <a id="_idIndexMarker290"/>increment of <strong class="source-inline">16</strong>. When applied to the <strong class="source-inline">Dense</strong> layer, it becomes an array of possible values in the search space for <strong class="source-inline">hp_units</strong>.</p>&#13;
			<h2 id="_idParaDest-96"><a id="_idTextAnchor184"/>Using hp.Choice for hyperparameter definition</h2>&#13;
			<p>If you have<a id="_idIndexMarker291"/> a set of values in mind and these values do <a id="_idIndexMarker292"/>not fall into incremental steps, you may specify a list of values as shown in the following line of code:</p>&#13;
			<p class="source-code">hp_units = hp.Choice('units', values = [64, 80, 90])</p>&#13;
			<p><strong class="source-inline">hp_Choice</strong> is a flexible type for hyperparameters. It can also be used to define algorithmic hyperparameters such as activation functions. All it needs is the name of possible activation functions. A search space for different activation functions may look like this:</p>&#13;
			<p class="source-code">hp_activation = hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'])</p>&#13;
			<p>Then the definition for the layer that uses this hyperparameter would be:</p>&#13;
			<p class="source-code">tf.keras.layers.Dense(units = hp_units, activation = hp_activation)</p>&#13;
			<p>Another place where <strong class="source-inline">hp.Choice</strong> may be applied is when you want to try different optimizers:</p>&#13;
			<p class="source-code">hp_optimizer = hp.Choice('selected_optimizer', ['sgd', 'adam'])</p>&#13;
			<p>Then, in the model compilation step, where an optimizer is specified in the training workflow, you would simply define <strong class="source-inline">optimizer</strong> as <strong class="source-inline">hp_optimizer</strong>:</p>&#13;
			<p class="source-code">model.compile(optimizer = hp_optimizer, loss = …, metrics = …)</p>&#13;
			<p>In the preceding <a id="_idIndexMarker293"/>example, we pass <strong class="source-inline">hp_optimizer</strong> into<a id="_idIndexMarker294"/> the model compilation step as our selection for the optimizer to be used in the training process.</p>&#13;
			<h2 id="_idParaDest-97"><a id="_idTextAnchor185"/>Using hp.Float for hyperparameter definition</h2>&#13;
			<p>Floating points<a id="_idIndexMarker295"/> frequently appear as parameters in <a id="_idIndexMarker296"/>the training workflow, such as the learning rate for the optimizer. Here is an example that demonstrates how it is defined in a case with the optimizer's learning rate as a hyperparameter:</p>&#13;
			<p class="source-code">hp_learning_rate = hp.Float('learning_rate', min_value = 1e-4, max_value = 1e-2, step = 1e-3)</p>&#13;
			<p class="source-code">optimizer=tf.keras.optimizers.SGD(lr=hp_learning_rate, momentum=0.5)</p>&#13;
			<p>In the preceding code, we define a search space for our optimizer's learning rate. We then pass the <strong class="source-inline">hp_learning_rate</strong> object into the optimizer definition.</p>&#13;
			<p>As an example, I created a <strong class="source-inline">model_builder</strong> function. This function accepts <strong class="source-inline">hp object</strong>, which defines the hyperparameter search space, and then passes the <strong class="source-inline">hp object</strong> into the model architecture. The function returns the completed model. Here is the <strong class="source-inline">model_builder</strong> function:</p>&#13;
			<p class="source-code">def model_builder(hp):</p>&#13;
			<p class="source-code">    </p>&#13;
			<p class="source-code">    hp_units = hp.Int('units', min_value = 64, max_value = 256, </p>&#13;
			<p class="source-code">                                                     step = 64) </p>&#13;
			<p class="source-code">    hp_activation = hp.Choice('dense_activation', </p>&#13;
			<p class="source-code">        values=['relu', 'tanh', 'sigmoid'])</p>&#13;
			<p class="source-code">    IMAGE_SIZE = (224, 224)</p>&#13;
			<p class="source-code">    model = tf.keras.Sequential([</p>&#13;
			<p class="source-code">    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)), </p>&#13;
			<p class="source-code">    hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4', trainable=False),</p>&#13;
			<p class="source-code">    tf.keras.layers.Flatten(),</p>&#13;
			<p class="source-code">    tf.keras.layers.Dense(units = hp_units, </p>&#13;
			<p class="source-code">                          activation = hp_activation, </p>&#13;
			<p class="source-code">                          kernel_initializer='glorot_uniform'),</p>&#13;
			<p class="source-code">    tf.keras.layers.Dense(5, activation='softmax', </p>&#13;
			<p class="source-code">                                         name = 'custom_class')</p>&#13;
			<p class="source-code">    ])</p>&#13;
			<p class="source-code">    model.build([None, 224, 224, 3])</p>&#13;
			<p class="source-code">    </p>&#13;
			<p class="source-code">    model.compile(</p>&#13;
			<p class="source-code">        optimizer=tf.keras.optimizers.SGD(lr=1e-2, </p>&#13;
			<p class="source-code">                                                momentum=0.5), </p>&#13;
			<p class="source-code">        loss=tf.keras.losses.CategoricalCrossentropy(</p>&#13;
			<p class="source-code">                        from_logits=True, label_smoothing=0.1),</p>&#13;
			<p class="source-code">        metrics=['accuracy'])</p>&#13;
			<p class="source-code">  </p>&#13;
			<p class="source-code">return model</p>&#13;
			<p>With the Keras Tuner API, the search space format and the way in which the search space is referenced inside the model layer or training algorithm are straightforward and provide great<a id="_idIndexMarker297"/> flexibility. All that was done was defining a<a id="_idIndexMarker298"/> search space, then passing the object holding the search space into the model definition. It would be a daunting task to handle the conditional logic following the grid search approach.</p>&#13;
			<p>Next, we will take a look at how to use Keras Tuner classes to specify the following three different search algorithms: </p>&#13;
			<ul>&#13;
				<li>Hyperband</li>&#13;
				<li>Bayesian optimizati<a id="_idTextAnchor186"/>on</li>&#13;
				<li>Random search</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-98"><a id="_idTextAnchor187"/>Delineating hyperparameter search algorithms</h1>&#13;
			<p>In this section, we will <a id="_idIndexMarker299"/>take a closer look at three algorithms that traverse the hyperparameter search space. These algorithms are implemented by t<a id="_idTextAnchor188"/>he <strong class="source-inline">tf.keras</strong> API.</p>&#13;
			<h2 id="_idParaDest-99"><a id="_idTextAnchor189"/>Hyperband</h2>&#13;
			<p>Hyperparameter<a id="_idIndexMarker300"/> search is an inherently tedious <a id="_idIndexMarker301"/>process that requires a budget <strong class="source-inline">B</strong> to test a finite set of possible hyperparameter configurations <strong class="source-inline">n</strong>. In this context, budget simply means compute time as indicated by the epoch, and the training data subsets. The hyperband algorithm takes advantage of early stopping and successive halving so that it can evaluate more hyperparameter configurations in a given time and with a given set of hardware resources. Early stopping helps eliminate underperforming configurations before too much training time is invested in them.</p>&#13;
			<p>The successive halving method is very intuitive: for a set of hyperparameter configurations, run them through the same budget (that is, epoch, memory, and training data subset size). Then we rank the performance of these configurations, discarding the configurations in the worst half. This process is repeated until only one configuration remains. This is similar to playoff brackets in that at every bracket, half of the configurations are eliminated, until only one remains.</p>&#13;
			<p>There are two <strong class="source-inline">for</strong> loops in the Hyperband algorithm: </p>&#13;
			<ul>&#13;
				<li>The inner loop, which performs successive halving that discards a portion of hyperparameter configurations, thereby reducing the search space</li>&#13;
				<li>An outer loop, which iterates over different combinations of <strong class="source-inline">B</strong> and <strong class="source-inline">n</strong></li>&#13;
			</ul>&#13;
			<p>In early iterations, there are many candidate configurations. As each candidate is given a portion of budget <strong class="source-inline">B</strong> to train, early stopping ensures that a fraction (that is, half) of these configurations are discarded early before too much training time is wasted. As brackets become smaller through successive halving, fewer candidate configurations remain, and therefore each<a id="_idIndexMarker302"/> candidate is more likely to get a higher <a id="_idIndexMarker303"/>portion of <strong class="source-inline">B</strong>. This continues until the last hyperparameter configuration remains. Therefore, you may think of the Hyperband algorithm as an approach for selecting the best hyperparameter configurations that cuts the losses early by discarding low-performing configurations.  </p>&#13;
			<p>Here is a reference for more information on the Hyperband algorithm: <a href="https://openreview.net/pdf?id=ry18Ww5ee">https://openreview.net/pdf?id=ry18Ww5ee</a>.</p>&#13;
			<p>Now let's take a look at how to define a tuner instance that uses the Hyperband algorithm (for a detailed description of the API and its parameters, see <a href="https://keras-team.github.io/keras-tuner/documentation/tuners/">https://keras-team.github.io/keras-tuner/documentation/tuners/</a>). Here is an example: </p>&#13;
			<p class="source-code">import kerastuner as kt</p>&#13;
			<p class="source-code">import tensorflow_hub as hub</p>&#13;
			<p class="source-code">import tensorflow as tf</p>&#13;
			<p class="source-code">from absl import flags</p>&#13;
			<p class="source-code">flags_obj = flags.FLAGS</p>&#13;
			<p class="source-code">strategy = tf.distribute.MirroredStrategy()</p>&#13;
			<p class="source-code">tuner = kt.Hyperband(</p>&#13;
			<p class="source-code">            hypermodel = model_builder,</p>&#13;
			<p class="source-code">            objective = 'val_accuracy', </p>&#13;
			<p class="source-code">            max_epochs = 3,</p>&#13;
			<p class="source-code">            factor = 2,</p>&#13;
			<p class="source-code">            distribution_strategy=strategy,</p>&#13;
			<p class="source-code">            directory = flags_obj.model_dir,</p>&#13;
			<p class="source-code">            project_name = 'hp_tune_hb',</p>&#13;
			<p class="source-code">            overwrite = True)</p>&#13;
			<p>Here is a <a id="_idIndexMarker304"/>description of the parameters<a id="_idIndexMarker305"/> shown:</p>&#13;
			<ul>&#13;
				<li><strong class="source-inline">hypermodel</strong>: A function of the class that builds a model architecture.</li>&#13;
				<li><strong class="source-inline">objective</strong>: Performance metrics for evaluation.</li>&#13;
				<li><strong class="source-inline">max_epoch</strong>: The maximum number of epochs to train a model.</li>&#13;
				<li><strong class="source-inline">factor</strong>: Reduction for the number of epochs and number of models for each bracket. It selects configurations ranked in the top 1/<strong class="source-inline">factor</strong> of all configurations. A higher <strong class="source-inline">factor</strong> means more pruning and therefore it's quicker for the search process to identify a top performer.</li>&#13;
				<li><strong class="source-inline">distribution_strategy</strong>: This is used if hardware is available for distributed training.</li>&#13;
				<li><strong class="source-inline">directory</strong>: The target directory or path to write the search results.</li>&#13;
				<li><strong class="source-inline">project_name</strong>: The name used as a prefix for files saved by the tuner.</li>&#13;
				<li><strong class="source-inline">overwrite</strong>: This is a Boolean. If <strong class="source-inline">True</strong>, then hyperparameter search will start from scratch. Let's explain a bit more about this API. In this case, <strong class="source-inline">kt</strong> is the tuner object. In the hyperband definition, <strong class="source-inline">hypermodel</strong> designates a function that builds the model architecture.</li>&#13;
			</ul>&#13;
			<p>In this example, we will define the search space for the number of nodes (<strong class="source-inline">hp_units</strong>) in the middle <strong class="source-inline">Dense</strong> layer of the model architecture, as well as the search space for the activation function (<strong class="source-inline">hp_activation</strong>) of that layer. After these definitions, we construct the model architecture, pass these <strong class="source-inline">hp</strong> objects into the destined layer, compile the model, and return the model.</p>&#13;
			<p>Notice <strong class="source-inline">hp</strong> in the function signature. It indicates this is the entry function for the model structure definition, where<a id="_idIndexMarker306"/> the hyperparameters are specified. In this example, there are two <a id="_idIndexMarker307"/>hyperparameters:</p>&#13;
			<p class="source-code">hp_units = hp.Int('units', min_value = 64, max_value = 256, step = 64)</p>&#13;
			<p class="source-code">hp_activation = hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'])</p>&#13;
			<p>Inside the model's sequential API definition, you will find these hyperparameters in one of the <strong class="source-inline">Dense</strong> layers:</p>&#13;
			<p class="source-code">tf.keras.layers.Dense(units = hp_units, activation = hp_activation, kernel_initializer='glorot_uniform'),</p>&#13;
			<p>Before exiting this function, you would compile the model and return the model to the tuner instance. Now let's begin with the training of the Hyperband hyperparameter:</p>&#13;
			<ol>&#13;
				<li>Now that the tuner and its search algorithm are defined, this is how you would set up the search:<p class="source-code">tuner.search(train_ds,</p><p class="source-code">        steps_per_epoch=STEPS_PER_EPOCHS,</p><p class="source-code">        validation_data=val_ds,</p><p class="source-code">        validation_steps=VALIDATION_STEPS,</p><p class="source-code">        epochs=30,</p><p class="source-code">        callbacks=[tf.keras.callbacks.EarlyStopping(</p><p class="source-code">                                        'val_accuracy')])</p><p>In this example, <strong class="source-inline">train_ds</strong> is the training dataset, while <strong class="source-inline">val_ds</strong> is the cross-validation dataset. The rest of the parameters are the same as seen in a typical training routine. </p></li>&#13;
				<li>After the search is done, you may retrieve the best hyperparameter configuration <a id="_idIndexMarker308"/>through<a id="_idIndexMarker309"/> an object:<p class="source-code">best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]</p><p class="source-code">print(f'''</p><p class="source-code">        The hyperparameter search is done. </p><p class="source-code">        The best number of nodes in the dense layer is {best_hps.get('units')}.</p><p class="source-code">        The best activation function in mid dense layer is {best_hps.get('dense_activation')}.</p><p class="source-code">        ''')</p><p>By default, <strong class="source-inline">num_trials = 1</strong> indicates this will return the best model. Since this is a list object, we retrieve it by the first index of a list, which is <strong class="source-inline">0</strong>. The <strong class="source-inline">print</strong> statement shows how the item in <strong class="source-inline">best_hps</strong> may be referenced. </p></li>&#13;
				<li>It is recommended that once you have <strong class="source-inline">best_hps</strong>, you should retrain your model with these parameters. We will start with the <strong class="source-inline">tuner</strong> object initialized with <strong class="source-inline">best_hps</strong>: <p class="source-code">model = tuner.hypermodel.build(best_hps)</p></li>&#13;
				<li>Then we may define checkpoints and callbacks for the formal training:<p class="source-code">checkpoint_prefix = os.path.join(flags_obj.model_dir, 'best_hp_train_ckpt_{epoch}')</p><p class="source-code">    callbacks = [</p><p class="source-code">    tf.keras.callbacks.ModelCheckpoint(</p><p class="source-code">		filepath=checkpoint_prefix,</p><p class="source-code">		save_weights_only=True)]</p></li>&#13;
				<li>Now let's <a id="_idIndexMarker310"/>call the <strong class="source-inline">fit</strong> function to start<a id="_idIndexMarker311"/> training with the best hyperparameter configuration:<p class="source-code">model.fit(</p><p class="source-code">        train_ds,</p><p class="source-code">        epochs=30, steps_per_epoch=STEPS_PER_EPOCHS,</p><p class="source-code">        validation_data=val_ds,</p><p class="source-code">        validation_steps=VALIDATION_STEPS,</p><p class="source-code">        callbacks=callbacks)</p></li>&#13;
				<li>Once training is completed, save the trained model:<p class="source-code">model_save_dir = os.path.join(flags_obj.model_dir, </p><p class="source-code">                                       'best_save_model')</p><p class="source-code">model.save(model_save_dir)</p><p>Now the model trained with the hyperband hyperparameter search is saved in the file path <a id="_idIndexMarker312"/>designated by <strong class="source-inline">model_save_dir</strong>. Next, we<a id="_idIndexMarker313"/> are going to take a look at another algorithm for hyperparameter search: Bayesian optimization.</p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-100"><a id="_idTextAnchor190"/>Bayesian optimization</h2>&#13;
			<p>This method<a id="_idIndexMarker314"/> leverages what is learned from<a id="_idIndexMarker315"/> the initial training samples and nudges changes in hyperparameter values towards the favorable direction of the search space. Actually, what was learned from the initial training samples is a probabilistic function that models the value of our objective function. This <strong class="bold">probabilistic</strong> function, also known as a <strong class="bold">surrogate</strong> function, models the distribution of our objective (that is, validation loss) as a Gaussian process. With a surrogate function ready, the next hyperparameter configuration candidate is selected such that it is most likely to improve (that is, minimize, if the objective is validation loss) the surrogate function.  </p>&#13;
			<p>The <strong class="source-inline">tuner</strong> instance invokes this algorithm in a straightforward fashion. Here is an example:</p>&#13;
			<p class="source-code">tuner = kt.BayesianOptimization(</p>&#13;
			<p class="source-code">            hypermodel = model_builder,</p>&#13;
			<p class="source-code">            objective ='val_accuracy',</p>&#13;
			<p class="source-code">            max_trials = 50,</p>&#13;
			<p class="source-code">            directory = flags_obj.model_dir,</p>&#13;
			<p class="source-code">            project_name = 'hp_tune_bo',</p>&#13;
			<p class="source-code">            overwrite = True</p>&#13;
			<p class="source-code">            )</p>&#13;
			<p>This line of code defines a <strong class="source-inline">tuner</strong> object that I set up to use the Bayesian optimization algorithm as a means for hyperparameter optimization. Similar to Hyperband, it requires a function definition for <strong class="source-inline">hypermodel</strong>. In this case, <strong class="source-inline">model_builder</strong> from Hyperband is used again. The criterion for optimization is validation accuracy. The maximum number of trials is set to <strong class="source-inline">50</strong>, and we will specify the directory in which to save the model as user input during job submission. The user input for <strong class="source-inline">model_dir</strong> is carried by <strong class="source-inline">flags_obj.model_dir</strong>.</p>&#13;
			<p>As indicated by the <strong class="source-inline">BayesianOptimization</strong> API, there are not many differences in the function signature compared to Hyperband. <strong class="source-inline">max_trials</strong> is the maximum number of hyperparameter configurations to try. This value may be pre-empted or ignored if the search space is exhausted.</p>&#13;
			<p>The next step is the<a id="_idIndexMarker316"/> same as seen <a id="_idIndexMarker317"/>in the <em class="italic">Hyperband</em> section when launching the search process:</p>&#13;
			<p class="source-code">tuner.search(train_ds,</p>&#13;
			<p class="source-code">        steps_per_epoch=STEPS_PER_EPOCHS,</p>&#13;
			<p class="source-code">        validation_data=val_ds,</p>&#13;
			<p class="source-code">        validation_steps=VALIDATION_STEPS,</p>&#13;
			<p class="source-code">        epochs=30,</p>&#13;
			<p class="source-code">        callbacks=[tf.keras.callbacks.EarlyStopping(</p>&#13;
			<p class="source-code">                                              'val_accuracy')])</p>&#13;
			<p>And the rest of it, such as retrieving the best hyperparameter configuration and training the model with this configuration, is all the same as in the <em class="italic">Hyperband</em> section.</p>&#13;
			<h2 id="_idParaDest-101"><a id="_idTextAnchor191"/>Random search</h2>&#13;
			<p>Random search is<a id="_idIndexMarker318"/> simply a random selection<a id="_idIndexMarker319"/> of the hyperparameter configuration search space. Here's an example definition:</p>&#13;
			<p class="source-code">tuner = kt.RandomSearch(</p>&#13;
			<p class="source-code">            hypermodel = model_builder, </p>&#13;
			<p class="source-code">            objective='val_accuracy',</p>&#13;
			<p class="source-code">            max_trials = 5,</p>&#13;
			<p class="source-code">            directory = flags_obj.model_dir,</p>&#13;
			<p class="source-code">            project_name = 'hp_tune_rs',</p>&#13;
			<p class="source-code">            overwrite = True)</p>&#13;
			<p>In the <strong class="source-inline">RandomSearch</strong> API in the preceding code, we define the <strong class="source-inline">model_builder</strong> function as <strong class="source-inline">hypermodel</strong>. This function contains our hyperparameter objects that hold definitions for the hyperparameter name and search space. <strong class="source-inline">hypermodel</strong> specifies the name of our function, which will accept the best hyperparameters found by the search and use these values to build a model. Our objective is to find the best set of hyperparameters<a id="_idIndexMarker320"/> that maximizes validation <a id="_idIndexMarker321"/>accuracy, and we set <strong class="source-inline">max_trials</strong> to <strong class="source-inline">5</strong>. The directory to save the model is provided as user input. The user input for <strong class="source-inline">model_dir</strong> is captured by the <strong class="source-inline">flags_obj.model_dir</strong> object.</p>&#13;
			<p class="callout-heading">A few words about <strong class="source-inline">directory</strong></p>&#13;
			<p class="callout">The <strong class="source-inline">directory</strong> argument is required in all three types of algorithm. It is the target where search results will be stored. This argument accepts a text string and is very flexible. The text string may indicate text passed by the input flag (as in the case of <strong class="source-inline">flags_obj.model_dir</strong>) when this code is run as a script. Alternatively, if you are using a notebook environment, the text string may be a fil<a id="_idTextAnchor192"/><a id="_idTextAnchor193"/>e path or a cloud storage bucket path. </p>&#13;
			<h1 id="_idParaDest-102"><a id="_idTextAnchor194"/>Submitting tuning jobs in a local environment</h1>&#13;
			<p>Since the<a id="_idIndexMarker322"/> hyperparameter tuning process is inherently<a id="_idIndexMarker323"/> time-consuming, it is more practical to run it from a script rather than in a notebook environment. Also, although in a sense, a hyperparameter tuning process consists of multiple model training jobs, the tuner API and search workflow require a certain code refactoring style. The most obvious point is that we must wrap the model structure around a function (in our example, a function named <strong class="source-inline">model_builder</strong>), whose signature indicates that hyperparameter arrays are expected to be referenced in the model structure. </p>&#13;
			<p>You may find the code and instructions in the GitHub repository: <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_06/localtuningwork">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_06/localtuningwork</a></p>&#13;
			<p>With the help of the following code, we will set up user inputs or flags and perhaps assign default values to these flags when necessary. Let's have a quick review of how user inputs may be handled and defined in the Python <strong class="source-inline">script.absl</strong> library, and the APIs that are commonly used for handling user input:</p>&#13;
			<ol>&#13;
				<li value="1">Import the <strong class="source-inline">absl</strong> library and the relevant APIs:<p class="source-code">from absl import flags</p><p class="source-code">from absl import logging</p><p class="source-code">from absl import app</p></li>&#13;
				<li>Next, we will use the following lines of code to indicate user inputs or flags:<p class="source-code">tf.compat.v1.flags.DEFINE_string('model_dir', 'default_model_dir', 'Directory or bucket for storing checkpoint model.')</p><p class="source-code">tf.compat.v1.flags.DEFINE_bool('fine_tuning_choice', False, 'Retrain base parameters')</p><p class="source-code">tf.compat.v1.flags.DEFINE_integer('train_batch_size', 32, 'Number of samples in a training batch')</p><p class="source-code">tf.compat.v1.flags.DEFINE_integer('validation_batch_size', 40, 'Number of samples in a validation batch')</p><p class="source-code">tf.compat.v1.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')</p><p class="source-code">tf.compat.v1.flags.DEFINE_string('tuner_type', 'Hyperband', 'Type of tuner. Default is hyperband')</p><p>The first<a id="_idIndexMarker324"/> argument is the name of the input flag, followed<a id="_idIndexMarker325"/> by its default value, then an explanation. The preceding examples demonstrate commonly used type casting to these flags: <strong class="source-inline">string</strong>, <strong class="source-inline">Boolean</strong>, <strong class="source-inline">integer</strong>, and <strong class="source-inline">float</strong>.</p></li>&#13;
				<li>In the code, how do we reference and make use of these flags? It turns out we need to use a <strong class="source-inline">flags.FLAGS</strong> object in the function where the input flags are used. This function could be <strong class="source-inline">main()</strong> or any function. In many cases, for convenience and readability, we will assign this object to a variable:<p class="source-code"> flags_obj = flags.FLAGS</p></li>&#13;
				<li>Now, to refer to <strong class="source-inline">model_dir</strong>, we just need to do the following:<p class="source-code">flags_obj.model_dir</p><p>This effectively decodes the object the and <strong class="source-inline">model_dir</strong> attribute as a text string. </p></li>&#13;
				<li>Now let's see an example script. We will start with the <strong class="source-inline">import</strong> statements to bring all the libraries we will need into the scope:<p class="source-code">import kerastuner as kt</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">import os</p><p class="source-code">import IPython</p><p class="source-code">from kerastuner import HyperParameters</p><p class="source-code">from absl import flags</p><p class="source-code">from absl import logging</p><p class="source-code">from absl import app</p></li>&#13;
				<li>Define the<a id="_idIndexMarker326"/> user input argument<a id="_idIndexMarker327"/> names, default values, and short explanations:<p class="source-code">tf.compat.v1.flags.DEFINE_string('model_dir', 'default_model_dir', 'Directory or bucket for storing checkpoint model.')</p><p class="source-code">tf.compat.v1.flags.DEFINE_bool('fine_tuning_choice', False, 'Retrain base parameters')</p><p class="source-code">tf.compat.v1.flags.DEFINE_integer('train_batch_size', 32, 'Number of samples in a training batch')</p><p class="source-code">tf.compat.v1.flags.DEFINE_integer('validation_batch_size', 40, 'Number of samples in a validation batch')</p></li>&#13;
				<li>Define a function for loading working data. In this case, we will load it directly from TensorFlow for convenience:<p class="source-code">def get_builtin_data():</p><p class="source-code">    data_dir = tf.keras.utils.get_file(</p><p class="source-code">'flower_photos', 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',</p><p class="source-code">    	untar=True)</p><p class="source-code">    return data_dir</p><p>This function <a id="_idIndexMarker328"/>invokes the <strong class="source-inline">tf.keras</strong> API to <a id="_idIndexMarker329"/>retrieve the built-in image data that comes with TensorFlow. It is hosted in Google's public-facing storage. It is compressed, so we need to set <strong class="source-inline">untar</strong> to <strong class="source-inline">True</strong>.</p></li>&#13;
				<li>We also create a function called <strong class="source-inline">make_generators</strong>. This is a function that we will use to make data generators to stream the image data into the model training process:<p class="source-code">def make_generators(data_dir, flags_obj):</p><p class="source-code">    BATCH_SIZE = flags_obj.train_batch_size</p><p class="source-code">    IMAGE_SIZE = (224, 224)</p><p class="source-code">    datagen_kwargs = dict(rescale=1./255, </p><p class="source-code">                                    validation_split=.20)</p><p class="source-code">    dataflow_kwargs = dict(target_size=IMAGE_SIZE, </p><p class="source-code">                           batch_size=BATCH_SIZE,</p><p class="source-code">                           interpolation='bilinear')</p><p class="source-code">    valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(</p><p class="source-code">        **datagen_kwargs)</p><p class="source-code">    valid_generator = valid_datagen.flow_from_directory(</p><p class="source-code">        data_dir, subset='validation', shuffle=False, **dataflow_kwargs)</p><p>The function in the preceding code accepts a data path and user input. <strong class="source-inline">train_batch_size</strong> is one of the user inputs. This value is used to define <strong class="source-inline">BATCH_SIZE</strong> in this<a id="_idIndexMarker330"/> function. The validation generator is<a id="_idIndexMarker331"/> created first. We may have different preferences for training data, such as the options for data augmentation.</p></li>&#13;
				<li>Let's continue with the <strong class="source-inline">make_generators</strong> function. In this example, by default we are not going to do data augmentation on the training data. At the end of this function, <strong class="source-inline">train_generator</strong> is returned alongside <strong class="source-inline">valid_generator</strong>:<p class="source-code">    do_data_augmentation = False </p><p class="source-code">    if do_data_augmentation:</p><p class="source-code">        train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(</p><p class="source-code">            rotation_range=40,</p><p class="source-code">            horizontal_flip=True,</p><p class="source-code">            width_shift_range=0.2, </p><p class="source-code">            height_shift_range=0.2,</p><p class="source-code">            shear_range=0.2, zoom_range=0.2,</p><p class="source-code">            **datagen_kwargs)</p><p class="source-code">    else:</p><p class="source-code">        train_datagen = valid_datagen</p><p class="source-code">        train_generator = train_datagen.flow_from_directory(</p><p class="source-code">            data_dir, subset='training', shuffle=True, **dataflow_kwargs)</p><p class="source-code">return train_generator, valid_generator</p><p>This function will create two generators: one for training data, the other one for cross-validation data. See <a href="B16070_04_Final_JM_ePub.xhtml#_idTextAnchor101"><em class="italic">Chapter 4</em></a>, <em class="italic">Reusable Models and Scalable Data Pipeline</em>, the <em class="italic">Creating a generator to feed image data at scale</em> section, for more details.</p></li>&#13;
				<li>Next, we<a id="_idIndexMarker332"/> define a function to retrieve the <a id="_idIndexMarker333"/>index to label mapping. As the model outputs a prediction, the prediction is in the form of an integer between <strong class="source-inline">0</strong> and <strong class="source-inline">4</strong>. Each integer corresponds to a class name of the flowers:<p class="source-code">def map_labels(train_generator):</p><p class="source-code">    labels_idx = (train_generator.class_indices)</p><p class="source-code">    idx_labels = dict((v,k) for k,v in labels_idx.items())</p><p class="source-code">return idx_labels</p><p>The function in the preceding code iterates through the flower type index and the corresponding flower type name, and creates a dictionary as a lookup. Now let's proceed towards building a model architecture.</p></li>&#13;
				<li>The following<a id="_idIndexMarker334"/> function builds the model <a id="_idIndexMarker335"/>architecture, as described in the <em class="italic">Hyperband</em> section:<p class="source-code">def model_builder(hp):</p><p class="source-code">os.environ['TFHUB_CACHE_DIR'] =      </p><p class="source-code">         '/Users/XXXXX/Downloads/imagenet_resnet_v2_50_feature_vector_4'</p><p class="source-code">    hp_units = hp.Int('units', min_value = 64, </p><p class="source-code">                              max_value = 256, step = 64)</p><p class="source-code">    IMAGE_SIZE = (224, 224)</p><p class="source-code">    model = tf.keras.Sequential([</p><p class="source-code">    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)), </p><p class="source-code">    hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4', trainable=False),</p><p class="source-code">    tf.keras.layers.Flatten(),</p><p class="source-code">    tf.keras.layers.Dense(units = hp_units, </p><p class="source-code">                                     activation = 'relu', </p><p class="source-code">                    kernel_initializer='glorot_uniform'),</p><p class="source-code">    tf.keras.layers.Dense(5, activation='softmax', </p><p class="source-code">                                   name = 'custom_class')</p><p class="source-code">    ])</p><p class="source-code">    model.build([None, 224, 224, 3])</p><p class="source-code">    hp_learning_rate = hp.Choice('learning_rate', </p><p class="source-code">                                   values = [1e-2, 1e-4])</p><p class="source-code">    model.compile(</p><p class="source-code">        optimizer=tf.keras.optimizers.SGD(</p><p class="source-code">                      lr=hp_learning_rate, momentum=0.5), </p><p class="source-code">        loss=tf.keras.losses.CategoricalCrossentropy(</p><p class="source-code">                  from_logits=True, label_smoothing=0.1),</p><p class="source-code">        metrics=['accuracy'])</p><p class="source-code">    return model</p></li>&#13;
				<li>Define an object to clear the screen as the hyperparameter search moves around the search space:<p class="source-code">class ClearTrainingOutput(tf.keras.callbacks.Callback):</p><p class="source-code">        def on_train_end(*args, **kwargs):</p><p class="source-code">            IPython.display.clear_output(wait = True)</p><p>This will help <a id="_idIndexMarker336"/>clear some of the printed output during<a id="_idIndexMarker337"/> the search process. This is passed into a callback.</p></li>&#13;
				<li>This is the main driver for the training script:<p class="source-code">def main(_):</p><p class="source-code">    flags_obj = flags.FLAGS</p><p class="source-code">    strategy = tf.distribute.MirroredStrategy()</p><p class="source-code">    data_dir = get_builtin_data()</p><p class="source-code">    train_gtr, validation_gtr = make_generators(data_dir, 	                                               flags_obj)</p><p class="source-code">    idx_labels = map_labels(train_gtr)</p><p>In the preceding code, we set the distributed training strategy, defined the data source, and created training and validation data generators. Also, label mapping is retrieved.</p></li>&#13;
			</ol>&#13;
			<p>In the following logical block of conditional code, we handle the choice for the hyperparameter search algorithm. All three choices are present: Bayesian optimization, random search, and Hyperband. The default choice is Hyperband. Within each<a id="_idIndexMarker338"/> choice, there is a <strong class="source-inline">hypermodel</strong> attribute. This<a id="_idIndexMarker339"/> attribute specifies the name of the function that will take up the best hyperparameters to build the model:</p>&#13;
			<p class="source-code">    '''Runs the hyperparameter search.'''</p>&#13;
			<p class="source-code">    </p>&#13;
			<p class="source-code">    if(flags_obj.tuner_type.lower() == 'BayesianOptimization'.lower()):</p>&#13;
			<p class="source-code">        tuner = kt.BayesianOptimization(</p>&#13;
			<p class="source-code">            hypermodel = model_builder,</p>&#13;
			<p class="source-code">            objective ='val_accuracy',</p>&#13;
			<p class="source-code">            tune_new_entries = True,</p>&#13;
			<p class="source-code">            allow_new_entries = True,</p>&#13;
			<p class="source-code">            max_trials = 5,</p>&#13;
			<p class="source-code">            directory = flags_obj.model_dir,</p>&#13;
			<p class="source-code">            project_name = 'hp_tune_bo',</p>&#13;
			<p class="source-code">            overwrite = True</p>&#13;
			<p class="source-code">            )</p>&#13;
			<p class="source-code">    elif (flags_obj.tuner_type.lower() == 'RandomSearch'.lower()):</p>&#13;
			<p class="source-code">        tuner = kt.RandomSearch(</p>&#13;
			<p class="source-code">            hypermodel = model_builder, </p>&#13;
			<p class="source-code">            objective='val_accuracy',</p>&#13;
			<p class="source-code">            tune_new_entries = True, </p>&#13;
			<p class="source-code">            allow_new_entries = True,</p>&#13;
			<p class="source-code">            max_trials = 5,</p>&#13;
			<p class="source-code">            directory = flags_obj.model_dir,</p>&#13;
			<p class="source-code">            project_name = 'hp_tune_rs',</p>&#13;
			<p class="source-code">            overwrite = True)</p>&#13;
			<p>Unless it's specified<a id="_idIndexMarker340"/> via input to use either<a id="_idIndexMarker341"/> Bayesian optimization or random search, the default choice is Hyperband. This is indicated in the <strong class="source-inline">else</strong> block in the following code:</p>&#13;
			<p class="source-code">else: </p>&#13;
			<p class="source-code">    # Default choice for tuning algorithm is hyperband.</p>&#13;
			<p class="source-code">        tuner = kt.Hyperband(</p>&#13;
			<p class="source-code">            hypermodel = model_builder,</p>&#13;
			<p class="source-code">            objective = 'val_accuracy', </p>&#13;
			<p class="source-code">            max_epochs = 3,</p>&#13;
			<p class="source-code">            factor = 2,</p>&#13;
			<p class="source-code">            distribution_strategy=strategy,</p>&#13;
			<p class="source-code">            directory = flags_obj.model_dir,</p>&#13;
			<p class="source-code">            project_name = 'hp_tune_hb',</p>&#13;
			<p class="source-code">            overwrite = True)</p>&#13;
			<p>Now the search algorithm is executed based on the logic of the preceding code; we need to pass the best hyperparameters. For our own information, we may use the <strong class="source-inline">get_gest_hyperparameters</strong> API to print out the best hyperparameters. We <a id="_idIndexMarker342"/>will get the optimal <a id="_idIndexMarker343"/>hyperparameters with the help of the following code:</p>&#13;
			<p class="source-code">    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]</p>&#13;
			<p class="source-code">    print(f'''</p>&#13;
			<p class="source-code">        The hyperparameter search is done. </p>&#13;
			<p class="source-code">        The best number of nodes in the dense layer is {best_hps.get('units')}.</p>&#13;
			<p class="source-code">        The optimal learning rate for the optimizer is       {best_hps.get('learning_rate')}.</p>&#13;
			<p class="source-code">        ''')</p>&#13;
			<p>Now we can pass these best hyperparameters, <strong class="source-inline">best_hp</strong>, to the model and train the model with these values. The <strong class="source-inline">tuner.hypermodel.build</strong> API handles the passing of these values to the model.</p>&#13;
			<p>In the following code, we will set up the training and validation data batches, create a callback object, and start the training with the <strong class="source-inline">fit</strong> API:</p>&#13;
			<p class="source-code">    # Build the model with the optimal hyperparameters and train it on the data</p>&#13;
			<p class="source-code">    model = tuner.hypermodel.build(best_hps)</p>&#13;
			<p class="source-code">    checkpoint_prefix = os.path.join(flags_obj.model_dir, 'best_hp_train_ckpt_{epoch}')</p>&#13;
			<p class="source-code">    callbacks = [</p>&#13;
			<p class="source-code">    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,</p>&#13;
			<p class="source-code">                                       save_weights_only=True)]</p>&#13;
			<p class="source-code">    steps_per_epoch = train_gtr.samples // train_gtr.batch_size</p>&#13;
			<p class="source-code">    validation_steps = validation_gtr.samples // validation_gtr.batch_size</p>&#13;
			<p class="source-code">    model.fit(</p>&#13;
			<p class="source-code">        train_gtr,</p>&#13;
			<p class="source-code">        epochs=3, steps_per_epoch=steps_per_epoch,</p>&#13;
			<p class="source-code">        validation_data=validation_gtr,</p>&#13;
			<p class="source-code">        validation_steps=validation_steps,</p>&#13;
			<p class="source-code">        callbacks=callbacks)</p>&#13;
			<p>Here, we log the<a id="_idIndexMarker344"/> output of the destination<a id="_idIndexMarker345"/> directory for the saved model on screen:</p>&#13;
			<p class="source-code">logging.info('INSIDE MAIN FUNCTION user input model_dir %s', 	                                           flags_obj.model_dir)</p>&#13;
			<p class="source-code">    # Save model trained with chosen HP in user specified bucket location</p>&#13;
			<p class="source-code">    model_save_dir = os.path.join(flags_obj.model_dir, </p>&#13;
			<p class="source-code">                                             'best_save_model')</p>&#13;
			<p class="source-code">    model.save(model_save_dir)</p>&#13;
			<p class="source-code">&#13;
</p>&#13;
			<p class="source-code">if __name__ == '__main__':</p>&#13;
			<p class="source-code">    app.run(main)</p>&#13;
			<p>To run this as a script (<strong class="source-inline">hp_kt_resnet_local.py</strong>), you could simply invoke it with the following command:</p>&#13;
			<p class="source-code">python3 hp_kt_resnet_local_pub.py \</p>&#13;
			<p class="source-code">--model_dir=resnet_local_hb_output  \</p>&#13;
			<p class="source-code">--train_epoch_best=2 \</p>&#13;
			<p class="source-code">--tuner_type=hyperband</p>&#13;
			<p>In the preceding <a id="_idIndexMarker346"/>command, we invoke the <strong class="source-inline">python3</strong> runtime<a id="_idIndexMarker347"/> to execute our training script, <strong class="source-inline">hp_kt_resnet_local.py</strong>. <strong class="source-inline">model_dir</strong> is the place we wish to save the model. <strong class="source-inline">Tuner_type</strong> designates the selection of the hyperparameter search algorithm. Other algorithm <a id="_idTextAnchor195"/>choices you may try are <em class="italic">Bayesian optimization and random search</em>. </p>&#13;
			<h1 id="_idParaDest-103"><a id="_idTextAnchor196"/>Submitting tuning jobs in Google's AI Platform</h1>&#13;
			<p>Now we are ready<a id="_idIndexMarker348"/> to use Google's AI Platform <a id="_idIndexMarker349"/>to perform hyperparameter training. You may download everything you need from the GitHub repository for this chapter. For the AI Platform code in this section, you can refer to the <strong class="source-inline">gcptuningwork</strong> file in this chapter's folder in the GitHub repository for the book.</p>&#13;
			<p>In the cloud, we have access to powerful machines that can speed up our search process. Overall, the approach we will leverage is very similar to what we saw in the previous section about submitting a local Python script training job. We will use the <strong class="source-inline">tf.compat.v1.flag</strong> method to handle user input or flags. The rest of the script follows a similar structure, with the exception of data handling, because we will use <strong class="source-inline">TFRecord</strong> instead of <strong class="source-inline">ImageGenerator</strong> and a conditional flag for the distributed training strategy.</p>&#13;
			<p>Since the tuning job is submitted to AI Platform from a remote node (that is, your local compute environment), some <a id="_idIndexMarker350"/>prerequisites <a id="_idIndexMarker351"/>need to be met (see <a href="B16070_05_Final_JM_ePub.xhtml#_idTextAnchor145"><em class="italic">Chapter 5</em></a>, <em class="italic">Training at Scale</em>):</p>&#13;
			<ol>&#13;
				<li value="1">In the directory where the tuning job will be invoked, <strong class="source-inline">setup.py</strong> needs to be updated to include <strong class="source-inline">keras-tuner</strong>. And while we are at it, let's also add IPython. So, edit the <strong class="source-inline">setup.py</strong> file as follows:<p class="source-code">from setuptools import find_packages</p><p class="source-code">from setuptools import setup</p><p class="source-code">setup(</p><p class="source-code">    name='official',</p><p class="source-code">    install_requires=['IPython', 'keras-tuner', 'tensorflow-datasets~=3.1', 'tensorflow_hub&gt;=0.6.0'],</p><p class="source-code">    packages=find_packages()</p><p class="source-code">)</p><p>This is the entire content of <strong class="source-inline">setup.py</strong>. In this file, we specify the libraries we need for our training job and instruct the runtime to find these libraries with the <strong class="source-inline">find_packages</strong> function.</p></li>&#13;
				<li>You are now ready to submit a tuning job. In the following command, the job is submitted to Cloud TPU to run in the distributed training strategy:<p class="source-code"><strong class="bold">gcloud ai-platform jobs submit training hp_kt_resnet_tpu_hb_test \</strong></p><p class="source-code"><strong class="bold">--staging-bucket=gs://ai-tpu-experiment \</strong></p><p class="source-code"><strong class="bold">--package-path=tfk \</strong></p><p class="source-code"><strong class="bold">--module-name=tfk.tuner.hp_kt_resnet_tpu_act \</strong></p><p class="source-code"><strong class="bold">--runtime-version=2.2 \</strong></p><p class="source-code"><strong class="bold">--python-version=3.7 \</strong></p><p class="source-code"><strong class="bold">--scale-tier=BASIC_TPU \</strong></p><p class="source-code"><strong class="bold">--region=us-central1 \</strong></p><p class="source-code"><strong class="bold">--use-chief-in-tf-config='true' \</strong></p><p class="source-code"><strong class="bold">-- \</strong></p><p class="source-code"><strong class="bold">--distribution_strategy=tpu \</strong></p><p class="source-code"><strong class="bold">--data_dir=gs://ai-tpu-experiment/tfrecord-flowers \</strong></p><p class="source-code"><strong class="bold">--model_dir=gs://ai-tpu-experiment/hp_kt_resnet_tpu_hb_test \</strong></p><p class="source-code"><strong class="bold">--tuner_type=hyperband</strong></p></li>&#13;
			</ol>&#13;
			<p>Notice the<a id="_idIndexMarker352"/> separator <strong class="source-inline">-- \</strong> in the preceding <a id="_idIndexMarker353"/>code, before <strong class="source-inline">-- \</strong>, they are Google Cloud-specific arguments. We submit a training script from our environment to Cloud TPU. For the training script, we need to specify the package path and module name. The Python version and TensorFlow runtime are also selected. We will use <strong class="source-inline">BASIC_TPU</strong> in the <strong class="source-inline">us-central1</strong> region.</p>&#13;
			<p>After <strong class="source-inline">-- \</strong> are the custom arguments for training scripts. These arguments are defined for and used in the training script. We designated the value <strong class="source-inline">tpu</strong> for our choice of distribution training strategy. Further, the training data location is designated by <strong class="source-inline">data_dir</strong>. And once the training job is done, the model will be saved in <strong class="source-inline">model_dir</strong>. Finally, we select <strong class="source-inline">HYPERBAND</strong> as our <strong class="source-inline">tuner_type</strong> for the hyperparameter tuning algorithm.</p>&#13;
			<p>And from the current directory, where the preceding command is invoked, the training script is stored in the <strong class="source-inline">/python/ScriptProject/hp_kt_resnet_tpu_act.py</strong> folder.</p>&#13;
			<p>This training script performs a hyperparameter search for two hyperparameters: the number of units in a middle <strong class="source-inline">Dense</strong> layer of our image classification model, and the activation function. The added <strong class="source-inline">tuner_type</strong> flag lets the user select the algorithm: Hyperband, Bayesian optimization, or random search. Once the search is completed, it then trains the model<a id="_idIndexMarker354"/> with the best hyperparameter<a id="_idIndexMarker355"/> configuration and saves the model to a storage bucket. </p>&#13;
			<p class="callout-heading">Note</p>&#13;
			<p class="callout">The code is lengthy, so you can find the entire code and instructions in the following GitHub repository: <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_06/gcptuningwork">https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_06/gcptuningwork</a>.</p>&#13;
			<p class="callout">The main driver script for training is available at <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_06/gcptuningwork/tfk/tuner/hp_kt_resnet_tpu_act.py">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_06/gcptuningwork/tfk/tuner/hp_kt_resnet_tpu_act.py</a>.</p>&#13;
			<p>Once the training is completed, you will see an outpu<a id="_idTextAnchor197"/>t in the cloud storage specified by <strong class="source-inline">model_dir</strong> as shown in Figure 6.1:</p>&#13;
			<div>&#13;
				<div id="_idContainer108" class="IMG---Figure">&#13;
					<img src="Images/image0012.jpg" alt="Figure 6.1 – Hyperparameter tuning and training job output&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 6.1 – Hyperparameter tuning and training job output</p>&#13;
			<p>In the storage bucket, there <a id="_idIndexMarker356"/>are the model assets saved from training using the best hyperparameter<a id="_idIndexMarker357"/> configuration in the <strong class="source-inline">best_save_model</strong> folder. Further, we can see that each trial of the hyperparameter tuning workflow is also saved in the <strong class="source-inline">hp_tune_hb</strong> folder.</p>&#13;
			<p>Of all the search algorithms, Hyperband is the newest approach and offers an  effective and efficient search experience based on an exploitation-exploration strategy. It is often the fastest algorithm to converge to a winning configuration. From the hardware choice perspective, for this example, Cloud TPU offers the shortest runtime. However, since hyperparameter search is inherently a time-consuming process, data size and data I/O also are also important factors that impact the speed of the search. Sometimes it is better to start with a smaller dataset or smaller search space to eliminate some selections from further analysis.</p>&#13;
			<h1 id="_idParaDest-104"><a id="_idTextAnchor198"/>Summary</h1>&#13;
			<p>In this chapter, we learned how to use Keras Tuner in Google Cloud AI Platform. We learned how to run the hyperparameter search, and we learned how to train a model with the best hyperparameter configuration. We have also seen that in a typical Keras style, integrating Keras Tuner into our existing model training workflow is very easy, especially with the simple treatment of hyperparameters as just arrays of a certain data type. This really opens up the choices for hyperparameters, and we do not need to implement the search logic or complicated conditional loops to keep track of the results. </p>&#13;
			<p>In the next chapter, we will see the latest model optimization techniques that reduce the model size. As a result, our model can be leaner and more compact. </p>&#13;
		</div>&#13;
	</div></body></html>