<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Representation Learning - Implementing Word Embeddings</h1>
                </header>
            
            <article>
                
<p class="calibre2">Machine learning is a science that is mainly based on statistics and linear algebra. Applying matrix operations is very common among most machine learning or deep learning architectures because of backpropagation. This is the main reason deep learning, or machine learning in general, accepts <span class="calibre10">only </span>real-valued quantities as input. This fact contradicts many applications, such as machine translation, sentiment analysis, and so on; they have text as an input. So, in order to use deep learning for this application, we need to have it in the form that deep learning accepts!</p>
<p class="calibre2">In this chapter, we are going to introduce the field of representation learning, which is a way to learn a real-valued representation from text while preserving the semantics of the actual text. For example, the representation of love should be very close to the representation of adore because they are used in very similar contexts.</p>
<p class="calibre2">So, the following topics will be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">Introduction to representation learning</li>
<li class="calibre8">Word2Vec</li>
<li class="calibre8">A practical example of the skip-gram architecture</li>
<li class="calibre8">Skip-gram Word2Vec implementation</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to representation learning</h1>
                </header>
            
            <article>
                
<p class="calibre2">All the machine learning algorithms or architectures that we have used so far require the input to be real-valued or matrices of real-valued quantities, and that's a common theme in machine learning. For example, in the convolution neural network, we had to feed raw pixel values of images as model inputs. In this part, we are dealing with text, so we need to encode our text somehow and produce real-valued quantities that can be fed to a machine learning algorithm. In order to encode input text as real-valued quantities, we need to use an intermediate science called <strong class="calibre13">Natural Language Processing</strong> (<strong class="calibre13">NLP</strong>).</p>
<p class="calibre2">We mentioned that in this kind of pipeline, where we feed text to a machine learning model such as sentiment analysis, this will be problematic and won't work because we won't be able to apply backpropagation or any other operations such as dot product on the input, which is a string. So, we need to use a mechanism of NLP that will enable us to build an intermediate representation of the text that can carry the same information as the text and also be fed to the machine learning models.</p>
<p class="calibre2">We need to convert each word or token in the input text to a real-valued vector. These vectors will be useless if they don't carry the patterns, information, meaning, and semantics of the original input. For example, as in real text, the two words love and adore are very similar to each other and carry the same meaning. We need the resultant real-valued vectors that will represent them to be close to each other and be in the same vector space. So, the vector representation of these two words along with another word that isn't similar to them will be like this diagram:</p>
<div class="CDPAlignCenter"><img src="assets/b7e0177f-a510-4ef9-9034-667ec7828f6b.png" class="calibre137"/></div>
<div class="CDPAlignCenter1">Figure 15.1: Vector representation of words</div>
<p class="calibre2">There are many techniques that can be used for this task. This family of techniques is called <strong class="calibre13">embeddings</strong>, where you're embedding text into another real-valued vector space.</p>
<p class="calibre2">As we'll see later on, this vector space is very interesting actually, because you will find out that you can drive a word's vectors from other words that are similar to it or even do some geography in this space.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Word2Vec</h1>
                </header>
            
            <article>
                
<p class="calibre2">Word2Vec is one of the widely used embedding techniques in the area of NLP. This model creates real-valued vectors from input text by looking at the contextual information the input word appears in. So, you will find out that similar words will be mentioned in very similar contexts, and hence the model will learn that those two words should be placed close to each other in the particular embedding space.</p>
<p class="calibre2">From the statements in the following diagram, the model will learn that the words <strong class="calibre13">love</strong> and <strong class="calibre13">adore</strong> share very similar contexts and should be placed very close to each other in the resulting vector space. The <span class="calibre10">context of </span>like could be a bit similar as well to the word love, but it won't be as close to love as the word adore:</p>
<div class="CDPAlignCenter"><img src="assets/f0ad5b13-4057-486b-ae7d-90aac51560df.png" class="calibre138"/></div>
<div class="CDPAlignCenter1">Figure 15.2: Sample of sentiment sentences</div>
<p class="calibre2">The Word2Vec model also relies on semantic features of input sentences; for example, the two words adore and love are mainly used in a positive context and usually precede noun phrases or nouns. Again, the model will learn that these two words have something in common and it will be more likely to put the vector representation of these two vectors in a similar context. So, the structure of the sentence will tell the Word2Vec model a lot about similar words.</p>
<p class="calibre2">In practice, people feed a large corpus of text to the Word2Vec model. The model will learn to produce similar vectors for similar words, and it will do so for each unique word in the input text.</p>
<p class="calibre2">All of these words' vectors will be combined and the final output will be an embedding matrix where each row represents the real-valued vector representation of a specific unique word.</p>
<div class="CDPAlignCenter"><img src="assets/312e4a86-0742-47ae-9db6-ab700a6d4376.png" class="calibre139"/></div>
<div class="CDPAlignCenter1">Figure 15.3: Example of Word2Vec model pipeline</div>
<p class="calibre2">So, the final output of the model will be an embedding matrix for all the unique words in the training corpus. Usually, good embedding matrices could contain millions of real-valued vectors.<br class="calibre20"/>
Word2Vec modeling uses a window to scan the sentence and then tries to predict the vector of the middle word of that window based on its contextual information; the Word2Vec model will scan a sentence at a time. Similar to any machine learning technique, we need to define a cost function for the Word2Vec model and its corresponding optimization criteria that will make the model capable of generating real-valued vectors for each unique image and also relate the vectors to each other based on their contextual information</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building Word2Vec model</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we will go through some deeper details of how can we build a Word2Vec model. As we mentioned previously, our final goal is to have a trained model that will able to generate real-valued vector representation for the input textual data which is also called word embeddings.</p>
<p class="innercell">During the training of the model, we will use the maximum likelihood method (<a href="https://en.wikipedia.org/wiki/Maximum_likelihood" target="_blank" class="calibre11">https://en.wikipedia.org/wiki/Maximum_likelihood</a>), which can be used to maximize the probability of the next word <em class="calibre19">w<sub class="calibre28">t</sub></em> in the input sentence given the previous words that the model has seen, which we can call <em class="calibre19">h</em>.</p>
<p class="innercell">This maximum likelihood method will be expressed in terms of the softmax function:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation23" src="assets/8312e4a7-cfe0-47dd-9b24-42670c3aca92.png"/></div>
<p class="calibre2">Here, the <em class="calibre19">score</em> function computes a value to represent the compatibility of the target word <em class="calibre19">w<sub class="calibre28">t</sub></em> with respect to the context <em class="calibre19">h</em>. This model will be trained on the input sequences while training to maximize the likelihood<span class="calibre10"> </span><span class="calibre10">on the training input data</span><span class="calibre10"> (log likelihood is used for mathematical simplicity and derivation with the log):</span></p>
<div class="CDPAlignCenter"><img class="fm-editor-equation24" src="assets/ad309d16-c7ef-4cef-9e0e-f3b791538f1d.png"/></div>
<p class="calibre2">So, the <em class="calibre19">ML</em> method will try to maximize the above equation which, will result in a probabilistic language model. But the calculation of this is very computationally expensive, as we need to compute each probability using the score function for all the words in the<br class="calibre20"/>
vocabulary <em class="calibre19">V</em> words <em class="calibre19">w'</em>, in the corresponding current context <em class="calibre19">h</em> of this model. This will happen at every training step.</p>
<div class="CDPAlignCenter"><img src="assets/202fe239-8362-46dc-ab60-3a54281cf6cd.png" class="calibre140"/></div>
<div class="CDPAlignCenter1">Figure 15.4: General architecture of a probabilistic language model</div>
<p class="innercell">Because of the computational expensiveness of building the probabilistic language model, people tend to use different techniques that are less computationally expensive, such as <strong class="calibre13">Continuous Bag-of-Words</strong> (<strong class="calibre13">CBOW</strong>) and skip-gram models.</p>
<p class="innercell">These models are trained to build a binary classification with logistic regression to separate between the real target words <em class="calibre19">w<sub class="calibre28">t</sub></em> and <em class="calibre19">h</em> noise or imaginary words <img class="fm-editor-equation25" src="assets/91f9ba2d-f6dc-4587-b482-b348d4b4560b.png"/><strong class="calibre13">,</strong> which is in the same context. The following diagram simplifies this idea using the CBOW technique:</p>
<div class="CDPAlignCenter"><img src="assets/4663156d-d15c-4994-8fa0-9942283b6244.png" class="calibre141"/></div>
<div class="CDPAlignCenter1">Figure 15.5: General architecture of skip-gram model</div>
<p class="calibre2">The next diagram, shows the two architectures that you can use for building the Word2Vec model:<br class="calibre20"/></p>
<div class="CDPAlignCenter"><img src="assets/6ec47f15-c055-4e80-8173-a86d0670dc70.png" class="calibre142"/></div>
<div class="CDPAlignCenter1">Figure 15.6: different architectures for the Word2Vec model</div>
<p class="calibre2">To be more formal, the objective function of these techniques maximizes the following:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation26" src="assets/bbd635d1-b9bd-42b3-915e-d6fcdd476168.png"/></div>
<p class="calibre2">Where:</p>
<ul class="calibre7">
<li class="calibre8"><strong class="calibre1"><img class="fm-editor-equation27" src="assets/e3e0bbb6-3b7d-4a79-bcfa-1311153492a7.png"/></strong> is the probability of the binary logistic regression based on the model seeing the word <em class="calibre25">w</em> in the context <em class="calibre25">h</em> in the dataset <em class="calibre25">D</em><strong class="calibre1">,</strong> which is calculated in terms of the θ vector. This vector represents the learned embeddings.</li>
<li class="calibre8"><img class="fm-editor-equation28" src="assets/9653f073-6172-4710-800d-692f0ca28fbf.png"/>is the imaginary or noisy words that we can generate from a noisy probabilistic distribution, such as the unigram of the training input examples.</li>
</ul>
<p class="calibre2">To sum up, the objective of these models is to discriminate between real and imaginary inputs, and hence assign higher probability to real words and less probability for the case of imaginary or noisy words.</p>
<p class="calibre2">This objective is maximized when the model assigns high probabilities to real words and low probabilities to noise words.</p>
<div class="packtinfobox">Technically, the process of assigning high probability to real words is is called <strong class="calibre1">negative sampling</strong> (<a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" class="calibre11">https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</a>), and there is good mathematical motivation for using this loss function: the updates it proposes approximate the updates of the softmax function in the limit. But computationally, it is especially appealing because computing the loss function now scales only with the number of noise words that we select (<em class="calibre25">k</em>), and not all words in the vocabulary (<em class="calibre25">V</em>). This makes it much faster to train. We will actually make use of the very similar <strong class="calibre1">noise-contrastive estimation</strong> (<strong class="calibre1">NCE</strong>) (<a href="https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf" target="_blank" class="calibre11">https://papers.nips.cc/paper/5165-learning-word-embeddings-efficiently-with-noise-contrastive-estimation.pdf</a>) loss, for which TensorFlow has a handy helper function, <kbd class="calibre12">tf.nn.nce_loss()</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A practical example of the skip-gram architecture</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's go through a practical example and see how skip-gram models will work in this situation:</p>
<pre class="calibre21">the quick brown fox jumped over the lazy dog</pre>
<p class="calibre2">First off, we need to make a dataset of words and their corresponding context. Defining the context is up to us, but it has to make sense. So, we'll take a window around the target word and take a word from the right and another from the left.</p>
<p class="calibre2">By following this contextual technique, we will end up with the following set of words and their corresponding context:</p>
<pre class="calibre21">([the, brown], quick), ([quick, fox], brown), ([brown, jumped], fox), ...</pre>
<p class="calibre2">The generated words and their corresponding context will be represented as pairs of <kbd class="calibre12">(context, target)</kbd>. The idea of skip-gram models is the inverse of CBOW ones. In the skip- gram model, we will try to predict the context of the word based on its target word. For example, considering the first pair, the skip-gram model will try to predict <kbd class="calibre12">the</kbd> and <kbd class="calibre12">brown</kbd> from the target word <kbd class="calibre12">quick</kbd>, and so on. So, we can rewrite our dataset as follows:</p>
<pre class="calibre21">(quick, the), (quick, brown), (brown, quick), (brown, fox), ...</pre>
<p class="calibre2">Now, we have a set of input and output pairs.</p>
<p class="calibre2">Let's try to mimic the training process at specific step <em class="calibre19">t</em>. So, the skip-gram model will take the first training sample where the input is the word <kbd class="calibre12">quick</kbd> and the target output is the word <kbd class="calibre12">the</kbd>. Next, we need to construct the noisy input as well, so we are going to randomly select from the unigrams of the input data. For simplicity, the size of the noisy vector will be only one. For example, we can select the word <kbd class="calibre12">sheep</kbd> as a noisy example.</p>
<p class="calibre2">Now, we can go ahead and compute the loss between the real pair and the noisy one as:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation29" src="assets/01d022e1-f1ed-4382-9b53-b3ecb87473e9.png"/></div>
<p class="calibre2">The goal in this case is to update the θ parameter to improve the previous objective function. Typically, we can use gradient for this. So, we will try to calculate the gradient of the loss with respect to the objective function parameter θ, which will be represented by <img class="fm-editor-equation30" src="assets/534a4675-7ec2-4f49-8ff8-7e28502eaabf.png"/>.</p>
<p class="calibre2">After the training process, we can visualize some results based on their reduced dimensions of the real-valued vector representation. You will find that this vector space is very interesting because you can do lots of interesting stuff with it. For example, you can learn Analogy in this space by saying that king is to queen as man is to woman. We can even derive the woman vector by subtracting the king vector from the queen one and adding the man; the result of this will be very close to the actual learned vector of the woman. You can also learn geography in this space.</p>
<div class="CDPAlignCenter"><img src="assets/0a72b1e7-003f-43b2-9360-c26ff8e4fc94.png" class="calibre143"/></div>
<div class="CDPAlignCenter1">Figure 15.7: Projection of the learned vectors to two dimensions using t-distributed stochastic neighbor embedding (t-SNE) dimensionality reduction technique</div>
<p class="calibre2">The preceding example gives very good intuition behind these vectors and how they'll be useful for most NLP applications such as machine translation or <strong class="calibre13">part-of-speech</strong> (<strong class="calibre13">POS</strong>) <span class="calibre10">tagging</span>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Skip-gram Word2Vec implementation</h1>
                </header>
            
            <article>
                
<p class="calibre2">After understanding the mathematical details of how skip-gram models work, we are going to implement skip-gram, which encodes words into real-valued vectors that have certain properties (hence the name Word2Vec). By implementing this architecture, you will get a clue of how the process of learning another representation works.</p>
<p class="calibre2">Text is the main input for a lot of natural language processing applications such as machine translation, sentiment analysis, and text to speech systems. So, learning a real-valued representation for the text will help us use different deep learning techniques for these tasks.</p>
<p class="calibre2">In the early chapters of this book, we introduced something called one-hot encoding, which produces a vector of zeros except for the index of the word that this vector represents. So, you may wonder why we are not using it here. This method is very inefficient because usually you have a big set of distinct words, maybe something like 50,000 words, and using one-hot encoding for this will produce a vector of 49,999 entries set to zero and only one entry set to one.</p>
<p class="calibre2">Having a very sparse input like this will result in a huge waste of computation because of the matrix multiplications that we'd do in the hidden layers of the neural network.</p>
<div class="CDPAlignCenter"><img src="assets/af1fa91e-a31e-42f8-bbce-edde8e8bb703.png" class="calibre144"/></div>
<div class="CDPAlignCenter1">Figure 15.8: One-hot encoding which will result in huge waste of computation</div>
<p class="calibre2">As we mentioned previously, the outcome of using one-hot encoding will be a very sparse vector, especially when you have a huge amount of distinct words that you want to encode.</p>
<p class="calibre2">The following figure shows that when we multiply this sparse vector of all zeros except for one entry by a matrix of weights, the output will be only the row of the matrix that corresponds to the one value of the sparse vector:</p>
<div class="CDPAlignCenter"><img src="assets/a3c6bad7-0f91-483c-aa1f-74c6c20a6a2f.png" class="calibre145"/></div>
<div class="CDPAlignCenter1">Figure 15.9: The effect of multiplying a one-hot vector with almost all zeros by hidden layer weight matrix</div>
<p class="calibre2">To avoid this huge waste of computation, we will be using embeddings, which is just a fully-connected layer with some embedding weights. In this layer, we skip this inefficient multiplication and look up the embedding weights of the embedding layer from something called <strong class="calibre13">weight matrix</strong>.</p>
<p class="calibre2">So, instead of the waste that results from the computation, we are going to use this weight lookup this weight matrix to find the embedding weights. First, need to build this lookup take. To do this, we are going to encode all the input words as integers, as shown in the following figure, and then to get the corresponding values for this word, we are going to use its integer representation as the row number in this weight matrix. The process of finding the corresponding embedding values of a specific word is called <strong class="calibre13">embedding lookup.</strong> As mentioned previously, the embedding layer will be just a fully connected layer, where the number of units represents the embedding dimension.</p>
<div class="CDPAlignCenter"><img src="assets/489f39af-fc62-473d-9b21-b34bbb90ba1d.png" class="calibre146"/></div>
<div class="CDPAlignCenter1">Figure 15.10: Tokenized lookup table</div>
<p class="calibre2">You can see that this process is very intuitive and straightforward; we just need to follow these steps:</p>
<ol class="calibre16">
<li class="calibre8">Define the lookup table that will be considered as a weight matrix</li>
<li class="calibre8">Define the embedding layer as a fully connected hidden layer with specific number of units (embedding dimensions)</li>
<li class="calibre8">Use the weight matrix lookup as an alternative for the computationally unnecessary matrix multiplication</li>
<li class="calibre8">Finally, train the lookup table as any weight matrix</li>
</ol>
<p class="calibre2">As we mentioned earlier, we are going to build a skip-gram Word2Vec model in this section, which is an efficient way of learning a representation for words while preserving the semantic information that the words have.</p>
<p class="calibre2">So, let's go ahead and build a Word2Vec model using the skip-gram architecture, which is proven to better than others.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data analysis and pre-processing</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we are going to define some helper functions that will enable us to build a good Word2Vec model. For this implementation, we are going to use a cleaned version of Wikipedia (<a href="http://mattmahoney.net/dc/textdata.html" target="_blank" class="calibre11">http://mattmahoney.net/dc/textdata.html</a>).</p>
<p class="calibre2">So, let's start off by importing the required packages for this implementation:</p>
<pre class="calibre21">#importing the required packages for this implementation<br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/>#Packages for downloading the dataset<br class="title-page-name"/>from urllib.request import urlretrieve<br class="title-page-name"/>from os.path import isfile, isdir<br class="title-page-name"/>from tqdm import tqdm<br class="title-page-name"/>import zipfile<br class="title-page-name"/><br class="title-page-name"/>#packages for data preprocessing<br class="title-page-name"/>import re<br class="title-page-name"/>from collections import Counter<br class="title-page-name"/>import random</pre>
<p class="calibre2">Next up, we are going to define a class that will be used to download the dataset if it was not downloaded before:</p>
<pre class="calibre21"># In this implementation we will use a cleaned up version of Wikipedia from Matt Mahoney.<br class="title-page-name"/># So we will define a helper class that will helps to download the dataset<br class="title-page-name"/>wiki_dataset_folder_path = 'wikipedia_data'<br class="title-page-name"/>wiki_dataset_filename = 'text8.zip'<br class="title-page-name"/>wiki_dataset_name = 'Text8 Dataset'<br class="title-page-name"/><br class="title-page-name"/>class DLProgress(tqdm):<br class="title-page-name"/>    <br class="title-page-name"/>    last_block = 0<br class="title-page-name"/><br class="title-page-name"/>    def hook(self, block_num=1, block_size=1, total_size=None):<br class="title-page-name"/>        self.total = total_size<br class="title-page-name"/>        self.update((block_num - self.last_block) * block_size)<br class="title-page-name"/>        self.last_block = block_num<br class="title-page-name"/>        <br class="title-page-name"/># Cheking if the file is not already downloaded<br class="title-page-name"/>if not isfile(wiki_dataset_filename):<br class="title-page-name"/>    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=wiki_dataset_name) as pbar:<br class="title-page-name"/>        urlretrieve(<br class="title-page-name"/>            'http://mattmahoney.net/dc/text8.zip',<br class="title-page-name"/>            wiki_dataset_filename,<br class="title-page-name"/>            pbar.hook)<br class="title-page-name"/><br class="title-page-name"/># Checking if the data is already extracted if not extract it<br class="title-page-name"/>if not isdir(wiki_dataset_folder_path):<br class="title-page-name"/>    with zipfile.ZipFile(wiki_dataset_filename) as zip_ref:<br class="title-page-name"/>        zip_ref.extractall(wiki_dataset_folder_path)<br class="title-page-name"/>        <br class="title-page-name"/>with open('wikipedia_data/text8') as f:<br class="title-page-name"/>    cleaned_wikipedia_text = f.read()<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/><br class="title-page-name"/>Text8 Dataset: 31.4MB [00:39, 794kB/s]                             </pre>
<p class="calibre2">We can have a look at the first 100 characters of this dataset:</p>
<pre class="calibre21">cleaned_wikipedia_text[0:100]<br class="title-page-name"/><br class="title-page-name"/>' anarchism originated as a term of abuse first used against early working class radicals including t'</pre>
<p class="calibre2">Next up, we are going to preprocess the text, so we are going to define a helper function that will help us to replace special characters such as punctuation ones into a know token. Also, to reduce the amount of noise in the input text, you might want to remove words that don't appear frequently in the text:</p>
<pre class="calibre21">def preprocess_text(input_text):<br class="title-page-name"/><br class="title-page-name"/>    # Replace punctuation with some special tokens so we can use them in our model<br class="title-page-name"/>    input_text = input_text.lower()<br class="title-page-name"/>    input_text = input_text.replace('.', ' &lt;PERIOD&gt; ')<br class="title-page-name"/>    input_text = input_text.replace(',', ' &lt;COMMA&gt; ')<br class="title-page-name"/>    input_text = input_text.replace('"', ' &lt;QUOTATION_MARK&gt; ')<br class="title-page-name"/>    input_text = input_text.replace(';', ' &lt;SEMICOLON&gt; ')<br class="title-page-name"/>    input_text = input_text.replace('!', ' &lt;EXCLAMATION_MARK&gt; ')<br class="title-page-name"/>    input_text = input_text.replace('?', ' &lt;QUESTION_MARK&gt; ')<br class="title-page-name"/>    input_text = input_text.replace('(', ' &lt;LEFT_PAREN&gt; ')<br class="title-page-name"/>    input_text = input_text.replace(')', ' &lt;RIGHT_PAREN&gt; ')<br class="title-page-name"/>    input_text = input_text.replace('--', ' &lt;HYPHENS&gt; ')<br class="title-page-name"/>    input_text = input_text.replace('?', ' &lt;QUESTION_MARK&gt; ')<br class="title-page-name"/>   <br class="title-page-name"/>    input_text = input_text.replace(':', ' &lt;COLON&gt; ')<br class="title-page-name"/>    text_words = input_text.split()<br class="title-page-name"/>    <br class="title-page-name"/>    # neglecting all the words that have five occurrences of fewer<br class="title-page-name"/>    text_word_counts = Counter(text_words)<br class="title-page-name"/>    trimmed_words = [word for word in text_words if text_word_counts[word] &gt; 5]<br class="title-page-name"/><br class="title-page-name"/>    return trimmed_words</pre>
<p class="calibre2">Now, let's call this function on the input text and have a look at the output:</p>
<pre class="calibre21">preprocessed_words = preprocess_text(cleaned_wikipedia_text)<br class="title-page-name"/>print(preprocessed_words[:30])</pre>
<div class="title-page-name">
<pre class="calibre21">Output:<br class="title-page-name"/>['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including', 'the', 'diggers', 'of', 'the', 'english', 'revolution', 'and', 'the', 'sans', 'culottes', 'of', 'the', 'french', 'revolution', 'whilst']</pre></div>
<p class="calibre2">Let's see how many words and distinct words we have for the pre-processed version of the text:</p>
<pre class="calibre21">print("Total number of words in the text: {}".format(len(preprocessed_words)))<br class="title-page-name"/>print("Total number of unique words in the text: {}".format(len(set(preprocessed_words))))<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/><br class="title-page-name"/>Total number of words in the text: 16680599
Total number of unique words in the text: 63641</pre>
<p class="calibre2">And here, I'm creating dictionaries to covert words to integers and backwards, that is, integers to words. The integers are assigned in descending frequency order, so the most frequent word (<kbd class="calibre12">the</kbd>) is given the integer <kbd class="calibre12">0</kbd>, the next most frequent gets <kbd class="calibre12">1</kbd>, and so on. The words are converted to integers and stored in the list <kbd class="calibre12">int_words</kbd>.</p>
<p class="calibre2">As mentioned earlier in this section, we need to use the integer indexes of the words to look up their values in the weight matrix, so we are going to words to integers and integers to words. This will help us to look up the words and also get the actual word of a specific index. For example, the most repeated word in the input text will be indexed at position 0, followed by the second most repeated one, and so on.</p>
<p class="calibre2">So, let's define a function to create this lookup table:</p>
<pre class="calibre21">def create_lookuptables(input_words):<br class="title-page-name"/> """<br class="title-page-name"/> Creating lookup tables for vocan<br class="title-page-name"/> <br class="title-page-name"/> Function arguments:<br class="title-page-name"/> param words: Input list of words<br class="title-page-name"/> """<br class="title-page-name"/> input_word_counts = Counter(input_words)<br class="title-page-name"/> sorted_vocab = sorted(input_word_counts, key=input_word_counts.get, reverse=True)<br class="title-page-name"/> integer_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab)}<br class="title-page-name"/> vocab_to_integer = {word: ii for ii, word in integer_to_vocab.items()}<br class="title-page-name"/> <br class="title-page-name"/> # returning A tuple of dicts<br class="title-page-name"/> return vocab_to_integer, integer_to_vocab</pre>
<p class="calibre2">Now, let's call the defined function to create the lookup table:</p>
<pre class="calibre21">vocab_to_integer, integer_to_vocab = create_lookuptables(preprocessed_words)<br class="title-page-name"/>integer_words = [vocab_to_integer[word] for word in preprocessed_words]</pre>
<p class="calibre2">To build a more accurate model, we can remove words that don't change the context much  as <kbd class="calibre12">of</kbd>, <kbd class="calibre12">for</kbd>, <kbd class="calibre12">the</kbd>, and so on. So, it is practically proven that we can build more accurate models while discarding these kinds of words. The process of removing context-irrelevant words from the context is called <strong class="calibre13">subsampling</strong>. In order to define a general mechanism for word discarding, Mikolov introduced a function for calculating the discard probability of a certain word, which is given by:</p>
<div class="CDPAlignCenter"><img class="fm-editor-equation31" src="assets/6e9b5ce4-d28c-4a71-aa15-1527b32e8d9b.png"/></div>
<p class="calibre2">Where:</p>
<ul class="calibre7">
<li class="calibre8"><em class="calibre25">t</em> is a threshold parameter for word discarding</li>
<li class="calibre8"><em class="calibre25">f(w<sub class="calibre28">i</sub>)</em> is the frequency of a specific target word <em class="calibre25">w<sub class="calibre28">i</sub></em> in the input dataset</li>
</ul>
<p class="calibre2">So, we are going to implement a helper function that will calculate the discarding probability of each word in the dataset:</p>
<pre class="calibre21"># removing context-irrelevant words threshold<br class="title-page-name"/>word_threshold = 1e-5<br class="title-page-name"/><br class="title-page-name"/>word_counts = Counter(integer_words)<br class="title-page-name"/>total_number_words = len(integer_words)<br class="title-page-name"/><br class="title-page-name"/>#Calculating the freqs for the words<br class="title-page-name"/>frequencies = {word: count/total_number_words for word, count in word_counts.items()}<br class="title-page-name"/><br class="title-page-name"/>#Calculating the discard probability<br class="title-page-name"/>prob_drop = {word: 1 - np.sqrt(word_threshold/frequencies[word]) for word in word_counts}<br class="title-page-name"/>training_words = [word for word in integer_words if random.random() &lt; (1 - prob_drop[word])]</pre>
<p class="calibre2">Now, we have a more refined and clean version of the input text.</p>
<p class="calibre2">We mentioned that the skip-gram architecture considers the context of the target word while producing its real-valued representation, so it defines a window around the target word that has size <em class="calibre19">C</em>.</p>
<p class="calibre2">Instead of treating all contextual words equally, we are going to assign less weight for words that are a bit far from the target word. For example, if we choose the size of the window to be <em class="calibre19">C = 4</em>, then we are going to select a random number <em class="calibre19">L</em> from the range of 1 to <em class="calibre19">C</em>, and then sample <em class="calibre19">L</em> words from the history and the future of the current word. For more details about this, refer to the Mikolov et al paper at: <a href="https://arxiv.org/pdf/1301.3781.pdf" target="_blank" class="calibre11">https://arxiv.org/pdf/1301.3781.pdf</a>.</p>
<p class="calibre2">So, let's go ahead and define this function:</p>
<pre class="calibre21"># Defining a function that returns the words around specific index in a specific window<br class="title-page-name"/>def get_target(input_words, ind, context_window_size=5):<br class="title-page-name"/>    <br class="title-page-name"/>    #selecting random number to be used for genearting words form history and feature of the current word<br class="title-page-name"/>    rnd_num = np.random.randint(1, context_window_size+1)<br class="title-page-name"/>    start_ind = ind - rnd_num if (ind - rnd_num) &gt; 0 else 0<br class="title-page-name"/>    stop_ind = ind + rnd_num<br class="title-page-name"/>    <br class="title-page-name"/>    target_words = set(input_words[start_ind:ind] + input_words[ind+1:stop_ind+1])<br class="title-page-name"/>    <br class="title-page-name"/>    return list(target_words)    </pre>
<p class="calibre2">Also, let's define a generator function to generate a random batch from the training samples and get the contextual word for each word in that batch:</p>
<pre class="calibre21">#Defining a function for generating word batches as a tuple (inputs, targets)<br class="title-page-name"/>def generate_random_batches(input_words, train_batch_size, context_window_size=5):<br class="title-page-name"/>    <br class="title-page-name"/>    num_batches = len(input_words)//train_batch_size<br class="title-page-name"/>    <br class="title-page-name"/>    # working on only only full batches<br class="title-page-name"/>    input_words = input_words[:num_batches*train_batch_size]<br class="title-page-name"/>    <br class="title-page-name"/>    for ind in range(0, len(input_words), train_batch_size):<br class="title-page-name"/>        input_vals, target = [], []<br class="title-page-name"/>        input_batch = input_words[ind:ind+train_batch_size]<br class="title-page-name"/>        <br class="title-page-name"/>        #Getting the context for each word<br class="title-page-name"/>        for ii in range(len(input_batch)):<br class="title-page-name"/>            batch_input_vals = input_batch[ii]<br class="title-page-name"/>            batch_target = get_target(input_batch, ii, context_window_size)<br class="title-page-name"/>            <br class="title-page-name"/>            target.extend(batch_target)<br class="title-page-name"/>            input_vals.extend([batch_input_vals]*len(batch_target))<br class="title-page-name"/>        yield input_vals, target</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">Next up, we are going to use the following structure to build the computational graph:</p>
<div class="CDPAlignCenter"><img src="assets/f0d6926a-bce1-4013-82a6-f3f5ab664f81.png" class="calibre147"/></div>
<div class="CDPAlignCenter1">Figure 15.11: Model architecture</div>
<p class="calibre2">So, as mentioned previously, we are going to use an embedding layer that will try to learn a special real-valued representation for these words. Thus, the words will be fed as one-hot vectors. The idea is to train this network to build up the weight matrix.</p>
<p class="calibre2">So, let's start off by creating the input to our model:</p>
<pre class="calibre21">train_graph = tf.Graph()<br class="title-page-name"/><br class="title-page-name"/>#defining the inputs placeholders of the model<br class="title-page-name"/>with train_graph.as_default():<br class="title-page-name"/>    inputs_values = tf.placeholder(tf.int32, [None], name='inputs_values')<br class="title-page-name"/>    labels_values = tf.placeholder(tf.int32, [None, None], name='labels_values')</pre>
<p class="calibre2">The weight or embedding matrix that we are trying to build will have the following shape:</p>
<pre class="calibre21">num_words X num_hidden_neurons</pre>
<p class="calibre2">Also, we don't have to implement the lookup function ourselves because it's already available in Tensorflow: <kbd class="calibre12">tf.nn.embedding_lookup()</kbd>. So, it will use the integer encoding of the words and locate their corresponding rows in the weight matrix.</p>
<p class="calibre2">The weight matrix will be randomly initialized from a uniform distribution:</p>
<pre class="calibre21">num_vocab = len(integer_to_vocab)<br class="title-page-name"/><br class="title-page-name"/>num_embedding =  300<br class="title-page-name"/>with train_graph.as_default():<br class="title-page-name"/>    embedding_layer = tf.Variable(tf.random_uniform((num_vocab, num_embedding), -1, 1))<br class="title-page-name"/>    <br class="title-page-name"/>    # Next, we are going to use tf.nn.embedding_lookup function to get the output of the hidden layer<br class="title-page-name"/>    embed_tensors = tf.nn.embedding_lookup(embedding_layer, inputs_values) </pre>
<p class="calibre2">It's very inefficient to update all the embedding weights of the embedding layer at once. Instead of this, we will use the negative sampling technique which will only update the weight of the correct word with a small subset of the incorrect ones.</p>
<p class="calibre2">Also, we don't have to implement this function ourselves as it's already there in TensorFlow <strong class="calibre13"><kbd class="calibre12">tf.nn.sampled_softmax_loss</kbd>:</strong></p>
<pre class="calibre21"># Number of negative labels to sample<br class="title-page-name"/>num_sampled = 100<br class="title-page-name"/><br class="title-page-name"/>with train_graph.as_default():<br class="title-page-name"/>    # create softmax weights and biases<br class="title-page-name"/>    softmax_weights = tf.Variable(tf.truncated_normal((num_vocab, num_embedding))) <br class="title-page-name"/>    softmax_biases = tf.Variable(tf.zeros(num_vocab), name="softmax_bias") <br class="title-page-name"/>    <br class="title-page-name"/>    # Calculating the model loss using negative sampling<br class="title-page-name"/>    model_loss = tf.nn.sampled_softmax_loss(<br class="title-page-name"/>        weights=softmax_weights,<br class="title-page-name"/>        biases=softmax_biases,<br class="title-page-name"/>        labels=labels_values,<br class="title-page-name"/>        inputs=embed_tensors,<br class="title-page-name"/>        num_sampled=num_sampled,<br class="title-page-name"/>        num_classes=num_vocab)<br class="title-page-name"/>    <br class="title-page-name"/>    model_cost = tf.reduce_mean(model_loss)<br class="title-page-name"/>    model_optimizer = tf.train.AdamOptimizer().minimize(model_cost)</pre>
<p class="calibre2">To validate our trained model, we are going to sample some frequent or common words and some uncommon words and try to print our their closest set of words based on the learned representation of the skip-gram architecture:</p>
<pre class="calibre21">with train_graph.as_default():<br class="title-page-name"/>  <br class="title-page-name"/>    # set of random words for evaluating similarity on<br class="title-page-name"/>    valid_num_words = 16 <br class="title-page-name"/>    valid_window = 100<br class="title-page-name"/>    <br class="title-page-name"/>    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent <br class="title-page-name"/>    valid_samples = np.array(random.sample(range(valid_window), valid_num_words//2))<br class="title-page-name"/>    valid_samples = np.append(valid_samples, <br class="title-page-name"/>                               random.sample(range(1000,1000+valid_window), valid_num_words//2))<br class="title-page-name"/>    <br class="title-page-name"/>    valid_dataset_samples = tf.constant(valid_samples, dtype=tf.int32)<br class="title-page-name"/>    <br class="title-page-name"/>    # Calculating the cosine distance<br class="title-page-name"/>    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding_layer), 1, keep_dims=True))<br class="title-page-name"/>    normalized_embed = embedding_layer / norm<br class="title-page-name"/>    valid_embedding = tf.nn.embedding_lookup(normalized_embed, valid_dataset_samples)<br class="title-page-name"/>    cosine_similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embed))</pre>
<p class="calibre2">Now, we have all the bits and pieces for our model and we are ready to kick off the training process.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training</h1>
                </header>
            
            <article>
                
<p class="calibre2">Let's go ahead and kick off the training process:</p>
<pre class="calibre21">num_epochs = 10<br class="title-page-name"/>train_batch_size = 1000<br class="title-page-name"/>contextual_window_size = 10<br class="title-page-name"/><br class="title-page-name"/>with train_graph.as_default():<br class="title-page-name"/>    saver = tf.train.Saver()<br class="title-page-name"/><br class="title-page-name"/>with tf.Session(graph=train_graph) as sess:<br class="title-page-name"/>    <br class="title-page-name"/>    iteration_num = 1<br class="title-page-name"/>    average_loss = 0<br class="title-page-name"/>    <br class="title-page-name"/>    #Initializing all the vairables<br class="title-page-name"/>    sess.run(tf.global_variables_initializer())<br class="title-page-name"/><br class="title-page-name"/>    for e in range(1, num_epochs+1):<br class="title-page-name"/>        <br class="title-page-name"/>        #Generating random batch for training<br class="title-page-name"/>        batches = generate_random_batches(training_words, train_batch_size, contextual_window_size)<br class="title-page-name"/>       <br class="title-page-name"/>        #Iterating through the batch samples<br class="title-page-name"/>        for input_vals, target in batches:<br class="title-page-name"/>            <br class="title-page-name"/>            #Creating the feed dict<br class="title-page-name"/>            feed_dict = {inputs_values: input_vals,<br class="title-page-name"/>                    labels_values: np.array(target)[:, None]}<br class="title-page-name"/>            <br class="title-page-name"/>            train_loss, _ = sess.run([model_cost, model_optimizer], feed_dict=feed_dict)<br class="title-page-name"/>            <br class="title-page-name"/>            #commulating the loss<br class="title-page-name"/>            average_loss += train_loss<br class="title-page-name"/>            <br class="title-page-name"/>            #Printing out the results after 100 iteration<br class="title-page-name"/>            if iteration_num % 100 == 0: <br class="title-page-name"/>                print("Epoch Number {}/{}".format(e, num_epochs),<br class="title-page-name"/>                      "Iteration Number: {}".format(iteration_num),<br class="title-page-name"/>                      "Avg. Training loss: {:.4f}".format(average_loss/100))<br class="title-page-name"/>                average_loss = 0<br class="title-page-name"/>            <br class="title-page-name"/>            <br class="title-page-name"/>            if iteration_num % 1000 == 0:<br class="title-page-name"/>                <br class="title-page-name"/>                ## Using cosine similarity to get the nearest words to a word<br class="title-page-name"/>                similarity = cosine_similarity.eval()<br class="title-page-name"/>                for i in range(valid_num_words):<br class="title-page-name"/>                    valid_word = integer_to_vocab[valid_samples[i]]<br class="title-page-name"/>                    <br class="title-page-name"/>                    # number of nearest neighbors<br class="title-page-name"/>                    top_k = 8 <br class="title-page-name"/>                    nearest_words = (-similarity[i, :]).argsort()[1:top_k+1]<br class="title-page-name"/>                    msg = 'The nearest to %s:' % valid_word<br class="title-page-name"/>                    for k in range(top_k):<br class="title-page-name"/>                        similar_word = integer_to_vocab[nearest_words[k]]<br class="title-page-name"/>                        msg = '%s %s,' % (msg, similar_word)<br class="title-page-name"/>                    print(msg)<br class="title-page-name"/>            <br class="title-page-name"/>            iteration_num += 1<br class="title-page-name"/>    save_path = saver.save(sess, "checkpoints/cleaned_wikipedia_version.ckpt")<br class="title-page-name"/>    embed_mat = sess.run(normalized_embed)</pre>
<p class="calibre2">After running the preceding code snippet for 10 epochs, you will get the following output:</p>
<div class="title-page-name">
<div class="title-page-name">
<pre class="calibre21">Epoch Number 10/10 Iteration Number: 43100 Avg. Training loss: 5.0380
Epoch Number 10/10 Iteration Number: 43200 Avg. Training loss: 4.9619
Epoch Number 10/10 Iteration Number: 43300 Avg. Training loss: 4.9463
Epoch Number 10/10 Iteration Number: 43400 Avg. Training loss: 4.9728
Epoch Number 10/10 Iteration Number: 43500 Avg. Training loss: 4.9872
Epoch Number 10/10 Iteration Number: 43600 Avg. Training loss: 5.0534
Epoch Number 10/10 Iteration Number: 43700 Avg. Training loss: 4.8261
Epoch Number 10/10 Iteration Number: 43800 Avg. Training loss: 4.8752
Epoch Number 10/10 Iteration Number: 43900 Avg. Training loss: 4.9818
Epoch Number 10/10 Iteration Number: 44000 Avg. Training loss: 4.9251
The nearest to nine: one, seven, zero, two, three, four, eight, five,
The nearest to such: is, as, or, some, have, be, that, physical,
The nearest to who: his, him, he, did, to, had, was, whom,
The nearest to two: zero, one, three, seven, four, five, six, nine,
The nearest to which: as, a, the, in, to, also, for, is,
The nearest to seven: eight, one, three, five, four, six, zero, two,
The nearest to american: actor, nine, singer, actress, musician, comedian, athlete, songwriter,<br class="title-page-name"/>The nearest to many: as, other, some, have, also, these, are, or,<br class="title-page-name"/>The nearest to powers: constitution, constitutional, formally, assembly, state, legislative, general, government,<br class="title-page-name"/>The nearest to question: questions, existence, whether, answer, truth, reality, notion, does,<br class="title-page-name"/>The nearest to channel: tv, television, broadcasts, broadcasting, radio, channels, broadcast, stations,<br class="title-page-name"/>The nearest to recorded: band, rock, studio, songs, album, song, recording, pop,<br class="title-page-name"/>The nearest to arts: art, school, alumni, schools, students, university, renowned, education,<br class="title-page-name"/>The nearest to orthodox: churches, orthodoxy, church, catholic, catholics, oriental, christianity, christians,<br class="title-page-name"/>The nearest to scale: scales, parts, important, note, between, its, see, measured,<br class="title-page-name"/>The nearest to mean: is, exactly, defined, denote, hence, are, meaning, example,<br class="title-page-name"/><br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45100 Avg. Training loss: 4.8466<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45200 Avg. Training loss: 4.8836<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45300 Avg. Training loss: 4.9016<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45400 Avg. Training loss: 5.0218<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45500 Avg. Training loss: 5.1409<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45600 Avg. Training loss: 4.7864<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45700 Avg. Training loss: 4.9312<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45800 Avg. Training loss: 4.9097<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 45900 Avg. Training loss: 4.6924<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 46000 Avg. Training loss: 4.8999<br class="title-page-name"/>The nearest to nine: one, eight, seven, six, four, five, american, two,<br class="title-page-name"/>The nearest to such: can, example, examples, some, be, which, this, or,<br class="title-page-name"/>The nearest to who: him, his, himself, he, was, whom, men, said,<br class="title-page-name"/>The nearest to two: zero, five, three, four, six, one, seven, nine<br class="title-page-name"/>The nearest to which: to, is, a, the, that, it, and, with,<br class="title-page-name"/>The nearest to seven: one, six, eight, five, nine, four, three, two,<br class="title-page-name"/>The nearest to american: musician, actor, actress, nine, singer, politician, d, one,<br class="title-page-name"/>The nearest to many: often, as, most, modern, such, and, widely, traditional,<br class="title-page-name"/>The nearest to powers: constitutional, formally, power, rule, exercised, parliamentary, constitution, control,<br class="title-page-name"/>The nearest to question: questions, what, answer, existence, prove, merely, true, statements,<br class="title-page-name"/>The nearest to channel: network, channels, broadcasts, stations, cable, broadcast, broadcasting, radio,<br class="title-page-name"/>The nearest to recorded: songs, band, song, rock, album, bands, music, studio,<br class="title-page-name"/>The nearest to arts: art, school, martial, schools, students, styles, education, student,<br class="title-page-name"/>The nearest to orthodox: orthodoxy, churches, church, christianity, christians, catholics, christian, oriental,<br class="title-page-name"/>The nearest to scale: scales, can, amounts, depends, tend, are, structural, for,<br class="title-page-name"/>The nearest to mean: we, defined, is, exactly, equivalent, denote, number, above,<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 46100 Avg. Training loss: 4.8583<br class="title-page-name"/>Epoch Number 10/10 Iteration Number: 46200 Avg. Training loss: 4.8887</pre></div>
</div>
<p class="calibre2">As you can see from the output, the network somehow learned some semantically useful representation of the input words. To help us get a clearer picture of the embedding matrix, we are going to use a dimensionality reduction technique such as t-SNE to reduce the real-valued vectors to two dimensions, and then we'll visualize them and label each point with its corresponding word:</p>
<pre class="calibre21">num_visualize_words = 500<br class="title-page-name"/>tsne_obj = TSNE()<br class="title-page-name"/>embedding_tsne = tsne_obj.fit_transform(embedding_matrix[:num_visualize_words, :])<br class="title-page-name"/><br class="title-page-name"/>fig, ax = plt.subplots(figsize=(14, 14))<br class="title-page-name"/>for ind in range(num_visualize_words):<br class="title-page-name"/>    plt.scatter(*embedding_tsne[ind, :], color='steelblue')<br class="title-page-name"/>    plt.annotate(integer_to_vocab[ind], (embedding_tsne[ind, 0], embedding_tsne[ind, 1]), alpha=0.7)</pre>
<pre class="calibre21"><br class="title-page-name"/>Output:</pre>
<div class="CDPAlignCenter"><img src="assets/e7cd06e7-0156-4b62-84f1-7650c2c067d7.png" class="calibre148"/></div>
<div class="packtfigref2">Figure 15.12: A visualization of word vectors</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we went through the idea of representation learning and why it's useful for doing deep learning or machine learning in general on input that's not in a real-valued form. Also, we covered one of the adopted techniques for converting words into real-valued vectors<span class="calibre10">—W</span>ord2Vec—which has very interesting properties. Finally, we implemented the Word2Vec model using the skip-gram architecture.</p>
<p class="calibre2">Next up, you will see the practical use of these learned representations in a sentiment analysis example, where we need to convert the input text to real-valued vectors.</p>


            </article>

            
        </section>
    </body></html>