<html><head></head><body>
		<div>
			<div id="_idContainer371" class="Content">
			</div>
		</div>
		<div id="_idContainer372" class="Content">
			<h1 id="_idParaDest-198"><a id="_idTextAnchor222"/>11. Generative Models</h1>
		</div>
		<div id="_idContainer404" class="Content">
			<p class="callout-heading">Overview </p>
			<p class="callout">This chapter introduces you to generative models—their components, how they function, and what they can do. You will start with generative <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) networks and how to use them to generate new text. You will then learn about <strong class="bold">generative adversarial networks</strong> (<strong class="bold">GANs</strong>) and how to create new data, before moving on to <strong class="bold">deep convolutional generative adversarial networks</strong> (<strong class="bold">DCGANs</strong>) and creating your own images. </p>
			<p class="callout">By the end of the chapter, you will know how to effectively use different types of GANs and generate various types of new data.</p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor223"/>Introduction</h1>
			<p>In this chapter, you will explore generative models, which are types of unsupervised learning algorithms that generate completely new artificial data. Generative models differ from predictive models in that they aim to generate new samples from the same distribution of training data. While the purpose of these models may be very different from those covered in other chapters, you can and will use many of the concepts learned in prior chapters, including loading and preprocessing various data files, hyperparameter tuning, and building convolutional and <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>). In this chapter, you will learn about one way to generate new samples from a training dataset, which is to use LSTM models to complete sequences of data based on initial seed data. </p>
			<p>Another way that you will learn about is the concept of two neural networks competing against one another in an adversarial way, that is, a generator generating samples and a discriminator trying to distinguish between the generated and real samples. As both models train simultaneously, the generator generates more realistic samples as the discriminator can more accurately distinguish between the "real" and "fake" data over time. These networks working together are called GANs. Generative models can be used to generate new text data, audio samples, and images.</p>
			<p>In this chapter, you will focus primarily on three areas of generative models – text generation or language modeling, GANs, and DCGANs.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor224"/>Text Generation</h1>
			<p>In <em class="italic">Chapter 9</em>, <em class="italic">Recurrent Neural Networks</em>, you were introduced to <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) and text generation (also known as language modeling), as you worked with some sequential data problems. In this section, you will be extending your sequence model for text generation using the same dataset to generate extended headlines. </p>
			<p>Previously in this book, you saw that sequential data is data in which each point in the dataset is dependent on the point prior and the order of the data is important. Recall the example with the bag of words from <em class="italic">Chapter 9</em>, <em class="italic">Recurrent Neural Networks</em>. With the <em class="italic">bag-of-words</em> approach, you simply used a set of word counts to derive meaning from their use. As you can see in <em class="italic">Figure 11.1</em>, these two sentences have completely opposite semantic meanings, but would be identical in a bag-of-words format. While this may be an effective strategy for some problems, it's not an ideal approach for predicting the next word or words.</p>
			<div>
				<div id="_idContainer373" class="IMG---Figure">
					<img src="image/B16341_11_01.jpg" alt="Figure 11.1: An example of identical words with differing semantics&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1: An example of identical words with differing semantics</p>
			<p>Consider the following example of a language model. You are given a sentence or a phrase, <strong class="source-inline">yesterday I took my car out for a</strong>, and are asked to predict the word that comes next in the sequence. Here, an appropriate word to complete the sequence would be <strong class="source-inline">drive</strong>.</p>
			<div>
				<div id="_idContainer374" class="IMG---Figure">
					<img src="image/B16341_11_02.jpg" alt="Figure 11.2: Sentence example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2: Sentence example</p>
			<p>To be successful in working with sequential data, you need a neural network capable of storing the value of the sequence. For this, you can use RNNs and LSTMs. LSTMs that are used for generating new sequences, such as text generation or language modeling, are known as generative LSTMs. </p>
			<p>Let's do a simple review of RNNs and LSTMs.</p>
			<p>Essentially, RNNs loop back on themselves, storing information and repeating the process, in a continuous cycle. Information is first transformed into vectors so that it can be processed by machines. The RNN then processes the vector sequence one at a time. As the RNN processes each vector, the vector gets passed through the previous hidden state. In this way, the hidden state retains information from the previous step, acting as a type of memory. It does this by combining the input and the previous hidden state with a tanh function that compresses the values between <strong class="source-inline">-1</strong> and <strong class="source-inline">1</strong>.</p>
			<p>Essentially, this is how the RNN functions. RNNs don't need a lot of computation and work well with short sequences. Simply put, RNNs are networks that have loops that allow information to persist over time. </p>
			<div>
				<div id="_idContainer375" class="IMG---Figure">
					<img src="image/B16341_11_03.jpg" alt="Figure 11.3: RNN data flow&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3: RNN data flow</p>
			<p>RNNs do come with a couple of challenges—most notably, the exploding and vanishing gradient problems.</p>
			<p>The <strong class="bold">exploding gradient problem</strong> is what happens when gradients become too large for optimization. The opposite problem may occur where your gradients are too small. This is what is known as the <strong class="bold">vanishing gradient problem</strong>. This happens when gradients become increasingly smaller as you make repeated multiplications. Since the size of the gradient determines the size of the weight updates, exploding or vanishing gradients mean that the network can no longer be trained. This is a very real problem when it comes to training RNNs since the output of the networks feeds back into the input. The vanishing and exploding gradient issues were covered in <em class="italic">Chapter 9</em>, <em class="italic">Recurrent Neural Networks</em>, and more details of how these issues are solved can be found there.</p>
			<p>LSTMs can selectively control the flow of information within each LSTM node. With added control, you can more easily adjust the model to prevent potential problems with gradients. </p>
			<div>
				<div id="_idContainer376" class="IMG---Figure">
					<img src="image/B16341_11_04.jpg" alt="Figure 11.4: LSTM architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4: LSTM architecture</p>
			<p>So, what enables LSTMs to track and store information throughout many time steps? You'll recall from <em class="italic">Chapter 9</em>, <em class="italic">Recurrent Neural Networks</em>, that the key building block behind the LSTM is the structure called a <em class="italic">gate</em>, which allows the LSTM to selectively add or remove information to its cell state. </p>
			<p>Gates consist of a bounding function such as sigmoid or tanh. For example, if the function were sigmoid, it would force its input to be between zero and one. Intuitively, you can think of this as capturing how much of the information passed through the gate should be retained. This should be between zero and one, effectively <em class="italic">gating</em> the flow of information.</p>
			<p>LSTMs process information through four simple steps. </p>
			<p>They first forget their irrelevant history. Second, they perform a computation to store relevant parts of new information, and thirdly, they use these two steps together to selectively update their internal state. Finally, they generate an output.</p>
			<div>
				<div id="_idContainer377" class="IMG---Figure">
					<img src="image/B16341_11_05.jpg" alt="Figure 11.5: LSTM processing steps&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.5: LSTM processing steps</p>
			<p>This was a bit of a refresher on LSTMs and how they can selectively control and regulate the flow of information. Now that you've reviewed LSTMs and their architecture, you can put some of these concepts to work by reviewing your code and LSTM model.</p>
			<p>You can create an LSTM model in the following manner using a sequential model. This LSTM contains four hidden layers, each with <strong class="source-inline">50</strong>, <strong class="source-inline">60</strong>, <strong class="source-inline">80</strong>, and <strong class="source-inline">120</strong> units and a ReLU activation function. The <strong class="source-inline">return_sequences</strong> parameter is set to <strong class="source-inline">True</strong> for all but the last layer since they are not the final LSTM layer in the network:</p>
			<p class="source-code">regressor = Sequential()</p>
			<p class="source-code">regressor.add(LSTM(units= 50, activation = 'relu', \</p>
			<p class="source-code">                   return_sequences = True, \</p>
			<p class="source-code">                   input_shape = (X_train.shape[1], 5)))</p>
			<p class="source-code">regressor.add(Dropout(0.2))</p>
			<p class="source-code">regressor.add(LSTM(units= 60, activation = 'relu', \</p>
			<p class="source-code">                   return_sequences = True))</p>
			<p class="source-code">regressor.add(Dropout(0.3))</p>
			<p class="source-code">regressor.add(LSTM(units= 80, activation = 'relu', \</p>
			<p class="source-code">              return_sequences = True))</p>
			<p class="source-code">regressor.add(Dropout(0.4))</p>
			<p class="source-code">regressor.add(LSTM(units= 120, activation = 'relu'))</p>
			<p class="source-code">regressor.add(Dropout(0.5))</p>
			<p class="source-code">regressor.add(Dense(units = 1))</p>
			<p>Now that you've recalled how to create RNNs with LSTM layers, you'll next learn how to apply them to natural language text and generate new text in a sequence.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor225"/>Extending NLP Sequence Models to Generate Text</h2>
			<p><strong class="bold">NLP</strong> takes data in the form of natural language that has traditionally been very difficult for machines to make sense of and turns it into data that can be useful for machine learning applications. This data can take the form of characters, words, sentences, or paragraphs. You will be focusing on text generation in this section. </p>
			<p>As a quick review, <em class="italic">preprocessing</em> generally entails all the steps needed to train your model. Some common steps include <em class="italic">data cleaning</em>, <em class="italic">transformation</em>, and <em class="italic">data reduction</em>. For NLP, more specifically, the steps could be all or some of the following:</p>
			<ul>
				<li><strong class="bold">Dataset cleaning</strong> encompasses the conversion of the case to lowercase, removing punctuation.</li>
				<li><strong class="bold">Tokenization</strong> is breaking up a character sequence into specified units called tokens.</li>
				<li><strong class="bold">Padding</strong> is a way to make input sentences of different sizes the same by padding them.</li>
				<li><strong class="bold">Padding the sequences</strong> refers to making sure that the sequences have a uniform length.</li>
				<li><strong class="bold">Stemming</strong> is truncating words down to their stem. For example, the words <strong class="source-inline">rainy</strong> and <strong class="source-inline">raining</strong> both have the stem <strong class="source-inline">rain</strong>.</li>
			</ul>
			<p>Let's take a closer look at what the process looks like.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor226"/>Dataset Cleaning</h2>
			<p>Here, you create a function, <strong class="source-inline">clean_text</strong>, that returns a list of words after cleaning. Now, save all text as lowercase with <strong class="source-inline">lower()</strong> method, encoded with <strong class="source-inline">utf8</strong> for character standardization. Finally, output 10 headlines from your corpus:</p>
			<p class="source-code">def clean_text(txt):</p>
			<p class="source-code">    txt = "".join(v for v in txt \</p>
			<p class="source-code">                  if v not in string.punctuation).lower()</p>
			<p class="source-code">    txt = txt.encode("utf8").decode("ascii",'ignore')</p>
			<p class="source-code">    return txt </p>
			<p class="source-code">corpus = [clean_text(x) for x in all_headlines]</p>
			<p class="source-code">corpus[:10]</p>
			<p>Cleaning the text in this manner is a great way to standardize text to input into a model. Converting all words to lowercase in the same encoding ensures consistency of the text. It also ensures that capitalization or different encodings of the same words are not treated as different words by any model that is created.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor227"/>Generating a Sequence and Tokenization</h2>
			<p>Neural networks expect input data in a consistent, numerical format. Much like how images are processed for image classification models, where each image is represented as a three-dimensional array, and are often resized to meet the expectations of the model, text must be processed similarly. Luckily, Keras has a number of utility classes and functions to aid with processing text data for neural networks. One such class is <strong class="source-inline">Tokenizer</strong>, which vectorizes a text corpus by converting the corpus into a sequence of integers. The following code imports the <strong class="source-inline">Tokenizer</strong> class from Keras:</p>
			<p class="source-code">from keras.preprocessing.text import Tokenizer</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor228"/>Generating a Sequence of n-gram Tokens</h2>
			<p>Here, you create a function named <strong class="source-inline">get_seq_of_tokens</strong>. With <strong class="source-inline">tokenizer.fit_on_texts</strong>, you extract tokens from the corpus. Each integer output corresponds with a specific word. The <strong class="source-inline">input_seq</strong> parameter is initialized as an empty list, <strong class="source-inline">[]</strong>. With <strong class="source-inline">token_list =</strong> <strong class="source-inline">tokenizer.texts_to_sequences</strong>, you convert text to the tokenized equivalent. With <strong class="source-inline">n_gram_sequence</strong> <strong class="source-inline">= token_list</strong>, you generate the n-gram sequences. Using <strong class="source-inline">input_seq.append(n_gram_sequence)</strong>, you append each sequence to the list of your features:</p>
			<p class="source-code">tokenizer = Tokenizer()</p>
			<p class="source-code">def get_seq_of_tokens(corpus):</p>
			<p class="source-code">    tokenizer.fit_on_texts(corpus)</p>
			<p class="source-code">    all_words = len(tokenizer.word_index) + 1</p>
			<p class="source-code">    </p>
			<p class="source-code">    input_seq = []</p>
			<p class="source-code">    for line in corpus:</p>
			<p class="source-code">        token_list = tokenizer.texts_to_sequences([line])[0]</p>
			<p class="source-code">        for i in range(1, len(token_list)):</p>
			<p class="source-code">            n_gram_sequence = token_list[:i+1]</p>
			<p class="source-code">            input_seq.append(n_gram_sequence)</p>
			<p class="source-code">    return input_seq, all_words</p>
			<p class="source-code">your_sequences, all_words = get_seq_of_tokens(corpus)</p>
			<p class="source-code">your_sequences[:10]</p>
			<p><strong class="source-inline">get_seq_of_tokens</strong> ensures that a corpus is broken up into sequences of equal length. If a corpus is too short for the network's expected input, the resultant sequence will have to be padded.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor229"/>Padding Sequences</h2>
			<p>Here, you create a <strong class="source-inline">generate_padded_sequences</strong> function that takes <strong class="source-inline">input_seq</strong> as input. The <strong class="source-inline">pad_sequences</strong> function is used to pad the sequences to make their lengths equal. In the function, first, the maximum sequence length is determined by calculating the length of each input sequence. Once the maximum sequence length is determined, all other sequences are padded to match. Next, the <strong class="source-inline">predictors</strong> and <strong class="source-inline">label</strong> parameters are created. The <strong class="source-inline">label</strong> parameter is the last word of the sequence, and the <strong class="source-inline">predictors</strong> parameter is all the preceding words. Finally, the <strong class="source-inline">label</strong> parameter is converted to a categorical array:</p>
			<p class="source-code">def generate_padded_sequences(input_seq):</p>
			<p class="source-code">    max_sequence_len = max([len(x) for x in input_seq])</p>
			<p class="source-code">    input_seq = np.array(pad_sequences\</p>
			<p class="source-code">                         (input_seq, maxlen=max_sequence_len, \</p>
			<p class="source-code">                          padding='pre'))</p>
			<p class="source-code">    </p>
			<p class="source-code">    predictors, label = input_seq[:,:-1],input_seq[:,-1]</p>
			<p class="source-code">    label = keras.utils.to_categorical(label, num_classes=all_words)</p>
			<p class="source-code">    return predictors, label, max_sequence_len</p>
			<p class="source-code">predictors, label, max_sequence_len = generate_padded_sequences\</p>
			<p class="source-code">                                      (your_sequences)</p>
			<p>Now that you have learned some preprocessing and cleaning steps for working with natural language, including cleaning, generating n-gram sequences, and padding sequences for consistent lengths, you are ready for your first exercise of the chapter, that is, text generation.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor230"/>Exercise 11.01: Generating Text </h2>
			<p>In this exercise, you will use the LSTM model from <em class="italic">Exercise 9.02</em>, <em class="italic">Building an RNN with LSTM Layer Nvidia Stock Prediction</em>, to extend your prediction sequence and generate new text. In that exercise, you created an LSTM model to predict the stock price of Nvidia by feeding the historical stock prices to the model. The model was able to use LSTM layers to understand patterns in the historical stock prices for future predictions.</p>
			<p>In this exercise, you will use the same principle applied to text, by feeding the historical headlines to the model. You will use the <strong class="source-inline">articles.csv</strong> dataset for this exercise. The dataset contains 831 news headlines from the New York Times in CSV format. Along with the headlines, the dataset also contains several attributes about the news article, including the publication date, print page, and keywords. You are required to generate new news headlines using the given dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find <strong class="source-inline">articles.csv</strong> here: <a href="http://packt.link/RQVoB">http://packt.link/RQVoB</a>.</p>
			<p>Perform the following steps to complete this exercise:</p>
			<ol>
				<li>Open a new Jupyter or Colab notebook.</li>
				<li>Import the following libraries:<p class="source-code">from keras.preprocessing.sequence import pad_sequences</p><p class="source-code">from keras.models import Sequential</p><p class="source-code">from keras.layers import Embedding, LSTM, Dense, Dropout</p><p class="source-code">import tensorflow.keras.utils as ku </p><p class="source-code">from keras.preprocessing.text import Tokenizer</p><p class="source-code">import pandas as pd</p><p class="source-code">import numpy as np</p><p class="source-code">from keras.callbacks import EarlyStopping</p><p class="source-code">import string, os </p><p class="source-code">import warnings</p><p class="source-code">warnings.filterwarnings("ignore")</p><p class="source-code">warnings.simplefilter(action='ignore', category=FutureWarning)</p><p>You should get the following output:</p><p class="source-code">Using TensorFlow backend.</p></li>
				<li>Load the dataset locally by setting <strong class="source-inline">your_dir</strong> to <strong class="source-inline">content/</strong>. Create a <strong class="source-inline">your_headlines</strong> parameter as an empty list and use a <strong class="source-inline">for</strong> loop to iterate over:<p class="source-code">your_dir = 'content/'</p><p class="source-code">your_headlines = []</p><p class="source-code">for filename in os.listdir(your_dir):</p><p class="source-code">    if 'Articles' in filename:</p><p class="source-code">        article_df = pd.read_csv(your_dir + filename)</p><p class="source-code">        your_headlines.extend(list(article_df.headline.values))</p><p class="source-code">        break</p><p class="source-code">your_headlines = [h for h in your_headlines if h != "Unknown"]</p><p class="source-code">len(our_headlines)</p><p>The output will represent the number of headlines in your dataset:</p><p class="source-code">831</p></li>
				<li>Now, create a <strong class="source-inline">clean_text</strong> function to return a list of cleaned words. Convert the text to lowercase with <strong class="source-inline">lower()</strong> method and encode it with <strong class="source-inline">utf8</strong> for character standardization. Finally, output 20 headlines from your corpus:<p class="source-code">def clean_text(txt):</p><p class="source-code">    txt = "".join(v for v in txt \</p><p class="source-code">                  if v not in string.punctuation).lower()</p><p class="source-code">    txt = txt.encode("utf8").decode("ascii",'ignore')</p><p class="source-code">    return txt </p><p class="source-code">corpus = [clean_text(x) for x in all_headlines]</p><p class="source-code">corpus[60:80]</p><p>You should get the following output:</p><div id="_idContainer378" class="IMG---Figure"><img src="image/B16341_11_06.jpg" alt="Figure 11.6: Corpus&#13;&#10;"/></div><p class="figure-caption">Figure 11.6: Corpus</p></li>
				<li>With <strong class="source-inline">tokenizer.fit</strong>, extract tokens from the corpus. Each integer output corresponds to a specific word. The <strong class="source-inline">input_seq</strong> parameter is initialized as an empty list, <strong class="source-inline">[]</strong>. With <strong class="source-inline">token_list =</strong> <strong class="source-inline">tokenizer.texts_to_sequences</strong>, you convert each sentence into its tokenized equivalent. With <strong class="source-inline">n_gram_sequence = token_list</strong>, you generate the n-gram sequences. Using <strong class="source-inline">input_seq.append(n_gram_sequence)</strong>, you append each sequence to a list of features:<p class="source-code">tokenizer = Tokenizer()</p><p class="source-code">def get_seq_of_tokens(corpus):</p><p class="source-code">    tokenizer.fit_on_texts(corpus)</p><p class="source-code">    all_words = len(tokenizer.word_index) + 1</p><p class="source-code">    </p><p class="source-code">    input_seq = []</p><p class="source-code">    for line in corpus:</p><p class="source-code">        token_list = tokenizer.texts_to_sequences([line])[0]</p><p class="source-code">        for i in range(1, len(token_list)):</p><p class="source-code">            n_gram_sequence = token_list[:i+1]</p><p class="source-code">            input_seq.append(n_gram_sequence)</p><p class="source-code">    return input_seq, all_words</p><p class="source-code">your_sequences, all_words = get_seq_of_tokens(corpus)</p><p class="source-code">your_sequences[:20]</p><p>You should get the following output:</p><div id="_idContainer379" class="IMG---Figure"><img src="image/B16341_11_07.jpg" alt="Figure 11.7: n-gram tokens&#13;&#10;"/></div><p class="figure-caption">Figure 11.7: n-gram tokens</p><p>The output shows the n-gram tokens of the headlines. For each headline, the number of n-grams is determined by the length of the headline.</p></li>
				<li>Pad the sequences and obtain the variables, <strong class="source-inline">predictors</strong> and <strong class="source-inline">target</strong>:<p class="source-code">def generate_padded_sequences(input_seq):</p><p class="source-code">    max_sequence_len = max([len(x) for x in input_seq])</p><p class="source-code">    input_seq = np.array(pad_sequences\</p><p class="source-code">                         (input_seq, maxlen=max_sequence_len, \</p><p class="source-code">                          padding='pre'))</p><p class="source-code">    predictors, label = input_seq[:,:-1],input_seq[:,-1]</p><p class="source-code">    label = ku.to_categorical(label, num_classes=all_words)</p><p class="source-code">    return predictors, label, max_sequence_len</p><p class="source-code">predictors, label, \</p><p class="source-code">max_sequence_len = generate_padded_sequences(inp_seq)</p></li>
				<li>Prepare your model for training. Add an input embedding layer with <strong class="source-inline">model.add(Embedding)</strong>, a hidden LSTM layer with <strong class="source-inline">model.add(LSTM(100))</strong>, and a dropout of 10%. Then, add the output layer with <strong class="source-inline">model.add(Dense)</strong> using the softmax activation function. With <strong class="source-inline">compile()</strong> method, configure your model for training, setting your loss function to <strong class="source-inline">categorical_crossentropy</strong>. Use the Adam optimizer:<p class="source-code">def create_model(max_sequence_len, all_words):</p><p class="source-code">    input_len = max_sequence_len - 1</p><p class="source-code">    model = Sequential()</p><p class="source-code">    </p><p class="source-code">    model.add(Embedding(all_words, 10, input_length=input_len))</p><p class="source-code">    </p><p class="source-code">    model.add(LSTM(100))</p><p class="source-code">    model.add(Dropout(0.1))</p><p class="source-code">    </p><p class="source-code">    model.add(Dense(all_words, activation='softmax'))</p><p class="source-code">    model.compile(loss='categorical_crossentropy', \</p><p class="source-code">                  optimizer='adam')</p><p class="source-code">    </p><p class="source-code">    return model</p><p class="source-code">model = create_model(max_sequence_len, all_words)</p><p class="source-code">model.summary()</p><p>You should get the following output:</p><div id="_idContainer380" class="IMG---Figure"><img src="image/B16341_11_08.jpg" alt="Figure 11.8: Model summary&#13;&#10;"/></div><p class="figure-caption">Figure 11.8: Model summary</p></li>
				<li>Fit the model and set <strong class="source-inline">epochs</strong> to <strong class="source-inline">200</strong> and <strong class="source-inline">verbose</strong> to <strong class="source-inline">5</strong>:<p class="source-code">model.fit(predictors, label, epochs=200, verbose=5)</p><p>You should get the following output:</p><div id="_idContainer381" class="IMG---Figure"><img src="image/B16341_11_09.jpg" alt="Figure 11.9: Training the model&#13;&#10;"/></div><p class="figure-caption">Figure 11.9: Training the model</p></li>
				<li>Create a function that will generate a headline given a starting seed text, the number of words to generate, the model, and the maximum sequence length. The function will include a <strong class="source-inline">for</strong> loop to iterate over the number of words to generate. In each iteration, the tokenizer will tokenize the text, and then pad the sequence before predicting the next word in the sequence. Next, the iteration will convert the token back into a word and add it to the sentence. Once the <strong class="source-inline">for</strong> loop completes, the generated headline will be returned:<p class="source-code">def generate_text(seed_text, next_words, model, max_sequence_len):</p><p class="source-code">    for _ in range(next_words):</p><p class="source-code">        token_list = tokenizer.texts_to_sequences([seed_text])[0]</p><p class="source-code">        token_list = pad_sequences([token_list], \</p><p class="source-code">                                    maxlen = max_sequence_len-1, \</p><p class="source-code">                                    padding='pre')</p><p class="source-code">        predicted = model.predict\</p><p class="source-code">                    (token_list, verbose=0)</p><p class="source-code">        output_word = ""</p><p class="source-code">        for word,index in tokenizer.word_index.items():</p><p class="source-code">            if index == predicted.any():</p><p class="source-code">                output_word = word</p><p class="source-code">                break</p><p class="source-code">        seed_text += " "+output_word</p><p class="source-code">    return seed_text.title()</p></li>
				<li>Finally, output some of your generated text with the <strong class="source-inline">print</strong> function by printing the output of the function you created in <em class="italic">Step 9</em>. Use the <strong class="source-inline">10 ways</strong>, <strong class="source-inline">europe looks to</strong>, <strong class="source-inline">best way</strong>, <strong class="source-inline">homeless in</strong>, <strong class="source-inline">unexpected results</strong>, and <strong class="source-inline">critics warn</strong> seed words with the corresponding number of words to generate; that is, <strong class="source-inline">11</strong>, <strong class="source-inline">8</strong>, <strong class="source-inline">10</strong>, <strong class="source-inline">10</strong>, <strong class="source-inline">10</strong>, and <strong class="source-inline">10</strong>, respectively:<p class="source-code">print (generate_text("10 ways", 11, model, max_sequence_len))</p><p class="source-code">print (generate_text("europe looks to", 8, model, \</p><p class="source-code">                     max_sequence_len))</p><p class="source-code">print (generate_text("best way", 10, model, max_sequence_len))</p><p class="source-code">print (generate_text("homeless in", 10, model, max_sequence_len))</p><p class="source-code">print (generate_text("unexpected results", 10, model,\</p><p class="source-code">                     max_sequence_len))</p><p class="source-code">print (generate_text("critics warn", 10, model, \</p><p class="source-code">                     max_sequence_len))</p><p>You should get the following output:</p><div id="_idContainer382" class="IMG---Figure"><img src="image/B16341_11_10.jpg" alt="Figure 11.10: Generated text&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.10: Generated text</p>
			<p>The output shows the generated headlines with the seed text provided. The words generated are limited to what was included in the training dataset, which itself was fairly limited in size, leading to some nonsensical results.</p>
			<p>Now that you've generated text with an LSTM in your first exercise, let's move on to working with images by using GANs to generate new images based on a given dataset.</p>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor231"/>Generative Adversarial Networks </h1>
			<p>GANs are networks that generate new, synthetic data by learning patterns and underlying representations from a training dataset. The GAN does this by using two networks that compete with one another in an adversarial fashion. These networks are called the <strong class="bold">generator</strong> and <strong class="bold">discriminator</strong>. </p>
			<p>To see how these networks compete with one another, consider the following example. The example will skip over a few details that will make more sense as you get to them later in the chapter. </p>
			<p>Imagine two entities: a money counterfeiter and a business owner. The counterfeiter attempts to make a currency that looks authentic to fool the business owner into thinking the currency is legitimate. By contrast, the business owner tries to identify any fake bills, so that they don't end up with just a piece of worthless paper instead of real currency. </p>
			<p>This is essentially what GANs do. The counterfeiter in this example is the generator, and the business owner is the discriminator. The generator creates an image and passes it to the discriminator. The discriminator checks whether the image is real or not, and both networks compete against each other, driving improvements within one another. </p>
			<p>The generator's mission is to create a synthetic sample of data that can fool the discriminator. The generator will try to trick the discriminator into thinking that the sample is real. The discriminator's mission is to be able to correctly classify a synthetic sample created by the generator.</p>
			<div>
				<div id="_idContainer383" class="IMG---Figure">
					<img src="image/B16341_11_11.jpg" alt="Figure 11.11: GAN-generated images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.11: GAN-generated images</p>
			<p>The next sections will look a bit closer at the generator and discriminator and how they function individually, before considering both in combination in the <em class="italic">The Adversarial Network</em> section.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor232"/>The Generator Network </h2>
			<p>As discussed, GANs are utilized for unsupervised learning tasks in machine learning. GANs consist of two models (a generator and a discriminator) that automatically discover and learn the patterns in input data. The two models compete with one another to analyze, capture, and create variations within data. GANs can be used to generate new data that looks like it could have come from the original data.</p>
			<p>First up is the generator model. How does the generator create synthetic data?</p>
			<p>The generator receives input as a <em class="italic">fixed-length random vector</em> called the <strong class="bold">latent vector</strong>, which goes into the generator network. This is sometimes referred to as the <strong class="bold">random noise seed</strong>. A new sample is generated from it. The generated instance is then sent to the discriminator for classification. Through random noise, the generator learns which outputs were more convincing and continues to improve in that direction. </p>
			<div>
				<div id="_idContainer384" class="IMG---Figure">
					<img src="image/B16341_11_12.jpg" alt="Figure 11.12: Input and output model in the generator network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.12: Input and output model in the generator network</p>
			<p>In the following figure, you can see that the discriminator takes input from both real data and the generator. The generator neural network attempts to generate data that looks real to the discriminator. </p>
			<p>The generator doesn't get to see what the real data is. The main goal of the generator is to convince the discriminator to classify its output as real. </p>
			<div>
				<div id="_idContainer385" class="IMG---Figure">
					<img src="image/B16341_11_13.jpg" alt="Figure 11.13: Two sources of data for the discriminator model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.13: Two sources of data for the discriminator model</p>
			<p>The GAN includes the following components:</p>
			<ul>
				<li>Noisy input vector</li>
				<li>Discriminator network </li>
				<li>Generator loss</li>
			</ul>
			<p>Backpropagation is used to adjust the weights in the optimal direction by calculating a weight's impact on the output. The backpropagation method is used to obtain gradients and these gradients can help change the generator weights.</p>
			<div>
				<div id="_idContainer386" class="IMG---Figure">
					<img src="image/B16341_11_14.jpg" alt="Figure 11.14: Backpropagation in GAN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.14: Backpropagation in GAN</p>
			<p>The basic procedure of a single generator iteration looks something like this: </p>
			<ol>
				<li value="1">Based on real data from a dataset, <em class="italic">sample random noise</em> is used. </li>
				<li>The <em class="italic">generator</em> produces <em class="italic">output</em> from the noise. </li>
				<li>The <em class="italic">discriminator</em> classifies the output as "<em class="italic">real</em>" or "<em class="italic">fake</em>." </li>
				<li>The <em class="italic">loss</em> from this classification is calculated, followed by <em class="italic">backpropagation through the generator</em> and <em class="italic">discriminator</em> to obtain the <em class="italic">gradients</em>. </li>
				<li>The <em class="italic">gradients</em> are used to adjust the generator <em class="italic">weights</em>.</li>
			</ol>
			<p>Now, to code the generator, the first step is to define your generator model. You begin by creating your generator function with <strong class="source-inline">define_your_gen</strong>. The number of outputs of your generator should match the size of the data you are trying to synthesize. Therefore, the final layer of your generator should be a dense layer with the number of units equal to the expected size of the output:</p>
			<p class="source-code">model.add(Dense(n_outputs, activation='linear'))</p>
			<p>The model will not compile because it does not directly fit the generator model.</p>
			<p>The code block will look something like the following:</p>
			<p class="source-code">def define_your_gen(latent_dim, n_outputs=2):</p>
			<p class="source-code">    model = Sequential()</p>
			<p class="source-code">    model.add(Dense(5, activation='relu', \</p>
			<p class="source-code">                    kernel_initializer='he_uniform', \</p>
			<p class="source-code">                    input_dim=latent_dim))</p>
			<p class="source-code">    model.add(Dense(n_outputs, activation='linear'))</p>
			<p class="source-code">    return model</p>
			<p>The generator composes one half of the GAN; the other half is the discriminator.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor233"/>The Discriminator Network</h2>
			<p>A <strong class="bold">discriminator</strong> is a neural network model that learns to identify real data from the fake data that the generator sends as input. The two sources of training data are the authentic data samples and the fake generator samples:</p>
			<ul>
				<li>Real data instances are used by the discriminator as positive samples during the training.</li>
				<li>Synthetic data instances created by the generator are used as fake examples during the training process.</li>
			</ul>
			<div>
				<div id="_idContainer387" class="IMG---Figure">
					<img src="image/B16341_11_15.jpg" alt="Figure 11.15: Inputs for the discriminator network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.15: Inputs for the discriminator network</p>
			<p>During the discriminator training process, the discriminator is connected to the generator and discriminator loss. It requires both real data and synthetic data from the generator, but only uses the discriminator loss for weight updates. </p>
			<div>
				<div id="_idContainer388" class="IMG---Figure">
					<img src="image/B16341_11_16.jpg" alt="Figure 11.16: Backpropagation with discriminator loss&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.16: Backpropagation with discriminator loss</p>
			<p>Now let's take a look at how the discriminator works with some code.</p>
			<p>Your first step is to define your discriminator model with <strong class="source-inline">define_disc()</strong>.</p>
			<p>The model takes a vector from your generator and makes a prediction as to whether the sample is real or fake. Therefore, you use binary classification.</p>
			<p>You're creating a simple GAN, so you will only need one hidden layer. Use <strong class="source-inline">model.add(Dense(25)</strong> to create the hidden layer.</p>
			<p>Again, your activation function will be ReLU with <strong class="source-inline">activation='relu'</strong> and the <strong class="source-inline">he_uniform</strong> weight initialization with <strong class="source-inline">kernel_initializer='he_uniform'</strong>.</p>
			<p>Your output layer will only need a single node for binary classification. To ensure your output is zero or one, you will use the sigmoid activation function:</p>
			<p class="source-code">model.add(Dense(1, activation='sigmoid'))</p>
			<p>The model will attempt to minimize your loss function. Use Adam for your stochastic gradient descent:</p>
			<p class="source-code">model.compile(loss='binary_crossentropy', \</p>
			<p class="source-code">              optimizer='adam', metrics=['accuracy'])</p>
			<p>Here's a look at your discriminator model code:</p>
			<p class="source-code">def define_disc(n_inputs=2):</p>
			<p class="source-code">    model = Sequential()</p>
			<p class="source-code">    model.add(Dense(25, activation='relu', \</p>
			<p class="source-code">                    kernel_initializer='he_uniform', \</p>
			<p class="source-code">                    input_dim=n_inputs))</p>
			<p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p>
			<p class="source-code">    model.compile(loss='binary_crossentropy', \</p>
			<p class="source-code">                  optimizer='adam', metrics=['accuracy'])</p>
			<p class="source-code">    return model</p>
			<p>Now that you know how to create both models that compose the GAN, you can learn how to combine them to create your GAN in the next section.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor234"/>The Adversarial Network</h2>
			<p>GANs consist of two networks, a generator, which is represented as <img src="image/B16341_11_16a.png" alt="16a"/>, and a discriminator, represented as <img src="image/B16341_11_16b.png" alt="16b"/>. Both networks play an adversarial game. The generator network tries to learn the underlying distribution of the training data and generates similar samples, while the discriminator network tries to catch the fake samples generated by the generator. </p>
			<p>The generator network takes a sample and generates a fake sample of data. The generator is trained to increase the probability of the discriminator network making mistakes. The discriminator network decides whether the data is generated or taken from the real sample using binary classification with the help of a sigmoid function. The sigmoid function ensures that the output is zero or one.</p>
			<p>The following list represents an overview of a typical GAN at work:</p>
			<ol>
				<li value="1">First, a <em class="italic">noise vector</em> or the <em class="italic">input vector</em> is fed to the generator network. </li>
				<li>The generator creates synthetic data samples.</li>
				<li>Authentic data is passed to the discriminator along with the synthetic data.</li>
				<li>The discriminator then identifies the data and classifies it as real or fake.</li>
				<li>The model is trained and the loss backpropagated into both the discriminator and generator networks.</li>
			</ol>
			<div>
				<div id="_idContainer391" class="IMG---Figure">
					<img src="image/B16341_11_17.jpg" alt="Figure 11.17: GAN model with input and output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.17: GAN model with input and output</p>
			<p>To code an adversarial network, the following steps are necessary. Each of these is described in detail in the following sections:</p>
			<ol>
				<li value="1">Combine the generator and discriminator models in your GAN.</li>
				<li>Generate real samples with class labels.</li>
				<li>Create points in latent space to use as input for the generator.</li>
				<li>Use the generator to create fake samples.</li>
				<li>Evaluate the discriminator performance.</li>
				<li>Train the generator and discriminator.</li>
				<li>Create the latent space, generator, discriminator, and GAN, and train the GAN on the training data.</li>
			</ol>
			<p>Now that you've explored the inner workings of the generator and discriminator, take a look at how you can combine the models to compete with one another. </p>
			<h3 id="_idParaDest-211"><a id="_idTextAnchor235"/>Combining the Generative and Discriminative Models</h3>
			<p>The <strong class="source-inline">define_your_gan()</strong> function creates your combined model. </p>
			<p>While creating the combined GAN model, freeze the weights of the discriminator model by specifying <strong class="source-inline">discriminator.trainable = False</strong>. This prevents the discriminator weights from getting updated while you update the generator weights.</p>
			<p>Now, you can add both models with <strong class="source-inline">model.add(generator)</strong> and <strong class="source-inline">model.add(discriminator)</strong>.</p>
			<p>Then, specify <strong class="source-inline">binary_crossentropy</strong> as the loss function and Adam as your optimizer while compiling your model:</p>
			<p class="source-code">def define_your_gan(generator, discriminator):</p>
			<p class="source-code">    discriminator.trainable = False</p>
			<p class="source-code">    model = Sequential()</p>
			<p class="source-code">    model.add(generator)</p>
			<p class="source-code">    model.add(discriminator)</p>
			<p class="source-code">    model.compile(loss='binary_crossentropy', optimizer='adam')</p>
			<p class="source-code">    return model</p>
			<h3 id="_idParaDest-212"><a id="_idTextAnchor236"/>Generating Real Samples with Class Labels</h3>
			<p>Now extract real samples from the dataset to inspect fake samples against them. You can use the <strong class="source-inline">generate_real()</strong> function defined previously. In the first line of the function, <strong class="source-inline">rand(n) – 0.5</strong>, create random numbers on <strong class="source-inline">n</strong> in the range of <strong class="source-inline">-0.5</strong> to <strong class="source-inline">0.5</strong>. Use <strong class="source-inline">hstack</strong> to stack your array. Now you can generate class labels with <strong class="source-inline">y = ones((n, 1))</strong>:</p>
			<p class="source-code">def generate_real(n):</p>
			<p class="source-code">    X1 = rand(n) - 0.5</p>
			<p class="source-code">    X2 = X1 * X1</p>
			<p class="source-code">    X1 = X1.reshape(n, 1)</p>
			<p class="source-code">    X2 = X2.reshape(n, 1)</p>
			<p class="source-code">    X = hstack((X1, X2))</p>
			<p class="source-code">    y = ones((n, 1))</p>
			<p class="source-code">    return X, y</p>
			<h3 id="_idParaDest-213"><a id="_idTextAnchor237"/>Creating Latent Points for the Generator</h3>
			<p>Next, use the generator model to create fake samples. You need to generate the same number of points in the latent space with your <strong class="source-inline">gen_latent_points()</strong> function. These latent points will be passed to the generator to create samples. This function generates uniformly random samples from NumPy's <strong class="source-inline">randn</strong> function. The number will correspond to the latent dimension multiplied by the number of samples to generate. This array of random numbers will then be reshaped to match the expected input of the generator:</p>
			<p class="source-code">def gen_latent_points(latent_dim, n):</p>
			<p class="source-code">    x_input = randn(latent_dim * n)</p>
			<p class="source-code">    x_input = x_input.reshape(n, latent_dim)</p>
			<p class="source-code">    return x_input</p>
			<h3 id="_idParaDest-214"><a id="_idTextAnchor238"/>Using the Generator to Generate Fake Samples and Class Labels</h3>
			<p>The <strong class="source-inline">gen_fake()</strong> function generates fake samples with a class label of zero. This function generates the latent points using the function created in the previous step. Then, the generator will generate samples based on the latent points. Finally, the class label, <strong class="source-inline">y</strong>,is generated as an array of zeros representing the fact that this is synthetic data:</p>
			<p class="source-code">def gen_fake(generator, latent_dim, n):</p>
			<p class="source-code">    x_input = gen_latent_points(latent_dim, n)</p>
			<p class="source-code">    X = generator.predict(x_input)</p>
			<p class="source-code">    y = zeros((n, 1))</p>
			<p class="source-code">    return X, y</p>
			<h3 id="_idParaDest-215"><a id="_idTextAnchor239"/>Evaluating the Discriminator Model</h3>
			<p>The following <strong class="source-inline">performance_summary()</strong> function is used to plot both real and fake data points. The function generates real values and synthetic data and evaluates the performance of the discriminator via its accuracy in identifying the synthetic images. Then, it finally plots both the real and synthetic images for visual review:</p>
			<p class="source-code">def performance_summary(epoch, generator, \</p>
			<p class="source-code">                        discriminator, latent_dim, n=100):</p>
			<p class="source-code">    x_real, y_real = generate_real(n)</p>
			<p class="source-code">    _, acc_real = discriminator.evaluate\</p>
			<p class="source-code">                  (x_real, y_real, verbose=0)</p>
			<p class="source-code">    x_fake, y_fake = gen_fake\</p>
			<p class="source-code">                     (generator, latent_dim, n)</p>
			<p class="source-code">    _, acc_fake = discriminator.evaluate\</p>
			<p class="source-code">                  (x_fake, y_fake, verbose=0)</p>
			<p class="source-code">    print(epoch, acc_real, acc_fake)</p>
			<p class="source-code">    plt.scatter(x_real[:, 0], x_real[:, 1], color='green')</p>
			<p class="source-code">    plt.scatter(x_fake[:, 0], x_fake[:, 1], color='red')</p>
			<p class="source-code">    plt.show()</p>
			<h3 id="_idParaDest-216"><a id="_idTextAnchor240"/>Training the Generator and Discriminator</h3>
			<p>Now, train your model with the <strong class="source-inline">train()</strong> function. This function contains a <strong class="source-inline">for</strong> loop to iterate through the epochs. At each epoch, real data is sampled with a size equal to half the batch, and then synthetic data is generated. Then, the discriminator trains on the real, followed by the synthetic, data. Then, the GAN model is trained. When the epoch number is a multiple of the input argument, <strong class="source-inline">n_eval</strong>, a performance summary is generated:</p>
			<p class="source-code">def train(g_model, d_model, your_gan_model, \</p>
			<p class="source-code">          latent_dim, n_epochs=1000, n_batch=128, n_eval=100):</p>
			<p class="source-code">    half_batch = int(n_batch / 2)</p>
			<p class="source-code">    for i in range(n_epochs):</p>
			<p class="source-code">        x_real, y_real = generate_real(half_batch)</p>
			<p class="source-code">        x_fake, y_fake = gen_fake\</p>
			<p class="source-code">                         (g_model, latent_dim, half_batch)</p>
			<p class="source-code">        d_model.train_on_batch(x_real, y_real)</p>
			<p class="source-code">        d_model.train_on_batch(x_fake, y_fake)</p>
			<p class="source-code">        x_gan = gen_latent_points(latent_dim, n_batch)</p>
			<p class="source-code">        y_gan = ones((n_batch, 1))</p>
			<p class="source-code">        your_gan_model.train_on_batch(x_gan, y_gan)</p>
			<p class="source-code">        if (i+1) % n_eval == 0:</p>
			<p class="source-code">            performance_summary(i, g_model, d_model, latent_dim)</p>
			<h3 id="_idParaDest-217"><a id="_idTextAnchor241"/>Creating the Latent Space, Generator, Discriminator, GAN, and Training Data</h3>
			<p>You can combine all the steps to build and train the model. Here, <strong class="source-inline">latent_dim</strong> is set to <strong class="source-inline">5</strong>, representing five latent dimensions:</p>
			<p class="source-code">latent_dim = 5</p>
			<p class="source-code">generator = define_gen(latent_dim)</p>
			<p class="source-code">discriminator = define_discrim()</p>
			<p class="source-code">your_gan_model = define_your_gan(generator, discriminator)</p>
			<p class="source-code">train(generator, discriminator, your_gan_model, latent_dim)</p>
			<p>In this section, you learned about GANs, different components, the generator and discriminator, and how you combine them to create an adversarial network. You will now use these concepts to generate sequences with your own GAN.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor242"/>Exercise 11.02: Generating Sequences with GANs </h2>
			<p>In this exercise, you will use a GAN to create a model that generates a quadratic function (<strong class="source-inline">y=x</strong><span class="superscript">2</span>) for values of <strong class="source-inline">x</strong> between <strong class="source-inline">-0.5</strong> and <strong class="source-inline">0.5</strong>. You will create a generator that will simulate the normal distribution and then square the values to simulate the quadratic function. You will also create a discriminator that will discriminate between a true quadratic function and the output from the generator. Next, you will combine them to create your GAN model. Finally, you will train your GAN model and evaluate your model, comparing the results from the generator against a true quadratic function. </p>
			<p>Perform the following steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a new Jupyter or Colab notebook and import the following libraries:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from numpy import hstack, zeros, ones</p><p class="source-code">from numpy.random import rand, randn</p><p class="source-code">from keras.layers import Dense</p><p class="source-code">import matplotlib.pyplot as plt </p></li>
				<li>Define the generator model. Begin by creating your generator function with <strong class="source-inline">define_gen</strong>. <p>Use Keras' <strong class="source-inline">linear</strong> activation function for the last layer of the generator network because the output vector should consist of continuous real values as a normal distribution does. The first element of the output vector has a range of <strong class="source-inline">[-0.5,0.5]</strong>. Since you will only consider values of <strong class="source-inline">x</strong> between these two values, the second element has a range of <strong class="source-inline">[0.0,0.25]</strong>:</p><p class="source-code">def define_gen(latent_dim, n_outputs=2):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(15, activation='relu', \</p><p class="source-code">              kernel_initializer='he_uniform', \</p><p class="source-code">              input_dim=latent_dim))</p><p class="source-code">    model.add(Dense(n_outputs, activation='linear'))</p><p class="source-code">    return model</p></li>
				<li>Now, with <strong class="source-inline">define_disc()</strong>, define your discriminator. The discriminator network has a binary output that identifies whether the input is real or fake. For this reason, use sigmoid as the activation function and binary cross-entropy as your loss.<p>You're creating a simple GAN, so use one hidden layer with <strong class="source-inline">25</strong> nodes. Use ReLU activation and <strong class="source-inline">he_uniform</strong> weight initialization. Your output layer will only need a single node for binary classification. Use Adam as your optimizer. The model will attempt to minimize your loss function:</p><p class="source-code">def define_disc(n_inputs=2):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(25, activation='relu', \</p><p class="source-code">                    kernel_initializer='he_uniform', \</p><p class="source-code">                    input_dim=n_inputs))</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    model.compile(loss='binary_crossentropy', \</p><p class="source-code">                  optimizer='adam', metrics=['accuracy'])</p><p class="source-code">    return model</p></li>
				<li>Now, add both models with <strong class="source-inline">model.add(generator)</strong> and <strong class="source-inline">model.add(discriminator)</strong>. Then, specify binary cross-entropy as your loss function and Adam as your optimizer, while compiling your model:<p class="source-code">def define_your_gan(generator, discriminator):</p><p class="source-code">    discriminator.trainable = False</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(generator)</p><p class="source-code">    model.add(discriminator)</p><p class="source-code">    model.compile(loss='binary_crossentropy', optimizer='adam')</p><p class="source-code">    return model</p></li>
				<li>Extract real samples from your dataset to inspect fake samples against them. Use the <strong class="source-inline">generate_real()</strong> function defined previously. <strong class="source-inline">rand(n) – 0.5</strong> creates random numbers on <strong class="source-inline">n</strong> in the range of <strong class="source-inline">-0.5</strong> to <strong class="source-inline">0.5</strong>. Use <strong class="source-inline">hstack</strong> to stack your array. Now, generate class labels with<strong class="source-inline"> y = ones((n, 1))</strong>:<p class="source-code">def generate_real(n):</p><p class="source-code">    X1 = rand(n) - 0.5</p><p class="source-code">    X2 = X1 * X1</p><p class="source-code">    X1 = X1.reshape(n, 1)</p><p class="source-code">    X2 = X2.reshape(n, 1)</p><p class="source-code">    X = hstack((X1, X2))</p><p class="source-code">    y = ones((n, 1))</p><p class="source-code">    return X, y</p></li>
				<li>Next, set the generator model to create fake samples. Generate the same number of points in the latent space with your <strong class="source-inline">gen_latent_points()</strong> function. Then, pass them to the generator and use them to create samples:<p class="source-code">def gen_latent_points(latent_dim, n):</p><p class="source-code">    x_input = randn(latent_dim * n)</p><p class="source-code">    x_input = x_input.reshape(n, latent_dim)</p><p class="source-code">    return x_input</p></li>
				<li>Use the generator to generate fake samples with class labels:<p class="source-code">def gen_fake(generator, latent_dim, n):</p><p class="source-code">    x_input = gen_latent_points(latent_dim, n)</p><p class="source-code">    X = generator.predict(x_input)</p><p class="source-code">    y = zeros((n, 1))</p><p class="source-code">    return X, y</p></li>
				<li>Evaluate the discriminator model. The <strong class="source-inline">performance_summary()</strong> function will plot both real and fake data points:<p class="source-code">def performance_summary(epoch, generator, \</p><p class="source-code">                        discriminator, latent_dim, n=100):</p><p class="source-code">    x_real, y_real = generate_real(n)</p><p class="source-code">    _, acc_real = discriminator.evaluate\</p><p class="source-code">                  (x_real, y_real, verbose=0)</p><p class="source-code">    x_fake, y_fake = gen_fake\</p><p class="source-code">                     (generator, latent_dim, n)</p><p class="source-code">    _, acc_fake = discriminator.evaluate\</p><p class="source-code">                  (x_fake, y_fake, verbose=0)</p><p class="source-code">    print(epoch, acc_real, acc_fake)</p><p class="source-code">    plt.scatter(x_real[:, 0], x_real[:, 1], color='green')</p><p class="source-code">    plt.scatter(x_fake[:, 0], x_fake[:, 1], color='red')</p><p class="source-code">    plt.show()</p></li>
				<li>Now, train your model with the <strong class="source-inline">train()</strong> function:<p class="source-code">def train(g_model, d_model, your_gan_model, \</p><p class="source-code">          latent_dim, n_epochs=1000, \</p><p class="source-code">          n_batch=128, n_eval=100):</p><p class="source-code">    half_batch = int(n_batch / 2)</p><p class="source-code">    for i in range(n_epochs):</p><p class="source-code">        x_real, y_real = generate_real(half_batch)</p><p class="source-code">        x_fake, y_fake = gen_fake\</p><p class="source-code">                         (g_model, latent_dim, half_batch)</p><p class="source-code">        d_model.train_on_batch(x_real, y_real)</p><p class="source-code">        d_model.train_on_batch(x_fake, y_fake)</p><p class="source-code">        x_gan = gen_latent_points(latent_dim, n_batch)</p><p class="source-code">        y_gan = ones((n_batch, 1))</p><p class="source-code">        your_gan_model.train_on_batch(x_gan, y_gan)</p><p class="source-code">        if (i+1) % n_eval == 0:</p><p class="source-code">            performance_summary(i, g_model, d_model, latent_dim)</p></li>
				<li>Create a parameter for the latent dimension and set it equal to <strong class="source-inline">5</strong>. Then, create a generator, discriminator, and GAN using the respective functions. Train the generator, discriminator, and GAN models using the <strong class="source-inline">train</strong> function:<p class="source-code">latent_dim = 5</p><p class="source-code">generator = define_gen(latent_dim)</p><p class="source-code">discriminator = define_disc()</p><p class="source-code">your_gan_model = define_your_gan(generator, discriminator)</p><p class="source-code">train(generator, discriminator, your_gan_model, latent_dim)</p><p>You will get the following output:</p><div id="_idContainer392" class="IMG---Figure"><img src="image/B16341_11_18.jpg" alt="Figure 11.18: Distribution of real and fake data&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.18: Distribution of real and fake data</p>
			<p>The output shows the generator progressively improving by generating points that more closely resemble a quadratic function. In early epochs, the points generated by the generator, indicated by the blue dots, show little similarity to the true quadratic function, indicated by the red dots. However, by the final epoch, the points generated by the generator almost lie on top of the true points, demonstrating that the generator has almost captured the true underlying function – the quadratic.</p>
			<p>In this exercise, you utilized the different components of a generative model to create data that fits a quadratic function. As you can see in <em class="italic">Figure 11.18</em>, by the final epoch, the fake data resembles the real data, showing that the generator can capture the quadratic function well. </p>
			<p>Now it's time for the final section of the book, on DCGANs, where you'll be creating your own images.</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor243"/>Deep Convolutional Generative Adversarial Networks (DCGANs)</h1>
			<p>DCGANs use convolutional neural networks instead of simple neural networks for both the discriminator and the generator. They can generate higher-quality images and are commonly used for this purpose.</p>
			<p>The generator is a set of convolutional layers with fractional stride convolutions, also known as transpose convolutions. Layers with transpose convolutions upsample the input image at every convolutional layer, which increases the spatial dimensions of the images after each layer. </p>
			<p>The discriminator is a set of convolutional layers with stride convolutions, so it downsamples the input image at every convolutional layer, decreasing the spatial dimensions of the images after each layer.</p>
			<p>Consider the following two images. Can you identify which one is fake and which one is real? Take a moment and look carefully at each of them. </p>
			<div>
				<div id="_idContainer393" class="IMG---Figure">
					<img src="image/B16341_11_19.jpg" alt="Figure 11.19: Face example&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.19: Face example</p>
			<p>You may be surprised to find out that neither of the images shown is of real people. These images were created using images of real people, but they are not of real people. They were created by two competing neural networks. </p>
			<p>As you know, a GAN is composed of two different neural networks: the discriminator and the generator. What looks different right away is that each of these networks has different inputs and outputs. This is key to understanding how GANs can do what they do.</p>
			<p>For the discriminator, the input is an image—a 3D tensor (height, width, color). The output is a single number that is used to make the classification. In <em class="italic">Figure 11.20</em>, you can see <strong class="source-inline">[0.95]</strong>. It implies there is a 95% chance that the tomato image is real.</p>
			<p>For the generator, the input is a generated random seed vector of numbers. The output is an image.</p>
			<p>The generator network learns to generate images similar to the ones in the dataset, while the discriminator learns to discriminate the original images from the generated ones. In this competitive fashion, they learn to generate realistic images like the ones in the training dataset.</p>
			<div>
				<div id="_idContainer394" class="IMG---Figure">
					<img src="image/B16341_11_20.jpg" alt="Figure 11.20: Discriminator and generator networks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.20: Discriminator and generator networks</p>
			<p>Let's take a look at how the generator trains. One of the key points to take away from <em class="italic">Figure 11.20</em> is that the generator network has <em class="italic">weights static</em>, while the discriminator network shows <em class="italic">weights trained</em>. This is important because this enables you to differentiate how the GAN loss function changes from updates to the weights on the generator and discriminator independently. </p>
			<p>Note that <strong class="source-inline">X</strong> (the random seed) is fed into the model to produce <strong class="source-inline">y</strong>. Your model outputs what you predict.</p>
			<div>
				<div id="_idContainer395" class="IMG---Figure">
					<img src="image/B16341_11_21.jpg" alt="Figure 11.21: How the generator is trained&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.21: How the generator is trained</p>
			<p>Another important point to keep in mind is that the generator trains without ever seeing any of the real data. The generator's only goal is to fool the discriminator.</p>
			<p>Now, consider the training process of the discriminator network. The discriminator is trained on a training dataset consisting of an equal number of real and fake (generated) images. The real images are sampled randomly from the original dataset and are labeled as one. An equal number of fake images is generated using the generator network and are labeled as zero.</p>
			<div>
				<div id="_idContainer396" class="IMG---Figure">
					<img src="image/B16341_11_22.jpg" alt="Figure 11.22: How the discriminator is trained&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.22: How the discriminator is trained</p>
			<p>The core differences between the original "vanilla" GAN and DCGAN correspond to the differences in the architecture. Pooling layers of the vanilla GAN are replaced with transposed convolutions in the generator and stride convolutions in the discriminator of the DCGAN. The generator and discriminator of DCGANs both use batch normalization layers, except for the generator output layer and the discriminator input layer. Also, the fully connected hidden layers of DCGANs are removed. Finally, the activation functions in DCGANs are generally different to reflect the use of convolutional layers. In the generator, ReLU is used for all layers except for the output layer, where tanh is used, and for the discriminator, Leaky ReLU is used for all layers. </p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor244"/>Training a DCGAN</h2>
			<p>To start, you're going to set all the constants that will define your DCGAN.</p>
			<p>The resolution of the images that you want to generate is specified by the <strong class="source-inline">gen_res</strong> parameter. The final resolution will be <strong class="source-inline">32*gen_res</strong> for the height and width of the image. You will use <strong class="source-inline">gen_res = 3</strong>, which results in an image resolution of <strong class="source-inline">96x96</strong>.</p>
			<p>Image channels, <strong class="source-inline">img_chan</strong>, are simply how many numbers per pixel the image has. For color, you need a pixel value for each of the three color channels: <strong class="bold">red, green, and blue</strong> (<strong class="bold">RGB</strong>). So, your image channel should be set to <strong class="source-inline">3</strong>.</p>
			<p>Your preview image rows and columns (<strong class="source-inline">img_rows</strong> and <strong class="source-inline">img_cols</strong>) will be how many images you want to display in a row and a column. For example, if you were to choose a preview image row of <strong class="source-inline">4</strong>, and a preview column value of <strong class="source-inline">4</strong>, you would get a total of 16 images displayed.</p>
			<p><strong class="source-inline">data_path</strong> is where your data is stored on your computer. This provides the path needed for the code to access and store data.</p>
			<p><strong class="source-inline">epoch</strong> is the number of passes when training the data.</p>
			<p>Batch size, <strong class="source-inline">num_batch</strong>, is the number of training samples per iteration.</p>
			<p>Buffer size, <strong class="source-inline">num_buffer</strong>, is the random shuffle that is used. You will simply set this to your dataset size.</p>
			<p>Seed vector, <strong class="source-inline">seed_vector</strong>, is the size of the vector of seeds that will be used to generate images.</p>
			<p>Consider the following sample to see how to initialize all the constants that define your DCGAN:</p>
			<p class="source-code">gen_res = 3</p>
			<p class="source-code">gen_square = 32 * gen_res</p>
			<p class="source-code">img_chan = 3</p>
			<p class="source-code">img_rows = 5</p>
			<p class="source-code">img_cols = 5</p>
			<p class="source-code">img_margin = 16</p>
			<p class="source-code">seed_vector = 200</p>
			<p class="source-code">data_path = '/content/drive/MyDrive/Datasets\</p>
			<p class="source-code">            '/apple-or-tomato/training_set/'</p>
			<p class="source-code">epochs = 1000</p>
			<p class="source-code">num_batch = 32</p>
			<p class="source-code">num_buffer = 1000</p>
			<p>Now you can build the generator and the discriminator. Start by defining your generator function with <strong class="source-inline">def create_generator</strong>, using <strong class="source-inline">seed_size</strong> and <strong class="source-inline">channels</strong> as arguments:</p>
			<p class="source-code">def create_generator(seed_size, channels):</p>
			<p class="source-code">    model = Sequential()</p>
			<p>Now, you will create the generated image that is going to come from an <em class="italic">input seed</em>; different seed numbers will generate different images and your seed size will determine how many different images are generated.</p>
			<p>Next, add a dense layer with <strong class="source-inline">4*4*256</strong> as the dimensionality of your output space, and use the ReLU activation function. <strong class="source-inline">input_dim</strong> is an input shape, which you will have equal to <strong class="source-inline">seed_size</strong>.</p>
			<p>Use the following code to add a layer that reshapes your inputs to match your output space of <strong class="source-inline">4*4*256</strong>:</p>
			<p class="source-code">model.add(Reshape((4,4,256)))</p>
			<p>Your <strong class="source-inline">UpSampling2D</strong> layer is a simple layer that doubles the dimensions of input. It must be followed by a convolutional layer (<strong class="source-inline">Conv2D</strong>):</p>
			<p class="source-code">model.add(UpSampling2D())</p>
			<p>Add your <strong class="source-inline">Conv2D</strong> layer with <strong class="source-inline">256</strong> as your input. You can choose <strong class="source-inline">kernel_size=3</strong> for your <strong class="source-inline">3x3</strong> convolution filter. With <strong class="source-inline">padding="same"</strong>, you can ensure that the layer's outputs will have the same spatial dimensions as its inputs:</p>
			<p class="source-code">model.add(Conv2D(256,kernel_size=3,padding="same"))</p>
			<p>Use batch normalization to normalize your individual layers and help prevent gradient problems. Momentum can be anywhere in the range of <strong class="source-inline">0.0</strong> to <strong class="source-inline">0.99</strong>. Here, use <strong class="source-inline">momentum=0.8</strong>:</p>
			<p class="source-code">model.add(BatchNormalization(momentum=0.8))</p>
			<p>On your final CNN layer, you will use the tanh activation function to ensure that your output images are in the range <strong class="source-inline">-1</strong> to <strong class="source-inline">1</strong>:</p>
			<p class="source-code">model.add(Conv2D(channels,kernel_size=3,padding="same"))</p>
			<p class="source-code">model.add(Activation("tanh"))</p>
			<p>The complete code block should look like this:</p>
			<p class="source-code">def create_generator(seed_size, channels):</p>
			<p class="source-code">    model = Sequential()</p>
			<p class="source-code">    model.add(Dense(4*4*256,activation="relu", \</p>
			<p class="source-code">                    input_dim=seed_size))</p>
			<p class="source-code">    model.add(Reshape((4,4,256)))</p>
			<p class="source-code">    model.add(UpSampling2D())</p>
			<p class="source-code">    model.add(Conv2D(256,kernel_size=3,padding="same"))</p>
			<p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p>
			<p class="source-code">    model.add(Activation("relu"))</p>
			<p class="source-code">    model.add(UpSampling2D())</p>
			<p class="source-code">    model.add(Conv2D(256,kernel_size=3,padding="same"))</p>
			<p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p>
			<p class="source-code">    model.add(Activation("relu"))</p>
			<p class="source-code">   </p>
			<p class="source-code">    model.add(UpSampling2D())</p>
			<p class="source-code">    model.add(Conv2D(128,kernel_size=3,padding="same"))</p>
			<p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p>
			<p class="source-code">    model.add(Activation("relu"))</p>
			<p class="source-code">    if gen_res&gt;1:</p>
			<p class="source-code">      model.add(UpSampling2D(size=(gen_res,gen_res)))</p>
			<p class="source-code">      model.add(Conv2D(128,kernel_size=3,padding="same"))</p>
			<p class="source-code">      model.add(BatchNormalization(momentum=0.8))</p>
			<p class="source-code">      model.add(Activation("relu"))</p>
			<p class="source-code">    model.add(Conv2D(channels,kernel_size=3,padding="same"))</p>
			<p class="source-code">    model.add(Activation("tanh"))</p>
			<p class="source-code">    return model</p>
			<p>Now you can define your discriminator:</p>
			<p class="source-code">def create_discriminator(image_shape):</p>
			<p class="source-code">    model = Sequential()</p>
			<p>Here, use a <strong class="source-inline">Conv2D</strong> layer. You can choose <strong class="source-inline">kernel_size=3</strong> for your <strong class="source-inline">3x3</strong> convolution filter. With <strong class="source-inline">strides=2</strong>, you specify how many strides are for your "sliding window." Set <strong class="source-inline">input_shape=image_shape</strong> to ensure they match, and again, with <strong class="source-inline">padding="same"</strong>, you ensure that the layer's outputs will have the same spatial dimensions as its inputs. Add a LeakyReLU activation function after the <strong class="source-inline">Conv2D</strong> layer for all discriminator layers:</p>
			<p class="source-code">model.add(Conv2D(32, kernel_size=3, \</p>
			<p class="source-code">                 strides=2, input_shape=image_shape, \</p>
			<p class="source-code">                 padding="same"))</p>
			<p class="source-code">model.add(LeakyReLU(alpha=0.2))</p>
			<p>The <strong class="source-inline">Flatten</strong> layer converts your data into a single feature vector for input into your last layer:</p>
			<p class="source-code">model.add(Flatten())</p>
			<p>For your activation function, use sigmoid for binary classification output:</p>
			<p class="source-code">model.add(Dense(1, activation='sigmoid'))</p>
			<p>The complete code block should look like this:</p>
			<p class="source-code">def create_discriminator(image_shape):</p>
			<p class="source-code">    model = Sequential()</p>
			<p class="source-code">    model.add(Conv2D(32, kernel_size=3, strides=2, \</p>
			<p class="source-code">                     input_shape=image_shape, </p>
			<p class="source-code">                     padding="same"))</p>
			<p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p>
			<p class="source-code">    model.add(Dropout(0.25))</p>
			<p class="source-code">    model.add(Conv2D(64, kernel_size=3, strides=2, \</p>
			<p class="source-code">                     padding="same"))</p>
			<p class="source-code">    model.add(ZeroPadding2D(padding=((0,1),(0,1))))</p>
			<p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p>
			<p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p>
			<p class="source-code">    model.add(Dropout(0.25))</p>
			<p class="source-code">    model.add(Conv2D(128, kernel_size=3, strides=2, \</p>
			<p class="source-code">                     padding="same"))</p>
			<p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p>
			<p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p>
			<p class="source-code">    model.add(Dropout(0.25))</p>
			<p class="source-code">    model.add(Conv2D(256, kernel_size=3, strides=1, \</p>
			<p class="source-code">                     padding="same"))</p>
			<p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p>
			<p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p>
			<p class="source-code">    model.add(Dropout(0.25))</p>
			<p class="source-code">    model.add(Conv2D(512, kernel_size=3, \</p>
			<p class="source-code">                     strides=1, padding="same"))</p>
			<p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p>
			<p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p>
			<p class="source-code">    model.add(Dropout(0.25))</p>
			<p class="source-code">    model.add(Flatten())</p>
			<p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p>
			<p class="source-code">    return model</p>
			<p>Next, create your loss functions. Since the outputs of the discriminator and generator networks are different, you need to define two separate loss functions for them. Moreover, they need to be trained separately in independent passes through the networks. </p>
			<p>You can use <strong class="source-inline">tf.keras.losses.BinaryCrossentropy</strong> for <strong class="source-inline">cross_entropy</strong>. This calculates the loss between true and predicted labels. Then, define the <strong class="source-inline">discrim_loss</strong> function from your <strong class="source-inline">real_output</strong> and <strong class="source-inline">fake_output</strong> parameters using <strong class="source-inline">tf.ones</strong> and <strong class="source-inline">tf.zeros</strong> to calculate <strong class="source-inline">total_loss</strong>:</p>
			<p class="source-code">cross_entropy = tf.keras.losses.BinaryCrossentropy()</p>
			<p class="source-code">def discrim_loss(real_output, fake_output):</p>
			<p class="source-code">    real_loss = cross_entropy(tf.ones_like(real_output), \</p>
			<p class="source-code">                              real_output)</p>
			<p class="source-code">    fake_loss = cross_entropy(tf.zeros_like(fake_output), \</p>
			<p class="source-code">                              fake_output)</p>
			<p class="source-code">    total_loss = real_loss + fake_loss</p>
			<p class="source-code">    return total_loss</p>
			<p class="source-code">def gen_loss(fake_output):</p>
			<p class="source-code">    return cross_entropy(tf.ones_like(fake_output), \</p>
			<p class="source-code">                         fake_output)</p>
			<p>The Adam optimizer is used for the generator and discriminator, with the same learning rate and momentum:</p>
			<p class="source-code">gen_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)</p>
			<p class="source-code">disc_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)</p>
			<p>Here, you have your individual training step. It's very important that you only modify one network's weights at a time. With <strong class="source-inline">tf.GradientTape()</strong>, you can train the discriminator and generator at the same time, but separately from one another. This is how TensorFlow does automatic differentiation. It calculates the derivatives. You'll see that it creates two "tapes" – <strong class="source-inline">gen_tape</strong> and <strong class="source-inline">disc_tape</strong>. </p>
			<p>Then, create <strong class="source-inline">real_output</strong> and <strong class="source-inline">fake_output</strong> for the discriminator. Use this for the generator loss (<strong class="source-inline">g_loss</strong>). Now, you can calculate the discriminator loss (<strong class="source-inline">d_loss</strong>), calculate the gradients of both the generator and discriminator with <strong class="source-inline">gradients_of_generator</strong> and <strong class="source-inline">gradients_of_discriminator</strong>, and apply them:</p>
			<p class="source-code">@tf.function</p>
			<p class="source-code">def train_step(images):</p>
			<p class="source-code">    seed = tf.random.normal([num_batch, seed_vector])</p>
			<p class="source-code">    with tf.GradientTape() as gen_tape, \</p>
			<p class="source-code">              tf.GradientTape() as disc_tape:</p>
			<p class="source-code">    gen_imgs = generator(seed, training=True)</p>
			<p class="source-code">    real_output = discriminator(images, training=True)</p>
			<p class="source-code">    fake_output = discriminator(gen_imgs, training=True)</p>
			<p class="source-code">    g_loss = gen_loss(fake_output)</p>
			<p class="source-code">    d_loss = discrim_loss(real_output, fake_output)</p>
			<p class="source-code">    </p>
			<p class="source-code">    gradients_of_generator = gen_tape.gradient(\</p>
			<p class="source-code">        g_loss, generator.trainable_variables)</p>
			<p class="source-code">    gradients_of_discriminator = disc_tape.gradient(\</p>
			<p class="source-code">        d_loss, discriminator.trainable_variables)</p>
			<p class="source-code">    gen_optimizer.apply_gradients(zip(</p>
			<p class="source-code">        gradients_of_generator, generator.trainable_variables))</p>
			<p class="source-code">    disc_optimizer.apply_gradients(zip(</p>
			<p class="source-code">        gradients_of_discriminator, </p>
			<p class="source-code">        discriminator.trainable_variables))</p>
			<p class="source-code">    return g_loss,d_loss</p>
			<p>Next, create a number of fixed seeds with <strong class="source-inline">fixed_seeds</strong>, a seed for each image displayed, and for each seed vector. This is done so you can track the same images, observing the changes over time. With <strong class="source-inline">for epoch in range</strong>, you are tracking your time. Loop through each batch with <strong class="source-inline">for image_batch in dataset</strong>. Now, continue to track your loss for both the generator and discriminator with <strong class="source-inline">generator_loss</strong> and <strong class="source-inline">discriminator_loss</strong>. Now you have a nice display of all this information as it trains:</p>
			<p class="source-code">def train(dataset, epochs):</p>
			<p class="source-code">    fixed_seed = np.random.normal\</p>
			<p class="source-code">                (0, 1, (img_rows * img_cols, seed_vector))</p>
			<p class="source-code">    start = time.time()</p>
			<p class="source-code">    for epoch in range(epochs):</p>
			<p class="source-code">        epoch_start = time.time()</p>
			<p class="source-code">        g_loss_list = []</p>
			<p class="source-code">        d_loss_list = []</p>
			<p class="source-code">        for image_batch in dataset:</p>
			<p class="source-code">            t = train_step(image_batch)</p>
			<p class="source-code">            g_loss_list.append(t[0])</p>
			<p class="source-code">            d_loss_list.append(t[1])</p>
			<p class="source-code">        generator_loss = sum(g_loss_list) / len(g_loss_list)</p>
			<p class="source-code">        discriminator_loss = sum(d_loss_list) / len(d_loss_list)</p>
			<p class="source-code">        epoch_elapsed = time.time()-epoch_start</p>
			<p class="source-code">        print (f'Epoch {epoch+1}, gen loss={generator_loss}', \</p>
			<p class="source-code">               f'disc loss={discriminator_loss},'\</p>
			<p class="source-code">               f' {time_string(epoch_elapsed)}')</p>
			<p class="source-code">        save_images(epoch,fixed_seed)</p>
			<p class="source-code">    elapsed = time.time()-start</p>
			<p class="source-code">    print (f'Training time: {time_string(elapsed)}')</p>
			<p>In this last section, you took an additional step in using generative networks. You learned how to train a DCGAN and how to utilize the generator and discriminator together to create your very own images.</p>
			<p>In the next exercise, you will implement what you have learned so far in this section.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor245"/>Exercise 11.03: Generating Images with DCGAN </h2>
			<p>In this exercise, you will generate your own images from scratch using a DCGAN. You will build your DCGAN with a generator and discriminator that both have convolutional layers. Then, you will train your DCGAN on images of a tomato, and throughout the training process, you will output generated images from the generator to track the performance of the generator.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find <strong class="source-inline">tomato-or-apple</strong> dataset here: <a href="https://packt.link/6Z8vW">https://packt.link/6Z8vW</a>.</p>
			<p>For this exercise, it is recommended that you use Google Colab:</p>
			<ol>
				<li value="1">Load Google Colab and Google Drive:<p class="source-code">try:</p><p class="source-code">    from google.colab import drive</p><p class="source-code">    drive.mount('/content/drive', force_remount=True)</p><p class="source-code">    COLAB = True</p><p class="source-code">    print("Note: using Google CoLab")</p><p class="source-code">    %tensorflow_version 2.x</p><p class="source-code">except:</p><p class="source-code">    print("Note: not using Google CoLab")</p><p class="source-code">    COLAB = False</p><p>Your output should look something like this:</p><p class="source-code">Mounted at /content/drive</p><p class="source-code">Note: using Google Colab</p></li>
				<li>Import the relevant libraries: <p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.layers</p><p class="source-code">import Input, Reshape, Dropout, Dense </p><p class="source-code">from tensorflow.keras.layers</p><p class="source-code">import Flatten, BatchNormalization</p><p class="source-code">from tensorflow.keras.layers</p><p class="source-code">import Activation, ZeroPadding2D</p><p class="source-code">from tensorflow.keras.layers import LeakyReLU</p><p class="source-code">from tensorflow.keras.layers import UpSampling2D, Conv2D</p><p class="source-code">from tensorflow.keras.models</p><p class="source-code">import Sequential, Model, load_model</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">import zipfile</p><p class="source-code">import numpy as np</p><p class="source-code">from PIL import Image</p><p class="source-code">from tqdm import tqdm</p><p class="source-code">import os </p><p class="source-code">import time</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from skimage.io import imread</p></li>
				<li>Format a time string to track your time usage: <p class="source-code">def time_string(sec_elapsed):</p><p class="source-code">    hour = int(sec_elapsed / (60 * 60))</p><p class="source-code">    minute = int((sec_elapsed % (60 * 60)) / 60)</p><p class="source-code">    second = sec_elapsed % 60</p><p class="source-code">    return "{}:{:&gt;02}:{:&gt;05.2f}".format(hour, minute, second)</p></li>
				<li>Set the generation resolution to <strong class="source-inline">3</strong>. Also, set <strong class="source-inline">img_rows</strong> and <strong class="source-inline">img_cols</strong> to <strong class="source-inline">5</strong> and <strong class="source-inline">img_margin</strong> to <strong class="source-inline">16</strong> so that your preview images will be a <strong class="source-inline">5x5</strong> array (25 images) with a 16-pixel margin.<p>Set <strong class="source-inline">seed_vector</strong> equal to <strong class="source-inline">200</strong>. Set <strong class="source-inline">data_path</strong> to where you stored your image dataset. As you can see, you are using Google Drive here. If you don't know your data path, you can simply locate where your files are, right-click, and select <strong class="source-inline">Copy Path</strong>. Set your epochs to <strong class="source-inline">1000</strong>.</p><p>Finally, print the parameters:</p><p class="source-code">gen_res = 3 </p><p class="source-code">gen_square = 32 * gen_res</p><p class="source-code">img_chan = 3</p><p class="source-code">img_rows = 5</p><p class="source-code">img_cols = 5</p><p class="source-code">img_margin = 16</p><p class="source-code">seed_vector = 200</p><p class="source-code">data_path = '/content/drive/MyDrive/Datasets'\</p><p class="source-code">            '/apple-or-tomato/training_set/'</p><p class="source-code">epochs = 5000</p><p class="source-code">num_batch = 32</p><p class="source-code">num_buffer = 60000</p><p class="source-code">print(f"Will generate a resolution of {gen_res}.")</p><p class="source-code">print(f"Will generate {gen_square}px square images.")</p><p class="source-code">print(f"Will generate {img_chan} image channels.")</p><p class="source-code">print(f"Will generate {img_rows} preview rows.")</p><p class="source-code">print(f"Will generate {img_cols} preview columns.")</p><p class="source-code">print(f"Our preview margin equals {img_margin}.")</p><p class="source-code">print(f"Our data path is: {data_path}.")</p><p class="source-code">print(f"Our number of epochs are: {epochs}.")</p><p class="source-code">print(f"Will generate a batch size of {num_batch}.")</p><p class="source-code">print(f"Will generate a buffer size of {num_buffer}.")</p><p>Your output should look something like this:</p><div id="_idContainer397" class="IMG---Figure"><img src="image/B16341_11_23.jpg" alt="Figure 11.23: Output showing parameters&#13;&#10;"/></div><p class="figure-caption">Figure 11.23: Output showing parameters</p></li>
				<li>Load and preprocess the images. Here, you will save a NumPy preprocessed file. Load the previous training NumPy file. The name of the binary file of the images has the dimensions of the images encoded in it:<p class="source-code">training_binary_path = os.path.join(data_path,\</p><p class="source-code">        f'training_data_{gen_square}_{gen_square}.npy')</p><p class="source-code">print(f"Looking for file: {training_binary_path}")</p><p class="source-code">if not os.path.isfile(training_binary_path):</p><p class="source-code">    start = time.time()</p><p class="source-code">    print("Loading training images...")</p><p class="source-code">    train_data = []</p><p class="source-code">    images_path = os.path.join(data_path,'tomato')</p><p class="source-code">    for filename in tqdm(os.listdir(images_path)):</p><p class="source-code">        path = os.path.join(images_path,filename)</p><p class="source-code">        images = Image.open(path).resize((gen_square,</p><p class="source-code">            gen_square),Image.ANTIALIAS)</p><p class="source-code">        train_data.append(np.asarray(images))</p><p class="source-code">    train_data = np.reshape(train_data,(-1,gen_square,</p><p class="source-code">            gen_square,img_chan))</p><p class="source-code">    train_data = train_data.astype(np.float32)</p><p class="source-code">    train_data = train_data / 127.5 - 1.</p><p class="source-code">    print("Saving training images...")</p><p class="source-code">    np.save(training_binary_path,train_data)</p><p class="source-code">    elapsed = time.time()-start</p><p class="source-code">    print (f'Image preprocessing time: {time_string(elapsed)}')</p><p class="source-code">else:</p><p class="source-code">    print("Loading the training data...")</p><p class="source-code">    train_data = np.load(training_binary_path)</p></li>
				<li>Batch and shuffle the data. Use the <strong class="source-inline">tensorflow.data.Dataset</strong> object library to use its functions to shuffle the dataset and create batches:<p class="source-code">train_dataset = tf.data.Dataset.from_tensor_slices(train_data) \</p><p class="source-code">                  .shuffle(num_buffer).batch(num_batch)</p></li>
				<li>Build the generator:<p class="source-code">def create_generator(seed_size, channels):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Dense(4*4*256,activation="relu", \</p><p class="source-code">                    input_dim=seed_size))</p><p class="source-code">    model.add(Reshape((4,4,256)))</p><p class="source-code">    model.add(UpSampling2D())</p><p class="source-code">    model.add(Conv2D(256,kernel_size=3,padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(Activation("relu"))</p><p class="source-code">    model.add(UpSampling2D())</p><p class="source-code">    model.add(Conv2D(256,kernel_size=3,padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(Activation("relu"))</p><p class="source-code">   </p><p class="source-code">    model.add(UpSampling2D())</p><p class="source-code">    model.add(Conv2D(128,kernel_size=3,padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(Activation("relu"))</p><p class="source-code">    if gen_res&gt;1:</p><p class="source-code">        model.add(UpSampling2D(size=(gen_res,gen_res)))</p><p class="source-code">        model.add(Conv2D(128,kernel_size=3,padding="same"))</p><p class="source-code">        model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">        model.add(Activation("relu"))</p><p class="source-code">    model.add(Conv2D(channels,kernel_size=3,padding="same"))</p><p class="source-code">    model.add(Activation("tanh"))</p><p class="source-code">    return model</p></li>
				<li>Build the discriminator:<p class="source-code">def create_discriminator(image_shape):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(Conv2D(32, kernel_size=3, strides=2, \</p><p class="source-code">                     input_shape=image_shape, </p><p class="source-code">                     padding="same"))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Conv2D(64, kernel_size=3, \</p><p class="source-code">                     strides=2, padding="same"))</p><p class="source-code">    model.add(ZeroPadding2D(padding=((0,1),(0,1))))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Conv2D(128, kernel_size=3, strides=2, \</p><p class="source-code">                     padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Conv2D(256, kernel_size=3, strides=1, \</p><p class="source-code">                     padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Conv2D(512, kernel_size=3, strides=1, \</p><p class="source-code">                     padding="same"))</p><p class="source-code">    model.add(BatchNormalization(momentum=0.8))</p><p class="source-code">    model.add(LeakyReLU(alpha=0.2))</p><p class="source-code">    model.add(Dropout(0.25))</p><p class="source-code">    model.add(Flatten())</p><p class="source-code">    model.add(Dense(1, activation='sigmoid'))</p><p class="source-code">    return model</p></li>
				<li>During the training process, display generated images to get some insight into the progress that's been made. Save the images. At regular intervals of 100 epochs, save a grid of images to evaluate the progress:<p class="source-code">def save_images(cnt,noise):</p><p class="source-code">    img_array = np.full(( </p><p class="source-code">      img_margin + (img_rows * (gen_square+img_margin)), </p><p class="source-code">      img_margin + (img_cols * (gen_square+img_margin)), 3), </p><p class="source-code">      255, dtype=np.uint8)</p><p class="source-code">    gen_imgs = generator.predict(noise)</p><p class="source-code">    gen_imgs = 0.5 * gen_imgs + 0.5</p><p class="source-code">    img_count = 0</p><p class="source-code">    for row in range(img_rows):</p><p class="source-code">    for col in range(img_cols):</p><p class="source-code">        r = row * (gen_square+16) + img_margin</p><p class="source-code">        c = col * (gen_square+16) + img_margin</p><p class="source-code">        img_array[r:r+gen_square,c:c+gen_square] \</p><p class="source-code">            = gen_imgs[img_count] * 255</p><p class="source-code">        img_count += 1</p><p class="source-code">    output_path = os.path.join(data_path,'output')</p><p class="source-code">    if not os.path.exists(output_path):</p><p class="source-code">    os.makedirs(output_path)</p><p class="source-code">    filename = os.path.join(output_path,f"train-{cnt}.png")</p><p class="source-code">    im = Image.fromarray(img_array)</p><p class="source-code">    im.save(filename)</p></li>
				<li>Now, create a generator that generates noise:<p class="source-code">generator = create_generator(seed_vector, img_chan)</p><p class="source-code">noise = tf.random.normal([1, seed_vector])</p><p class="source-code">gen_img = generator(noise, training=False)</p><p class="source-code">plt.imshow(gen_img[0, :, :, 0])</p><p>Your output should look something like this:</p><div id="_idContainer398" class="IMG---Figure"><img src="image/B16341_11_24.jpg" alt="Figure 11.24: Output showing noise&#13;&#10;"/></div><p class="figure-caption">Figure 11.24: Output showing noise</p></li>
				<li>View one of the images generated by typing in the following commands:<p class="source-code">img_shape = (gen_square,gen_square,img_chan)</p><p class="source-code">discriminator = create_discriminator(img_shape)</p><p class="source-code">decision = discriminator(gen_img)</p><p class="source-code">print (decision)</p><p>Your output should look something like this:</p><p class="source-code">tf.Tensor([[0.4994658]], shape=(1,1), dtype=float32)</p></li>
				<li>Create your loss functions. Since the outputs of the discriminator and generator networks are different, you need to define two separate loss functions for them. Moreover, they need to be trained separately in independent passes through the networks. Use <strong class="source-inline">tf.keras.losses.BinaryCrossentropy</strong> for <strong class="source-inline">cross_entropy</strong>. This calculates the loss between true and predicted labels. Then, define the <strong class="source-inline">discrim_loss</strong> function from <strong class="source-inline">real_output</strong> and <strong class="source-inline">fake_output</strong> using <strong class="source-inline">tf.ones</strong> and <strong class="source-inline">tf.zeros</strong> to calculate <strong class="source-inline">total_loss</strong>:<p class="source-code">cross_entropy = tf.keras.losses.BinaryCrossentropy()</p><p class="source-code">def discrim_loss(real_output, fake_output):</p><p class="source-code">    real_loss = cross_entropy(tf.ones_like(real_output), \</p><p class="source-code">                              real_output)</p><p class="source-code">    fake_loss = cross_entropy(tf.zeros_like(fake_output), \</p><p class="source-code">                              fake_output)</p><p class="source-code">    total_loss = real_loss + fake_loss</p><p class="source-code">    return total_loss</p><p class="source-code">def gen_loss(fake_output):</p><p class="source-code">    return cross_entropy(tf.ones_like(fake_output), \</p><p class="source-code">                         fake_output)</p></li>
				<li>Create two Adam optimizers (one for the generator and one for the discriminator), using the same learning rate and momentum for each:<p class="source-code">gen_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)</p><p class="source-code">disc_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)</p></li>
				<li>Create a function to implement an individual training step. With <strong class="source-inline">tf.GradientTape()</strong>, train the discriminator and generator at the same time, but separately from one another. <p>Then, create <strong class="source-inline">real_output</strong> and <strong class="source-inline">fake_output</strong> for the discriminator. Use this for the generator loss (<strong class="source-inline">g_loss</strong>). Then, calculate the discriminator loss (<strong class="source-inline">d_loss</strong>) and calculate the gradients of both the generator and discriminator with <strong class="source-inline">gradients_of_generator</strong> and <strong class="source-inline">gradients_of_discriminator</strong>, and apply them:</p><p class="source-code">@tf.function</p><p class="source-code">def train_step(images):</p><p class="source-code">    seed = tf.random.normal([num_batch, seed_vector])</p><p class="source-code">    with tf.GradientTape() as gen_tape, \</p><p class="source-code">        tf.GradientTape() as disc_tape:</p><p class="source-code">    gen_imgs = generator(seed, training=True)</p><p class="source-code">    real_output = discriminator(images, training=True)</p><p class="source-code">    fake_output = discriminator(gen_imgs, training=True)</p><p class="source-code">    g_loss = gen_loss(fake_output)</p><p class="source-code">    d_loss = discrim_loss(real_output, fake_output)</p><p class="source-code">    </p><p class="source-code">    gradients_of_generator = gen_tape.gradient(\</p><p class="source-code">        g_loss, generator.trainable_variables)</p><p class="source-code">    gradients_of_discriminator = disc_tape.gradient(\</p><p class="source-code">        d_loss, discriminator.trainable_variables)</p><p class="source-code">    gen_optimizer.apply_gradients(zip(</p><p class="source-code">        gradients_of_generator, generator.trainable_variables))</p><p class="source-code">    disc_optimizer.apply_gradients(zip(</p><p class="source-code">        gradients_of_discriminator, </p><p class="source-code">        discriminator.trainable_variables))</p><p class="source-code">    return g_loss,d_loss</p></li>
				<li>Create an array number of fixed seeds with <strong class="source-inline">fixed_seeds</strong> equal to the number of images displayed along one dimension and the seed vector along the other dimension so that you can track the same images. This allows you to see how individual seeds evolve over time. Loop through each batch with <strong class="source-inline">for image_batch in dataset</strong>. Continue to track your loss for both the generator and discriminator with <strong class="source-inline">generator_loss</strong> and <strong class="source-inline">discriminator_loss</strong>. You get a nice display of all this information as it trains:<p class="source-code">def train(dataset, epochs):</p><p class="source-code">    fixed_seed = np.random.normal(0, 1, (img_rows * img_cols, </p><p class="source-code">                                        seed_vector))</p><p class="source-code">    start = time.time()</p><p class="source-code">    for epoch in range(epochs):</p><p class="source-code">    epoch_start = time.time()</p><p class="source-code">    g_loss_list = []</p><p class="source-code">    d_loss_list = []</p><p class="source-code">    for image_batch in dataset:</p><p class="source-code">        t = train_step(image_batch)</p><p class="source-code">        g_loss_list.append(t[0])</p><p class="source-code">        d_loss_list.append(t[1])</p><p class="source-code">    generator_loss = sum(g_loss_list) / len(g_loss_list)</p><p class="source-code">    discriminator_loss = sum(d_loss_list) / len(d_loss_list)</p><p class="source-code">    epoch_elapsed = time.time()-epoch_start</p><p class="source-code">    print (f'Epoch {epoch+1}, gen loss={generator_loss}', \</p><p class="source-code">           f'disc loss={discriminator_loss},'\</p><p class="source-code">           f' {time_string(epoch_elapsed)}')</p><p class="source-code">    save_images(epoch,fixed_seed)</p><p class="source-code">    elapsed = time.time()-start</p><p class="source-code">    print (f'Training time: {time_string(elapsed)}')</p></li>
				<li>Train on your training dataset:<p class="source-code">train(train_dataset, epochs)</p><p>Your output should look something like this:</p><div id="_idContainer399" class="IMG---Figure"><img src="image/B16341_11_25.jpg" alt="Figure 11.25: Training output&#13;&#10;"/></div><p class="figure-caption">Figure 11.25: Training output</p></li>
				<li>Take a closer look at the generated images, <strong class="source-inline">train-0</strong>, <strong class="source-inline">train-100</strong>, <strong class="source-inline">train-250</strong>, <strong class="source-inline">train-500</strong>, and <strong class="source-inline">train-999</strong>. These images were automatically saved during the training process, as specified in the <strong class="source-inline">train</strong> function:<p class="source-code">a = imread('/content/drive/MyDrive/Datasets'\</p><p class="source-code">           '/apple-or-tomato/training_set/output/train-0.png')</p><p class="source-code">plt.imshow(a)</p><p>You will get output like the following:</p><div id="_idContainer400" class="IMG---Figure"><img src="image/B16341_11_26.jpg" alt="Figure 11.26: Output images after first epoch completed&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 11.26: Output images after first epoch completed</p>
			<p>Now, run the following commands:</p>
			<p class="source-code">a = imread('/content/drive/MyDrive/Datasets'\</p>
			<p class="source-code">           '/apple-or-tomato/training_set/output/train-100.png')</p>
			<p class="source-code">plt.imshow(a)</p>
			<p>You will get output like the following:</p>
			<div>
				<div id="_idContainer401" class="IMG---Figure">
					<img src="image/B16341_11_27.jpg" alt="Figure 11.27: Output images after 101st epoch completed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.27: Output images after 101st epoch completed</p>
			<p>Also, run the following commands:</p>
			<p class="source-code">a = imread('/content/drive/MyDrive/Datasets'\</p>
			<p class="source-code">           '/apple-or-tomato/training_set/output/train-500.png')</p>
			<p class="source-code">plt.imshow(a)</p>
			<p>You will get output like the following:</p>
			<div>
				<div id="_idContainer402" class="IMG---Figure">
					<img src="image/B16341_11_28.jpg" alt="Figure 11.28: Output images after 501st epoch completed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.28: Output images after 501st epoch completed</p>
			<p>Now, run the following commands:</p>
			<p class="source-code">a = imread('/content/drive/MyDrive/Datasets'\</p>
			<p class="source-code">           '/apple-or-tomato/training_set/output/train-999.png')</p>
			<p class="source-code">plt.imshow(a)</p>
			<p>You will get output like the following:</p>
			<div>
				<div id="_idContainer403" class="IMG---Figure">
					<img src="image/B16341_11_29.jpg" alt="Figure 11.29: Output images after 1,000th epoch completed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.29: Output images after 1,000th epoch completed</p>
			<p>The output shows that after 1,000 epochs, the images of the synthetic tomatoes generated by the generator look very similar to real tomatoes and the images improve during the training process.</p>
			<p>In this exercise, you created your own images with a DCGAN. As you can see from <em class="italic">Figure 11.29</em>, the results are impressive. While some of the images are easy to determine as fake, others look very real. </p>
			<p>In the next section, you will complete a final activity to put all that you've learned in this chapter to work and generate your own images with a GAN. </p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor246"/>Activity 11.01: Generating Images Using GANs</h2>
			<p>In this activity, you will build a GAN to generate new images. You will then compare the results between a DCGAN and a vanilla GAN by creating one of each and training them on the same dataset for the same 500 epochs. This activity will demonstrate the difference that model architecture can have on the output and show why having an appropriate model is so important. You will use the <strong class="source-inline">banana-or-orange</strong> dataset. You'll only be using the banana training set images to train and generate new images.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find <strong class="source-inline">banana-or-orange</strong> dataset here: <a href="https://packt.link/z6TCy">https://packt.link/z6TCy</a>.</p>
			<p>Perform the following steps to complete the activity:</p>
			<ol>
				<li value="1">Load Google Colab and Google Drive. <p>Import the relevant libraries, including <strong class="source-inline">tensorflow</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">zipfile</strong>, <strong class="source-inline">tqdm</strong>, <strong class="source-inline">zipfile</strong>, <strong class="source-inline">skimage</strong>, <strong class="source-inline">time</strong>, and <strong class="source-inline">os</strong>.</p></li>
				<li>Create a function to format a time string to track your time usage. </li>
				<li>Set the generation resolution to <strong class="source-inline">3</strong>. Also, set <strong class="source-inline">img_rows</strong> and <strong class="source-inline">img_cols</strong> to <strong class="source-inline">5</strong> and <strong class="source-inline">img_margin</strong> to <strong class="source-inline">16</strong> so that your preview images will be a <strong class="source-inline">5x5</strong> array (25 images) with a 16-pixel margin. Set <strong class="source-inline">seed_vector</strong> equal to <strong class="source-inline">200</strong>, <strong class="source-inline">data_path</strong> to where you stored your image dataset, and <strong class="source-inline">epochs</strong> to <strong class="source-inline">500</strong>. Finally, print the parameters.</li>
				<li>If a NumPy preprocessed file exists from prior execution, then load it into memory; otherwise, preprocess the data and save the image binary.</li>
				<li>Batch and shuffle the data. Use the <strong class="source-inline">tensorflow.data.Dataset</strong> object library to use its functions to shuffle the dataset and create batches.</li>
				<li>Build the generator for the DCGAN. </li>
				<li>Build the discriminator for the DCGAN.</li>
				<li>Build the generator for the vanilla GAN.</li>
				<li>Build the discriminator for the vanilla GAN.</li>
				<li>Create a function to generate and save images that can be used to view progress during the model's training.</li>
				<li>Next, initialize the generator for the DCGAN and view the output. </li>
				<li>Initialize the generator for the vanilla GAN and view the output.</li>
				<li>Print the decision of the DCGAN discriminator evaluated on the seed image.</li>
				<li>Print the decision of the vanilla GAN discriminator evaluated on the seed image.</li>
				<li>Create your loss functions. Since the output of both the discriminator and generator networks is different, you can define two separate loss functions for them. Moreover, they need to be trained separately in independent passes through the networks. Both GANs can utilize the same loss functions for their discriminators and generators. You can use <strong class="source-inline">tf.keras.losses.BinaryCrossentropy</strong> for <strong class="source-inline">cross_entropy</strong>. This calculates the loss between true and predicted labels. Then, define the <strong class="source-inline">discrim_loss</strong> function from <strong class="source-inline">real_output</strong> and <strong class="source-inline">fake_output</strong> using <strong class="source-inline">tf.ones</strong> and <strong class="source-inline">tf.zeros</strong> to calculate <strong class="source-inline">total_loss</strong>.</li>
				<li>Create two Adam optimizers, one for the generator and one for the discriminator. Use the same learning rate and momentum for each.</li>
				<li>Create <strong class="source-inline">real_output</strong> and <strong class="source-inline">fake_output</strong> for the discriminator. Use this for the generator loss (<strong class="source-inline">g_loss</strong>). Then, calculate the discriminator loss (<strong class="source-inline">d_loss</strong>) and the gradients of both the generator and discriminator with <strong class="source-inline">gradients_of_generator</strong> and <strong class="source-inline">gradients_of_discriminator</strong> and apply them. Encapsulate these steps within a function, passing in the generator, discriminator, and images and returning the generator loss (<strong class="source-inline">g_loss</strong>) and discriminator loss (<strong class="source-inline">d_loss</strong>).</li>
				<li>Next, create a number of fixed seeds with <strong class="source-inline">fixed_seeds</strong> equal to the number of images to display so that you can track the same images. This allows you to see how individual seeds evolve over time, tracking your time with <strong class="source-inline">for epoch in range</strong>. Now, loop through each batch with <strong class="source-inline">for image_batch in dataset</strong>. Continue to track your loss for both the generator and discriminator with <strong class="source-inline">generator_loss</strong> and <strong class="source-inline">discriminator_loss</strong>. Now, you have a nice display of all this information as it trains.</li>
				<li>Train the DCGAN model on your training dataset.</li>
				<li>Train the vanilla model on your training dataset.</li>
				<li>View your images generated by the DCGAN model after the 100th epoch.</li>
				<li>View your images generated by the DCGAN model after the 500th epoch.</li>
				<li>View your images generated by the vanilla GAN model after the 100th epoch.</li>
				<li>View your images generated by the vanilla GAN model after the 500th epoch.<p class="callout-heading">Note</p><p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor285">this link</a>.</p></li>
			</ol>
			<h1 id="_idParaDest-223"><a id="_idTextAnchor247"/>Summary</h1>
			<p>In this chapter, you learned about a very exciting class of machine learning models called generative models. You discovered the amazing potential of this new and continually developing field in machine learning by using a generative LSTM on a language modeling challenge to generate textual output. </p>
			<p>Then, you learned about generative adversarial models. You implemented a GAN to generate data for a normal distribution of points. You also went even further into deep convolutional neural networks (DCGANS), discovering how to use one of the most powerful applications of GANs while creating new images of tomatoes and bananas that exhibited human-recognizable characteristics of the fruits on which they were trained.</p>
			<p>We hope you enjoyed the final chapter of <em class="italic">The TensorFlow Workshop</em> and the book as a whole. </p>
			<p>Let's take a look back at the amazing journey that you have completed. First, you started by learning the basics of TensorFlow and how to perform operations on the building blocks of ANNs—tensors. Then, you learned how to load and preprocess a variety of data types in TensorFlow, including tabular data, images, audio files, and text. </p>
			<p>Next, you learned about a variety of resources that can be used in conjunction with TensorFlow to aid in your development, including TensorBoard for visualizing important components of your model, TensorFlow Hub for accessing pre-trained models, and Google Colab for building and training models in a managed environment. Then, you dived into building sequential models to solve regression and classification. </p>
			<p>To improve model performance, you then learned about regularization and hyperparameter tuning, which are used to ensure that your models perform well not only on the data they are trained upon, but also on new, unseen data. From there, you explored convolutional neural networks, which are an excellent choice when working with image data. After that, you learned in-depth how to utilize pre-trained networks to solve your own problems and fine-tune them to your own data. Then, you learned how to build and train RNNs, which are best used when working with sequential data, such as stock prices or even natural language. In the later part of the book, you explored more advanced TensorFlow capabilities using the Functional API and how to develop anything you might need in TensorFlow, before finally learning how to use TensorFlow for more creative endeavors via generative models.</p>
			<p>With this book, you have not only taken your first steps in TensorFlow, but also now learned how to create models and provide solutions to complex problems. It's been an exciting journey from beginning to end, and we wish you luck in your continuing progress. </p>
		</div>
		<div>
			<div id="_idContainer405" class="Content">
			</div>
		</div>
	</body></html>