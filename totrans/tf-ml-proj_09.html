<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generating Matching Shoe Bags from Shoe Images Using DiscoGANs</h1>
                </header>
            
            <article>
                
<p>Human beings are quite smart when it comes to understanding the relationship between different domains. For example, we can easily understand the relationship between a S<span>panish sentence and its translated version in English</span>. We can even guess which color tie to wear to match a certain kind of suit. While it seems easy for humans, this is not a straightforward process for machines.</p>
<p>The task of style transfer across different domains for machines can be framed as a conditional image generation problem. Given an image from one domain, can we learn to map to an image from a different domain.</p>
<p>While there have been many approaches to achieve this using pairwise labeled data from two different domains, these approaches are fraught with problems. The major issue with these approaches is obtaining the pairwise labeled data, which is both an expensive and time-consuming process.</p>
<p class="mce-root">In this chapter, we will learn about an approach for learning style transfer without explicitly providing pairwise labeled data to the algorithm. This approach, known as DiscoGANs, is highlighted in the recently released paper by Kim et. al  named <em>Learning to Discover Cross-Domain Relations with Generative Adversarial Networks</em> (<a href="https://arxiv.org/pdf/1703.05192.pdf">https://arxiv.org/pdf/1703.05192.pdf</a>). Specifically, we will try to generate matching shoes from shoe bag images.</p>
<p>The remainder of this chapter is organized as follows:</p>
<ul>
<li>Introduction to <strong>Generative Adversarial Networks</strong> (<strong>GANs</strong>)</li>
<li>What are DiscoGANs?</li>
<li>How to generate matching shoes from shoe bag images and vice versa</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding generative models</h1>
                </header>
            
            <article>
                
<p>An unsupervised learning model that learns the underlying data distribution of the training set and generates new data that may or may not have variations is commonly known as a <strong>generative model</strong>. Knowing the true underlying distribution might not always be a possibility, hence the neural network trains on a function that tries to be as close a match as possible to the true distribution.</p>
<p>The most common methods used to train generative models are as follows:</p>
<ul>
<li><strong>Variational autoencoders: </strong>A high dimensional input image is encoded by an auto-encoder to create a lower dimensional representation. During this process, it is of the utmost importance to preserve the underlying data distribution. This encoder can only be used to map to the input image using a decoder and cannot introduce any variability to generate similar images. The VAE introduces variability by generating constrained latent vectors that still follow the underlying distribution. Though VAEs help in creating probabilistic graphical models, the generated images tend to be slightly blurry.</li>
<li><strong>PixelRNN/PixelCNN: </strong>These auto-regressive models are used to train networks that model the conditional distribution of a successive individual pixel, given previous pixels starting from the top left. RNNs move horizontally and vertically over any image. The training for PixelRNNs is a stable and simple process with better log likelihoods than other models, but they are time consuming and relatively inefficient.</li>
<li><strong>Generative adversarial networks: </strong>Generative adversarial networks were first published in the 2014 paper by <em>Goodfellow et al.</em> (<a href="https://arxiv.org/abs/1406.2661">https://arxiv.org/abs/1406.2661</a><span>)</span><span>. These can be thought of as a competition framework with two adversaries called the generator and the discriminator. These are nothing but two differentiable functions in the form of neural networks. The generator takes a randomly generated input known as a latent sample and produces an image. The overall objective of the generator is to generate an image that is as close as possible to the real input image (such as MNIST digits) and give it as an input to the discriminator.</span></li>
</ul>
<p style="padding-left: 60px"><span>The discriminator is essentially a classifier that's trained to distinguish between real images (original MNIST digits) and fake images (output of the generator). Ideally, after being trained, the generator should adapt its parameters and capture the underlying training data distribution and fool the discriminator about its input being a real image.</span></p>
<p style="padding-left: 60px"><span>Let's consider an analogy that is inspired by the real world. Imagine that GANs work like the relationship between a forger making counterfeit currency and the police identifying and discarding that forged currency. The aim of the forger is to try and pass off the fake currency as real currency in the market. This is analogous to what a generator tries to do. The police try and inspect every currency note it can, accepting the original notes and scrapping the fake ones. The police know of the details of the original currency and compare it to the properties of the currency in question in order to make a decision regarding its authenticity. If there is a match, the currency is retained; otherwise, it is scrapped. This is in line with the work of a discriminator.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training GANs</h1>
                </header>
            
            <article>
                
<p>The following diagram illustrates the basic architecture of GANs:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-159 image-border" src="assets/6296918a-8ac6-4574-b1ff-2368e6946b4c.png" style="width:144.75em;height:52.00em;"/></p>
<p>A random input is used to generate a sample of data. For example, a generator, <em>G(z)</em>, uses a prior distribution, <em>p(z)</em>, to achieve an input, <em>z</em>.<span> Using <em>z</em>, it then generates</span> some <span>data. This output is fed as input to the discriminator neural network, <em>D(x)</em>. It takes an input x from </span><img class="fm-editor-equation" src="assets/1ff29fae-3f62-4e27-8ebe-6c1a91597318.png" style="font-size: 1em;width:5.25em;height:1.83em;"/>,<span> where </span><img class="fm-editor-equation" src="assets/49289ba1-2cdd-4375-9077-36f8649f8e3a.png" style="font-size: 1em;width:5.25em;height:1.83em;"/> <span>is our real data distribution. <em>D(x)</em> then solves a binary classification problem using the <kbd>sigmoid</kbd> function, which gives us an output in the range of 0 to 1</span><span>.</span></p>
<p>GANS are trained to be part of a competition between the generator and discriminator. The objective function can be represented mathematically as follows:</p>
<p class="CDPAlignLeft CDPAlign" style="padding-left: 90px"><img class="alignnone size-full wp-image-269 image-border" src="assets/61972a78-46bc-4575-aa1b-c94c1fe6e907.png" style="width:41.08em;height:4.58em;"/></p>
<p><span><span>In this, the following applies</span></span>:</p>
<ul>
<li><img class="fm-editor-equation" src="assets/f619e329-5665-425b-a42f-227ca87bc47e.png" style="width:1.92em;height:1.58em;"/> denotes the parameters of the discriminator</li>
<li><img class="fm-editor-equation" src="assets/ea4c4428-39c2-4afb-9c6c-2a4a6ac531ee.png" style="width:1.42em;height:1.75em;"/> denotes the parameters of the generator</li>
<li><img class="fm-editor-equation" src="assets/173c7e8d-6049-4dcb-8467-b7fa266f45ff.png" style="width:3.08em;height:1.25em;"/>denotes the underlying distribution of training data</li>
<li><img class="fm-editor-equation" src="assets/dc44753e-ecd8-423b-9633-8be92c39bbb5.png" style="width:4.83em;height:1.83em;"/> denotes the discriminator operation over input images <em>x</em><a href="https://www.codecogs.com/eqnedit.php?latex=x%250"/></li>
<li><img class="fm-editor-equation" src="assets/22d6d1fc-22dd-4ca2-9fcb-b1683de79bbc.png" style="width:4.58em;height:2.00em;"/> denotes the generator operation over latent sample <em>z</em></li>
<li><img class="fm-editor-equation" src="assets/7fdda8a8-8bdf-4bf7-9683-c546acea033a.png" style="color: #333333;font-size: 1em;width:8.67em;height:2.17em;"/> denotes the discriminator output for generated fake data <img class="fm-editor-equation" src="assets/0b5145dd-c1da-404d-9ca8-c20abecc4498.png" style="width:3.17em;height:1.83em;"/></li>
</ul>
<p>In the objective function, the first term from the left represents the cross entropy of the discriminator's output from a real distribution (<img class="fm-editor-equation" src="assets/71f08933-8f06-4736-a01b-d04cfe1736ef.png" style="width:3.08em;height:1.25em;"/>). The second term from the left is the cross entropy between the random distribution (<img class="fm-editor-equation" src="assets/bebcff21-5a62-4571-94ec-1e3dfcde3341.png" style="width:1.50em;height:1.25em;"/>) and one minus the prediction of the discriminator on the output of the generator that was generated using random sample z from <img class="fm-editor-equation" src="assets/817e906e-f25e-40d8-b1e0-d3010aa96031.png" style="width:1.50em;height:1.25em;"/>. The discriminator tries to maximize both terms to classify the images as real and fake, respectively. On the other hand, the generator tries to fool the discriminator by minimizing this objective. </p>
<p>In order to train GANs, gradient-based optimization algorithms, such as stochastic gradient desc<span>ent, are used. Algorithmically, it flows as follows:</span></p>
<ol>
<li>First, sample <em>m</em> noise samples and <em>m</em> real data samples.</li>
<li>Freeze the generator, that is, set the training as false so that the generator network only does a forward pass without any back propagation. Train the discriminator on this data.</li>
<li>Sample different <em>m</em> noise samples.</li>
<li>Freeze the discriminator and train the generator on this data.</li>
<li>Iterate through the preceding steps.</li>
</ol>
<p>Formally, the pseudocode is as follows.</p>
<p class="CDPAlignLeft CDPAlign">In this example, we are performing mini-batch stochastic gradient descent training of generative adversarial nets. The number of steps to apply to the discriminator, <em>k</em>, is a hyper parameter. We used <em>k=1</em>, the least expensive option, in our experiment:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-207 image-border" src="assets/193ed986-2232-4d70-8c9b-0849d7207c54.png" style="width:56.33em;height:30.75em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Pseudocode for GAN training. With k=1, this equates to training D, then G, one after the other. Adapted from Goodfellow et al. 2014</div>
<p>The gradient-based updates can use any standard gradient-based learning rule. We used momentum in our experiment. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications</h1>
                </header>
            
            <article>
                
<p>Some of the applications of GANs include converting monochrome or black and white images into colored images, filling additional details in an image, such as the insertion of objects into a partial image or into an image with only edges, and constructing images representing what somebody would look like when they are older given an image of their present self.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Challenges</h1>
                </header>
            
            <article>
                
<p>Though GANs generate one of the sharpest images from a given piece of input data, their optimization is difficult to achieve due to unstable training dynamics. They also suffer from other challenges, such as mode collapse and bad initialization. Mode collapse is a phenomena where, if the data is multimodal, the generator is never incentivized to cover both modes, which leads to lower variability among generated samples and, hence, lower utility of GANs. If all generated samples start to become identical, it leads to complete collapse. In cases where most of the samples show some commonality, there is partial collapse of the model. At the core of this, GANs work on an objective function that aims to achieve optimization of min-max, but if the initial parameters end up being inefficient, then it becomes an oscillating process with no true optimization. In addition to this, there are issues such as GANs failing to differentiate the count of particular objects that should occur at a location. <span><span>For example,</span></span> GANs have no idea that there can't be more than two eyes and can generate images of human faces with 3 eyes.  There are also issues with GANs being unable to adapt to a 3D perspective, such as front and posterior view. This gives a flat 2D image instead of depth for a 3D object.</p>
<p>Different variants of GANs have evolved over time. Some of them are as follows: </p>
<ul>
<li><strong>Deep convolutional GANs</strong> (<strong>DCGANs</strong>) were one of the first major improvements on the GAN architecture. It is made up of convolutional layers that avoid the use of max pooling or fully connected layers. The convolutional stride and transposed convolution for downsampling and upsampling is majorly used by this. It also uses ReLU activation in the generator and LeakyReLU in the discriminator.</li>
<li><strong>InfoGANs</strong> are another variant of GANs that try and encode meaningful features of the image (for example, rotation) in parts of the noise vector, z. </li>
<li><strong>Conditional GANs</strong> (<strong>cGANs</strong>) use extra conditional information that describes some aspect of the data as input to the generator and discriminator. For example, if we are dealing with vehicles, the condition could describe attributes such as four-wheeled or two-wheeled. This helps generate better samples and additional features. In this chapter, we will mainly focus on DiscoGANs, which are described in the following section.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding DiscoGANs</h1>
                </header>
            
            <article>
                
<p>In this section, we are mainly going to take a closer look at Discovery GANS, which are popularly known as <strong>DiscoGANs</strong>.</p>
<p>Before going further, let's try to understand reconstruction loss in machine learning, since this is<span> one of the concepts that this chapter is majorly dependent on.</span> When learning about the representation of an unstructured data type such as an image/text, we want our model to encode the data in such a manner that when it's decoded, the underlying image/text can be generated back. To incorporate this condition in the model explicitly, we use a reconstruction loss (essentially the Euclidean distance between the reconstructed and original image) in training the model.</p>
<p>Style transfer has been one of the most prominent use cases of GANs. Style transfer basically refers to the problem where, if you are given an image/data in one domain, is it possible to successfully generate an image/data in another domain. This problem has become quite famous among several researchers.</p>
<p>You can read more about style transfer problems from the paper  Neural Style Transfer: A Review  (<a href="https://arxiv.org/abs/1705.04058">https://arxiv.org/abs/1705.04058</a>) by Jing et. al. However, most of the work is done by using an explicitly paired dataset that's generated by humans or other algorithms. This puts limitations on these approaches, since paired data is seldom available and is too costly to generate.</p>
<p>DiscoGANs, on the other hand, propose a method of learning cross-domain relations without the explicit need for paired datasets. This method takes an image from one domain and generates the corresponding image from the other domain. Let's say we are trying to transfer an image from Domain A to Domain B. During the learning process, we force the generated image to be the image-based representation of the image from Domain A through a reconstruction loss and to be as close to the image in Domain B as possible through a GAN loss, as mentioned earlier. Essentially, this approach tends to generate a bijective (one-to-one) mapping between two domains, rather than a many-to-one or one-to-many mapping.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fundamental units of a DiscoGAN</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, normal GANs have a generator and a discriminator. Let's try to understand the building blocks of DiscoGANs and then proceed to understand how to combine them so that we can learn about cross-domain relationships. These are as follows:</p>
<ul>
<li><span><span><strong>Generator:</strong> </span></span>In the original GANs, the generator would take an input vector<span> </span><em>z</em><span> </span><span>randomly sampled from, say, Gaussian distribution, and generate fake images. In this case, however, since we are looking to transfer images from one domain to another, we replace the input vector</span><span> </span><em>z</em><span> </span><span>with an image. Here are the parameters of the generator function:</span></li>
</ul>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Parameters</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
</tr>
<tr>
<td>
<p>Input image size</p>
</td>
<td>
<p>64x64x3</p>
</td>
</tr>
<tr>
<td>
<p>Output image size</p>
</td>
<td>
<p>64x64x3</p>
</td>
</tr>
<tr>
<td>
<p># Convolutional layers</p>
</td>
<td>
<p>4</p>
</td>
</tr>
<tr>
<td>
<p># Conv transpose/Deconv layers</p>
</td>
<td>
<p>4</p>
</td>
</tr>
<tr>
<td>
<p>Normalizer function</p>
</td>
<td>
<p>Batch Normalization</p>
</td>
</tr>
<tr>
<td>
<p>Activation function</p>
</td>
<td>
<p>LeakyReLU</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p style="padding-left: 60px">Before specifying the structure of each particular layer, let's try to understand a few of the terms that were mentioned in the parameters. </p>
<ul>
<li><strong>Transposed convolution: </strong><span><span>As we mentioned previously, generators are used to generate images from the input vector. In our case, the input image is first convolved by 4 convolutional layers, which produce an embedding. Generating an image from the embedding involves upsampling from a low resolution to a higher resolution.</span></span></li>
</ul>
<p style="padding-left: 60px">General ways of upsampling include manual feature engineering to interpolate lower dimensional images. A better approach could be to employ Transposed Convolution, also known as the <strong>fractional stride convolution</strong>/<strong>deconvolution</strong>. It doesn't employ any predefined interpolation method. Let's say that we have a 4x4 matrix that is convolved with a 3x3 filter (stride 1 and no padding); this will result in a matrix of size 2x2. As you can see, we downsampled the original image from 4x4 to 2x2. The process of going from 2x2 back to 4x4 can be achieved through transposed convolution.</p>
<p style="padding-left: 60px">From an implementation perspective, the built-in function in TensorFlow for defining convolutional layers can be used directly with the<span> </span><kbd>num_outputs</kbd><span> </span>value, which can be changed to perform upsampling.</p>
<ul>
<li><strong>Batch normalization</strong><span>: This</span><span> method is used to counter the internal covariance shift that happens in deep neural networks.</span></li>
</ul>
<p style="padding-left: 60px">A covariance shift can be defined as when the model is able to predict when the distribution of inputs changes. Let's say we train a model to detect black and white images of dogs. During the inference phase, if we supply colored images of dogs to the model, it will not perform well. This is because the model learned the parameters based on black and white images, which are not suitable for predicting colored images.</p>
<p style="padding-left: 60px">Deep neural networks experience what is known as an internal covariance shift, since changes in the parameters of an internal layer change the distribution of input in the next layer. To fix this issue, we normalize the output of each batch by using its mean and variance, and pass on a weighted combination of mean and variance to the next layer. Due to the weighted combination, batch normalization adds two extra parameters in each layer of the neural network.</p>
<p style="padding-left: 60px">Batch normalization helps speed up training and tends to reduce overfitting because of its regularization effects.</p>
<p style="padding-left: 60px">In this model, we use batch normalization in all of the convolutional and convolutional transpose layers, except the first and the last layers.</p>
<ul>
<li><strong>Leaky ReLU: </strong><strong>ReLU</strong><span>, </span><span>or</span><span> </span><strong>rectified linear units</strong><span>, </span><span>are quite popular in the deep learning domain today as activation functions. ReLU units in deep neural networks can be fragile at times since they can cause neurons to die or never get activated again</span><span> </span>at any<span> </span><span>data point. The following diagram illustrates the ReLU function:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-204 image-border" src="assets/96327a14-715d-473b-a725-fbc12a33026e.png" style="width:35.58em;height:26.75em;"/></p>
<p style="padding-left: 60px"><span>Leaky ReLU is used to try and fix this problem. They have small negative values instead of zeros for negative input values. This avoids the dying issue with regard to neurons. The following diagram illustrates a sample Leaky ReLU function:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-205 image-border" src="assets/89a49618-25a6-4d85-9714-8b94641120cd.png" style="width:35.58em;height:26.83em;"/></p>
<ul>
<li><strong>Discriminator</strong>: In the GANs that we described previously, the generator takes an input vector that's been randomly sampled from, say, a Gaussian distribution, and generates fake images. In this case, however, since we are looking to transfer images from one domain to another, we replace the input vector with an image. </li>
</ul>
<p style="padding-left: 60px">The parameters of the discriminator from an architectural standpoint are as follows:</p>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Layers:</strong> The discriminator is made up of 5 convolutional layers, each stacked on top of the other, and then followed by two fully connected layers.</li>
<li><strong>Activation:</strong> Leaky ReLU activation is used for all layers except the last fully connected layer. The last layer uses <kbd>sigmoid</kbd> to predict the probability of a sample.</li>
<li><strong>Normalizer:</strong><span> This performs b</span>atch normalization, except on the first and last layers of the network.</li>
<li class="mce-root"><strong>Stride:</strong><span> A stride length of 2 is used for all of the convolutional layers.</span></li>
</ul>
</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DiscoGAN modeling</h1>
                </header>
            
            <article>
                
<p style="color: black">For each mapping, that is, <strong>handbags</strong> (denoted by <strong>b</strong>) to <strong>shoes</strong> (denoted by <strong>s</strong>), or vice versa, we add two generators. Let's say, for the mapping <strong>b</strong> to <strong>s,</strong> the first generator maps the input image from domain <strong>b</strong> to <strong>s</strong>, while the second generator reconstructs the image from domain <strong>s</strong> to domain <strong>b</strong>. Intuitively, we need a second generator to achieve the objective (one-to-one) mapping we talked about in the previous sections. Mathematically, this can be represented as follows:</p>
<p class="CDPAlignCenter CDPAlign" style="color: black"><img class="fm-editor-equation" src="assets/55e10729-cbc7-4f69-b8b5-b1ef79cef683.png" style="width:8.42em;height:1.83em;"/></p>
<p class="CDPAlignCenter CDPAlign" style="color: black"><img class="fm-editor-equation" src="assets/78432287-0789-4a32-9c18-9ef46abe80b7.png" style="width:8.08em;height:1.75em;"/></p>
<p style="color: black">While modeling, since this is a very hard constraint to satisfy, we add a reconstruction loss. Reconstruction loss is given as follows:</p>
<p class="CDPAlignCenter CDPAlign" style="color: black"><img class="fm-editor-equation" src="assets/7967859e-4513-4b97-86c1-4a74e0255488.png" style="width:16.83em;height:1.83em;"/></p>
<p style="color: black">Now, the usual GAN loss that is required to generate fake images of another domain is given by the following equation:</p>
<p class="CDPAlignCenter CDPAlign" style="color: black"><img class="fm-editor-equation" src="assets/f5c80320-09bc-4d6a-8d6b-b94c42ec5f99.png" style="width:20.83em;height:1.83em;"/> </p>
<p style="color: black">For each mapping, the generator receives two losses:</p>
<ul>
<li>The reconstruction loss, which tries to see how well we can map the generated image to its original domain</li>
<li>The usual GAN loss, which is for the task of fooling the discriminator</li>
</ul>
<p style="color: black">In this case, the discriminator is the usual discriminator, with the loss that we mentioned in the section of <em>Training GANs</em>. Let's denote it by using <img class="fm-editor-equation" src="assets/ab0bc834-4b50-4fd0-a928-30a465dfbe1c.png" style="width:2.58em;height:1.67em;"/>.</p>
<p style="color: black">The total generator and discriminator loss is given by the following equation:</p>
<p class="CDPAlignCenter CDPAlign" style="color: black"><img class="fm-editor-equation" src="assets/3a5e2a5b-5eba-4322-b245-4ed3e71702fb.png" style="width:28.08em;height:1.67em;"/></p>
<p class="CDPAlignCenter CDPAlign" style="color: black"><img class="fm-editor-equation" src="assets/1571492e-824a-4eed-846e-caa3fe9f0a69.png" style="width:9.58em;height:1.42em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a DiscoGAN model</h1>
                </header>
            
            <article>
                
<p>The base datasets in this problem are obtained from the <kbd>edges2handbags</kbd> (<a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz">https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2handbags.tar.gz</a>) and <kbd>edges2shoes</kbd> (<a href="https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz">https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/edges2shoes.tar.gz</a>) datasets. Each image that's present in these datasets contain two sub-images. One is the colored image of the object, while the other is the image of the edges of the corresponding color image.  </p>
<p>Follow the steps to build a DiscoGAN model:</p>
<ol>
<li>First, resize and crop the images in this dataset to obtain the handbag and shoe images:</li>
</ol>
<pre style="padding-left: 60px">def extract_files(<em>data_dir</em>,<em>type</em> = 'bags'):<br/>   '''<br/> :param data_dir: Input directory<br/> :param type: bags or shoes<br/>   :return: saves the cropped files to the bags to shoes directory<br/>   '''<br/><br/>   input_file_dir = os.path.join(os.getcwd(),<em>data_dir</em><span>, "train")<br/></span>   result_dir = os.path.join(os.getcwd(),<em>type</em>)<br/>   <em>if not</em> os.path.exists(result_dir):<br/>       os.makedirs(result_dir)<br/><br/>   file_names= os.listdir(input_file_dir)<br/>   <em>for</em> file <em>in</em> file_names:<br/>       input_image = Image.open(os.path.join(input_file_dir,file))<br/>       input_image = input_image.resize([128, 64])<br/>       input_image = input_image.crop([64, 0, 128, 64])  # Cropping only the colored image. Excluding the edge image<br/>       input_image.save(os.path.join(result_dir,file))</pre>
<ol start="2">
<li>Save the images in the corresponding folders of <kbd>bags</kbd> and <kbd>shoes</kbd>. Some of the sample images are shown as follows:</li>
</ol>
<table style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td><img class="alignnone size-full wp-image-217 image-border" src="assets/eaab41c1-360b-48f1-b667-fd83e0169c31.png" style="width:6.50em;height:6.50em;"/></td>
<td><img class="alignnone size-full wp-image-218 image-border" src="assets/50d09e05-b3a9-4323-a25d-c6b20b021356.png" style="width:7.75em;height:7.42em;"/></td>
<td><img class="alignnone size-full wp-image-219 image-border" src="assets/db30ccae-c0fe-43df-bd2c-404654975f3b.png" style="width:6.67em;height:7.92em;"/></td>
<td><img class="alignnone size-full wp-image-1017 image-border" src="assets/7f0fbff0-68c3-4714-a612-fa2bf1beefb6.png" style="width:6.58em;height:6.92em;"/></td>
</tr>
<tr>
<td class="CDPAlignRight CDPAlign"><span>Shoes</span></td>
<td/>
<td class="CDPAlignRight CDPAlign"><span>Bags</span></td>
<td/>
</tr>
</tbody>
</table>
<ol start="3">
<li>Implement the <kbd>generator</kbd> function with  4 convolutional layers followed by 4 convolutional transpose (or deconv) layers.<span>The kernel size used in this scenario is 4, while the </span><kbd>stride</kbd><span> is </span><kbd>2</kbd><span> and </span><kbd>1</kbd><span> for the convolutional and deconv layers, respectively.</span> Leaky Relu is used as activation function in all the layers. Code for the function is as follows:</li>
</ol>
<pre style="padding-left: 60px">def generator(x, initializer, s<em>cope_name</em> = 'generator',<em>reuse</em>=<em>False</em>):<br/>   <em>with</em> tf.variable_scope(<em>scope_name</em>) <em>as</em> scope:<br/>       <em>if</em> <em>reuse</em>:<br/>           scope.reuse_variables()<br/>       conv1 = tf.contrib.layers.conv2d(inputs=<em>x</em>, num_outputs=32, kernel_size=4, stride=2, padding="SAME",reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, weights_initializer=<em>initializer</em>,<br/>                                        scope="disc_conv1")  # 32 x 32 x 32<br/>       conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding="SAME",<br/>                                        reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                        weights_initializer=<em>initializer</em>, scope="disc_conv2")  # 16 x 16 x 64<br/>       conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding="SAME",<br/>                                        reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                        weights_initializer=<em>initializer</em>, scope="disc_conv3")  # 8 x 8 x 128<br/>       conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding="SAME",<br/>                                        reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                        weights_initializer=<em>initializer</em>, scope="disc_conv4")  # 4 x 4 x 256<br/><br/>       deconv1 = tf.contrib.layers.conv2d(conv4, num_outputs=4 * 128, kernel_size=4, stride=1, padding="SAME",<br/>                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                              weights_initializer=<em>initializer</em>, scope="gen_conv1")<br/>       deconv1 = tf.reshape(deconv1, shape=[tf.shape(<em>x</em>)[0], 8, 8, 128])<br/><br/>       deconv2 = tf.contrib.layers.conv2d(deconv1, num_outputs=4 * 64, kernel_size=4, stride=1, padding="SAME",<br/>                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                              weights_initializer=<em>initializer</em>, scope="gen_conv2")<br/>       deconv2 = tf.reshape(deconv2, shape=[tf.shape(<em>x</em>)[0], 16, 16, 64])<br/><br/>       deconv3 = tf.contrib.layers.conv2d(deconv2, num_outputs=4 * 32, kernel_size=4, stride=1, padding="SAME",<br/>                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                              weights_initializer=<em>initializer</em>, scope="gen_conv3")<br/>       deconv3 = tf.reshape(deconv3, shape=[tf.shape(<em>x</em>)[0], 32, 32, 32])<br/><br/>       deconv4 = tf.contrib.layers.conv2d(deconv3, num_outputs=4 * 16, kernel_size=4, stride=1, padding="SAME",<br/>                                              activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                              weights_initializer=<em>initializer</em>, scope="gen_conv4")<br/>       deconv4 = tf.reshape(deconv4, shape=[tf.shape(<em>x</em>)[0], 64, 64, 16])<br/><br/>       recon = tf.contrib.layers.conv2d(deconv4, num_outputs=3, kernel_size=4, stride=1, padding="SAME", \<br/>                                            activation_fn=tf.nn.relu, scope="gen_conv5")<br/><br/>       return<span> recon</span><span><br/></span></pre>
<ol start="4">
<li>Define the discriminator using the parameters that we mentioned previously in section <em>Fundamental Units of a DiscoGAN</em>:</li>
</ol>
<pre style="padding-left: 60px">def discriminator(<em>x</em>,<em>initializer</em>, <em>scope_name</em> ='discriminator',  <em>reuse</em>=<em>False</em>):<br/>   <em>with</em> tf.variable_scope(<em>scope_name</em>) <em>as</em> scope:<br/>       <em>if</em> <em>reuse</em>:<br/>           scope.reuse_variables()<br/>       conv1 = tf.contrib.layers.conv2d(inputs=<em>x</em>, num_outputs=32, kernel_size=4, stride=2, padding="SAME",<br/>                                        reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, weights_initializer=<em>initializer</em>,<br/>                                        scope="disc_conv1")  # 32 x 32 x 32<br/>       conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding="SAME",<br/>                                        reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                        weights_initializer=<em>initializer</em>, scope="disc_conv2")  # 16 x 16 x 64<br/>       conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding="SAME",<br/>                                        reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                        weights_initializer=<em>initializer</em>, scope="disc_conv3")  # 8 x 8 x 128<br/>       conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding="SAME",<br/>                                        reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                        weights_initializer=<em>initializer</em>, scope="disc_conv4")  # 4 x 4 x 256<br/>       conv5 = tf.contrib.layers.conv2d(inputs=conv4, num_outputs=512, kernel_size=4, stride=2, padding="SAME",<br/>                                        reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu, normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                        weights_initializer=<em>initializer</em>, scope="disc_conv5")  # 2 x 2 x 512<br/>       fc1 = tf.reshape(conv5, shape=[tf.shape(<em>x</em>)[0], 2 * 2 * 512])<br/>       fc1 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=512, reuse=<em>reuse</em>, activation_fn=tf.nn.leaky_relu,<br/>                                               normalizer_fn=tf.contrib.layers.batch_norm,<br/>                                               weights_initializer=<em>initializer</em>, scope="disc_fc1")<br/>       fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1, reuse=<em>reuse</em>, activation_fn=tf.nn.sigmoid,<br/>                                               weights_initializer=<em>initializer</em>, scope="disc_fc2")<br/>       return fc2</pre>
<ol start="5">
<li>Use the following <kbd>define_network</kbd> function, which defines the two generators and two discriminator for each domain. In the function, t<span>he definition of  </span><kbd>generator</kbd><span> and </span><kbd>discriminator</kbd><span> remains the same as what we defined by using functions in the previous step. However, for DiscoGANs, the function defines one </span><kbd>generator</kbd><span> that generates fake images in another domain, and one </span><kbd>generator</kbd><span> that does the reconstruction. Also, the </span><kbd>discriminators</kbd><span> are defined for both real and fake images in each domain. Code the function is as follows:</span></li>
</ol>
<pre style="padding-left: 60px">def define_network(self):<br/>   # generators<br/>   # This one is used to generate fake data<br/>   self.gen_b_fake = generator(self.X_shoes, self.initializer,scope_name="generator_sb")<br/>   self.gen_s_fake =   generator(self.X_bags, self.initializer,scope_name="generator_bs")<br/>   # Reconstruction generators<br/>   # Note that parameters are being used from previous layers<br/>   self.gen_recon_s = generator(self.gen_b_fake, self.initializer,scope_name="generator_sb",  reuse=<em>True</em>)<br/>   self.gen_recon_b = generator(self.gen_s_fake,  self.initializer, scope_name="generator_bs", reuse=<em>True</em>)<br/>   # discriminator for Shoes<br/>   self.disc_s_real = discriminator(self.X_shoes,self.initializer, scope_name="discriminator_s")<br/>   self.disc_s_fake = discriminator(self.gen_s_fake,self.initializer, scope_name="discriminator_s", reuse=<em>True</em>)<br/>   # discriminator for Bags<br/>   self.disc_b_real = discriminator(self.X_bags,self.initializer,scope_name="discriminator_b")<br/>   self.disc_b_fake = discriminator(self.gen_b_fake, self.initializer, reuse=<em>True</em>,scope_name="discriminator_b")</pre>
<ol start="6">
<li>Let's define the <kbd>loss</kbd> function that we defined previously in <em>DiscoGAN modeling</em> section. Following function <kbd>define_loss</kbd> defines<span> the reconstruction loss based on the Euclidean distance between the reconstructed and original image. To generate the GAN and discriminator loss, the function uses the cross entropy function:</span></li>
</ol>
<pre style="padding-left: 60px">def define_loss(self):<br/>   # Reconstruction loss for generators<br/>   self.const_loss_s = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_s, self.X_shoes))<br/>   self.const_loss_b = tf.reduce_mean(tf.losses.mean_squared_error(self.gen_recon_b, self.X_bags))<br/>   # generator loss for GANs<br/>   self.gen_s_loss = tf.reduce_mean(<br/>       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.ones_like(self.disc_s_fake)))<br/>   self.gen_b_loss = tf.reduce_mean(<br/>       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.ones_like(self.disc_b_fake)))<br/>   # Total generator Loss<br/>   self.gen_loss =  (self.const_loss_b + self.const_loss_s)  + self.gen_s_loss + self.gen_b_loss<br/>   # Cross Entropy loss for discriminators for shoes and bags<br/>   # Shoes<br/>   self.disc_s_real_loss = tf.reduce_mean(<br/>       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_real, labels=tf.ones_like(self.disc_s_real)))<br/>   self.disc_s_fake_loss = tf.reduce_mean(<br/>       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_s_fake, labels=tf.zeros_like(self.disc_s_fake)))<br/>   self.disc_s_loss = self.disc_s_real_loss + self.disc_s_fake_loss  # Combined<br/>   # Bags<br/>   self.disc_b_real_loss = tf.reduce_mean(<br/>       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_real, labels=tf.ones_like(self.disc_b_real)))<br/>   self.disc_b_fake_loss = tf.reduce_mean(<br/>       tf.nn.sigmoid_cross_entropy_with_logits(logits=self.disc_b_fake, labels=tf.zeros_like(self.disc_b_fake)))<br/>   self.disc_b_loss = self.disc_b_real_loss + self.disc_b_fake_loss<br/>   # Total discriminator Loss<br/>   self.disc_loss = self.disc_b_loss + self.disc_s_loss</pre>
<ol start="7">
<li>Use the  <kbd>AdamOptimizer</kbd> that was defined in <a href="60549866-497e-4dfa-890c-6651f34cf8e4.xhtml">Chapter 3</a>, <em>Sentiment Analysis in Your Browser Using TensorFlow.js</em>, of the book and implement the following <kbd>define_optimizer</kbd> function as follows:</li>
</ol>
<pre style="padding-left: 60px"><em>def</em> define_optimizer(self):<br/>   self.disc_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.disc_loss, var_list=self.disc_params)<br/>   self.gen_optimizer = tf.train.AdamOptimizer(LEARNING_RATE).minimize(self.gen_loss, var_list=self.gen_params)</pre>
<ol start="8">
<li>For debugging, write the summary into a logging file. While you can add anything to the summary, the function <kbd>summary_</kbd> below adds all of the losses just to observe the curves on how various losses change over time. Code for the function is as follows:</li>
</ol>
<pre style="padding-left: 60px"><em>def</em> summary_(self):<br/>   # Store the losses<br/>   tf.summary.scalar("gen_loss", self.gen_loss)<br/>   tf.summary.scalar("gen_s_loss", self.gen_s_loss)<br/>   tf.summary.scalar("gen_b_loss", self.gen_b_loss)<br/>   tf.summary.scalar("const_loss_s", self.const_loss_s)<br/>   tf.summary.scalar("const_loss_b", self.const_loss_b)<br/>   tf.summary.scalar("disc_loss", self.disc_loss)<br/>   tf.summary.scalar("disc_b_loss", self.disc_b_loss)<br/>   tf.summary.scalar("disc_s_loss", self.disc_s_loss)<br/><br/>   # Histograms for all vars<br/>   <em>for</em> var <em>in</em> tf.trainable_variables():<br/>       tf.summary.histogram(var.name, var)<br/><br/>   self.summary_ = tf.summary.merge_all()</pre>
<ol start="9">
<li>Define the following parameters for training the model:</li>
</ol>
<ul>
<li style="list-style-type: none">
<ul>
<li>Batch Size: 256</li>
<li>Learning rate: 0.0002</li>
<li>Epochs = 100,000 (use more if you are not getting the desired result)</li>
</ul>
</li>
</ul>
<ol start="10">
<li>Use the following code to train the model. Here is the brief explanation of what it does:
<ol>
<li>For each Epoch, the code obtains the mini batch images of both shoes and bags. It passes the mini batch through the model to update the discriminator loss first.</li>
<li>Samples a mini batch again for both shoes and bags and updates the generator loss keeping discriminator parameters fixed.</li>
<li>For every 10 epochs, it writes a summary to Tensorboard. </li>
<li>For every 1000 epochs, it randomly samples 1 image from both bags and shoes dataset and saves the reconstructed and fake images for visualization purposes</li>
<li>Also, for every 1000 epochs, it saves the model which can be helpful if you want to restore training at some point.</li>
</ol>
</li>
</ol>
<pre>print ("Starting Training")<br/><em>for</em> global_step <em>in</em> range(start_epoch,EPOCHS):<br/>   shoe_batch = get_next_batch(BATCH_SIZE,"shoes")<br/>   bag_batch = get_next_batch(BATCH_SIZE,"bags")<br/>   feed_dict_batch = {<em>model</em>.X_bags: bag_batch, <em>model</em>.X_shoes: shoe_batch}<br/>   op_list = [<em>model</em>.disc_optimizer, <em>model</em>.gen_optimizer, <em>model</em>.disc_loss, <em>model</em>.gen_loss, <em>model</em>.summary_]<br/>   _, _, disc_loss, gen_loss, summary_ = sess.run(op_list, feed_dict=feed_dict_batch)<br/>   shoe_batch = get_next_batch(BATCH_SIZE, "shoes")<br/>   bag_batch = get_next_batch(BATCH_SIZE, "bags")<br/>   feed_dict_batch = {<em>model</em>.X_bags: bag_batch, <em>model</em>.X_shoes: shoe_batch}<br/>   _, gen_loss = sess.run([<em>model</em>.gen_optimizer, <em>model</em>.gen_loss], feed_dict=feed_dict_batch)<br/>   <em>if</em> global_step%10 ==0:<br/>       train_writer.add_summary(summary_,global_step)<br/>   <em>if</em> global_step%100 == 0:<br/>       print("EPOCH:" + str(global_step) + "\tgenerator Loss: " + str(gen_loss) + "\tdiscriminator Loss: " + str(disc_loss))<br/>   <em>if</em> global_step % 1000 == 0:<br/>       shoe_sample = get_next_batch(1, "shoes")<br/>       bag_sample = get_next_batch(1, "bags")<br/>       ops = [<em>model</em>.gen_s_fake, <em>model</em>.gen_b_fake, <em>model</em>.gen_recon_s, <em>model</em>.gen_recon_b]<br/>       gen_s_fake, gen_b_fake, gen_recon_s, gen_recon_b = sess.run(ops, feed_dict={<em>model</em>.X_shoes: shoe_sample, <em>model</em>.X_bags: bag_sample})<br/>       save_image(global_step, gen_s_fake, str("gen_s_fake_") + str(global_step))<br/>       save_image(global_step,gen_b_fake, str("gen_b_fake_") + str(global_step))<br/>       save_image(global_step, gen_recon_s, str("gen_recon_s_") + str(global_step))<br/>       save_image(global_step, gen_recon_b, str("gen_recon_b_") + str(global_step))<br/>   <em>if</em> global_step % 1000 == 0:<br/>       <em>if not</em> os.path.exists("./model"):<br/>           os.makedirs("./model")<br/>       saver.save(sess, "./model" + '/model-' + str(global_step) + '.ckpt')<br/>       print("Saved Model")<br/>print ("Starting Training")<br/><em>for</em> global_step <em>in</em> range(start_epoch,EPOCHS):<br/>   shoe_batch = get_next_batch(BATCH_SIZE,"shoes")<br/>   bag_batch = get_next_batch(BATCH_SIZE,"bags")<br/>   feed_dict_batch = {<em>model</em>.X_bags: bag_batch, <em>model</em>.X_shoes: shoe_batch}<br/>   op_list = [<em>model</em>.disc_optimizer, <em>model</em>.gen_optimizer, <em>model</em>.disc_loss, <em>model</em>.gen_loss, <em>model</em>.summary_]<br/>   _, _, disc_loss, gen_loss, summary_ = sess.run(op_list, feed_dict=feed_dict_batch)<br/>   shoe_batch = get_next_batch(BATCH_SIZE, "shoes")<br/>   bag_batch = get_next_batch(BATCH_SIZE, "bags")<br/>   feed_dict_batch = {<em>model</em>.X_bags: bag_batch, <em>model</em>.X_shoes: shoe_batch}<br/>   _, gen_loss = sess.run([<em>model</em>.gen_optimizer, <em>model</em>.gen_loss], feed_dict=feed_dict_batch)<br/>   <em>if</em> global_step%10 ==0:<br/>       train_writer.add_summary(summary_,global_step)<br/>   <em>if</em> global_step%100 == 0:<br/>       print("EPOCH:" + str(global_step) + "\tgenerator Loss: " + str(gen_loss) + "\tdiscriminator Loss: " + str(disc_loss))<br/>   <em>if</em> global_step % 1000 == 0:<br/>       shoe_sample = get_next_batch(1, "shoes")<br/>       bag_sample = get_next_batch(1, "bags")<br/>       ops = [<em>model</em>.gen_s_fake, <em>model</em>.gen_b_fake, <em>model</em>.gen_recon_s, <em>model</em>.gen_recon_b]<br/>       gen_s_fake, gen_b_fake, gen_recon_s, gen_recon_b = sess.run(ops, feed_dict={<em>model</em>.X_shoes: shoe_sample, <em>model</em>.X_bags: bag_sample})<br/>       save_image(global_step, gen_s_fake, str("gen_s_fake_") + str(global_step))<br/>       save_image(global_step,gen_b_fake, str("gen_b_fake_") + str(global_step))<br/>       save_image(global_step, gen_recon_s, str("gen_recon_s_") + str(global_step))<br/>       save_image(global_step, gen_recon_b, str("gen_recon_b_") + str(global_step))<br/>   <em>if</em> global_step % 1000 == 0:<br/>       <em>if not</em> os.path.exists("./model"):<br/>           os.makedirs("./model")<br/>       saver.save(sess, "./model" + '/model-' + str(global_step) + '.ckpt')<br/><br/>       print("Saved Model")<br/><br/></pre>
<div class="packt_tip"><span>We carried out the training on one GTX 1080 graphics card, which took </span>a significant amount of time. Highly recommended to use a GPU with better processing than GTX 1080 if possible. </div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we first looked at what <span>GANs</span> are. They are a new kind of generative model that helps us to generate new images. </p>
<p>We also touched upon other kinds of generative models, such as Variational Auto-encoders and PixelRNN, to get an overview of different kinds of generative models. We also talked about different kinds of GANs to discuss the progress that had been made in this space since the first paper on GANs was published in 2014.</p>
<p>Then, we learned about DiscoGANs, a new type of GAN that can help us to learn about cross- domain relationships. Specifically, in this chapter, our focus was on building a model to generate handbag images from shoes and vice versa.</p>
<p>Finally, we learned about the architecture of DiscoGANs and how they differ from usual GANs.</p>
<p>In the next chapter, we will learn how to implement capsule networks on the Fashion MNIST dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ul>
<li>Can you change the training parameters like learning rate, batch size and observe the changes in quality of the reconstructed and fake images?</li>
<li>Can you visualize the stored summary in Tensorboard to understand the learning process?</li>
<li>Can you think of other datasets which can be used for Style transfer?</li>
</ul>


            </article>

            
        </section>
    </body></html>