<html><head></head><body>
		<div id="_idContainer100">
			<h1 id="_idParaDest-166"><em class="italic"><a id="_idTextAnchor193"/>Chapter 7</em>: Deploying Deep RL Agents to the Cloud</h1>
			<p>The cloud has become the <em class="italic">de facto</em> platform of deployment for AI-based products and solutions. Deep learning models running in the cloud are becoming increasingly common. The deployment of reinforcement learning-based agents to the cloud is, however, still very limited for a variety of reasons. This chapter contains recipes to equip yourself with tools and details to get ahead of the curve and build cloud-based Simulation-as-a-Service and Agent/Bot-as-a-Service applications using deep RL.</p>
			<p>Specifically, the following recipes are discussed in this chapter:</p>
			<ul>
				<li>Implementing the RL agent’s runtime components</li>
				<li>Building RL environment simulators as a service</li>
				<li>Training RL agents using a remote simulator service</li>
				<li>Testing/evaluating RL agents</li>
				<li>Packaging RL agents for deployment – a trading bot</li>
				<li>Deploying RL agents to the cloud – a trading Bot-as-a-Service</li>
			</ul>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor194"/>Technical requirements</h1>
			<p>The code in the book is extensively tested on Ubuntu 18.04 and Ubuntu 20.04 and should work with later versions of Ubuntu if Python 3.6+ is available. With Python 3.6+ installed along with the necessary Python packages as listed before the start of each of the recipes, the code should run fine on Windows and macOS X too. It is advised to create and use a Python virtual environment named <strong class="source-inline">tf2rl-cookbook</strong> to install the packages and run the code in this book. Miniconda or Anaconda installation for Python virtual environment management is recommended.</p>
			<p>The complete code for each recipe in each chapter is available here: <a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook</a>.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor195"/>Implementing the RL agent’s runtime components</h1>
			<p>We have looked at several agent algorithm<a id="_idIndexMarker705"/> implementations in the previous chapters. You may have noticed from recipes in the previous chapters (especially <a href="B15074_03_ePub_AM.xhtml#_idTextAnchor090"><em class="italic">Chapter 3</em></a>, <em class="italic">Implementing Advanced Deep RL Algorithms</em>), where we implemented RL agent training code, that some parts of the agent code were conditionally executed. For example, the experience replay routine was only run when a certain condition (such as the number of samples in the replay memory) was met, and so on. That begs the question: what are the essential components in an agent that is required, especially when we do not aim to train it further and only execute a learned policy?</p>
			<p>This recipe will help you distill the implementation of<a id="_idIndexMarker706"/> the <strong class="bold">Soft Actor-Critic</strong> (<strong class="bold">SAC</strong>) agent down to the minimal set of components – those that are absolutely necessary for the runtime of your agent.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor196"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repository. WebGym is built on top of the miniwob-plusplus benchmark (<a href="https://github.com/stanfordnlp/miniwob-plusplus">https://github.com/stanfordnlp/miniwob-plusplus</a>), which is also made available as part of this book’s code repository for ease of use. If the following <strong class="source-inline">import</strong> statements run without issues, you are ready to get started:</p>
			<p class="source-code">import functools</p>
			<p class="source-code">from collections import deque</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">import tensorflow_probability as tfp</p>
			<p class="source-code">from tensorflow.keras.layers import Concatenate, Dense, Input</p>
			<p class="source-code">from tensorflow.keras.models import Model</p>
			<p class="source-code">from tensorflow.keras.optimizers import Adam</p>
			<p class="source-code">tf.keras.backend.set_floatx(“float64”)</p>
			<p>Now, let’s begin!</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor197"/>How to do it…</h2>
			<p>The<a id="_idIndexMarker707"/> following steps provide the implementation details for the minimal runtime necessary to utilize an SAC agent. Let’s jump right into the details:</p>
			<ol>
				<li>First, let’s implement the actor component, which is going to be a TensorFlow 2.x model:<p class="source-code">def actor(state_shape, action_shape, units=(512, 256, 64)):</p><p class="source-code">    state_shape_flattened = \</p><p class="source-code">        functools.reduce(lambda x, y: x * y, state_shape)</p><p class="source-code">    state = Input(shape=state_shape_flattened)</p><p class="source-code">    x = Dense(units[0], name=”L0”, activation=”relu”)\</p><p class="source-code">             (state)</p><p class="source-code">    for index in range(1, len(units)):</p><p class="source-code">        x = Dense(units[index], name=”L{}”.format(index),</p><p class="source-code">                  activation=”relu”)(x)</p><p class="source-code">    actions_mean = Dense(action_shape[0], \</p><p class="source-code">                         name=”Out_mean”)(x)</p><p class="source-code">    actions_std = Dense(action_shape[0], </p><p class="source-code">                         name=”Out_std”)(x)</p><p class="source-code">    model = Model(inputs=state, outputs=[actions_mean,</p><p class="source-code">                  actions_std])</p><p class="source-code">    return model</p></li>
				<li>Next, let’s implement the critic component, which is also going to be a TensorFlow 2.x model:<p class="source-code">def critic(state_shape, action_shape, units=(512, 256, 64)):</p><p class="source-code">    state_shape_flattened = \</p><p class="source-code">        functools.reduce(lambda x, y: x * y, state_shape)</p><p class="source-code">    inputs = [Input(shape=state_shape_flattened), \</p><p class="source-code">              Input(shape=action_shape)]</p><p class="source-code">    concat = Concatenate(axis=-1)(inputs)</p><p class="source-code">    x = Dense(units[0], name=”Hidden0”, \</p><p class="source-code">              activation=”relu”)(concat)</p><p class="source-code">    for index in range(1, len(units)):</p><p class="source-code">        x = Dense(units[index], \</p><p class="source-code">                  name=”Hidden{}”.format(index), \</p><p class="source-code">                  activation=”relu”)(x)</p><p class="source-code">    output = Dense(1, name=”Out_QVal”)(x)</p><p class="source-code">    model = Model(inputs=inputs, outputs=output)</p><p class="source-code">    return model</p></li>
				<li>Now, let’s <a id="_idIndexMarker708"/>implement a utility function to update the weights of a target model given a source TensorFlow 2.x model:<p class="source-code">def update_target_weights(model, target_model, tau=0.005):</p><p class="source-code">    weights = model.get_weights()</p><p class="source-code">    target_weights = target_model.get_weights()</p><p class="source-code">    for i in range(len(target_weights)):  </p><p class="source-code">    # set tau% of target model to be new weights</p><p class="source-code">        target_weights[i] = weights[i] * tau + \</p><p class="source-code">                            target_weights[i] * (1 - tau)</p><p class="source-code">    target_model.set_weights(target_weights)</p></li>
				<li>Now, we can <a id="_idIndexMarker709"/>begin our SAC agent runtime class implementation. We will split our implementation into the following steps. Let’s start with the class implementation and define the constructor arguments in this step:<p class="source-code">class SAC(object):</p><p class="source-code">    def __init__(</p><p class="source-code">        self,</p><p class="source-code">        observation_shape,</p><p class="source-code">        action_space,</p><p class="source-code">        lr_actor=3e-5,</p><p class="source-code">        lr_critic=3e-4,</p><p class="source-code">        actor_units=(64, 64),</p><p class="source-code">        critic_units=(64, 64),</p><p class="source-code">        auto_alpha=True,</p><p class="source-code">        alpha=0.2,</p><p class="source-code">        tau=0.005,</p><p class="source-code">        gamma=0.99,</p><p class="source-code">        batch_size=128,</p><p class="source-code">        memory_cap=100000,</p><p class="source-code">    ):</p></li>
				<li>Let’s now initialize the state/observation shapes, action shapes, and action limits/bounds for the agent and also initialize a deque to store the agent’s memory:<p class="source-code">        self.state_shape = observation_shape  # shape of </p><p class="source-code">        # observations</p><p class="source-code">        self.action_shape = action_space.shape  # number </p><p class="source-code">        # of actions</p><p class="source-code">        self.action_bound = \</p><p class="source-code">            (action_space.high - action_space.low) / 2</p><p class="source-code">        self.action_shift = \</p><p class="source-code">            (action_space.high + action_space.low) / 2</p><p class="source-code">        self.memory = deque(maxlen=int(memory_cap))</p></li>
				<li>In this step, let’s<a id="_idIndexMarker710"/> define and initialize the actor component:<p class="source-code">        # Define and initialize actor network</p><p class="source-code">        self.actor = actor(self.state_shape, </p><p class="source-code">                           self.action_shape, </p><p class="source-code">                           actor_units)</p><p class="source-code">        self.actor_optimizer = \</p><p class="source-code">            Adam(learning_rate=lr_actor)</p><p class="source-code">        self.log_std_min = -20</p><p class="source-code">        self.log_std_max = 2</p><p class="source-code">        print(self.actor.summary())</p></li>
				<li>Let’s now define and initialize the critic components:<p class="source-code">        # Define and initialize critic networks</p><p class="source-code">        self.critic_1 = critic(self.state_shape, </p><p class="source-code">                               self.action_shape, </p><p class="source-code">                               critic_units)</p><p class="source-code">        self.critic_target_1 = critic(self.state_shape,</p><p class="source-code">                                      self.action_shape,</p><p class="source-code">                                      critic_units)</p><p class="source-code">        self.critic_optimizer_1 = \</p><p class="source-code">            Adam(learning_rate=lr_critic)</p><p class="source-code">        update_target_weights(self.critic_1, </p><p class="source-code">                              self.critic_target_1, </p><p class="source-code">                              tau=1.0)</p><p class="source-code">        self.critic_2 = critic(self.state_shape, </p><p class="source-code">                               self.action_shape, </p><p class="source-code">                               critic_units)</p><p class="source-code">        self.critic_target_2 = critic(self.state_shape,</p><p class="source-code">                                      self.action_shape,</p><p class="source-code">                                      critic_units)</p><p class="source-code">        self.critic_optimizer_2 = \</p><p class="source-code">            Adam(learning_rate=lr_critic)</p><p class="source-code">        update_target_weights(self.critic_2, </p><p class="source-code">                              self.critic_target_2, </p><p class="source-code">                              tau=1.0)</p><p class="source-code">        print(self.critic_1.summary())</p></li>
				<li>In this step, let’s<a id="_idIndexMarker711"/> initialize the temperature and target entropy for the SAC agent based on the <strong class="source-inline">auto_alpha</strong> flag:<p class="source-code">        # Define and initialize temperature alpha and </p><p class="source-code">        # target entropy</p><p class="source-code">        self.auto_alpha = auto_alpha</p><p class="source-code">        if auto_alpha:</p><p class="source-code">            self.target_entropy = \</p><p class="source-code">                -np.prod(self.action_shape)</p><p class="source-code">            self.log_alpha = tf.Variable(0.0, </p><p class="source-code">                                        dtype=tf.float64)</p><p class="source-code">            self.alpha = tf.Variable(0.0, </p><p class="source-code">                                     dtype=tf.float64)</p><p class="source-code">            self.alpha.assign(tf.exp(self.log_alpha))</p><p class="source-code">            self.alpha_optimizer = \</p><p class="source-code">                Adam(learning_rate=lr_actor)</p><p class="source-code">        else:</p><p class="source-code">            self.alpha = tf.Variable(alpha, </p><p class="source-code">                                     dtype=tf.float64)</p></li>
				<li>Let’s complete the constructor implementation by setting the hyperparameters and initializing the <a id="_idIndexMarker712"/>training progress summary dictionary for TensorBoard logging:<p class="source-code">        # Set hyperparameters</p><p class="source-code">        self.gamma = gamma  # discount factor</p><p class="source-code">        self.tau = tau  # target model update</p><p class="source-code">        self.batch_size = batch_size</p><p class="source-code">        # Tensorboard</p><p class="source-code">        self.summaries = {}</p></li>
				<li>With the<a id="_idIndexMarker713"/> constructor implementation completed, let’s now move on to implement the <strong class="source-inline">process_action</strong> function, which takes the raw action from the agent and processes it so that it can be executed:<p class="source-code">    def process_actions(self, mean, log_std, test=False, </p><p class="source-code">    eps=1e-6):</p><p class="source-code">        std = tf.math.exp(log_std)</p><p class="source-code">        raw_actions = mean</p><p class="source-code">        if not test:</p><p class="source-code">            raw_actions += tf.random.normal(shape=mean.\</p><p class="source-code">                           shape, dtype=tf.float64) * std</p><p class="source-code">        log_prob_u = tfp.distributions.Normal(loc=mean,</p><p class="source-code">                         scale=std).log_prob(raw_actions)</p><p class="source-code">        actions = tf.math.tanh(raw_actions)</p><p class="source-code">        log_prob = tf.reduce_sum(log_prob_u - \</p><p class="source-code">                     tf.math.log(1 - actions ** 2 + eps))</p><p class="source-code">        actions = actions * self.action_bound + \</p><p class="source-code">                  self.action_shift</p><p class="source-code">        return actions, log_prob</p></li>
				<li>This step is <a id="_idIndexMarker714"/>crucial. We are going to implement the <strong class="source-inline">act</strong> method, which will take as input the state and generate and return the action to be executed:<p class="source-code">    def act(self, state, test=False, use_random=False):</p><p class="source-code">        state = state.reshape(-1)  # Flatten state</p><p class="source-code">        state = np.expand_dims(state, axis=0).\</p><p class="source-code">                                     astype(np.float64)</p><p class="source-code">        if use_random:</p><p class="source-code">            a = tf.random.uniform(</p><p class="source-code">                shape=(1, self.action_shape[0]), </p><p class="source-code">                       minval=-1, maxval=1, </p><p class="source-code">                       dtype=tf.float64</p><p class="source-code">            )</p><p class="source-code">        else:</p><p class="source-code">            means, log_stds = self.actor.predict(state)</p><p class="source-code">            log_stds = tf.clip_by_value(log_stds, </p><p class="source-code">                                        self.log_std_min,</p><p class="source-code">                                        self.log_std_max)</p><p class="source-code">            a, log_prob = self.process_actions(means,</p><p class="source-code">                                               log_stds, </p><p class="source-code">                                               test=test)</p><p class="source-code">        q1 = self.critic_1.predict([state, a])[0][0]</p><p class="source-code">        q2 = self.critic_2.predict([state, a])[0][0]</p><p class="source-code">        self.summaries[“q_min”] = tf.math.minimum(q1, q2)</p><p class="source-code">        self.summaries[“q_mean”] = np.mean([q1, q2])</p><p class="source-code">        return a</p></li>
				<li>Finally, let’s<a id="_idIndexMarker715"/> implement utility methods to load the actor and critic model weights from previously trained models:<p class="source-code">    def load_actor(self, a_fn):</p><p class="source-code">        self.actor.load_weights(a_fn)</p><p class="source-code">        print(self.actor.summary())</p><p class="source-code">    def load_critic(self, c_fn):</p><p class="source-code">        self.critic_1.load_weights(c_fn)</p><p class="source-code">        self.critic_target_1.load_weights(c_fn)</p><p class="source-code">        self.critic_2.load_weights(c_fn)</p><p class="source-code">        self.critic_target_2.load_weights(c_fn)</p><p class="source-code">        print(self.critic_1.summary())</p></li>
			</ol>
			<p>That completes the implementation of all the necessary runtime components for the SAC RL agent! </p>
			<h2 id="_idParaDest-171"><a id="_idTextAnchor198"/>How it works…</h2>
			<p>In this recipe, we implemented the essential runtime components for the SAC agent. The runtime components include the actor and critic model definitions, a mechanism to load weights from previously trained agent models, and an agent interface to generate actions given states <a id="_idIndexMarker716"/>using the actor’s prediction and to process the prediction to generate an executable action.</p>
			<p>The runtime components for other actor-critic-based RL agent algorithms, such as A2C, A3C, and DDPG, as well as their extensions and variants, will be very similar, if not the same.</p>
			<p>It’s now time to move on to the next recipe!</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor199"/>Building RL environment simulators as a service</h1>
			<p>This<a id="_idIndexMarker717"/> recipe will walk you through the process<a id="_idIndexMarker718"/> of converting your RL training environment/simulator into a service. This will allow you to offer Simulation-as-a-Service for training RL agents!</p>
			<p>So far, we have trained several RL agents in a variety of environments using different simulators depending on the task to be solved. The training scripts used the Open AI Gym interface to talk to the environment running in the same process, or locally in a different process. This recipe will guide you through the process of converting any OpenAI Gym-compatible training environment (including your custom RL training environments) into a service that can be deployed locally or remotely as a service. Once built and deployed, an agent training client can connect to the sim server and train one or more agents remotely. </p>
			<p>As a concrete example, we will take our <strong class="source-inline">tradegym</strong> library, which is a collection of the RL training environments for cryptocurrency and stock trading that we built in the previous chapters, and expose them<a id="_idIndexMarker719"/> through a <strong class="bold">RESTful HTTP interface</strong> for training RL agents.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor200"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repository. </p>
			<p>We will also <a id="_idIndexMarker720"/>need to create a new Python module called <strong class="source-inline">tradegym</strong>, which contains the <strong class="source-inline">crypto_trading_env.py</strong>, <strong class="source-inline">stock_trading_continuous_env.py</strong>, <strong class="source-inline">trading_utils.py</strong>, and other custom trading <a id="_idIndexMarker721"/>environments we implemented in the previous chapters. You will find the <strong class="source-inline">tradegym</strong> module <a id="_idIndexMarker722"/>with its contents in the book’s code repository as well.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor201"/>How to do it…</h2>
			<p>Our implementation will contain two core modules – the <strong class="source-inline">tradegym</strong> server and the <strong class="source-inline">tradegym</strong> client, which are built based on the OpenAI Gym HTTP API. The recipe will focus on the customizations and the core components of the HTTP service interface. We will first define a minimum set of custom environments exposed as part of the <strong class="source-inline">tradegym</strong> library and then build the server and client modules:</p>
			<ol>
				<li value="1">Let’s first make sure the minimal contents of the <strong class="source-inline">tradegym</strong> library’s <strong class="source-inline">__init__.py</strong> file exists so that we can import these environments:<p class="source-code">import sys</p><p class="source-code">import os</p><p class="source-code">from gym.envs.registration import register</p><p class="source-code">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</p><p class="source-code">_AVAILABLE_ENVS = {</p><p class="source-code">    “CryptoTradingEnv-v0”: {</p><p class="source-code">        “entry_point”: \</p><p class="source-code">          “tradegym.crypto_trading_env:CryptoTradingEnv”,</p><p class="source-code">        “description”: “Crypto Trading RL environment”,</p><p class="source-code">    },</p><p class="source-code">    “StockTradingContinuousEnv-v0”: {</p><p class="source-code">        “entry_point”: “tradegym.stock_trading_\</p><p class="source-code">             continuous_env:StockTradingContinuousEnv”,</p><p class="source-code">        “description”: “Stock Trading RL environment with continous action space”,</p><p class="source-code">    },</p><p class="source-code">}</p><p class="source-code">for env_id, val in _AVAILABLE_ENVS.items():</p><p class="source-code">    register(id=env_id, entry_point=val.get(</p><p class="source-code">                                     “entry_point”))</p></li>
				<li>We can<a id="_idIndexMarker723"/> now begin our <strong class="source-inline">tradegym</strong> server <a id="_idIndexMarker724"/>implementation as <strong class="source-inline">tradegym_http_server.py</strong>. We will finalize the implementation in the following several steps. Let’s begin by importing the necessary Python modules:<p class="source-code">import argparse</p><p class="source-code">import json</p><p class="source-code">import logging</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">import uuid</p><p class="source-code">import numpy as np</p><p class="source-code">import six</p><p class="source-code">from flask import Flask, jsonify, request</p><p class="source-code">import gym</p></li>
				<li>Next, we will import the <strong class="source-inline">tradegym</strong> module to register the available environments with the Gym registry:<p class="source-code">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</p><p class="source-code">import tradegym  # Register tradegym envs with OpenAI Gym </p><p class="source-code"># registry</p></li>
				<li>Let’s now <a id="_idIndexMarker725"/>look at the skeleton of the environment<a id="_idIndexMarker726"/> container class with comments describing what each method does. You can refer to the full implementation of <strong class="source-inline">tradegym_http_server.py</strong> in the book’s code repository under <strong class="source-inline">chapter7</strong>. We will start with the class definition in this step and complete the skeleton in the following steps:<p class="source-code">class Envs(object):</p><p class="source-code">    def __init__(self):</p><p class="source-code">        self.envs = {}</p><p class="source-code">        self.id_len = 8  # Number of chars in instance_id</p></li>
				<li>In this step, we will look at the two helper methods that are useful for managing the environment instances. They enable look up and delete operations:<p class="source-code">    def _lookup_env(self, instance_id):</p><p class="source-code">        """Lookup environment based on instance_id and </p><p class="source-code">           throw error if not found"""</p><p class="source-code">    def _remove_env(self, instance_id):</p><p class="source-code">        """Delete environment associated with </p><p class="source-code">           instance_id"""</p></li>
				<li>Next, we will look at a few other methods that help with the environment management operations:<p class="source-code">    def create(self, env_id, seed=None):</p><p class="source-code">        """Create (make) an instance of the environment </p><p class="source-code">           with `env_id` and return the instance_id"""</p><p class="source-code">    def list_all(self):</p><p class="source-code">        """Return a dictionary of all the active </p><p class="source-code">           environments with instance_id as keys"""</p><p class="source-code">    def reset(self, instance_id):</p><p class="source-code">        """Reset the environment pointed to by the </p><p class="source-code">           instance_id"""</p><p class="source-code">        </p><p class="source-code">    def env_close(self, instance_id):</p><p class="source-code">        """Call .close() on the environment and remove </p><p class="source-code">           instance_id from the list of all envs"""</p></li>
				<li>The methods<a id="_idIndexMarker727"/> discussed in this step enable the<a id="_idIndexMarker728"/> core operation of the RL environment, which have 1-1 correspondences with the core Gym API:<p class="source-code">    def step(self, instance_id, action, render):</p><p class="source-code">        """Perform a single step in the environment </p><p class="source-code">           pointed to by the instance_id and return </p><p class="source-code">           observation, reward, done and info"""</p><p class="source-code">    def get_action_space_contains(self, instance_id, x):</p><p class="source-code">        """Check if the given environment’s action space </p><p class="source-code">           contains x"""</p><p class="source-code">    def get_action_space_info(self, instance_id):</p><p class="source-code">        """Return the observation space infor for the </p><p class="source-code">           given environment instance_id"""</p><p class="source-code">    def get_action_space_sample(self, instance_id):</p><p class="source-code">        """Return a sample action for the environment </p><p class="source-code">           referred by the instance_id"""</p><p class="source-code">    def get_observation_space_contains(self, instance_id, </p><p class="source-code">    j):</p><p class="source-code">        """Return true is the environment’s observation </p><p class="source-code">           space contains `j`. False otherwise"""</p><p class="source-code">    def get_observation_space_info(self, instance_id):</p><p class="source-code">        """Return the observation space for the </p><p class="source-code">           environment referred by the instance_id"""</p><p class="source-code">    def _get_space_properties(self, space):</p><p class="source-code">        """Return a dictionary containing the attributes </p><p class="source-code">           and values of the given Gym Spce (Discrete, </p><p class="source-code">           Box etc.)"""</p></li>
				<li>With the <a id="_idIndexMarker729"/>preceding skeleton (and implementation), we <a id="_idIndexMarker730"/>can look at a few samples to expose these operations as REST APIs using<a id="_idIndexMarker731"/> the <strong class="bold">Flask</strong> Python library. We will discuss the core server application setup and the route setup for the create, reset, and step methods in the following steps. Let’s look at the server application setup that exposes the endpoint handlers:<p class="source-code">app = Flask(__name__)</p><p class="source-code">envs = Envs()</p></li>
				<li>We can now<a id="_idIndexMarker732"/> look at the REST API route definition for <a id="_idIndexMarker733"/>the <strong class="bold">HTTP POST</strong> endpoint at <strong class="source-inline">v1/envs</strong>. This<a id="_idIndexMarker734"/> takes in an <strong class="source-inline">env_id</strong>, which should be a valid Gym environment ID (such as our custom <strong class="source-inline">StockTradingContinuous-v0</strong> or <strong class="source-inline">MountainCar-v0</strong>, which is available in the Gym registry) and returns an <strong class="source-inline">instance_id</strong>:<p class="source-code">@app.route(“/v1/envs/”, methods=[“POST”])</p><p class="source-code">def env_create():</p><p class="source-code">    env_id = get_required_param(request.get_json(), </p><p class="source-code">                                “env_id”)</p><p class="source-code">    seed = get_optional_param(request.get_json(), </p><p class="source-code">                               “seed”, None)</p><p class="source-code">    instance_id = envs.create(env_id, seed)</p><p class="source-code">    return jsonify(instance_id=instance_id)</p></li>
				<li>Next, we will look at the REST API route definition for the HTTP POST endpoint at <strong class="source-inline">v1/envs/&lt;instance_id&gt;/reset</strong>, where <strong class="source-inline">&lt;instance_id&gt;</strong> can be any of the IDs returned by the <strong class="source-inline">env_create()</strong> method:<p class="source-code">@app.route(“/v1/envs/&lt;instance_id&gt;/reset/”, </p><p class="source-code">           methods=[“POST”])</p><p class="source-code">def env_reset(instance_id):</p><p class="source-code">    observation = envs.reset(instance_id)</p><p class="source-code">    if np.isscalar(observation):</p><p class="source-code">        observation = observation.item()</p><p class="source-code">    return jsonify(observation=observation)</p></li>
				<li>Next, we will look at the route definition for the endpoint at <strong class="source-inline">v1/envs/&lt;instance_id&gt;/step</strong>, which is the endpoint that will likely be called the most number of <a id="_idTextAnchor202"/>times in an RL training loop:<p class="source-code">@app.route(“/v1/envs/&lt;instance_id&gt;/step/”,</p><p class="source-code">           methods=[“POST”])</p><p class="source-code">def env_step(instance_id):</p><p class="source-code">    json = request.get_json()</p><p class="source-code">    action = get_required_param(json, “action”)</p><p class="source-code">    render = get_optional_param(json, “render”, False)</p><p class="source-code">    [obs_jsonable, reward, done, info] = envs.step(instance_id, action, render)</p><p class="source-code">    return jsonify(observation=obs_jsonable, </p><p class="source-code">                   reward=reward, done=done, info=info)</p></li>
				<li>For the <a id="_idIndexMarker735"/>remaining route definitions on the <strong class="source-inline">tradegym</strong> server, refer <a id="_idIndexMarker736"/>to the book’s code repository. We will implement a <strong class="source-inline">__main__</strong> function in the <strong class="source-inline">tradegym</strong> server script to launch the server when executed (which we will be using later in this recipe to test):<p class="source-code">if __name__ == “__main__”:</p><p class="source-code">    parser = argparse.ArgumentParser(description=”Start a</p><p class="source-code">                                    Gym HTTP API server”)</p><p class="source-code">    parser.add_argument(“-l”,“--listen”, help=”interface\</p><p class="source-code">                        to listen to”, default=”0.0.0.0”)</p><p class="source-code">    parser.add_argument(“-p”, “--port”, default=6666, \</p><p class="source-code">                        type=int, help=”port to bind to”)</p><p class="source-code">    args = parser.parse_args()</p><p class="source-code">    print(“Server starting at: “ + \</p><p class="source-code">           “http://{}:{}”.format(args.listen, args.port))</p><p class="source-code">    app.run(host=args.listen, port=args.port, debug=True)</p></li>
				<li>We will now <a id="_idIndexMarker737"/>move on to understand the implementation of the <strong class="source-inline">tradegym</strong> client. The full implementation is available in <strong class="source-inline">tradegym_http_client.py</strong> in the book’s code repository under <strong class="source-inline">chapter7</strong>. We will begin by importing the necessary Python modules in this step and continue to implement the client wrapper in the following steps:<p class="source-code">import json</p><p class="source-code">import logging</p><p class="source-code">import os</p><p class="source-code">import requests</p><p class="source-code">import six.moves.urllib.parse as urlparse</p></li>
				<li>The client class provides a <a id="_idIndexMarker738"/>Python wrapper to interface with the <strong class="source-inline">tradegym</strong> HTTP server. The constructor of the client class takes in the server’s address (IP and port information) to connect. Let’s look at the constructor implementation:<p class="source-code">class Client(object):</p><p class="source-code">    def __init__(self, remote_base):</p><p class="source-code">        self.remote_base = remote_base</p><p class="source-code">        self.session = requests.Session()</p><p class="source-code">        self.session.headers.update({“Content-type”: \</p><p class="source-code">                                     “application/json”})</p></li>
				<li>Reproducing all the standard Gym HTTP client methods here will not be a sensible use of the space available for this book and therefore, we will focus on the core wrapper methods, such as <strong class="source-inline">env_create</strong>, <strong class="source-inline">env_reset</strong>, and <strong class="source-inline">env_step</strong>, which we will use extensively in our agent training script. For a complete implementation, please refer to the book’s code repository. Let’s look at the <strong class="source-inline">env_create</strong> wrapper for creating an instance of an RL simulation environment on the remote <strong class="source-inline">tradegym</strong> server:<p class="source-code">    def env_create(self, env_id):</p><p class="source-code">        route = “/v1/envs/”</p><p class="source-code">        data = {“env_id”: env_id}</p><p class="source-code">        resp = self._post_request(route, data)</p><p class="source-code">        instance_id = resp[“instance_id”]</p><p class="source-code">        return instance_id</p></li>
				<li>In this step, we<a id="_idIndexMarker739"/> will look at the wrapper method <a id="_idIndexMarker740"/>that calls the <strong class="source-inline">reset</strong> method on a specific environment using the unique <strong class="source-inline">instance_id</strong> returned by the <strong class="source-inline">tradegym</strong> server when the <strong class="source-inline">env_create</strong> call is made:<p class="source-code">    def env_reset(self, instance_id):</p><p class="source-code">        route = “/v1/envs/{}/reset/”.format(instance_id)</p><p class="source-code">        resp = self._post_request(route, None)</p><p class="source-code">        observation = resp[“observation”]</p><p class="source-code">        return observation</p></li>
				<li>The most frequently used method on the <strong class="source-inline">tradegym</strong> client’s <strong class="source-inline">Client</strong> class is the <strong class="source-inline">step</strong> method. Let’s look at the implementation, which should look straightforward to you:<p class="source-code">    def env_step(self, instance_id, action, </p><p class="source-code">    render=False):</p><p class="source-code">        route = “/v1/envs/{}/step/”.format(instance_id)</p><p class="source-code">        data = {“action”: action, “render”: render}</p><p class="source-code">        resp = self._post_request(route, data)</p><p class="source-code">        observation = resp[“observation”]</p><p class="source-code">        reward = resp[“reward”]</p><p class="source-code">        done = resp[“done”]</p><p class="source-code">        info = resp[“info”]</p><p class="source-code">        return [observation, reward, done, info]</p></li>
				<li>With the other<a id="_idIndexMarker741"/> client wrapper methods in place, we can implement the <strong class="source-inline">__main__</strong> routine to connect to the <strong class="source-inline">tradegym</strong> server and call a few methods as an example to test whether everything is working <a id="_idIndexMarker742"/>as expected. Let’s write out the <strong class="source-inline">__main__</strong> routine:<p class="source-code">if __name__ == “__main__”:</p><p class="source-code">    remote_base = “http://127.0.0.1:6666”</p><p class="source-code">    client = Client(remote_base)</p><p class="source-code">    # Create environment</p><p class="source-code">    env_id = “StockTradingContinuousEnv-v0”</p><p class="source-code">    # env_id = “CartPole-v0”</p><p class="source-code">    instance_id = client.env_create(env_id)</p><p class="source-code">    # Check properties</p><p class="source-code">    all_envs = client.env_list_all()</p><p class="source-code">    logger.info(f”all_envs:{all_envs}”)</p><p class="source-code">    action_info = \</p><p class="source-code">        client.env_action_space_info(instance_id)</p><p class="source-code">    logger.info(f”action_info:{action_info}”)</p><p class="source-code">    obs_info = \</p><p class="source-code">        client.env_observation_space_info(instance_id)</p><p class="source-code">    # logger.info(f”obs_info:{obs_info}”)</p><p class="source-code">    # Run a single step</p><p class="source-code">    init_obs = client.env_reset(instance_id)</p><p class="source-code">    [observation, reward, done, info] = \</p><p class="source-code">        client.env_step(instance_id, 1, True)</p><p class="source-code">    logger.info(f”reward:{reward} done:{done} \</p><p class="source-code">                  info:{info}”)</p></li>
				<li>We can now<a id="_idIndexMarker743"/> start to actually create a client <a id="_idIndexMarker744"/>instance and check the <strong class="source-inline">tradegym</strong> service! First, we need to launch the <strong class="source-inline">tradegym</strong> ser<a id="_idTextAnchor203"/>ver by executing the following command:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python tradegym_http_server.py</strong></p></li>
				<li>Now, we can launch the <strong class="source-inline">tradegym</strong> client by running the following command in a separate terminal:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python tradegym_http_client.py</strong></p></li>
				<li>You should see an output similar to the following in the terminal where you launched the <strong class="source-inline">tradegym_http_client.py</strong> script:<p class="source-code"><strong class="bold">all_envs:{‘114c5e8f’: ‘StockTradingContinuousEnv-v0’, ‘6287385e’: ‘StockTradingContinuousEnv-v0’, ‘d55c97c0’: ‘StockTradingContinuousEnv-v0’, ‘fd355ed8’: ‘StockTradingContinuousEnv-v0’}</strong></p><p class="source-code"><strong class="bold">action_info:{‘high’: [1.0], ‘low’: [-1.0], ‘name’: ‘Box’, ‘shape’: [1]}</strong></p><p class="source-code"><strong class="bold">reward:0.0 done:False info:{}</strong></p></li>
			</ol>
			<p>That completes the recipe! Let’s briefly recap on how it works.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor204"/>How it works…</h2>
			<p>The <strong class="source-inline">tradegym</strong> server <a id="_idIndexMarker745"/>provides an environment container<a id="_idIndexMarker746"/> class and exposes the environment interface through a REST API. The <strong class="source-inline">tradegym</strong> client provides Python wrapper methods to interact with the RL environment through the REST API. </p>
			<p>The <strong class="source-inline">Envs</strong> class acts as a manager for the environments instantiated on the <strong class="source-inline">tradegym</strong> server. It also acts as a container for several environments as a client can send a request to create multiple (same or different) environments. When the <strong class="source-inline">tradegym</strong> client requests the <strong class="source-inline">tradegym</strong> server using the REST API to create a new environment, the server creates an instance of the requested environment and returns a unique instance ID (example: <strong class="source-inline">8kdi4289</strong>). From that point on, the client can refer to specific environments using the instance ID. This allows the client and the agent training code to interact with multiple environments simultaneously. Thus, the <strong class="source-inline">tradegym</strong> server acts as a true service with a RESTful interface over HTTP.</p>
			<p>Ready for the next recipe? Let’s do it.</p>
			<h1 id="_idParaDest-176"><a id="_idTextAnchor205"/>Training RL agents using a remote simulator service</h1>
			<p>In this recipe, we will <a id="_idIndexMarker747"/>look at how we can utilize a <a id="_idIndexMarker748"/>remote simulator service to train our agent. We will be reusing the SAC agent implementation from one of the previous chapters and will focus on how we can train the SAC, or any of your RL agents for that matter, using an RL simulator that is running elsewhere (on the cloud, for example) as a service. We will leverage the <strong class="source-inline">tradegym</strong> server we built in the previous recipe to provide us with the RL simulator service for this recipe.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor206"/>Getting ready</h2>
			<p>To complete this recipe, and to ensure that you have the latest version, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repository. If the following <strong class="source-inline">import</strong> statements run without issues, you are ready to get started:</p>
			<p class="source-code">import datetime</p>
			<p class="source-code">import os</p>
			<p class="source-code">import sys</p>
			<p class="source-code">import logging</p>
			<p class="source-code">import gym.spaces</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</p>
			<p class="source-code">from tradegym_http_client import Client</p>
			<p class="source-code">from sac_agent_base import SAC</p>
			<p>Let’s get right into it.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor207"/>How to do it…</h2>
			<p>We will<a id="_idIndexMarker749"/> implement the core parts of the<a id="_idIndexMarker750"/> training script and leave out command-line configuration and other non-essential functionalities to keep the script concise. Let’s name our script <strong class="source-inline">3_training_rl_agents_using_remote_sims.py</strong>.</p>
			<p>Let’s get started!</p>
			<ol>
				<li value="1">Let’s first create an application-level child logger, add a stream handler to it, and then set the logging level:<p class="source-code"># Create an App-level child logger</p><p class="source-code">logger = logging.getLogger(“TFRL-cookbook-ch7-training-with-sim-server”)</p><p class="source-code"># Set handler for this logger to handle messages</p><p class="source-code">logger.addHandler(logging.StreamHandler())</p><p class="source-code"># Set logging-level for this logger’s handler</p><p class="source-code">logger.setLevel(logging.DEBUG)</p></li>
				<li>Next, let’s <a id="_idIndexMarker751"/>create a TensorFlow <strong class="source-inline">SummaryWriter</strong> to log <a id="_idIndexMarker752"/>the agent’s training progress:<p class="source-code">current_time = datetime.datetime.now().strftime(“%Y%m%d-%H%M%S”)</p><p class="source-code">train_log_dir = os.path.join(“logs”, “TFRL-Cookbook-Ch4-SAC”, current_time)</p><p class="source-code">summary_writer = tf.summary.create_file_writer(train_log_dir)</p></li>
				<li>We can now get to the core of our implementation. Let’s start the implementation of the <strong class="source-inline">__main__</strong> function and continue the implementation in the following steps. Let’s first set up the client to connect to the sim service using the server address:<p class="source-code">if __name__ == “__main__”:</p><p class="source-code">    # Set up client to connect to sim server</p><p class="source-code">    sim_service_address = “http://127.0.0.1:6666”</p><p class="source-code">    client = Client(sim_service_address)</p></li>
				<li>Next, let’s <a id="_idIndexMarker753"/>ask the server to create <a id="_idIndexMarker754"/>our desired RL training environment to train our agent:<p class="source-code">    # Set up training environment</p><p class="source-code">    env_id = “StockTradingContinuousEnv-v0”</p><p class="source-code">    instance_id = client.env_create(env_id)</p></li>
				<li>Now, let’s initialize our agent:<p class="source-code">    # Set up agent</p><p class="source-code">    observation_space_info = \</p><p class="source-code">        client.env_observation_space_info(instance_id)</p><p class="source-code">    observation_shape = \</p><p class="source-code">        observation_space_info.get(“shape”)</p><p class="source-code">    action_space_info = \</p><p class="source-code">        client.env_action_space_info(instance_id)</p><p class="source-code">    action_space = gym.spaces.Box(</p><p class="source-code">        np.array(action_space_info.get(“low”)),</p><p class="source-code">        np.array(action_space_info.get(“high”)),</p><p class="source-code">        action_space_info.get(“shape”),</p><p class="source-code">    )</p><p class="source-code">    agent = SAC(observation_shape, action_space)</p></li>
				<li>We are now ready to configure our training using a few hyperparameters:<p class="source-code">    # Configure training</p><p class="source-code">    max_epochs = 30000</p><p class="source-code">    random_epochs = 0.6 * max_epochs</p><p class="source-code">    max_steps = 100</p><p class="source-code">    save_freq = 500</p><p class="source-code">    reward = 0</p><p class="source-code">    done = False</p><p class="source-code">    done, use_random, episode, steps, epoch, \</p><p class="source-code">    episode_reward = (</p><p class="source-code">        False,</p><p class="source-code">        True,</p><p class="source-code">        0,</p><p class="source-code">        0,</p><p class="source-code">        0,</p><p class="source-code">        0,</p><p class="source-code">    )</p></li>
				<li>With<a id="_idIndexMarker755"/> that, we are ready to start <a id="_idIndexMarker756"/>our outer training loop:<p class="source-code">    cur_state = client.env_reset(instance_id)</p><p class="source-code">    # Start training</p><p class="source-code">    while epoch &lt; max_epochs:</p><p class="source-code">        if steps &gt; max_steps:</p><p class="source-code">            done = True</p></li>
				<li>Let’s now handle the case where an episode has ended and <strong class="source-inline">done</strong> is set to <strong class="source-inline">True</strong>:<p class="source-code">        if done:</p><p class="source-code">            episode += 1</p><p class="source-code">            logger.info(</p><p class="source-code">                f”episode:{episode} \</p><p class="source-code">                 cumulative_reward:{episode_reward} \</p><p class="source-code">                 steps:{steps} epochs:{epoch}”)</p><p class="source-code">            with summary_writer.as_default():</p><p class="source-code">                tf.summary.scalar(“Main/episode_reward”, </p><p class="source-code">                            episode_reward, step=episode)</p><p class="source-code">                tf.summary.scalar(“Main/episode_steps”,</p><p class="source-code">                                   steps, step=episode)</p><p class="source-code">            summary_writer.flush()</p><p class="source-code">            done, cur_state, steps, episode_reward = (</p><p class="source-code">                False, </p><p class="source-code">            client.env_reset(instance_id), 0, 0,)</p><p class="source-code">            if episode % save_freq == 0:</p><p class="source-code">                agent.save_model(</p><p class="source-code">                    f”sac_actor_episode{episode}_\</p><p class="source-code">                      {env_id}.h5”,</p><p class="source-code">                    f”sac_critic_episode{episode}_\</p><p class="source-code">                      {env_id}.h5”,</p><p class="source-code">                )</p></li>
				<li>Now for <a id="_idIndexMarker757"/>the crucial steps! Let’s<a id="_idIndexMarker758"/> use the agent’s <strong class="source-inline">act</strong> and <strong class="source-inline">train</strong> methods to collect experience by acting (take actions) and training the agent using the collected experience:<p class="source-code">        if epoch &gt; random_epochs:</p><p class="source-code">            use_random = False</p><p class="source-code">        action = agent.act(np.array(cur_state), </p><p class="source-code">                           use_random=use_random)</p><p class="source-code">        next_state, reward, done, _ = client.env_step(</p><p class="source-code">            instance_id, action.numpy().tolist()</p><p class="source-code">        )</p><p class="source-code">        agent.train(np.array(cur_state), action, reward,</p><p class="source-code">                    np.array(next_state), done)</p></li>
				<li>Let’s now <a id="_idIndexMarker759"/>update the variables to prepare for the next step in the episode:<p class="source-code">        cur_state = next_state</p><p class="source-code">        episode_reward += reward</p><p class="source-code">        steps += 1</p><p class="source-code">        epoch += 1</p><p class="source-code">        # Update Tensorboard with Agent’s training status</p><p class="source-code">        agent.log_status(summary_writer, epoch, reward)</p><p class="source-code">        summary_writer.flush()</p></li>
				<li>That completes<a id="_idIndexMarker760"/> our training loop. It was simple, wasn't it? Let’s not forget to save the agent’s model after training so that we can use the trained model when it’s time for deployment:<p class="source-code">    agent.save_model(</p><p class="source-code">        f”sac_actor_final_episode_{env_id}.h5”, \</p><p class="source-code">        f”sac_critic_final_episode_{env_id}.h5”</p><p class="source-code">    )</p></li>
				<li>You can now proceed and run the script using the following command:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python 3_training_rl_agents_using_remote_sims.py</strong></p></li>
				<li>Did we forget something? Which sim server is the client connecting to? Is the sim server running?! If you get a long error on the command line that ends with a line that looks like the following, it certainly means that the sim server is not running: <p class="source-code"><strong class="bold">Failed to establish a new connection: [Errno 111] Connection refused’))</strong></p></li>
				<li>Let’s do it correctly this time! Let’s make sure our sim server is running by launching the <strong class="source-inline">tradegym</strong> server using the following command:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python tradegym_http_server.py</strong></p></li>
				<li>We can now<a id="_idIndexMarker761"/> launch the agent training <a id="_idIndexMarker762"/>script using the following command (same as before):<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python 3_training_rl_agents_using_remote_sims.py</strong></p></li>
				<li>You should see an output similar to the following:<p class="source-code">...</p><p class="source-code">Total params: 16,257</p><p class="source-code">Trainable params: 16,257</p><p class="source-code">Non-trainable params: 0</p><p class="source-code">__________________________________________________________________________________________________</p><p class="source-code">None</p><p class="source-code">episode:1 cumulative_reward:370.45421418744525 steps:9 epochs:9</p><p class="source-code">episode:2 cumulative_reward:334.52956448599605 steps:9 epochs:18</p><p class="source-code">episode:3 cumulative_reward:375.27432450733943 steps:9 epochs:27</p><p class="source-code">episode:4 cumulative_reward:363.7160827166332 steps:9 epochs:36</p><p class="source-code">episode:5 cumulative_reward:363.2819222532322 steps:9 epochs:45</p><p class="source-code">...</p></li>
			</ol>
			<p>That completes our script for training RL agents using remote sims! </p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor208"/>How it works…</h2>
			<p>So far, we have been directly using the <strong class="source-inline">gym</strong> library to interact with the sim since we were running the RL environment simulators as part of the agent training scripts. While this will be good enough <a id="_idIndexMarker763"/>for CPU-bound local simulators, as<a id="_idIndexMarker764"/> we start to use advanced simulators or simulators that we don’t own, or even those cases where we don’t want to run or manage the simulator instances, we can leverage the client wrapper we built using our previous recipe in this chapter and talk to tradegym-like RL environments that expose a REST API for interfacing. In this recipe, the agent training script utilizes the <strong class="source-inline">tradegym</strong> client module to interact with a remote <strong class="source-inline">tradegym</strong> server to complete the RL training loop.</p>
			<p>With that, let’s move on to the next recipe to see how we can evaluate previously trained agents.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor209"/>Testing/evaluating RL agents</h1>
			<p>Let’s assume<a id="_idIndexMarker765"/> that you have<a id="_idIndexMarker766"/> trained the SAC agent in one of the trading environments using the training script (previous recipe) and that you have several versions of the trained agent models, each with different policy network architectures or hyperparameters or your own tweaks and customizations to improve its performance. When you want to deploy an agent, you want to make sure that you pick the best performing agent, don’t you?</p>
			<p>This recipe will help you build a lean script to evaluate a given pre-trained agent model locally so that you can get a quantitative performance assessment and compare several trained models before choosing the right agent model for deployment. Specifically, we will use the <strong class="source-inline">tradegym</strong> module and the <strong class="source-inline">sac_agent_runtime</strong> module that we built earlier in this chapter to evaluate the agent models that we train.</p>
			<p>Let’s get started!</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor210"/>Getting ready</h2>
			<p>To complete this <a id="_idIndexMarker767"/>recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the<a id="_idIndexMarker768"/> latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repository. If the following <strong class="source-inline">import</strong> statements run without issues, you are ready to get started:</p>
			<p class="source-code">#!/bin/env/python</p>
			<p class="source-code">import os</p>
			<p class="source-code">import sys</p>
			<p class="source-code">from argparse import ArgumentParser</p>
			<p class="source-code">import imageio</p>
			<p class="source-code">import gym</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor211"/>How to do it…</h2>
			<p>Let’s focus on creating a simple, but complete, agent evaluation script:</p>
			<ol>
				<li value="1">First, let’s import the <strong class="source-inline">tradegym</strong> module for the training environment and the SAC agent runtime:<p class="source-code">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</p><p class="source-code">import tradegym  # Register tradegym envs with OpenAI Gym registry</p><p class="source-code">from sac_agent_runtime import SAC</p></li>
				<li>Next, let’s create a command-line argument parser to handle command-line configurations:<p class="source-code">parser = ArgumentParser(prog=”TFRL-Cookbook-Ch7-Evaluating-RL-Agents”)</p><p class="source-code">parser.add_argument(“--agent”, default=”SAC”, help=”Name of Agent. Default=SAC”)</p></li>
				<li>Let’s now add support for the <strong class="source-inline">--env</strong> argument to specify the RL environment ID and <strong class="source-inline">–-num-episodes</strong> to specify the number of episodes for evaluating the agent. Let’s have<a id="_idIndexMarker769"/> some sensible default values for both the arguments so that<a id="_idIndexMarker770"/> we can run the script even without any arguments for quick (or lazy?!) testing:<p class="source-code">parser.add_argument(</p><p class="source-code">    “--env”,</p><p class="source-code">    default=”StockTradingContinuousEnv-v0”,</p><p class="source-code">    help=”Name of Gym env. Default=StockTradingContinuousEnv-v0”,</p><p class="source-code">)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    “--num-episodes”,</p><p class="source-code">    default=10,</p><p class="source-code">    help=”Number of episodes to evaluate the agent.\</p><p class="source-code">          Default=100”,</p><p class="source-code">)</p></li>
				<li>Let’s also add support for <strong class="source-inline">–-trained-models-dir</strong> to specify the directory containing the trained models, and the <strong class="source-inline">–-model-version</strong> flag to specify the specific version of the model in that trained directory:<p class="source-code">parser.add_argument(</p><p class="source-code">    “--trained-models-dir”,</p><p class="source-code">    default=”trained_models”,</p><p class="source-code">    help=”Directory contained trained models. Default=trained_models”,</p><p class="source-code">)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    “--model-version”,</p><p class="source-code">    default=”episode100”,</p><p class="source-code">    help=”Trained model version. Default=episode100”,</p><p class="source-code">)</p></li>
				<li>We are now ready to finalize the argument parsing:<p class="source-code">args = parser.parse_args()</p></li>
				<li>Let’s begin our<a id="_idIndexMarker771"/> implementation of the <strong class="source-inline">__main__</strong> method in this step<a id="_idIndexMarker772"/> and continue its implementation in the following steps. Let’s start by creating a local instance of the RL environment in which we want to evaluate the agent:<p class="source-code">if __name__ == “__main__”:</p><p class="source-code">    # Create an instance of the evaluation environment</p><p class="source-code">    env = gym.make(args.env)</p></li>
				<li>Let’s now initialize the agent class. For now, we only support the SAC agent, but it is quite easy to add support for other agents we have discussed in this book:<p class="source-code">    if args.agent != “SAC”:</p><p class="source-code">        print(f”Unsupported Agent: {args.agent}. Using \</p><p class="source-code">                SAC Agent”)</p><p class="source-code">        args.agent = “SAC”</p><p class="source-code">    # Create an instance of the Soft Actor-Critic Agent</p><p class="source-code">    agent = SAC(env.observation_space.shape, \</p><p class="source-code">                env.action_space)</p></li>
				<li>Next, let’s load <a id="_idIndexMarker773"/>the trained <a id="_idIndexMarker774"/>agent models:<p class="source-code">    # Load trained Agent model/brain</p><p class="source-code">    model_version = args.model_version</p><p class="source-code">    agent.load_actor(</p><p class="source-code">        os.path.join(args.trained_models_dir, \</p><p class="source-code">                     f”sac_actor_{model_version}.h5”)</p><p class="source-code">    )</p><p class="source-code">    agent.load_critic(</p><p class="source-code">        os.path.join(args.trained_models_dir, \</p><p class="source-code">                     f”sac_critic_{model_version}.h5”)</p><p class="source-code">    )</p><p class="source-code">    print(f”Loaded {args.agent} agent with trained \</p><p class="source-code">            model version:{model_version}”)</p></li>
				<li>We are now ready to evaluate the agent using the trained models in the test environment:<p class="source-code">    # Evaluate/Test/Rollout Agent with trained model/</p><p class="source-code">    # brain</p><p class="source-code">    video = imageio.get_writer(“agent_eval_video.mp4”,\</p><p class="source-code">                                fps=30)</p><p class="source-code">    avg_reward = 0</p><p class="source-code">    for i in range(args.num_episodes):</p><p class="source-code">        cur_state, done, rewards = env.reset(), False, 0</p><p class="source-code">        while not done:</p><p class="source-code">            action = agent.act(cur_state, test=True)</p><p class="source-code">            next_state, reward, done, _ = \</p><p class="source-code">                                env.step(action[0])</p><p class="source-code">            cur_state = next_state</p><p class="source-code">            rewards += reward</p><p class="source-code">            if render:</p><p class="source-code">                video.append_data(env.render(mode=\</p><p class="source-code">                                            ”rgb_array”))</p><p class="source-code">        print(f”Episode#:{i} cumulative_reward:\</p><p class="source-code">                {rewards}”)</p><p class="source-code">        avg_reward += rewards</p><p class="source-code">    avg_reward /= args.num_episodes</p><p class="source-code">    video.close()</p><p class="source-code">    print(f”Average rewards over {args.num_episodes} \</p><p class="source-code">            episodes: {avg_reward}”)</p></li>
				<li>Let’s now <a id="_idIndexMarker775"/>try to evaluate the agent in the <strong class="source-inline">StockTradingContinuous-v0</strong> environment. Note that the market data source for the stocks <a id="_idIndexMarker776"/>trading environment in <strong class="source-inline">data/MSFT.csv</strong> and <strong class="source-inline">data/TSLA.csv</strong> can be different from the market data used for training! After all, we want to evaluate how well the agent has learned to trade! Run the following command to launch the agent evaluation script:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$ python 4_evaluating_rl_agents.py</strong></p></li>
				<li>Depending<a id="_idIndexMarker777"/> on how well your trained agents are, you will see output <a id="_idIndexMarker778"/>on the console similar to the following (reward values will vary):<p class="source-code">...</p><p class="source-code">==================================================================================================</p><p class="source-code">Total params: 16,257</p><p class="source-code">Trainable params: 16,257</p><p class="source-code">Non-trainable params: 0</p><p class="source-code">__________________________________________________________________________________________________</p><p class="source-code">None</p><p class="source-code">Loaded SAC agent with trained model version:episode100</p><p class="source-code">Episode#:0 cumulative_reward:382.5117154452246</p><p class="source-code">Episode#:1 cumulative_reward:359.27720004181674</p><p class="source-code">Episode#:2 cumulative_reward:370.92829808499664</p><p class="source-code">Episode#:3 cumulative_reward:341.44002189086007</p><p class="source-code">Episode#:4 cumulative_reward:364.32631211784394</p><p class="source-code">Episode#:5 cumulative_reward:385.89219327764476</p><p class="source-code">Episode#:6 cumulative_reward:365.2120387185878</p><p class="source-code">Episode#:7 cumulative_reward:339.98494537310785</p><p class="source-code">Episode#:8 cumulative_reward:362.7133769241483</p><p class="source-code">Episode#:9 cumulative_reward:379.12388043270073</p><p class="source-code">Average rewards over 10 episodes: 365.1409982306931</p><p class="source-code">...</p></li>
			</ol>
			<p>That’s it!</p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor212"/>How it works…</h2>
			<p>We initialized an SAC <a id="_idIndexMarker779"/>agent with only the runtime components that are required to evaluate the agent using the <strong class="source-inline">sac_agent_runtime</strong> module and then loaded previously trained model versions (for both the actor and the critic), which are both customizable using the<a id="_idIndexMarker780"/> command-line arguments. We then created a local instance of the <strong class="source-inline">StockTradingContinuousEnv-v0</strong> environment using the <strong class="source-inline">tradegym</strong> library and evaluated our agents to get the cumulative reward as one of the quantitative indicators of the performance of the trained agent models.</p>
			<p>Now that we know how to evaluate and pick the best performing agent, let’s move on to the next recipe to understand how to package the trained agent for deployment!</p>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor213"/>Packaging RL agents for deployment – a trading bot</h1>
			<p>This is one of <a id="_idIndexMarker781"/>the crucial recipes of this chapter, where we will be <a id="_idIndexMarker782"/>discussing how to package the agent so that we can deploy on the cloud (next recipe!) as a service. We will be implementing a script that takes our trained agent models and exposes the <strong class="source-inline">act</strong> method as a RESTful service. We will then package the agent and the API script into a <strong class="bold">Docker</strong> container that <a id="_idIndexMarker783"/>is ready to be deployed to the cloud! By the end of this recipe, you will have built a deployment-ready Docker container with your trained RL agent that is ready to create and offer your Agent/Bot-as-a-Service!</p>
			<p>Let’s jump into the details.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor214"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment to match the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in the cookbook’s code repository. If the following <strong class="source-inline">import</strong> statements run without issues, you are ready to undertake the next step, which is to set up Docker:</p>
			<p class="source-code">import os</p>
			<p class="source-code">import sys</p>
			<p class="source-code">from argparse import ArgumentParser</p>
			<p class="source-code">import gym.spaces</p>
			<p class="source-code">from flask import Flask, request</p>
			<p class="source-code">import numpy as np</p>
			<p>For this recipe, you will need Docker installed. Please follow the official setup instructions to install Docker for your platform. You can find the instructions at <a href="https://docs.docker.com/get-docker/">https://docs.docker.com/get-docker/</a>.</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor215"/>How to do it…</h2>
			<p>We will first<a id="_idIndexMarker784"/> implement the script to expose the agent’s <strong class="source-inline">act</strong> method as <a id="_idIndexMarker785"/>a REST service and then move on to create the Dockerfile for containerizing the agent:</p>
			<ol>
				<li value="1">First, let’s import the <strong class="source-inline">sac_agent_runtime</strong> that we built earlier in this chapter:<p class="source-code">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</p><p class="source-code">from sac_agent_runtime import SAC</p></li>
				<li>Next, let’s create a handler for the command-line argument and <strong class="source-inline">–-agent</strong> as the first supported argument to allow specification of the agent algorithm we want to use:<p class="source-code">parser = ArgumentParser(</p><p class="source-code">    prog=”TFRL-Cookbook-Ch7-Packaging-RL-Agents-For-Cloud-Deployments”</p><p class="source-code">)</p><p class="source-code">parser.add_argument(“--agent”, default=”SAC”, help=”Name of Agent. Default=SAC”)</p></li>
				<li>Next, let’s add arguments to allow specification of the IP address and the port of the host server where our agent will be deployed. We will set and use the defaults for now and can change them from the command line when we need to:<p class="source-code">parser.add_argument(</p><p class="source-code">    “--host-ip”,</p><p class="source-code">    default=”0.0.0.0”,</p><p class="source-code">    help=”IP Address of the host server where Agent  </p><p class="source-code">          service is run. Default=127.0.0.1”,</p><p class="source-code">)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    “--host-port”,</p><p class="source-code">    default=”5555”,</p><p class="source-code">    help=”Port on the host server to use for Agent </p><p class="source-code">          service. Default=5555”,</p><p class="source-code">)</p></li>
				<li>Next, let’s add <a id="_idIndexMarker786"/>support for arguments to specify the directory <a id="_idIndexMarker787"/>containing the trained agent models and the specific model version to use:<p class="source-code">parser.add_argument(</p><p class="source-code">    “--trained-models-dir”,</p><p class="source-code">    default=”trained_models”,</p><p class="source-code">    help=”Directory contained trained models. \</p><p class="source-code">          Default=trained_models”,</p><p class="source-code">)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    “--model-version”,</p><p class="source-code">    default=”episode100”,</p><p class="source-code">    help=”Trained model version. Default=episode100”,</p><p class="source-code">)</p></li>
				<li>As the final set <a id="_idIndexMarker788"/>of supported arguments, let’s add arguments to allow specification of the observation shape and the action space specifications<a id="_idIndexMarker789"/> based on the trained model configuration:<p class="source-code">parser.add_argument(</p><p class="source-code">    “--observation-shape”,</p><p class="source-code">    default=(6, 31),</p><p class="source-code">    help=”Shape of observations. Default=(6, 31)”,</p><p class="source-code">)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    “--action-space-low”, default=[-1], help=”Low value \</p><p class="source-code">     of action space. Default=[-1]”</p><p class="source-code">)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    “--action-space-high”, default=[1], help=”High value\</p><p class="source-code">     of action space. Default=[1]”</p><p class="source-code">)</p><p class="source-code">parser.add_argument(</p><p class="source-code">    “--action-shape”, default=(1,), help=”Shape of \</p><p class="source-code">    actions. Default=(1,)”</p><p class="source-code">)</p></li>
				<li>We can now finalize the argument parser and start the implementation of the <strong class="source-inline">__main__</strong> function:<p class="source-code">args = parser.parse_args()</p><p class="source-code">if __name__ == “__main__”:</p></li>
				<li>First, let’s<a id="_idIndexMarker790"/> load the runtime <a id="_idIndexMarker791"/>configurations for the agent:<p class="source-code">    if args.agent != “SAC”:</p><p class="source-code">        print(f”Unsupported Agent: {args.agent}. Using \</p><p class="source-code">                SAC Agent”)</p><p class="source-code">        args.agent = “SAC”</p><p class="source-code">    # Set Agent’s runtime configs</p><p class="source-code">    observation_shape = args.observation_shape</p><p class="source-code">    action_space = gym.spaces.Box(</p><p class="source-code">        np.array(args.action_space_low),</p><p class="source-code">        np.array(args.action_space_high),</p><p class="source-code">        args.action_shape,</p><p class="source-code">    )</p></li>
				<li>Next, let’s create an instance of the agent and load the weights for the agent’s actor and critic networks from the pre-trained model:<p class="source-code">    # Create an instance of the Agent</p><p class="source-code">    agent = SAC(observation_shape, action_space)</p><p class="source-code">    # Load trained Agent model/brain</p><p class="source-code">    model_version = args.model_version</p><p class="source-code">    agent.load_actor(</p><p class="source-code">        os.path.join(args.trained_models_dir, \</p><p class="source-code">                     f”sac_actor_{model_version}.h5”)</p><p class="source-code">    )</p><p class="source-code">    agent.load_critic(</p><p class="source-code">        os.path.join(args.trained_models_dir, \</p><p class="source-code">                     f”sac_critic_{model_version}.h5”)</p><p class="source-code">    )</p><p class="source-code">    print(f”Loaded {args.agent} agent with trained model\</p><p class="source-code">             version:{model_version}”)</p></li>
				<li>We can now set up the service endpoint using Flask, which is going to be as simple as shown in the <a id="_idIndexMarker792"/>following lines of code. Note that we are <a id="_idIndexMarker793"/>exposing the agent’s <strong class="source-inline">act</strong> method at the <strong class="source-inline">/v1/act</strong> endpoint:<p class="source-code">    # Setup Agent (http) service</p><p class="source-code">    app = Flask(__name__)</p><p class="source-code">    @app.route(“/v1/act”, methods=[“POST”])</p><p class="source-code">    def get_action():</p><p class="source-code">        data = request.get_json()</p><p class="source-code">        action = agent.act(np.array(data.get(</p><p class="source-code">                           “observation”)), test=True)</p><p class="source-code">        return {“action”: action.numpy().tolist()}</p></li>
				<li>Finally, we just have to add the line that launches the Flask application to start the service when executed:<p class="source-code">    # Launch/Run the Agent (http) service</p><p class="source-code">    app.run(host=args.host_ip, port=args.host_port, </p><p class="source-code">            debug=True)</p></li>
				<li>Our REST API implementation for the agent is ready. We can now focus on creating a Docker container for the agent service. We will start to implement the Dockerfile by specifying the base image to be <strong class="source-inline">nvidia/cuda:*</strong> so that we have the GPU drivers that are necessary to utilize the GPU on the server where we will be deploying the agent. The following code lines in this and the following steps will go into a file named <strong class="source-inline">Dockerfile</strong>:<p class="source-code">FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04</p><p class="source-code"># TensorFlow2.x Reinforcement Learning Cookbook</p><p class="source-code"># Chapter 7: Deploying Deep RL Agents to the cloud</p><p class="source-code">LABEL maintainer=”emailid@domain.tld”</p></li>
				<li>Let’s now install <a id="_idIndexMarker794"/>a few system-level packages that are <a id="_idIndexMarker795"/>necessary and clean up the files to save disk space: <p class="source-code">RUN apt-get install -y wget git make cmake zlib1g-dev &amp;&amp; rm -rf /var/lib/apt/lists/*</p></li>
				<li>To execute our agent runtime with all the necessary packages installed, we will make use of the conda Python environment. So, let’s go ahead and go through the instructions to download and set up <strong class="source-inline">miniconda</strong> in the container:<p class="source-code">ENV PATH=”/root/miniconda3/bin:${PATH}”</p><p class="source-code">ARG PATH=”/root/miniconda3/bin:${PATH}”</p><p class="source-code">RUN apt-get update</p><p class="source-code">RUN wget \</p><p class="source-code">    https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh \</p><p class="source-code">    &amp;&amp; mkdir /root/.conda \</p><p class="source-code">    &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b \</p><p class="source-code">    &amp;&amp; rm -f Miniconda3-latest-Linux-x86_64.sh</p><p class="source-code"># conda&gt;4.9.0 is required for `--no-capture-output`</p><p class="source-code">RUN conda update -n base conda</p></li>
				<li>Let’s now copy the source code of this chapter into the container and create the conda environment using the list of packages specified in the <strong class="source-inline">tfrl-cookbook.yml</strong> file:<p class="source-code">ADD . /root/tf-rl-cookbook/ch7</p><p class="source-code">WORKDIR /root/tf-rl-cookbook/ch7</p><p class="source-code">RUN conda env create -f “tfrl-cookbook.yml” -n “tfrl-cookbook”</p></li>
				<li>Finally, we just have to set the <strong class="source-inline">ENTRYPOINT</strong> for the container and the <strong class="source-inline">CMD</strong>, which will be passed as arguments to the <strong class="source-inline">ENTRYPOINT</strong> when the container is launched:<p class="source-code">ENTRYPOINT [ “conda”, “run”, “--no-capture-output”, “-n”, “tfrl-cookbook”, “python” ]</p><p class="source-code">CMD [ “5_packaging_rl_agents_for_deployment.py” ]</p></li>
				<li>That completes our<a id="_idIndexMarker796"/> Dockerfile and we are now ready to <a id="_idIndexMarker797"/>package our agent by building the Docker container. You can run the following command to build the Docker container as per the instructions in the Dockerfile and tag it with a container image name of your choice. Let’s use the following command:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$docker build -f Dockerfile -t tfrl-cookbook/ch7-trading-bot:latest</strong></p></li>
				<li>If you are running the preceding command for the first time, it may take quite some time for Docker to build the container. Subsequent runs or updates will run faster as intermediate layers might already have been cached when run for the first time. When things are running smoothly, you will see an output similar to the following (note that most of the layers are cached since I had already built the container previously):<p class="source-code"><strong class="bold">Sending build context to Docker daemon  1.793MB</strong></p><p class="source-code"><strong class="bold">Step 1/13 : FROM nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04</strong></p><p class="source-code"><strong class="bold"> ---&gt; a3bd8cb789b0</strong></p><p class="source-code"><strong class="bold">Step 2/13 : LABEL maintainer=”emailid@domain.tld”</strong></p><p class="source-code"><strong class="bold"> ---&gt; Using cache</strong></p><p class="source-code"><strong class="bold"> ---&gt; 4322623c24c8</strong></p><p class="source-code"><strong class="bold">Step 3/13 : ENV PATH=”/root/miniconda3/bin:${PATH}”</strong></p><p class="source-code"><strong class="bold"> ---&gt; Using cache</strong></p><p class="source-code"><strong class="bold"> ---&gt; e9e8c882662a</strong></p><p class="source-code"><strong class="bold">Step 4/13 : ARG PATH=”/root/miniconda3/bin:${PATH}”</strong></p><p class="source-code"><strong class="bold"> ---&gt; Using cache</strong></p><p class="source-code"><strong class="bold"> ---&gt; 31d45d5bcb05</strong></p><p class="source-code"><strong class="bold">Step 5/13 : RUN apt-get update</strong></p><p class="source-code"><strong class="bold"> ---&gt; Using cache</strong></p><p class="source-code"><strong class="bold"> ---&gt; 3f7ed3eb3c76</strong></p><p class="source-code"><strong class="bold">Step 6/13 : RUN apt-get install -y wget git make cmake zlib1g-dev &amp;&amp; rm -rf /var/lib/apt/lists/*</strong></p><p class="source-code"><strong class="bold"> ---&gt; Using cache</strong></p><p class="source-code"><strong class="bold"> ---&gt; 0ffb6752f5f6</strong></p><p class="source-code"><strong class="bold">Step 7/13 : RUN wget     https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh     &amp;&amp; mkdir /root/.conda     &amp;&amp; bash Miniconda3-latest-Linux-x86_64.sh -b     &amp;&amp; rm -f Miniconda3-latest-Linux-x86_64.sh</strong></p><p class="source-code"><strong class="bold"> ---&gt; Using cache</strong></p></li>
				<li>For the layers that involve<a id="_idIndexMarker798"/> COPY/ADD file operations from the disk, the instructions will be run as they cannot be cached. For example, you<a id="_idIndexMarker799"/> will see that the following steps from <em class="italic">step 9</em> will continue to be run fresh without using any cache. This is normal even if you have already built the container:<p class="source-code"><strong class="bold">Step 9/13 : ADD . /root/tf-rl-cookbook/ch7</strong></p><p class="source-code"><strong class="bold"> ---&gt; ed8541c42ebc</strong></p><p class="source-code"><strong class="bold">Step 10/13 : WORKDIR /root/tf-rl-cookbook/ch7</strong></p><p class="source-code"><strong class="bold"> ---&gt; Running in f5a9c6ad485c</strong></p><p class="source-code"><strong class="bold">Removing intermediate container f5a9c6ad485c</strong></p><p class="source-code"><strong class="bold"> ---&gt; 695ca00c6db3</strong></p><p class="source-code"><strong class="bold">Step 11/13 : RUN conda env create -f “tfrl-cookbook.yml” -n “tfrl-cookbook”</strong></p><p class="source-code"><strong class="bold"> ---&gt; Running in b2a9706721e7</strong></p><p class="source-code"><strong class="bold">Collecting package metadata (repodata.json): ...working... done</strong></p><p class="source-code"><strong class="bold">Solving environment: ...working... done...</strong></p></li>
				<li>Finally, when the Docker container is built, you will see something like the following message:<p class="source-code"><strong class="bold">Step 13/13 : CMD [ “2_packaging_rl_agents_for_deployment.py” ]</strong></p><p class="source-code"><strong class="bold"> ---&gt; Running in 336e442b0218</strong></p><p class="source-code"><strong class="bold">Removing intermediate container 336e442b0218</strong></p><p class="source-code"><strong class="bold"> ---&gt; cc1caea406e6</strong></p><p class="source-code"><strong class="bold">Successfully built cc1caea406e6</strong></p><p class="source-code"><strong class="bold">Successfully tagged tfrl-cookbook/ch7:latest</strong></p></li>
			</ol>
			<p>Congratulations on successfully packing your RL agent for deployment!</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor216"/>How it works…</h2>
			<p>We leveraged the <strong class="source-inline">sac_agent_runtime</strong> we built earlier in this chapter to create and initialize an SAC agent instance. We then loaded pre-trained agent models for both the actor and the critic. After that, we exposed the SAC agent’s <strong class="source-inline">act</strong> method as a REST API with an HTTP POST endpoint for <a id="_idIndexMarker800"/>taking the observations as the POST message and returning actions as the response to the POST request. Finally, we launched the script as a Flask application to start <a id="_idIndexMarker801"/>serving. </p>
			<p>In the second part of the recipe, we packaged the agent actioa-serving application as a Docker container and prepared it for deployment!</p>
			<p>We are now on the verge of deploying the agent to the cloud! March on to the next recipe and find out how.</p>
			<h1 id="_idParaDest-188"><a id="_idTextAnchor217"/>Deploying RL agents to the cloud – a trading Bot-as-a-Service</h1>
			<p>The ultimate goal of<a id="_idIndexMarker802"/> training an RL agent is to use it for taking actions given<a id="_idIndexMarker803"/> new observations. In the case of our stock trading SAC agent, we have so far learned to train, evaluate, and package the best performing agent model to build our trading bot. While we focused on one particular application (autonomous trading bot), you can see how easy it is to change the training environment or agent algorithms based on the recipes in earlier chapters of this book. This recipe will walk you through the steps to deploy the Docker containerized/packaged RL agent to the cloud and run the Bot-as-a-Service.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor218"/>Getting ready</h2>
			<p>To complete this recipe, you will need access to a cloud service such as Azure, AWS, GCP, Heroku or another cloud service provider that allows you to host and run your Docker container. If you are a student, you can make use of GitHub’s student developer pack (<a href="https://education.github.com/pack">https://education.github.com/pack</a>), which, as of 2020, allows you to avail yourself of several benefits for free, including $100 worth of Microsoft Azure credits or $50 platform credit on DigitalOcean if you are a new user. </p>
			<p>Several guides exist on how to push your Docker container to the cloud and to deploy/run the container as a service. For example, if you have an Azure account, you can following the official guide here: <a href="https://docs.microsoft.com/en-us/azure/container-instances/container-instances-quickstart">https://docs.microsoft.com/en-us/azure/container-instances/container-instances-quickstart</a>.</p>
			<p>The guide walks you through the various options (the CLI, Portal, PowerShell, ARM templates, and the Docker CLI) to deploy your Docker container-based agent service.</p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor219"/>How to do it…</h2>
			<p>We will first deploy the<a id="_idIndexMarker804"/> trading bot locally and test it out. After that, we can <a id="_idIndexMarker805"/>deploy it to your cloud service of choice. As an example, this recipe will walk you through the steps to deploy it to Heroku (<a href="https://heroku.com">https://heroku.com</a>). </p>
			<p>Let’s begin:</p>
			<ol>
				<li value="1">Let’s first build our Docker container containing our trading bot using the following command. Note that if you have already built the container by following the previous recipe in this chapter, the following command may finish executing faster depending on the cached layers and your changes to the Dockerfile:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$docker build -f Dockerfile -t tfrl-cookbook/ch7-trading-bot:latest</strong></p></li>
				<li>Once the Docker container with the bot has been built successfully, we can launch the bot using the following command:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$docker run -it -p 5555:5555 tfrl-cookbook/ch7-trading-bot</strong></p></li>
				<li>If all goes well, you should see console output somewhat similar to the following, indicating that the bot is up and running and ready to act:<p class="source-code"><strong class="bold">...==================================================================================================</strong></p><p class="source-code"><strong class="bold">Total params: 16,257</strong></p><p class="source-code"><strong class="bold">Trainable params: 16,257</strong></p><p class="source-code"><strong class="bold">Non-trainable params: 0</strong></p><p class="source-code"><strong class="bold">__________________________________________________________________________________________________</strong></p><p class="source-code"><strong class="bold">None</strong></p><p class="source-code"><strong class="bold">Loaded SAC agent with trained model version:episode100</strong></p><p class="source-code"><strong class="bold"> * Debugger is active!</strong></p><p class="source-code"><strong class="bold"> * Debugger PIN: 604-104-903</strong></p><p class="source-code"><strong class="bold">...</strong></p></li>
				<li>Now that you have <a id="_idIndexMarker806"/>deployed the trading bot locally (on your own server), let’s create a simple script to utilize the Bot-as-a-Service that you have<a id="_idIndexMarker807"/> built. Create a file named <strong class="source-inline">test_agent_service.py</strong> with the following content: <p class="source-code">#Simple test script for the deployed Trading Bot-as-a-Service</p><p class="source-code">import os</p><p class="source-code">import sys</p><p class="source-code">import gym</p><p class="source-code">import requests</p><p class="source-code">sys.path.append(os.path.dirname(os.path.abspath(__file__)))</p><p class="source-code">import tradegym  # Register tradegym envs with OpenAI Gym # registry</p><p class="source-code">host_ip = “127.0.0.1”</p><p class="source-code">host_port = 5555</p><p class="source-code">endpoint = “v1/act”</p><p class="source-code">env = gym.make(“StockTradingContinuousEnv-v0”)</p><p class="source-code">post_data = {“observation”: env.observation_space.sample().tolist()}</p><p class="source-code">res = requests.post(f”http://{host_ip}:{host_port}/{endpoint}”, json=post_data)</p><p class="source-code">if res.ok:</p><p class="source-code">    print(f”Received Agent action:{res.json()}”)</p></li>
				<li>You can execute the script using the following command:<p class="source-code"><strong class="bold">(tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$python test_agent_service.py</strong></p></li>
				<li>Note that your bot<a id="_idIndexMarker808"/> container still needs to be running. Once you<a id="_idIndexMarker809"/> execute the preceding command, you will see an output similar to the following line on the console output of your bot, indicating that a new POST message was received at the <strong class="source-inline">/v1/act</strong> endpoint, which was served with an HTTP response status of 200, indicating success:<p class="source-code">172.17.0.1 - - [00/Mmm/YYYY hh:mm:ss] “POST /v1/act HTTP/1.1” 200 -</p></li>
				<li>You will also notice that your test script will print out an output similar to the following on its console window, indicating that it received an action from the trading bot:<p class="source-code">Received Agent action:{‘action’: [[0.008385116065491426]]}</p></li>
				<li>It’s time to deploy your trading bot to a cloud platform so that you or others can access it over the internet! As discussed in the <em class="italic">Getting started</em> section, you have several options in terms of choosing the cloud provider where you want to host your Docker container image and deploy the RL agent Bot-as-a-Service. We will use Heroku as an example as it offers free hosting and a simple CLI. First, you need to install the Heroku CLI. Follow the official instructions listed at <a href="https://devcenter.heroku.com/articles/heroku-cli">https://devcenter.heroku.com/articles/heroku-cli</a> to install the Heroku CLI for your platform (Linux/Windows/macOS X). On Ubuntu Linux, we can use the following command:<p class="source-code"><strong class="bold">sudo snap install --classic heroku</strong></p></li>
				<li>Once the Heroku <a id="_idIndexMarker810"/>CLI is installed, you can log in to the Heroku<a id="_idIndexMarker811"/> container registry using the following command:<p class="source-code"><strong class="bold">heroku container:login</strong></p></li>
				<li>Next, run the following command from the directory containing the Dockerfile for the agent; for example:<p class="source-code"><strong class="bold">tfrl-cookbook)praveen@desktop:~/tensorflow2-reinforcement-learning-cookbook/src/ch7-cloud-deploy-deep-rl-agents$heroku create</strong></p></li>
				<li>If you have not logged in to Heroku, you will be prompted to log in: <p class="source-code"><strong class="bold">Creating app... !</strong></p><p class="source-code"><strong class="bold">     Invalid credentials provided.</strong></p><p class="source-code"><strong class="bold"> ›   Warning: heroku update available from 7.46.2 to </strong></p><p class="source-code"><strong class="bold">7.47.0.</strong></p><p class="source-code"><strong class="bold">heroku: Press any key to open up the browser to login or q to exit:</strong></p></li>
				<li>Once you are logged in, you will get an output similar to the following:<p class="source-code"><strong class="bold">Creating salty-fortress-4191... done, stack is heroku-18</strong></p><p class="source-code"><strong class="bold">https://salty-fortress-4191.herokuapp.com/ | https://git.heroku.com/salty-fortress-4191.git</strong></p></li>
				<li>That’s the address of your container registry on Heroku. You can now build your bot container and push it to Heroku using the following command:<p class="source-code"><strong class="bold">heroku container:push web</strong></p></li>
				<li>Once that process completes, you can release the bot container image to your Heroku app using the following command:<p class="source-code"><strong class="bold">heroku container:release web</strong></p></li>
				<li>Congratulations! You have just deployed your bot to the cloud! You can now access your bot at the new address, such as <a href="https://salty-fortress-4191.herokuapp.com/">https://salty-fortress-4191.herokuapp.com/</a> in the sample used in the preceding code. You should be able to send observations to your bot and get the actions in return! Congratulations on deploying your Bot-as-a-Service!</li>
			</ol>
			<p>We are now ready to<a id="_idIndexMarker812"/> wrap up the chapter.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor220"/>How it works…</h2>
			<p>We first built and launched<a id="_idIndexMarker813"/> the Docker container locally on your machine by using the <strong class="source-inline">docker run</strong> command and specifying that you want the local port <strong class="source-inline">5555</strong> to be mapped with the container’s port <strong class="source-inline">5555</strong>. This will allow the host (your machine) to communicate with your container using that port as if it were a local port on the machine. Following deployment, we used a test script that uses the Python <strong class="source-inline">request</strong> library to create a POST request with sample data for the observation values and sent it to the bot running in the container. We observed how the bot responded to the request via the command-line status output followed by a returned success response containing the bot’s trading action. </p>
			<p>We then deployed the same container with the bot to the cloud (Heroku). Following a successful deployment, the bot was accessible over the web using the public <strong class="source-inline">herokuapp</strong> URL created automatically by Heroku.</p>
			<p>That completes the recipe and the chapter! I hope you enjoyed working through it. See you in the next chapter.</p>
		</div>
</body></html>