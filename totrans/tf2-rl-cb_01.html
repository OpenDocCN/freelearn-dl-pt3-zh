<html><head></head><body>
		<div id="_idContainer024">
			<h1 id="_idParaDest-16"><em class="italic"><a id="_idTextAnchor015"/>Chapter 1</em>: Developing Building Blocks for Deep Reinforcement Learning Using Tensorflow 2.x</h1>
			<p>This chapter provides a practical and concrete description of the fundamentals of <strong class="bold">Deep Reinforcement Learning</strong> (<strong class="bold">Deep RL</strong>) filled with recipes for implementing the building blocks using the latest major version of <strong class="bold">TensorFlow 2.x</strong>. It includes recipes for getting started with RL environments, <strong class="bold">OpenAI Gym</strong>, developing neural network-based agents, and evolutionary neural agents for addressing applications with both discrete and continuous value spaces for Deep RL.</p>
			<p>The following recipes are discussed in this chapter:</p>
			<ul>
				<li>Building an environment and reward mechanism for training RL agents</li>
				<li>Implementing neural network-based RL policies for discrete action spaces and decision-making problems</li>
				<li>Implementing neural network-based RL policies for continuous action spaces and continuous-control problems</li>
				<li>Working with OpenAI Gym for RL training environments</li>
				<li>Building a neural agent </li>
				<li>Building a neural evolutionary agent</li>
			</ul>
			<h1 id="_idParaDest-17"><a id="_idTextAnchor016"/>Technical requirements</h1>
			<p>The code in the book has been extensively tested on Ubuntu 18.04 and Ubuntu 20.04 and should work with later versions of Ubuntu as long as Python 3.6+ is available. With Python 3.6 installed along with the necessary Python packages as listed before the start of each of the recipes, the code should run fine on Windows and macOS X too. It is advised to create and use a Python virtual environment named <strong class="source-inline">tf2rl-cookbook</strong> to install the packages and run the code in this book. Miniconda or Anaconda installation for Python virtual environment management is recommended.The complete code for each recipe in this chapter will be available here: <a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook</a>.</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>Building an environment and reward mechanism for training RL agents</h1>
			<p>This recipe will walk you<a id="_idIndexMarker000"/> through the steps to build a <strong class="bold">Gridworld</strong> learning environment to train RL agents. Gridworld is a simple environment where the world is represented as a grid. Each location on the grid can <a id="_idIndexMarker001"/>be referred to as a cell. The goal of an agent in this environment is to find its way to the goal state in a grid like the one shown here:</p>
			<div>
				<div id="_idContainer005" class="IMG---Figure">
					<img src="image/B15074_01_001.jpg" alt="Figure 1.1 – A screenshot of the Gridworld environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – A screenshot of the Gridworld environment</p>
			<p>The agent's location is represented by the blue cell in the grid, while the goal and a mine/bomb/obstacle's location is represented in the grid using green and red cells, respectively. The agent (blue cell) needs to find its way through the grid to reach the goal (green cell) without running over the mine/bomb (red cell).</p>
			<h2 id="_idParaDest-19"><a id="_idTextAnchor018"/>Getting ready</h2>
			<p>To complete this recipe, you will first need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/Conda virtual environment and <strong class="source-inline">pip install numpy gym</strong>. If the following import statements run without issues, you are ready to get started!</p>
			<p class="source-code">import copy</p>
			<p class="source-code">import sys</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p>Now we can begin.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>How to do it…</h2>
			<p>To train RL agents, we need a learning environment that is akin to the datasets used in supervised learning. The learning environment is a simulator that provides the <a id="_idIndexMarker002"/>observation for the RL<a id="_idIndexMarker003"/> agent, supports a set of actions that the RL agent can perform by executing the actions, and returns the resultant/new observation as a result of the agent taking the action.</p>
			<p>Perform the following steps to implement a Gridworld learning environment that represents a simple 2D map with colored cells representing the location of the agent, goal, mine/bomb/obstacle, wall, and empty space on a grid:</p>
			<ol>
				<li>We'll start by first defining the mapping between different cell states and their color codes to be used in the Gridworld environment:<p class="source-code">EMPTY = BLACK = 0</p><p class="source-code">WALL = GRAY = 1</p><p class="source-code">AGENT = BLUE = 2</p><p class="source-code">MINE = RED = 3</p><p class="source-code">GOAL = GREEN = 4</p><p class="source-code">SUCCESS = PINK = 5</p></li>
				<li>Next, generate a color map using RGB intensity values:<p class="source-code">COLOR_MAP = {</p><p class="source-code">    BLACK: [0.0, 0.0, 0.0],</p><p class="source-code">    GRAY: [0.5, 0.5, 0.5],</p><p class="source-code">    BLUE: [0.0, 0.0, 1.0],</p><p class="source-code">    RED: [1.0, 0.0, 0.0],</p><p class="source-code">    GREEN: [0.0, 1.0, 0.0],</p><p class="source-code">    PINK: [1.0, 0.0, 1.0],</p><p class="source-code">}</p></li>
				<li>Let's now define the <a id="_idIndexMarker004"/>action mapping:<p class="source-code">NOOP = 0</p><p class="source-code">DOWN = 1</p><p class="source-code">UP = 2</p><p class="source-code">LEFT = 3</p><p class="source-code">RIGHT = 4</p></li>
				<li>Let's then<a id="_idIndexMarker005"/> create a <strong class="source-inline">GridworldEnv</strong> class with an <strong class="source-inline">__init__</strong> function to define necessary class variables, including the observation and action space:<p class="source-code">class GridworldEnv():</p><p class="source-code">	def __init__(self):</p><p>We will implement <strong class="source-inline">__init__()</strong> in the following steps.</p></li>
				<li>In this step, let's define the layout of the Gridworld environment using the grid cell state mapping:<p class="source-code">	self.grid_layout = """</p><p class="source-code">        1 1 1 1 1 1 1 1</p><p class="source-code">        1 2 0 0 0 0 0 1</p><p class="source-code">        1 0 1 1 1 0 0 1</p><p class="source-code">        1 0 1 0 1 0 0 1</p><p class="source-code">        1 0 1 4 1 0 0 1</p><p class="source-code">        1 0 3 0 0 0 0 1</p><p class="source-code">        1 0 0 0 0 0 0 1</p><p class="source-code">        1 1 1 1 1 1 1 1</p><p class="source-code">        """</p><p>In the preceding layout, <strong class="source-inline">0</strong> corresponds to the empty cells, <strong class="source-inline">1</strong> corresponds to walls, <strong class="source-inline">2</strong> corresponds <a id="_idIndexMarker006"/>to the agent's starting location, <strong class="source-inline">3</strong> corresponds to the location of the<a id="_idIndexMarker007"/> mine/bomb/obstacle, and <strong class="source-inline">4</strong> corresponds to the goal location based on the mapping we defined in step 1.</p></li>
				<li>Now, we are ready to define the observation space for the Gridworld RL environment:<p class="source-code">	self.initial_grid_state = np.fromstring(</p><p class="source-code">                    self.grid_layout, dtype=int, sep=" ")</p><p class="source-code">	self.initial_grid_state = \</p><p class="source-code">                    self.initial_grid_state.reshape(8, 8)</p><p class="source-code">	self.grid_state = copy.deepcopy(</p><p class="source-code">                                 self.initial_grid_state)</p><p class="source-code">	self.observation_space = gym.spaces.Box(</p><p class="source-code">		low=0, high=6, shape=self.grid_state.shape</p><p class="source-code">	)</p><p class="source-code">	self.img_shape = [256, 256, 3]</p><p class="source-code">	self.metadata = {"render.modes": ["human"]}</p></li>
				<li>Let's define the action space and the mapping between the actions and the movement of the <a id="_idIndexMarker008"/>agent in the grid:<p class="source-code">	   self.action_space = gym.spaces.Discrete(5)</p><p class="source-code">        self.actions = [NOOP, UP, DOWN, LEFT, RIGHT]</p><p class="source-code">        self.action_pos_dict = {</p><p class="source-code">            NOOP: [0, 0],</p><p class="source-code">            UP: [-1, 0],</p><p class="source-code">            DOWN: [1, 0],</p><p class="source-code">            LEFT: [0, -1],</p><p class="source-code">            RIGHT: [0, 1],</p><p class="source-code">        }</p></li>
				<li>Let's now wrap <a id="_idIndexMarker009"/>up the <strong class="source-inline">__init__</strong> function by initializing the agent's start and goal states using the <strong class="source-inline">get_state()</strong> method (which we will implement in the next step):<p class="source-code">(self.agent_start_state, self.agent_goal_state,) = \</p><p class="source-code">                                         self.get_state()</p></li>
				<li>Now we need to implement the <strong class="source-inline">get_state()</strong> method that returns the start and goal state for the Gridworld environment:<p class="source-code">def get_state(self):</p><p class="source-code">        start_state = np.where(self.grid_state == AGENT)</p><p class="source-code">        goal_state = np.where(self.grid_state == GOAL)</p><p class="source-code">        start_or_goal_not_found = not (start_state[0] \</p><p class="source-code">                                       and goal_state[0])</p><p class="source-code">        if start_or_goal_not_found:</p><p class="source-code">            sys.exit(</p><p class="source-code">                "Start and/or Goal state not present in </p><p class="source-code">                 the Gridworld. "</p><p class="source-code">                "Check the Grid layout"</p><p class="source-code">            )</p><p class="source-code">        start_state = (start_state[0][0], </p><p class="source-code">                       start_state[1][0])</p><p class="source-code">        goal_state = (goal_state[0][0], goal_state[1][0])</p><p class="source-code">        return start_state, goal_state</p></li>
				<li>In this step, we <a id="_idIndexMarker010"/>will be implementing the <strong class="source-inline">step(action)</strong> method to execute the action and retrieve the<a id="_idIndexMarker011"/> next state/observation, the associated reward, and whether the episode ended:<p class="source-code">def step(self, action):</p><p class="source-code">        """return next observation, reward, done, info"""</p><p class="source-code">        action = int(action)</p><p class="source-code">        info = {"success": True}</p><p class="source-code">        done = False</p><p class="source-code">        reward = 0.0</p><p class="source-code">        next_obs = (</p><p class="source-code">            self.agent_state[0] + \</p><p class="source-code">                self.action_pos_dict[action][0],</p><p class="source-code">            self.agent_state[1] + \</p><p class="source-code">                self.action_pos_dict[action][1],</p><p class="source-code">        )</p></li>
				<li>Next, let's specify the<a id="_idIndexMarker012"/> rewards and <a id="_idIndexMarker013"/>finally, return <strong class="source-inline">grid_state</strong>, <strong class="source-inline">reward</strong>, <strong class="source-inline">done</strong>, and <strong class="source-inline">info</strong>:<p class="source-code"> # Determine the reward</p><p class="source-code">        if action == NOOP:</p><p class="source-code">            return self.grid_state, reward, False, info</p><p class="source-code">        next_state_valid = (</p><p class="source-code">            next_obs[0] &lt; 0 or next_obs[0] &gt;= \</p><p class="source-code">                                self.grid_state.shape[0]</p><p class="source-code">        ) or (next_obs[1] &lt; 0 or next_obs[1] &gt;= \</p><p class="source-code">                                self.grid_state.shape[1])</p><p class="source-code">        if next_state_valid:</p><p class="source-code">            info["success"] = False</p><p class="source-code">            return self.grid_state, reward, False, info</p><p class="source-code">        next_state = self.grid_state[next_obs[0], </p><p class="source-code">                                     next_obs[1]]</p><p class="source-code">        if next_state == EMPTY:</p><p class="source-code">            self.grid_state[next_obs[0], </p><p class="source-code">                            next_obs[1]] = AGENT</p><p class="source-code">        elif next_state == WALL:</p><p class="source-code">            info["success"] = False</p><p class="source-code">            reward = -0.1</p><p class="source-code">            return self.grid_state, reward, False, info</p><p class="source-code">        elif next_state == GOAL:</p><p class="source-code">            done = True</p><p class="source-code">            reward = 1</p><p class="source-code">        elif next_state == MINE:</p><p class="source-code">            done = True</p><p class="source-code">            reward = -1        # self._render("human")</p><p class="source-code">        self.grid_state[self.agent_state[0], </p><p class="source-code">                        self.agent_state[1]] = EMPTY</p><p class="source-code">        self.agent_state = copy.deepcopy(next_obs)</p><p class="source-code">        return self.grid_state, reward, done, info</p></li>
				<li>Up next is the <strong class="source-inline">reset()</strong> method, which resets the Gridworld environment when an<a id="_idIndexMarker014"/> episode<a id="_idIndexMarker015"/> completes (or if a request to reset the environment is made):<p class="source-code">def reset(self):</p><p class="source-code">        self.grid_state = copy.deepcopy(</p><p class="source-code">                                 self.initial_grid_state)</p><p class="source-code">        (self.agent_state, self.agent_goal_state,) = \</p><p class="source-code">                                         self.get_state()</p><p class="source-code">        return self.grid_state</p></li>
				<li> To visualize the state of the Gridworld environment in a human-friendly manner, let's implement a render function that will convert the <strong class="source-inline">grid_layout</strong> that we defined in step 5 to an image and display it. With that, the Gridworld environment implementation will be complete!<p class="source-code">def gridarray_to_image(self, img_shape=None):</p><p class="source-code">        if img_shape is None:</p><p class="source-code">            img_shape = self.img_shape</p><p class="source-code">        observation = np.random.randn(*img_shape) * 0.0</p><p class="source-code">        scale_x = int(observation.shape[0] / self.grid_\</p><p class="source-code">                                         state.shape[0])</p><p class="source-code">        scale_y = int(observation.shape[1] / self.grid_\</p><p class="source-code">                                         state.shape[1])</p><p class="source-code">        for i in range(self.grid_state.shape[0]):</p><p class="source-code">            for j in range(self.grid_state.shape[1]):</p><p class="source-code">                for k in range(3):  # 3-channel RGB image</p><p class="source-code">                    pixel_value = \</p><p class="source-code">                      COLOR_MAP[self.grid_state[i, j]][k]</p><p class="source-code">                    observation[</p><p class="source-code">                        i * scale_x : (i + 1) * scale_x,</p><p class="source-code">                        j * scale_y : (j + 1) * scale_y,</p><p class="source-code">                        k,</p><p class="source-code">                    ] = pixel_value</p><p class="source-code">        return (255 * observation).astype(np.uint8)</p><p class="source-code">    def render(self, mode="human", close=False):</p><p class="source-code">        if close:</p><p class="source-code">            if self.viewer is not None:</p><p class="source-code">                self.viewer.close()</p><p class="source-code">                self.viewer = None</p><p class="source-code">            return</p><p class="source-code">        img = self.gridarray_to_image()</p><p class="source-code">        if mode == "rgb_array":</p><p class="source-code">            return img</p><p class="source-code">        elif mode == "human":</p><p class="source-code">            from gym.envs.classic_control import \</p><p class="source-code">               rendering</p><p class="source-code">            if self.viewer is None:</p><p class="source-code">                self.viewer = \</p><p class="source-code">                        rendering.SimpleImageViewer()</p><p class="source-code">            self.viewer.imshow(img)</p></li>
				<li> To test whether the <a id="_idIndexMarker016"/>environment is working as <a id="_idIndexMarker017"/>expected, let's add a <strong class="source-inline">__main__</strong> function that gets executed if the environment script is run directly:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">	env = GridworldEnv()</p><p class="source-code">	obs = env.reset()</p><p class="source-code">	# Sample a random action from the action space</p><p class="source-code">	action = env.action_space.sample()</p><p class="source-code">	next_obs, reward, done, info = env.step(action)</p><p class="source-code">	print(f"reward:{reward} done:{done} info:{info}")</p><p class="source-code">	env.render()</p><p class="source-code">	env.close()</p></li>
				<li>All set! The Gridworld environment is ready and we can quickly test it by running the script (<strong class="source-inline">python envs/gridworld.py</strong>). An output such as the following will be displayed:<p class="source-code">reward:0.0 done:False info:{'success': True}</p><p>The following<a id="_idIndexMarker018"/> rendering of the Gridworld environment will also <a id="_idIndexMarker019"/>be displayed:</p></li>
			</ol>
			<div>
				<div id="_idContainer006" class="IMG---Figure">
					<img src="image/B15074_01_002.jpg" alt="Figure 1.2 – The Gridworld "/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – The Gridworld</p>
			<p>Let's now see how it works!</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor020"/>How it works…</h2>
			<p>The <strong class="source-inline">grid_layout</strong> defined in step 5 in the <em class="italic">How to do it…</em> section represents the state of the learning environment. The Gridworld environment defines the observation space, action spaces, and the rewarding mechanism to implement a <strong class="bold">Markov Decision Process</strong> (<strong class="bold">MDP</strong>). We sample a valid action <a id="_idIndexMarker020"/>from the action space of the environment and step the environment<a id="_idIndexMarker021"/> with the chosen action, which results in the new observation, reward, and a done status Boolean (representing if the episode has finished) as the response from the Gridworld environment. The <strong class="source-inline">env.render()</strong> method converts the environment's internal grid representation to an image and displays it for visual understanding.</p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>Implementing neural network-based RL policies for discrete action spaces and decision-making problems</h1>
			<p>Many environments (both simulated and real) for RL requires the RL agent to choose <a id="_idIndexMarker022"/>an action from a list of actions or, in other words, take discrete actions. While simple linear functions can be used to represent policies for such agents, they are often not scalable to complex problems. A non-linear function approximator such as a (deep) neural network can approximate arbitrary functions, even those required to solve complex problems.</p>
			<p>The neural network-based policy<a id="_idIndexMarker023"/> network is a crucial building block for advanced RL and <strong class="bold">Deep RL</strong> and will be applicable to <a id="_idIndexMarker024"/>general, discrete decision-making problems.</p>
			<p>By the end of this recipe, you will have an agent with a neural network-based policy implemented in <strong class="bold">TensorFlow 2.x</strong> that can take<a id="_idIndexMarker025"/> actions in the <strong class="bold">Gridworld</strong> environment and (with little or no modifications) in any discrete-action space environment.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Getting ready</h2>
			<p>Activate the <strong class="source-inline">tf2rl-cookbook</strong> Python virtual environment and run the following to install and import the packages:</p>
			<p class="source-code">pip install --upgrade numpy tensorflow tensorflow_probability seaborn </p>
			<p class="source-code">import seaborn as sns</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow import keras</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">import tensorflow_probability as tfp</p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>How to do it…</h2>
			<p>We will look at<a id="_idIndexMarker026"/> policy distribution types that can be used by agents in environments with discrete action spaces:</p>
			<ol>
				<li value="1">Let's begin by creating a binary policy distribution in TensorFlow 2.x using the <strong class="source-inline">tensorflow_probability</strong> library:<p class="source-code">binary_policy = tfp.distributions.Bernoulli(probs=0.5)</p><p class="source-code">for i in range(5):</p><p class="source-code">    action = binary_policy.sample(1)</p><p class="source-code">    print("Action:", action)</p><p>The preceding code should print something like the following: </p><p class="source-code">Action: tf.Tensor([0], shape=(1,), dtype=int32)</p><p class="source-code">Action: tf.Tensor([1], shape=(1,), dtype=int32)</p><p class="source-code">Action: tf.Tensor([0], shape=(1,), dtype=int32)</p><p class="source-code">Action: tf.Tensor([1], shape=(1,), dtype=int32)</p><p class="source-code">Action: tf.Tensor([1], shape=(1,), dtype=int32)</p><p class="callout-heading">Important note</p><p class="callout">The values of the action that you get will differ from what is shown here because they will be sampled from the Bernoulli distribution, which is not a deterministic process.</p></li>
				<li>Let's quickly visualize the binary policy distribution:<p class="source-code"># Sample 500 actions from the binary policy distribution</p><p class="source-code">sample_actions = binary_policy.sample(500)</p><p class="source-code">sns.distplot(sample_actions)</p><p>The preceding code will<a id="_idIndexMarker027"/> generate a distribution plot as shown here:</p><div id="_idContainer007" class="IMG---Figure"><img src="image/B15074_01_003.jpg" alt="Figure 1.3 – A distribution plot of the binary policy "/></div><p class="figure-caption">Figure 1.3 – A distribution plot of the binary policy</p></li>
				<li>In this step, we will be implementing a discrete policy distribution. A categorical distribution over a single discrete variable with <em class="italic">k</em> finite categories is referred to as a <strong class="bold">multinoulli</strong> distribution. The <a id="_idIndexMarker028"/>generalization of the multinoulli distribution to multiple trials is the<a id="_idIndexMarker029"/> multinomial distribution that we will be using to represent discrete policy distributions:<p class="source-code">action_dim = 4  # Dimension of the discrete action space</p><p class="source-code">action_probabilities = [0.25, 0.25, 0.25, 0.25]</p><p class="source-code">discrete_policy = tfp.distributions.Multinomial(probs=action_probabilities, total_count=1)</p><p class="source-code">for i in range(5):</p><p class="source-code">    action = discrete_policy.sample(1)</p><p class="source-code">    print(action)</p><p>The preceding code should print something along the lines of the following:</p><p class="callout-heading">Important note</p><p class="callout">The values of the action that you get will differ from what is shown here because they will be sampled from the multinomial distribution, which is not a deterministic process.</p><p class="source-code">tf.Tensor([[0. 0. 0. 1.]], shape=(1, 4), dtype=float32)</p><p class="source-code">tf.Tensor([[0. 0. 1. 0.]], shape=(1, 4), dtype=float32)</p><p class="source-code">tf.Tensor([[0. 0. 1. 0.]], shape=(1, 4), dtype=float32)</p><p class="source-code">tf.Tensor([[1. 0. 0. 0.]], shape=(1, 4), dtype=float32)</p><p class="source-code">tf.Tensor([[0. 1. 0. 0.]], shape=(1, 4), dtype=float32)</p></li>
				<li>Next, we visualize the discrete probability distribution:<p class="source-code">sns.distplot(discrete_policy.sample(1))</p><p>The preceding code will <a id="_idIndexMarker030"/>generate a distribution plot, like the one shown here for <strong class="source-inline">discrete_policy</strong>:</p><div id="_idContainer008" class="IMG---Figure"><img src="image/B15074_01_004.jpg" alt="Figure 1.4 – A distribution plot of the discrete policy "/></div><p class="figure-caption">Figure 1.4 – A distribution plot of the discrete policy</p></li>
				<li>Then, calculate the entropy of a discrete policy:<p class="source-code">def entropy(action_probs):</p><p class="source-code">    return -tf.reduce_sum(action_probs * \</p><p class="source-code">                      tf.math.log(action_probs), axis=-1)</p><p class="source-code">action_probabilities = [0.25, 0.25, 0.25, 0.25]</p><p class="source-code">print(entropy(action_probabilities))</p></li>
				<li>Also, implement a discrete policy class:<p class="source-code">class DiscretePolicy(object):</p><p class="source-code">    def __init__(self, num_actions):</p><p class="source-code">        self.action_dim = num_actions</p><p class="source-code">    def sample(self, actino_logits):</p><p class="source-code">        self.distribution = tfp.distributions.Multinomial(logits=action_logits, total_count=1)</p><p class="source-code">        return self.distribution.sample(1)</p><p class="source-code">    def get_action(self, action_logits):</p><p class="source-code">        action = self.sample(action_logits)</p><p class="source-code">        return np.where(action)[-1]  </p><p class="source-code">        # Return the action index</p><p class="source-code">    def entropy(self, action_probabilities):</p><p class="source-code">        return – tf.reduce_sum(action_probabilities * tf.math.log(action_probabilities), axis=-1)</p></li>
				<li>Now we implement a<a id="_idIndexMarker031"/> helper method to evaluate the agent in a given environment:<p class="source-code">def evaluate(agent, env, render=True):</p><p class="source-code">    obs, episode_reward, done, step_num = env.reset(), </p><p class="source-code">                                          0.0, False, 0</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(obs)</p><p class="source-code">        obs, reward, done, info = env.step(action)</p><p class="source-code">        episode_reward += reward</p><p class="source-code">        step_num += 1</p><p class="source-code">        if render:</p><p class="source-code">            env.render()</p><p class="source-code">    return step_num, episode_reward, done, info</p></li>
				<li>Let's now<a id="_idIndexMarker032"/> implement a neural network Brain class using TensorFlow 2.x:<p class="source-code">class Brain(keras.Model):</p><p class="source-code">    def __init__(self, action_dim=5, </p><p class="source-code">                 input_shape=(1, 8 * 8)):</p><p class="source-code">        """Initialize the Agent's Brain model</p><p class="source-code">        Args:</p><p class="source-code">            action_dim (int): Number of actions</p><p class="source-code">        """</p><p class="source-code">        super(Brain, self).__init__()</p><p class="source-code">        self.dense1 = layers.Dense(32, input_shape=\</p><p class="source-code">                          input_shape, activation="relu")</p><p class="source-code">        self.logits = layers.Dense(action_dim)</p><p class="source-code">    def call(self, inputs):</p><p class="source-code">        x = tf.convert_to_tensor(inputs)</p><p class="source-code">        if len(x.shape) &gt;= 2 and x.shape[0] != 1:</p><p class="source-code">            x = tf.reshape(x, (1, -1))</p><p class="source-code">        return self.logits(self.dense1(x))</p><p class="source-code">    def process(self, observations):</p><p class="source-code"># Process batch observations using `call(inputs)` behind-the-scenes</p><p class="source-code">        action_logits = \</p><p class="source-code">                     self.predict_on_batch(observations)</p><p class="source-code">        return action_logits</p></li>
				<li>Let's now implement a simple agent class that uses a <strong class="source-inline">DiscretePolicy</strong> object to act in discrete environments:<p class="source-code">class Agent(object):</p><p class="source-code">    def __init__(self, action_dim=5, </p><p class="source-code">                 input_dim=(1, 8 * 8)):</p><p class="source-code">        self.brain = Brain(action_dim, input_dim)</p><p class="source-code">        self.policy = DiscretePolicy(action_dim)</p><p class="source-code">    def get_action(self, obs):</p><p class="source-code">        action_logits = self.brain.process(obs)</p><p class="source-code">        action = self.policy.get_action(</p><p class="source-code">                            np.squeeze(action_logits, 0))</p><p class="source-code">        return action</p></li>
				<li>Let's now test the agent in <strong class="source-inline">GridworldEnv</strong>:<p class="source-code">from envs.gridworld import GridworldEnv</p><p class="source-code">env = GridworldEnv()</p><p class="source-code">agent = Agent(env.action_space.n, </p><p class="source-code">              env.observation_space.shape)</p><p class="source-code">steps, reward, done, info = evaluate(agent, env)</p><p class="source-code">print(f"steps:{steps} reward:{reward} done:{done} info:{info}")</p><p class="source-code">env.close()</p></li>
			</ol>
			<p>This shows how to implement the policy. We will see how this works in the following section.</p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>How it works…</h2>
			<p>One of the central components of an RL agent is the policy function that maps between observations and actions. Formally, a policy is a distribution over actions that prescribes the probabilities of choosing an action given an observation.</p>
			<p>In environments where the agent can take at most two different actions, for example, in a binary action space, we can represent the policy using a <strong class="bold">Bernoulli distribution</strong>, where the probability of taking action 0 is given by <img src="image/Formula_01_001.png" alt=""/>, and the probability of taking action 1 is given by <img src="image/Formula_01_002.png" alt=""/>, which gives rise to the following probability distribution:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/Formula_01_003.jpg" alt=""/>
				</div>
			</div>
			<p>A discrete probability distribution can be used to represent an RL agent's policy when the agent can take one of <em class="italic">k</em> possible actions in an environment.</p>
			<p>In a general sense, such distributions can be used to describe the possible results of a random variable that can take one of <em class="italic">k</em> possible categories and is therefore also called a <strong class="bold">categorical distribution</strong>. This is a generalization of the<a id="_idIndexMarker033"/> Bernoulli distribution to k-way events and is therefore a multinoulli distribution.</p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor025"/>Implementing neural network-based RL policies for continuous action spaces and continuous-control problems</h1>
			<p>Reinforcement learning<a id="_idIndexMarker034"/> has been used to achieve the state of the art in many control problems, not only in games as varied as Atari, Go, Chess, Shogi, and StarCraft, but also in real-world deployments, such as HVAC control systems.</p>
			<p>In environments where the action space is continuous, meaning that the actions are real-valued, a real-valued, continuous policy distribution is necessary. A continuous probability distribution can be used to represent an RL agent's policy when the action space of the environment contains real numbers. In a general sense, such distributions can be used to describe the possible results of a random variable when the random variable can take any (real) value.</p>
			<p>Once the recipe is complete, you will have a complete script to control a car in two dimensions to drive up a hill using the <strong class="source-inline">MountainCarContinuous</strong> environment with a continuous action space. A screenshot from the <strong class="source-inline">MountainCarContinuous</strong> environment is shown here:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B15074_01_005.jpg" alt="Figure 1.5 – A screenshot of the MountainCarContinuous environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – A screenshot of the MountainCarContinuous environment</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor026"/>Getting ready</h2>
			<p>Activate the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python<a id="_idIndexMarker035"/> environment and run the following command to install and import the necessary Python packages for this recipe:</p>
			<p class="source-code">pip install --upgrade tensorflow_probability</p>
			<p class="source-code">import tensorflow_probability as tfp</p>
			<p class="source-code">import seaborn as sns</p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>How to do it…</h2>
			<p>We will begin by creating continuous policy distributions using <strong class="bold">TensorFlow 2.x</strong> and the <strong class="source-inline">tensorflow_probability</strong> library and <a id="_idIndexMarker036"/>build upon the necessary action sampling methods to generate action for a given continuous space of an RL environment:</p>
			<ol>
				<li value="1">We create a continuous policy distribution in TensorFlow 2.x using the <strong class="source-inline">tensorflow_probability</strong> library. We will use a Gaussian/normal distribution to create a policy distribution over continuous values:<p class="source-code">sample_actions = continuous_policy.sample(500)</p><p class="source-code">sns.distplot(sample_actions)</p></li>
				<li>Next, we visualize a continuous policy distribution:<p class="source-code">sample_actions = continuous_policy.sample(500)</p><p class="source-code">sns.distplot(sample_actions)</p><p>The preceding code will<a id="_idIndexMarker037"/> generate a distribution plot of the continuous policy, like the plot shown here:</p><div id="_idContainer013" class="IMG---Figure"><img src="image/B15074_01_006.jpg" alt="Figure 1.6 – A distribution plot of the continuous policy "/></div><p class="figure-caption">Figure 1.6 – A distribution plot of the continuous policy</p></li>
				<li>Let's now implement a continuous policy distribution using a Gaussian/normal distribution:<p class="source-code">mu = 0.0  # Mean = 0.0</p><p class="source-code">sigma = 1.0  # Std deviation = 1.0</p><p class="source-code">continuous_policy = tfp.distributions.Normal(loc=mu,</p><p class="source-code">                                             scale=sigma)</p><p class="source-code"># action = continuous_policy.sample(10)</p><p class="source-code">for i in range(10):</p><p class="source-code">    action = continuous_policy.sample(1)</p><p class="source-code">    print(action)</p><p>The preceding code <a id="_idIndexMarker038"/>should print something similar to what is shown in the following code block:</p><p class="source-code">tf.Tensor([-0.2527136], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([1.3262751], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([0.81889665], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([1.754675], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([0.30025303], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([-0.61728036], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([0.40142158], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([1.3219402], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([0.8791297], shape=(1,), dtype=float32)</p><p class="source-code">tf.Tensor([0.30356944], shape=(1,), dtype=float32)</p><p class="callout-heading">Important note</p><p class="callout">The values of the action that you get will differ from what is shown here because they will be sampled from the Gaussian distribution, which is not a deterministic process.</p></li>
				<li>Let's now move one step further and implement a multi-dimensional continuous policy. A <strong class="bold">multivariate Gaussian distribution</strong> can be used to represent multi-dimensional <a id="_idIndexMarker039"/>continuous policies. Such polices are useful for agents when<a id="_idIndexMarker040"/> acting in environments with action spaces that are multi-dimensional, as well as continuous and real-valued:<p class="source-code">mu = [0.0, 0.0]</p><p class="source-code">covariance_diag = [3.0, 3.0]</p><p class="source-code">continuous_multidim_policy = tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=covariance_diag)</p><p class="source-code"># action = continuous_multidim_policy.sample(10)</p><p class="source-code">for i in range(10):</p><p class="source-code">    action = continuous_multidim_policy.sample(1)</p><p class="source-code">    print(action)</p><p>The preceding code should print something similar to what follows:</p><p class="callout-heading">Important note </p><p class="callout">The values of the action that you get will differ from what is shown here because they will be sampled from the multivariate Gaussian/normal distribution, which is not a deterministic process).</p><p class="source-code"> tf.Tensor([[ 1.7003113 -2.5801306]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[ 2.744986  -0.5607129]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[ 6.696332  -3.3528223]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[ 1.2496299 -8.301748 ]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[2.0009246 3.557394 ]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[-4.491785  -1.0101566]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[ 3.0810184 -0.9008362]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[1.4185237 2.2145705]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[-1.9961193 -2.1251974]], shape=(1, 2), dtype=float32)</p><p class="source-code">tf.Tensor([[-1.2200387 -4.3516426]], shape=(1, 2), dtype=float32)</p></li>
				<li>Before moving on, let's visualize the<a id="_idIndexMarker041"/> multi-dimensional continuous policy:<p class="source-code">sample_actions = continuous_multidim_policy.sample(500)</p><p class="source-code">sns.jointplot(sample_actions[:, 0], sample_actions[:, 1], kind='scatter')</p><p>The preceding code will generate a joint distribution plot similar to the plot shown here:</p><div id="_idContainer014" class="IMG---Figure"><img src="image/B15074_01_007.jpg" alt="Figure 1.7 – Joint distribution plot of a multi-dimensional continuous policy "/></div><p class="figure-caption">Figure 1.7 – Joint distribution plot of a multi-dimensional continuous policy</p></li>
				<li>Now, we are ready to<a id="_idIndexMarker042"/> implement the continuous policy class:<p class="source-code">class ContinuousPolicy(object):</p><p class="source-code">    def __init__(self, action_dim):</p><p class="source-code">        self.action_dim = action_dim</p><p class="source-code">    def sample(self, mu, var):</p><p class="source-code">        self.distribution = \</p><p class="source-code">            tfp.distributions.Normal(loc=mu, scale=sigma)</p><p class="source-code">        return self.distribution.sample(1)</p><p class="source-code">    def get_action(self, mu, var):</p><p class="source-code">        action = self.sample(mu, var)</p><p class="source-code">        return action</p></li>
				<li>As a next step, let's implement a <a id="_idIndexMarker043"/>multi-dimensional continuous policy class:<p class="source-code">import tensorflow_probability as tfp</p><p class="source-code">import numpy as np</p><p class="source-code">class ContinuousMultiDimensionalPolicy(object):</p><p class="source-code">    def __init__(self, num_actions):</p><p class="source-code">        self.action_dim = num_actions</p><p class="source-code">    def sample(self, mu, covariance_diag):</p><p class="source-code">        self.distribution = tfp.distributions.\</p><p class="source-code">                         MultivariateNormalDiag(loc=mu,</p><p class="source-code">                         scale_diag=covariance_diag)</p><p class="source-code">        return self.distribution.sample(1)</p><p class="source-code">    def get_action(self, mu, covariance_diag):</p><p class="source-code">        action = self.sample(mu, covariance_diag)</p><p class="source-code">        return action</p></li>
				<li>Let's now implement a function to evaluate an agent in an environment with a continuous action space to assess episodic performance:<p class="source-code">def evaluate(agent, env, render=True):</p><p class="source-code">    obs, episode_reward, done, step_num = env.reset(),</p><p class="source-code">                                          0.0, False, 0</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(obs)</p><p class="source-code">        obs, reward, done, info = env.step(action)</p><p class="source-code">        episode_reward += reward</p><p class="source-code">        step_num += 1</p><p class="source-code">        if render:</p><p class="source-code">            env.render()</p><p class="source-code">    return step_num, episode_reward, done, info</p></li>
				<li>We are now ready to <a id="_idIndexMarker044"/>test the agent in a continuous action environment:<p class="source-code">from neural_agent import Brain</p><p class="source-code">import gym</p><p class="source-code">env = gym.make("MountainCarContinuous-v0")Implementing a Neural-network Brain class using TensorFlow 2.x. </p><p class="source-code">          class Brain(keras.Model):</p><p class="source-code">    def __init__(self, action_dim=5, </p><p class="source-code">                 input_shape=(1, 8 * 8)):</p><p class="source-code">        """Initialize the Agent's Brain model</p><p class="source-code">        Args:</p><p class="source-code">            action_dim (int): Number of actions</p><p class="source-code">        """</p><p class="source-code">        super(Brain, self).__init__()</p><p class="source-code">        self.dense1 = layers.Dense(32, </p><p class="source-code">              input_shape=input_shape, activation="relu")</p><p class="source-code">        self.logits = layers.Dense(action_dim)</p><p class="source-code">    def call(self, inputs):</p><p class="source-code">        x = tf.convert_to_tensor(inputs)</p><p class="source-code">        if len(x.shape) &gt;= 2 and x.shape[0] != 1:</p><p class="source-code">            x = tf.reshape(x, (1, -1))</p><p class="source-code">        return self.logits(self.dense1(x))</p><p class="source-code">    def process(self, observations):</p><p class="source-code">        # Process batch observations using `call(inputs)`</p><p class="source-code">        # behind-the-scenes</p><p class="source-code">        action_logits = \</p><p class="source-code">            self.predict_on_batch(observations)</p><p class="source-code">        return action_logits</p></li>
				<li>Let's implement a simple <a id="_idIndexMarker045"/>agent class that utilizes the <strong class="source-inline">ContinuousPolicy</strong> object to act in continuous action space environments:<p class="source-code">class Agent(object):</p><p class="source-code">    def __init__(self, action_dim=5, </p><p class="source-code">                 input_dim=(1, 8 * 8)):</p><p class="source-code">        self.brain = Brain(action_dim, input_dim)</p><p class="source-code">        self.policy = ContinuousPolicy(action_dim)</p><p class="source-code">    def get_action(self, obs):</p><p class="source-code">        action_logits = self.brain.process(obs)</p><p class="source-code">        action = self.policy.get_action(*np.\</p><p class="source-code">                        squeeze(action_logits, 0))</p><p class="source-code">        return action</p></li>
				<li>As a final step, we will test the <a id="_idIndexMarker046"/>performance of the agent in a continuous action space environment:<p class="source-code">from neural_agent import Brain</p><p class="source-code">import gym</p><p class="source-code">env = gym.make("MountainCarContinuous-v0") </p><p class="source-code">action_dim = 2 * env.action_space.shape[0]  </p><p class="source-code">    # 2 values (mu &amp; sigma) for one action dim</p><p class="source-code">agent = Agent(action_dim, env.observation_space.shape)</p><p class="source-code">steps, reward, done, info = evaluate(agent, env)</p><p class="source-code">print(f"steps:{steps} reward:{reward} done:{done} info:{info}")</p><p class="source-code">env.close()</p><p>The preceding script will call the <strong class="source-inline">MountainCarContinuous</strong> environment, render it to the screen, and show how the agent is performing in this continuous action space environment:</p></li>
			</ol>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B15074_01_008.jpg" alt="Figure 1.8 – A screenshot of the agent in the MountainCarContinuous-v0 environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – A screenshot of the agent in the MountainCarContinuous-v0 environment</p>
			<p>Next, let's explore how it works.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>How it works…</h2>
			<p>We implemented a <a id="_idIndexMarker047"/>continuous-valued policy for RL agents using a <strong class="bold">Gaussian distribution</strong>. Gaussian distribution, which is also known as <strong class="bold">normal distribution</strong>, is the most widely used distribution for real numbers. It is represented using two parameters, <a id="_idTextAnchor029"/>µ and σ. We<a id="_idIndexMarker048"/> generated continuous-valued actions from such a policy by sampling from the distribution, based on the probability density that is given by the following equation:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/Formula_01_004.jpg" alt=""/>
				</div>
			</div>
			<p>The <strong class="bold">multivariate normal distribution</strong> extends the normal distribution to multiple variables. We used this distribution to generate <a id="_idIndexMarker049"/>multi-dimensional continuous policies.</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor030"/>Working with OpenAI Gym for RL training environments</h1>
			<p>This recipe provides a<a id="_idIndexMarker050"/> quick run-through for getting up and running with OpenAI Gym environments. The Gym environment and the interface <a id="_idIndexMarker051"/>provide a platform for training RL agents and is the most widely used and accepted RL environment interface.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor031"/>Getting ready</h2>
			<p>We will be needing the full installation of OpenAI Gym to be able to use the available environments. Please follow the Gym installation steps listed at <a href="https://github.com/openai/gym#id5">https://github.com/openai/gym#id5</a>.</p>
			<p>As a minimum, you should execute the following command:</p>
			<p class="source-code">pip install gym[atari]</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor032"/>How to do it…</h2>
			<p>Let's start by picking<a id="_idIndexMarker052"/> an environment and exploring the Gym interface. You may already be familiar with the basic function calls to create a <a id="_idIndexMarker053"/>Gym environment from the previous recipes.</p>
			<p>Your steps should be formatted like so:</p>
			<ol>
				<li value="1">Let's first explore the list of environments in Gym:<p class="source-code">#!/usr/bin/env python</p><p class="source-code">from gym import envs</p><p class="source-code">env_names = [spec.id for spec in envs.registry.all()]</p><p class="source-code">for name in sorted(env_names):</p><p class="source-code">    print(name)</p></li>
				<li>This script will print the names of all the environments available through your Gym installation, sorted alphabetically. You can run this script using the following command to see the names of the environments that are installed and available in your system. You should see a long list of environments listed. The first few are shown in the following screenshot for your reference:<div id="_idContainer017" class="IMG---Figure"><img src="image/B15074_01_009.jpg" alt="Figure 1.9 – List of environments available using the openai-gym package "/></div><p class="figure-caption">Figure 1.9 – List of environments available using the openai-gym package</p><p>Let's now see how we can<a id="_idIndexMarker054"/> run one of the Gym environments.</p></li>
				<li>The following script will let you <a id="_idIndexMarker055"/>explore any of the available Gym environments:<p class="source-code">#!/usr/bin/env python</p><p class="source-code">import gym</p><p class="source-code">import sys</p><p class="source-code">def run_gym_env(argv):</p><p class="source-code">    env = gym.make(argv[1]) # Name of the environment </p><p class="source-code">                            # supplied as 1st argument</p><p class="source-code">    env.reset()</p><p class="source-code">    for _ in range(int(argv[2])):</p><p class="source-code">        env.render()</p><p class="source-code">        env.step(env.action_space.sample())</p><p class="source-code">    env.close()</p><p class="source-code">if __name__ == "__main__":</p><p class="source-code">    run_gym_env(sys.argv)</p></li>
				<li>You can save the<a id="_idIndexMarker056"/> preceding script to <strong class="source-inline">run_gym_env.py</strong> and run the script like this: <p class="source-code"><strong class="bold">(tf2rl-cookbook) praveen@g5: ~/tf2rl-cookbook/ch1/src$python run_gym_env.py Alien-v4 1000</strong></p><p>The script will render<a id="_idIndexMarker057"/> the <strong class="source-inline">Alien-v4</strong> environment, which should look like the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B15074_01_010.jpg" alt="Figure 1.10 – Sample output of run_gym_env.py with Alien-v4 1000 as the arguments "/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10 – Sample output of run_gym_env.py with Alien-v4 1000 as the arguments</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">You can change <strong class="source-inline">Alien-v4</strong> to any of the available Gym environments listed in the previous step.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor033"/>How it works…</h2>
			<p>A summary of <a id="_idIndexMarker058"/>how the Gym environments work is<a id="_idIndexMarker059"/> presented in the following table:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/Table_1.1.jpg" alt="Table 1.1 – Summary of the Gym environment interface "/>
				</div>
			</div>
			<p class="figure-caption">Table 1.1 – Summary of the Gym environment interface</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>See also</h2>
			<p>You<a id="_idIndexMarker060"/> can find more information<a id="_idIndexMarker061"/> on OpenAI Gym <a id="_idIndexMarker062"/>here: <a href="http://gym.openai.com/">http://gym.openai.com/</a>.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/>Building a neural agent</h1>
			<p>This recipe will guide you through<a id="_idIndexMarker063"/> the steps to build a complete agent and the agent-environment interaction loop, which is the main building block for any RL application. When you complete the recipe, you will have an executable script where a simple agent tries to act in a Gridworld environment. A glimpse of what the agent you build will likely be doing is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B15074_01_011.jpg" alt="Figure 1.11 – Screenshot of output from the neural_agent.py script "/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – Screenshot of output from the neural_agent.py script</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>Getting ready</h2>
			<p>Let's get started by activating the <strong class="source-inline">tf2rl-cookbook</strong> Conda Python environment and running the following<a id="_idIndexMarker064"/> code to install and import the necessary Python modules:</p>
			<p class="source-code">pip install tensorflow gym tqdm  # Run this line in a terminal</p>
			<p class="source-code">
import tensorflow as tf</p>
			<p class="source-code">from tensorflow import keras</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import envs</p>
			<p class="source-code">from tqdm import tqdm</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>How to do it…</h2>
			<p>We will start by implementing a Brain class powered by a neural network implemented using TensorFlow 2.x:</p>
			<ol>
				<li value="1">Let's first initialize a <a id="_idIndexMarker065"/>neural brain model using TensorFlow 2.x and the Keras functional API:<p class="source-code">class Brain(keras.Model):</p><p class="source-code">    def __init__(self, action_dim=5, </p><p class="source-code">                   input_shape=(1, 8 * 8)):</p><p class="source-code">        """Initialize the Agent's Brain model</p><p class="source-code">        Args:</p><p class="source-code">            action_dim (int): Number of actions</p><p class="source-code">        """</p><p class="source-code">        super(Brain, self).__init__()</p><p class="source-code">        self.dense1 = layers.Dense(32, input_shape= \</p><p class="source-code">                          input_shape, activation="relu")</p><p class="source-code">        self.logits = layers.Dense(action_dim)</p></li>
				<li>Next, we implement the Brain class's <strong class="source-inline">call(…)</strong> method: <p class="source-code">def call(self, inputs):</p><p class="source-code">        x = tf.convert_to_tensor(inputs)</p><p class="source-code">        if len(x.shape) &gt;= 2 and x.shape[0] != 1:</p><p class="source-code">            x = tf.reshape(x, (1, -1))</p><p class="source-code">        return self.logits(self.dense1(x))</p></li>
				<li>Now we need to implement the Brain class's <strong class="source-inline">process()</strong> method to conveniently perform predictions on a batch of inputs/observations:<p class="source-code">def process(self, observations):</p><p class="source-code">        # Process batch observations using `call(inputs)`</p><p class="source-code">        # behind-the-scenes</p><p class="source-code">        action_logits = \</p><p class="source-code">            self.predict_on_batch(observations)</p><p class="source-code">        return action_logits</p></li>
				<li>Let's now implement the<a id="_idIndexMarker066"/> init function of the agent class:<p class="source-code">class Agent(object):</p><p class="source-code">    def __init__(self, action_dim=5, </p><p class="source-code">                 input_shape=(1, 8 * 8)):</p><p class="source-code">        """Agent with a neural-network brain powered</p><p class="source-code">           policy</p><p class="source-code">        Args:</p><p class="source-code">            brain (keras.Model): Neural Network based </p><p class="source-code">        model</p><p class="source-code">        """</p><p class="source-code">        self.brain = Brain(action_dim, input_shape)</p><p class="source-code">        self.policy = self.policy_mlp</p></li>
				<li>Now let's define a simple policy function for the agent:<p class="source-code">def policy_mlp(self, observations):</p><p class="source-code">        observations = observations.reshape(1, -1)</p><p class="source-code">        # action_logits = self.brain(observations)</p><p class="source-code">        action_logits = self.brain.process(observations)</p><p class="source-code">        action = tf.random.categorical(tf.math.\</p><p class="source-code">                       log(action_logits), num_samples=1)</p><p class="source-code">        return tf.squeeze(action, axis=1)</p></li>
				<li>After that, let's implement a convenient <strong class="source-inline">get_action</strong> method for the agent:<p class="source-code">def get_action(self, observations):</p><p class="source-code">        return self.policy(observations)</p></li>
				<li>Let's now create a placeholder function for <strong class="source-inline">learn()</strong> that will be implemented as part of RL<a id="_idIndexMarker067"/> algorithm implementation in future recipes:<p class="source-code">def learn(self, samples):</p><p class="source-code">        raise NotImplementedError</p><p>This completes our basic agent implementation with the necessary ingredients!</p></li>
				<li>Let's now evaluate the agent in a given environment for one episode:<p class="source-code">def evaluate(agent, env, render=True):</p><p class="source-code">    obs, episode_reward, done, step_num = env.reset(),</p><p class="source-code">                                          0.0, False, 0</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(obs)</p><p class="source-code">        obs, reward, done, info = env.step(action)</p><p class="source-code">        episode_reward += reward</p><p class="source-code">        step_num += 1</p><p class="source-code">        if render:</p><p class="source-code">            env.render()</p><p class="source-code">    return step_num, episode_reward, done, info</p></li>
				<li>Finally, let's implement the main function:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = gym.make("Gridworld-v0")</p><p class="source-code">    agent = Agent(env.action_space.n, </p><p class="source-code">                  env.observation_space.shape)</p><p class="source-code">    for episode in tqdm(range(10)):</p><p class="source-code">        steps, episode_reward, done, info = \</p><p class="source-code">                                     evaluate(agent, env)</p><p class="source-code">        print(f"EpReward:{episode_reward:.2f}\</p><p class="source-code">               steps:{steps} done:{done} info:{info}")</p><p class="source-code">    env.close()</p></li>
				<li>Execute the script as follows:<p class="source-code"><strong class="bold">python neural_agent.py</strong></p><p>You should see the Gridworld environment <a id="_idIndexMarker068"/>GUI pop up. This will show you what the agent is doing in the environment, and it will look like the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B15074_01_012.jpg" alt="Figure 1.12 – A screenshot of the neural agent acting in the Gridworld environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – A screenshot of the neural agent acting in the Gridworld environment</p>
			<p>This provides a simple, yet complete, recipe to build an agent and the agent-environment interaction loop. All that is left is to add the RL algorithm of your choice to the <strong class="source-inline">learn()</strong> method and the agent will start acting intelligently! </p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor038"/>How it works…</h2>
			<p>This recipe puts together the necessary ingredients to build a complete agent-environment system. The <strong class="source-inline">Brain</strong> class implements the neural network that serves as the processing unit of the agent, and the agent class utilizes the <strong class="source-inline">Brain</strong> class and a simple policy that chooses <a id="_idIndexMarker069"/>an action based on the output of the brain after processing the observations received from the environment.</p>
			<p>We implemented the <strong class="source-inline">Brain</strong> class as a subclass of the <strong class="source-inline">keras.Model</strong> class, which allows us to define a custom neural network-based model for the agent's brain. The <strong class="source-inline">__init__</strong> method initializes the <strong class="source-inline">Brain</strong> model and defines the necessary layers using the <strong class="bold">TensorFlow 2.x Keras functional API</strong>. In this <strong class="source-inline">Brain</strong> model, we<a id="_idIndexMarker070"/> are creating two <strong class="bold">dense</strong> (also known as <strong class="bold">fully-connected</strong>) layers to <a id="_idIndexMarker071"/>build our starter neural network. In addition to the <strong class="source-inline">__init__</strong> method, the <strong class="source-inline">call(…)</strong> method is also a mandatory method that needs to be implemented by child classes inheriting from the <strong class="source-inline">keras.Model</strong> class. The <strong class="source-inline">call(…) </strong>method first converts the inputs to a TensorFlow 2.x tensor and then flattens the inputs to be of the shape <strong class="source-inline">1 x total_number_of_elements</strong> in the input tensor. For example, if the input data has a shape of 8 x 8 (8 rows and 8 columns), the data is first converted to a tensor and the shape is flattened to 1 x 8 * 8 = 1 x 64. The flattened inputs are then processed by the dense1 layer, which contains 32 neurons and a ReLU activation function. Finally, the logits layer processes the output from the previous layer and produces n number of outputs corresponding to the action dimension (n).</p>
			<p>The <strong class="source-inline">predict_on_batch(…)</strong> method performs predictions on the batch of inputs given as the argument. This function (unlike the <strong class="source-inline">predict()</strong> function of <strong class="bold">Keras</strong>) assumes that the inputs (observations) provided as the argument are exactly one batch of inputs and thus feeds the batch to the network without any further splitting of the input data.</p>
			<p>We then implemented the <strong class="source-inline">Agent</strong> class and, in the agent initialization function, we created an object instance of the Brain class by defining the following:</p>
			<p class="source-code">self.brain = Brain(action_dim, input_shape)</p>
			<p>Here, <strong class="source-inline">input_shape</strong> is the shape of the input that is expected to be processed by the brain, and <strong class="source-inline">action_dim</strong> is the shape of the output expected from the brain. The agent's policy is defined to be a custom <strong class="bold">Multi-Layer Perceptron (MLP</strong>)-based policy based on the brain's neural network architecture. Note<a id="_idIndexMarker072"/> that we can reuse <strong class="source-inline">DiscretePolicy</strong> from the previous recipe to initialize the agent's policy as well.</p>
			<p>The agent's policy function, <strong class="source-inline">policy_mlp</strong>, flattens the input observations and sends it for processing by the agent's brain to receive the <strong class="source-inline">action_logits</strong>, which are the unnormalized probabilities for the actions. The final action to be taken is obtained by using TensorFlow 2.x's <strong class="source-inline">categorical</strong> method from the random module, which samples a<a id="_idIndexMarker073"/> valid action from the given <strong class="source-inline">action_logits</strong> (unnormalized probabilities).</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">If all of the observations supplied to the <strong class="source-inline">predict_on_batch</strong> function cannot be accommodated in the given amount of GPU memory or <a id="_idIndexMarker074"/>RAM (CPU), the operation can cause a GPU <strong class="bold">Out Of Memory</strong> (<strong class="bold">OOM</strong>) error.</p>
			<p>The main function that gets launched – if the <strong class="source-inline">neural_agent.py</strong> script is run directly – creates an instance of the Gridworld-v0 environment, initializes an agent using the action and observation space of this environment, and starts evaluating the agent for 10 episodes.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor039"/>Building a neural evolutionary agent</h1>
			<p>Evolutionary methods are<a id="_idIndexMarker075"/> based on black-box optimization and are also known as gradient-free <a id="_idIndexMarker076"/>methods since no gradient computation is involved. This recipe will walk you through the steps for implementing a simple, approximate cross-entropy-based neural evolutionary agent using <strong class="bold">TensorFlow 2.x</strong>. </p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>Getting ready</h2>
			<p>Activate the <strong class="source-inline">tf2rl-cookbook</strong> Python environment and import the following packages necessary to run this recipe:</p>
			<p class="source-code">from collections import namedtuple</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow import keras</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">from tqdm import tqdm</p>
			<p class="source-code">import envs</p>
			<p>With the packages installed, we are<a id="_idIndexMarker077"/> ready to begin.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>How to do it…</h2>
			<p>Let's put together all that we have learned in this chapter to build a neural agent that improves its policy to navigate the Gridworld environment using an evolutionary process:</p>
			<ol>
				<li value="1">Let's start by importing the basic neural agent and the Brain class from <strong class="source-inline">neural_agent.py</strong>:<p class="source-code">from neural_agent import Agent, Brain</p><p class="source-code">from envs.gridworld import GridworldEnv</p></li>
				<li>Next, let's implement a method to roll out the agent in a given environment for one episode and return <strong class="source-inline">obs_batch</strong>, <strong class="source-inline">actions_batch</strong>, and <strong class="source-inline">episode_reward</strong>:<p class="source-code">def rollout(agent, env, render=False):</p><p class="source-code">    obs, episode_reward, done, step_num = env.reset(),</p><p class="source-code">							 0.0, False, 0</p><p class="source-code">    observations, actions = [], []</p><p class="source-code">    episode_reward = 0.0</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(obs)</p><p class="source-code">        next_obs, reward, done, info = env.step(action)</p><p class="source-code">        # Save experience</p><p class="source-code">        observations.append(np.array(obs).reshape(1, -1))  	        # Convert to numpy &amp; reshape (8, 8) to (1, 64)</p><p class="source-code">        actions.append(action)</p><p class="source-code">        episode_reward += reward</p><p class="source-code">        </p><p class="source-code">        obs = next_obs</p><p class="source-code">        step_num += 1</p><p class="source-code">        if render:</p><p class="source-code">            env.render()</p><p class="source-code">    env.close()</p><p class="source-code">    return observations, actions, episode_reward</p></li>
				<li>Let's now test the trajectory<a id="_idIndexMarker078"/> rollout method:<p class="source-code">env = GridworldEnv()</p><p class="source-code"># input_shape = (env.observation_space.shape[0] * \</p><p class="source-code">                 env.observation_space.shape[1], )</p><p class="source-code">brain = Brain(env.action_space.n)</p><p class="source-code">agent = Agent(brain)</p><p class="source-code">obs_batch, actions_batch, episode_reward = rollout(agent,</p><p class="source-code">                                                   env)</p></li>
				<li>Now, it's time for us to verify that the experience data generated using the rollouts is coherent:<p class="source-code">assert len(obs_batch) == len(actions_batch)</p></li>
				<li>Let's now roll out multiple<a id="_idIndexMarker079"/> complete trajectories to collect experience data:<p class="source-code"># Trajectory: (obs_batch, actions_batch, episode_reward)</p><p class="source-code"># Rollout 100 episodes; Maximum possible steps = 100 * 100 = 10e4</p><p class="source-code">trajectories = [rollout(agent, env, render=True) \</p><p class="source-code">                for _ in tqdm(range(100))]</p></li>
				<li>We can then visualize the reward distribution from a sample of experience data. Let's also plot a red vertical line at the 50th percentile of the episode reward values in the collected experience data:<p class="source-code">from tqdm.auto import tqdm</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">sample_ep_rewards = [rollout(agent, env)[-1] for _ in \</p><p class="source-code">                     tqdm(range(100))]</p><p class="source-code">plt.hist(sample_ep_rewards, bins=10, histtype="bar");</p><p>Running this code will generate a plot like the one shown in the following diagram:</p><div id="_idContainer022" class="IMG---Figure"><img src="image/B15074_01_013.jpg" alt="Figure 1.13 – Histogram plot of the episode reward values "/></div><p class="figure-caption">Figure 1.13 – Histogram plot of the episode reward values</p></li>
				<li>Let's now create a <a id="_idIndexMarker080"/>container for storing trajectories:<p class="source-code">from collections import namedtuple</p><p class="source-code">Trajectory = namedtuple("Trajectory", ["obs", "actions",</p><p class="source-code">                                       "reward"])</p><p class="source-code"># Example for understanding the operations:</p><p class="source-code">print(Trajectory(*(1, 2, 3)))</p><p class="source-code"># Explanation: `*` unpacks the tuples into individual </p><p class="source-code"># values</p><p class="source-code">Trajectory(*(1, 2, 3)) == Trajectory(1, 2, 3)</p><p class="source-code"># The rollout(...) function returns a tuple of 3 values: </p><p class="source-code"># (obs, actions, rewards)</p><p class="source-code"># The Trajectory namedtuple can be used to collect </p><p class="source-code"># and store mini batch of experience to train the neuro </p><p class="source-code"># evolution agent</p><p class="source-code">trajectories = [Trajectory(*rollout(agent, env)) \</p><p class="source-code">                for _ in range(2)]</p></li>
				<li>Now it's time to choose <a id="_idIndexMarker081"/>elite experiences for the evolution process:<p class="source-code">def gather_elite_xp(trajectories, elitism_criterion):</p><p class="source-code">    """Gather elite trajectories from the batch of </p><p class="source-code">       trajectories</p><p class="source-code">    Args:</p><p class="source-code">        batch_trajectories (List): List of episode \</p><p class="source-code">        trajectories containing experiences (obs,</p><p class="source-code">                                  actions,episode_reward)</p><p class="source-code">    Returns:</p><p class="source-code">        elite_batch_obs</p><p class="source-code">        elite_batch_actions</p><p class="source-code">        elite_reard_threshold</p><p class="source-code">        </p><p class="source-code">    """</p><p class="source-code">    batch_obs, batch_actions, </p><p class="source-code">    batch_rewards = zip(*trajectories)</p><p class="source-code">    reward_threshold = np.percentile(batch_rewards,</p><p class="source-code">                                     elitism_criterion)</p><p class="source-code">    indices = [index for index, value in enumerate(</p><p class="source-code">             batch_rewards) if value &gt;= reward_threshold]</p><p class="source-code">    </p><p class="source-code">    elite_batch_obs = [batch_obs[i] for i in indices]</p><p class="source-code">    elite_batch_actions = [batch_actions[i] for i in \</p><p class="source-code">                            indices]</p><p class="source-code">    unpacked_elite_batch_obs = [item for items in \</p><p class="source-code">                       elite_batch_obs for item in items]</p><p class="source-code">    unpacked_elite_batch_actions = [item for items in \</p><p class="source-code">                   elite_batch_actions for item in items]</p><p class="source-code">    return np.array(unpacked_elite_batch_obs), \</p><p class="source-code">           np.array(unpacked_elite_batch_actions), \</p><p class="source-code">           reward_threshold</p></li>
				<li>Let's now test the elite<a id="_idIndexMarker082"/> experience gathering routine:<p class="source-code">elite_obs, elite_actions, reward_threshold = gather_elite_xp(trajectories, elitism_criterion=75)</p></li>
				<li>Let's now look at implementing a helper method to convert discrete action indices to one-hot encoded vectors or probability distribution over actions:<p class="source-code">def gen_action_distribution(action_index, action_dim=5):</p><p class="source-code">    action_distribution = np.zeros(action_dim).\</p><p class="source-code">                               astype(type(action_index))</p><p class="source-code">    action_distribution[action_index] = 1</p><p class="source-code">    action_distribution = \</p><p class="source-code">                   np.expand_dims(action_distribution, 0)</p><p class="source-code">    return action_distribution</p></li>
				<li>It's now time to test the action distribution generation function:<p class="source-code">elite_action_distributions = np.array([gen_action_distribution(a.item()) for a in elite_actions])</p></li>
				<li>Now, let's create and compile the neural network brain with TensorFlow 2.x using the Keras functional API:<p class="source-code">brain = Brain(env.action_space.n)</p><p class="source-code">brain.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])</p></li>
				<li>You can now test the brain training loop as follows:<p class="source-code">elite_obs, elite_action_distributions = elite_obs.astype("float16"), elite_action_distributions.astype("float16")</p><p class="source-code">brain.fit(elite_obs, elite_action_distributions, batch_size=128, epochs=1);</p><p>This should produce the <a id="_idIndexMarker083"/>following output:</p><p class="source-code">1/1 [==============================] - 0s 960us/step - loss: 0.8060 - accuracy: 0.4900</p><p class="callout-heading"> Note</p><p class="callout"> The numbers may vary.</p></li>
				<li>The next big step is to implement an agent class that can be initialized with a brain to act in an environment:<p class="source-code">class Agent(object):</p><p class="source-code">    def __init__(self, brain):</p><p class="source-code">        """Agent with a neural-network brain powered </p><p class="source-code">           policy</p><p class="source-code">        Args:</p><p class="source-code">            brain (keras.Model): Neural Network based \</p><p class="source-code">            model</p><p class="source-code">        """</p><p class="source-code">        self.brain = brain</p><p class="source-code">        self.policy = self.policy_mlp</p><p class="source-code">    def policy_mlp(self, observations):</p><p class="source-code">        observations = observations.reshape(1, -1)</p><p class="source-code">        action_logits = self.brain.process(observations)</p><p class="source-code">        action = tf.random.categorical(</p><p class="source-code">               tf.math.log(action_logits), num_samples=1)</p><p class="source-code">        return tf.squeeze(action, axis=1)</p><p class="source-code">    def get_action(self, observations):</p><p class="source-code">        return self.policy(observations)</p></li>
				<li>Next, we will implement a <a id="_idIndexMarker084"/>helper function to evaluate the agent in a given environment:<p class="source-code">def evaluate(agent, env, render=True):</p><p class="source-code">    obs, episode_reward, done, step_num = env.reset(),</p><p class="source-code">                                          0.0, False, 0</p><p class="source-code">    while not done:</p><p class="source-code">        action = agent.get_action(obs)</p><p class="source-code">        obs, reward, done, info = env.step(action)</p><p class="source-code">        episode_reward += reward</p><p class="source-code">        step_num += 1</p><p class="source-code">        if render:</p><p class="source-code">            env.render()</p><p class="source-code">    return step_num, episode_reward, done, info</p></li>
				<li>Let's now test the agent evaluation loop:<p class="source-code">env = GridworldEnv()</p><p class="source-code">agent = Agent(brain)</p><p class="source-code">for episode in tqdm(range(10)):</p><p class="source-code">    steps, episode_reward, done, info = evaluate(agent,</p><p class="source-code">                                                 env)</p><p class="source-code">env.close()</p></li>
				<li>As a next step, let's define the parameters for the training loop:<p class="source-code">total_trajectory_rollouts = 70</p><p class="source-code">elitism_criterion = 70  # percentile</p><p class="source-code">num_epochs = 200</p><p class="source-code">mean_rewards = []</p><p class="source-code">elite_reward_thresholds = []</p></li>
				<li>Let's now<a id="_idIndexMarker085"/> create the <strong class="source-inline">environment</strong>, <strong class="source-inline">brain</strong>, and <strong class="source-inline">agent</strong> objects:<p class="source-code">env = GridworldEnv()</p><p class="source-code">input_shape = (env.observation_space.shape[0] * \</p><p class="source-code">               env.observation_space.shape[1], )</p><p class="source-code">brain = Brain(env.action_space.n)</p><p class="source-code">brain.compile(loss="categorical_crossentropy",</p><p class="source-code">              optimizer="adam", metrics=["accuracy"])</p><p class="source-code">agent = Agent(brain)</p><p class="source-code">for i in tqdm(range(num_epochs)):</p><p class="source-code">    trajectories = [Trajectory(*rollout(agent, env)) \</p><p class="source-code">               for _ in range(total_trajectory_rollouts)]</p><p class="source-code">    _, _, batch_rewards = zip(*trajectories)</p><p class="source-code">    elite_obs, elite_actions, elite_threshold = \</p><p class="source-code">                   gather_elite_xp(trajectories, </p><p class="source-code">                   elitism_criterion=elitism_criterion)</p><p class="source-code">    elite_action_distributions = \</p><p class="source-code">        np.array([gen_action_distribution(a.item()) \</p><p class="source-code">                     for a in elite_actions])</p><p class="source-code">    elite_obs, elite_action_distributions = \</p><p class="source-code">        elite_obs.astype("float16"), </p><p class="source-code">        elite_action_distributions.astype("float16")</p><p class="source-code">    brain.fit(elite_obs, elite_action_distributions, </p><p class="source-code">              batch_size=128, epochs=3, verbose=0);</p><p class="source-code">    mean_rewards.append(np.mean(batch_rewards))</p><p class="source-code">    elite_reward_thresholds.append(elite_threshold)</p><p class="source-code">    print(f"Episode#:{i + 1} elite-reward-\</p><p class="source-code">          threshold:{elite_reward_thresholds[-1]:.2f} \</p><p class="source-code">          reward:{mean_rewards[-1]:.2f} ")</p><p class="source-code">plt.plot(mean_rewards, 'r', label="mean_reward")</p><p class="source-code">plt.plot(elite_reward_thresholds, 'g', </p><p class="source-code">         label="elites_reward_threshold")</p><p class="source-code">plt.legend()</p><p class="source-code">plt.grid()</p><p class="source-code">plt.show()</p><p>This will generate a plot like the<a id="_idIndexMarker086"/> one shown in the following diagram:</p><p class="callout-heading">Important note</p><p class="callout">The episode rewards will vary and the plots may look different.</p></li>
			</ol>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B15074_01_014.jpg" alt="Figure 1.14 – Plot of the mean reward (solid, red) and reward threshold for elites (dotted, green) "/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14 – Plot of the mean reward (solid, red) and reward threshold for elites (dotted, green)</p>
			<p>The solid line in the plot is the<a id="_idIndexMarker087"/> mean reward obtained by the neural evolutionary agent, and the dotted line shows the reward threshold used for determining the elites.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor042"/>How it works…</h2>
			<p>On every iteration, the evolutionary process rolls out or collects a bunch of trajectories to build up the experience data using the current set of neural weights in the agent's brain. An elite selection process is then employed that picks the top <em class="italic">k</em> percentile (elitism criterion) trajectories/experiences based on the episode reward obtained in that trajectory. This shortlisted <a id="_idIndexMarker088"/>experience data is then used to update the agent's brain model. The process repeats for a preset number of iterations allowing the agent's brain model to improve and collect more rewards.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>See also</h2>
			<p>For more information, I suggest reading <em class="italic">The CMA Evolution Strategy: A Tutorial</em>: <a href="https://arxiv.org/pdf/1604.00772.pdf">https://arxiv.org/pdf/1604.00772.pdf</a>.</p>
		</div>
		<div>
			<div id="_idContainer025">
			</div>
		</div>
</body></html>