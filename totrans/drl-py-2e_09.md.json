["```\nimport random\nimport gym\nimport numpy as np\nfrom collections import deque\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.optimizers import Adam \n```", "```\nenv = gym.make(\"MsPacman-v0\") \n```", "```\nstate_size = (88, 80, 1) \n```", "```\naction_size = env.action_space.n \n```", "```\ncolor = np.array([210, 164, 74]).mean()\ndef preprocess_state(state): \n```", "```\n image = state[1:176:2, ::2] \n```", "```\n image = image.mean(axis=2) \n```", "```\n image[image==color] = 0 \n```", "```\n image = (image - 128) / 128 - 1 \n```", "```\n image = np.expand_dims(image.reshape(88, 80, 1), axis=0)\n    return image \n```", "```\nclass DQN: \n```", "```\n def __init__(self, state_size, action_size): \n```", "```\n self.state_size = state_size \n```", "```\n self.action_size = action_size \n```", "```\n self.replay_buffer = deque(maxlen=5000) \n```", "```\n self.gamma = 0.9 \n```", "```\n self.epsilon = 0.8 \n```", "```\n self.update_rate = 1000 \n```", "```\n self.main_network = self.build_network() \n```", "```\n self.target_network = self.build_network() \n```", "```\n self.target_network.set_weights(self.main_network.get_weights()) \n```", "```\n def build_network(self): \n```", "```\n model = Sequential()\n        model.add(Conv2D(32, (8, 8), strides=4, padding='same', input_shape=self.state_size))\n        model.add(Activation('relu')) \n```", "```\n model.add(Conv2D(64, (4, 4), strides=2, padding='same'))\n        model.add(Activation('relu')) \n```", "```\n model.add(Conv2D(64, (3, 3), strides=1, padding='same'))\n        model.add(Activation('relu')) \n```", "```\n model.add(Flatten()) \n```", "```\n model.add(Dense(512, activation='relu'))\n        model.add(Dense(self.action_size, activation='linear')) \n```", "```\n model.compile(loss='mse', optimizer=Adam()) \n```", "```\n return model \n```", "```\n def store_transistion(self, state, action,\n                          reward, next_state, done):\n        self.replay_buffer.append((state, action,\n                                   reward, next_state, done)) \n```", "```\n def epsilon_greedy(self, state):\n        if random.uniform(0,1) < self.epsilon:\n            return np.random.randint(self.action_size)\n        Q_values = self.main_network.predict(state)\n        return np.argmax(Q_values[0]) \n```", "```\n def train(self, batch_size): \n```", "```\n minibatch = random.sample(self.replay_buffer, batch_size) \n```", "```\n for state, action, reward, next_state, done in minibatch:\n            if not done:\n                target_Q = (reward + self.gamma * np.amax(\n                    self.target_network.predict(next_state)))\n            else:\n                target_Q = reward \n```", "```\n Q_values = self.main_network.predict(state) \n```", "```\n Q_values[0][action] = target_Q \n```", "```\n self.main_network.fit(state, Q_values, epochs=1, \n                                  verbose=0) \n```", "```\n def update_target_network(self):\n        self.target_network.set_weights(self.main_network.get_weights()) \n```", "```\nnum_episodes = 500 \n```", "```\nnum_timesteps = 20000 \n```", "```\nbatch_size = 8 \n```", "```\nnum_screens = 4 \n```", "```\ndqn = DQN(state_size, action_size) \n```", "```\ndone = False \n```", "```\ntime_step = 0 \n```", "```\nfor i in range(num_episodes): \n```", "```\n Return = 0 \n```", "```\n state = preprocess_state(env.reset()) \n```", "```\n for t in range(num_timesteps): \n```", "```\n env.render() \n```", "```\n time_step += 1 \n```", "```\n if time_step % dqn.update_rate == 0:\n            dqn.update_target_network() \n```", "```\n action = dqn.epsilon_greedy(state) \n```", "```\n next_state, reward, done, _ = env.step(action) \n```", "```\n next_state = preprocess_state(next_state) \n```", "```\n dqn.store_transistion(state, action, reward, next_state, done) \n```", "```\n state = next_state \n```", "```\n Return += reward \n```", "```\n if done:\n            print('Episode: ',i, ',' 'Return', Return)\n            break \n```", "```\n if len(dqn.replay_buffer) > batch_size:\n            dqn.train(batch_size) \n```"]