<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Preface</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span><strong>Reinforcement learning</strong> (<strong>RL</strong>) is a popular and promising branch of artificial intelligence that involves making smarter models and agents that can automatically determine ideal behavior based on changing requirements. <em>Reinforcement Learning Algorithms with Python</em> will help you master RL algorithms and understand their implementation as you build self-learning agents.</span><br/>
<br/>
<span>Starting with an introduction to the tools, libraries, and setup needed to work in the RL environment, this book covers the building blocks of RL and delves into value-based methods such as the application of Q-learning and SARSA algorithms. You'll learn how to use a combination of Q-learning and neural networks to solve complex problems. Furthermore, you'll study policy gradient methods, TRPO, and PPO, to improve performance and stability, before moving on to the DDPG and TD3 deterministic algorithms. This book also covers how imitation learning techniques work and how Dagger can teach an agent to fly. You'll discover evolutionary strategies and black-box optimization techniques. Finally, you'll get to grips with exploration approaches such as UCB and UCB1 and develop a meta-algorithm called ESBAS.</span><br/>
<br/>
<span>By the end of the book, you'll have worked with key RL algorithms to overcome challenges in real-world applications, and you'll be part of the RL research community.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Who this book is for</h1>
                </header>
            
            <article>
                
<p>If you are an AI researcher, deep learning user, or anyone who wants to learn RL from scratch, this book is for you. You'll also find this RL book useful if you want to learn about the advancements in the field. Working knowledge of Python is necessary.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What this book covers</h1>
                </header>
            
            <article>
                
<p><a href="0469636f-09ff-417a-84ec-3fc0e4b4be08.xhtml">Chapter 1</a>, <em>The Landscape of Reinforcement Learning</em>, gives you an insight into RL. It describes the problems that RL is good at solving and the applications where RL algorithms are already adopted. It also introduces the tools, the libraries, and the setup needed for the completion of the projects in the following chapters.</p>
<p><a href="716e2db3-37c0-4f2c-8397-81c0c812c80e.xhtml">Chapter 2</a>,<em> Implementing RL Cycle and OpenAI Gym</em>, <span>describes the main cycle of the RL algorithms, the toolkit used to develop the algorithms, and the different types of environments. You will be able to develop a random agent using the OpenAI Gym interface to play CartPole using random actions. You will also learn how to use the OpenAI Gym interface to run other environments.</span></p>
<p><a href="f2414b11-976a-4410-92d8-89ee54745d99.xhtml">Chapter 3</a>, <em>Solving Problems with Dynamic Programming</em>, i<span>ntroduces to you the core ideas, terminology, and approaches of RL. You will learn about the main blocks of RL and develop a general idea about how RL algorithms can be created to solve a problem. You will also learn the differences between model-based and model-free algorithms and the categorization of reinforcement learning algorithms. Dynamic programming will be used to solve the game FrozenLake.</span></p>
<p><a href="6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml">Chapter 4</a>, <em>Q-Learning and SARSA Applications</em>, talks <span>about value-based methods, in particular Q-learning and SARSA, two algorithms that differ from dynamic programming and scale well on large problems. To become confident with these algorithms, you will apply them to the FrozenLake game and study the differences from dynamic programming.</span></p>
<p><a href="b2fa8158-6d3c-469a-964d-a800942472ca.xhtml"/><a href="b2fa8158-6d3c-469a-964d-a800942472ca.xhtml">Chapter 5</a>, <em>Deep Q-Networks</em>, describes how <span>neural networks and <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) in particular are applied to Q-learning. You'll learn why the combination of Q-learning and neural networks produces incredible results and how its use can open the door to a much larger variety of problems. Furthermore, you'll apply the DQN to an Atari game using the OpenAI Gym interface.</span></p>
<p><a href="6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml">Chapter 6</a>, <em>Learning Stochastic and PG Optimization</em>, <span>introduces a new family of model-free algorithms: policy gradient methods. You will learn the differences between policy gradient and value-based methods, and you'll learn about their strengths and weaknesses. Then you will implement the REINFORCE and Actor-Critic algorithms to solve a new game called LunarLander.</span></p>
<p><a href="4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml">Chapter 7</a>, <em>TRPO and PPO Implementation</em>, proposes a modification of policy gradient methods using new mechanisms to control the improvement of the policy. These mechanisms are used to improve the stability and convergence of the policy gradient algorithms. In particular you'll learn and implement two main policy gradient methods that use these techniques, namely TRPO and PPO.<span> You will implement them on RoboSchool, an environment with a continuous action space.</span></p>
<p><a href="d902d278-a1f5-438a-9ddd-6d9d665f2fa2.xhtml">Chapter 8</a>, <em>DDPG and TD3 Applications</em>, introduces a new category of algorithms called deterministic policy algorithms that combine both policy gradient and Q-learning. You will learn about the underlying concepts and implement DDPG and TD3, two deep deterministic algorithms, on a new environment.</p>
<p><a href="7e8448cf-7b74-4f07-99bd-da8f98f4505c.xhtml">Chapter 9</a>, <em>Model-Based RL</em>, <span>illustrates RL algorithms that learn the model of the environment to plan future actions, or, to learn a policy. You will be taught how they work, their strengths, and why they are preferred in many situations. To master them, you will implement a model-based algorithm on Roboschool.</span></p>
<p><a href="e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml">Chapter 10</a>, <em>Imitation Learning with the DAgger Algorithm</em>, explains how imitation learning works and how it can be applied and adapted to a problem. You will learn about the most well-known imitation learning algorithm, DAgger. To become confident with it, you will implement it to speed up the learning process of an agent on FlappyBird.</p>
<p><a href="dab022a7-3243-4e45-9f91-39a82df3a248.xhtml">Chapter 11</a>, <em>Understanding Black-Box Optimization Algorithms</em>, <span>explores evolutionary algorithms, a class of black-box optimization algorithms that don't rely on backpropagation. These algorithms are gaining interest because of their fast training and easy parallelization across hundreds or thousands of cores. This chapter provides a theoretical and practical background of these algorithms by focusing particularly on the Evolution Strategy algorithm, a type of evolutionary algorithm.</span></p>
<p><a href="800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml">Chapter 12</a>, <em>Developing ESBAS Algorithm</em>, <span>introduces the important exploration-exploitation dilemma, which is specific to RL. The dilemma is demonstrated using the multi-armed bandit problem and is solved using approaches such as UCB and UCB1. Then, you will learn about the problem of algorithm selection and develop a meta-algorithm called ESBAS. This algorithm uses UCB1 to select the most appropriate RL algorithm for each situation.</span></p>
<p><a href="719f001d-db10-47a4-98c5-fe95666f7c32.xhtml">Chapter 13</a>, <em>Practical Implementations to Resolve RL Challenges</em>, <span>takes a look at the major challenges in this field and explains some practices and methods to overcome them. You will also learn about some of the challenges of applying RL to real-world problems, future developments of deep RL, and their social impact in the world.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">To get the most out of this book</h1>
                </header>
            
            <article>
                
<p>Working knowledge of Python is necessary. Knowledge of RL and the various tools used for it will also be beneficial.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the example code files</h1>
                </header>
            
            <article>
                
<p>You can download the example code files for this book from your account at <a href="http://www.packt.com" target="_blank">www.packt.com</a>. If you purchased this book elsewhere, you can visit <a href="https://www.packtpub.com/support">www.packtpub.com/support</a> and register to have the files emailed directly to you.</p>
<p>You can download the code files by following these steps:</p>
<ol>
<li>Log in or register at <a href="http://www.packt.com" target="_blank">www.packt.com</a>.</li>
<li>Select the <span class="packt_screen">Support</span> tab.</li>
<li>Click on <span class="packt_screen">Code Downloads</span>.</li>
<li>Enter the name of the book in the <span class="packt_screen">Search</span> box and follow the onscreen instructions.</li>
</ol>
<p>Once the file is downloaded, please make sure that you unzip or extract the folder using the latest version of:</p>
<ul>
<li>WinRAR/7-Zip for Windows</li>
<li>Zipeg/iZip/UnRarX for Mac</li>
<li>7-Zip/PeaZip for Linux</li>
</ul>
<p><span>The code bundle for the book is also hosted on GitHub at</span><span> <a href="https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python">https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python</a></span><span>. </span><span>In case there's an update to the code, it will be updated on the existing GitHub repository.</span></p>
<p><span>We also have other code bundles from our rich catalog of books and videos available at</span><span> </span><strong><span class="Object"><a href="https://github.com/PacktPublishing/" target="_blank">https://github.com/PacktPublishing/</a></span></strong><span>. Check them out!</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Download the color images</h1>
                </header>
            
            <article>
                
<p>We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf">http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf</a><a href="http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf">.</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Conventions used</h1>
                </header>
            
            <article>
                
<p>There are a number of text conventions used throughout this book.</p>
<p><kbd>CodeInText</kbd>: <span>Indicates c</span>ode words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. <span>Here is an example:</span> "<span>In this book, we use Python 3.7, but all versions above 3.5 should work. We also assume that you've already installed </span><kbd>numpy</kbd><span> and </span><kbd>matplotlib</kbd><span>.</span>"</p>
<p>A block of code is set as follows:</p>
<pre>import gym<br/><br/># create the environment <br/>env = gym.make("CartPole-v1")<br/># reset the environment before starting<br/>env.reset()<br/><br/># loop 10 times<br/>for i in range(10):<br/>    # take a random action<br/>    env.step(env.action_space.sample())<br/>    # render the game<br/>   env.render()<br/><br/># close the environment<br/>env.close()</pre>
<p>Any command-line input or output is written as follows:</p>
<pre><strong>$ git clone https://github.com/pybox2d/pybox2d<a href="https://github.com/pybox2d/pybox2d"><br/></a>$ cd pybox2d</strong><br/><strong>$ pip install -e .</strong></pre>
<p><strong>Bold</strong>: Indicates a new term, an important word, or w<span>ords that you see onscreen. For example, words in menus or dialog boxes appear in the text like this. Here is an example: "In <strong>reinforcement learning</strong> (<strong>RL</strong>), the algorithm is called the agent, and it learns from the data provided by an environment.</span><span>"</span></p>
<div class="packt_infobox">Warnings or important notes appear like this.</div>
<div class="packt_tip">Tips and tricks appear like this.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Get in touch</h1>
                </header>
            
            <article>
                
<p>Feedback from our readers is always welcome.</p>
<p class="mce-root"><strong>General feedback</strong>: If you have questions about any aspect of this book, <span>mention the book title in the subject of your message and</span> email us at <kbd><span>customercare@packtpub.com</span></kbd>.</p>
<p><strong>Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you would report this to us. Please visit <a href="https://www.packtpub.com/support/errata" target="_blank">www.packtpub.com/support/errata</a>, selecting your book, clicking on the Errata Submission Form link, and entering the details.</p>
<p><strong>Piracy</strong>: If you come across any illegal copies of our works in any form on the Internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <kbd>copyright@packt.com</kbd> with a link to the material.</p>
<p class="mce-root"><strong>If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com/" target="_blank">authors.packtpub.com</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviews</h1>
                </header>
            
            <article>
                
<p>Please leave a review. Once you have read and used this book, why not leave a review on the site that you purchased it from? Potential readers can then see and use your unbiased opinion to make purchase decisions, we at Packt can understand what you think about our products, and our authors can see your feedback on their book. Thank you!</p>
<p>For more information about Packt, please visit <a href="http://www.packt.com/" target="_blank">packt.com</a>.</p>


            </article>

            
        </section>
    </body></html>