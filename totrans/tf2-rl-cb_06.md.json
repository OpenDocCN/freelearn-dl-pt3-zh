["```\n    import os\n    import gym\n    from PIL import Image\n    from miniwob.action import MiniWoBCoordClick\n    from miniwob.environment import MiniWoBEnvironment\n    ```", "```\n    cur_path_dir = \\\n        os.path.dirname(os.path.realpath(__file__))\n    miniwob_dir = os.path.join(cur_path_dir, \"miniwob\",\n                               \"html\", \"miniwob\")\n    ```", "```\n    class MiniWoBEnv(MiniWoBEnvironment, gym.Env):\n        def __init__(\n            self,\n            env_name: str,\n            obs_im_shape,\n            num_instances: int = 1,\n            miniwob_dir: str = miniwob_dir,\n            seeds: list = [1],\n        ):\n            super().__init__(env_name)\n            self.base_url = f\"file://{miniwob_dir}\"\n            self.configure(num_instances=num_instances,\n                         seeds=seeds, base_url=self.base_url)\n            # self.set_record_screenshots(True)\n            self.obs_im_shape = obs_im_shape\n    ```", "```\n    def reset(self, seeds=[1], mode=None, \n    record_screenshots=False):\n            \"\"\"Forces stop and start all instances.\n            Args:\n                seeds (list[object]): Random seeds to set for \n                each instance;\n                    If specified, len(seeds) must be equal to\n                    the number of instances.\n                    A None entry in the list = do not set a \n                    new seed.\n                mode (str): If specified, set the data mode \n                    to this value before starting new\n                    episodes.\n                record_screenshots (bool): Whether to record \n                    screenshots of the states.\n            Returns:\n                states (list[MiniWoBState])\n            \"\"\"\n            miniwob_state = super().reset(seeds, mode, \n                      record_screenshots=True)\n            return [\n                state.screenshot.resize(self.obs_im_shape, \n                                        Image.ANTIALIAS)\n                for state in miniwob_state\n            ]\n    ```", "```\n        def step(self, actions):\n            \"\"\"Applies an action on each instance and returns \n            the results.\n            Args:\n                actions (list[MiniWoBAction or None])\n            Returns:\n                tuple (states, rewards, dones, info)\n                    states (list[PIL.Image.Image])\n                    rewards (list[float])\n                    dones (list[bool])\n                    info (dict): additional debug \n                    information.\n                        Global debug information is directly \n                        in the root level\n                        Local information for instance i is \n                        in info['n'][i]\n            \"\"\"\n    ```", "```\n            states, rewards, dones, info = \\\n                                    super().step(actions)\n            # Obtain screenshot & Resize image obs to match \n            # config\n            img_states = [\n                state.screenshot.resize(self.obs_im_shape) \\\n                if not dones[i] else None\n                for i, state in enumerate(states)\n            ]\n            return img_states, rewards, dones, info\n    ```", "```\n    if __name__ == \"__main__\":\n        env = MiniWoBVisualEnv(\"click-pie\")\n        for _ in range(10):\n            obs = env.reset()\n            done = False\n            while not done:\n                action = [MiniWoBCoordClick(90, 150)]\n                obs, reward, done, info = env.step(action)\n                [ob.show() for ob in obs if ob is not None]\n        env.close()\n    ```", "```\n    import gym.spaces\n    import numpy as np\n    import string\n    from miniwob_env import MiniWoBEnv\n    from miniwob.action import MiniWoBCoordClick, MiniWoBType\n    ```", "```\n    class MiniWoBVisualClickEnv(MiniWoBEnv):\n        def __init__(self, name, num_instances=1):\n            \"\"\"RL environment with visual observations and \n               touch/mouse-click action space\n                Two dimensional, continuous-valued action \n                space allows Agents to specify (x, y)\n                coordinates on the visual rendering to click/\n                touch to interact with the world-of bits\n            Args:\n                name (str): Name of the supported \\\n                MiniWoB-PlusPlus environment\n                num_instances (int, optional): Number of \\\n                parallel env instances. Defaults to 1.\n            \"\"\"\n            self.miniwob_env_name = name\n            self.task_width = 160\n            self.task_height = 210\n            self.obs_im_width = 64\n            self.obs_im_height = 64\n            self.num_channels = 3  # RGB\n            self.obs_im_size = (self.obs_im_width, \\\n                                self.obs_im_height)\n            super().__init__(self.miniwob_env_name, \n                             self.obs_im_size, \n                             num_instances)\n    ```", "```\n            self.observation_space = gym.spaces.Box(\n                0,\n                255,\n                (self.obs_im_width, self.obs_im_height, \n                 self.num_channels),\n                dtype=int,\n            )\n            self.action_space = gym.spaces.Box(\n                low=np.array([0, 0]),\n                high=np.array([self.task_width, \n                               self.task_height]),\n                shape=(2,),\n                dtype=int,\n            )\n    ```", "```\n        def reset(self, seeds=[1]):\n            \"\"\"Forces stop and start all instances.\n            Args:\n                seeds (list[object]): Random seeds to set for \n                each instance;\n                    If specified, len(seeds) must be equal to \n                    the number of instances.\n                    A None entry in the list = do not set a \n                    new seed.\n            Returns:\n                states (list[PIL.Image])\n            \"\"\"\n            obs = super().reset(seeds)\n            # Click somewhere to Start!\n            # miniwob_state, _, _, _ = super().step(\n            # self.num_instances * [MiniWoBCoordClick(10,10)]\n            # )\n            return obs\n    ```", "```\n        def step(self, actions):\n            \"\"\"Applies an action on each instance and returns \n               the results.\n            Args:\n                actions (list[(x, y) or None]);\n                  - x is the number of pixels from the left\n                    of browser window\n                  - y is the number of pixels from the top of \n                    browser window\n            Returns:\n                tuple (states, rewards, dones, info)\n                    states (list[PIL.Image.Image])\n                    rewards (list[float])\n                    dones (list[bool])\n                    info (dict): additional debug \n                    information.\n                        Global debug information is directly \n                        in the root level\n                        Local information for instance i is \n                        in info['n'][i]\n            \"\"\"\n    ```", "```\n            assert (\n                len(actions) == self.num_instances\n            ), f\"Expected len(actions)={self.num_instances}.\\\n                 Got {len(actions)}.\"\n            def clamp(action, low=self.action_space.low,\\\n                       high=self.action_space.high):\n                low_x, low_y = low\n                high_x, high_y = high\n                return (\n                    max(low_x, min(action[0], high_x)),\n                    max(low_y, min(action[1], high_y)),\n                )\n            miniwob_actions = \\\n                [MiniWoBCoordClick(*clamp(action)) if action\\\n                is not None else None for action in actions]\n            return super().step(miniwob_actions)\n    ```", "```\n    class MiniWoBClickButtonVisualEnv(MiniWoBVisualClickEnv):\n        def __init__(self, num_instances=1):\n            super().__init__(\"click-button\", num_instances)\n    ```", "```\n    import sys\n    import os\n    from gym.envs.registration import register\n    sys.path.append(os.path.dirname(os.path.abspath(__file__)))\n    _AVAILABLE_ENVS = {\n        \"MiniWoBClickButtonVisualEnv-v0\": {\n            \"entry_point\": \\\n                 \"webgym.envs:MiniWoBClickButtonVisualEnv\",\n            \"discription\": \"Click the button on a web page\",\n        }\n    }\n    for env_id, val in _AVAILABLE_ENVS.items():\n        register(id=env_id, \n                 entry_point=val.get(\"entry_point\"))\n    ```", "```\nimport argparse\nimport os\nfrom datetime import datetime\nimport gym\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (Conv2D,Dense,Dropout,Flatten,Input,Lambda,MaxPool2D,)\nimport webgym  # Used to register webgym environments\n```", "```\n    parser = argparse.ArgumentParser(prog=\"TFRL-Cookbook-Ch5-Click-To-Action-Agent\")\n    parser.add_argument(\"--env\", default=\"MiniWoBClickButtonVisualEnv-v0\")\n    parser.add_argument(\"--update-freq\", type=int, default=16)\n    parser.add_argument(\"--epochs\", type=int, default=3)\n    parser.add_argument(\"--actor-lr\", type=float, default=1e-4)\n    parser.add_argument(\"--Critic-lr\", type=float, default=1e-4)\n    parser.add_argument(\"--clip-ratio\", type=float, default=0.1)\n    parser.add_argument(\"--gae-lambda\", type=float, default=0.95)\n    parser.add_argument(\"--gamma\", type=float, default=0.99)\n    parser.add_argument(\"--logdir\", default=\"logs\")\n    ```", "```\n    args = parser.parse_args()\n    logdir = os.path.join(\n        args.logdir, parser.prog, args.env, \\\n        datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    )\n    print(f\"Saving training logs to:{logdir}\")\n    writer = tf.summary.create_file_writer(logdir)\n    ```", "```\n    class Actor:\n        def __init__(self, state_dim, action_dim, \n        action_bound, std_bound):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.action_bound = np.array(action_bound)\n            self.std_bound = std_bound\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.eps = 1e-5\n            self.model = self.nn_model()\n            self.model.summary()  # Print a summary of the \n            @ Actor model\n            self.opt = \\\n                tf.keras.optimizers.Nadam(args.actor_lr)\n    ```", "```\n        def nn_model(self):\n            obs_input = Input(self.state_dim)\n            conv1 = Conv2D(\n                filters=64,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"same\",\n                input_shape=self.state_dim,\n                data_format=\"channels_last\",\n                activation=\"relu\",\n            )(obs_input)\n            pool1 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                              (conv1)\n            conv2 = Conv2D(\n                filters=32,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool1)\n            pool2 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                             (conv2)\n    ```", "```\n            flat = Flatten()(pool2)\n            dense1 = Dense(\n                16, activation=\"relu\", \\\n                   kernel_initializer=self.weight_initializer\n            )(flat)\n            dropout1 = Dropout(0.3)(dense1)\n            dense2 = Dense(\n                8, activation=\"relu\", \\\n                   kernel_initializer=self.weight_initializer\n            )(dropout1)\n            dropout2 = Dropout(0.3)(dense2)\n            # action_dim[0] = 2\n            output_val = Dense(\n                self.action_dim[0],\n                activation=\"relu\",\n                kernel_initializer=self.weight_initializer,\n            )(dropout2)\n    ```", "```\n            mu_output = Lambda(\n                lambda x: tf.clip_by_value(x * \\\n                  self.action_bound, 1e-9, self.action_bound)\n            )(output_val)\n            std_output_1 = Dense(\n                self.action_dim[0],\n                activation=\"softplus\",\n                kernel_initializer=self.weight_initializer,\n            )(dropout2)\n            std_output = Lambda(\n                lambda x: tf.clip_by_value(\n                    x * self.action_bound, 1e-9, \\\n                    self.action_bound / 2\n                )\n            )(std_output_1)\n            return tf.keras.models.Model(\n                inputs=obs_input, outputs=[mu_output, std_output], name=\"Actor\"\n            )\n    ```", "```\n        def get_action(self, state):\n            # Convert [Image] to np.array(np.adarray)\n            state_np = np.array([np.array(s) for s in state])\n            if len(state_np.shape) == 3:\n                # Convert (w, h, c) to (1, w, h, c)\n                state_np = np.expand_dims(state_np, 0)\n            mu, std = self.model.predict(state_np)\n            action = np.random.normal(mu, std + self.eps, \\\n                                size=self.action_dim).astype(\n                \"int\"\n            )\n            # Clip action to be between 0 and max obs screen \n            # size\n            action = np.clip(action, 0, self.action_bound)\n            # 1 Action per instance of env; Env expects: \n            # (num_instances, actions)\n            action = (action,)\n            log_policy = self.log_pdf(mu, std, action)\n            return log_policy, action\n    ```", "```\n        def train(self, log_old_policy, states, actions, \n        gaes):\n            with tf.GradientTape() as tape:\n                mu, std = self.model(states, training=True)\n                log_new_policy = self.log_pdf(mu, std, \n                                              actions)\n                loss = self.compute_loss(log_old_policy, \n                               log_new_policy, actions, gaes)\n            grads = tape.gradient(loss, \n                              self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \n                             self.model.trainable_variables))\n            return loss\n    ```", "```\n        def compute_loss(self, log_old_policy, \n        log_new_policy, actions, gaes):\n            # Avoid INF in exp by setting 80 as the upper \n            # bound since,\n            # tf.exp(x) for x>88 yeilds NaN (float32)\n            ratio = tf.exp(\n                tf.minimum(log_new_policy - \\\n                    tf.stop_gradient(log_old_policy), 80)\n            )\n            gaes = tf.stop_gradient(gaes)\n            clipped_ratio = tf.clip_by_value(\n                ratio, 1.0 - args.clip_ratio, 1.0 + \\\n                args.clip_ratio\n            )\n            surrogate = -tf.minimum(ratio * gaes, \\\n                                    clipped_ratio * gaes)\n            return tf.reduce_mean(surrogate)\n    ```", "```\n        def log_pdf(self, mu, std, action):\n            std = tf.clip_by_value(std, self.std_bound[0],\n                                   self.std_bound[1])\n            var = std ** 2\n            log_policy_pdf = -0.5 * (action - mu) ** 2 / var\\\n                             - 0.5 * tf.math.log(\n                var * 2 * np.pi\n            )\n            return tf.reduce_sum(log_policy_pdf, 1,\n                                 keepdims=True)\n    ```", "```\n    class Critic:\n        def __init__(self, state_dim):\n            self.state_dim = state_dim\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.model = self.nn_model()\n            self.model.summary()  # Print a summary of the \n            # Critic model\n            self.opt = \\\n                tf.keras.optimizers.Nadam(args.Critic_lr)\n    ```", "```\n            obs_input = Input(self.state_dim)\n            conv1 = Conv2D(\n                filters=64,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"same\",\n                input_shape=self.state_dim,\n                data_format=\"channels_last\",\n                activation=\"relu\",\n            )(obs_input)\n            pool1 = MaxPool2D(pool_size=(3, 3), strides=2)\\\n                              (conv1)\n            conv2 = Conv2D(\n                filters=32,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool1)\n            pool2 = MaxPool2D(pool_size=(3, 3), strides=2)\\\n                              (conv2)\n    ```", "```\n            flat = Flatten()(pool2)\n            dense1 = Dense(\n                16, activation=\"relu\", \\\n                   kernel_initializer=self.weight_initializer\n            )(flat)\n            dropout1 = Dropout(0.3)(dense1)\n            dense2 = Dense(\n                8, activation=\"relu\", \\\n                   kernel_initializer=self.weight_initializer\n            )(dropout1)\n            dropout2 = Dropout(0.3)(dense2)\n            value = Dense(\n                1, activation=\"linear\", \\\n                   kernel_initializer=self.weight_initializer\n            )(dropout2)\n    ```", "```\n        def compute_loss(self, v_pred, td_targets):\n            mse = tf.keras.losses.MeanSquaredError()\n            return mse(td_targets, v_pred)\n    ```", "```\n        def train(self, states, td_targets):\n            with tf.GradientTape() as tape:\n                v_pred = self.model(states, training=True)\n                # assert v_pred.shape == td_targets.shape\n                loss = self.compute_loss(v_pred, \\\n                                tf.stop_gradient(td_targets))\n            grads = tape.gradient(loss, \\\n                              self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \\\n                             self.model.trainable_variables))\n            return loss\n    ```", "```\n    class PPOAgent:\n        def __init__(self, env):\n            self.env = env\n            self.state_dim = self.env.observation_space.shape\n            self.action_dim = self.env.action_space.shape\n            # Set action_bounds to be within the actual \n            # task-window/browser-view of the Agent\n            self.action_bound = [self.env.task_width, \n                                 self.env.task_height]\n            self.std_bound = [1e-2, 1.0]\n            self.actor = Actor(\n                self.state_dim, self.action_dim, \n                self.action_bound, self.std_bound\n            )\n            self.Critic = Critic(self.state_dim)\n    ```", "```\n        def gae_target(self, rewards, v_values, next_v_value, \n        done):\n            n_step_targets = np.zeros_like(rewards)\n            gae = np.zeros_like(rewards)\n            gae_cumulative = 0\n            forward_val = 0\n            if not done:\n                forward_val = next_v_value\n            for k in reversed(range(0, len(rewards))):\n                delta = rewards[k] + args.gamma * \\\n                        forward_val - v_values[k]\n                gae_cumulative = args.gamma * \\\n                                 args.gae_lambda * \\\n                                 gae_cumulative + delta\n                gae[k] = gae_cumulative\n                forward_val = v_values[k]\n                n_step_targets[k] = gae[k] + v_values[k]\n            return gae, n_step_targets\n    ```", "```\n        def train(self, max_episodes=1000):\n            with writer.as_default():\n                for ep in range(max_episodes):\n                    state_batch = []\n                    action_batch = []\n                    reward_batch = []\n                    old_policy_batch = []\n                    episode_reward, done = 0, False\n                    state = self.env.reset()\n                    prev_state = state\n                    step_num = 0\n    ```", "```\n                        while not done:\n                            log_old_policy, action = \\\n                                 self.actor.get_action(state)\n                        next_state, reward, dones, _ = \\\n                             self.env.step(action)\n                        step_num += 1\n                        print(\n                            f\"ep#:{ep} step#:{step_num} \\\n                            step_rew:{reward} \\\n                            action:{action} dones:{dones}\"\n                        )\n                        done = np.all(dones)\n                        if done:\n                            next_state = prev_state\n                        else:\n                            prev_state = next_state\n                        state = np.array([np.array(s) for s\\\n                                          in state])\n                        next_state = np.array([np.array(s) \\\n                                        for s in next_state])\n                        reward = np.reshape(reward, [1, 1])\n                        log_old_policy = np.reshape(\n                                      log_old_policy, [1, 1])\n                        state_batch.append(state)\n                        action_batch.append(action)\n                        reward_batch.append((reward + 8) / 8)\n                        old_policy_batch.append(\n                                              log_old_policy)\n    ```", "```\n                        if len(state_batch) >= \\\n                        args.update_freq or done:\n                            states = \\\n                                np.array([state.squeeze() \\\n                                for state in state_batch])\n                            # Convert ([x, y],) to [x, y]\n                            actions = np.array([action[0] \\\n                                for action in action_batch])\n                            rewards = np.array(\n                                [reward.squeeze() for reward\\\n                                 in reward_batch]\n                            )\n                            old_policies = np.array(\n                                [old_pi.squeeze() for old_pi\\\n                                 in old_policy_batch]\n                            )\n                            v_values = self.Critic.model.\\\n                                predict(states)\n                            next_v_value = self.Critic.\\\n                                model.predict(next_state)\n                            gaes, td_targets = \\\n                                self.gae_target(\n                                    rewards, v_values, \\\n                                    next_v_value, done\n                            )\n                            actor_losses, Critic_losses=[],[]\n    ```", "```\n                            for epoch in range(args.epochs):\n                                actor_loss = \\\n                                    self.actor.train(\n                                        old_policies, states,\n                                        actions, gaes\n                                    )\n                                actor_losses.append(\n                                     actor_loss)\n                                Critic_loss = \\\n                                    self.Critic.train(states,\n                                                  td_targets)\n                                Critic_losses.append(\n                                                 Critic_loss)\n                            # Plot mean actor & Critic losses \n                            # on every update\n                            tf.summary.scalar(\"actor_loss\", \n                                 np.mean(actor_losses), \n                                 step=ep)\n                            tf.summary.scalar(\n                                \"Critic_loss\", \n                                 np.mean(Critic_losses),\n                                 step=ep\n                            )\n                            state_batch = []\n                            action_batch = []\n                            reward_batch = []\n                            old_policy_batch = []\n                        episode_reward += reward[0][0]\n                        state = next_state[0]\n    ```", "```\n    if __name__ == \"__main__\":\n        env_name = \"MiniWoBClickButtonVisualEnv-v0\"\n        env = gym.make(env_name)\n        cta_Agent = PPOAgent(env)\n        cta_Agent.train()\n    ```", "```\nimport argparse\nimport os\nfrom datetime import datetime\nimport gym\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (Conv2D,Dense,Dropout,Flatten,Input,Lambda,MaxPool2D,)\nimport webgym  # Used to register webgym environments\n```", "```\n    parser = argparse.ArgumentParser(prog=\"TFRL-Cookbook-Ch5-Login-Agent\")\n    parser.add_argument(\"--env\", default=\"MiniWoBLoginUserVisualEnv-v0\")\n    parser.add_argument(\"--update-freq\", type=int, default=16)\n    parser.add_argument(\"--epochs\", type=int, default=3)\n    parser.add_argument(\"--actor-lr\", type=float, default=1e-4)\n    parser.add_argument(\"--Critic-lr\", type=float, default=1e-4)\n    parser.add_argument(\"--clip-ratio\", type=float, default=0.1)\n    parser.add_argument(\"--gae-lambda\", type=float, default=0.95)\n    parser.add_argument(\"--gamma\", type=float, default=0.99)\n    parser.add_argument(\"--logdir\", default=\"logs\")\n    args = parser.parse_args()\n    logdir = os.path.join(\n        args.logdir, parser.prog, args.env, \\\n        datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    )\n    print(f\"Saving training logs to:{logdir}\")\n    writer = tf.summary.create_file_writer(logdir)\n    ```", "```\n    class Critic:\n        def __init__(self, state_dim):\n            self.state_dim = state_dim\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.model = self.nn_model()\n            self.model.summary()  # Print a summary of the \n            # Critic model\n            self.opt = \\\n                tf.keras.optimizers.Nadam(args.Critic_lr)\n    ```", "```\n        def nn_model(self):\n            obs_input = Input(self.state_dim)\n            conv1 = Conv2D(\n                filters=64,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"same\",\n                input_shape=self.state_dim,\n                data_format=\"channels_last\",\n                activation=\"relu\",\n            )(obs_input)\n            pool1 = MaxPool2D(pool_size=(3, 3), strides=2)\\\n                             (conv1)\n            conv2 = Conv2D(\n                filters=32,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool1)\n            pool2 = MaxPool2D(pool_size=(3, 3), strides=2)\n                             (conv2)\n    ```", "```\n            conv3 = Conv2D(\n                filters=16,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool2)\n            pool3 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                              (conv3)\n            conv4 = Conv2D(\n                filters=8,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool3)\n            pool4 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n    (conv4)\n    ```", "```\n            flat = Flatten()(pool4)\n            dense1 = Dense(\n                16, activation=\"relu\", \n                kernel_initializer=self.weight_initializer\n            )(flat)\n            dropout1 = Dropout(0.3)(dense1)\n            dense2 = Dense(\n                8, activation=\"relu\", \n                kernel_initializer=self.weight_initializer\n            )(dropout1)\n            dropout2 = Dropout(0.3)(dense2)\n            value = Dense(\n                1, activation=\"linear\", \n                kernel_initializer=self.weight_initializer\n            )(dropout2)\n            return tf.keras.models.Model(inputs=obs_input, \n                             outputs=value, name=\"Critic\")\n    ```", "```\n        def compute_loss(self, v_pred, td_targets):\n            mse = tf.keras.losses.MeanSquaredError()\n            return mse(td_targets, v_pred)\n        def train(self, states, td_targets):\n            with tf.GradientTape() as tape:\n                v_pred = self.model(states, training=True)\n                # assert v_pred.shape == td_targets.shape\n                loss = self.compute_loss(v_pred, \n                                tf.stop_gradient(td_targets))\n            grads = tape.gradient(loss, \n                              self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \n                             self.model.trainable_variables))\n            return loss\n    ```", "```\n    class Actor:\n        def __init__(self, state_dim, action_dim, \n        action_bound, std_bound):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.action_bound = np.array(action_bound)\n            self.std_bound = std_bound\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.eps = 1e-5\n            self.model = self.nn_model()\n            self.model.summary()  # Print a summary of the \n            # Actor model\n            self.opt = tf.keras.optimizers.Nadam(\n                                              args.actor_lr)\n    ```", "```\n            # action_dim[0] = 2\n            output_val = Dense(\n                self.action_dim[0],\n                activation=\"relu\",\n                kernel_initializer=self.weight_initializer,\n            )(dropout2)\n            # Scale & clip x[i] to be in range [0, \n            # action_bound[i]]\n            mu_output = Lambda(\n                lambda x: tf.clip_by_value(x * \\\n                  self.action_bound, 1e-9, self.action_bound)\n            )(output_val)\n            std_output_1 = Dense(\n                self.action_dim[0],\n                activation=\"softplus\",\n                kernel_initializer=self.weight_initializer,\n            )(dropout2)\n            std_output = Lambda(\n                lambda x: tf.clip_by_value(\n                    x * self.action_bound, 1e-9, \n                    self.action_bound / 2\n                )\n            )(std_output_1)\n            return tf.keras.models.Model(\n                inputs=obs_input, outputs=[mu_output, \n                  std_output], name=\"Actor\"\n            )\n    ```", "```\n        def log_pdf(self, mu, std, action):\n            std = tf.clip_by_value(std, self.std_bound[0], \n                                   self.std_bound[1])\n            var = std ** 2\n            log_policy_pdf = -0.5 * (action - mu) ** 2 / var\\\n                             - 0.5 * tf.math.log(\n                var * 2 * np.pi\n            )\n            return tf.reduce_sum(log_policy_pdf, 1, \n                                 keepdims=True)\n        def compute_loss(self, log_old_policy, \n                         log_new_policy, actions, gaes):\n            # Avoid INF in exp by setting 80 as the upper \n            # bound since,\n            # tf.exp(x) for x>88 yeilds NaN (float32)\n            ratio = tf.exp(\n                tf.minimum(log_new_policy - \\\n                        tf.stop_gradient(log_old_policy), 80)\n            )\n            gaes = tf.stop_gradient(gaes)\n            clipped_ratio = tf.clip_by_value(\n                ratio, 1.0 - args.clip_ratio, 1.0 + \\\n                args.clip_ratio\n            )\n            surrogate = -tf.minimum(ratio * gaes, \n                                    clipped_ratio * gaes)\n            return tf.reduce_mean(surrogate)\n    ```", "```\n        def train(self, log_old_policy, states, actions, \n        gaes):\n            with tf.GradientTape() as tape:\n                mu, std = self.model(states, training=True)\n                log_new_policy = self.log_pdf(mu, std, \n                                              actions)\n                loss = self.compute_loss(log_old_policy, \n                                         log_new_policy,\n                                         actions, gaes)\n            grads = tape.gradient(loss, \n                              self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \n                             self.model.trainable_variables))\n            return loss\n    ```", "```\n        def get_action(self, state):\n            # Convert [Image] to np.array(np.adarray)\n            state_np = np.array([np.array(s) for s in state])\n            if len(state_np.shape) == 3:\n                # Convert (w, h, c) to (1, w, h, c)\n                state_np = np.expand_dims(state_np, 0)\n            mu, std = self.model.predict(state_np)\n            action = np.random.normal(mu, std + self.eps, \n                                size=self.action_dim).astype(\n                \"int\"\n            )\n            # Clip action to be between 0 and max obs \n            # screen size\n            action = np.clip(action, 0, self.action_bound)\n            # 1 Action per instance of env; Env expects: \n            # (num_instances, actions)\n            action = (action,)\n            log_policy = self.log_pdf(mu, std, action)\n            return log_policy, action\n    ```", "```\n                   while not done:\n                        # self.env.render()\n                        log_old_policy, action = \\\n                            self.actor.get_action(state)\n                        next_state, reward, dones, _ = \\\n                            self.env.step(action)\n                        step_num += 1\n                        # Convert action[2] from int idx to \n                        # char for verbose printing\n                        action_print = []\n                        for a in action:  # Map apply\n                            action_verbose = (a[:2], \\\n                            self.get_typed_char(a[2]))\n                            action_print.append(\n                                          action_verbose)\n                        print(\n                            f\"ep#:{ep} step#:{step_num} \n                            step_rew:{reward} \\\n                            action:{action_print} \\\n                            dones:{dones}\"\n                        )\n                        done = np.all(dones)\n                        if done:\n                            next_state = prev_state\n                        else:\n                            prev_state = next_state\n                        state = np.array([np.array(s) for \\\n                            s in state])\n                        next_state = np.array([np.array(s) \\\n                            for s in next_state])\n                        reward = np.reshape(reward, [1, 1])\n                        log_old_policy = np.reshape(\n                                      log_old_policy, [1, 1])\n                        state_batch.append(state)\n                        action_batch.append(action)\n                        reward_batch.append((reward + 8) / 8)\n                        old_policy_batch.append(\\\n                                              log_old_policy)\n    ```", "```\n                        if len(state_batch) >= \\\n                            args.update_freq or done:\n                            states = np.array([state.\\\n                                         squeeze() for state\\\n                                         in state_batch])\n                            actions = np.array([action[0]\\\n                                 for action in action_batch])\n                            rewards = np.array(\n                                [reward.squeeze() for reward\\\n                                 in reward_batch])\n                            old_policies = np.array(\n                                [old_pi.squeeze() for old_pi\\\n                                 in old_policy_batch])\n                            v_values = self.Critic.model.\\\n                                           predict(states)\n                            next_v_value = self.Critic.\\\n                                    model.predict(next_state)\n                            gaes, td_targets = \\\n                                        self.gae_target(\n                                rewards, v_values, \\\n                                next_v_value, done)\n                            actor_losses, Critic_losses=[],[]\n                            for epoch in range(args.epochs):\n                                actor_loss = \\\n                                    self.actor.train(\n                                        old_policies, states,\n                                      actions, gaes)\n                                actor_losses.append(\n                                                  actor_loss)\n                                Critic_loss = self.Critic.\\\n                                    train(states, td_targets)\n                                Critic_losses.append(\n                                                 Critic_loss)\n    ```", "```\n    if __name__ == \"__main__\":\n        env_name = \"MiniWoBLoginUserVisualEnv-v0\"\n        env = gym.make(env_name)\n        cta_Agent = PPOAgent(env)\n        cta_Agent.train()\n    ```", "```\nimport argparse\nimport os\nimport random\nfrom collections import deque\nfrom datetime import datetime\nimport gym\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (Conv2D,Dense,Dropout, Flatten, Input,Lambda,MaxPool2D)\nimport webgym  # Used to register webgym environments\n```", "```\n    parser = argparse.ArgumentParser(\n        prog=\"TFRL-Cookbook-Ch5-SocialMedia-Mute-User-DDPGAgent\"\n    )\n    parser.add_argument(\"--env\", default=\"Pendulum-v0\")\n    parser.add_argument(\"--actor_lr\", type=float, default=0.0005)\n    parser.add_argument(\"--Critic_lr\", type=float, default=0.001)\n    parser.add_argument(\"--batch_size\", type=int, default=64)\n    parser.add_argument(\"--tau\", type=float, default=0.05)\n    parser.add_argument(\"--gamma\", type=float, default=0.99)\n    parser.add_argument(\"--train_start\", type=int, default=2000)\n    parser.add_argument(\"--logdir\", default=\"logs\")\n    args = parser.parse_args()\n    ```", "```\n    logdir = os.path.join(\n        args.logdir, parser.prog, args.env, \\\n        datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    )\n    print(f\"Saving training logs to:{logdir}\")\n    writer = tf.summary.create_file_writer(logdir)\t\n    ```", "```\n    class ReplayBuffer:\n        def __init__(self, capacity=10000):\n            self.buffer = deque(maxlen=capacity)\n        def store(self, state, action, reward, next_state, \n        done):\n            self.buffer.append([state, action, reward, \n                                next_state, done])\n        def sample(self):\n            sample = random.sample(self.buffer, \n                                   args.batch_size)\n            states, actions, rewards, next_states, done = \\\n                                map(np.asarray, zip(*sample))\n            states = \\\n                np.array(states).reshape(args.batch_size, -1)\n            next_states = np.array(next_states).\\\n                          reshape(args.batch_size, -1)\n            return states, actions, rewards, next_states,\\\n            done\n        def size(self):\n            return len(self.buffer)\n    ```", "```\n    class Actor:\n        def __init__(self, state_dim, action_dim, \n        action_bound):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.action_bound = action_bound\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.eps = 1e-5\n            self.model = self.nn_model()\n            self.opt = tf.keras.optimizers.Adam(\n                                               args.actor_lr)\n    ```", "```\n        def train(self, states, q_grads):\n            with tf.GradientTape() as tape:\n                grads = tape.gradient(\n                    self.model(states), \n                    self.model.trainable_variables, \n                    -q_grads\n                )\n            self.opt.apply_gradients(zip(grads, \\\n                             self.model.trainable_variables))\n        def predict(self, state):\n            return self.model.predict(state)\n    ```", "```\n        def get_action(self, state):\n            # Convert [Image] to np.array(np.adarray)\n            state_np = np.array([np.array(s) for s in state])\n            if len(state_np.shape) == 3:\n                # Convert (w, h, c) to (1, w, h, c)\n                state_np = np.expand_dims(state_np, 0)\n            action = self.model.predict(state_np)\n            # Clip action to be between 0 and max obs \n            # screen size\n            action = np.clip(action, 0, self.action_bound)\n            # 1 Action per instance of env; Env expects: \n            # (num_instances, actions)\n            return action\n    ```", "```\n    class Critic:\n        def __init__(self, state_dim, action_dim):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.model = self.nn_model()\n            self.opt = \\\n                tf.keras.optimizers.Adam(args.Critic_lr)\n    ```", "```\n        def predict(self, inputs):\n            return self.model.predict(inputs)\n        def q_gradients(self, states, actions):\n            actions = tf.convert_to_tensor(actions)\n            with tf.GradientTape() as tape:\n                tape.watch(actions)\n                q_values = self.model([states, actions])\n                q_values = tf.squeeze(q_values)\n            return tape.gradient(q_values, actions)\n    ```", "```\n        def compute_loss(self, v_pred, td_targets):\n            mse = tf.keras.losses.MeanSquaredError()\n            return mse(td_targets, v_pred)\n        def train(self, states, actions, td_targets):\n            with tf.GradientTape() as tape:\n                v_pred = self.model([states, actions], \n                                     training=True)\n                assert v_pred.shape == td_targets.shape\n                loss = self.compute_loss(v_pred, \n                               tf.stop_gradient(td_targets))\n            grads = tape.gradient(loss, \n                              self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \n                             self.model.trainable_variables))\n            return loss\n    ```", "```\n    class DDPGAgent:\n        def __init__(self, env):\n            self.env = env\n            self.state_dim = self.env.observation_space.shape\n            self.action_dim = self.env.action_space.shape\n            self.action_bound = self.env.action_space.high\n            self.buffer = ReplayBuffer()\n            self.actor = Actor(self.state_dim, \n                          self.action_dim, self.action_bound)\n            self.Critic = Critic(self.state_dim, \n                                 self.action_dim)\n            self.target_actor = Actor(self.state_dim, \n                          self.action_dim, self.action_bound)\n            self.target_Critic = Critic(self.state_dim, \n                                        self.action_dim)\n            actor_weights = self.actor.model.get_weights()\n            Critic_weights = self.Critic.model.get_weights()\n            self.target_actor.model.set_weights(\n                                              actor_weights)\n            self.target_Critic.model.set_weights\n                                             (Critic_weights)\n    ```", "```\n        def update_target(self):\n            actor_weights = self.actor.model.get_weights()\n            t_actor_weights = \\\n                self.target_actor.model.get_weights()\n            Critic_weights = self.Critic.model.get_weights()\n            t_Critic_weights = \\\n                self.target_Critic.model.get_weights()\n            for i in range(len(actor_weights)):\n                t_actor_weights[i] = (args.tau * \\\n                actor_weights[i] + (1 - args.tau) * \\\n                t_actor_weights[i])\n            for i in range(len(Critic_weights)):\n                t_Critic_weights[i] = (args.tau * \\\n                Critic_weights[i] + (1 - args.tau) * \\\n                t_Critic_weights[i])\n            self.target_actor.model.set_weights(\n                                          t_actor_weights)\n            self.target_Critic.model.set_weights(\n                                         t_Critic_weights)\n    ```", "```\n        def get_td_target(self, rewards, q_values, dones):\n            targets = np.asarray(q_values)\n            for i in range(q_values.shape[0]):\n                if dones[i]:\n                    targets[i] = rewards[i]\n                else:\n                    targets[i] = args.gamma * q_values[i]\n            return targets\n    ```", "```\n        def add_ou_noise(self, x, rho=0.15, mu=0, dt=1e-1,\n                         sigma=0.2, dim=1):\n            return (\n                x + rho * (mu - x) * dt + sigma * \\\n                np.sqrt(dt) * np.random.normal(size=dim))\n    ```", "```\n        def replay_experience(self):\n            for _ in range(10):\n                states, actions, rewards, next_states, \\\n                    dones = self.buffer.sample()\n                target_q_values = self.target_Critic.predict(\n                    [next_states, \n                     self.target_actor.predict(next_states)])\n                td_targets = self.get_td_target(rewards,\n                                      target_q_values, dones)\n                self.Critic.train(states, actions, \n                                  td_targets)\n                s_actions = self.actor.predict(states)\n                s_grads = self.Critic.q_gradients(states, \n                                                  s_actions)\n                grads = np.array(s_grads).reshape(\n                                       (-1, self.action_dim))\n                self.actor.train(states, grads)\n                self.update_target()\n    ```", "```\n        def train(self, max_episodes=1000):\n            with writer.as_default():\n                for ep in range(max_episodes):\n                    step_num, episode_reward, done = 0, 0,\\\n                                                     False\n                    state = self.env.reset()\n                    prev_state = state\n                    bg_noise = np.random.randint(\n                        self.env.action_space.low,\n                        self.env.action_space.high,\n                        self.env.action_space.shape,\n                    )\n    ```", "```\n                    while not done:\n                        # self.env.render()\n                        action = self.actor.get_action(state)\n                        noise = self.add_ou_noise(bg_noise,\\\n                                         dim=self.action_dim)\n                        action = np.clip(action + noise, 0, \\\n                             self.action_bound).astype(\"int\")\n                        next_state, reward, dones, _ = \\\n                                       self.env.step(action)\n                        done = np.all(dones)\n                        if done:\n                            next_state = prev_state\n                        else:\n                            prev_state = next_state\n    ```", "```\n                    for (s, a, r, s_n, d) in zip(next_state,\\\n                    action, reward, next_state, dones):\n                            self.buffer.store(s, a, \\\n                                         (r + 8) / 8, s_n, d)\n                            episode_reward += r\n                        step_num += 1  # 1 across \n                        # num_instances\n                        print(f\"ep#:{ep} step#:{step_num} \\\n                              step_rew:{reward} \\\n                              action:{action} dones:{dones}\")\n                        bg_noise = noise\n                        state = next_state\n    ```", "```\n                    if (self.buffer.size() >= args.batch_size\n                        and self.buffer.size() >= \\\n                        args.train_start):\n                        self.replay_experience()\n                    print(f\"Episode#{ep} \\\n                            Reward:{episode_reward}\")\n                    tf.summary.scalar(\"episode_reward\", \\\n                                       episode_reward, \\\n                                       step=ep)\n    ```", "```\n    if __name__ == \"__main__\":\n        env_name = \"MiniWoBBookFlightVisualEnv-v0\"\n        env = gym.make(env_name)\n        Agent = DDPGAgent(env)\n        Agent.train()\n    ```", "```\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (\n    Conv2D,\n    Dense,\n    Dropout,\n    Flatten,\n    Input,\n    Lambda,\n    MaxPool2D,\n)\nimport webgym  # Used to register webgym environments\n```", "```\n    parser = argparse.ArgumentParser(\n        prog=\"TFRL-Cookbook-Ch5-Important-Emails-Manager-Agent\"\n    )\n    parser.add_argument(\"--env\", default=\"MiniWoBEmailInboxImportantVisualEnv-v0\")\n    ```", "```\n    args = parser.parse_args()\n    logdir = os.path.join(\n        args.logdir, parser.prog, args.env, \\\n        datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n    )\n    print(f\"Saving training logs to:{logdir}\")\n    writer = tf.summary.create_file_writer(logdir)\n    ```", "```\n    class Actor:\n        def __init__(self, state_dim, action_dim, \n        action_bound, std_bound):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.action_bound = np.array(action_bound)\n            self.std_bound = std_bound\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.eps = 1e-5\n            self.model = self.nn_model()\n            self.model.summary()  # Print a summary of the \n            # Actor model\n            self.opt = \\\n                tf.keras.optimizers.Nadam(args.actor_lr)\n    ```", "```\n        def nn_model(self):\n            obs_input = Input(self.state_dim)\n            conv1 = Conv2D(\n                filters=32,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"same\",\n                input_shape=self.state_dim,\n                data_format=\"channels_last\",\n                activation=\"relu\",\n            )(obs_input)\n            pool1 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                             (conv1)\n            conv2 = Conv2D(\n                filters=32,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool1)\n            pool2 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                             (conv2)\n    ```", "```\n            conv3 = Conv2D(\n                filters=16,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool2)\n            pool3 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                             (conv3)\n            conv4 = Conv2D(\n                filters=16,\n                kernel_size=(3, 3),\n                strides=(1, 1),\n                padding=\"valid\",\n                activation=\"relu\",\n            )(pool3)\n            pool4 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                             (conv4)\n    ```", "```\n           flat = Flatten()(pool4)\n            dense1 = Dense(\n                16, activation=\"relu\", \\\n                   kernel_initializer=self.weight_initializer\n            )(flat)\n            dropout1 = Dropout(0.3)(dense1)\n            dense2 = Dense(\n                8, activation=\"relu\", \\\n                   kernel_initializer=self.weight_initializer\n            )(dropout1)\n            dropout2 = Dropout(0.3)(dense2)\n            # action_dim[0] = 2\n            output_val = Dense(\n                self.action_dim[0],\n                activation=\"relu\",\n                kernel_initializer=self.weight_initializer,\n            )(dropout2)\n    ```", "```\n            # Scale & clip x[i] to be in range [0, \n                                             action_bound[i]]\n            mu_output = Lambda(\n                lambda x: tf.clip_by_value(x * \\\n                  self.action_bound, 1e-9, self.action_bound)\n            )(output_val)\n            std_output_1 = Dense(\n                self.action_dim[0],\n                activation=\"softplus\",\n                kernel_initializer=self.weight_initializer,\n            )(dropout2)\n            std_output = Lambda(\n                lambda x: tf.clip_by_value(\n                    x * self.action_bound, 1e-9, \\\n                    self.action_bound / 2))(std_output_1)\n            return tf.keras.models.Model(\n                inputs=obs_input, outputs=[mu_output, \\\n                           std_output], name=\"Actor\"\n            )\n    ```", "```\n    class Critic:\n        def __init__(self, state_dim):\n            self.state_dim = state_dim\n            self.weight_initializer = \\\n                    tf.keras.initializers.he_normal()\n            self.model = self.nn_model()\n            self.model.summary()  # Print a summary of the \n            # Critic model\n            self.opt = \\\n                tf.keras.optimizers.Nadam(args.Critic_lr)\n    ```", "```\n        def compute_loss(self, v_pred, td_targets):\n            mse = tf.keras.losses.MeanSquaredError()\n            return mse(td_targets, v_pred)\n        def train(self, states, td_targets):\n            with tf.GradientTape() as tape:\n                v_pred = self.model(states, training=True)\n                # assert v_pred.shape == td_targets.shape\n                loss = self.compute_loss(v_pred, \\\n                           tf.stop_gradient(td_targets))\n            grads = tape.gradient(loss, \\\n                              self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \\\n                             self.model.trainable_variables))\n            return loss\n    ```", "```\n    class PPOAgent:\n        def __init__(self, env):\n            self.env = env\n            self.state_dim = self.env.observation_space.shape\n            self.action_dim = self.env.action_space.shape\n            # Set action_bounds to be within the actual \n            # task-window/browser-view of the Agent\n            self.action_bound = [self.env.task_width, \\\n                                 self.env.task_height]\n            self.std_bound = [1e-2, 1.0]\n            self.actor = Actor(\n                self.state_dim, self.action_dim, \\\n                self.action_bound, self.std_bound\n            )\n            self.Critic = Critic(self.state_dim)\n    ```", "```\n    if __name__ == \"__main__\":\n        env_name = \"MiniWoBEmailInboxImportantVisualEnv-v0\"\n        env = gym.make(env_name)\n        cta_Agent = PPOAgent(env)\n        cta_Agent.train()\n    ```", "```\nimport tensorflow as tf\nfrom tensorflow.keras.layers import (Conv2D, Dense, Dropout, Flatten, Input, Lambda, MaxPool2D, concatenate,)\nimport webgym  # Used to register webgym environments\n```", "```\n    class ReplayBuffer:\n        def __init__(self, capacity=10000):\n            self.buffer = deque(maxlen=capacity)\n        def store(self, state, action, reward, next_state,\n        done):\n            self.buffer.append([state, action, reward, \n                                next_state, done])\n        def sample(self):\n            sample = random.sample(self.buffer, \n                                   args.batch_size)\n            states, actions, rewards, next_states, done = \\\n                                map(np.asarray, zip(*sample))\n            states = \\\n                np.array(states).reshape(args.batch_size, -1)\n            next_states = np.array(next_states).\\\n                           reshape(args.batch_size, -1)\n            return states, actions, rewards, next_states,\\\n            done\n        def size(self):\n            return len(self.buffer)\n    ```", "```\n    class Actor:\n        def __init__(self, state_dim, action_dim, \n        action_bound):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.action_bound = action_bound\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.eps = 1e-5\n            self.model = self.nn_model()\n            self.opt = \\\n                tf.keras.optimizers.Adam(args.actor_lr)\n    ```", "```\n        def nn_model(self):\n            obs_input = Input(self.state_dim)\n            conv1 = Conv2D(filters=64, kernel_size=(3, 3),\\ \n                           strides=(1, 1), padding=\"same\", \\\n                           input_shape=self.state_dim, \\\n                           data_format=\"channels_last\", \\\n                           activation=\"relu\")(obs_input)\n            pool1 = MaxPool2D(pool_size=(3, 3), \\\n                              strides=1)(conv1)\n            conv2 = Conv2D(filters=32, kernel_size=(3, 3),\\\n                           strides=(1, 1), padding=\"valid\", \\\n                           activation=\"relu\",)(pool1)\n            pool2 = MaxPool2D(pool_size=(3, 3), strides=1)\\\n                              (conv2)\n    ```", "```\n            flat = Flatten()(pool2)\n            dense1 = Dense(\n                16, activation=\"relu\", \\\n                kernel_initializer=self.weight_initializer)\\\n                (flat)\n            dropout1 = Dropout(0.3)(dense1)\n            dense2 = Dense(8, activation=\"relu\", \\\n                kernel_initializer=self.weight_initializer)\\\n                (dropout1)\n            dropout2 = Dropout(0.3)(dense2)\n            # action_dim[0] = 2\n            output_val = Dense(self.action_dim[0], \n                               activation=\"relu\",\n                               kernel_initializer= \\\n                                   self.weight_initializer,)\\\n                               (dropout2)\n            # Scale & clip x[i] to be in range \n            # [0, action_bound[i]]\n            mu_output=Lambda(lambda x: tf.clip_by_value(x *\\\n                             self.action_bound, 1e-9, \\\n                             self.action_bound))(output_val)\n            return tf.keras.models.Model(inputs=obs_input, \n                                         outputs=mu_output, \n                                         name=\"Actor\")\n    ```", "```\n        def train(self, states, q_grads):\n            with tf.GradientTape() as tape:\n                grads = tape.gradient(self.model(states),\\ \n                             self.model.trainable_variables,\\\n                             -q_grads)\n            self.opt.apply_gradients(zip(grads, \\\n                 self.model.trainable_variables))\n        def predict(self, state):\n            return self.model.predict(state)\n    ```", "```\n        def get_action(self, state):\n            # Convert [Image] to np.array(np.adarray)\n            state_np = np.array([np.array(s) for s in state])\n            if len(state_np.shape) == 3:\n                # Convert (w, h, c) to (1, w, h, c)\n                state_np = np.expand_dims(state_np, 0)\n            action = self.model.predict(state_np)\n            action = np.clip(action, 0, self.action_bound)\n            return action\n    ```", "```\n    class Critic:\n        def __init__(self, state_dim, action_dim):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.weight_initializer = \\\n                tf.keras.initializers.he_normal()\n            self.model = self.nn_model()\n            self.opt = \\\n                tf.keras.optimizers.Adam(args.Critic_lr)\n    ```", "```\n        def nn_model(self):\n            obs_input = Input(self.state_dim)\n            conv1 = Conv2D(filters=64, kernel_size=(3, 3), \n                           strides=(1, 1), padding=\"same\", \n                           input_shape=self.state_dim, \n                           data_format=\"channels_last\",\n                           activation=\"relu\",)(obs_input)\n            pool1 = MaxPool2D(pool_size=(3, 3), strides=2)\\\n                             (conv1)\n            conv2 = Conv2D(filters=32, kernel_size=(3, 3), \n                           strides=(1, 1), padding=\"valid\", \n                           activation=\"relu\",)(pool1)\n            pool2 = MaxPool2D(pool_size=(3, 3), \n                              strides=2)(conv2)\n    ```", "```\n            flat = Flatten()(pool2)\n            dense1 = Dense(16, activation=\"relu\", \n                           kernel_initializer= \\\n                               self.weight_initializer)(flat)\n            dropout1 = Dropout(0.3)(dense1)\n            dense2 = Dense(8, activation=\"relu\", \n                           kernel_initializer= \\\n                               self.weight_initializer)\\\n                           (dropout1)\n            dropout2 = Dropout(0.3)(dense2)\n            value = Dense(1, activation=\"linear\", \n                          kernel_initializer= \\\n                              self.weight_initializer)\\\n                          (dropout2)\n            return tf.keras.models.Model(inputs=obs_input, \n                                         outputs=value,\n                                         name=\"Critic\")\n    ```", "```\n        def q_gradients(self, states, actions):\n            actions = tf.convert_to_tensor(actions)\n            with tf.GradientTape() as tape:\n                tape.watch(actions)\n                q_values = self.model([states, actions])\n                q_values = tf.squeeze(q_values)\n            return tape.gradient(q_values, actions)\n        def compute_loss(self, v_pred, td_targets):\n            mse = tf.keras.losses.MeanSquaredError()\n            return mse(td_targets, v_pred)\n    ```", "```\n        def predict(self, inputs):\n            return self.model.predict(inputs)    \n        def train(self, states, actions, td_targets):\n            with tf.GradientTape() as tape:\n                v_pred = self.model([states, actions],\\\n                                     training=True)\n                assert v_pred.shape == td_targets.shape\n                loss = self.compute_loss(v_pred, \\\n                                tf.stop_gradient(td_targets))\n            grads = tape.gradient(loss, \\\n                             self.model.trainable_variables)\n            self.opt.apply_gradients(zip(grads, \\\n                            self.model.trainable_variables))\n            return loss\n    ```", "```\n    class DDPGAgent:\n        def __init__(self, env):\n            self.env = env\n            self.state_dim = self.env.observation_space.shape\n            self.action_dim = self.env.action_space.shape\n            self.action_bound = self.env.action_space.high\n            self.buffer = ReplayBuffer()\n            self.actor = Actor(self.state_dim, \n                               self.action_dim, \n                               self.action_bound)\n            self.Critic = Critic(self.state_dim, \n                                 self.action_dim)\n            self.target_actor = Actor(self.state_dim, \n                                      self.action_dim, \n                                      self.action_bound)\n            self.target_Critic = Critic(self.state_dim, \n                                        self.action_dim)\n            actor_weights = self.actor.model.get_weights()\n            Critic_weights = self.Critic.model.get_weights()\n            self.target_actor.model.set_weights(\n                                             actor_weights)\n            self.target_Critic.model.set_weights(\n                                            Critic_weights)\n    ```", "```\n        def update_target(self):\n            actor_weights = self.actor.model.get_weights()\n            t_actor_weights = \\\n                self.target_actor.model.get_weights()\n            Critic_weights = self.Critic.model.get_weights()\n            t_Critic_weights = \\\n                self.target_Critic.model.get_weights()\n            for i in range(len(actor_weights)):\n                t_actor_weights[i] = (args.tau * \\\n                                      actor_weights[i] + \\\n                                      (1 - args.tau) * \\\n                                      t_actor_weights[i])\n            for i in range(len(Critic_weights)):\n                t_Critic_weights[i] = (args.tau * \\\n                                       Critic_weights[i] + \\\n                                       (1 - args.tau) * \\\n                                       t_Critic_weights[i])\n            self.target_actor.model.set_weights(\n                                            t_actor_weights)\n            self.target_Critic.model.set_weights(\n                                            t_Critic_weights)\n    ```", "```\n        def train(self, max_episodes=1000):\n            with writer.as_default():\n                for ep in range(max_episodes):\n                    step_num, episode_reward, done = 0, 0, \\\n                                                     False\n                    state = self.env.reset()\n                    prev_state = state\n                    bg_noise = np.random.randint( \n                                 self.env.action_space.low, \n                                 self.env.action_space.high, \n                                 self.env.action_space.shape)\n    ```", "```\n                    while not done:\n                        action = self.actor.get_action(state)\n                        noise = self.add_ou_noise(bg_noise, \n                                         dim=self.action_dim)\n                        action = np.clip(action + noise, 0,\n                             self.action_bound).astype(\"int\")\n                        next_state, reward, dones, _ = \\\n                             self.env.step(action)\n                        done = np.all(dones)\n                        if done:\n                            next_state = prev_state\n                        else:\n                            prev_state = next_state\n                        for (s, a, r, s_n, d) in zip\\\n                        (next_state, action, reward, \\\n                         next_state, dones):\n                            self.buffer.store(s, a, \\\n                                       (r + 8) / 8, s_n, d)\n                            episode_reward += r\n                        step_num += 1  \n                        # 1 across num_instances\n                        bg_noise = noise\n                        state = next_state\n                    if (self.buffer.size() >= args.batch_size\n                        and self.buffer.size() >= \\\n                            args.train_start):\n                        self.replay_experience()\n                    tf.summary.scalar(\"episode_reward\", \n                                       episode_reward, \n                                       step=ep)\n    ```", "```\n    if __name__ == \"__main__\":\n        env_name = \"MiniWoBSocialMediaMuteUserVisualEnv-v0\"\n        env = gym.make(env_name)\n        Agent = DDPGAgent(env)\n        Agent.train()\n    ```"]