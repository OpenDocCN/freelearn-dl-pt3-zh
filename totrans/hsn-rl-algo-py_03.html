<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Implementing RL Cycle and OpenAI Gym</h1>
                </header>
            
            <article>
                
<p>In every machine learning project, an algorithm learns rules and instructions <span>from a training dataset, with a view to </span>performing a task better. In <strong>reinforcement learning</strong> (<strong>RL</strong>), the algorithm is called the agent, and it learns from the data provided by an environment. Here, the environment is a continuous source of information that returns data according to the agent's actions. And, because the data returned by an environment could be potentially infinite, there are many conceptual and practical differences among the supervised settings that arise while training. For the purpose of this chapter, however, it is important to highlight the fact that different environments not only provide different tasks to accomplish, but can also have different types of input, output, and reward signals, while also requiring the adaptation of the algorithm in each case. For example, a robot could <span>either </span>sense its state from a visual input, such as an RGB camera, or from discrete internal sensors. </p>
<p>In this chapter, you'll set up the environment required to code RL algorithms and build your first algorithm. Despite being a simple algorithm that plays CartPole, it offers a useful baseline to master the basic RL cycle before moving on to more advanced RL algorithms. Also, because, in the later chapters, you'll code many deep neural networks, here, we'll give you a brief recap about TensorFlow and introduce TensorBoard, a visualization tool. </p>
<p>Almost all the environments used throughout the book are based on the interface open sourced by OpenAI called <strong>Gym</strong>. Therefore, we'll take a look at it and use some of its built-in environments. Then, before moving on to an in-depth examination of RL algorithms in the next chapters, we'll list and explain the strengths and differences of a number of open source environments. In this way, you'll have a broad and practical overview of the problems that can be tackled with RL.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Setting up the environment</li>
<li>OpenAI Gym and RL cycles</li>
<li>TensorFlow</li>
<li>TensorBoard</li>
<li>Types of RL environments</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the environment</h1>
                </header>
            
            <article>
                
<p>The following are the three main tools required to create deep RL algorithms:</p>
<ul>
<li><strong>Programming language</strong>: Python is the first choice for the development of machine learning algorithms on account of its simplicity and the third-party libraries that are built around it. </li>
<li><strong>Deep learning framework</strong>: In this book, we use TensorFlow because, as we'll see in the <em>TensorFlow</em> section, it is scalable, flexible, and very expressive. Despite this, many other frameworks can be used in its place, including PyTorch and Caffe.</li>
<li><strong>Environment</strong>: Throughout the book, we'll use many different environments to demonstrate how to deal with different types of problems and to highlight the strengths of RL algorithms.</li>
</ul>
<p>In this book, we use Python 3.7, but all versions above 3.5 should work. We also assume that you've already installed <kbd>numpy</kbd> and <kbd>matplotlib</kbd>.</p>
<p>If you haven't already installed TensorFlow, you can do so through their website or by typing the following in a Terminal window:</p>
<pre><strong>$ pip install tensorflow</strong></pre>
<p>Alternatively, you can type the following command, if your machine has GPUs: </p>
<pre><strong>$ pip install tensorflow-gpu</strong></pre>
<p>You can find all the installation instructions and the exercises relating to this chapter on the GitHub repository, which can be found here: <a href="https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python">https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python</a><a href="https://github.com/PacktPublishing/Hands-On-Reinforcement-Learning-Algorithms-with-Python">.</a></p>
<p>Now, let's look at how to install the environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing OpenAI Gym </h1>
                </header>
            
            <article>
                
<p>OpenAI Gym offers a general interface as well as a broad variety of environments.</p>
<p>To install it, we will use the following commands.</p>
<p>On OSX, we can use the following:</p>
<pre><strong>$ brew install cmake boost boost-python sdl2 swig wget</strong></pre>
<p>On Ubuntu 16.04, we will use the following command:</p>
<pre><strong>$ apt-get install -y python-pyglet python3-opengl zlib1g-dev libjpeg-dev patchelf cmake swig libboost-all-dev libsdl2-dev libosmesa6-dev xvfb ffmpeg</strong></pre>
<p>On Ubuntu 18.04, we will use the following command:</p>
<pre><strong>$ sudo apt install -y python3-dev zlib1g-dev libjpeg-dev cmake swig python-pyglet python3-opengl libboost-all-dev libsdl2-dev libosmesa6-dev patchelf ffmpeg xvfb</strong> </pre>
<p>After running the preceding command for your respective OS, the following command is used:</p>
<pre><strong>$ git clone https://github.com/openai/gym.git </strong><br/><strong><span class="pl-c1">$ cd</span> gym</strong><br/><strong><span>$ pip install -e '.[all]'</span></strong></pre>
<p>Certain Gym environments also require the installation of <kbd>pybox2d</kbd>:</p>
<pre><strong>$ git clone https://github.com/pybox2d/pybox2d<a href="https://github.com/pybox2d/pybox2d"><br/></a>$ cd pybox2d</strong><br/><strong>$ pip install -e .</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Roboschool </h1>
                </header>
            
            <article>
                
<p>The final environment we are interested in is Roboschool, a simulator for robots. It's easy to install, but if you encounter any errors, take a look at its GitHub repository:</p>
<pre><strong>$ pip install roboschool</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenAI Gym and RL cycles</h1>
                </header>
            
            <article>
                
<p>Since RL requires an agent and an environment to interact with each other, the first example that may spring to mind is the earth, the physical world we live in. Unfortunately, for now, it is actually used <span>in only a few cases</span>. W<span>ith the current algorithms, t</span>he problems stem from the large number of interactions that an agent has to execute with the environment in order to learn good behaviors. It may require hundreds, thousands, or even millions of actions, requiring way <span>too much time to be feasible. One solution is to use simulated environments to start the learning process and, only at the end, fine-tune it in the real world. This approach is way better than learning just from the world around it, but still requires slow real-world interactions. However, in many cases, the task can be fully simulated. To research and implement RL algorithms, games, video games, and robot simulators are a perfect testbed because, in order to be solved, they require capabilities such as planning, strategy, and long-term memory. Moreover, games have a clear reward system and can be completely simulated in an artificial environment (computers), allowing fast interactions that accelerate the learning process. </span>For these reasons, in this book, we'll use mostly video games and robot simulators to demonstrate the capabilities of RL algorithms.</p>
<p>OpenAI Gym, an open source toolkit for developing and researching RL algorithms, was created to provide a common and shared interface for environments, while making a large and diverse collection of environments available. These include Atari 2600 games, continuous control tasks, classic control theory problems, simulated robotic goal-based tasks, and simple text games. Owing to its generality, many environments created by third parties are using the Gym interface. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Developing an RL cycle</h1>
                </header>
            
            <article>
                
<p><span>A basic RL cycle is shown in the following code block. This essentially makes the RL model play for 10 moves while rendering the game at each step</span>:</p>
<pre>import gym<br/><br/># create the environment <br/><span>env = gym.make("CartPole-v1")<br/></span># reset the environment before starting<br/><span>env.reset()<br/></span><br/># loop 10 times<br/><span>for i in range(10):<br/></span>    # take a random action<br/><span>    env.step(env.action_space.sample())<br/>    # render the game<br/></span>   env.render()<br/><br/># close the environment<br/><span>env.close()<br/></span></pre>
<p class="mce-root">This leads to the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2061 image-border" src="assets/6b77b90c-3aad-4e87-a1fc-2de8c5ec6805.png" style="width:19.83em;height:17.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.1: Rendering of CartPole</div>
<p class="mce-root">Let's take a closer look at the code. It starts by creating a new environment named <kbd>CartPole-v1</kbd>, a classic game used in control theory problems. However, before using it, the environment is initialized by calling <kbd>reset()</kbd><em>.</em> After doing so, the cycle loops 10 times. In each iteration, <kbd><span>env.action_space.sample()</span></kbd> <span>samples a random action, </span><span>executes it in the environment with </span><kbd>env.step()</kbd><em>,</em> <span>and displays the result</span> with the <kbd>render()</kbd> method<span>; that is, the current state of the game, as in the preceding screenshot</span><span>. In the end,</span> <span>the environment is closed by calling <kbd>env.close()</kbd>.</span></p>
<div class="packt_infobox">Don't worry if the following code outputs deprecation warnings; they are there to notify you that some functions have been changed. The code will still be functioning correctly.</div>
<p>This cycle is the same for every environment that uses the Gym interface, but for now, the agent can only play random actions without having any feedback, which is essential to any RL problem. </p>
<div class="packt_infobox">In RL, you may see the terms <strong>state</strong> and <strong>observation</strong> being used almost interchangeably, but they are not the same. We talk about state when all the information pertaining to the environment is encoded in it. We talk about observation when only a part of the actual state of the environment is visible to the agent, such as the perception of a robot. To simplify this, OpenAI Gym always uses the term observation.</div>
<p>The following diagram shows the flow of the cycle:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1804 image-border" src="assets/81b29847-365f-4f0f-be9f-48fdc91fe028.png" style="width:23.58em;height:11.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2.2: Basic RL cycle according to OpenAI Gym. The environment returns the next state, a reward, a done flag, and some additional information</div>
<p>Indeed, the <kbd>step()</kbd> method returns four variables that provide information about the interaction with the environment. The preceding diagram shows the loop between the agent and environment, as well as the variables exchanged; namely, <strong>Observation</strong>, <strong>Reward</strong>, <strong>Done</strong>, and <strong>Info</strong>. <strong>Observation</strong> is an object that represents the new observation (or state) of the environment. <strong>Reward</strong> is a float number that represents the number of rewards obtained in the last action. <strong>Done</strong> is a Boolean value that is used on tasks that are episodic; that is, tasks that are limited in terms of the number of interactions. Whenever <kbd>done</kbd><em> </em>is <kbd>True</kbd>, this means that the episode has terminated and that the environment should be reset. For example, <kbd>done</kbd> is <kbd>True</kbd> when the task has been completed or the agent has died. <strong>Info</strong>, on the other hand, is a dictionary that provides extra information about the environment but that usually isn't used.</p>
<p><span>If you have never heard of CartPole, it's a game with t</span><span>he goal of balancing a pendulum acting on a horizontal cart. A reward of +1 is provided for every timestep when the pendulum is in the upright position. The episode ends when it is too unbalanced or it manages to balance itself for more than 200 timesteps (collecting a maximum cumulative reward of 200).</span></p>
<p>We can now create a more complete algorithm that plays 10 games and prints the accumulated reward for each game using the following code:</p>
<pre>import gym<br/><br/># create and initialize the environment<br/>env = gym.make("CartPole-v1")<br/>env.reset()<br/><br/># play 10 games<br/>for i in range(10):<br/>    # initialize the variables<br/>    done = False<br/>    game_rew = 0<br/><br/>    while not done:<br/>        # choose a random action<br/>        action = env.action_space.sample()<br/>        # take a step in the environment<br/>        new_obs, rew, done, info = env.step(action)<br/>        game_rew += rew<br/>    <br/>        # when is done, print the cumulative reward of the game and reset the environment<br/>        if done:<br/>            print('Episode %d finished, reward:%d' % (i, game_rew))<br/>            env.reset()</pre>
<p>The output will be similar to the following:</p>
<pre><strong><span>Episode: 0, Reward:13</span></strong><br/><strong><span>Episode: 1, Reward:16</span></strong><br/><strong><span>Episode: 2, Reward:23</span></strong><br/><strong><span>Episode: 3, Reward:17</span></strong><br/><strong><span>Episode: 4, Reward:30</span></strong><br/><strong><span>Episode: 5, Reward:18</span></strong><br/><strong><span>Episode: 6, Reward:14</span></strong><br/><strong><span>Episode: 7, Reward:28</span></strong><br/><strong><span>Episode: 8, Reward:22</span></strong><br/><strong><span>Episode: 9, Reward:16</span></strong></pre>
<p class="mce-root">The following table shows the output of the <kbd>step()</kbd> method over the last four actions of a game:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 49%" class="CDPAlignCenter CDPAlign"><strong>Observation</strong></td>
<td style="width: 21%" class="CDPAlignCenter CDPAlign"><strong>Reward</strong></td>
<td style="width: 13.999%" class="CDPAlignCenter CDPAlign"><strong>Done</strong></td>
<td style="width: 10.001%" class="CDPAlignCenter CDPAlign"><strong>Info</strong></td>
</tr>
<tr>
<td style="width: 49%" class="CDPAlignCenter CDPAlign"><span>[-0.05356921, -0.38150626, 0.12529277, 0.9449761 ]</span></td>
<td style="width: 21%" class="CDPAlignCenter CDPAlign"><span>1.0 </span></td>
<td style="width: 13.999%" class="CDPAlignCenter CDPAlign"><span>False </span></td>
<td style="width: 10.001%" class="CDPAlignCenter CDPAlign"><span>{}</span></td>
</tr>
<tr>
<td style="width: 49%" class="CDPAlignCenter CDPAlign"><span>[-0.06119933, -0.57807287, 0.14419229, 1.27425449]</span></td>
<td style="width: 21%" class="CDPAlignCenter CDPAlign"><span>1.0</span></td>
<td style="width: 13.999%" class="CDPAlignCenter CDPAlign"><span>False </span></td>
<td style="width: 10.001%" class="CDPAlignCenter CDPAlign"><span>{}</span></td>
</tr>
<tr>
<td style="width: 49%" class="CDPAlignCenter CDPAlign"><span>[-0.07276079, -0.38505429, 0.16967738, 1.02997704]</span></td>
<td style="width: 21%" class="CDPAlignCenter CDPAlign"><span>1.0</span></td>
<td style="width: 13.999%" class="CDPAlignCenter CDPAlign"><span>False </span></td>
<td style="width: 10.001%" class="CDPAlignCenter CDPAlign"><span>{}</span></td>
</tr>
<tr>
<td style="width: 49%" class="CDPAlignCenter CDPAlign"><span>[-0.08046188, -0.58197758, 0.19027692, 1.37076617]</span></td>
<td style="width: 21%" class="CDPAlignCenter CDPAlign"><span>1.0</span></td>
<td style="width: 13.999%" class="CDPAlignCenter CDPAlign"><span>False </span></td>
<td style="width: 10.001%" class="CDPAlignCenter CDPAlign"><span>{}</span></td>
</tr>
<tr>
<td style="width: 49%" class="CDPAlignCenter CDPAlign"><span>[-0.09210143, -0.3896757, 0.21769224, 1.14312384]</span></td>
<td style="width: 21%" class="CDPAlignCenter CDPAlign"><span>1.0</span></td>
<td style="width: 13.999%" class="CDPAlignCenter CDPAlign"><span>True</span></td>
<td style="width: 10.001%" class="CDPAlignCenter CDPAlign"><span>{}</span></td>
</tr>
</tbody>
</table>
<p> </p>
<p>Notice that the environment's observation is encoded in a 1 x 4 array; that the reward, as we expected, is always 1; and that <kbd>done</kbd> is <kbd>True</kbd> only in the last row when the game is terminated. Also, <strong>Info</strong>, in this case, is empty.</p>
<p>In the upcoming chapters, we'll create agents that play CartPole by taking more intelligent actions depending on the current state of the pole. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting used to spaces</h1>
                </header>
            
            <article>
                
<p>In OpenAI Gym, actions and observations are mostly instances of the <kbd>Discrete</kbd> or <kbd>Box</kbd> <span>class.</span> These two classes represent different spaces. <kbd>Box</kbd> represents an <em>n</em>-dimensional array, while <kbd>Discrete</kbd>, on the other hand, is a space that allows a fixed range of non-negative numbers. In the preceding table, we have already seen that the observation of CartPole is encoded by four floats, meaning that it's an instance of the <kbd>Box</kbd> <span>class. I</span>t is possible to check the type and dimension of the observation spaces by printing the <kbd>env.observation_space</kbd> variable:</p>
<pre><span>import gym</span><br/><br/><span>env = gym.make('CartPole-v1')</span><br/><span>print(env.observation_space)<br/></span></pre>
<p><span>Indeed, as we expected, the output is as follows:</span></p>
<pre><strong><span>&gt;&gt;  Box(4,)</span></strong></pre>
<div class="packt_infobox">In this book, we mark the output of <kbd>print()</kbd> by introducing the printed text with <kbd>&gt;&gt;</kbd>.</div>
<p>In the same way, it is possible to check the dimension of the action space:</p>
<pre>print(env.action_space)</pre>
<p>This results in the following output:</p>
<pre><strong>&gt;&gt; Discrete(2)</strong></pre>
<p>In particular, <kbd>Discrete(2)</kbd> means that the actions could either <span>have the </span>value <kbd>0</kbd> or <kbd>1</kbd>. Indeed, if we use the sampling function used in the preceding example, we obtain <kbd>0</kbd> or <kbd>1</kbd> (in CartPole, this means left or right<em>)</em>:</p>
<pre><span>print(env.action_space.sample())<br/></span><strong>&gt;&gt; 0</strong><br/>print(env.action_space.sample())<br/><strong>&gt;&gt; 1</strong></pre>
<p>The <kbd>low</kbd><em> </em>and <kbd>high</kbd> instance attributes return the minimum and maximum values allowed by a <kbd>Box</kbd> space: </p>
<pre><span>print(env.observation_space.low)</span><br/><strong><span>&gt;&gt; [-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]</span></strong><br/><span>print(env.observation_space.high)</span><br/><strong><span>&gt;&gt; [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Development of ML models using TensorFlow</h1>
                </header>
            
            <article>
                
<p>TensorFlow is a machine learning framework that performs high-performance numerical computations. TensorFlow owes its popularity to its high quality and vast amount of documentation, its ability to <span>easily </span>serve models at scale in production environments, and the friendly interface to GPUs and TPUs.</p>
<p>TensorFlow, to facilitate the development and deployment of ML models, has many high-level APIs, including Keras, Eager Execution, and Estimators. These APIs are very useful in many contexts, but, in order to develop RL algorithms, we'll only use low-level APIs.</p>
<p>Now, let's code immediately using <strong>TensorFlow</strong>. The following lines of code execute the sum of the constants, <kbd>a</kbd> and <kbd>b</kbd>, created with <kbd>tf.constant()</kbd>:</p>
<pre>import tensorflow as tf<br/><br/># create two constants: a and b<br/>a = tf.constant(4)<br/>b = tf.constant(3)<br/><br/># perform a computation<br/>c = a + b<br/><br/># create a session<br/>session = tf.Session()<br/># run the session. It computes the sum<br/>res = session.run(c)<br/>print(res)</pre>
<p>A particularity of TensorFlow is the fact that it expresses all computations as a computational graph that has to first be defined and later executed. Only after execution will the results be available. In the following example, after the operation, <kbd>c = a + b</kbd>, <kbd>c</kbd> doesn't hold the end value. Indeed, if you print <kbd>c</kbd> before creating the session, you'll obtain the following:</p>
<pre><strong>&gt;&gt; Tensor("add:0", shape=(), dtype=int32)</strong></pre>
<p>This is the class of the <kbd>c</kbd> variable, not the result of the addition. </p>
<p><span>Moreover, execution has to be done inside a session that is instantiated with </span><kbd>tf.Session()</kbd><span>. Then, to perform the computation, the operation has to be passed as input to the</span> <kbd>run</kbd><span> function of the session just created.</span> Thus, to actually compute the graph and consequently sum <kbd>a</kbd> and <kbd>b</kbd>, we need to create a session and pass <kbd>c</kbd> as an input to <kbd>session.run</kbd>:</p>
<pre>session = tf.Session()<br/>res = session.run(c)<br/>print(res)<br/><br/><strong>&gt;&gt; 7</strong></pre>
<div class="packt_infobox"><span>If you are using Jupyter Notebook, make sure to reset the previous graph by running </span><kbd>tf.reset_default_graph()</kbd>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tensor</h1>
                </header>
            
            <article>
                
<p><span>The variables in TensorFlow are represented as tensors that are arrays of any number of dimensions. There are three main types of tensors—</span><kbd>tf.Variable</kbd><span>, <kbd>tf.constant</kbd>, and <kbd>tf.placeholder</kbd>. Except for <kbd>tf.Variable</kbd>, all the other tensors are immutable.</span></p>
<p>To check the shape of a tensor, we will use the following code:</p>
<pre># constant<br/>a = tf.constant(1)<br/>print(a.shape)<br/><strong>&gt;&gt; ()</strong><br/><br/># array of five elements<br/>b = tf.constant([1,2,3,4,5])<br/>print(b.shape)<br/><strong>&gt;&gt; (5,)</strong></pre>
<p>The elements of a tensor are easily accessible, and the mechanisms are similar to those employed by Python:</p>
<pre>a = tf.constant([1,2,3,4,5])<br/>first_three_elem = a[:3]<br/>fourth_elem = a[3]<br/><br/>sess = tf.Session()<br/>print(sess.run(first_three_elem))<br/><br/><strong>&gt;&gt; array([1,2,3])</strong><br/><br/>print(sess.run(fourth_elem))<br/><br/><strong>&gt;&gt; 4</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Constant</h1>
                </header>
            
            <article>
                
<p>As we have already seen, a constant is an immutable type of tensor that can be easily created using <kbd>tf.constant</kbd>:</p>
<pre>a = tf.constant([1.0, 1.1, 2.1, 3.1], dtype=tf.float32, name='a_const')<br/>print(a)<br/><br/><strong>&gt;&gt; Tensor("a_const:0", shape=(4,), dtype=float32)</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Placeholder</h1>
                </header>
            
            <article>
                
<p>A placeholder is a tensor that is fed at runtime. Usually, placeholders are used as input for models. Every input passed to a computational graph at runtime is fed with <kbd>feed_dict</kbd>. <kbd>feed_dict</kbd> is an optional argument that allows the caller to override the value of tensors in the graph. In the following snippet, the <kbd>a</kbd> placeholder is overridden by <kbd>[[0.1,0.2,0.3]]</kbd>:</p>
<pre>import tensorflow as tf<br/><br/>a = tf.placeholder(shape=(1,3), dtype=tf.float32)<br/>b = tf.constant([[10,10,10]], dtype=tf.float32)<br/><br/>c = a + b<br/><br/>sess = tf.Session()<br/>res = sess.run(c, feed_dict={a:[[0.1,0.2,0.3]]})<br/>print(res)<br/><br/><strong>&gt;&gt; [[10.1 10.2 10.3]]</strong></pre>
<p>If the size of the first dimension of the input is not known during the creation of the graph, TensorFlow can take care of it. Just set it to <kbd>None</kbd>:</p>
<pre>import tensorflow as tf<br/>import numpy as np<br/><br/># NB: the first dimension is 'None', meaning that it can be of any length<br/>a = tf.placeholder(shape=(None,3), dtype=tf.float32)<br/>b = tf.placeholder(shape=(None,3), dtype=tf.float32)<br/><br/>c = a + b<br/>print(a)<br/><br/><strong>&gt;&gt; Tensor("Placeholder:0", shape=(?, 3), dtype=float32)</strong><br/><br/>sess = tf.Session()<br/>print(sess.run(c, feed_dict={a:[[0.1,0.2,0.3]], b:[[10,10,10]]}))<br/><br/><strong>&gt;&gt; [[10.1 10.2 10.3]]</strong><br/><br/>v_a = np.array([[1,2,3],[4,5,6]])<br/>v_b = np.array([[6,5,4],[3,2,1]])<br/>print(sess.run(c, feed_dict={a:v_a, b:v_b}))<br/><br/><strong>&gt;&gt; [[7. 7. 7.]</strong><br/><strong>    [7. 7. 7.]]</strong></pre>
<p>This feature is useful when the number of training examples is not known initially.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variable</h1>
                </header>
            
            <article>
                
<p>A <strong>variable</strong> is a mutable tensor that can be trained using an optimizer. F<span>or example, t</span>hey can be the free variables that constitute the weights and biases of a neural network. </p>
<p>We will now create two variables, one uniformly initialized, and one initialized with constant values:</p>
<pre>import tensorflow as tf<br/>import numpy as np<br/><br/># variable initialized randomly<br/>var = tf.get_variable("first_variable", shape=[1,3], dtype=tf.float32)<br/><br/># variable initialized with constant values<br/>init_val = np.array([4,5])<br/>var2 = tf.get_variable("second_variable", shape=[1,2], dtype=tf.int32, initializer=tf.constant_initializer(init_val))<br/><br/># create the session<br/>sess = tf.Session()<br/># initialize all the variables<br/>sess.run(tf.global_variables_initializer())<br/><br/>print(sess.run(var))<br/><br/><strong>&gt;&gt; [[ 0.93119466 -1.0498083  -0.2198658 ]]</strong><br/><br/>print(sess.run(var2))<br/><br/><strong>&gt;&gt; [[4 5]]</strong></pre>
<p>The variables aren't initialized until <kbd>global_variables_initializer()</kbd> is called.</p>
<p>All the variables created in this way are set as <kbd>trainable</kbd>, meaning that the graph can modify them, for example, after an optimization operation. The variables can be set as non-trainable, as follows:</p>
<pre>var2 = tf.get_variable("variable", shape=[1,2], trainable=False, dtype=tf.int32)</pre>
<p>An easy way to access all the variables is as follows:</p>
<pre>print(tf.global_variables())<br/><br/><strong>&gt;&gt; [&lt;tf.Variable 'first_variable:0' shape=(1, 3) dtype=float32_ref&gt;, &lt;tf.Variable 'second_variable:0' shape=(1, 2) dtype=int32_ref&gt;]</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a graph</h1>
                </header>
            
            <article>
                
<p>A <strong>graph</strong> represents low-level computations in terms of the dependencies between operations. In TensorFlow, you first define a graph, and then create a session that executes the operations in the graph.</p>
<p>The way a graph is built, computed, and optimized in TensorFlow allows a high degree of parallelism, distributed execution, and portability, all very important properties when building machine learning models.</p>
<p>To give you an idea of the structure of a graph produced internally by TensorFlow, the following program produces the computational graph demonstrated in <span>the following diagram:</span></p>
<pre>import tensorflow as tf<br/>import numpy as np<br/><br/>const1 = tf.constant(3.0, name='constant1')<br/><br/>var = tf.get_variable("variable1", shape=[1,2], dtype=tf.float32)<br/>var2 = tf.get_variable("variable2", shape=[1,2], trainable=False, dtype=tf.float32)<br/><br/>op1 = const1 * var<br/>op2 = op1 + var2<br/>op3 = tf.reduce_mean(op2)<br/><br/>sess = tf.Session()<br/>sess.run(tf.global_variables_initializer())<br/>sess.run(op3)</pre>
<p>This results in the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1805 image-border" src="assets/34e69931-f42d-439c-b29e-656073e965d4.png" style="width:23.00em;height:22.42em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2.3: Example of a computational graph</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Simple linear regression example</h1>
                </header>
            
            <article>
                
<p>To better digest all the concepts, let's now create a simple linear regression model. First, we have to import all the libraries and set a random seed, both for NumPy and TensorFlow (so that we'll all have the same results):</p>
<pre><span>import </span>tensorflow<span> as </span>tf<br/><span>import </span>numpy<span> as np</span><br/>from datetime import datetime<br/><br/><span>np.random.seed(10)</span><br/><span>tf.set_random_seed(10)</span></pre>
<p>Then, we can create a synthetic dataset consisting of 100 examples, as shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1806 image-border" src="assets/a558b08f-c230-46e9-b908-aae0172ee7e8.png" style="width:29.25em;height:22.08em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.4: Dataset used in the linear regression example</div>
<p><span>Because this is a linear regression example</span><span>, <em>y = W * X + b</em>, where <em>W</em></span><sub> </sub><span>and <em>b</em></span><span> are arbitrary values. In this example, we set</span> <span><kbd>W</kbd> = <kbd>0.5</kbd></span><span> and <kbd>b</kbd> = <kbd>1.4</kbd></span><span>. Additionally, we add some normal random noise:</span></p>
<pre><span>W, b = 0.5, 1.4</span><br/><span># create a dataset of 100 examples</span><br/><span>X = np.linspace(0,100, num=100)</span><br/><span># add random noise to the y labels</span><br/><span>y = np.random.normal(loc=W * X + b, scale=2.0, size=len(X))</span></pre>
<p>The next step involves creating the placeholders for the input and the output, and the variables of the weight and bias of the linear model. During training, these two variables will be optimized to be as similar as possible to the weight and bias of the dataset:</p>
<pre># create the placeholders<br/>x_ph = tf.placeholder(shape=[None,], dtype=tf.float32)<br/>y_ph = tf.placeholder(shape=[None,], dtype=tf.float32)<br/><br/># create the variables<br/>v_weight = tf.get_variable("weight", shape=[1], dtype=tf.float32)<br/>v_bias = tf.get_variable("bias", shape=[1], dtype=tf.float32)</pre>
<p>Then, we build the computational graph defining the linear operation and the <strong>mean squared error</strong> (<strong>MSE</strong>) loss:</p>
<pre><span># linear computation</span><br/><span>out = v_weight * x_ph + v_bias</span><br/><br/><span># compute the mean squared error</span><br/><span>loss = tf.reduce_mean((out - y_ph)**2)</span></pre>
<p>We can now instantiate the optimizer and call <kbd>minimize()</kbd> to minimize the MSE loss. <kbd>minimize()</kbd> first computes the gradients of the variables (<kbd>v_weight</kbd> and <kbd>v_bias</kbd>) and then applies the gradient, updating the variables:</p>
<pre>opt = tf.train.AdamOptimizer(0.4).minimize(loss)</pre>
<p>Now, let's create a session and initialize all the variables:</p>
<pre>session = tf.Session()<br/>session.run(tf.global_variables_initializer())</pre>
<p>The training is done by running the optimizer multiple times while feeding the dataset to the graph. To keep track of the state of the model, the MSE loss and the model variables (weight and bias) are printed <span>e</span>very 40 epochs:</p>
<pre># loop to train the parameters<br/>for ep in range(210):<br/>    # run the optimizer and get the loss<br/>    train_loss, _ = session.run([loss, opt], feed_dict={x_ph:X, y_ph:y})<br/> <br/>    # print epoch number and loss<br/>    if ep % 40 == 0:<br/>        print('Epoch: %3d, MSE: %.4f, W: %.3f, b: %.3f' % (ep, train_loss, session.run(v_weight), session.run(v_bias)))</pre>
<p>In the end, we can print the final values of the variables:</p>
<pre>print('Final weight: %.3f, bias: %.3f' % (session.run(v_weight), session.run(v_bias)))</pre>
<p>The output will be similar to the following:</p>
<pre><strong>&gt;&gt;  Epoch: 0, MSE: 4617.4390, weight: 1.295, bias: -0.407</strong><br/><strong>    Epoch: 40, MSE: 5.3334, weight: 0.496, bias: -0.727</strong><br/><strong>    Epoch: 80, MSE: 4.5894, weight: 0.529, bias: -0.012</strong><br/><strong>    Epoch: 120, MSE: 4.1029, weight: 0.512, bias: 0.608</strong><br/><strong>    Epoch: 160, MSE: 3.8552, weight: 0.506, bias: 1.092</strong><br/><strong>    Epoch: 200, MSE: 3.7597, weight: 0.501, bias: 1.418</strong><br/><strong>    Final weight: 0.500, bias: 1.473</strong></pre>
<p>During the training phase, it's possible to see that the MSE loss would decrease toward a non-zero value (of about 3.71). That's because we added random noise to the dataset that prevents the MSE from reaching a perfect value of 0.</p>
<p>Also, as anticipated, with regard to the weight and bias of the model approach, the values of <kbd>0.500</kbd> and <kbd>1.473</kbd> are precisely the values around which the dataset has been built. The blue line visible in the following screenshot is the prediction of the trained linear model, while the points are our training examples:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1807 image-border" src="assets/c1eeea8a-b731-4fd5-bff9-6679bd86cdea.png" style="width:30.08em;height:23.00em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.5: Linear regression model predictions</div>
<div class="packt_infobox">For all the color references in the chapter, please refer to the color images bundle: <a href="http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf.">http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing TensorBoard</h1>
                </header>
            
            <article>
                
<p>Keeping track of how variables change during the training of a model can be a tedious job. For instance, in the linear regression example, we kept track of the MSE loss and of the parameters of the model by printing them every 40 epochs. As the complexity of the algorithms increases, there is an increase in the number of variables and metrics to be monitored. Fortunately, this is where TensorBoard comes to the rescue.</p>
<p><span>TensorBoard</span><span> is a suite of visualization tools that can be used to plot metrics, visualize TensorFlow graphs, and visualize additional information. </span>A typical TensorBoard <span>screen </span>is similar to the one shown in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2062 image-border" src="assets/0c541147-d088-4565-9993-2e32ab112850.png" style="width:136.83em;height:80.92em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 2.6: Scalar TensorBoard page</div>
<p><span>The integration of TensorBoard </span><span>with Tensor</span><span>Flow code</span> <span>is pretty straightforward as it involves only a few tweaks to the code. In particular, to visualize the MSE loss over time and monitor the weight and bias of our linear regression model using TensorBoard, it is first necessary to attach the loss tensor to</span> <kbd>tf.summar.scalar()</kbd> <span>and the model's parameters to</span> <kbd>tf.summary.histogram()</kbd><em>. </em><span>The following snippet should be added after the call to the optimizer:</span></p>
<pre>tf.summary.scalar('MSEloss', loss)<br/>tf.summary.histogram('model_weight', v_weight)<br/>tf.summary.histogram('model_bias', v_bias)</pre>
<p>Then, to simplify the process and handle them as a single summary, we can merge them:</p>
<pre>all_summary = tf.summary.merge_all()</pre>
<p>At this point, we have to instantiate a <kbd>FileWriter</kbd> instance that will log all the summary information in a file:</p>
<pre>now = datetime.now()<br/>clock_time = "{}_{}.{}.{}".format(now.day, now.hour, now.minute, now.second)<br/>file_writer = tf.summary.FileWriter('log_dir/'+clock_time, tf.get_default_graph())</pre>
<p>The first two lines create a unique filename using the current date and time. I<span>n the third line,</span> the path of the file and the TensorFlow graph are passed <span>t</span><span>o </span><kbd>FileWriter()</kbd>. The second parameter is optional and represents the graph to visualize.</p>
<p>The final change is done in the training loop by replacing the previous line, <kbd>train_loss, _ = session.run(..)</kbd>, with the following:</p>
<pre>train_loss, _, train_summary = session.run([loss, opt, all_summary], feed_dict={x_ph:X, y_ph:y})<br/>file_writer.add_summary(train_summary, ep)</pre>
<p>First, <kbd>all_summary</kbd> is executed in the current session, and then the result is added to <kbd>file_writer</kbd> to be saved in the file. This procedure will run the three summaries that were merged <span>previously </span>and log them in the log file. TensorBoard will then read from this file and visualize the scalar, the two histograms, and the computation graph.</p>
<p>Remember to close <kbd>file_writer</kbd> <span>at the end, as follows:</span></p>
<pre>file_writer.close()</pre>
<p>Finally, we can open TensorBoard by going to the working directory and typing the following in a terminal:</p>
<pre><strong>$ tensorboard --logdir=log_dir</strong></pre>
<p>This command creates a web server that listens to port <kbd>6006</kbd>. To start TensorBoard, you have to go to the link that TensorBoard shows you:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1801 image-border" src="assets/7ca1b057-02de-40bc-994b-2822f44fecae.png" style="width:43.67em;height:15.83em;"/></p>
<div style="width: 1034px" class="packt_figref CDPAlignCenter CDPAlign">Figure 2.7: Histogram of the linear regression model's parameters</div>
<p class="mce-root">You can now browse TensorBoard by clicking on the tabs at the top of the page to access the plots, the histograms, and the graph. In the preceding—as well as the following—screenshots, you can see some of the results visualized on those pages. The plots and the graphs are interactive, so take some time to explore them in order to improve your understanding of their use. Also check the TensorBoard official documentation (<a href="https://www.tensorflow.org/guide/summaries_and_tensorboard">https://www.tensorflow.org/guide/summaries_and_tensorboard</a>) to learn more about the additional features included in TensorBoard:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1802 image-border" src="assets/6a568965-c104-40d1-94b3-536ae8e0bc0d.png" style="width:55.33em;height:27.58em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.8: Scalar plot of the MSE loss</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Types of RL environments</h1>
                </header>
            
            <article>
                
<p>Environments, similar to labeled datasets in supervised learning, are the essential part of RL as they dictate the information that has to be learned and the choice of algorithms. In this section, we'll take a look at the main differences between the types of environments and list some of the most important open source environments.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why different environments?</h1>
                </header>
            
            <article>
                
<p>While, for real applications, the choice of environment is dictated by the task to be learned, for research applications, usually, the choice is dictated by intrinsic features of the environment. In this latter case, the end goal is not to train the agent on a specific task, but to show some task-related capabilities.</p>
<p>For instance, if the goal is to create a multi-agent RL algorithm, the environment should have at least two agents with a means to communicate with one another, regardless of the end task. Instead, to create a lifelong learner (agents that continuously create and learn more difficult tasks using the knowledge acquired in previous easier tasks), the primary quality that the environment should have is the ability to adapt to new situations and a realistic domain. </p>
<p>Task aside, environments can differ by other characteristics, such as complexity, observation space, action space, and reward function: </p>
<ul>
<li><strong>Complexity</strong>: Environments can spread across a wide spectrum, from the balance of a pole to the manipulation of physical objects with a robot hand. More complex environments can be chosen to show the capability of an algorithm to deal with a large state space that mimics the complexity of the world. On the other hand, simpler ones can be used to show only some specific qualities.</li>
<li><strong>Observation space</strong>: As we have already seen, the observation space can range from the full state of the environment to only a partial observation perceived by the perception systems, such as row images.</li>
<li><strong>Action space</strong>: Environments with a large continuous action space challenge the agent to deal with real-value vectors, whereas discrete actions are easier to learn as they have only a limited number of actions available. </li>
<li><strong>Reward function</strong>: Environments with hard explorations and delayed rewards, such as Montezuma's revenge, are very challenging to solve. Surprisingly, only a few algorithms are able to reach human levels. For this reason, these environments are used as a test bed for algorithms that propose to address the exploration problem.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Open source environments</h1>
                </header>
            
            <article>
                
<p><span>How can we design an environment that meets our requirements? Fortunately, there are many open source environments that are built to tackle specific or broader problems. By way of an example, CoinRun, shown in the following screenshot, was created to measure the generalization capabilities of an algorithm:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1808 image-border" src="assets/a82a2e04-618c-49bb-b0b8-3f7a74f3cb1c.png" style="width:35.75em;height:31.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.9: The CoinRun environment</div>
<p>We will now list some of the main open source environments available. These are created by different teams and companies, but almost all of them use the OpenAI Gym interface:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1809 image-border" src="assets/63951647-17fb-4b34-b107-63834c6408da.png" style="width:49.33em;height:23.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 2.10: Roboschool environment</div>
<ul>
<li><strong>Gym Atari</strong> (<a href="https://gym.openai.com/envs/#atari">https://gym.openai.com/envs/#atari</a>): Includes Atari 2600 games with screen images as input. They are useful for measuring the performance of RL algorithms on a wide variety of games with the same observation space.</li>
<li><strong>Gym Classic control</strong> (<a href="https://gym.openai.com/envs/#classic_control">https://gym.openai.com/envs/#classic_control</a>): Classic games that can be used for the easy evaluation and debugging of an algorithm.</li>
<li><strong>Gym MuJoCo</strong> (<a href="https://gym.openai.com/envs/#mujoco">https://gym.openai.com/envs/#mujoco</a>): Includes continuous control tasks (such as Ant, and HalfCheetah) built on top of MuJoCo, a physics engine that requires a paid license (a free license is available for <span>students</span>).</li>
<li><strong>MalmoEnv</strong> (<a href="https://github.com/Microsoft/malmo">https://github.com/Microsoft/malmo</a>): An environment built on top of Minecraft.</li>
<li><strong>Pommerman</strong> (<a href="https://github.com/MultiAgentLearning/playground">https://github.com/MultiAgentLearning/playground</a>): A great environment for training multi-agent algorithms. Pommerman is a variant of the famous Bomberman.</li>
<li><strong>Roboschool</strong> (<a href="https://github.com/openai/roboschool">https://github.com/openai/roboschool</a>): A robot simulation environment integrated with OpenAI Gym. It includes an environment replica of MuJoCo, as shown in the preceding screenshot, two interactive environments to improve the robustness of the agent, and one multiplayer environment. </li>
<li><strong>Duckietown</strong> (<a href="https://github.com/duckietown/gym-duckietown">https://github.com/duckietown/gym-duckietown</a>): A self-driving car simulator with different maps and obstacles.</li>
<li><strong>PLE</strong> (<a href="https://github.com/ntasfi/PyGame-Learning-Environment">https://github.com/ntasfi/PyGame-Learning-Environment</a>): PLE includes many different arcade games, such as Monster Kong, FlappyBird, and Snake.</li>
<li><strong>Unity ML-Agents</strong> (<a href="https://github.com/Unity-Technologies/ml-agents">https://github.com/Unity-Technologies/ml-agents</a>): Environments built on top of Unity with realistic physics. ML-agents allow a great degree of freedom and the possibility to create your own environment using Unity.</li>
<li><strong>CoinRun</strong> (<a href="https://github.com/openai/coinrun">https://github.com/openai/coinrun</a>): An environment that addresses the problem of overfitting in RL. It generates different environments for training and testing.</li>
<li><strong>DeepMind Lab</strong> (<a href="https://github.com/deepmind/lab">https://github.com/deepmind/lab</a>): Provides a suite of 3D environments for navigation and puzzle tasks.</li>
<li><strong>DeepMind PySC2</strong> (<a href="https://github.com/deepmind/pysc2">https://github.com/deepmind/pysc2</a>): An environment for <span>learning </span>the complex game, StarCraft II.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Hopefully, in this chapter, you have learned about all the tools and components needed to build RL algorithms. You set up the Python environment required to develop RL algorithms and programmed your first algorithm using an OpenAI Gym environment. As the majority of state-of-the-art RL algorithms involve deep learning, you have been introduced to TensorFlow, a deep learning framework that you'll use throughout the book. The use of TensorFlow speeds up the development of deep RL algorithms as it deals with complex parts of deep neural networks such as backpropagation. Furthermore, TensorFlow is provided with TensorBoard, a visualization tool that is used to monitor and help the algorithm debugging process. </p>
<p>Because we'll be using many environments in the subsequent chapters, it's important to have a clear understanding of their differences and distinctiveness. By now, you should also be able to choose the best environments for your own projects, but bear in mind that despite the fact that we provided you with a comprehensive list, there may be many others that could <span>better suit </span><span>your problem.</span></p>
<p>That being said, in the following chapters, you'll finally learn how to develop RL algorithms. Specifically, in the next chapter, you will be presented with algorithms that can be used in simple problems where the environment is completely known. After those, we'll build more sophisticated ones that can deal with more complex cases.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What's the output of the <kbd>step()</kbd> function in Gym?</li>
<li>How can you sample an action using the OpenAI Gym interface?</li>
<li>What's the main difference between the <kbd>Box</kbd> and <kbd>Discrete</kbd> classses?</li>
<li>Why are deep learning frameworks used in RL?</li>
<li>What's a tensor?</li>
<li>What can be visualized in TensorBoard?</li>
<li>To create a self-driving car, which of the environments mentioned in the chapter would you use?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>For the <span>TensorFlow </span>official guide, refer to the following link: <a href="https://www.tensorflow.org/guide/low_level_intro">https://www.tensorflow.org/guide/low_level_intro</a>.</li>
<li>For the <span>TensorBoard </span>official guide, refer to the following link: <a href="https://www.tensorflow.org/guide/summaries_and_tensorboard">https://www.tensorflow.org/guide/summaries_and_tensorboard</a>.</li>
</ul>


            </article>

            
        </section>
    </body></html>