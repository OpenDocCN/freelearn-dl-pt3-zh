["```\ngit clone https://github.com/PacktPublishing/learn-tensorflow-enterprise.git\n```", "```\npip install keras-tuner\n```", "```\ntf.keras.layers.Dense(units = hp_units, activation = 'relu')\n```", "```\nhp = kt.HyperParameters()\n```", "```\nhp_units = hp.Int('units', min_value = 64, max_value = 256, step = 16)\n```", "```\nhp_units = hp.Choice('units', values = [64, 80, 90])\n```", "```\nhp_activation = hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'])\n```", "```\ntf.keras.layers.Dense(units = hp_units, activation = hp_activation)\n```", "```\nhp_optimizer = hp.Choice('selected_optimizer', ['sgd', 'adam'])\n```", "```\nmodel.compile(optimizer = hp_optimizer, loss = …, metrics = …)\n```", "```\nhp_learning_rate = hp.Float('learning_rate', min_value = 1e-4, max_value = 1e-2, step = 1e-3)\n```", "```\noptimizer=tf.keras.optimizers.SGD(lr=hp_learning_rate, momentum=0.5)\n```", "```\ndef model_builder(hp):\n```", "```\n    hp_units = hp.Int('units', min_value = 64, max_value = 256, \n```", "```\n                                                     step = 64) \n```", "```\n    hp_activation = hp.Choice('dense_activation', \n```", "```\n        values=['relu', 'tanh', 'sigmoid'])\n```", "```\n    IMAGE_SIZE = (224, 224)\n```", "```\n    model = tf.keras.Sequential([\n```", "```\n    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)), \n```", "```\n    hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4', trainable=False),\n```", "```\n    tf.keras.layers.Flatten(),\n```", "```\n    tf.keras.layers.Dense(units = hp_units, \n```", "```\n                          activation = hp_activation, \n```", "```\n                          kernel_initializer='glorot_uniform'),\n```", "```\n    tf.keras.layers.Dense(5, activation='softmax', \n```", "```\n                                         name = 'custom_class')\n```", "```\n    ])\n```", "```\n    model.build([None, 224, 224, 3])\n```", "```\n    model.compile(\n```", "```\n        optimizer=tf.keras.optimizers.SGD(lr=1e-2, \n```", "```\n                                                momentum=0.5), \n```", "```\n        loss=tf.keras.losses.CategoricalCrossentropy(\n```", "```\n                        from_logits=True, label_smoothing=0.1),\n```", "```\n        metrics=['accuracy'])\n```", "```\nreturn model\n```", "```\nimport kerastuner as kt\n```", "```\nimport tensorflow_hub as hub\n```", "```\nimport tensorflow as tf\n```", "```\nfrom absl import flags\n```", "```\nflags_obj = flags.FLAGS\n```", "```\nstrategy = tf.distribute.MirroredStrategy()\n```", "```\ntuner = kt.Hyperband(\n```", "```\n            hypermodel = model_builder,\n```", "```\n            objective = 'val_accuracy', \n```", "```\n            max_epochs = 3,\n```", "```\n            factor = 2,\n```", "```\n            distribution_strategy=strategy,\n```", "```\n            directory = flags_obj.model_dir,\n```", "```\n            project_name = 'hp_tune_hb',\n```", "```\n            overwrite = True)\n```", "```\nhp_units = hp.Int('units', min_value = 64, max_value = 256, step = 64)\n```", "```\nhp_activation = hp.Choice('dense_activation', values=['relu', 'tanh', 'sigmoid'])\n```", "```\ntf.keras.layers.Dense(units = hp_units, activation = hp_activation, kernel_initializer='glorot_uniform'),\n```", "```\n    tuner.search(train_ds,\n            steps_per_epoch=STEPS_PER_EPOCHS,\n            validation_data=val_ds,\n            validation_steps=VALIDATION_STEPS,\n            epochs=30,\n            callbacks=[tf.keras.callbacks.EarlyStopping(\n                                            'val_accuracy')])\n    ```", "```\n    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n    print(f'''\n            The hyperparameter search is done. \n            The best number of nodes in the dense layer is {best_hps.get('units')}.\n            The best activation function in mid dense layer is {best_hps.get('dense_activation')}.\n            ''')\n    ```", "```\n    model = tuner.hypermodel.build(best_hps)\n    ```", "```\n    checkpoint_prefix = os.path.join(flags_obj.model_dir, 'best_hp_train_ckpt_{epoch}')\n        callbacks = [\n        tf.keras.callbacks.ModelCheckpoint(\n    \t\tfilepath=checkpoint_prefix,\n    \t\tsave_weights_only=True)]\n    ```", "```\n    model.fit(\n            train_ds,\n            epochs=30, steps_per_epoch=STEPS_PER_EPOCHS,\n            validation_data=val_ds,\n            validation_steps=VALIDATION_STEPS,\n            callbacks=callbacks)\n    ```", "```\n    model_save_dir = os.path.join(flags_obj.model_dir, \n                                           'best_save_model')\n    model.save(model_save_dir)\n    ```", "```\ntuner = kt.BayesianOptimization(\n```", "```\n            hypermodel = model_builder,\n```", "```\n            objective ='val_accuracy',\n```", "```\n            max_trials = 50,\n```", "```\n            directory = flags_obj.model_dir,\n```", "```\n            project_name = 'hp_tune_bo',\n```", "```\n            overwrite = True\n```", "```\n            )\n```", "```\ntuner.search(train_ds,\n```", "```\n        steps_per_epoch=STEPS_PER_EPOCHS,\n```", "```\n        validation_data=val_ds,\n```", "```\n        validation_steps=VALIDATION_STEPS,\n```", "```\n        epochs=30,\n```", "```\n        callbacks=[tf.keras.callbacks.EarlyStopping(\n```", "```\n                                              'val_accuracy')])\n```", "```\ntuner = kt.RandomSearch(\n```", "```\n            hypermodel = model_builder, \n```", "```\n            objective='val_accuracy',\n```", "```\n            max_trials = 5,\n```", "```\n            directory = flags_obj.model_dir,\n```", "```\n            project_name = 'hp_tune_rs',\n```", "```\n            overwrite = True)\n```", "```\n    from absl import flags\n    from absl import logging\n    from absl import app\n    ```", "```\n    tf.compat.v1.flags.DEFINE_string('model_dir', 'default_model_dir', 'Directory or bucket for storing checkpoint model.')\n    tf.compat.v1.flags.DEFINE_bool('fine_tuning_choice', False, 'Retrain base parameters')\n    tf.compat.v1.flags.DEFINE_integer('train_batch_size', 32, 'Number of samples in a training batch')\n    tf.compat.v1.flags.DEFINE_integer('validation_batch_size', 40, 'Number of samples in a validation batch')\n    tf.compat.v1.flags.DEFINE_float('learning_rate', 0.01, 'Initial learning rate.')\n    tf.compat.v1.flags.DEFINE_string('tuner_type', 'Hyperband', 'Type of tuner. Default is hyperband')\n    ```", "```\n     flags_obj = flags.FLAGS\n    ```", "```\n    flags_obj.model_dir\n    ```", "```\n    import kerastuner as kt\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    import tensorflow_datasets as tfds\n    import os\n    import IPython\n    from kerastuner import HyperParameters\n    from absl import flags\n    from absl import logging\n    from absl import app\n    ```", "```\n    tf.compat.v1.flags.DEFINE_string('model_dir', 'default_model_dir', 'Directory or bucket for storing checkpoint model.')\n    tf.compat.v1.flags.DEFINE_bool('fine_tuning_choice', False, 'Retrain base parameters')\n    tf.compat.v1.flags.DEFINE_integer('train_batch_size', 32, 'Number of samples in a training batch')\n    tf.compat.v1.flags.DEFINE_integer('validation_batch_size', 40, 'Number of samples in a validation batch')\n    ```", "```\n    def get_builtin_data():\n        data_dir = tf.keras.utils.get_file(\n    'flower_photos', 'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n        \tuntar=True)\n        return data_dir\n    ```", "```\n    def make_generators(data_dir, flags_obj):\n        BATCH_SIZE = flags_obj.train_batch_size\n        IMAGE_SIZE = (224, 224)\n        datagen_kwargs = dict(rescale=1./255, \n                                        validation_split=.20)\n        dataflow_kwargs = dict(target_size=IMAGE_SIZE, \n                               batch_size=BATCH_SIZE,\n                               interpolation='bilinear')\n        valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n            **datagen_kwargs)\n        valid_generator = valid_datagen.flow_from_directory(\n            data_dir, subset='validation', shuffle=False, **dataflow_kwargs)\n    ```", "```\n        do_data_augmentation = False \n        if do_data_augmentation:\n            train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n                rotation_range=40,\n                horizontal_flip=True,\n                width_shift_range=0.2, \n                height_shift_range=0.2,\n                shear_range=0.2, zoom_range=0.2,\n                **datagen_kwargs)\n        else:\n            train_datagen = valid_datagen\n            train_generator = train_datagen.flow_from_directory(\n                data_dir, subset='training', shuffle=True, **dataflow_kwargs)\n    return train_generator, valid_generator\n    ```", "```\n    def map_labels(train_generator):\n        labels_idx = (train_generator.class_indices)\n        idx_labels = dict((v,k) for k,v in labels_idx.items())\n    return idx_labels\n    ```", "```\n    def model_builder(hp):\n    os.environ['TFHUB_CACHE_DIR'] =      \n             '/Users/XXXXX/Downloads/imagenet_resnet_v2_50_feature_vector_4'\n        hp_units = hp.Int('units', min_value = 64, \n                                  max_value = 256, step = 64)\n        IMAGE_SIZE = (224, 224)\n        model = tf.keras.Sequential([\n        tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)), \n        hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4', trainable=False),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units = hp_units, \n                                         activation = 'relu', \n                        kernel_initializer='glorot_uniform'),\n        tf.keras.layers.Dense(5, activation='softmax', \n                                       name = 'custom_class')\n        ])\n        model.build([None, 224, 224, 3])\n        hp_learning_rate = hp.Choice('learning_rate', \n                                       values = [1e-2, 1e-4])\n        model.compile(\n            optimizer=tf.keras.optimizers.SGD(\n                          lr=hp_learning_rate, momentum=0.5), \n            loss=tf.keras.losses.CategoricalCrossentropy(\n                      from_logits=True, label_smoothing=0.1),\n            metrics=['accuracy'])\n        return model\n    ```", "```\n    class ClearTrainingOutput(tf.keras.callbacks.Callback):\n            def on_train_end(*args, **kwargs):\n                IPython.display.clear_output(wait = True)\n    ```", "```\n    def main(_):\n        flags_obj = flags.FLAGS\n        strategy = tf.distribute.MirroredStrategy()\n        data_dir = get_builtin_data()\n        train_gtr, validation_gtr = make_generators(data_dir, \t                                               flags_obj)\n        idx_labels = map_labels(train_gtr)\n    ```", "```\n    '''Runs the hyperparameter search.'''\n```", "```\n    if(flags_obj.tuner_type.lower() == 'BayesianOptimization'.lower()):\n```", "```\n        tuner = kt.BayesianOptimization(\n```", "```\n            hypermodel = model_builder,\n```", "```\n            objective ='val_accuracy',\n```", "```\n            tune_new_entries = True,\n```", "```\n            allow_new_entries = True,\n```", "```\n            max_trials = 5,\n```", "```\n            directory = flags_obj.model_dir,\n```", "```\n            project_name = 'hp_tune_bo',\n```", "```\n            overwrite = True\n```", "```\n            )\n```", "```\n    elif (flags_obj.tuner_type.lower() == 'RandomSearch'.lower()):\n```", "```\n        tuner = kt.RandomSearch(\n```", "```\n            hypermodel = model_builder, \n```", "```\n            objective='val_accuracy',\n```", "```\n            tune_new_entries = True, \n```", "```\n            allow_new_entries = True,\n```", "```\n            max_trials = 5,\n```", "```\n            directory = flags_obj.model_dir,\n```", "```\n            project_name = 'hp_tune_rs',\n```", "```\n            overwrite = True)\n```", "```\nelse: \n```", "```\n    # Default choice for tuning algorithm is hyperband.\n```", "```\n        tuner = kt.Hyperband(\n```", "```\n            hypermodel = model_builder,\n```", "```\n            objective = 'val_accuracy', \n```", "```\n            max_epochs = 3,\n```", "```\n            factor = 2,\n```", "```\n            distribution_strategy=strategy,\n```", "```\n            directory = flags_obj.model_dir,\n```", "```\n            project_name = 'hp_tune_hb',\n```", "```\n            overwrite = True)\n```", "```\n    best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n```", "```\n    print(f'''\n```", "```\n        The hyperparameter search is done. \n```", "```\n        The best number of nodes in the dense layer is {best_hps.get('units')}.\n```", "```\n        The optimal learning rate for the optimizer is       {best_hps.get('learning_rate')}.\n```", "```\n        ''')\n```", "```\n    # Build the model with the optimal hyperparameters and train it on the data\n```", "```\n    model = tuner.hypermodel.build(best_hps)\n```", "```\n    checkpoint_prefix = os.path.join(flags_obj.model_dir, 'best_hp_train_ckpt_{epoch}')\n```", "```\n    callbacks = [\n```", "```\n    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n```", "```\n                                       save_weights_only=True)]\n```", "```\n    steps_per_epoch = train_gtr.samples // train_gtr.batch_size\n```", "```\n    validation_steps = validation_gtr.samples // validation_gtr.batch_size\n```", "```\n    model.fit(\n```", "```\n        train_gtr,\n```", "```\n        epochs=3, steps_per_epoch=steps_per_epoch,\n```", "```\n        validation_data=validation_gtr,\n```", "```\n        validation_steps=validation_steps,\n```", "```\n        callbacks=callbacks)\n```", "```\nlogging.info('INSIDE MAIN FUNCTION user input model_dir %s', \t                                           flags_obj.model_dir)\n```", "```\n    # Save model trained with chosen HP in user specified bucket location\n```", "```\n    model_save_dir = os.path.join(flags_obj.model_dir, \n```", "```\n                                             'best_save_model')\n```", "```\n    model.save(model_save_dir)\n```", "```\nif __name__ == '__main__':\n```", "```\n    app.run(main)\n```", "```\npython3 hp_kt_resnet_local_pub.py \\\n```", "```\n--model_dir=resnet_local_hb_output  \\\n```", "```\n--train_epoch_best=2 \\\n```", "```\n--tuner_type=hyperband\n```", "```\n    from setuptools import find_packages\n    from setuptools import setup\n    setup(\n        name='official',\n        install_requires=['IPython', 'keras-tuner', 'tensorflow-datasets~=3.1', 'tensorflow_hub>=0.6.0'],\n        packages=find_packages()\n    )\n    ```", "```\n    gcloud ai-platform jobs submit training hp_kt_resnet_tpu_hb_test \\\n    --staging-bucket=gs://ai-tpu-experiment \\\n    --package-path=tfk \\\n    --module-name=tfk.tuner.hp_kt_resnet_tpu_act \\\n    --runtime-version=2.2 \\\n    --python-version=3.7 \\\n    --scale-tier=BASIC_TPU \\\n    --region=us-central1 \\\n    --use-chief-in-tf-config='true' \\\n    -- \\\n    --distribution_strategy=tpu \\\n    --data_dir=gs://ai-tpu-experiment/tfrecord-flowers \\\n    --model_dir=gs://ai-tpu-experiment/hp_kt_resnet_tpu_hb_test \\\n    --tuner_type=hyperband\n    ```"]