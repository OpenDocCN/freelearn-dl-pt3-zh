<html><head></head><body>
		<div id="_idContainer074">
			<h1 id="_idParaDest-108"><em class="italic"><a id="_idTextAnchor135"/>Chapter 4</em>: Reinforcement Learning in the Real World – Building Cryptocurrency Trading Agents</h1>
			<p><strong class="bold">Deep reinforcement learning</strong> (<strong class="bold">deep RL</strong>) agents have a lot of potential when it comes to solving challenging problems in the real world and a lot of opportunities exist. However, only a few successful stories of using deep RL agents in the real world beyond games exist due to the various challenges associated with real-world deployments of RL agents. This chapter contains recipes that will help you successfully develop RL agents for an interesting and rewarding real-world problem: <strong class="bold">cryptocurrency trading</strong>. The recipes in this chapter contain information on how to implement custom OpenAI Gym-compatible learning environments for cryptocurrency trading with both discrete and continuous-value action spaces. In addition, you will learn how to build and train RL agents for trading cryptocurrency. Trading learning environments will also be provided.</p>
			<p>Specifically, the following recipes will be covered in this chapter:</p>
			<ul>
				<li>Building a Bitcoin trading RL platform using real market data</li>
				<li>Building an Ethereum trading RL platform using price charts</li>
				<li>Building an advanced cryptocurrency trading platform for RL agents</li>
				<li>Training a cryptocurrency trading bot using RL</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor136"/>Technical requirements</h1>
			<p>The code in the book has been extensively tested on Ubuntu 18.04 and Ubuntu 20.04 and should work with later versions of Ubuntu if Python 3.6+ is available. With Python 3.6+ installed, along with the necessary Python packages listed at the start of each of recipe, the code should run fine on Windows and macOS X too. You should create and use a Python virtual environment named <strong class="source-inline">tf2rl-cookbook</strong> to install the packages and run the code in this book. Installing Miniconda or Anaconda for Python virtual environment management is recommended. </p>
			<p>The complete code for each recipe in each chapter is available here: <a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook</a>.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor137"/>Building a Bitcoin trading RL platform using real market data</h1>
			<p>This recipe will <a id="_idIndexMarker391"/>help you build a<a id="_idIndexMarker392"/> cryptocurrency trading RL environment for your agents. This environment simulates a Bitcoin trading exchange based on real-world data from the Gemini cryptocurrency exchange. In this environment, your RL agent can place buy/sell/hold trades and get rewards based on the profit/loss it makes, starting with an initial cash balance in the agent's trading account.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor138"/>Getting ready</h2>
			<p>To complete this recipe, make sure you have the latest version. You will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure to update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>) in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without issues, you are ready to get started:</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from typing import Dict</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">from gym import spaces</p>
			<p>Now, let's begin!</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor139"/>How to do it…</h2>
			<p>Follow these steps to learn how to implement <strong class="source-inline">CryptoTradingEnv</strong>:</p>
			<ol>
				<li>Let's begin by importing the necessary Python modules.</li>
				<li>We'll also be using the <strong class="source-inline">TradeVisualizer</strong> class implemented in <strong class="source-inline">trading_utils.py</strong>. We'll discuss this in more deail when we actually use it:<p class="source-code">from trading_utils import TradeVisualizer</p></li>
				<li>To make it easy to configure the cryptocurrency trading environment, we will set up an<a id="_idIndexMarker393"/> environment<a id="_idIndexMarker394"/> config dictionary. Notice that our cryptocurrency trading environment has been configured so that we can trade Bitcoin based on real data from the Gemini cryptocurrency exchange:<p class="source-code">env_config = {</p><p class="source-code">    "exchange": "Gemini", # Cryptocurrency exchange</p><p class="source-code">    # (Gemini, coinbase, kraken, etc.)</p><p class="source-code">    "ticker": "BTCUSD", # CryptoFiat</p><p class="source-code">    "frequency": "daily", # daily/hourly/minutes</p><p class="source-code">    "opening_account_balance": 100000,</p><p class="source-code">    # Number of steps (days) of data provided to the </p><p class="source-code">    # agent in one observation.</p><p class="source-code">    "observation_horizon_sequence_length": 30,</p><p class="source-code">    "order_size": 1, # Number of coins to buy per </p><p class="source-code">    # buy/sell order</p><p class="source-code">}</p></li>
				<li>Let's begin our <strong class="source-inline">CryptoTradingEnv</strong> class definition:<p class="source-code">class CryptoTradingEnv(gym.Env):</p><p class="source-code">    def __init__(self, env_config: Dict = env_config):</p><p class="source-code">        super(CryptoTradingEnv, self).__init__()</p><p class="source-code">        self.ticker = env_config.get("ticker", "BTCUSD")</p><p class="source-code">        data_dir = os.path.join(os.path.dirname(os.path.\</p><p class="source-code">                             realpath(__file__)), "data")</p><p class="source-code">        self.exchange = env_config["exchange"]</p><p class="source-code">        freq = env_config["frequency"]</p><p class="source-code">        if freq == "daily":</p><p class="source-code">            self.freq_suffix = "d"</p><p class="source-code">        elif freq == "hourly":</p><p class="source-code">            self.freq_suffix = "1hr"</p><p class="source-code">        elif freq == "minutes":</p><p class="source-code">            self.freq_suffix = "1min"</p></li>
				<li>We'll be<a id="_idIndexMarker395"/> using a file object<a id="_idIndexMarker396"/> as our cryptocurrency exchange data source. We must make sure that the data source exists before loading/streaming the data into memory:<p class="source-code">        self.ticker_file_stream = os.path.join(</p><p class="source-code">            f"{data_dir}",</p><p class="source-code">            f"{'_'.join([self.exchange, self.ticker,</p><p class="source-code">                         self.freq_suffix])}.csv",</p><p class="source-code">        )</p><p class="source-code">        assert os.path.isfile(</p><p class="source-code">            self.ticker_file_stream</p><p class="source-code">        ), f"Cryptocurrency data file stream not found \</p><p class="source-code">          at: data/{self.ticker_file_stream}.csv"</p><p class="source-code">        # Cryptocurrency exchange data stream. An offline </p><p class="source-code">        # file stream is used. Alternatively, a web</p><p class="source-code">        # API can be used to pull live data.</p><p class="source-code">        self.ohlcv_df = pd.read_csv(self.ticker_file_\</p><p class="source-code">            stream, skiprows=1).sort_values(by="Date"</p><p class="source-code">        )</p></li>
				<li>The opening<a id="_idIndexMarker397"/> balance in<a id="_idIndexMarker398"/> the Agent's account is configured using <strong class="source-inline">env_config</strong>. Let's initialize the opening account balance based on the configured value:<p class="source-code">self.opening_account_balance = env_config["opening_account_balance"]</p></li>
				<li>Next, let's define the action and observation space for this cryptocurrency trading environment using the standard space type definitions provided by the OpenAI Gym library:<p class="source-code">        # Action: 0-&gt; Hold; 1-&gt; Buy; 2 -&gt;Sell;</p><p class="source-code">        self.action_space = spaces.Discrete(3)</p><p class="source-code">        self.observation_features = [</p><p class="source-code">            "Open",</p><p class="source-code">            "High",</p><p class="source-code">            "Low",</p><p class="source-code">            "Close",</p><p class="source-code">            "Volume BTC",</p><p class="source-code">            "Volume USD",</p><p class="source-code">        ]</p><p class="source-code">        self.horizon = env_config.get(</p><p class="source-code">                   "observation_horizon_sequence_length")</p><p class="source-code">        self.observation_space = spaces.Box(</p><p class="source-code">            low=0,</p><p class="source-code">            high=1,</p><p class="source-code">            shape=(len(self.observation_features), </p><p class="source-code">                       self.horizon + 1),</p><p class="source-code">            dtype=np.float,</p><p class="source-code">        )</p></li>
				<li>Let's define <a id="_idIndexMarker399"/>the trade <a id="_idIndexMarker400"/>order size that will be executed when the agent places a trade:<p class="source-code">        self.order_size = env_config.get("order_size")</p></li>
				<li>With that, we have successfully initialized the environment! Now, let's move on and define the <strong class="source-inline">step(…)</strong> method. You will notice that we have simplified the implementation of the <strong class="source-inline">step (…)</strong> method for ease of understanding using two helper member methods: <strong class="source-inline">self.execute_trade_action</strong> and <strong class="source-inline">self.get_observation</strong>. We'll define these helper member methods later, once we have finished implementing the basic RL Gym environment methods (<strong class="source-inline">step</strong>, <strong class="source-inline">reset</strong>, and <strong class="source-inline">render</strong>) . Now, let's look at the implementation of the <strong class="source-inline">step</strong> method:<p class="source-code">def step(self, action):</p><p class="source-code">        # Execute one step within the trading environment</p><p class="source-code">        self.execute_trade_action(action)</p><p class="source-code">        self.current_step += 1</p><p class="source-code">        reward = self.account_value - \</p><p class="source-code">                 self.opening_account_balance  </p><p class="source-code">                # Profit (loss)</p><p class="source-code">        done = self.account_value &lt;= 0 or \</p><p class="source-code">               self.current_step &gt;= len(</p><p class="source-code">            self.ohlcv_df.loc[:, "Open"].values</p><p class="source-code">        )</p><p class="source-code">        obs = self.get_observation()</p><p class="source-code">        return obs, reward, done, {}</p></li>
				<li>Now, let's <a id="_idIndexMarker401"/>define <a id="_idIndexMarker402"/>the <strong class="source-inline">reset()</strong> method, which will be executed at the start of every episode:<p class="source-code">def reset(self):</p><p class="source-code">        # Reset the state of the environment to an </p><p class="source-code">        # initial state</p><p class="source-code">        self.cash_balance = self.opening_account_balance</p><p class="source-code">        self.account_value = self.opening_account_balance</p><p class="source-code">        self.num_coins_held = 0</p><p class="source-code">        self.cost_basis = 0</p><p class="source-code">        self.current_step = 0</p><p class="source-code">        self.trades = []</p><p class="source-code">        if self.viz is None:</p><p class="source-code">            self.viz = TradeVisualizer(</p><p class="source-code">                self.ticker,</p><p class="source-code">                self.ticker_file_stream,</p><p class="source-code">                "TFRL-Cookbook Ch4-CryptoTradingEnv",</p><p class="source-code">                skiprows=1,  # Skip the first line with </p><p class="source-code">                # the data download source URL</p><p class="source-code">            )</p><p class="source-code">        return self.get_observation()</p></li>
				<li>As the next <a id="_idIndexMarker403"/>step, we'll <a id="_idIndexMarker404"/>define the <strong class="source-inline">render()</strong> method, which will provide us with a view into the cryptocurrency trading environment so that we understand what's going on! This is where we will be using the <strong class="source-inline">TradeVisualizer</strong> class from the <strong class="source-inline">trading_utils.py</strong> file. <strong class="source-inline">TradeVisualizer</strong> helps us visualize the live account balance of the Agent as the Agent learns in the environment. The visualizer also provides a visual indication of the buy and sell trades that the Agent performs by performing actions in the environment. A sample screenshot of the output from the <strong class="source-inline">render()</strong> method has been provided here for your reference:<div id="_idContainer070" class="IMG---Figure"><img src="image/B15074_04_01.jpg" alt="Figure 4.1 – A sample rendering of the CryptoTradingEnv environment "/></div><p class="figure-caption">Figure 4.1 – A sample rendering of the CryptoTradingEnv environment</p><p>Now, let's<a id="_idIndexMarker405"/> implement<a id="_idIndexMarker406"/> the <strong class="source-inline">render()</strong> method:</p><p class="source-code">    def render(self, **kwargs):</p><p class="source-code">        # Render the environment to the screen</p><p class="source-code">        if self.current_step &gt; self.horizon:</p><p class="source-code">            self.viz.render(</p><p class="source-code">                self.current_step,</p><p class="source-code">                self.account_value,</p><p class="source-code">                self.trades,</p><p class="source-code">                window_size=self.horizon,</p><p class="source-code">            )</p></li>
				<li>Next, we'll implement a method that will close all the visualization windows once the training is complete:<p class="source-code">    def close(self):</p><p class="source-code">        if self.viz is not None:</p><p class="source-code">            self.viz.close()</p><p class="source-code">            self.viz = None</p></li>
				<li>Now, we can<a id="_idIndexMarker407"/> implement<a id="_idIndexMarker408"/> the <strong class="source-inline">execute_trade_action</strong> method, which we used in the <strong class="source-inline">step (…)</strong> method earlier in Step 9. We'll split the implementation into three steps, one for each order type: Hold, Buy, and Sell. Let's start with the Hold order type as that's the simplest. You will see why in a bit!<p class="source-code">    def execute_trade_action(self, action):</p><p class="source-code">        if action == 0:  # Hold position</p><p class="source-code">            return</p><p class="source-code">        </p></li>
				<li>We actually need to implement one more intermediate step before we can move on and implement the Buy and Sell order execution logic. Here, we must determine the order type (buy versus sell) and then the price of the Bitcoin at the current simulated time:<p class="source-code">        order_type = "buy" if action == 1 else "sell"</p><p class="source-code">        # Stochastically determine the current price </p><p class="source-code">        # based on Market Open &amp; Close</p><p class="source-code">        current_price = random.uniform(</p><p class="source-code">            self.ohlcv_df.loc[self.current_step, "Open"],</p><p class="source-code">            self.ohlcv_df.loc[self.current_step, </p><p class="source-code">                              "Close"],</p><p class="source-code">        )</p></li>
				<li>Now, we <a id="_idIndexMarker409"/>are ready to <a id="_idIndexMarker410"/>implement the logic for executing a Buy trade order, as follows:<p class="source-code">        if order_type == "buy":</p><p class="source-code">            allowable_coins = \</p><p class="source-code">                int(self.cash_balance / current_price)</p><p class="source-code">            if allowable_coins &lt; self.order_size:</p><p class="source-code">                # Not enough cash to execute a buy order</p><p class="source-code">                return</p><p class="source-code">            # Simulate a BUY order and execute it at </p><p class="source-code">            # current_price</p><p class="source-code">            num_coins_bought = self.order_size</p><p class="source-code">            current_cost = self.cost_basis * \</p><p class="source-code">                           self.num_coins_held</p><p class="source-code">            additional_cost = num_coins_bought * \</p><p class="source-code">                              current_price</p><p class="source-code">            self.cash_balance -= additional_cost</p><p class="source-code">            self.cost_basis = (current_cost + \</p><p class="source-code">                additional_cost) / (</p><p class="source-code">                self.num_coins_held + num_coins_bought</p><p class="source-code">            )</p><p class="source-code">            self.num_coins_held += num_coins_bought</p></li>
				<li>Let's update<a id="_idIndexMarker411"/> the <strong class="source-inline">trades</strong> list<a id="_idIndexMarker412"/> with the latest buy trade:<p class="source-code">            self.trades.append(</p><p class="source-code">                {</p><p class="source-code">                    "type": "buy",</p><p class="source-code">                    "step": self.current_step,</p><p class="source-code">                    "shares": num_coins_bought,</p><p class="source-code">                    "proceeds": additional_cost,</p><p class="source-code">                }</p><p class="source-code">            )</p></li>
				<li>The next step is to implement the logic for executing Sell trade orders:<p class="source-code">        elif order_type == "sell":</p><p class="source-code">            # Simulate a SELL order and execute it at </p><p class="source-code">            # current_price</p><p class="source-code">            if self.num_coins_held &lt; self.order_size:</p><p class="source-code">                # Not enough coins to execute a sell </p><p class="source-code">                # order</p><p class="source-code">                return</p><p class="source-code">            num_coins_sold = self.order_size</p><p class="source-code">            self.cash_balance += num_coins_sold * \</p><p class="source-code">                                 current_price</p><p class="source-code">            self.num_coins_held -= num_coins_sold</p><p class="source-code">            sale_proceeds = num_coins_sold * \</p><p class="source-code">                            current_price</p><p class="source-code">            self.trades.append(</p><p class="source-code">                {</p><p class="source-code">                    "type": "sell",</p><p class="source-code">                    "step": self.current_step,</p><p class="source-code">                    "shares": num_coins_sold,</p><p class="source-code">                    "proceeds": sale_proceeds,</p><p class="source-code">                }</p><p class="source-code">            )</p></li>
				<li>To finish up<a id="_idIndexMarker413"/> our trade <a id="_idIndexMarker414"/>execution function, we need to add a couple of lines of code that will update the account value once the trade order has been executed:<p class="source-code">        if self.num_coins_held == 0:</p><p class="source-code">            self.cost_basis = 0</p><p class="source-code">        # Update account value</p><p class="source-code">        self.account_value = self.cash_balance + \</p><p class="source-code">                             self.num_coins_held * \</p><p class="source-code">                             current_price</p></li>
				<li>With that, we have finished implementing a Bitcoin trading RL environment powered by real BTCUSD data from the Gemini cryptocurrency exchange! Let's look at how we can easily create the environment and run a sample, rather than using a random agent in this environment with just six lines of code:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = CryptoTradingEnv()</p><p class="source-code">    obs = env.reset()</p><p class="source-code">    for _ in range(600):</p><p class="source-code">        action = env.action_space.sample()</p><p class="source-code">        next_obs, reward, done, _ = env.step(action)</p><p class="source-code">        env.render()</p><p>You should<a id="_idIndexMarker415"/> see the sample<a id="_idIndexMarker416"/> random agent acting in the <strong class="source-inline">CryptoTradingEnv</strong> environment. The <strong class="source-inline">env.render()</strong> function should produce a rendering that looks similar to the following:</p></li>
			</ol>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B15074_04_02.jpg" alt="Figure 4.2 – A rendering of the CryptoTradingEnv environment showing the agent's current account balance and the buy/sell trade being executed "/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – A rendering of the CryptoTradingEnv environment showing the agent's current account balance and the buy/sell trade being executed</p>
			<p>Now, let's see how this all works.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor140"/>How it works…</h2>
			<p>In this recipe, we<a id="_idIndexMarker417"/> implemented<a id="_idIndexMarker418"/> the <strong class="source-inline">CryptoTradingEnv</strong> function, which offers tabular observations of shape (6, horizon + 1), where the horizon can be configured through the <strong class="source-inline">env_config</strong> dictionary. The horizon parameter specifies the horizon of the duration of the time window (for example, 3 days) that the Agent is allowed to observe the cryptocurrency market data at every step before making a trade. Once the Agent takes one of the allowed discrete actions – 0(hold), 1(buy), or 2(sell) – the appropriate trade is executed at the current exchange price of the cryptocurrency (Bitcoin) and the trading account balance is updated accordingly. The Agent will also receive a reward based on the profit (or loss) that's made through the trades from the start of the episode.</p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor141"/>Building an Ethereum trading RL platform using price charts </h1>
			<p>This recipe will <a id="_idIndexMarker419"/>teach you to <a id="_idIndexMarker420"/>implement an Ethereum cryptocurrency trading environment for RL Agents with visual observations. The Agent will observe a price chart with Open, High, Low, Close, and Volume information over a specified time period to take an action (Hold, Buy, or Sell). The objective of the Agent is to maximize its reward, which is the profit you would make if you deployed the Agent to trade in your account!</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor142"/>Getting ready</h2>
			<p>To complete this recipe, make sure you have the latest version. You will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that will update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>), which can be found in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without any issues, you are ready to get started:</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from typing import Dict</p>
			<p class="source-code">import cv2</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">from gym import spaces</p>
			<p class="source-code">from trading_utils import TradeVisualizer</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor143"/>How to do it…</h2>
			<p>Let's follow the OpenAI Gym framework in order to implement our learning environment interface. We will add some logic that will simulate cryptocurrency trade execution and reward the agent appropriately since this will aid your learning.</p>
			<p>Follow these steps to complete your implementation:</p>
			<ol>
				<li value="1">Let's begin <a id="_idIndexMarker421"/>by configuring<a id="_idIndexMarker422"/> the environment using a dictionary:<p class="source-code">env_config = {</p><p class="source-code">    "exchange": "Gemini",  # Cryptocurrency exchange </p><p class="source-code">    # (Gemini, coinbase, kraken, etc.)</p><p class="source-code">    "ticker": "ETHUSD",  # CryptoFiat</p><p class="source-code">    "frequency": "daily",  # daily/hourly/minutes</p><p class="source-code">    "opening_account_balance": 100000,</p><p class="source-code">    # Number of steps (days) of data provided to the </p><p class="source-code">    # agent in one observation</p><p class="source-code">    "observation_horizon_sequence_length": 30,</p><p class="source-code">    "order_size": 1,  # Number of coins to buy per </p><p class="source-code">    # buy/sell order</p><p class="source-code">}</p></li>
				<li>Let's define the <strong class="source-inline">CryptoTradingVisualEnv</strong> class and load the settings from <strong class="source-inline">env_config</strong>:<p class="source-code">class CryptoTradingVisualEnv(gym.Env):</p><p class="source-code">    def __init__(self, env_config: Dict = env_config):</p><p class="source-code">        """Cryptocurrency trading environment for RL </p><p class="source-code">        agents</p><p class="source-code">        The observations are cryptocurrency price info </p><p class="source-code">        (OHLCV) over a horizon as specified in </p><p class="source-code">        env_config. Action space is discrete to perform </p><p class="source-code">        buy/sell/hold trades.</p><p class="source-code">        Args:</p><p class="source-code">            ticker(str, optional): Ticker symbol for the\</p><p class="source-code">            crypto-fiat currency pair.</p><p class="source-code">            Defaults to "ETHUSD".</p><p class="source-code">            env_config (Dict): Env configuration values</p><p class="source-code">        """</p><p class="source-code">        super(CryptoTradingVisualEnv, self).__init__()</p><p class="source-code">        self.ticker = env_config.get("ticker", "ETHUSD")</p><p class="source-code">        data_dir = os.path.join(os.path.dirname(os.path.\</p><p class="source-code">                             realpath(__file__)), "data")</p><p class="source-code">        self.exchange = env_config["exchange"]</p><p class="source-code">        freq = env_config["frequency"]</p></li>
				<li>As the next <a id="_idIndexMarker423"/>step, based <a id="_idIndexMarker424"/>on the frequency configuration for the market data feed, let's load the cryptocurrency exchange data from the input stream:<p class="source-code">    if freq == "daily":</p><p class="source-code">            self.freq_suffix = "d"</p><p class="source-code">        elif freq == "hourly":</p><p class="source-code">            self.freq_suffix = "1hr"</p><p class="source-code">        elif freq == "minutes":</p><p class="source-code">            self.freq_suffix = "1min"</p><p class="source-code">        self.ticker_file_stream = os.path.join(</p><p class="source-code">            f"{data_dir}",</p><p class="source-code">            f"{'_'.join([self.exchange, self.ticker, \</p><p class="source-code">                         self.freq_suffix])}.csv",</p><p class="source-code">        )</p><p class="source-code">        assert os.path.isfile(</p><p class="source-code">            self.ticker_file_stream</p><p class="source-code">        ), f"Cryptocurrency exchange data file stream \</p><p class="source-code">        not found at: data/{self.ticker_file_stream}.csv"</p><p class="source-code">        # Cryptocurrency exchange data stream. An offline </p><p class="source-code">        # file stream is used. Alternatively, a web</p><p class="source-code">        # API can be used to pull live data.</p><p class="source-code">        self.ohlcv_df = pd.read_csv(self.ticker_file_\</p><p class="source-code">            stream, skiprows=1).sort_values(</p><p class="source-code">            by="Date"</p><p class="source-code">        )</p></li>
				<li>Let's <a id="_idIndexMarker425"/>initialize other <a id="_idIndexMarker426"/>environment class variables and define the state and action space:<p class="source-code">        self.opening_account_balance = \</p><p class="source-code">            env_config["opening_account_balance"]</p><p class="source-code">        # Action: 0-&gt; Hold; 1-&gt; Buy; 2 -&gt;Sell;</p><p class="source-code">        self.action_space = spaces.Discrete(3)</p><p class="source-code">        self.observation_features = [</p><p class="source-code">            "Open",</p><p class="source-code">            "High",</p><p class="source-code">            "Low",</p><p class="source-code">            "Close",</p><p class="source-code">            "Volume ETH",</p><p class="source-code">            "Volume USD",</p><p class="source-code">        ]</p><p class="source-code">        self.obs_width, self.obs_height = 128, 128</p><p class="source-code">        self.horizon = env_config.get("</p><p class="source-code">            observation_horizon_sequence_length")</p><p class="source-code">        self.observation_space = spaces.Box(</p><p class="source-code">            low=0, high=255, shape=(128, 128, 3),</p><p class="source-code">            dtype=np.uint8,</p><p class="source-code">        )</p><p class="source-code">        self.order_size = env_config.get("order_size")</p><p class="source-code">        self.viz = None  # Visualizer</p></li>
				<li>Let's define<a id="_idIndexMarker427"/> the <strong class="source-inline">reset</strong> method<a id="_idIndexMarker428"/> in order to (re)initialize the environment class variables:<p class="source-code">    def reset(self):</p><p class="source-code">        # Reset the state of the environment to an </p><p class="source-code">        # initial state</p><p class="source-code">        self.cash_balance = self.opening_account_balance</p><p class="source-code">        self.account_value = self.opening_account_balance</p><p class="source-code">        self.num_coins_held = 0</p><p class="source-code">        self.cost_basis = 0</p><p class="source-code">        self.current_step = 0</p><p class="source-code">        self.trades = []</p><p class="source-code">        if self.viz is None:</p><p class="source-code">            self.viz = TradeVisualizer(</p><p class="source-code">                self.ticker,</p><p class="source-code">                self.ticker_file_stream,</p><p class="source-code">                "TFRL-Cookbook\</p><p class="source-code">                   Ch4-CryptoTradingVisualEnv",</p><p class="source-code">                skiprows=1,</p><p class="source-code">            )</p><p class="source-code">        return self.get_observation()</p></li>
				<li>The key<a id="_idIndexMarker429"/> feature of this <a id="_idIndexMarker430"/>environment is that the Agent's observations are images of the price chart, similar to the one you can see on a human trader's computer screen. This chart contains flashy plots with red and green bars and candles! Let's define the <strong class="source-inline">get_observation</strong> method in order to return an image of the charting screen:<p class="source-code">    def get_observation(self):</p><p class="source-code">        """Return a view of the Ticker price chart as </p><p class="source-code">           image observation</p><p class="source-code">        Returns:</p><p class="source-code">            img_observation(np.ndarray): Image of ticker</p><p class="source-code">            candle stick plot with volume bars as </p><p class="source-code">            observation</p><p class="source-code">        """</p><p class="source-code">        img_observation = \</p><p class="source-code">            self.viz.render_image_observation(</p><p class="source-code">            self.current_step, self.horizon</p><p class="source-code">        )</p><p class="source-code">        img_observation = cv2.resize(</p><p class="source-code">            img_observation, dsize=(128, 128), </p><p class="source-code">            interpolation=cv2.INTER_CUBIC</p><p class="source-code">        )</p><p class="source-code">        return img_observation</p></li>
				<li>Now, we'll implement<a id="_idIndexMarker431"/> the<a id="_idIndexMarker432"/> trade execution logic of the trading environment. The current price of the Ethereum cryptocurrency (in USD) must be extracted from the market data stream (a file, in this case):<p class="source-code">    def execute_trade_action(self, action):</p><p class="source-code">        if action == 0:  # Hold position</p><p class="source-code">            return</p><p class="source-code">        order_type = "buy" if action == 1 else "sell"</p><p class="source-code">        # Stochastically determine the current price</p><p class="source-code">        # based on Market Open &amp; Close</p><p class="source-code">        current_price = random.uniform(</p><p class="source-code">            self.ohlcv_df.loc[self.current_step, "Open"],</p><p class="source-code">            self.ohlcv_df.loc[self.current_step, </p><p class="source-code">                              "Close"],</p><p class="source-code">        )</p></li>
				<li>If the Agent decides to execute a buy order, we must calculate the number of Ethereum <a id="_idIndexMarker433"/>tokens/coins<a id="_idIndexMarker434"/> the Agent can buy in a single step and execute the "Buy" order at the simulated exchange:<p class="source-code">            # Buy Order             allowable_coins = \</p><p class="source-code">                int(self.cash_balance / current_price)</p><p class="source-code">            if allowable_coins &lt; self.order_size:</p><p class="source-code">                # Not enough cash to execute a buy order</p><p class="source-code">                return</p><p class="source-code">            # Simulate a BUY order and execute it at </p><p class="source-code">            # current_price</p><p class="source-code">            num_coins_bought = self.order_size</p><p class="source-code">            current_cost = self.cost_basis * \</p><p class="source-code">                           self.num_coins_held</p><p class="source-code">            additional_cost = num_coins_bought * \</p><p class="source-code">                              current_price</p><p class="source-code">            self.cash_balance -= additional_cost</p><p class="source-code">            self.cost_basis = \</p><p class="source-code">                (current_cost + additional_cost) / (</p><p class="source-code">                self.num_coins_held + num_coins_bought</p><p class="source-code">            )</p><p class="source-code">            self.num_coins_held += num_coins_bought</p><p class="source-code">            self.trades.append(</p><p class="source-code">                {</p><p class="source-code">                    "type": "buy",</p><p class="source-code">                    "step": self.current_step,</p><p class="source-code">                    "shares": num_coins_bought,</p><p class="source-code">                    "proceeds": additional_cost,</p><p class="source-code">                }</p><p class="source-code">            )</p></li>
				<li>Instead, if<a id="_idIndexMarker435"/> the Agent<a id="_idIndexMarker436"/> decides to sell, the following logic will execute the sell order:<p class="source-code">           # Simulate a SELL order and execute it at </p><p class="source-code">           # current_price</p><p class="source-code">            if self.num_coins_held &lt; self.order_size:</p><p class="source-code">                # Not enough coins to execute a sell</p><p class="source-code">                # order</p><p class="source-code">                return</p><p class="source-code">            num_coins_sold = self.order_size</p><p class="source-code">            self.cash_balance += num_coins_sold * \</p><p class="source-code">                                 current_price</p><p class="source-code">            self.num_coins_held -= num_coins_sold</p><p class="source-code">            sale_proceeds = num_coins_sold * \</p><p class="source-code">                            current_price</p><p class="source-code">            self.trades.append(</p><p class="source-code">                {</p><p class="source-code">                    "type": "sell",</p><p class="source-code">                    "step": self.current_step,</p><p class="source-code">                    "shares": num_coins_sold,</p><p class="source-code">                    "proceeds": sale_proceeds,</p><p class="source-code">                }</p><p class="source-code">            )</p></li>
				<li>Let's <a id="_idIndexMarker437"/>update the account <a id="_idIndexMarker438"/>balance to reflect the effect of the Buy/Sell trade:<p class="source-code">        if self.num_coins_held == 0:</p><p class="source-code">            self.cost_basis = 0</p><p class="source-code">        # Update account value</p><p class="source-code">        self.account_value = self.cash_balance + \</p><p class="source-code">                             self.num_coins_held * \</p><p class="source-code">                             current_price</p></li>
				<li>We are now ready to implement the <strong class="source-inline">step</strong> method:<p class="source-code">    def step(self, action):</p><p class="source-code">        # Execute one step within the trading environment</p><p class="source-code">        self.execute_trade_action(action)</p><p class="source-code">        self.current_step += 1</p><p class="source-code">        reward = self.account_value - \</p><p class="source-code">            self.opening_account_balance  # Profit (loss)</p><p class="source-code">        done = self.account_value &lt;= 0 or \</p><p class="source-code">                 self.current_step &gt;= len(</p><p class="source-code">            self.ohlcv_df.loc[:, "Open"].values</p><p class="source-code">        )</p><p class="source-code">        obs = self.get_observation()</p><p class="source-code">        return obs, reward, done, {}</p></li>
				<li>Let's <a id="_idIndexMarker439"/>implement a method <a id="_idIndexMarker440"/>that will render the current state as an image to the screen. This will help us understand what's going on in the environment while the Agent is learning to trade:<p class="source-code">    def render(self, **kwargs):</p><p class="source-code">        # Render the environment to the screen</p><p class="source-code">        if self.current_step &gt; self.horizon:</p><p class="source-code">            self.viz.render(</p><p class="source-code">                self.current_step,</p><p class="source-code">                self.account_value,</p><p class="source-code">                self.trades,</p><p class="source-code">                window_size=self.horizon,</p><p class="source-code">            )</p></li>
				<li>That<a id="_idIndexMarker441"/> completes our<a id="_idIndexMarker442"/> implementation! Let's quickly check out the environment by using a random agent:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = CryptoTradingVisualEnv()</p><p class="source-code">    obs = env.reset()</p><p class="source-code">    for _ in range(600):</p><p class="source-code">        action = env.action_space.sample()</p><p class="source-code">        next_obs, reward, done, _ = env.step(action)</p><p class="source-code">        env.render()</p><p>You should see the sample random agent acting in <strong class="source-inline">CryptoTradinVisualEnv</strong>, wherein the agent receives visual/image observations similar to the one shown here:</p></li>
			</ol>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B15074_04_03.jpg" alt="Figure 4.3 – Sample observation sent to the learning Agent "/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Sample observation sent to the learning Agent</p>
			<p>That's it for this recipe!</p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor144"/>How it works…</h2>
			<p>In this <a id="_idIndexMarker443"/>recipe, we implemented a visual<a id="_idIndexMarker444"/> Ethereum cryptocurrency trading environment that provides images as input to the agents. The images contain charting information, such as Open, High, Low, Close, and Volume data. This chart looks like what a human trader's screen will look like and informs the agent about the current market signals.</p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor145"/>Building an advanced cryptocurrency trading platform for RL agents</h1>
			<p>Instead of<a id="_idIndexMarker445"/> allowing the Agent<a id="_idIndexMarker446"/> to only take discrete actions, such as buying/selling/holding a pre-set amount of Bitcoin or Ethereum tokens, what if we allowed the Agent to decide how many crypto coins/tokens it would like to buy or sell? That is exactly what this recipe will allow you to create in the form of a <strong class="source-inline">CryptoTradingVisualContinuousEnv</strong> RL environment.</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor146"/>Getting ready</h2>
			<p>To complete this recipe, you need to ensure you have the latest version. You will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that you update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>), which can be found in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without any issues, you are ready to get started:</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from typing import Dict</p>
			<p class="source-code">import cv2</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">from gym import spaces</p>
			<p class="source-code">from trading_utils import TradeVisualizer</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor147"/>How to do it…</h2>
			<p>This is going to be a complex environment as it uses high-dimensional images as observations and allows for continuous, real-value actions to be performed. However, you are likely familiar with the components of this recipe due to having experience implementing the previous recipes in this chapter.</p>
			<p>Let's get started:</p>
			<ol>
				<li value="1">First, we <a id="_idIndexMarker447"/>must define the<a id="_idIndexMarker448"/> configuration parameters that are allowed for this environment:<p class="source-code">env_config = {</p><p class="source-code">    "exchange": "Gemini",  # Cryptocurrency exchange </p><p class="source-code">     # (Gemini, coinbase, kraken, etc.)</p><p class="source-code">    "ticker": "BTCUSD",  # CryptoFiat</p><p class="source-code">    "frequency": "daily",  # daily/hourly/minutes</p><p class="source-code">    "opening_account_balance": 100000,</p><p class="source-code">    # Number of steps (days) of data provided to the </p><p class="source-code">    # agent in one observation</p><p class="source-code">    "observation_horizon_sequence_length": 30,</p><p class="source-code">}</p></li>
				<li>Let's jump right into the definition of the learning environment class:<p class="source-code">class CryptoTradingVisualContinuousEnv(gym.Env):</p><p class="source-code">    def __init__(self, env_config: Dict = env_config):</p><p class="source-code">        """Cryptocurrency trading environment for RL </p><p class="source-code">        agents with continuous action space</p><p class="source-code">        Args:</p><p class="source-code">            ticker (str, optional): Ticker symbol for the </p><p class="source-code">            crypto-fiat currency pair.</p><p class="source-code">            Defaults to "BTCUSD".</p><p class="source-code">            env_config (Dict): Env configuration values</p><p class="source-code">        """</p><p class="source-code">        super(CryptoTradingVisualContinuousEnv, </p><p class="source-code">              self).__init__()</p><p class="source-code">        self.ticker = env_config.get("ticker", "BTCUSD")</p><p class="source-code">        data_dir = os.path.join(os.path.dirname(os.path.\</p><p class="source-code">                             realpath(__file__)), "data")</p><p class="source-code">        self.exchange = env_config["exchange"]</p><p class="source-code">        freq = env_config["frequency"]</p><p class="source-code">        if freq == "daily":</p><p class="source-code">            self.freq_suffix = "d"</p><p class="source-code">        elif freq == "hourly":</p><p class="source-code">            self.freq_suffix = "1hr"</p><p class="source-code">        elif freq == "minutes":</p><p class="source-code">            self.freq_suffix = "1min"</p></li>
				<li>This step is <a id="_idIndexMarker449"/>straightforward<a id="_idIndexMarker450"/> as we simply load the market data into memory from the input source:<p class="source-code">        self.ticker_file_stream = os.path.join(</p><p class="source-code">            f"{data_dir}",</p><p class="source-code">            f"{'_'.join([self.exchange, self.ticker, \</p><p class="source-code">                         self.freq_suffix])}.csv",</p><p class="source-code">        )</p><p class="source-code">        assert os.path.isfile(</p><p class="source-code">            self.ticker_file_stream</p><p class="source-code">        ), f"Cryptocurrency exchange data file stream \</p><p class="source-code">        not found at: data/{self.ticker_file_stream}.csv"</p><p class="source-code">        # Cryptocurrency exchange data stream. An offline </p><p class="source-code">        # file stream is used. Alternatively, a web</p><p class="source-code">        # API can be used to pull live data.</p><p class="source-code">        self.ohlcv_df = pd.read_csv(</p><p class="source-code">            self.ticker_file_stream, </p><p class="source-code">            skiprows=1).sort_values(by="Date"</p><p class="source-code">        )</p><p class="source-code">        self.opening_account_balance = \</p><p class="source-code">            env_config["opening_account_balance"]</p></li>
				<li>Now, let's<a id="_idIndexMarker451"/> define the continuous<a id="_idIndexMarker452"/> action space and the observation space of the environment:<p class="source-code">        self.action_space = spaces.Box(</p><p class="source-code">            low=np.array([-1]), high=np.array([1]), \</p><p class="source-code">                         dtype=np.float</p><p class="source-code">        )</p><p class="source-code">        self.observation_features = [</p><p class="source-code">            "Open",</p><p class="source-code">            "High",</p><p class="source-code">            "Low",</p><p class="source-code">            "Close",</p><p class="source-code">            "Volume BTC",</p><p class="source-code">            "Volume USD",</p><p class="source-code">        ]</p><p class="source-code">        self.obs_width, self.obs_height = 128, 128</p><p class="source-code">        self.horizon = env_config.get(</p><p class="source-code">                   "observation_horizon_sequence_length")</p><p class="source-code">        self.observation_space = spaces.Box(</p><p class="source-code">            low=0, high=255, shape=(128, 128, 3), </p><p class="source-code">            dtype=np.uint8,</p><p class="source-code">        )</p></li>
				<li>Let's define<a id="_idIndexMarker453"/> the outline of<a id="_idIndexMarker454"/> the <strong class="source-inline">step</strong> method for the environment. We'll complete the helper method implementations in the following steps:<p class="source-code">    def step(self, action):</p><p class="source-code">        # Execute one step within the environment</p><p class="source-code">        self.execute_trade_action(action)</p><p class="source-code">        self.current_step += 1</p><p class="source-code">        reward = self.account_value - \</p><p class="source-code">            self.opening_account_balance  # Profit (loss)</p><p class="source-code">        done = self.account_value &lt;= 0 or \</p><p class="source-code">                self.current_step &gt;= len(</p><p class="source-code">            self.ohlcv_df.loc[:, "Open"].values</p><p class="source-code">        )</p><p class="source-code">        obs = self.get_observation()</p><p class="source-code">        return obs, reward, done, {}</p></li>
				<li>The first <a id="_idIndexMarker455"/>helper method is <a id="_idIndexMarker456"/>the <strong class="source-inline">execute_trade_action</strong> method. The implementation in the next few steps should be straightforward, given that the previous recipes also implemented the logic behind buying and selling cryptocurrency at an exchange rate:<p class="source-code">    def execute_trade_action(self, action):</p><p class="source-code">        if action == 0:  # Indicates "HODL" action</p><p class="source-code">            # HODL position; No trade to be executed</p><p class="source-code">            return</p><p class="source-code">        order_type = "buy" if action &gt; 0 else "sell"</p><p class="source-code">        order_fraction_of_allowable_coins = abs(action)</p><p class="source-code">        # Stochastically determine the current price </p><p class="source-code">        # based on Market Open &amp; Close</p><p class="source-code">        current_price = random.uniform(</p><p class="source-code">            self.ohlcv_df.loc[self.current_step, "Open"],</p><p class="source-code">            self.ohlcv_df.loc[self.current_step,</p><p class="source-code">                              "Close"],</p><p class="source-code">        )</p></li>
				<li>A Buy order <a id="_idIndexMarker457"/>at the exchange<a id="_idIndexMarker458"/> can be simulated as follows:<p class="source-code">        if order_type == "buy":</p><p class="source-code">            allowable_coins = \</p><p class="source-code">                int(self.cash_balance / current_price)</p><p class="source-code">            # Simulate a BUY order and execute it at </p><p class="source-code">            # current_price</p><p class="source-code">            num_coins_bought = int(allowable_coins * \</p><p class="source-code">            order_fraction_of_allowable_coins)</p><p class="source-code">            current_cost = self.cost_basis * \</p><p class="source-code">                           self.num_coins_held</p><p class="source-code">            additional_cost = num_coins_bought * \</p><p class="source-code">                              current_price</p><p class="source-code">            self.cash_balance -= additional_cost</p><p class="source-code">            self.cost_basis = (current_cost + \</p><p class="source-code">                               additional_cost) / (</p><p class="source-code">                self.num_coins_held + num_coins_bought</p><p class="source-code">            )</p><p class="source-code">            self.num_coins_held += num_coins_bought</p><p class="source-code">            if num_coins_bought &gt; 0:</p><p class="source-code">                self.trades.append(</p><p class="source-code">                    {</p><p class="source-code">                        "type": "buy",</p><p class="source-code">                        "step": self.current_step,</p><p class="source-code">                        "shares": num_coins_bought,</p><p class="source-code">                        "proceeds": additional_cost,</p><p class="source-code">                    }</p><p class="source-code">                )</p></li>
				<li>Similarly, a <a id="_idIndexMarker459"/>Sell order can<a id="_idIndexMarker460"/> be simulated in the following manner:<p class="source-code">        elif order_type == "sell":</p><p class="source-code">            # Simulate a SELL order and execute it at </p><p class="source-code">            # current_price</p><p class="source-code">            num_coins_sold = int(</p><p class="source-code">                self.num_coins_held * \</p><p class="source-code">                order_fraction_of_allowable_coins</p><p class="source-code">            )</p><p class="source-code">            self.cash_balance += num_coins_sold * \</p><p class="source-code">                                 current_price</p><p class="source-code">            self.num_coins_held -= num_coins_sold</p><p class="source-code">            sale_proceeds = num_coins_sold * \</p><p class="source-code">                            current_price</p><p class="source-code">            if num_coins_sold &gt; 0:</p><p class="source-code">                self.trades.append(</p><p class="source-code">                    {</p><p class="source-code">                        "type": "sell",</p><p class="source-code">                        "step": self.current_step,</p><p class="source-code">                        "shares": num_coins_sold,</p><p class="source-code">                        "proceeds": sale_proceeds,</p><p class="source-code">                    }</p><p class="source-code">                )</p></li>
				<li>Once the <a id="_idIndexMarker461"/>Buy/Sell<a id="_idIndexMarker462"/> order has been executed, the account balance needs to be updated:<p class="source-code">        if self.num_coins_held == 0:</p><p class="source-code">            self.cost_basis = 0</p><p class="source-code">        # Update account value</p><p class="source-code">        self.account_value = self.cash_balance + \</p><p class="source-code">                             self.num_coins_held * \</p><p class="source-code">                             current_price</p></li>
				<li>To test <strong class="source-inline">CryptoTradingVisualcontinuousEnv</strong>, you can use the following lines of code for the <strong class="source-inline">__main__</strong> function: <p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = CryptoTradingVisualContinuousEnv()</p><p class="source-code">    obs = env.reset()</p><p class="source-code">    for _ in range(600):</p><p class="source-code">        action = env.action_space.sample()</p><p class="source-code">        next_obs, reward, done, _ = env.step(action)</p><p class="source-code">        env.render()</p></li>
			</ol>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor148"/>How it works…</h2>
			<p><strong class="source-inline">CryptoTradingVisualcontinuousEnv</strong> provides an RL environment with a trader screen-like image as the observation and provides a continuous, real-valued action space for the Agents to act in. The actions in this environment are one-dimensional, continuous, and <a id="_idIndexMarker463"/>real-valued and the<a id="_idIndexMarker464"/> magnitude indicates the fraction amount of the crypto coins/tokens. If the action has a positive sign (0 to 1), it's interpreted as a Buy order, while if the action has a negative sign (-1 to 0), it's interpreted as a Sell order. The fraction amount is converted into a number of allowable coins that can be bought or sold based on the balance in the trading account.</p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor149"/>Training a cryptocurrency trading bot using RL</h1>
			<p>The soft actor-critic <a id="_idIndexMarker465"/>Agent is one of the most<a id="_idIndexMarker466"/> popular and state-of-the-art RL Agents available and is based on an off-policy, maximum entropy-based deep RL algorithm. This recipe provides all the ingredients you will need to build a soft actor-critic Agent from scratch using TensorFlow 2.x and train it for cryptocurrency (Bitcoin, Ethereum, and so on) trading using real data from the Gemini cryptocurrency exchange. </p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor150"/>Getting ready</h2>
			<p>To complete this recipe, make sure you have the latest version. You will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment. Make sure that you update the environment so that it matches the latest conda environment specification file (<strong class="source-inline">tfrl-cookbook.yml</strong>), which can be found in this cookbook's code repository. If the following <strong class="source-inline">import</strong> statements run without any issues, you are ready to get started:</p>
			<p class="source-code">mport functools</p>
			<p class="source-code">import os</p>
			<p class="source-code">import random</p>
			<p class="source-code">from collections import deque</p>
			<p class="source-code">from functools import reduce</p>
			<p class="source-code">import imageio</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">import tensorflow_probability as tfp</p>
			<p class="source-code">from tensorflow.keras.layers import Concatenate, Dense, Input</p>
			<p class="source-code">from tensorflow.keras.models import Model</p>
			<p class="source-code">from tensorflow.keras.optimizers import Adam</p>
			<p class="source-code">from crypto_trading_continuous_env import CryptoTradingContinuousEnv</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor151"/>How to do it…</h2>
			<p>This recipe will<a id="_idIndexMarker467"/> guide you through the step-by-step<a id="_idIndexMarker468"/> process of implementing the SAC Agent. It will also help you train the agent in the cryptocurrency trading environments so that you can automate your profit-making machine!</p>
			<p>Let's gear up and begin the implementation:</p>
			<ol>
				<li value="1">SAC is an actor-critic Agent, so it has both the actor and the critic components. Let's begin by defining our actor neural network using TensorFlow 2.x:<p class="source-code">def actor(state_shape, action_shape, units=(512, 256, 64)):</p><p class="source-code">    state_shape_flattened = \</p><p class="source-code">        functools.reduce(lambda x, y: x * y, state_shape)</p><p class="source-code">    state = Input(shape=state_shape_flattened)</p><p class="source-code">    x = Dense(units[0], name="L0", activation="relu")\</p><p class="source-code">              (state)</p><p class="source-code">    for index in range(1, len(units)):</p><p class="source-code">        x = Dense(units[index],name="L{}".format(index),\ </p><p class="source-code">                  activation="relu")(x)</p><p class="source-code">    actions_mean = Dense(action_shape[0], \</p><p class="source-code">                    name="Out_mean")(x)</p><p class="source-code">    actions_std = Dense(action_shape[0], \</p><p class="source-code">                   name="Out_std")(x)</p><p class="source-code">    model = Model(inputs=state, </p><p class="source-code">                  outputs=[actions_mean, actions_std])</p><p class="source-code">    return model</p></li>
				<li>Next, let's <a id="_idIndexMarker469"/>define the critic neural <a id="_idIndexMarker470"/>network:<p class="source-code">def critic(state_shape, action_shape, units=(512, 256, 64)):</p><p class="source-code">    state_shape_flattened = \</p><p class="source-code">        functools.reduce(lambda x, y: x * y, state_shape)</p><p class="source-code">    inputs = [Input(shape=state_shape_flattened),</p><p class="source-code">              Input(shape=action_shape)]</p><p class="source-code">    concat = Concatenate(axis=-1)(inputs)</p><p class="source-code">    x = Dense(units[0], name="Hidden0", </p><p class="source-code">              activation="relu")(concat)</p><p class="source-code">    for index in range(1, len(units)):</p><p class="source-code">        x = Dense(units[index], </p><p class="source-code">                  name="Hidden{}".format(index),</p><p class="source-code">                  activation="relu")(x)</p><p class="source-code">    output = Dense(1, name="Out_QVal")(x)</p><p class="source-code">    model = Model(inputs=inputs, outputs=output)</p><p class="source-code">    return model</p></li>
				<li>Given the <a id="_idIndexMarker471"/>current model weights and<a id="_idIndexMarker472"/> the target model weights, let's implement a quick function that will slowly update the target weights using <strong class="source-inline">tau</strong> as the averaging factor. This is like the Polyak averaging step:<p class="source-code">def update_target_weights(model, target_model, tau=0.005):</p><p class="source-code">    weights = model.get_weights()</p><p class="source-code">    target_weights = target_model.get_weights()</p><p class="source-code">    for i in range(len(target_weights)):  # set tau% of</p><p class="source-code">    # target model to be new weights</p><p class="source-code">        target_weights[i] = weights[i] * tau + \</p><p class="source-code">                            target_weights[i] * (1 - tau)</p><p class="source-code">    target_model.set_weights(target_weights)</p></li>
				<li>We are now ready to initialize our SAC Agent class:<p class="source-code">class SAC(object):</p><p class="source-code">    def __init__(</p><p class="source-code">        self,</p><p class="source-code">        env,</p><p class="source-code">        lr_actor=3e-5,</p><p class="source-code">        lr_critic=3e-4,</p><p class="source-code">        actor_units=(64, 64),</p><p class="source-code">        critic_units=(64, 64),</p><p class="source-code">        auto_alpha=True,</p><p class="source-code">        alpha=0.2,</p><p class="source-code">        tau=0.005,</p><p class="source-code">        gamma=0.99,</p><p class="source-code">        batch_size=128,</p><p class="source-code">        memory_cap=100000,</p><p class="source-code">    ):</p><p class="source-code">        self.env = env</p><p class="source-code">        self.state_shape = env.observation_space.shape  </p><p class="source-code">        # shape of observations</p><p class="source-code">        self.action_shape = env.action_space.shape  </p><p class="source-code">        # number of actions</p><p class="source-code">        self.action_bound = (env.action_space.high - \</p><p class="source-code">                             env.action_space.low) / 2</p><p class="source-code">        self.action_shift = (env.action_space.high + \</p><p class="source-code">                             env.action_space.low) / 2</p><p class="source-code">        self.memory = deque(maxlen=int(memory_cap))</p></li>
				<li>As the next step, we'll initialize the actor network and print a summary of the actor neural network:<p class="source-code">        # Define and initialize actor network</p><p class="source-code">        self.actor = actor(self.state_shape, </p><p class="source-code">                          self.action_shape, actor_units)</p><p class="source-code">        self.actor_optimizer = \</p><p class="source-code">            Adam(learning_rate=lr_actor)</p><p class="source-code">        self.log_std_min = -20</p><p class="source-code">        self.log_std_max = 2</p><p class="source-code">        print(self.actor.summary())</p></li>
				<li>Next, we'll <a id="_idIndexMarker473"/>define the two critic <a id="_idIndexMarker474"/>networks and print the summary of the critic neural network as well:<p class="source-code">        self.critic_1 = critic(self.state_shape, </p><p class="source-code">                         self.action_shape, critic_units)</p><p class="source-code">        self.critic_target_1 = critic(self.state_shape,</p><p class="source-code">                         self.action_shape, critic_units)</p><p class="source-code">        self.critic_optimizer_1 = \</p><p class="source-code">             Adam(learning_rate=lr_critic)</p><p class="source-code">        update_target_weights(self.critic_1, \</p><p class="source-code">                         self.critic_target_1, tau=1.0)</p><p class="source-code">        self.critic_2 = critic(self.state_shape, \</p><p class="source-code">                         self.action_shape, critic_units)</p><p class="source-code">        self.critic_target_2 = critic(self.state_shape,\</p><p class="source-code">                         self.action_shape, critic_units)</p><p class="source-code">        self.critic_optimizer_2 = \</p><p class="source-code">            Adam(learning_rate=lr_critic)</p><p class="source-code">        update_target_weights(self.critic_2, \</p><p class="source-code">            self.critic_target_2, tau=1.0)</p><p class="source-code">        print(self.critic_1.summary())</p></li>
				<li>Let's <a id="_idIndexMarker475"/>initialize the <strong class="source-inline">alpha</strong> temperature <a id="_idIndexMarker476"/>parameter and the target entropy:<p class="source-code">        self.auto_alpha = auto_alpha</p><p class="source-code">        if auto_alpha:</p><p class="source-code">            self.target_entropy = \</p><p class="source-code">                -np.prod(self.action_shape)</p><p class="source-code">            self.log_alpha = \</p><p class="source-code">                tf.Variable(0.0, dtype=tf.float64)</p><p class="source-code">            self.alpha = \</p><p class="source-code">                tf.Variable(0.0, dtype=tf.float64)</p><p class="source-code">            self.alpha.assign(tf.exp(self.log_alpha))</p><p class="source-code">            self.alpha_optimizer = \</p><p class="source-code">                Adam(learning_rate=lr_actor)</p><p class="source-code">        else:</p><p class="source-code">            self.alpha = tf.Variable(alpha, </p><p class="source-code">                                     dtype=tf.float64)</p></li>
				<li>We'll also initialize the other hyperparameters of SAC:<p class="source-code">        self.gamma = gamma  # discount factor</p><p class="source-code">        self.tau = tau  # target model update</p><p class="source-code">        self.batch_size = batch_size</p></li>
				<li>That completes the <strong class="source-inline">__init__</strong> method of the SAC agent. Next, we'll implement a method <a id="_idIndexMarker477"/>that will (pre)process<a id="_idIndexMarker478"/> the action that's taken:<p class="source-code">    def process_actions(self, mean, log_std, test=False, </p><p class="source-code">    eps=1e-6):</p><p class="source-code">        std = tf.math.exp(log_std)</p><p class="source-code">        raw_actions = mean</p><p class="source-code">        if not test:</p><p class="source-code">            raw_actions += tf.random.normal(shape=mean.\</p><p class="source-code">                           shape, dtype=tf.float64) * std</p><p class="source-code">        log_prob_u = tfp.distributions.Normal(loc=mean,</p><p class="source-code">                        scale=std).log_prob(raw_actions)</p><p class="source-code">        actions = tf.math.tanh(raw_actions)</p><p class="source-code">        log_prob = tf.reduce_sum(log_prob_u - \</p><p class="source-code">                    tf.math.log(1 - actions ** 2 + eps))</p><p class="source-code">        actions = actions * self.action_bound + \</p><p class="source-code">                   self.action_shift</p><p class="source-code">        return actions, log_prob</p></li>
				<li>We are now ready to implement the <strong class="source-inline">act</strong> method in order to generate the SAC agent's <a id="_idIndexMarker479"/>action, given <a id="_idIndexMarker480"/>a state:<p class="source-code">    def act(self, state, test=False, use_random=False):</p><p class="source-code">        state = state.reshape(-1)  # Flatten state</p><p class="source-code">        state = \</p><p class="source-code">         np.expand_dims(state, axis=0).astype(np.float64)</p><p class="source-code">        if use_random:</p><p class="source-code">            a = tf.random.uniform(</p><p class="source-code">                shape=(1, self.action_shape[0]), \</p><p class="source-code">                minval=-1, maxval=1, dtype=tf.float64</p><p class="source-code">            )</p><p class="source-code">        else:</p><p class="source-code">            means, log_stds = self.actor.predict(state)</p><p class="source-code">            log_stds = tf.clip_by_value(log_stds, </p><p class="source-code">                                        self.log_std_min,</p><p class="source-code">                                        self.log_std_max)</p><p class="source-code">            a, log_prob = self.process_actions(means,</p><p class="source-code">                                               log_stds, </p><p class="source-code">                                               test=test)</p><p class="source-code">        q1 = self.critic_1.predict([state, a])[0][0]</p><p class="source-code">        q2 = self.critic_2.predict([state, a])[0][0]</p><p class="source-code">        self.summaries["q_min"] = tf.math.minimum(q1, q2)</p><p class="source-code">        self.summaries["q_mean"] = np.mean([q1, q2])</p><p class="source-code">        return a</p></li>
				<li>In order to save experiences to the Replay memory, let's implement the <strong class="source-inline">remember</strong> function:<p class="source-code">    def remember(self, state, action, reward, next_state, </p><p class="source-code">    done):</p><p class="source-code">        state = state.reshape(-1)  # Flatten state</p><p class="source-code">        state = np.expand_dims(state, axis=0)</p><p class="source-code">        next_state = next_state.reshape(-1)  </p><p class="source-code">       # Flatten next-state</p><p class="source-code">        next_state = np.expand_dims(next_state, axis=0)</p><p class="source-code">        self.memory.append([state, action, reward, </p><p class="source-code">                            next_state, done])</p></li>
				<li>Now, let's begin<a id="_idIndexMarker481"/> implementing the<a id="_idIndexMarker482"/> experience replay process. We'll start by initializing the replay method. We'll complete the implementation of the replay method in the upcoming steps:<p class="source-code">    def replay(self):</p><p class="source-code">        if len(self.memory) &lt; self.batch_size:</p><p class="source-code">            return</p><p class="source-code">        samples = random.sample(self.memory, self.batch_size)</p><p class="source-code">        s = np.array(samples).T</p><p class="source-code">        states, actions, rewards, next_states, dones = [</p><p class="source-code">            np.vstack(s[i, :]).astype(np.float) for i in\</p><p class="source-code">            range(5)</p><p class="source-code">        ]</p></li>
				<li>Let's start a persistent <strong class="source-inline">GradientTape</strong> function and begin accumulating gradients. We'll do this by processing the actions and obtaining the next set of actions <a id="_idIndexMarker483"/>and<a id="_idIndexMarker484"/> log probabilities:<p class="source-code">        with tf.GradientTape(persistent=True) as tape:</p><p class="source-code">            # next state action log probs</p><p class="source-code">            means, log_stds = self.actor(next_states)</p><p class="source-code">            log_stds = tf.clip_by_value(log_stds, </p><p class="source-code">                                        self.log_std_min,</p><p class="source-code">                                        self.log_std_max)</p><p class="source-code">            next_actions, log_probs = \</p><p class="source-code">                self.process_actions(means, log_stds)</p></li>
				<li>With that, we can now compute the losses of the two critic networks:<p class="source-code">            current_q_1 = self.critic_1([states, </p><p class="source-code">                                         actions])</p><p class="source-code">            current_q_2 = self.critic_2([states,</p><p class="source-code">                                         actions])</p><p class="source-code">            next_q_1 = self.critic_target_1([next_states, </p><p class="source-code">                                           next_actions])</p><p class="source-code">            next_q_2 = self.critic_target_2([next_states,</p><p class="source-code">                                           next_actions])</p><p class="source-code">            next_q_min = tf.math.minimum(next_q_1,</p><p class="source-code">                                          next_q_2)</p><p class="source-code">            state_values = next_q_min - self.alpha * \</p><p class="source-code">                                        log_probs</p><p class="source-code">            target_qs = tf.stop_gradient(</p><p class="source-code">                rewards + state_values * self.gamma * \</p><p class="source-code">                (1.0 - dones)</p><p class="source-code">            )</p><p class="source-code">            critic_loss_1 = tf.reduce_mean(</p><p class="source-code">                0.5 * tf.math.square(current_q_1 - \</p><p class="source-code">                                     target_qs)</p><p class="source-code">            )</p><p class="source-code">            critic_loss_2 = tf.reduce_mean(</p><p class="source-code">                0.5 * tf.math.square(current_q_2 - \</p><p class="source-code">                                     target_qs)</p><p class="source-code">            )</p></li>
				<li>The current <a id="_idIndexMarker485"/>state-action and log<a id="_idIndexMarker486"/> probabilities, as prescribed by the actor, can be computed as follows:<p class="source-code">            means, log_stds = self.actor(states)</p><p class="source-code">            log_stds = tf.clip_by_value(log_stds, </p><p class="source-code">                                        self.log_std_min,</p><p class="source-code">                                        self.log_std_max)</p><p class="source-code">            actions, log_probs = \</p><p class="source-code">                self.process_actions(means, log_stds)</p></li>
				<li>We can now compute the actor loss and apply gradients to the critic:<p class="source-code">            current_q_1 = self.critic_1([states, </p><p class="source-code">                                         actions])</p><p class="source-code">            current_q_2 = self.critic_2([states,</p><p class="source-code">                                         actions])</p><p class="source-code">            current_q_min = tf.math.minimum(current_q_1,</p><p class="source-code">                                            current_q_2)</p><p class="source-code">            actor_loss = tf.reduce_mean(self.alpha * \</p><p class="source-code">                               log_probs - current_q_min)</p><p class="source-code">            if self.auto_alpha:</p><p class="source-code">                alpha_loss = -tf.reduce_mean(</p><p class="source-code">                    (self.log_alpha * \</p><p class="source-code">                    tf.stop_gradient(log_probs + \</p><p class="source-code">                                    self.target_entropy))</p><p class="source-code">                )</p><p class="source-code">        critic_grad = tape.gradient(</p><p class="source-code">            critic_loss_1, </p><p class="source-code">            self.critic_1.trainable_variables</p><p class="source-code">        )  </p><p class="source-code">        self.critic_optimizer_1.apply_gradients(</p><p class="source-code">            zip(critic_grad, </p><p class="source-code">            self.critic_1.trainable_variables)</p><p class="source-code">        )</p></li>
				<li>Similarly, we <a id="_idIndexMarker487"/>can compute and apply<a id="_idIndexMarker488"/> the actor's gradients:<p class="source-code">        critic_grad = tape.gradient(</p><p class="source-code">            critic_loss_2, </p><p class="source-code">        self.critic_2.trainable_variables</p><p class="source-code">        )  # compute actor gradient</p><p class="source-code">        self.critic_optimizer_2.apply_gradients(</p><p class="source-code">            zip(critic_grad, </p><p class="source-code">            self.critic_2.trainable_variables)</p><p class="source-code">        )</p><p class="source-code">        actor_grad = tape.gradient(</p><p class="source-code">            actor_loss, self.actor.trainable_variables</p><p class="source-code">        )  # compute actor gradient</p><p class="source-code">        self.actor_optimizer.apply_gradients(</p><p class="source-code">            zip(actor_grad, </p><p class="source-code">                self.actor.trainable_variables)</p><p class="source-code">        )</p></li>
				<li>Now, let's<a id="_idIndexMarker489"/> log the summaries to <a id="_idIndexMarker490"/>TensorBoard:<p class="source-code">        # tensorboard info</p><p class="source-code">        self.summaries["q1_loss"] = critic_loss_1</p><p class="source-code">        self.summaries["q2_loss"] = critic_loss_2</p><p class="source-code">        self.summaries["actor_loss"] = actor_loss</p><p class="source-code">        if self.auto_alpha:</p><p class="source-code">            # optimize temperature</p><p class="source-code">            alpha_grad = tape.gradient(alpha_loss, </p><p class="source-code">                                       [self.log_alpha])</p><p class="source-code">            self.alpha_optimizer.apply_gradients(</p><p class="source-code">                       zip(alpha_grad, [self.log_alpha]))</p><p class="source-code">            self.alpha.assign(tf.exp(self.log_alpha))</p><p class="source-code">            # tensorboard info</p><p class="source-code">            self.summaries["alpha_loss"] = alpha_loss</p></li>
				<li>That completes our experience replay method. Now, we can move on to the <strong class="source-inline">train</strong> method's implementation. Let's begin by initializing the <strong class="source-inline">train</strong> method. We will complete<a id="_idIndexMarker491"/> the implementation of<a id="_idIndexMarker492"/> this method in the following steps:<p class="source-code">    def train(self, max_epochs=8000, random_epochs=1000,</p><p class="source-code">    max_steps=1000, save_freq=50):</p><p class="source-code">        current_time = datetime.datetime.now().\</p><p class="source-code">                           strftime("%Y%m%d-%H%M%S")</p><p class="source-code">        train_log_dir = os.path.join("logs", </p><p class="source-code">                   "TFRL-Cookbook-Ch4-SAC", current_time)</p><p class="source-code">        summary_writer = \</p><p class="source-code">            tf.summary.create_file_writer(train_log_dir)</p><p class="source-code">        done, use_random, episode, steps, epoch, \</p><p class="source-code">        episode_reward = (</p><p class="source-code">            False,</p><p class="source-code">            True,</p><p class="source-code">            0,</p><p class="source-code">            0,</p><p class="source-code">            0,</p><p class="source-code">            0,</p><p class="source-code">        )</p><p class="source-code">        cur_state = self.env.reset()</p></li>
				<li>Now, we are <a id="_idIndexMarker493"/>ready to start the main training loop. First, let's handle the end of episode <a id="_idIndexMarker494"/>case:<p class="source-code">        while epoch &lt; max_epochs:</p><p class="source-code">            if steps &gt; max_steps:</p><p class="source-code">                done = True</p><p class="source-code">            if done:</p><p class="source-code">                episode += 1</p><p class="source-code">                print(</p><p class="source-code">                    "episode {}: {} total reward, </p><p class="source-code">                     {} alpha, {} steps, </p><p class="source-code">                     {} epochs".format(</p><p class="source-code">                        episode, episode_reward, </p><p class="source-code">                        self.alpha.numpy(), steps, epoch</p><p class="source-code">                    )</p><p class="source-code">                )</p><p class="source-code">                with summary_writer.as_default():</p><p class="source-code">                    tf.summary.scalar(</p><p class="source-code">                        "Main/episode_reward", \</p><p class="source-code">                         episode_reward, step=episode</p><p class="source-code">                    )</p><p class="source-code">                    tf.summary.scalar(</p><p class="source-code">                        "Main/episode_steps", </p><p class="source-code">                         steps, step=episode)</p><p class="source-code">                summary_writer.flush()</p><p class="source-code">                done, cur_state, steps, episode_reward =\</p><p class="source-code">                     False, self.env.reset(), 0, 0</p><p class="source-code">                if episode % save_freq == 0:</p><p class="source-code">                    self.save_model(</p><p class="source-code">                        "sac_actor_episode{}.h5".\</p><p class="source-code">                             format(episode),</p><p class="source-code">                        "sac_critic_episode{}.h5".\</p><p class="source-code">                             format(episode),</p><p class="source-code">                    )</p></li>
				<li>For every <a id="_idIndexMarker495"/>step into the environment, the<a id="_idIndexMarker496"/> following steps will need to be executed for the SAC agent to learn:<p class="source-code">            if epoch &gt; random_epochs and \</p><p class="source-code">                len(self.memory) &gt; self.batch_size:</p><p class="source-code">                use_random = False</p><p class="source-code">            action = self.act(cur_state, \</p><p class="source-code">               use_random=use_random)  # determine action</p><p class="source-code">            next_state, reward, done, _ = \</p><p class="source-code">                self.env.step(action[0])  # act on env</p><p class="source-code">            # self.env.render(mode='rgb_array')</p><p class="source-code">            self.remember(cur_state, action, reward, </p><p class="source-code">                        next_state, done)  #add to memory</p><p class="source-code">            self.replay()  # train models through memory</p><p class="source-code">            # replay</p><p class="source-code">            update_target_weights(</p><p class="source-code">                self.critic_1, self.critic_target_1, </p><p class="source-code">                tau=self.tau</p><p class="source-code">            )  # iterates target model</p><p class="source-code">            update_target_weights(self.critic_2, </p><p class="source-code">            self.critic_target_2, </p><p class="source-code">            tau=self.tau)</p><p class="source-code">            cur_state = next_state</p><p class="source-code">            episode_reward += reward</p><p class="source-code">            steps += 1</p><p class="source-code">            epoch += 1</p></li>
				<li>With the<a id="_idIndexMarker497"/> agent updates taken care of, we<a id="_idIndexMarker498"/> can now log some more useful information to TensorBoard:<p class="source-code">            # Tensorboard update</p><p class="source-code">            with summary_writer.as_default():</p><p class="source-code">                if len(self.memory) &gt; self.batch_size:</p><p class="source-code">                    tf.summary.scalar(</p><p class="source-code">                        "Loss/actor_loss", </p><p class="source-code">                         self.summaries["actor_loss"], </p><p class="source-code">                         step=epoch</p><p class="source-code">                    )</p><p class="source-code">                    tf.summary.scalar(</p><p class="source-code">                        "Loss/q1_loss", </p><p class="source-code">                         self.summaries["q1_loss"], </p><p class="source-code">                         step=epoch</p><p class="source-code">                    )</p><p class="source-code">                    tf.summary.scalar(</p><p class="source-code">                        "Loss/q2_loss", </p><p class="source-code">                       self.summaries["q2_loss"], </p><p class="source-code">                       step=epoch</p><p class="source-code">                    )</p><p class="source-code">                    if self.auto_alpha:</p><p class="source-code">                        tf.summary.scalar(</p><p class="source-code">                            "Loss/alpha_loss", </p><p class="source-code">                            self.summaries["alpha_loss"],</p><p class="source-code">                            step=epoch</p><p class="source-code">                        )</p><p class="source-code">                tf.summary.scalar("Stats/alpha", </p><p class="source-code">                                  self.alpha, step=epoch)</p><p class="source-code">                if self.auto_alpha:</p><p class="source-code">                    tf.summary.scalar("Stats/log_alpha",</p><p class="source-code">                              self.log_alpha, step=epoch)</p><p class="source-code">                tf.summary.scalar("Stats/q_min", </p><p class="source-code">                    self.summaries["q_min"], step=epoch)</p><p class="source-code">                tf.summary.scalar("Stats/q_mean", </p><p class="source-code">                    self.summaries["q_mean"], step=epoch)</p><p class="source-code">                tf.summary.scalar("Main/step_reward", </p><p class="source-code">                                   reward, step=epoch)</p><p class="source-code">            summary_writer.flush()</p></li>
				<li>As the last step in our train method implementation, we can save the actor and critic models to facilitate resuming our training or reloading from a checkpoint:<p class="source-code">        self.save_model(</p><p class="source-code">            "sac_actor_final_episode{}.h5".format(episode),</p><p class="source-code">            "sac_critic_final_episode{}.h5".format(episode),</p><p class="source-code">        )</p></li>
				<li>Now, we'll <a id="_idIndexMarker499"/>actually implement <a id="_idIndexMarker500"/>the <strong class="source-inline">save_model</strong> method we referenced previously:<p class="source-code">    def save_model(self, a_fn, c_fn):</p><p class="source-code">        self.actor.save(a_fn)</p><p class="source-code">        self.critic_1.save(c_fn)</p></li>
				<li>Let's quickly implement a method that will load the actor and critic states from the saved model so that we can restore/resume from a previously saved checkpoint when needed:<p class="source-code">    def load_actor(self, a_fn):</p><p class="source-code">        self.actor.load_weights(a_fn)</p><p class="source-code">        print(self.actor.summary())</p><p class="source-code">    def load_critic(self, c_fn):</p><p class="source-code">        self.critic_1.load_weights(c_fn)</p><p class="source-code">        self.critic_target_1.load_weights(c_fn)</p><p class="source-code">        self.critic_2.load_weights(c_fn)</p><p class="source-code">        self.critic_target_2.load_weights(c_fn)</p><p class="source-code">        print(self.critic_1.summary())</p></li>
				<li>To run <a id="_idIndexMarker501"/>the SAC agent in "test" mode, we<a id="_idIndexMarker502"/> can implement a helper method:<p class="source-code">    def test(self, render=True, fps=30, </p><p class="source-code">    filename="test_render.mp4"):</p><p class="source-code">        cur_state, done, rewards = self.env.reset(), \</p><p class="source-code">                                    False, 0</p><p class="source-code">        video = imageio.get_writer(filename, fps=fps)</p><p class="source-code">        while not done:</p><p class="source-code">            action = self.act(cur_state, test=True)</p><p class="source-code">            next_state, reward, done, _ = \</p><p class="source-code">                                 self.env.step(action[0])</p><p class="source-code">            cur_state = next_state</p><p class="source-code">            rewards += reward</p><p class="source-code">            if render:</p><p class="source-code">                video.append_data(</p><p class="source-code">                    self.env.render(mode="rgb_array"))</p><p class="source-code">        video.close()</p><p class="source-code">        return rewards</p></li>
				<li>That completes our SAC agent implementation. We are now ready to train the SAC agent in <strong class="source-inline">CryptoTradingContinuousEnv</strong>:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    gym_env = CryptoTradingContinuousEnv()</p><p class="source-code">    sac = SAC(gym_env)</p><p class="source-code">    # Load Actor and Critic from previously saved </p><p class="source-code">    # checkpoints</p><p class="source-code">    # sac.load_actor("sac_actor_episodexyz.h5")</p><p class="source-code">    # sac.load_critic("sac_critic_episodexyz.h5")</p><p class="source-code">    sac.train(max_epochs=100000, random_epochs=10000, </p><p class="source-code">              save_freq=50)</p><p class="source-code">    reward = sac.test()</p><p class="source-code">    print(reward)</p></li>
			</ol>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor152"/>How it works…</h2>
			<p>SAC is a powerful <a id="_idIndexMarker503"/>RL algorithm and has proven to <a id="_idIndexMarker504"/>be effective across a variety of RL simulation environments. SAC maximizes the entropy of the agent's policy, in addition to optimizing for the maximum episodic rewards. You can watch the progress of the agent as it learns to trade using the TensorBoard since this recipe includes code for logging the agent's progress along the way. You can launch TensorBoard using the following command:</p>
			<p class="source-code">tensorboard –-logdir=logs</p>
			<p>The preceding command will launch TensorBoard. You can access it with your browser at the default address of <strong class="source-inline">http://localhost:6006</strong>. A sample TensorBoard screenshot has been provided here for reference:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B15074_04_04.jpg" alt="Figure 4.4 – A screenshot of TensorBoard showing the SAC agent's training progress in CryptoTradingContinuousEnv "/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – A screenshot of TensorBoard showing the SAC agent's training progress in CryptoTradingContinuousEnv</p>
			<p>That <a id="_idIndexMarker505"/>concludes this recipe and this chapter. Happy <a id="_idIndexMarker506"/>training!</p>
		</div>
</body></html>