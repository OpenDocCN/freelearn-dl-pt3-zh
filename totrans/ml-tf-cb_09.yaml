- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) are the primary modern approach for
    modeling data that is sequential in nature. The word "recurrent" in the name of
    the architecture class refers to the fact that the output of the current step
    becomes the input to the next one (and potentially further ones as well). At each
    element in the sequence, the model considers both the current input and what it
    "remembers" about the preceding elements.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**) tasks are one of the primary areas
    of application for RNNs: if you are reading through this very sentence, you are
    picking up the context of each word from the words that came before it. NLP models
    based on RNNs can build on this approach to achieve generative tasks, such as
    novel text creation, as well as predictive ones such as sentiment classification
    or machine translation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time series – stock price prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Open-domain question answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first topic we''ll tackle is text generation: it demonstrates quite easily
    how we can use an RNN to generate novel content, and can therefore serve as a
    gentle introduction to RNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the best-known applications used to demonstrate the strength of RNNs
    is generating novel text (we will return to this application later, in the chapter
    on Transformer architectures).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will use a **Long Short-Term Memory** (**LSTM**) architecture—a
    popular variant of RNNs—to build a text generation model. The name LSTM comes
    from the motivation for their development: "vanilla" RNNs struggled with long
    dependencies (known as the vanishing gradient problem) and the architectural solution
    of LSTM solved that. LSTM models achieve that by maintaining a cell state, as
    well as a "carry" to ensure that the signal (in the form of a gradient) is not
    lost as the sequence is processed. At each time step, the LSTM model considers
    the current word, the carry, and the cell state jointly.'
  prefs: []
  type: TYPE_NORMAL
- en: The topic itself is not that trivial, but for practical purposes, full comprehension
    of the structural design is not essential. It suffices to keep in mind that an
    LSTM cell allows past information to be reinjected at a later point in time.
  prefs: []
  type: TYPE_NORMAL
- en: We will train our model on the NYT comment and headlines dataset ([https://www.kaggle.com/aashita/nyt-comments](https://www.kaggle.com/aashita/nyt-comments))
    and will use it to generate new headlines. We chose this dataset for its moderate
    size (the recipe should be reproducible without access to a powerful workstation)
    and availability (Kaggle is freely accessible, unlike some data sources accessible
    only via paywall).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As usual, first we import the necessary packages.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We want to make sure our results are reproducible – due to the nature of the
    interdependencies within the Python deep learning universe, we need to initialize
    multiple random mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step involves importing the necessary functionality from Keras itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, it is typically convenient—if not always in line with what purists
    deem best practice—to customize the level of warnings displayed in the execution
    of our code. It is mainly to deal with ubiquitous warnings around assigning value
    to a subset of a DataFrame: clean demonstration is more important in the current
    context than sticking to the coding standards expected in a production environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We shall define some functions that will streamline the code later on. First,
    let''s clean the text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s use a wrapper around the built-in TensorFlow tokenizer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'A frequently useful step is to wrap up a model-building step inside a function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is some boilerplate for padding the sequences (the utility of
    this will become clearer in the course of the recipe):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we create a function that will be used to generate text from our fitted
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to load our dataset (the `break` clause serves as a fast way
    to only pick up articles and not comment datasets):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can inspect the first few elements as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'As is usually the case with real-life text data, we need to clean the input
    text. For simplicity, we perform only the basic preprocessing: punctuation removal
    and conversion of all words to lowercase:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the top 10 rows look like after the cleaning operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The next step is tokenization. Language models require input data in the form
    of sequences—given a sequence of words (tokens), the generation task boils down
    to predicting the next most likely token in the context. We can utilize the built-in
    tokenizer from the `preprocessing` module of Keras.
  prefs: []
  type: TYPE_NORMAL
- en: 'After cleaning up, we tokenize the input text: this is a process of extracting
    individual tokens (words or terms) from a corpus. We utilize the built-in tokenizer
    to retrieve the tokens and their respective indices. Each document is converted
    into a series of tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The vectors like `[1,708]`, `[1,708, 251]` represent the n-grams generated from
    the input data, where an integer is an index of the token in the overall vocabulary
    generated from the corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have transformed our dataset into a format of sequences of tokens—possibly
    of different lengths. There are two choices: go with RaggedTensors (which are
    a slightly more advanced topic in terms of usage) or equalize the lengths to adhere
    to the standard requirement of most RNN models. For the sake of simplicity of
    presentation, we proceed with the latter solution: padding sequences shorter than
    the threshold using the `pad_sequence` function. This step is easily combined
    with formatting the data into predictors and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We utilize a simple LSTM architecture using the Sequential API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Input layer: takes the tokenized sequence'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'LSTM layer: generates the output using LSTM units – we take 100 as a default
    value for the sake of demonstration, but the parameter (along with several others)
    is customizable'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dropout layer: we regularize the LSTM output to reduce the risk of overfitting'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Output layer: generates the most likely output token:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now train our model using the standard Keras syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have a fitted model, we can examine its performance: how good are
    the headlines generated by our LSTM based on a seed text? We achieve this by tokenizing
    the seed text, padding the sequence, and passing it into the model to obtain our
    predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, even with a relatively simple setup (a moderately sized dataset
    and a vanilla model), we can generate text that looks somewhat realistic. Further
    fine-tuning would of course allow for more sophisticated content, which is a topic
    we will cover in *Chapter 10*, *Transformers*.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple excellent resources online for learning about RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For an excellent introduction – with great examples – see the post by Andrej
    Karpathy: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A curated list of resources (tutorials, repositories) can be found at [https://github.com/kjw0612/awesome-rnn](https://github.com/kjw0612/awesome-rnn)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another great introduction can be found at [https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf](https://medium.com/@humble_bee/rnn-recurrent-neural-networks-lstm-842ba7205bbf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A popular task in NLP is sentiment classification: based on the content of
    a text snippet, identify the sentiment expressed therein. Practical applications
    include analysis of reviews, survey responses, social media comments, or healthcare
    materials.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will train our network on the Sentiment140 dataset introduced in [https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf](https://www-cs.stanford.edu/people/alecmgo/papers/TwitterDistantSupervision09.pdf),
    which contains 1.6 million tweets annotated with three classes: negative, neutral,
    and positive. In order to avoid issues with locale, we standardize the encoding
    (this part is best done from the console level and not inside the notebook). The
    logic is the following: the original dataset contains raw text that—by its very
    nature—can contain non-standard characters (such as emojis, which are obviously
    common in social media communication). We want to convert the text to UTF8—the
    de facto standard for NLP in English. The fastest way to do it is by using a Linux
    command-line functionality:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Iconv` is a standard tool for conversion between encodings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `-f` and `-t` flags denote the input encoding and the target one, respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-o` specifies the output file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by importing the necessary packages as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the hyperparameters of our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The embedding dimension is the size of word embedding we will use. In this
    recipe, we will use GloVe: an unsupervised learning algorithm trained on aggregated
    word co-occurrence statistics from a combined corpus of Wikipedia and Gigaword.
    The resulting vectors for (English) words give us an efficient way of representing
    text and are commonly referred to as embeddings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_length` and `padding_type` are parameters specifying how we pad the sequences
    (see previous recipe).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`training_size` specifies the size of the target corpus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`test_portion` defines the proportion of the data we will use as a holdout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dropout_val` and `nof_units` are hyperparameters for the model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s encapsulate the model creation step into a function. We define a fairly
    simple one for our classification task—an embedding layer, followed by regularization
    and convolution, pooling, and then the RNN layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Collect the content of the corpus we will train on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert to sentence format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Normalize the sentence lengths with padding (see previous section):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Divide the dataset into training and holdout sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'A crucial step in using RNN-based models for NLP applications is the `embeddings`
    matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'With all the preparations completed, we can set up the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Training is performed in the usual way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also assess the quality of our model visually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Training versus validation accuracy over epochs'
  prefs: []
  type: TYPE_NORMAL
- en: '![Obraz zawierający zrzut ekranu  Opis wygenerowany automatycznie](img/B16254_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Training versus validation loss over epochs'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from both graphs, the model already achieves good performance
    after a limited number of epochs and it stabilizes after that, with only minor
    fluctuations. Potential improvements would involve early stopping, and extending
    the size of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Readers interested in the applications of RNNs to sentiment classification
    can investigate the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow documentation tutorial: [https://www.tensorflow.org/tutorials/text/text_classification_rnn](https://www.tensorflow.org/tutorials/text/text_classification_rnn)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://link.springer.com/chapter/10.1007/978-3-030-28364-3_49](https://link.springer.com/chapter/10.1007/978-3-030-28364-3_49)
    is one of many articles demonstrating the application of RNNs to sentiment detection
    and it contains an extensive list of references'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GloVe documentation can be found at [https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stock price prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sequential models such as RNNs are naturally well suited to time series prediction—and
    one of the most advertised applications is the prediction of financial quantities,
    especially prices of different financial instruments. In this recipe, we demonstrate
    how to apply LSTM to the problem of time series prediction. We will focus on the
    price of Bitcoin—the most popular cryptocurrency.
  prefs: []
  type: TYPE_NORMAL
- en: 'A disclaimer is in order: this is a demonstration example on a popular dataset.
    It is not intended as investment advice of any kind; building a reliable time
    series prediction model applicable in finance is a challenging endeavor, outside
    the scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We begin by importing the necessary packages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The general parameters for our task are the future horizon of our prediction
    and the hyperparameter for the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'As before, we will encapsulate our model creation step in a function. It accepts
    a single parameter, `units`, which is the dimension of the inner cells in LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now proceed to load the data, with the usual formatting of the timestamp.
    For the sake of our demonstration, we will predict the average daily price—hence
    the grouping operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to split the data into training and test periods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Preprocessing could theoretically be avoided, but it tends to help convergence
    in practice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Fitting the model is straightforward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'With a fitted model we can generate a prediction over the forecast horizon,
    keeping in mind the need to invert our normalization so that the values are back
    on the original scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what our forecasted results look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/B16254_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Actual price and predicted price over time'
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, it is clear that even a simple model can generate a reasonable prediction—with
    an important caveat: this approach only works as long as the environment is stationary,
    that is, the nature of the relationship between past and present values remains
    stable over time. Regime changes and sudden interventions might have a dramatic
    impact on the price, if for example a major jurisdiction were to restrict the
    usage of cryptocurrencies (as has been the case over the last decade). Such occurrences
    can be modeled, but they require more elaborate approaches to feature engineering
    and are outside the scope of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: Open-domain question answering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Question-answering** (**QA**) systems aim to emulate the human process of
    searching for information online, with machine learning methods employed to improve
    the accuracy of the provided answers. In this recipe, we will demonstrate how
    to use RNNs to predict long and short responses to questions about Wikipedia articles.
    We will use the Google Natural Questions dataset, along with which an excellent
    visualization helpful for understanding the idea behind QA can be found at [https://ai.google.com/research/NaturalQuestions/visualization](https://ai.google.com/research/NaturalQuestions/visualization).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The basic idea can be summarized as follows: for each article-question pair,
    you must predict/select long- and short-form answers to the question drawn *directly
    from the article*:'
  prefs: []
  type: TYPE_NORMAL
- en: A long answer would be a longer section of text that answers the question—several
    sentences or a paragraph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A short answer might be a sentence or phrase, or even in some cases a simple
    YES/NO. The short answers are always contained within, or a subset of, one of
    the plausible long answers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A given article can (and very often will) allow for both long *and* short answers,
    depending on the question.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The recipe presented in this chapter is adapted from code made public by Xing
    Han Lu: [https://www.kaggle.com/xhlulu](https://www.kaggle.com/xhlulu).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As usual, we start by loading the necessary packages. This time we are using
    the fasttext embeddings for our representation (available from [https://fasttext.cc/](https://fasttext.cc/)).
    Other popular choices include GloVe (used in the sentiment detection section)
    and ELMo ([https://allennlp.org/elmo](https://allennlp.org/elmo)). There is no
    clearly superior one in terms of performance on NLP tasks, so we''ll switch our
    choices as we go to demonstrate the different possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The general settings are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Our next step is to add some boilerplate code to streamline the code flow later.
    Since the task at hand is a little more involved than in the previous instances
    (or less intuitive), we wrap up more of the preparation work inside the dataset
    building functions. Due to the size of the dataset, we only load a subset of the
    training data and sample the negative-labeled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'With the next function, we train a Keras tokenizer to encode the text and questions
    into a list of integers (tokenization), then pad them to a fixed length to form
    a single NumPy array for text and another for questions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual with RNN-based models for NLP, we need an embeddings matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Next is our model construction step, wrapped up in a function:'
  prefs: []
  type: TYPE_NORMAL
- en: We build two 2-layer bidirectional LSTMs; one to read the questions, and one
    to read the text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We concatenate the output and pass it to a fully connected layer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We use sigmoid on the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'With the toolkit that we''ve defined, we can construct the datasets as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'This is what the dataset looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now construct the model itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The fitting is next, and that proceeds in the usual manner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can build a test set to have a look at our generated answers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate the actual predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '![Obraz zawierający stół  Opis wygenerowany automatycznie](img/B16254_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: As you can see, LSTM allows us to handle fairly abstract tasks such as answering
    different types of questions. The bulk of the work in this recipe was around formatting
    the data into a suitable input format, and then postprocessing the results—the
    actual modeling occurs in a very similar fashion to that in the preceding chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we have demonstrated the different capabilities of RNNs. They
    can handle diverse tasks with a sequential component (text generation and classification,
    time series prediction, and QA) within a unified framework. In the next chapter,
    we shall introduce transformers: an important architecture class that made it
    possible to reach new state-of-the-art results with NLP problems.'
  prefs: []
  type: TYPE_NORMAL
