<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer107">&#13;
			<p id="_idParaDest-77" class="chapter-number"><a id="_idTextAnchor145"/>Chapter 5: </p>&#13;
			<h1 id="_idParaDest-78"><a id="_idTextAnchor146"/>Training at Scale</h1>&#13;
			<p>When we build and train more complex models or use large amounts of data in an ingestion pipeline, we naturally want to make better use of all the compute time and memory resources at our disposal in a more efficient way. This is the major purpose of this chapter, as we are going to integrate what we learned in previous chapters with techniques for distributed training running in a cluster of compute nodes. </p>&#13;
			<p>TensorFlow has developed a high-level API for distributed training. Furthermore, this API integrates with the Keras API very well. As it turns out, the Keras API is now a first-class citizen in the TensorFlow ecosystem. Compared to the estimator API, Keras receives the most support when it comes to a distributed training strategy. Therefore, this chapter will predominantly focus on using the Keras API with a distributed training strategy. We will leverage Google Cloud resources to demonstrate how to make minimal changes to the Keras API code we are already familiar with and integrate it with the distributed training strategy. </p>&#13;
			<p>In this chapter, we will learn how to leverage Google Cloud's AI Platform and use the <strong class="bold">Tensor Processing Unit</strong> (<strong class="bold">TPU</strong>) or GPU to conduct transfer learning. Our goal is to use a prebuilt ResNet feature vector model as a base model to train with our own dataset, then store the model and assets in cloud storage. To do this, we will learn how to stream a training dataset as <strong class="source-inline">TFRecordDataset</strong> into the model training workflow, and designate a distributed training strategy for the TPU and GPU accelerators. All the code for this chapter can be found at <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_05">https://github.com/PacktPublishing/learn-tensorflow-enterprise/tree/master/chapter_05</a>.</p>&#13;
			<p>In this chapter, we will cover the following topics:</p>&#13;
			<ul>&#13;
				<li>Using the Cloud TPU through AI Platform</li>&#13;
				<li>Using the Cloud GPU through AI Platform</li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-79"><a id="_idTextAnchor147"/>Using the Cloud TPU through AI Platform</h1>&#13;
			<p>Before <a id="_idIndexMarker219"/>we begin, let's briefly discuss the possible costs you might<a id="_idIndexMarker220"/> incur in the <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>). All the<a id="_idIndexMarker221"/> scripts and examples here are catered to didactic purposes. Therefore, training epochs are usually set to minimally reasonable values. With that in mind, it is still worth noting that as we start to leverage cloud resources, we need to keep in mind the compute cluster's cost. You will find more information on AI Platform training charges<a id="_idIndexMarker222"/> here: <a href="https://cloud.google.com/ai-platform/training/pricing#examples_calculate_training_cost_using_price_per_hour">https://cloud.google.com/ai-platform/training/pricing#examples_calculate_training_cost_using_price_per_hour</a>.</p>&#13;
			<p>The examples in this book typically use the predefined scale tiers. In the predefined scale tiers listing at the preceding link, you will see the price per hour for different tiers. For example, <strong class="source-inline">BASIC_TPU</strong> is much more expensive than <strong class="source-inline">BASIC_GPU</strong>. We will use both in this chapter, as we will learn how to submit a training job to either the TPU or GPU. In my experience, each example in this book should complete its run between 20 to 60 minutes, with the parameters set either here in the book or in the GitHub repo. Your experience may vary, depending on your region and the availability of the compute resources.</p>&#13;
			<p>This cost does not include the cost of cloud storage, where you will read and write data or model artifacts. Remember to delete cloud storage when you are not using it. FYI, the cloud storage cost for the content and work related to this book is a very small fraction of the overall cost. </p>&#13;
			<p class="callout-heading">Tip</p>&#13;
			<p class="callout">Sometimes, when the GPU is in high demand, you may want to use the TPU, which is the fastest cluster that GCP offers. It may reduce the training time significantly, and likewise your expenses.</p>&#13;
			<p>If you haven't done so already, go ahead and clone the repo: </p>&#13;
			<p class="source-code"><strong class="bold">git clone </strong><a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise.git"><strong class="bold">https://github.com/PacktPublishing/learn-tensorflow-enterprise.git</strong></a></p>&#13;
			<p>As we have seen in previous chapters, Google's AI Platform offers a convenient development environment known as JupyterLab. It integrates with other Google Cloud services, such as BigQuery or cloud storage buckets, through SDKs. In this section, we are going to leverage Google Cloud's TPU for a distributed training workload. </p>&#13;
			<p>The TPU is a custom-built ASIC per Google's specification and design. It is an accelerator that is specifically optimized to handle deep learning calculations and algorithms. For this reason, a TPU is ideally suited for training complex neural networks and machine learning models with a virtually unlimited amount of training data. It completes a training routine in minutes where it would have taken hours in a single node machine.</p>&#13;
			<p>Currently, there are four<a id="_idIndexMarker223"/> types of TPU offerings: <strong class="bold">V2</strong>, <strong class="bold">V2 Pod</strong>, <strong class="bold">V3</strong>, and <strong class="bold">V3 Pod</strong>. For more details, refer to the official link, which includes<a id="_idIndexMarker224"/> Google Cloud's description of the benefits of the Cloud TPU: <a href="https://cloud.google.com/tpu/?_ga=2.138028336.-1825888872.1592693180">https://cloud.google.com/tpu/?_ga=2.138028336.-1825888872.1592693180</a>). For AI Platform instances that run TensorFlow Enterprise 2.1 or above, V3 is the preferred choice. With either V2 or V3, a <strong class="bold">TPU pod</strong> consists of<a id="_idIndexMarker225"/> multiple TPUs. A pod is basically a cluster of TPUs. For <a id="_idIndexMarker226"/>more details about the TPU and TPU pods, the following link describes the different versions of TPU pods and their runtimes for different machine learning training jobs: <a href="https://cloud.google.com/tpu/docs/system-architecture#configurations">https://cloud.google.com/tpu/docs/system-architecture#configurations</a>. Each pod, whether it's V2 or V3, can perform up to 100 petaFLOPS. This performance is reported at this link: <a href="https://techcrunch.com/2019/05/07/googles-newest-cloud-tpu-pods-feature-over-1000-tpus/">https://techcrunch.com/2019/05/07/googles-newest-cloud-tpu-pods-feature-over-1000-tpus/</a>.</p>&#13;
			<p>The benefit of a pod over a single TPU is the training speed and memory at your disposal for the training workflow. Compared to a V2 pod (512 cores = eight cores per TPU multiplied by 64 TPUs), each V2 TPU consists of eight cores, and each core is the basic unit for training data parallelism. At the core level, the TensorFlow distributed training strategy is executed. For demonstration and didactic purposes, all the examples in this section distribute a training strategy among eight cores within a TPU. The <strong class="source-inline">tf.distribute.TPUStrategy</strong> API is the means to distribute training in the TPU. This strategy implements synchronous distributed training coupled with the TPU's all-reduce operations across multiple TPU cores. </p>&#13;
			<p>We will use a Cloud TPU and submit a training job. In this example, we are going to see how to submit such a training job using the <strong class="bold">Google Cloud SDK</strong> in a client node (this could be another VM, or even your local machine). The SDK is responsible for authenticating your credentials and project ID so that the data and compute consumptions in Google Cloud can be billed. We are going to train image classification models to classify five different types of flowers (daisies, dandelions, roses, sunflowers, and tulips). These flower images are obtained from the TensorFlow 2.1 distribution. The images are shuffled at random and split into training, validation, and test sets. Each set is converted into <strong class="source-inline">tfrecord</strong> format with the images' original dimensions. The <strong class="source-inline">tfrecord</strong> images are stored in a <a id="_idIndexMarker227"/>Google Cloud storage bucket (it is assumed that your <strong class="source-inline">tfrecord</strong> is ready; generating <strong class="source-inline">tfrecord</strong> formatted data from raw images is not covered in this chapter). </p>&#13;
			<p>The training workflow <a id="_idIndexMarker228"/>will generate checkpoints and save model artifacts when the training is complete. These items are likewise saved in the storage bucket. Therefore, we will have to grant the TPU read and write access to our working storage bucket. </p>&#13;
			<p>Before we begin using the TPU, there are a few administrative items in Google Cloud to take care of. Let's get started.</p>&#13;
			<h2 id="_idParaDest-80"><a id="_idTextAnchor148"/>Installing the Cloud SDK</h2>&#13;
			<p>To install the <a id="_idIndexMarker229"/>Cloud SDK in the client node, download and install Google Cloud SDK. There is a good instruction page about how to install the Cloud SDK for different types of systems, be it Mac, Linux, or Windows. It is strongly recommended to follow the instructions at this link to install Google Cloud SDK: <a href="https://cloud.google.com/sdk/docs#install_the_latest_cloud_sdk_version">https://cloud.google.com/sdk/docs#install_the_latest_cloud_sdk_version</a>. Once the installation is done, you can verify it with the following command:</p>&#13;
			<p class="source-code">gcloud --help</p>&#13;
			<p>The preceding command will return the following <a id="_idTextAnchor149"/>output:</p>&#13;
			<div>&#13;
				<div id="_idContainer088" class="IMG---Figure">&#13;
					<img src="Images/image0011.jpg" alt="Figure 5.1 – gcloud SDK verification&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.1 – gcloud SDK verification</p>&#13;
			<p><em class="italic">Figure 5.1</em> shows<a id="_idIndexMarker230"/> the general format of the <strong class="source-inline">gcloud</strong> command. Use <em class="italic">Ctrl</em> + <em class="italic">C</em> to exit this mode and recover your Command Prompt. </p>&#13;
			<p>Granting the Cloud TPU access to your project</p>&#13;
			<p>From here, the setup<a id="_idIndexMarker231"/> instructions are from Google Cloud's own documentation site at this URL: <a href="https://cloud.google.com/ai-platform/training/docs/using-tpus#tpu-runtime-versions">https://cloud.google.com/ai-platform/training/docs/using-tpus#tpu-runtime-versions</a>:</p>&#13;
			<ol>&#13;
				<li>In this step, we are going to retrieve a cloud TPU service account name per our project ID. We can use the following command:<p class="source-code"><strong class="bold">curl -H 'Authorization: Bearer $(gcloud auth print-access-token)'  \  https://ml.googleapis.com/v1/projects/&lt;your-project-id&gt;:getConfig</strong></p><p>The preceding command will return the following<a id="_idTextAnchor150"/> output:</p><div id="_idContainer089" class="IMG---Figure"><img src="Images/image0031.jpg" alt="Figure 5.2 – TPU service account retrieval&#13;&#10;"/></div><p class="figure-caption">Figure 5.2 – TPU service account retrieval</p></li>&#13;
				<li>Make a note of the <strong class="source-inline">serviceAccountProject</strong> and <strong class="source-inline">tpuServiceAccount</strong> details.</li>&#13;
				<li>Once we know our TPU service account, we will have to initialize it as per the following command:<p class="source-code"><strong class="bold">curl -H 'Authorization: Bearer $(gcloud auth print-access-token)'  \  -H 'Content-Type: application/json' -d '{}'  \  https://serviceusage.googleapis.com/v1beta1/projects/&lt;serviceAccountProject&gt;/services/tpu.googleapis.com:generateServiceIdentity</strong></p></li>&#13;
			</ol>&#13;
			<p>The preceding <a id="_idIndexMarker232"/>command generates a Cloud TPU service account for you. Make sure you put your <strong class="source-inline">&lt;serviceAccountProject&gt;</strong> details in the URL.</p>&#13;
			<p>Adding a TPU service account as a member of the project</p>&#13;
			<p>The project<a id="_idIndexMarker233"/> we use must also know about the TPU service account. In <em class="italic">step 3</em> of the previous section, we passed our project's Bearer Token to our TPU service account so the TPU can access our project. Basically, it is similar to adding another member to this project, and in this case, the new member is the TPU service account:</p>&#13;
			<ol>&#13;
				<li value="1">We can use Google Cloud Console to achieve this, as shown in <em class="italic">Fig<a id="_idTextAnchor151"/>ure 5.</em><em class="italic">3</em>:<div id="_idContainer090" class="IMG---Figure"><img src="Images/image0051.jpg" alt="Figure 5.3 – The IAM &amp; Admin entry&#13;&#10;"/></div><p class="figure-caption">Figure 5.3 – The IAM &amp; Admin entry</p></li>&#13;
				<li>On <a id="_idIndexMarker234"/>the <strong class="bold">IAM</strong> screen, click the <strong class="bold">ADD</strong> button to add our TPU to this project, as in <em class="italic">Fig<a id="_idTextAnchor152"/>ure 5.4</em>:<div id="_idContainer091" class="IMG---Figure"><img src="Images/image0071.jpg" alt="Figure 5.4 – Adding a member to a project&#13;&#10;"/></div><p class="figure-caption">Figure 5.4 – Adding a member to a project</p></li>&#13;
				<li>Then, fill in the<a id="_idIndexMarker235"/> TPU service account details in the <strong class="bold">New members</strong> box. Under <strong class="bold">Select a role</strong>, find <strong class="bold">Service Agent Roles</strong>, and then find <strong class="bold">Cloud ML Service Agent</strong>. This is shown in <em class="italic">Fig<a id="_idTextAnchor153"/>ure 5.5</em>:<div id="_idContainer092" class="IMG---Figure"><img src="Images/image0091.jpg" alt="Figure 5.5 – Assigning the Cloud ML Service Agent role to the TPU service account&#13;&#10;"/></div><p class="figure-caption">Figure 5.5 – Assigning the Cloud ML Service Agent role to the TPU service account</p><p>We are not done with the TPU service account yet. We also have to let it access our training data and our storage to write the training results to, such as checkpoints and model assets. This means adding a couple of new roles for our TPU service account.</p></li>&#13;
				<li>Let's<a id="_idIndexMarker236"/> click <strong class="bold">Add another role</strong> and proceed to look for <strong class="bold">Project</strong>, as in <em class="italic">Fi<a id="_idTextAnchor154"/>gure 5.6</em>:<div id="_idContainer093" class="IMG---Figure"><img src="Images/image0111.jpg" alt="Figure 5.6 – Assigning the Project Viewer role to the TPU service account&#13;&#10;"/></div><p class="figure-caption">Figure 5.6 – Assigning the Project Viewer role to the TPU service account</p></li>&#13;
				<li>Likewise, we also add the <strong class="bold">Cloud Storage Admin</strong> role, as in <em class="italic">Fig<a id="_idTextAnchor155"/>ure 5.7</em>:<div id="_idContainer094" class="IMG---Figure"><img src="Images/image0131.jpg" alt="Figure 5.7 – Assigning the Storage Admin role to the TPU service account&#13;&#10;"/></div><p class="figure-caption">Figure 5.7 – Assigning the Storage Admin role to the TPU service account</p></li>&#13;
				<li>Once you<a id="_idIndexMarker237"/> have all three roles set up, click <strong class="bold">Save</strong>. </li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-81"><a id="_idTextAnchor156"/>Whitelisting access for reading training data and writing artifacts (alternative)</h2>&#13;
			<p>The previous <a id="_idIndexMarker238"/>method grants rather <a id="_idIndexMarker239"/>broad permissions to the TPU service. It allows the TPU to have an admin credential to all of your storage buckets. If you'd prefer to limit the TPU service to only certain buckets, then you can put the TPU service account in each <a id="_idIndexMarker240"/>bucket's <strong class="bold">access control list</strong> (<strong class="bold">ACL</strong>). You can do this for your training data bucket and if you want the training job to write to another bucket, then do the same for that as well:</p>&#13;
			<ol>&#13;
				<li value="1">We can<a id="_idIndexMarker241"/> start by editing the<a id="_idIndexMarker242"/> bucket permissions, as shown in <em class="italic">Figure 5.8</em>. Select the <strong class="bold">PERM<a id="_idTextAnchor157"/>ISSIONS</strong> tab:<div id="_idContainer095" class="IMG---Figure"><img src="Images/image0151.jpg" alt="Figure 5.8 – Editing the storage bucket permissions&#13;&#10;"/></div><p class="figure-caption">Figure 5.8 – Editing the storage bucket permissions</p></li>&#13;
				<li>Then, click on <strong class="bold">ADD</strong>, as shown in <em class="italic">F<a id="_idTextAnchor158"/>igure 5.</em><em class="italic">9</em>: <div id="_idContainer096" class="IMG---Figure"><img src="Images/image0171.jpg" alt="Figure 5.9 – Adding the TPU service account to the bucket ACL&#13;&#10;"/></div><p class="figure-caption">Figure 5.9 – Adding the TPU service account to the bucket ACL</p></li>&#13;
				<li>Then, add<a id="_idIndexMarker243"/> two new roles to the<a id="_idIndexMarker244"/> TPU service account by filling in the service account name, as in <em class="italic">Figure 5.10</em>. In this example, we will use the same bucket for hosting training data and writing training artifacts. Therefore, we need to add two roles from <strong class="bold">Cloud Storage Legacy</strong>: <strong class="bold">Storage Legacy Bucket Reader</strong> and <strong class="bold">Storage Legacy B<a id="_idTextAnchor159"/>ucket Writer</strong>:<div id="_idContainer097" class="IMG---Figure"><img src="Images/image0192.jpg" alt="Figure 5.10 – Whitelisting the TPU service account for two roles in the storage bucket&#13;&#10;"/></div><p class="figure-caption">Figure 5.10 – Whitelisting the TPU service account for two roles in the storage bucket</p></li>&#13;
				<li>Once <a id="_idIndexMarker245"/>these roles are <a id="_idIndexMarker246"/>added, click <strong class="bold">Save</strong>. </li>&#13;
			</ol>&#13;
			<p>We have completed the minimum administrative work in order to use the Cloud TPU for our model training workflow. In the next section, we are going to see how to refactor our code and set up the distributed training strategy for the TPU.</p>&#13;
			<h2 id="_idParaDest-82"><a id="_idTextAnchor160"/>Execution command and format</h2>&#13;
			<p>AI Platform also<a id="_idIndexMarker247"/> provides model training as a service. It enables users to submit a training job from their local environment's command line. The job will run on Google Cloud's compute cluster (with options for the CPU, TPU, or GPU in different pricing tiers). If you are new to the concept of<a id="_idIndexMarker248"/> training-as-a-service, refer to this link for details: <a href="https://cloud.google.com/ai-platform/training/docs/training-jobs">https://cloud.google.com/ai-platform/training/docs/training-jobs</a>. </p>&#13;
			<p>Besides cloud training jobs, AI Platform can also do cloud inferencing jobs. The command we are going to run is for submitting a cloud training job. You can keep this link as a reference as you follow along with this chapter's exercises: <a href="https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training">https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training</a>. Since the approach is to keep the training script in a client node (that is, a local computer with Google Cloud SDK installed), we need to let the Google Cloud runtime know, when executing <strong class="source-inline">gcloud ai-platform jobs submit training</strong>, where all the training scripts are. Also, because there are libraries that will be imported in our script, we need to specify the information, such as their versions and the library names and versions, in a file named <strong class="source-inline">setup.py</strong>. In order to accomplish this, it is necessary to create a small <strong class="source-inline">setup.py</strong> file in your working directory:</p>&#13;
			<ol>&#13;
				<li value="1">In the command terminal (for Mac OS X or Linux) of your working directory, type the following:<p class="source-code"><strong class="bold">cat &lt;&lt; END &gt; setup.py from setuptools import find_packages from setuptools import setup setup(    name='official',    install_requires=['tensorflow-datasets~=3.1',     	                      'tensorflow_hub&gt;=0.6.0'],    packages=find_packages())END</strong></p><p>The preceding code is generated according to <em class="italic">step 2</em> in this example. Alternatively, you can use a text editor to generate the following:</p><p class="source-code"><strong class="bold">from setuptools import find_packages from setuptools import setup setup(    name='official',    install_requires=['tensorflow-datasets~=3.1', 		                      'tensorflow_hub&gt;=0.6.0'],    packages=find_packages())</strong></p><p>You should save the preceding code as <strong class="source-inline">setup.py</strong>. In <strong class="source-inline">install_requires</strong>, you will see a Python list that contains TensorFlow datasets or <strong class="source-inline">tensorflow_hub</strong>. This is where dependencies are added to the runtime in Google Cloud AI Platform.</p></li>&#13;
				<li>Now that we are ready to set up the command for distributed training in the Cloud TPU, let's first take a look at the command and execution format. Recall that we stated earlier that this task will be executed using the Cloud SDK running in a client. In general, the client node will issue the <strong class="source-inline">gcloud</strong> command with input flags, such<a id="_idIndexMarker249"/> as this:<p class="source-code"><strong class="bold">gcloud ai-platform jobs submit training cloudtpu \</strong></p><p class="source-code"><strong class="bold">--staging-bucket=gs://ai-tpu-experiment \</strong></p><p class="source-code"><strong class="bold">--package-path=python \</strong></p><p class="source-code"><strong class="bold">--module-name=python.ScriptProject.traincloudtpu_resnet_cache \</strong></p><p class="source-code"><strong class="bold">--runtime-version=2.2 \</strong></p><p class="source-code"><strong class="bold">--python-version=3.7 \</strong></p><p class="source-code"><strong class="bold">--scale-tier=BASIC_TPU \</strong></p><p class="source-code"><strong class="bold">--region=us-central1 \</strong></p><p class="source-code"><strong class="bold">-- \</strong></p><p class="source-code"><strong class="bold">--distribution_strategy=tpu \</strong></p><p class="source-code"><strong class="bold">--model_dir=gs://ai-tpu-experiment/traincloudtpu_tfkd_resnet_cache \</strong></p><p class="source-code"><strong class="bold">--train_epochs=10 \</strong></p><p class="source-code"><strong class="bold">--data_dir=gs://ai-tpu-experiment/tfrecord-flowers</strong></p></li>&#13;
			</ol>&#13;
			<p>There are<a id="_idIndexMarker250"/> many more flags (user input parameters) available than shown here. For detailed descriptions of all the possible input parameters, refer to the TensorFlow Google Cloud AI Platform reference (<a href="https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training">https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training</a>) and the Cloud SDK documentation (<a href="https://cloud.google.com/sdk/gcloud/reference/ai-platform">https://cloud.google.com/sdk/gcloud/reference/ai-platform</a>).</p>&#13;
			<h2 id="_idParaDest-83"><a id="_idTextAnchor161"/>Cloud command arguments</h2>&#13;
			<p>The example command (discussed ahead in this section), illustrates a directory structure as shown in <em class="italic">Figure 5.11</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer098" class="IMG---Figure">&#13;
					<img src="Images/image0211.jpg" alt="Figure 5.11 – Directory structure and file organization in a local client for an example training run&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.11 – Directory structure and file organization in a local client for an example training run</p>&#13;
			<p>Some of the folder names in the preceding figure are personal choices: <strong class="source-inline">vs_code</strong>, <strong class="source-inline">python</strong>, <strong class="source-inline">ScriptProject</strong>. You can name these folders to your preference. The training script named <strong class="source-inline">traincloudtpu_resnet_cache</strong> is also a personal choice. </p>&#13;
			<p>Let's take a look at this example command. This example command can be divided into two parts based on <strong class="source-inline">-- \</strong>. The first part of the command includes the following:</p>&#13;
			<p class="source-code">gcloud ai-platform jobs submit training cloudtpu \</p>&#13;
			<p class="source-code">--staging-bucket=gs://ai-tpu-experiment \</p>&#13;
			<p class="source-code">--package-path=python \</p>&#13;
			<p class="source-code">--module-name=python.ScriptProject.traincloudtpu_resnet_cache \</p>&#13;
			<p class="source-code">--runtime-version=2.1 \</p>&#13;
			<p class="source-code">--python-version=3.7 \</p>&#13;
			<p class="source-code">--scale-tier=BASIC_TPU \</p>&#13;
			<p class="source-code">--region=us-central1 \</p>&#13;
			<p>This command is executed in the <strong class="source-inline">vs_code</strong> directory shown in <em class="italic">Figure 5.11</em>. In this directory, you will find <strong class="source-inline">setup.py</strong>. This is the file that tells the <strong class="source-inline">gcloud</strong> runtime about the dependencies or packages required for the training script. <strong class="source-inline">cloudtpu</strong> is just a name we provided for this training run. We also need to specify a staging bucket (a cloud storage bucket) for<a id="_idIndexMarker251"/> serialization of model artifacts during and after training. </p>&#13;
			<p><strong class="source-inline">package-path</strong> is the folder for organizing projects. In this case, within this package, we are interested in executing a training script, <strong class="source-inline">traincloudtpu_resnet_cache.py</strong>. In order for the <strong class="source-inline">gcloud</strong> runtime to find it, we need to specify the following:</p>&#13;
			<p class="source-code">module-name=python.ScriptProject.traincloudtpu_resnet_cache</p>&#13;
			<p>We then specify the TensorFlow Enterprise version to be 2.1 and the Python interpreter version to be 3.7, and a scale tier of <strong class="source-inline">BASIC_TPU</strong> should suffice for this example. We also set the region to be <strong class="source-inline">us-central1</strong>. The <strong class="source-inline">BASIC_TPU</strong> scale tier provides us with a master VM and a TPU VM with eight TPU V2 cores.</p>&#13;
			<p>As stated earlier, <strong class="source-inline">-- \</strong> separates the <strong class="source-inline">gcloud</strong> system flags from any other user-defined flags that are specified and serve as input parameters to the training script. This separation is necessary and by design. Do not mix system flags with user-defined flags. See the SDK reference (<a href="https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training">https://cloud.google.com/sdk/gcloud/reference/ai-platform/jobs/submit/training</a>) for details about positional arguments.</p>&#13;
			<p>Now, let's take a look at the second half of this command, which consists of user-defined flags:</p>&#13;
			<p class="source-code">--distribution_strategy=tpu \</p>&#13;
			<p class="source-code">--model_dir=gs://ai-tpu-experiment/traincloudtpu_tfkd_resnet_cache \</p>&#13;
			<p class="source-code">--train_epochs=10 \--data_dir=gs://ai-tpu-experiment/tfrecord-flowers</p>&#13;
			<p>We specify <strong class="source-inline">distribution_strategy=tpu</strong> as a user-defined flag because we may use this value in conditional logic to select the proper distribution strategy. We also specify <strong class="source-inline">model_dir</strong>, which is a cloud storage path that we grant write permissions to the TPU service in order to serialize checkpoints and model assets. Then, for the remaining flags, we specify the number of epochs for training in <strong class="source-inline">train_epochs</strong>, and the path to the training <a id="_idIndexMarker252"/>data indicated by <strong class="source-inline">data_dir</strong>, which is also a cloud storage path that we grant read permissions to the TPU service. The TPU's distributed training strategy (<a href="https://www.tensorflow.org/guide/distributed_training#tpustrategy">https://www.tensorflow.org/guide/distributed_training#tpustrategy</a>) implements all necessary operations across multiple cores. </p>&#13;
			<h2 id="_idParaDest-84"><a id="_idTextAnchor162"/>Organizing the training script</h2>&#13;
			<p>The general structure of the training script<a id="_idIndexMarker253"/> for this example follows a minimalist style. In practice, it is common to organize Python code into multiple files and modules. Therefore, we will have everything we need in one Python script, <strong class="source-inline">train.py</strong>. Its pseudo-code is as follows:</p>&#13;
			<p class="source-code">def run( input parameters ):</p>&#13;
			<p class="source-code">	# specify distribute strategy (https://cloud.google.com/</p>&#13;
			<p class="source-code">	ai-platform/training/docs/using-tpus)</p>&#13;
			<p class="source-code">	import tensorflow as tf</p>&#13;
			<p class="source-code">	if distribution_strategy==TPU: </p>&#13;
			<p class="source-code">		resolver = tf.distribute.cluster_resolver.				TPUClusterResolver()</p>&#13;
			<p class="source-code">	tf.config.experimental_connect_to_cluster(resolver)</p>&#13;
			<p class="source-code">	tf.tpu.experimental.initialize_tpu_system(resolver)</p>&#13;
			<p class="source-code">	strategy = tf.distribute.experimental.TPUStrategy(resolver)</p>&#13;
			<p class="source-code">	# build data streaming pipeline with tf.io and tf.data.TFRecordDataset</p>&#13;
			<p class="source-code">	# build model</p>&#13;
			<p class="source-code">	# train model</p>&#13;
			<p class="source-code">	# save results</p>&#13;
			<p class="source-code">def main():</p>&#13;
			<p class="source-code">run(input parameters)</p>&#13;
			<p class="source-code">if __name__ == '__main__'</p>&#13;
			<p class="source-code">app.run(main)</p>&#13;
			<p>Once the <strong class="source-inline">main()</strong> routine is run, it will invoke <strong class="source-inline">run()</strong>, where a training strategy is defined, then a data pipeline is built, followed by building and training a model, then finally, it saves the<a id="_idIndexMarker254"/> results to cloud storage. </p>&#13;
			<p>Next, we will dive into the actual code for <strong class="source-inline">train.py</strong>. Let's start with the data streaming pipeline.</p>&#13;
			<h2 id="_idParaDest-85"><a id="_idTextAnchor163"/>Data streaming pipeline</h2>&#13;
			<p>Currently, the<a id="_idIndexMarker255"/> only way to stream a dataset when using Google Cloud AI Platform is through <strong class="source-inline">tf.io</strong> and <strong class="source-inline">tf.dataTFRecordDataset</strong>. </p>&#13;
			<p>Our dataset (TFRecord) is already in a storage bucket. It is organized a<a id="_idTextAnchor164"/>s shown in <em class="italic">Figure 5.12</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer099" class="IMG---Figure">&#13;
					<img src="Images/image023.jpg" alt="Figure 5.12 – Cloud storage for the flower image classification dataset&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.12 – Cloud storage for the flower image classification dataset</p>&#13;
			<ol>&#13;
				<li value="1">In our<a id="_idIndexMarker256"/> training script's <strong class="source-inline">run</strong> function, we need to specify the path to the cloud storage for the training data. We can leverage <strong class="source-inline">tf.io.gfile</strong> to encode the filename pattern for multiple parts. Then, we use <strong class="source-inline">tf.data.TFRecordDataset</strong> to instantiate a dataset object:<p class="source-code">root_dir = flags_obj.data_dir # this is gs://&lt;bucket&gt;/folder where tfrecord are stored</p><p class="source-code">train_file_pattern = '{}/image_classification_builder-train*.tfrecord*'.format(root_dir)</p><p class="source-code">val_file_pattern = '{}/image_classification_builder-validation*.tfrecord*'.format(root_dir)</p><p class="source-code">train_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(train_file_pattern))</p><p class="source-code">val_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(val_file_pattern))</p><p class="source-code">train_all_ds = tf.data.TFRecordDataset(train_all_files,num_parallel_reads=tf.data.experimental.AUTOTUNE)</p><p class="source-code">val_all_ds = tf.data.TFRecordDataset(val_all_files,num_parallel_reads=tf.data.experimental.AUTOTUNE)</p><p>As the preceding code indicates, after we encode the dataset name patterns, we then use <strong class="source-inline">tf.data.Dataset.list_files</strong> to encode a list of all the filenames that follow the pattern. Then, <strong class="source-inline">tf.data.TFRecordDataset</strong> instantiates a dataset reader object. Once these lines are executed during runtime, it effectively establishes the connection between the TPU and cloud storage. The dataset object streams the data into the model during the training workflow. </p><p>Why don't we use a <strong class="source-inline">tf.keras</strong> generator pattern, such as <strong class="source-inline">ImageDataGenerator</strong> or <strong class="source-inline">flow_from_directory</strong>? Well, it turns out this is not yet supported by the <strong class="source-inline">gcloud ai-platform jobs submit training</strong> command. This pattern is very convenient when the data is mounted or directly available in the filesystem, and it also easily takes care of <a id="_idIndexMarker257"/>the one-hot encoding of labels for classification problems, image normalization, and standardization processes via optional arguments through user input. </p></li>&#13;
				<li>We have to handle image standardization (resampling to a different height and width) by writing our own function. Here is a function that performs these operations on <strong class="source-inline">TFRecordDataset</strong>:<p class="source-code">    def decode_and_resize(serialized_example):</p><p class="source-code">        # resized image should be [224, 224, 3] and have 	          value range [0, 255] </p><p class="source-code">        # label is integer index of class.</p><p class="source-code">    </p><p class="source-code">        parsed_features = tf.io.parse_single_example(</p><p class="source-code">        serialized_example,</p><p class="source-code">        features = {</p><p class="source-code">        'image/channels' :  tf.io.FixedLenFeature([],    	                                           tf.int64),</p><p class="source-code">        'image/class/label' :  tf.io.FixedLenFeature([], 	                                              tf.int64),</p><p class="source-code">        'image/class/text' : tf.io.FixedLenFeature([], 	                                           tf.string),</p><p class="source-code">        'image/colorspace' : tf.io.FixedLenFeature([], 	                                           tf.string),</p><p class="source-code">        'image/encoded' : tf.io.FixedLenFeature([], 	                                                                                      	                                        tf.string),</p><p class="source-code">        'image/filename' : tf.io.FixedLenFeature([], 	                                            	                                         tf.string),</p><p class="source-code">        'image/format' : tf.io.FixedLenFeature([], 	                                                                                    	                                       tf.string),</p><p class="source-code">        'image/height' : tf.io.FixedLenFeature([],  	                                         	                                        tf.int64),</p><p class="source-code">        'image/width' : tf.io.FixedLenFeature([], </p><p class="source-code">                                        tf.int64)</p><p class="source-code">        })</p><p class="source-code">        image = tf.io.decode_jpeg(parsed_features[</p><p class="source-code">                            'image/encoded'], channels=3)</p><p class="source-code">        label = tf.cast(parsed_features[</p><p class="source-code">                          'image/class/label'], tf.int32)</p><p class="source-code">        label_txt = tf.cast(parsed_features[</p><p class="source-code">                          'image/class/text'], tf.string)</p><p class="source-code">        label_one_hot = tf.one_hot(label, depth = 5)</p><p class="source-code">        resized_image = tf.image.resize(image, </p><p class="source-code">                            [224, 224], method='nearest')</p><p class="source-code">        return resized_image, label_one_hot</p><p>This <strong class="source-inline">decode_and_resize</strong> function parses the dataset to a JPEG image with the corresponding color value range, then parse the labels, one-hot encodes the image, and resizes the image using the nearest neighbor method in order to standardize it to 224 by 224 pixels for our model of choice (ResNet). This function also provides different ways to return the label, whether it is as plain text or an integer. If you wish, you can return labels in different notations and styles by simply adding the notations of your interest to the <strong class="source-inline">return</strong> tuple:</p><p class="source-code">  return resized_image, label_one_hot, label_txt, label</p><p>Then, unpack <a id="_idIndexMarker258"/>the return tuple according to their position (the order as indicated in the preceding <strong class="source-inline">return</strong> statement) in the caller function.</p></li>&#13;
				<li>Now that we have the <strong class="source-inline">decode_and_resize</strong> function ready, this is how we apply it to every element in our <strong class="source-inline">dataset</strong> object:<p class="source-code">    dataset = train_all_ds.map(decode_and_resize)</p><p class="source-code">val_dataset = val_all_ds.map(decode_and_resize)</p></li>&#13;
				<li>Then, we rescale or normalize the pixel values to be in a range of <strong class="source-inline">[0, 1]</strong> in each image so that all the images are within same pixel range for training. Let's create a <strong class="source-inline">normalize</strong> function:<p class="source-code">def normalize(image, label):</p><p class="source-code">        #Convert `image` from [0, 255] -&gt; [0, 1.0] floats </p><p class="source-code">        image = tf.cast(image, tf.float32) / 255. + 0.5</p><p class="source-code">        return image, label</p><p>We need to prepare the training data by applying a batch operation. We use the following function to achieve this:</p><p class="source-code">def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):</p><p class="source-code">        if cache:</p><p class="source-code">            if isinstance(cache, str):</p><p class="source-code">                ds = ds.cache(cache)</p><p class="source-code">            else:</p><p class="source-code">                ds = ds.cache()</p><p class="source-code">        ds = ds.shuffle(buffer_size=shuffle_buffer_size)</p><p class="source-code">        ds = ds.repeat()</p><p class="source-code">        ds = ds.batch(BATCH_SIZE)</p><p class="source-code">        AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p class="source-code">        ds = ds.prefetch(buffer_size=AUTOTUNE)</p><p class="source-code">        return ds</p><p>The preceding<a id="_idIndexMarker259"/> function accepts a dataset, then shuffles it and batches it based on a global variable, <strong class="source-inline">BATCH_SIZE</strong>, and prefetches it for the training pipeline.</p><p>We use the <strong class="source-inline">map</strong> method again to apply the <strong class="source-inline">normalize</strong> operation to our training and validation datasets:</p><p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p class="source-code">BATCH_SIZE = flags_obj.train_batch_size</p><p class="source-code">VALIDATION_BATCH_SIZE = flags_obj.validation_batch_size</p><p class="source-code">train_dataset = train_dataset.map(normalize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">val_dataset = val_dataset.map(normalize, num_parallel_calls=AUTOTUNE)</p><p class="source-code">val_ds = val_dataset.batch(VALIDATION_BATCH_SIZE)   </p><p class="source-code">train_ds = prepare_for_training(train_dataset)</p><p>This is the data pipeline part of the <strong class="source-inline">run</strong> function. We are not done with the <strong class="source-inline">run</strong> function yet.</p></li>&#13;
				<li>Next, we are going to set up the model and conduct the training. We will leverage the popular transfer learning technique, where a prebuilt model is applied and trained with our own training dataset. The prebuilt model of interest here is the ResNet-50 image classification model. Remember how we already specified<a id="_idIndexMarker260"/> our TPU-based distributed strategy for training? We can simply wrap the model definition and optimizer choice here with the strategy:<p class="source-code">with strategy.scope():</p><p class="source-code">  base_model = tf.keras.applications.ResNet50(</p><p class="source-code">      input_shape=(224,224,3), include_top=False, 	   	      weights='imagenet')</p><p class="source-code">  model = tf.keras.Sequential(</p><p class="source-code">      [base_model,</p><p class="source-code">       tf.keras.layers.GlobalAveragePooling2D(),</p><p class="source-code">       tf.keras.layers.Dense(5, </p><p class="source-code">                             activation='softmax', </p><p class="source-code">                             name = 'custom_class')</p><p class="source-code">       ])</p><p class="source-code">  lr_schedule = \</p><p class="source-code">  tf.keras.optimizers.schedules.ExponentialDecay(</p><p class="source-code">      0.05, decay_steps=100000, decay_rate=0.96)</p><p class="source-code">  optimizer = tf.keras.optimizers.SGD(</p><p class="source-code">      learning_rate=lr_schedule)</p><p class="source-code">model.compile(optimizer=optimizer, </p><p class="source-code">  loss=tf.keras.losses.CategoricalCrossentropy(</p><p class="source-code">      from_logits=True, label_smoothing=0.1),</p><p class="source-code">      metrics=['accuracy'])</p><p>The preceding code describes the model architecture, designates the optimization strategy for training, and compiles the model. We use the ResNet-50 feature vector as our base model for the classification of the five flower types. </p></li>&#13;
				<li>Then, we <a id="_idIndexMarker261"/>set up the checkpoint and callbacks with the help of the following code:<p class="source-code">checkpoint_prefix = os.path.join(flags_obj.model_dir, 	                                          'ckpt_{epoch}')</p><p class="source-code">    callbacks = [</p><p class="source-code">    tf.keras.callbacks.ModelCheckpoint</p><p class="source-code">		(filepath=checkpoint_prefix,                                    </p><p class="source-code">		save_weights_only=True)]</p><p>Callbacks will save the model weights and biases for each epoch of training separately as checkpoints.</p></li>&#13;
				<li>Next, we need to set up sample sizes at each epoch for training and cross validation:<p class="source-code">train_sample_size=0</p><p class="source-code">    for raw_record in train_all_ds:</p><p class="source-code">        train_sample_size += 1</p><p class="source-code">    print('TRAIN_SAMPLE_SIZE = ', train_sample_size)</p><p class="source-code">    validation_sample_size=0</p><p class="source-code">    for raw_record in val_all_ds:</p><p class="source-code">        validation_sample_size += 1</p><p class="source-code">    print('VALIDATION_SAMPLE_SIZE = ', </p><p class="source-code">           validation_sample_size)</p><p class="source-code">    steps_per_epoch = train_sample_size // BATCH_SIZE</p><p class="source-code">    validation_steps = validation_sample_size </p><p class="source-code">                                 // VALIDATION_BATCH_SIZE</p></li>&#13;
				<li>Then, at last, this<a id="_idIndexMarker262"/> is the code for the training process:<p class="source-code">hist = model.fit(</p><p class="source-code">        train_ds,</p><p class="source-code">        epochs=flags_obj.train_epochs, </p><p class="source-code">                         steps_per_epoch=steps_per_epoch,</p><p class="source-code">        validation_data=val_ds,</p><p class="source-code">        validation_steps=validation_steps,</p><p class="source-code">        callbacks=callbacks)</p><p class="source-code">    model_save_dir = os.path.join(flags_obj.model_dir,  	                                            </p><p class="source-code">'save_model')</p><p class="source-code">    model.save(model_save_dir)</p><p>This concludes the <strong class="source-inline">run</strong> function. This is a rather long function. Make sure you observe all the proper indentation demarcation. This is just a minimalist example for Google Cloud AI Platform. It includes all the necessary code and patterns for a scalable data pipeline, distributed training workflow, and TPU utilization. In your practice, you can organize and refactor your code to best suit your needs for clarity and maintainability. </p></li>&#13;
			</ol>&#13;
			<h2 id="_idParaDest-86"><a id="_idTextAnchor165"/>Submitting the training script</h2>&#13;
			<p>Now is the time to <a id="_idIndexMarker263"/>submit our training script. We submit it from the <strong class="source-inline">vs_code</strong> directory according to the local directory organization mentioned in <em class="italic">Figure 5.11</em>. The TensorFlow runtime version in Cloud AI Platform is not necessarily most up to date with respect to the TensorFlow stable release in Cloud AI Notebook. As we know, the current stable release in Cloud Notebook is TFE 2.3. However, in Cloud AI Platform, the most recent release is 2.2. Therefore we use <strong class="source-inline">--runtime-version=2.2</strong>.</p>&#13;
			<p>You can check this link to ascertain the latest runtime: <a href="https://cloud.google.com/ai-platform/prediction/docs/runtime-version-list">https://cloud.google.com/ai-platform/prediction/docs/runtime-version-list</a>.</p>&#13;
			<p>The following is the command and terminal output:</p>&#13;
			<p class="source-code">vs_code % gcloud ai-platform jobs submit training traincloudtpu_tfk_resnet50 \</p>&#13;
			<p class="source-code">--staging-bucket=gs://ai-tpu-experiment \</p>&#13;
			<p class="source-code">--package-path=python \</p>&#13;
			<p class="source-code">--module-name=python.ScriptProject.trainer \</p>&#13;
			<p class="source-code">--runtime-version=2.2 \</p>&#13;
			<p class="source-code">--python-version=3.7 \</p>&#13;
			<p class="source-code">--scale-tier=BASIC_TPU \</p>&#13;
			<p class="source-code">--region=us-central1 \</p>&#13;
			<p class="source-code">-- \</p>&#13;
			<p class="source-code">--distribution_strategy=tpu \</p>&#13;
			<p class="source-code">--model_dir=gs://ai-tpu-experiment/traincloudtpu_tfk_resnet50 \</p>&#13;
			<p class="source-code">--train_epochs=10 \</p>&#13;
			<p class="source-code">--data_dir=gs://ai-tpu-experiment/tfrecord-flowers</p>&#13;
			<p class="source-code">Job [traincloudtpu_tfk_resnet50] submitted successfully.</p>&#13;
			<p>Your job is still active. You can view the status of your job with the following command:</p>&#13;
			<p class="source-code">  $ gcloud ai-platform jobs describe traincloudtpu_tfk_resnet50</p>&#13;
			<p>Or, you can continue streaming the logs with the following command:</p>&#13;
			<p class="source-code">  $ gcloud ai-platform jobs stream-logs traincloudtpu_tfk_resnet50</p>&#13;
			<p class="source-code">jobId: traincloudtpu_tfk_resnet50</p>&#13;
			<p class="source-code">state: QUEUED</p>&#13;
			<p>The preceding command<a id="_idIndexMarker264"/> submits a piece of training code to the Cloud TPU. From the current directory (<strong class="source-inline">vs_code</strong>), there is a subfolder (<strong class="source-inline">python</strong>), which contains a <strong class="source-inline">ScriptProject</strong> module. In <strong class="source-inline">ScriptProject</strong>, there is a part of the script named <strong class="source-inline">trainer.py</strong>. You can see the content of <strong class="source-inline">trainer.py</strong> in the GitHub repo at <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py</a>.</p>&#13;
			<p>We also have to specify the following parameters, which are used in <strong class="source-inline">trainer.py</strong> (<a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py</a>):</p>&#13;
			<p class="source-code">Job name: traincloudtpu_tfk_resnet50</p>&#13;
			<p class="source-code">Staging bucket is gs://ai-tpu-experiment</p>&#13;
			<p class="source-code">Bucket to save the model is gs://ai-tpu-experiment/traincloudtpu_tfk_resnet50</p>&#13;
			<p class="source-code">Training data is in gs://tfrecord-dataset/flowers</p>&#13;
			<p>As soon as we submit the preceding command, it will be in the queue for execution in your Cloud AI Platform instance. To find out where we can monitor the training process, we can run <strong class="source-inline">gcloud ai-platform</strong><strong class="source-inline"> jobs describe traincloudtpu_tfk_resnet50</strong> to retrieve a URL to the running log:</p>&#13;
			<p class="source-code">vs_code % gcloud ai-platform jobs describe traincloudtpu_tfk_resnet50</p>&#13;
			<p class="source-code">createTime: ‚2020-08-09T20:59:16Z'</p>&#13;
			<p class="source-code">etag: QMhh5Jz_KMU=</p>&#13;
			<p class="source-code">jobId: traincloudtpu_tfk_resnet50</p>&#13;
			<p class="source-code">state: PREPARING</p>&#13;
			<p class="source-code">trainingInput:</p>&#13;
			<p class="source-code">  args:</p>&#13;
			<p class="source-code">  - --distribution_strategy=tpu</p>&#13;
			<p class="source-code">  - --model_dir=gs://ai-tpu-experiment/traincloudtpu_tfk_resnet50</p>&#13;
			<p class="source-code">  - --train_epochs=10</p>&#13;
			<p class="source-code">  - --data_dir=gs://ai-tpu-experiment/tfrecord-flowers</p>&#13;
			<p class="source-code">  packageUris:</p>&#13;
			<p class="source-code">  - gs://ai-tpu-experiment/traincloudtpu_tfk_resnet50/XXXXXX/official-0.0.0.tar.gz</p>&#13;
			<p class="source-code">  pythonModule: python.ScriptProject.trainer</p>&#13;
			<p class="source-code">  pythonVersion: '3.7'</p>&#13;
			<p class="source-code">  region: us-central1</p>&#13;
			<p class="source-code">  runtimeVersion: '2.2'</p>&#13;
			<p class="source-code">  scaleTier: BASIC_TPU</p>&#13;
			<p class="source-code">trainingOutput: {}</p>&#13;
			<p>You can<a id="_idIndexMarker265"/> view job in Cloud Console at <a href="https://console.cloud.google.com/mlengine/jobs/traincloudtpu_tfk_resnet50?project=project1-190517">https://console.cloud.google.com/mlengine/jobs/traincloudtpu_tfk_resnet50?project=project1-190517</a>.</p>&#13;
			<p>You can view the logs at <a href="https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Ftraincloudtpu_tfk_resnet50&amp;project=project1-190517">https://console.cloud.google.com/logs?resource=ml_job%2Fjob_id%2Ftraincloudtpu_tfk_resnet50&amp;project=project1-190517</a>.</p>&#13;
			<p>As per the preceding code the preceding highlighted output, we can go to the logs URL in a browser and observe the training progress. The following are some example excerpts (s<a id="_idTextAnchor166"/>ee <em class="italic">Figure 5.13</em> and <em class="italic">Figure 5.14</em>):</p>&#13;
			<div>&#13;
				<div id="_idContainer100" class="IMG---Figure">&#13;
					<img src="Images/image025.jpg" alt="Figure 5.13 – Google Cloud AI Platform TPU training log example excerpt 1&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.13 – Google Cloud AI Platform TPU training log example excerpt 1</p>&#13;
			<p>This is a lengthy<a id="_idIndexMarker266"/> log that will run until the training job is complete. Toward the end of the training r<a id="_idTextAnchor167"/>un, the log will look like this:</p>&#13;
			<div>&#13;
				<div id="_idContainer101" class="IMG---Figure">&#13;
					<img src="Images/image027.jpg" alt="Figure 5.14 – Google Cloud AI Platform TPU training log example excerpt 2&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.14 – Google Cloud AI Platform TPU training log example excerpt 2</p>&#13;
			<p>In the storage <a id="_idIndexMarker267"/>bucket, we see the folder created by the <a id="_idTextAnchor168"/>training workflow (<em class="italic">Figure 5.15</em>):</p>&#13;
			<div>&#13;
				<div id="_idContainer102" class="IMG---Figure">&#13;
					<img src="Images/image029.jpg" alt="Figure 5.15 – Folder created by the TPU training workflow&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.15 – Folder created by the TPU training workflow</p>&#13;
			<p>Inside this bucket, we see the checkpoints <a id="_idTextAnchor169"/>and model assets (<em class="italic">Figure 5.16</em>):</p>&#13;
			<div>&#13;
				<div id="_idContainer103" class="IMG---Figure">&#13;
					<img src="Images/image031.jpg" alt="Figure 5.16 – Model checkpoints and assets after the training workflow is completed by the TPU&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.16 – Model checkpoints and assets after the training workflow is completed by the TPU</p>&#13;
			<p>It is exactly the<a id="_idIndexMarker268"/> same as if the training is done on a local standalone machine. The complete <strong class="source-inline">trainer.py</strong> is available at <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/cnn_on_tpu/custom_model_on_tpu/trainer.py</a>.</p>&#13;
			<p>Next, we are going to take a look at how to reuse what we have learned here. As it turns out, if we want to use a model available in TensorFlow Hub, we can reuse the training patterns, file organization, and workflow. However, a slight change is required. This is because currently, the TPU has no direct access to TensorFlow Hub's module URL. </p>&#13;
			<h2 id="_idParaDest-87"><a id="_idTextAnchor170"/>Working with models in TensorFlow Hub </h2>&#13;
			<p>TensorFlow Hub hosts a huge<a id="_idIndexMarker269"/> collection of pre-trained models. However, in order to use them, the user or client code must be able to connect to the hub and download the model via the RESTful API to the client's TensorFlow runtime. Currently, this cannot be done with the TPU. Therefore, we have to download the model we are interested in from TensorFlow Hub to our local computer, then upload it to cloud storage, where it can be accessed by the TPU. Typically, the following is how you would implement a pre-trained model from TensorFlow Hub using the <strong class="source-inline">tf.keras</strong> API:</p>&#13;
			<p class="source-code">m = tf.keras.Sequential([</p>&#13;
			<p class="source-code">    hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4', trainable=False),  </p>&#13;
			<p class="source-code">    tf.keras.layers.Dense(num_classes, activation='softmax')</p>&#13;
			<p class="source-code">])</p>&#13;
			<p class="source-code">m.build([None, 224, 224, 3])  # Batch input shape.</p>&#13;
			<p>As shown in the preceding lines of code, the URL to a pre-trained model is passed into <strong class="source-inline">KerasLayer</strong>. However, currently, the TPU running in Cloud AI Platform has no direct access to TensorFlow Hub's URL. To download the model, follow the simple instructions from TensorFlow Hub'<a id="_idTextAnchor171"/>s site, as shown in <em class="italic">Figure 5.17</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer104" class="IMG---Figure">&#13;
					<img src="Images/image033.jpg" alt="Figure 5.17 – Downloading a pre-trained model from TensorFlow Hub&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.17 – Downloading a pre-trained model from TensorFlow Hub</p>&#13;
			<p>The model is compressed. Once <a id="_idIndexMarker270"/>it is extracted, you will see the c<a id="_idTextAnchor172"/>ontent, as shown in <em class="italic">Figure 5.18</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer105" class="IMG---Figure">&#13;
					<img src="Images/image035.jpg" alt="Figure 5.18 – Downloaded pre-trained model from TensorFlow Hub&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.18 – Downloaded pre-trained model from TensorFlow Hub</p>&#13;
			<p>Once the model is downloaded and extracted, let's upload it to a storage bucket accessible by the TPU servi<a id="_idTextAnchor173"/>ce account, as in <em class="italic">Figure 5.19</em>:</p>&#13;
			<div>&#13;
				<div id="_idContainer106" class="IMG---Figure">&#13;
					<img src="Images/image037.jpg" alt="Figure 5.19 – Uploading the pre-trained model to cloud storage&#13;&#10;"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="figure-caption">Figure 5.19 – Uploading the pre-trained model to cloud storage</p>&#13;
			<p>Notice that we <a id="_idIndexMarker271"/>created a <strong class="source-inline">model-cache-dir</strong> folder in our storage bucket, then selected <strong class="bold">UPLOAD FOLDER</strong>, and the model folder is now available for the TPU to use.</p>&#13;
			<p>Then, inside the <strong class="source-inline">run</strong> function, we need to leverage an environmental variable to tell the TPU where to find this model: </p>&#13;
			<p class="source-code">os.environ['TFHUB_CACHE_DIR'] = 'gs://ai-tpu-experiment/model-cache-dir/imagenet_resnet_v2_50_feature_vector_4'</p>&#13;
			<p>This line can be inserted before the model definition in the <strong class="source-inline">run</strong> function. In the model definition, we will specify the model architecture by using <strong class="source-inline">hub.KerasLayer</strong>, as usual: </p>&#13;
			<p class="source-code">with strategy.scope():</p>&#13;
			<p class="source-code">        </p>&#13;
			<p class="source-code">  model = tf.keras.Sequential([</p>&#13;
			<p class="source-code">     tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),</p>&#13;
			<p class="source-code">     hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4',</p>&#13;
			<p class="source-code">                   trainable=flags_obj.fine_tuning_choice),</p>&#13;
			<p class="source-code">            tf.keras.layers.Dense(5, activation='softmax', </p>&#13;
			<p class="source-code">                                         name = 'custom_class')</p>&#13;
			<p class="source-code">        ])</p>&#13;
			<p>Because we have the <strong class="source-inline">TFHUB_CACHE_DIR</strong> environmental variable already defined with our cloud<a id="_idIndexMarker272"/> storage name and path, when the TPU executes the <strong class="source-inline">hub.KerasLayer</strong> part of the model architecture code, the TPU runtime will look for the model from <strong class="source-inline">TFHUB_CACHE_DIR</strong> first instead of attempting to go through a RESTful API call to retrieve the model. After these small modifications are made to the training script, we can rename it as <strong class="source-inline">trainer_hub.py</strong>. The training work can be launched with a similar invocation style:</p>&#13;
			<p class="source-code">vs_code % gcloud ai-platform jobs submit training traincloudtpu_tfhub_resnet50 \</p>&#13;
			<p class="source-code">--staging-bucket=gs://ai-tpu-experiment \</p>&#13;
			<p class="source-code">--package-path=python \</p>&#13;
			<p class="source-code">--module-name=python.ScriptProject.trainer_hub \</p>&#13;
			<p class="source-code">--runtime-version=2.2 \</p>&#13;
			<p class="source-code">--python-version=3.7 \</p>&#13;
			<p class="source-code">--scale-tier=BASIC_TPU \</p>&#13;
			<p class="source-code">--region=us-central1 \</p>&#13;
			<p class="source-code">-- \</p>&#13;
			<p class="source-code">--distribution_strategy=tpu \</p>&#13;
			<p class="source-code">--model_dir=gs://ai-tpu-experiment/traincloudtpu_tfhub_resnet50 \</p>&#13;
			<p class="source-code">--train_epochs=10 \</p>&#13;
			<p class="source-code">--data_dir=gs://ai-tpu-experiment/tfrecord-flowers</p>&#13;
			<p class="source-code">Job [traincloudtpu_tfhub_resnet50] submitted successfully.</p>&#13;
			<p>Your job is still active. You may view the status of your job with the command</p>&#13;
			<p class="source-code">  $ gcloud ai-platform jobs describe traincloudtpu_tfhub_resnet50</p>&#13;
			<p>or continue streaming the logs with the command</p>&#13;
			<p class="source-code">  $ gcloud ai-platform jobs stream-logs traincloudtpu_tfhub_resnet50</p>&#13;
			<p class="source-code">jobId: traincloudtpu_tfhub_resnet50</p>&#13;
			<p class="source-code">state: QUEUED</p>&#13;
			<p>The<a id="_idIndexMarker273"/> complete <strong class="source-inline">trainer_hub.py</strong> code is in <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/tfhub_on_tpu/tfhub_resnet_fv_on_tpu/trainer_hub.py">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/tfhub_on_tpu/tfhub_resnet_fv_on_tpu/trainer_hub.py</a>.</p>&#13;
			<p>Next, we are going to take a look at how to use the <strong class="source-inline">gcloud ai-platform</strong> command to leverage the GPU for a similar training job.</p>&#13;
			<h1 id="_idParaDest-88"><a id="_idTextAnchor174"/>Using the Google Cloud GPU through AI Platform</h1>&#13;
			<p>Having worked <a id="_idIndexMarker274"/>through the previous section for utilizing <a id="_idIndexMarker275"/>Cloud TPU with AI Platform, we are ready to do the same with the GPU. As it turns out, the formats of training script and invocation commands are very similar. With the exception of a few more parameters and slight differences in the distributed strategy definition, everything else remains the same. </p>&#13;
			<p>There are several distributed strategies (<a href="https://www.tensorflow.org/guide/distributed_training#types_of_strategies">https://www.tensorflow.org/guide/distributed_training#types_of_strategies</a>) currently available. For a TensorFlow Enterprise distribution in Google AI Platform, <strong class="source-inline">MirroredStrategy</strong> and <strong class="source-inline">TPUStrategy</strong> are the only two that are fully supported. All the others are experimental. Therefore, in this section's example, we will use <strong class="source-inline">MirroredStrategy</strong>. This strategy creates copies of all the variables in the model on each GPU. As these variables are updated at each gradient decent step, the value updates are copied to each GPU synchronously. By default, this strategy uses an <strong class="source-inline">NVIDIA NCCL</strong> all-reduce implementation. Now, we are going to start with the following steps:</p>&#13;
			<ol>&#13;
				<li value="1">We can start <a id="_idIndexMarker276"/>with small modifications to the TPU <a id="_idIndexMarker277"/>training script used in the previous section. Let's implement a condition for selecting a distributed strategy based on the choice of TPU or GPU: <p class="source-code">    if flags_obj.distribution_strategy == 'tpu':</p><p class="source-code">        resolver = tf.distribute.cluster_resolver.TPUClusterResolver()</p><p class="source-code">        tf.config.experimental_connect_to_cluster(resolver)</p><p class="source-code">        tf.tpu.experimental.initialize_tpu_system(resolver)</p><p class="source-code">        strategy = tf.distribute.experimental.TPUStrategy(resolver)</p><p class="source-code">        strategy_scope = strategy.scope()</p><p class="source-code">        print('All devices: ', tf.config.list_logical_devices('TPU'))</p><p class="source-code">    elif flags_obj.distribution_strategy == 'gpu': </p><p class="source-code">        devices = ['device:GPU:%d' % i for i in range(flags_obj.num_gpus)]</p><p class="source-code">        strategy = tf.distribute.MirroredStrategy(device=devices)</p><p class="source-code">        strategy_scope = strategy.scope()</p><p>The full implementation of the training script for using <strong class="source-inline">MirroredStrategy</strong> with the GPU can be found at <a href="https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/tfhub_on_gpu/tfhub_resnet_fv_on_gpu/trainer_hub_gpu.py">https://github.com/PacktPublishing/learn-tensorflow-enterprise/blob/master/chapter_05/tfhub_on_gpu/tfhub_resnet_fv_on_gpu/trainer_hub_gpu.py</a>.</p><p>For <strong class="source-inline">MirroredStrategy</strong>, we will set <strong class="source-inline">scale-tier</strong> to <strong class="source-inline">BASIC_GPU</strong>. This will give us a single <a id="_idIndexMarker278"/>worker instance with one NVIDIA Tesla K80<a id="_idIndexMarker279"/> GPU. The command to invoke training with <strong class="source-inline">trainer_hub_gpu.py</strong> is as follows:</p><p class="source-code">vs_code % gcloud ai-platform jobs submit training traincloudgpu_tfhub_resnet_gpu_1 \</p><p class="source-code">--staging-bucket=gs://ai-tpu-experiment \</p><p class="source-code">--package-path=python \</p><p class="source-code">--module-name=python.ScriptProject.trainer_hub \</p><p class="source-code">--runtime-version=2.2 \</p><p class="source-code">--python-version=3.7 \</p><p class="source-code">--scale-tier=BASIC_GPU \</p><p class="source-code">--region=us-central1 \</p><p class="source-code">-- \</p><p class="source-code">--distribution_strategy=gpu \</p><p class="source-code">--model_dir=gs://ai-tpu-experiment/traincloudgpu_tfhub_resnet_gpu_1 \</p><p class="source-code">--train_epochs=10 \</p><p class="source-code">--data_dir=gs://ai-tpu-experiment/tfrecord-flowers</p><p class="source-code">Job [traincloudtpu_tfhub_resnet_gpu_1] submitted successfully.</p></li>&#13;
			</ol>&#13;
			<p>Your job is still active. You may view the status of your job with the command</p>&#13;
			<p class="source-code">  $ gcloud ai-platform jobs describe traincloudgpu_tfhub_resnet_gpu_1</p>&#13;
			<p>or continue streaming the logs with the command</p>&#13;
			<p class="source-code">  $ gcloud ai-platform jobs stream-logs traincloudtpu_tfhub_resnet_gpu_1</p>&#13;
			<p class="source-code">jobId: traincloudgpu_tfhub_resnet_gpu_1</p>&#13;
			<p class="source-code">state: QUEUED</p>&#13;
			<p>Notice that we<a id="_idIndexMarker280"/> <a id="_idTextAnchor175"/>changed <strong class="source-inline">scale-tier</strong> to <strong class="source-inline">BASIC_GPU</strong>. We set our <a id="_idIndexMarker281"/>script-specific <strong class="source-inline">distribution_strategy</strong> flag to <strong class="source-inline">gpu</strong>. This is how we specify what compute instance we want and the distribution strategy.</p>&#13;
			<h1 id="_idParaDest-89"><a id="_idTextAnchor176"/>Summary </h1>&#13;
			<p>From all the examples that we have covered in this chapter, we learned how to leverage a distributed training strategy with the TPU and GPU through AI Platform, which runs on TensorFlow Enterprise 2.2 distributions. AI Platform is a service that wraps around TPU or GPU accelerator hardware and manages the configuration and setup for your training job. </p>&#13;
			<p>Currently, in Google AI Platform, the data ingestion pipeline relies on <strong class="source-inline">TFRecordDataset</strong> to stream training data in batches into the model training workflow. We also learned how to leverage a prebuilt model downloaded from TensorFlow Hub through the use of the <strong class="source-inline">TFHUB_CACHE_DIR</strong> environment variable. This is also the means to import your own saved model from an offline estate into Google AI Platform. Overall, in this platform, we used a TensorFlow Enterprise 2.2 distribution to achieve scalable data streaming and distributed training on Google Cloud's TPU or GPU and serialized all the model checkpoints and assets back to cloud storage. </p>&#13;
			<p>In the next chapter, we will use Cloud AI Platform to submit a hyperparameter tuning job. Hyperparameter tuning is typically very time-consuming and resource-intensive. We will see how to take advantage of the compute power of the Cloud TPU for this process.</p>&#13;
		</div>&#13;
	</div></body></html>