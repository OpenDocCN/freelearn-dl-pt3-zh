- en: Assessment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A replay buffer is required for off-policy RL algorithms. We sample from the
    replay buffer a mini-batch of experiences and use it to train the *Q(s,a)* state-value
    function in DQN and the actor's policy in a DDPG.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We discount rewards, as there is more uncertainty about the long-term performance
    of the agent. So, immediate rewards have a higher weight, a reward earned in the
    next time step has a relatively lower weight, a reward earned in the subsequent
    time step has an even lower weight, and so on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The training of the agent will not be stable if *γ > 1*. The agent will fail
    to learn an optimal policy.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A model-based RL agent has the potential to perform well, but there is no guarantee
    that it will perform better than a model-free RL agent, as the model of the environment
    we are constructing need not always be a good one. It is also very hard to build
    an accurate enough model of the environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In deep RL, deep neural networks are used for the *Q(s,a)* and the actor's policy
    (the latter is true in an Actor-Critic setting). In the traditional RL algorithms,
    a tabular *Q(s, a)* is used but is not possible when the number of states is very
    large, as is usually the case in most problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A replay buffer is used in DQN in order to store past experiences, sample a
    mini-batch of data from it, and use it to train the agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Target networks help in the stability of the training. This is achieved by keeping
    an additional neural network whose weights are updated using an exponential moving
    average of the weights of the main neural network. Alternatively, another approach
    that is also widely used is to copy the weights of the main neural network to
    the target network once every few thousand steps or so.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One frame as the state will not help in the Atari Breakout problem. This is
    because no temporal information is deductible from one frame only. For instance,
    in one frame alone, the direction of motion of the ball cannot be obtained. If,
    however, we stack up multiple frames, the velocity and acceleration of the ball
    can be ascertained.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: L2 loss is known to overfit to outliers. Hence, the Huber loss is preferred,
    as it combines both L2 and L1 losses. See Wikipedia: [https://en.wikipedia.org/wiki/Huber_loss](https://en.wikipedia.org/wiki/Huber_loss).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RGB images can also be used. However, we will need extra weights for the first
    hidden layer of the neural network, as we now have three channels in each of the
    four frames in the state stack. This much finer detail for the state space is
    not required for Atari. However, RGB images can help in other applications, for
    example, in autonomous driving and/or robotics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DQN is known to overestimate the state-action value function, *Q(s,a)*. To overcome
    this, DDQN was introduced. DDQN has fewer problems than DQN regarding the overestimation
    of *Q(s,a)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dueling network architecture has separate streams for the advantage function
    and the state-value function. These are then combined to obtain *Q(s,a)*. This
    branching out and then combining is observed to result in a more stable training
    of the RL agent.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Prioritized Experience Replay** (**PER**) gives more importance to experience
    samples where the agent performs poorly, and so these samples are sampled more
    frequently than other samples where the agent performed well. By frequently using
    samples where the agent performed poorly, the agent is able to work on its weakness
    more often, and so PER speeds up the training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In some computer games, such as Atari Breakout, the simulator has too many frames
    per second. If a separate action is sampled from the policy in each of these time
    steps, the state of the agent may not change enough in one time step, as it is
    too small. Hence, sticky actions are used where the same action is repeated over
    a finite but fixed number of time steps, say *n*, and the total reward accrued
    over these n time steps is used as the reward for the action performed. In these
    n time steps, the state of the agent has changed sufficiently enough to be able
    to evaluate the efficacy of the action taken. Too small a value for n can prevent
    the agent from learning a good policy; likewise, too large a value can also be
    a problem. You must choose the right number of time steps over which the same
    action is taken, and this depends on the simulator used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DDPG is an off-policy algorithm, as it uses a replay buffer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In general, the same number of hidden layers and the number of neurons per hidden
    layer is used for the actor and the critic, but this is not required. Note that
    the output layer will be different for the actor and the critic, with the actor
    having the number of outputs equal to the number of actions; the critic will have
    only one output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DDPG is used for continuous control, that is, when the actions are continuous
    and real-valued. Atari Breakout has discrete actions, and so DDPG is not suitable
    for Atari Breakout.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use the `relu` activation function, and so the biases are initialized to
    small positive values so that they fire at the beginning of the training and allow
    gradients to back-propagate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is an exercise. See [https://gym.openai.com/envs/InvertedDoublePendulum-v2/](https://gym.openai.com/envs/InvertedDoublePendulum-v2/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is also an exercise. Notice what happens to the learning when the number
    of neurons is decreased in the first layer sequentially. In general, information
    bottlenecks are observed not only in an RL setting, but for any DL problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Asynchronous Advantage Actor-Critic Agents** (**A3C**) is an on-policy algorithm,
    as we do not use a replay buffer to sample data from. However, a temporary buffer
    is used to collect immediate samples, which are used to train once, after which
    the buffer is emptied.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Shannon entropy term is used as a regularizer—the higher the entropy, the
    better the policy is.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When too many worker threads are used, the training can slow down and can crash,
    as memory is limited. If, however, you have access to a large cluster of processors,
    then using a large number of worker threads/processes helps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Softmax is used in the policy network to obtain probabilities of different actions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An advantage function is widely used, as it decreases the variance of the policy
    gradient. *Section 3* of the A3C paper ([https://arxiv.org/pdf/1602.01783.pdf](https://arxiv.org/pdf/1602.01783.pdf))
    has more regarding this.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is an exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Trust Region Policy Optimization** (**TRPO**) has an objective function and
    a constraint. It hence requires a second order optimization such as a conjugate
    gradient. SGD and Adam are not applicable in TRPO.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The entropy term helps in regularization. It allows the agent to explore more.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We clip the policy ratio to limit the amount by which one update step will change
    the policy. If this clipping parameter epsilon is large, the policy can change
    drastically in each update, which can result in a sub-optimal policy, as the agent's
    policy is noisier and has too many fluctuations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The action is bounded between a negative and a positive value, and so the `tanh`
    activation function is used for `mu`. For sigma, the `softplus` is used as sigma
    and is always positive. The `tanh` function cannot be used for sigma, as `tanh`
    can result in negative values for sigma, which is meaningless!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Reward shaping generally helps with the training. But if it is done poorly,
    it will not help with the training. You must ensure that the reward shaping is
    done to keep the `reward` function dense as well as in appropriate ranges.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No, reward shaping is used only in the training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chapter 8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TORCS is a continuous control problem. DQN works only for discrete actions,
    and so it cannot be used in TORCS.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The initialization is another initialization strategy; you can also use a random
    uniform initialization with the `min` and `max` values of the range specified;
    another approach is to sample from a Gaussian with a zero mean and a specified
    sigma value. The interested reader must try these different initializers and compare
    the agent's performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `abs()` function is used in the `reward` function, as we penalize lateral
    drift from the center equally on either side (left or right). The first term is
    the longitudinal speed, and so no `abs()` function is required.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Gaussian noise added to the actions for exploration can be tapered down
    with episode count, and this can result in smoother driving. Surely, there are
    many other tricks you can do!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: DDPG is off-policy, but **Proximal Policy Optimization** (**PPO**) is the on-policy
    RL algorithm. Hence, DDPG requires a replay buffer to store past-experience samples,
    but PPO does not require a reply buffer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
