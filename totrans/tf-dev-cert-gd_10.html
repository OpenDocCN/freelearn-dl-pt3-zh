<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer158">
<h1 class="chapter-number" id="_idParaDest-176"><a id="_idTextAnchor226"/>10</h1>
<h1 id="_idParaDest-177"><a id="_idTextAnchor227"/>Introduction to Natural Language Processing</h1>
<p><a id="_idTextAnchor228"/>Today, we are faced with a staggering amount of text data coming at us from all directions, from social media platforms to email communications, and from text messages to online reviews. This exponential growth in text data has led to a rapid growth in the development of text-based applications powered by advanced deep learning techniques, used to unlock insights from text data. We find ourselves in the dawn of a transformative era, powered by tech giants such as Google and Microsoft and revolutionary start-ups such as OpenAI and Anthropic. These visionaries are leading the charge in building powerful solutions capable of solving a myriad of text-based challenges, such as summarizing large volumes of documents, extracting sentiments from online platforms, and generating text for blog posts – the list of uses <span class="No-Break">is endless.</span></p>
<p>Real-world text data can be messy; it could be riddled with unwanted information such as punctuation marks, special characters, and common words that may not contribute significantly to the text’s meaning. Hence, we will kick off this chapter by looking at some basic text preprocessing steps to help transform text data into a more digestible form in preparation for modeling. Again, you may wonder, how do these machines learn to understand text? How do they make sense of words and sentences, or even grasp their semantic meaning or the context<a id="_idIndexMarker500"/> in which words are used? In this chapter, we will journey through the fundamentals of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>). We will explore concepts such as tokenization, which deals with how we segment text into individual words or terms (tokens). We will also explore the concept of word embeddings – here, we will see how they enable models to capture the meaning, context, and relationship <span class="No-Break">between words.</span></p>
<p>Then, we will put together all we have learned in this chapter to build a sentiment analysis model, using the Yelp Polarity dataset to classify customer reviews. As an interactive activity, we will examine how to visualize word embeddings in TensorFlow; this can be useful in gaining a snapshot of how our model understands and represents different words. We will also explore various techniques to improve the performance of our sentiment analysis classifier. By the end of this chapter, you will have a good foundational understanding of how to preprocess and model text data, as well as the skills required to tackle real-world <span class="No-Break">NLP problems.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li><span class="No-Break">Text preprocessing</span></li>
<li>Building a <span class="No-Break">sentiment classifier</span></li>
<li><span class="No-Break">Embedding visualization</span></li>
<li><span class="No-Break">Model improvement</span></li>
</ul>
<h1 id="_idParaDest-178"><a id="_idTextAnchor229"/>Text preprocessing</h1>
<p>NLP is an exciting<a id="_idIndexMarker501"/> and evolving field that lies at the intersection of computer science and linguistics. It empowers computers with the ability to understand, analyze, interpret, and generate text data. However, working with text data presents a unique set of challenges, one that differs from the tabular and image data we worked with in the earlier sections of this book. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em> gives us a high-level overview of some of the inherent challenges that text data presents. Let’s drill into them and see what and how they present issues to us when building deep learning models with <span class="No-Break">text data.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer151">
<img alt="Figure 10.1 – The challenges presented by text data" height="312" src="image/B18118_10_001.jpg" width="1595"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – The challenges presented by text data</p>
<p>Text data in its natural form is unstructured, and this is just the beginning of the uniqueness of this interesting type of data we will work with in this chapter. Let's illustrate some of the issues by looking at these two sentences – “<em class="italic">The house next to ours is beautiful</em>” versus “<em class="italic">Our neighbor’s house is one everyone in this area admires</em>.” Both phrases<a id="_idIndexMarker502"/> have a similar sentiment, yet they have different structures and varying lengths. To humans, this lack of structure and varying length is not a challenge, but when we work with deep learning models, this could pose a challenge. To address<a id="_idIndexMarker503"/> these challenges, we can consider ideas such as tokenization, which refers<a id="_idIndexMarker504"/> to the splitting of text data into smaller units called tokens. These tokens could be used to represent words, sub-words, or individual characters. To handle the varying length of text data in preparation for modeling, we will reuse an old trick that we applied when working with image data with CNNs – padding. By padding the sentences, we can ensure that our data (such as sentences or paragraphs) is of the same length. This uniformity makes our data more digestible for <span class="No-Break">our models.</span></p>
<p>Again, we may come across words with multiple meanings and decoding the meaning of these words largely depends on the context in which they are used. For example, if we have a sentence reading “<em class="italic">I will be at the bank</em>,” without additional context, it is difficult to tell whether <em class="italic">bank</em> refers to a financial bank or a riverbank. Words such as this add an extra layer of complexity when modeling text data. To handle this issue, we need to apply techniques that capture the essence of words and their surrounding words. A good example of such a technique is word embeddings. Word embeddings are powerful vector representations that can be used to capture the semantic meaning of words, by enabling words with a similar meaning or context to have <span class="No-Break">similar representations.</span></p>
<p>Other issues we could face when working with text data are typos, spelling variations, and noise. To tackle these issues, we can use noise-filtering techniques to filter out URLs, special characters, and other irrelevant entities when collecting data online. Let’s say we have a sample sentence – “<em class="italic">Max loves to play chess at the London country club, and he is the best golfer on our street</em>.” When we examine this sentence, we see that it contains common words such as <em class="italic">and</em>, <em class="italic">is</em>, and <em class="italic">the</em>. Although these words are needed for linguistic coherence, in some instances, they may not add semantic value. If this is the case, then we may want to remove these words to reduce the dimensionality of <span class="No-Break">our data.</span></p>
<p>Now that we’ve covered some foundational ideas around the challenges of text data, let’s see how to preprocess text data by looking at how we can extract and clean text data on machine learning from Wikipedia. Here, we will see how to apply TensorFlow to perform techniques such as tokenization, padding, and using word embeddings to extract meaning from text. To access the sample data, use this link: <a href="https://en.wikipedia.org/wiki/Machine_learning">https://en.wikipedia.org/wiki/Machine_learning</a>. <span class="No-Break">Let’s begin:</span></p>
<ol>
<li>We will start by importing the <span class="No-Break">necessary libraries:</span><pre class="source-code">
import requests</pre><pre class="source-code">
from bs4 import BeautifulSoup</pre><pre class="source-code">
import re</pre><pre class="source-code">
from tensorflow.keras.preprocessing.text import Tokeni<a id="_idTextAnchor230"/>zer</pre><p class="list-inset">We use these libraries<a id="_idIndexMarker505"/> to effectively fetch, preprocess, and tokenize web data, preparing it for modeling with neural networks. When we want to access data from the internet, the <strong class="source-inline">requests</strong> library can prove to be a useful tool, enabling us to streamline the process of retrieving information from web pages by making requests to web servers and fetching web data. The collected data is often in the HTML format, which isn’t in the best shape for us to feed into our models. This is where <strong class="source-inline">BeautifulSoup</strong> (an intuitive tool for parsing HTML and XML) comes into the picture, enabling us to easily navigate and access the content we need. To perform string manipulation, text cleaning, or extracting patterns, we can use the <strong class="source-inline">re</strong> module. We also import the <strong class="source-inline">Tokenizer</strong> class from TensorFlow’s Keras API, which enables us to perform tokenization, thus converting our data into a <span class="No-Break">model-friendly format.</span></p></li>
</ol>
<ol>
<li value="2">Then, we assign our variable to the web page we want to scrape; in this case, we are interested in scraping data from the Wikipedia page on <span class="No-Break">machine learning:</span><pre class="source-code">
# Define the URL of the page</pre><pre class="source-code">
url = "https://en.wikipedia.org/wiki/Machine_learning"</pre><pre class="source-code">
# Send a GET request to the webpage</pre><pre class="source-code">
response = requests.get(url)</pre><p class="list-inset">We use the <strong class="source-inline">GET</strong> method to retrieve data from the web server. The web server replies with the status code that tells us whether the request was a success or failure. It also returns other metadata along with the HTML content of the web page – in our case, the Wikipedia page. We save the server’s response to the <strong class="source-inline">GET</strong> request in the <span class="No-Break"><strong class="source-inline">response</strong></span><span class="No-Break"> object.</span></p></li>
<li>We use the <strong class="source-inline">BeautifulSoup</strong> class to do the heavy lifting of parsing the HTML content, which we access by <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">response.content</strong></span><span class="No-Break">:</span><pre class="source-code">
# Parse the HTML content of the page with BeautifulSoup</pre><pre class="source-code">
soup = BeautifulSoup(response.content, 'html.parser')</pre><p class="list-inset">Here, we convert the raw HTML contents contained in <strong class="source-inline">response.content</strong> into a digestible format by specifying <strong class="source-inline">html.parser</strong> and storing the result in the <span class="No-Break"><strong class="source-inline">soup</strong></span><span class="No-Break"> variable.</span></p></li>
<li>Now, let’s extract all the text contents<a id="_idIndexMarker506"/> in <span class="No-Break">a paragraph:</span><pre class="source-code">
# Extract the text from all paragraph tags on the page</pre><pre class="source-code">
passage = " ".join([</pre><pre class="source-code">
    p.text for p in soup.find_all('p')])</pre><p class="list-inset">We use <strong class="source-inline">soup.find_all('p')</strong> to extract all the paragraphs stored in the <strong class="source-inline">soup</strong> variable. Then, we apply the <strong class="source-inline">join</strong> method to combine them into a body of text in which each paragraph is repeated by a space, and then we store this text in the <span class="No-Break"><strong class="source-inline">passage</strong></span><span class="No-Break"> variable.</span></p></li>
<li>The next step is the removal of stopwords from our data. Stopwords are common words that may not contain useful information in a certain use case. Hence, we may want to remove them to help reduce the dimensionality of our data, especially for tasks where these high-frequency words offer little importance, such as information retrieval or document clustering. Here, it may be wise to remove stopwords to enable faster convergence and produce better categorization. Examples of stopwords are words such as “and,” “the,” “in,” <span class="No-Break">and “is”:</span><pre class="source-code">
# Define a simple list of stopwords</pre><pre class="source-code">
stopwords = ["i", "me", "my", "myself", "we", "our",</pre><pre class="source-code">
    "ours", "ourselves", "you", "your",</pre><pre class="source-code">
    "yours", "yourself", "yourselves", "he",</pre><pre class="source-code">
    "him", "his", "himself", "she", "her",</pre><pre class="source-code">
    "hers", "herself", "it", "its", "itself",</pre><pre class="source-code">
    "they", "them", "their", "theirs",</pre><pre class="source-code">
    "themselves", "what", "which", "who",</pre><pre class="source-code">
    "whom", "this", "that", "these", "those",</pre><pre class="source-code">
    "am", "is", "are", "was", "were", "be",</pre><pre class="source-code">
    "been", "being", "have", "has", "had",</pre><pre class="source-code">
    "having", "do", "does", "did", "doing",</pre><pre class="source-code">
    "a", "an", "the", "and", "but", "if",</pre><pre class="source-code">
    "or", "because", "as", "until", "while",</pre><pre class="source-code">
    "of", "at", "by", "for", "with", "about",</pre><pre class="source-code">
    "against", "between", "into", "through",</pre><pre class="source-code">
    "during", "before", "after", "above",</pre><pre class="source-code">
    "below", "to", "from", "up", "down",</pre><pre class="source-code">
    "in", "out", "on", "off", "over",</pre><pre class="source-code">
    "under", "again", "further", "then",</pre><pre class="source-code">
    "once", "here", "there", "when", "where",</pre><pre class="source-code">
    "why", "how", "all", "any", "both",</pre><pre class="source-code">
    "each", "few", "more", "most", "other",</pre><pre class="source-code">
    "some", "such", "no", "nor", "not",</pre><pre class="source-code">
    "only", "own", "same", "so", "than",</pre><pre class="source-code">
    "too", "very", "s", "t", "can", "will",</pre><pre class="source-code">
    "just", "don", "should", "now"]</pre><p class="list-inset">We have defined a list of stopwords. This way, we have the flexibility of adding words of our choice to this list. This approach can be useful when working on domain-specific projects in which you may want to extend your <span class="No-Break">stopword list.</span></p></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This step might not always be beneficial. In some NLP tasks, stopwords might contain useful information. For example, in text generation or machine translation, a model needs to generate/translate stopwords to produce <span class="No-Break">coherent sente<a id="_idTextAnchor231"/>nces.</span></p>
<ol>
<li value="6">Let’s convert the entire passage<a id="_idIndexMarker507"/> into lowercase. We do this to ensure words with the same semantic meaning are not represented differently – for example, “DOG” and “dog.” By ensuring all our data is in lowercase, we introduce uniformity to our dataset, removing the possibility of duplicate representation of the same word. To convert our text to lowercase, we use the <span class="No-Break">following code:</span><pre class="source-code">
passage = passage.lower()</pre><p class="list-inset">When we run the code, it converts all our text data <span class="No-Break">to lowercase.</span></p></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Converting a body of text to lowercase isn’t always the best solution. In fact, in some use cases, such as sentiment analysis, converting to lowercase may lead to information loss because capital letters are usually used to express <span class="No-Break">strong emotions.</span></p>
<ol>
<li value="7">Next, let’s remove the HTML tags, special characters, and stopwords from the passage we gathered from Wikipedia. To do this, we‘ll use the <span class="No-Break">following code:</span><pre class="source-code">
# Remove HTML tags using regex</pre><pre class="source-code">
passage = re.sub(r'&lt;[^&gt;]+&gt;', '', passage)</pre><pre class="source-code">
# Remove unwanted special characters</pre><pre class="source-code">
passage = re.sub('[^a-zA-Z\s]', '', passage)</pre><pre class="source-code">
# Remove stopwords</pre><pre class="source-code">
passage = ' '.join(word for word in passage.split() if word not in stopwords)</pre><pre class="source-code">
# Print the cleaned passage</pre><pre class="source-code">
print(passage[:500])  # print only first 500 characters for b<a id="_idTextAnchor232"/>revity</pre><p class="list-inset">In the first line of code, we remove the HTML tags, after which we remove unwanted special characters in our data. Then, we pass the passage through a stopword filter to check and remove words that are in the stopword list, after which we combine the remaining words into a passage, separated by spaces between them. We print the first 500 characters to get an idea of what our processed text <span class="No-Break">looks like.</span></p></li>
<li>Let's print the first 500 characters<a id="_idIndexMarker508"/> and compare that with the web page shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span><pre class="source-code">
machine learning ml field devoted understanding building methods let machines learn methods leverage data improve computer performance set tasks machine learning algorithms build model based sample data known training data order make predictions decisions without explicitly programmed machine learning algorithms used wide variety applications medicine email filtering speech recognition agriculture computer vision difficult unfeasible develop conventional algorithms perform needed tasks subset ma</pre><p class="list-inset">When we compare the output with the web page, we can see that our text is all in lowercase and all the stopwords, special characters, and HTML tags have <span class="No-Break">been removed.</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="Figure 10.2 – A screenshot of the Wikipedia page on machine learning" height="653" src="image/B18118_10_002.jpg" width="1219"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – A screenshot of the Wikipedia page on machine learning</p>
<p>Here, we explored some simple steps<a id="_idIndexMarker509"/> in preparing text data for modeling with neural networks. Now, let’s extend our learning by <span class="No-Break">exploring tokenization.</span></p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor233"/>Tokenization</h2>
<p>We have looked <a id="_idIndexMarker510"/>at some important ideas about how to preprocess<a id="_idIndexMarker511"/> real-world text data. Our next step is to map out a strategy to prepare our text model. To do this, let’s begin by examining the concept of tokenization, which entails breaking sentences into smaller units called tokens. Tokenization can be implemented at character, sub-word, word, or even sentence level. It is common to see a lot of word-level tokenizers; however, the choice of tokenizer to use largely depends on the <span class="No-Break">use case.</span></p>
<p>Let’s see how we can apply tokenization to a sentence with a sample. Let’s say we have this sentence – “<em class="italic">I like playing chess in my leisure time</em>.” Applying word-level tokenization will give us output such as <strong class="source-inline">["i", "like", "playing", "chess", "in", "my", "leisure", "time"]</strong>, while if we decide to use character level tokenization, we will have output such as <strong class="source-inline">['I', ' ', 'l', 'i', 'k', 'e', ' ', 'p', 'l', 'a', 'y', 'i', 'n', 'g', ' ', 'c', 'h', 'e', 's', 's', ' ', 'i', 'n', ' ', 'm', 'y', ' ', 'l', 'e', 'i', 's', 'u', 'r', 'e', ' ', 't', 'i', 'm', 'e']</strong>. In word-level tokenization, we split across each word, while in character-level tokenization, we split on each character. You can also see that wide spaces within the sentence are included when we use character-level tokenization. For subword and sentence-level tokenization, we split into subwords and sentences<a id="_idIndexMarker512"/> respectively. Now, let’s use TensorFlow to implement word-level and <span class="No-Break">character-level</span><span class="No-Break"><a id="_idIndexMarker513"/></span><span class="No-Break"> tokenization.</span></p>
<h3>Word-level to<a id="_idTextAnchor234"/>kenization</h3>
<p>Let’s see how<a id="_idIndexMarker514"/> to perform word-level tokenization<a id="_idIndexMarker515"/> <span class="No-Break">with TensorFlow:</span></p>
<ol>
<li>We will begin by importing the <span class="No-Break"><strong class="source-inline">Tokenizer</strong></span><span class="No-Break"> class:</span><pre class="source-code">
from tensorflow.keras.preprocessing.text import Tokenizer</pre></li>
<li>Then, we create a variable called text to hold our sample sentence (<strong class="source-inline">"Machine learning is fascinating. It is a field full of challenges!"</strong>). We create an instance of the <strong class="source-inline">Tokenizer</strong> class to handle the tokenization of our <span class="No-Break">sample sentence:</span><pre class="source-code">
text = "Machine learning is fascinating. It is a field full of challenges!"</pre><pre class="source-code">
# Define the tokenizer and fit it on the text</pre><pre class="source-code">
tokenizer = Tokenizer()</pre><pre class="source-code">
tokenizer.fit_on_<a id="_idTextAnchor235"/>texts([text])</pre><p class="list-inset">We can pass several parameters into the <strong class="source-inline">tokenizer</strong> class, depending on our use case. For instance, we can set the maximum number of words we want to keep by using <strong class="source-inline">num_words</strong>. Also, we may want to convert our entire text into lowercase; we can do this with the <strong class="source-inline">tokenizer</strong> class. However, if we don’t specify these parameters, TensorFlow will apply the default parameters. Then, we use the <strong class="source-inline">fit_on_text</strong> method to fit the tokenizer on our text data. The <strong class="source-inline">fit_on_text</strong> method goes through the input text<a id="_idIndexMarker516"/> and creates a vocabulary made <a id="_idIndexMarker517"/>up of unique words. It also counts the number of occurrences of each word in our input <span class="No-Break">text data.</span></p></li>
<li>To view the mapping of words to integer values, we use the <strong class="source-inline">word_index</strong> property of our <span class="No-Break"><strong class="source-inline">tokenizer</strong></span><span class="No-Break"> object:</span><pre class="source-code">
# Print out the word index to see how words are tokenized</pre><pre class="source-code">
print(tokenizer.word_index)</pre><p class="list-inset">When we print out the result, we can see that <strong class="source-inline">word_index</strong> returns a dictionary of key-value pairs, where each key-value pair corresponds to a unique word and its assigned integer index in the <span class="No-Break">tokenizer’s vocabulary:</span></p><pre class="source-code">
{'is': 1, 'machine': 2, 'learning': 3, 'fascinating': 4, 'it': 5, 'a': 6, 'field': 7, 'full': 8, 'of': 9, 'challenges': 10}</pre></li>
</ol>
<p>You can see that the exclamation mark in our sample sentence is gone and the word <strong class="source-inline">'is'</strong> is listed only once. Also, you can see that our indexing begins at <strong class="source-inline">1</strong> and not <strong class="source-inline">0</strong>, because <strong class="source-inline">0</strong> is reserved as a special<a id="_idIndexMarker518"/> token, which we will encounter shortly. Now, let’s also examine how to perform<a id="_idIndexMarker519"/> <span class="No-Break">character-level tokenization.</span></p>
<h3>Character-level tokenization</h3>
<p>In character-level tokenization, we split the text<a id="_idIndexMarker520"/> on each character<a id="_idIndexMarker521"/> within our sample text. To do this with TensorFlow, we slightly modify the code we used for <span class="No-Break">word-level tokenization:</span></p>
<pre class="source-code">
# Define the tokenizer and fit it on the text
tokenizer = Tokenizer(char_level=True)
tokenizer.fit_on_texts([text])
# Print out the character index to see how characters are tokenized
print(tokenizer.word_index)</pre>
<p>Here, we set the <strong class="source-inline">char_level</strong> argument to <strong class="source-inline">True</strong> when we create our <strong class="source-inline">tokenizer</strong> instance. When we do this, we can see that only unique characters in our text will be treated as <span class="No-Break">separate tokens:</span></p>
<pre class="source-code">
{' ': 1, 'i': 2, 'a': 3, 'n': 4, 'l': 5, 'e': 6, 's': 7,
    'f': 8, 'c': 9, 'g': 10, 'h': 11, 't': 12, 'm': 13,
    'r': 14, '.': 15, 'd': 16, 'u': 17, 'o': 18, '!': 19}</pre>
<p>Note that every unique character is represented in this scenario, including whitespaces (<strong class="source-inline">' '</strong>) with a token value of <strong class="source-inline">1</strong>, full stops (<strong class="source-inline">'.'</strong>) with a token value of <strong class="source-inline">15</strong>, and exclamations (<strong class="source-inline">'!'</strong>) with a token value <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">19</strong></span><span class="No-Break">.</span></p>
<p>Another type of tokenization we talked about is subwords. Subwords involve breaking down words into commonly occurring groups of characters – for example, “unhappiness” might be tokenized into [“un”, “happiness”]. Once the text is tokenized, each token can be transformed into a numerical<a id="_idIndexMarker522"/> representation, using one of the encoding methods<a id="_idIndexMarker523"/> that we will discuss in this chapter. Now, let’s look at another concept <span class="No-Break">called sequencing.</span></p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor236"/>Sequencing</h2>
<p>The order in which words are used<a id="_idIndexMarker524"/> in a sentence is crucial<a id="_idIndexMarker525"/> to understanding the meaning they convey; sequencing is the process of converting sentences or a group of words or tokens into their numerical representations, preserving the sequential order of words when building NLP applications using neural networks. In TensorFlow, we can use the <strong class="source-inline">texts_to_sequences</strong> function to convert our tokenized text into a sequence of integers. From the output of our word-level tokenization step, we now know that our sample sentence (<strong class="source-inline">"Machine learning is fascinating. It is a field full of challenges!"</strong>) can be represented by a list <span class="No-Break">of tokens:</span></p>
<pre class="source-code">
# Convert the text to sequences
sequence = tokenizer.texts_to_sequences([text])
print(sequence)</pre>
<p>By converting text into sequences, we translate human-readable text into a machine-readable format while preserving the order in which words occur. When we print the result, we get <span class="No-Break">the following:</span></p>
<pre class="source-code">
[[2, 3, 1, 4, 5, 1, 6, 7, 8, 9, 10]]</pre>
<p>The output printed is the sequence of integers that represent the original text. In many real-world scenarios, we will have to handle sentences of varying lengths. While it is not a problem for humans to understand sentences irrespective of their length, neural networks require us to put our data in a defined type of input format. In the image classification section of this book, we used a fixed width and height when passing image data as input; with text data, we have to ensure that all our sentences are of the same length. To do this, let’s return<a id="_idIndexMarker526"/> to a concept we discussed in <a href="B18118_07.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 7</em></span></a><em class="italic">, </em><em class="italic">Image Classification with Convolutional Neural Networks,</em> padding, and see how we can leverage<a id="_idIndexMarker527"/> it to resolve the issue of varying <span class="No-Break">sentence lengths.</span></p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor237"/>Padding</h2>
<p>In <a href="B18118_07.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Image Classification with Convolutional Neural Networks,</em> we introduced<a id="_idIndexMarker528"/> the concept of padding<a id="_idIndexMarker529"/> when we discussed CNNs. In the context of NLP, padding is the process of adding elements to a sequence to ensure that it attains a desired length. To do this in TensorFlow, we use the <strong class="source-inline">pad_sequences</strong> function from the <strong class="source-inline">keras</strong> preprocessing module. Let’s use an example to explain the application of padding to <span class="No-Break">text data:</span></p>
<ol>
<li>Let’s say we have the following <span class="No-Break">four sentences:</span><pre class="source-code">
sentences = [</pre><pre class="source-code">
    "I love reading books.",</pre><pre class="source-code">
    "The cat sat on the mat.",</pre><pre class="source-code">
    "It's a beautiful day outside!",</pre><pre class="source-code">
    "Have you done your homework?"</pre><pre class="source-code">
]</pre><p class="list-inset">When we perform word-level tokenization and sequencing, the output will look <span class="No-Break">like this:</span></p><pre class="source-code">
[[2, 3, 4, 5], [1, 6, 7, 8, 1, 9],</pre><pre class="source-code">
    [10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]</pre><p class="list-inset">We can see that the length of our returned sequences varies with the second sentence longer than the other sentences. Let’s resolve this issue using <span class="No-Break">padding next.</span></p></li>
<li>We import <strong class="source-inline">pad_sequences</strong> from the TensorFlow Keras <span class="No-Break">preprocessing module:</span><pre class="source-code">
from tensorflow.keras.preprocessing.sequence import pad_sequences</pre><p class="list-inset">The <strong class="source-inline">pad_sequences</strong> function takes various parameters – here, we will discuss a few important ones, such as <strong class="source-inline">sequences</strong>, <strong class="source-inline">maxlen</strong>, <strong class="source-inline">truncating</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">padding</strong></span><span class="No-Break">.</span></p></li>
<li>Let’s start passing sequences as the only parameter and observe what the result <span class="No-Break">looks like:</span><pre class="source-code">
padded = pad_sequences(sequences)</pre><pre class="source-code">
print(padded)</pre><p class="list-inset">When we use the <strong class="source-inline">pad_sequence</strong> function, it ensures<a id="_idIndexMarker530"/> all the sentences<a id="_idIndexMarker531"/> are the same length as our longest sentence. To achieve this, a special token (<strong class="source-inline">0</strong>) is used to pad the shorter sentences until they are of the same length as the longest sentence. The special token (<strong class="source-inline">0</strong>) does not carry any meaning, and models are built to ignore them during training <span class="No-Break">and inference:</span></p><pre class="source-code">
[[ 0  0  2  3  4  5]</pre><pre class="source-code">
 [ 1  6  7  8  1  9]</pre><pre class="source-code">
 [ 0 10 11 12 13 14]</pre><pre class="source-code">
 [ 0 15 16 17 18 19]]</pre><p class="list-inset">From the output, we see that every other sentence has zeros added to it until its length is the same length as our longest sentence (sentence two), which has the longest sequence. Note that all the zeros are added at the beginning of each sentence. This scenario is known as <strong class="bold">prepadding</strong>. We add the zeros at the end of each sentence<a id="_idIndexMarker532"/> by using the <span class="No-Break"><strong class="source-inline">padding=post</strong></span><span class="No-Break"> parameter:</span></p><pre class="source-code">
# post padding</pre><pre class="source-code">
padded_sequences = pad_sequences(sequences,</pre><pre class="source-code">
    padding='post')</pre><pre class="source-code">
print(padded_sequences)</pre><p class="list-inset">When we print the result, we get <span class="No-Break">the following:</span></p><pre class="source-code">
[[ 2  3  4  5  0  0]</pre><pre class="source-code">
 [ 1  6  7  8  1  9]</pre><pre class="source-code">
 [10 11 12 13 14  0]</pre><pre class="source-code">
 [15 16 17 18 19  0]]</pre><p class="list-inset">In this case, we can see that the zeros<a id="_idIndexMarker533"/> are added at the end<a id="_idIndexMarker534"/> of <span class="No-Break">shorter sentences.</span></p></li>
<li>Another useful parameter is <strong class="source-inline">maxlen</strong>. It is used to specify the maximum length for all sequences we want to keep. In this case, any sequence greater than the specified <strong class="source-inline">maxlen</strong> will be truncated. To see how <strong class="source-inline">maxlen</strong> works, let’s add another sentence to our list <span class="No-Break">of sentences:</span><pre class="source-code">
Sentences = [</pre><pre class="source-code">
    "I love reading books.",</pre><pre class="source-code">
    "The cat sat on the mat.",</pre><pre class="source-code">
    "It's a beautiful day outside!",</pre><pre class="source-code">
    "Have you done your homework?",</pre><pre class="source-code">
    "Machine Learning is a very interesting subject that enables </pre><pre class="source-code">
     you build amazing solutions beyond your imagination."</pre><pre class="source-code">
]</pre></li>
<li>We take our new list of sentences and perform tokenization and sequencing on them. Then, we pad the numerical representations to ensure that our input data is of the same length, and to ensure that our special (<strong class="source-inline">0</strong>) tokens are added at the end of a sentence, we set <strong class="source-inline">padding</strong> to <strong class="source-inline">post</strong>. When we implement these steps, our output looks <span class="No-Break">like this:</span><pre class="source-code">
[[ 5  6  7  8  0  0  0  0  0  0  0  0  0  0  0  0]</pre><pre class="source-code">
 [ 1  9 10 11  1 12  0  0  0  0  0  0  0  0  0  0]</pre><pre class="source-code">
 [13  2 14 15 16  0  0  0  0  0  0  0  0  0  0  0]</pre><pre class="source-code">
 [17  3 18  4 19  0  0  0  0  0  0  0  0  0  0  0]</pre><pre class="source-code">
 [20 21 22  2 23 24 25 26 27  3 28 29 30 31  4 32]]</pre><p class="list-inset">From the output, we can see that one sentence<a id="_idIndexMarker535"/> is quite long, and the other sentences are largely made up of numerical representations <a id="_idIndexMarker536"/>with many zeros. In this case, it’s clear that our longest sentence is an outlier, since all the other sentences are much smaller. This can skew our model’s learning process and also increase the computation resource required to model our data, especially when we work with large datasets or limited computation resources. To fix this situation, we apply the <strong class="source-inline">maxlen</strong> parameter. It is important to use a good <strong class="source-inline">max_length</strong> value; otherwise, we could lose important information in our data due to truncation. It is a good idea to make the maximum length long enough to capture useful information without adding <span class="No-Break">much noise.</span></p></li>
<li>Let’s see how to apply <strong class="source-inline">maxlen</strong> in our example. We start by setting the <strong class="source-inline">max_length</strong> variable to <strong class="source-inline">10</strong>. This means it will take only a maximum of 10 tokens. We pass the <strong class="source-inline">maxlen</strong> parameter and print our <span class="No-Break">padded sequence:</span><pre class="source-code">
# Define the max length</pre><pre class="source-code">
max_length = 10</pre><pre class="source-code">
# Pad the sequences</pre><pre class="source-code">
padded = pad_sequences(sequences, padding='post',</pre><pre class="source-code">
    maxlen=max_length)</pre><pre class="source-code">
print(padded))</pre><p class="list-inset">Our result produces a much <span class="No-Break">shorter sequence:</span></p><pre class="source-code">
[[ 5  6  7  8  0  0  0  0  0  0]</pre><pre class="source-code">
 [ 1  9 10 11  1 12  0  0  0  0]</pre><pre class="source-code">
 [13  2 14 15 16  0  0  0  0  0]</pre><pre class="source-code">
 [17  3 18  4 19  0  0  0  0  0]</pre><pre class="source-code">
 [25 26 27  3 28 29 30 31  4 32]]</pre><p class="list-inset">Note that our longest sentence<a id="_idIndexMarker537"/> has been truncated at the beginning<a id="_idIndexMarker538"/> of the sequence. What if we want to truncate the sentence at the end? How do we achieve this? To do this, we introduce another parameter called <strong class="source-inline">truncating</strong> and set it <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">post</strong></span><span class="No-Break">:</span></p><pre class="source-code">
# Pad the sequences</pre><pre class="source-code">
padded = pad_sequences(sequences, padding='post',</pre><pre class="source-code">
    truncating='post', maxlen=max_length)</pre><pre class="source-code">
print(padded)</pre><p class="list-inset">Our result will look <span class="No-Break">like this:</span></p><pre class="source-code">
[[ 5  6  7  8  0  0  0  0  0  0]</pre><pre class="source-code">
 [ 1  9 10 11  1 12  0  0  0  0]</pre><pre class="source-code">
 [13  2 14 15 16  0  0  0  0  0]</pre><pre class="source-code">
 [17  3 18  4 19  0  0  0  0  <a id="_idTextAnchor238"/>0]</pre><pre class="source-code">
 [20 21 22  2 23 24 25 26 27  3]]</pre></li>
</ol>
<p>We now have all our sequences padded and truncating done at the end of the sentence. Now, what if we train our model to classify text using these five sentences, and we want to make a prediction on a new sentence (“<em class="italic">I love playing chess</em>”)? Remember from our training sentences that our model will have tokens to represent “I” and “love.” However, it has no way of knowing<a id="_idIndexMarker539"/> or representing “playing” and “Chess.” This presents us with another<a id="_idIndexMarker540"/> problem. Let’s look at how to <span class="No-Break">solve this.</span></p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor239"/>Out of vocabulary</h2>
<p>So far, we have seen<a id="_idIndexMarker541"/> how to prepare our data, moving from a sequence<a id="_idIndexMarker542"/> of text data that makes up sentences to numerical representations to train our models. Now, let’s say we build a text classification model, which we train using the five sentences in our sentence list. Of course, this is a hypothetical situation, which would hold true even when we train with a massive amount of text data, as we will eventually come across words that our model has not seen before in training, such as in this scenario with our sample test sentence (“<em class="italic">I love playing chess</em>”). This means we must prepare our model to handle words that are not present in our predefined vocabulary. To fix this issue, we use <strong class="bold">out-of-vocabulary</strong> (<strong class="bold">OOV</strong>) tokens, and this can be done by instantiating the <strong class="source-inline">oov_token="&lt;OOV&gt;"</strong> argument during the <span class="No-Break">tokenization process:</span></p>
<pre class="source-code">
# Define the tokenizer with an OOV token
tokenizer = Tokenizer(oov_token="&lt;OOV&gt;")
# Fit the tokenizer on the texts
tokenizer.fit_on_texts(sentences)
# Convert the texts to sequences
sequences = tokenizer.texts_to_sequences(sentences)
# Let's look at the word index
print(tokenizer.word_index)</pre>
<p>After this, we fit the tokenizer on the training sentences, converting the sentences to sequences of integers, and then we print out the <span class="No-Break">word index:</span></p>
<pre class="source-code">
{'&lt;OOV&gt;': 1, 'the': 2, 'a': 3, 'you': 4, 'your': 5, 'i': 6, 'love': 7, 'reading': 8, 'books': 9, 'cat': 10, 'sat': 11, 'on': 12, 'mat': 13, "it's": 14, 'beautiful': 15, 'day': 16, 'outside': 17, 'have': 18, 'done': 19, 'homework': 20, 'machine': 21, 'learning': 22, 'is': 23, 'very': 24, 'interesting': 25, 'subject': 26, 'that': 27, 'enables': 28, 'build': 29, 'amazing': 30, 'solutions': 31, 'beyond': 32, 'imagination': 33}</pre>
<p>Now, we can see that the <strong class="source-inline">"&lt;OOV&gt;"</strong> string is chosen to represent these OOV words and has a value of <strong class="source-inline">1</strong>. This token will take care of any unknown words that the model comes across. Let’s see this in action with our sample <span class="No-Break">test sentence:</span></p>
<pre class="source-code">
# Now let's convert a sentence with some OOV words
test_sentence = "I love playing chess"
test_sequence = tokenizer.texts_to_sequences(
    [test_sentence])
print(test_sequence)</pre>
<p>We pass in our test sentence (<strong class="source-inline">"I love playing chess"</strong>), which contains words our model has not seen before, and then use the <strong class="source-inline">texts_to_sequences</strong> method to convert the test sentence into a sequence. Because we fit the tokenizer on the training sentences, it will replace each word in the test sentence with its corresponding numerical representation from the word index. However, the words “playing” and “chess,” which were not present in our training sentences, will be replaced with the index of the special OOV token; hence, the <strong class="source-inline">print</strong> statement returns <span class="No-Break">the following:</span></p>
<pre class="source-code">
[[6, 7, 1, 1]]</pre>
<p>Here, the token value of <strong class="source-inline">1</strong> is used for the words <strong class="source-inline">playing</strong> and <strong class="source-inline">chess</strong>. Using the OOV token is a common practice in NLP to handle words that are not present in the training data but may appear in the test or <span class="No-Break">real-world data.</span></p>
<p>Now, we have our text data as a numerical representation. We have also preserved the sequence in which words occur; however, we need<a id="_idIndexMarker543"/> to find a way to capture the semantic meaning of the words<a id="_idIndexMarker544"/> and their relationships<a id="_idIndexMarker545"/> with each other. To do this, we use word embed<a id="_idTextAnchor240"/>dings. Let’s discuss word <span class="No-Break">embeddings next.</span></p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor241"/>Word embeddings</h2>
<p>A significant landmark<a id="_idIndexMarker546"/> in the field of NLP is the use of word embeddings. With word embeddings, we are able<a id="_idIndexMarker547"/> to solve many complex modern-day text-based problems. <strong class="bold">Word embeddings</strong> are a type of word representation that allows words with similar meanings to have a similar representation, with the ability to capture the context in which a word is used. Along with the context, word embedding is also able to capture the semantic and syntactic similarity between words and how a word relates to other words. This allows ML models to generalize better when using word embedding in comparison to instances where words are used as <span class="No-Break">standalone input.</span></p>
<p>Strategies such as one-hot encoding prove to be inefficient, as it builds a sparse representation of words largely made up of zeros. This happens because the more words we have in our vocabulary, the greater the number of zeros we will have in our resulting vector when we apply one-hot encoding. Conversely, word embedding is a dense vector representation in a continuous space that can capture the meaning, context, and relationship between words using dense and <span class="No-Break">low-dimensional vectors.</span></p>
<p>Let’s examine the following sample sentences and see how word <span class="No-Break">embedding works:</span></p>
<ol>
<li>She enjoys <span class="No-Break">reading books.</span></li>
<li>He likes <span class="No-Break">reading newspapers.</span></li>
<li>They are <span class="No-Break">eating grapes.</span></li>
</ol>
<p>We start by tokenizing each sentence and apply sequencing to transform each sentence into a sequence <span class="No-Break">of integers:</span></p>
<ol>
<li>[1, 2, <span class="No-Break">3, 4]</span></li>
<li>[5, 6, <span class="No-Break">3, 7]</span></li>
<li>[8, 9, <span class="No-Break">10, 11]</span></li>
</ol>
<p>Observe that with our returned sequence, we have successfully captured the order in which the words that make up each sentence occur. However, this approach fails to take into consideration the meaning of words or the relationship between words. For example, the words “enjoy” and “likes” both portray positive sentiments in sentence 1 and sentence 2, while both sentences have “reading” as their common action. When we design deep learning models, we want them to be aware that “books” and “newspapers” are more closely related and differ from words such as “grapes” and “eating,” as shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="Figure 10.3 – Word embedding" height="627" src="image/B18118_10_003.jpg" width="1190"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Word embedding</p>
<p>Word embeddings empower our models to capture relationships between words, thus enhancing our ability to build better-performing models. We have explored some foundational ideas around text preprocessing and data preparation, taking our text data from words to numerical representations, capturing both sequencing and the underlying relationships between words used in language. Let’s now put together everything we have learned and build a sentiment analysis model, using the Yelp Polarity dataset. We will start by training our own word embedding<a id="_idIndexMarker548"/> from scratch, after which we will apply pretrained word embedding<a id="_idTextAnchor242"/>s<a id="_idIndexMarker549"/> to our <span class="No-Break">use case.</span></p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor243"/>The Yelp Polarity dataset</h1>
<p>In this experiment, we will work<a id="_idIndexMarker550"/> with the Yelp Polarity dataset. This dataset is made up of a training size of 560,000 reviews and 38,000 reviews for testing, with each entry consisting of a text-based review and a label (positive – 1 and negative – 0). The data was drawn from customer reviews of restaurants, hair salons, locksmiths, and so on. This dataset presents some real challenges – for example, the reviews are made up of text with varying lengths, from short reviews to very long reviews. Also, the data contains the use of slang and different dialects. The dataset is available<a id="_idIndexMarker551"/> at this link:  <a href="https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews"><span class="No-Break">https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews</span></a><span class="No-Break">.</span></p>
<p>Let’s start building <span class="No-Break">our model:</span></p>
<ol>
<li>We will begin by loading<a id="_idIndexMarker552"/> the <span class="No-Break">required libraries:</span><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
import tensorflow_datasets as tfds</pre><pre class="source-code">
from tensorflow.keras.preprocessing.text import Tokenizer</pre><pre class="source-code">
from tensorflow.keras.preprocessing.sequence import pad_sequences</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
import io</pre><p class="list-inset">We import the necessary libraries to load, split, preprocess, and visualize word embeddings, and model our dataset wit<a id="_idTextAnchor244"/>h TensorFlow for our sentiment analysis <span class="No-Break">use case.</span></p></li>
<li>Then, we load <span class="No-Break">the dataset:</span><pre class="source-code">
# Load the Yelp Polarity Reviews dataset</pre><pre class="source-code">
(train_dataset, test_dataset),</pre><pre class="source-code">
    dataset_info = tfds.load('yelp_polarity_reviews',</pre><pre class="source-code">
        split=['train', 'test'], shuffle_files=True,</pre><pre class="source-code">
        with_info=True, as_supervised=True)</pre><p class="list-inset">We use the <strong class="source-inline">tf.load</strong> function to fetch datasets from TensorFlow datasets. Here, we specify our dataset, which is the Yelp Polarity reviews dataset. We also split our data into training and testing sets. We shuffle our data by setting shuffle to <strong class="source-inline">True</strong>, and we set <strong class="source-inline">with_info=True</strong> to ensure we can retrieve metadata of the dataset, which can be accessed using the <strong class="source-inline">dataset_info</strong> variable. We also set <strong class="source-inline">as_supervised=True</strong>; when we do this, it returns a tuple made up of the input and target rather than a dictionary. This way we can directly use the dataset with the <strong class="source-inline">fit</strong> method to train our models. We now have our training dataset as <strong class="source-inline">train_dataset</strong> and our testing set as <strong class="source-inline">test_dataset</strong>; both datasets are <strong class="source-inline">tf.data.Dataset</strong> objects. Let’s proceed with some quick data exploration before we build our sentiment analysis models on our training data and evaluate them on the <span class="No-Break">testing data.</span></p></li>
<li>Let’s write some functions<a id="_idIndexMarker553"/> to enable us to explore <span class="No-Break">our dataset:</span><pre class="source-code">
def get_reviews(dataset, num_samples=5):</pre><pre class="source-code">
    reviews = []</pre><pre class="source-code">
    for text, label in dataset.take(num_samples):</pre><pre class="source-code">
        reviews.append((text.numpy().decode('utf-8'), </pre><pre class="source-code">
            label.numpy()))</pre><pre class="source-code">
    return reviews</pre><pre class="source-code">
def dataset_insights(dataset, num_samples=2000):</pre><pre class="source-code">
    total_reviews = 0</pre><pre class="source-code">
    total_positive = 0</pre><pre class="source-code">
    total_negative = 0</pre><pre class="source-code">
    total_length = 0</pre><pre class="source-code">
    min_length = float('inf')</pre><pre class="source-code">
    max_length = 0</pre><pre class="source-code">
    for text, label in dataset.take(num_samples):</pre><pre class="source-code">
        total_reviews += 1</pre><pre class="source-code">
        review_length = len(text.numpy().decode(</pre><pre class="source-code">
            'utf-8').split())</pre><pre class="source-code">
        total_length += review_length</pre><pre class="source-code">
        if review_length &lt; min_length:</pre><pre class="source-code">
            min_length = review_length</pre><pre class="source-code">
        if review_length &gt; max_length:</pre><pre class="source-code">
            max_length = review_length</pre><pre class="source-code">
        if label.numpy() == 1:</pre><pre class="source-code">
            total_positive += 1</pre><pre class="source-code">
        else:</pre><pre class="source-code">
            total_negative += 1</pre><pre class="source-code">
    avg_length = total_length / total_reviews</pre><pre class="source-code">
    return min_length, max_length, avg_length,</pre><pre class="source-code">
        total_positive, total_negative</pre><pre class="source-code">
def plot_reviews(positive, negative):</pre><pre class="source-code">
    labels = ['Positive', 'Negative']</pre><pre class="source-code">
    counts = [positive, negative]</pre><pre class="source-code">
    plt.bar(labels, counts, color=['blue', 'red'])</pre><pre class="source-code">
    plt.xlabel('Review Type')</pre><pre class="source-code">
    plt.ylabel('Count')</pre><pre class="source-code">
    plt.title('Distribution of Reviews')</pre><pre class="source-code">
    plt.show()</pre><p class="list-inset">We use the <strong class="source-inline">get_reviews</strong> function to examine reviews from either the training or testing sets. This function displays the specified number of reviews and their corresponding labels; by default, it displays the first five reviews. However, we can set this parameter to any number we want. The second function is the <strong class="source-inline">dataset_insight</strong> function – this function performs several analyses, such as extracting the shortest, longest, and average length of reviews. It also generates the total count of positive and negative reviews in the dataset. Because we are working with a large dataset, we set <strong class="source-inline">dataset_insight</strong> to explore the first 2,000 samples. If you increase the number of samples, it will take a long time to analyze the data. We pass the total positive and negative count of reviews into the <strong class="source-inline">plot_reviews</strong> function to give us a graphical distribution of <span class="No-Break">the data.</span></p></li>
<li>Let’s check out the first seven<a id="_idIndexMarker554"/> reviews in our <span class="No-Break">training data:</span><pre class="source-code">
# Check out some reviews</pre><pre class="source-code">
print("Training Set Reviews:")</pre><pre class="source-code">
train_reviews = get_reviews(train_dataset, 7)</pre><pre class="source-code">
for review, label in train_reviews:</pre><pre class="source-code">
    print(f"Label: {label}, Review: {review[:100]}")</pre><p class="list-inset">When we run the code, it returns the top seven reviews. Also, we only return the first 100 characters of each review <span class="No-Break">for brevity:</span></p><pre class="source-code">
Training Set Reviews:</pre><pre class="source-code">
Label: 1, Review: The Groovy P. and I ventured to his old stomping grounds for lunch today.  The '5 and Diner' on 16th...</pre><pre class="source-code">
Label: 0, Review: Mediocre burgers - if you are in the area and want a fast food burger, Fatburger is  a better bet th...</pre><pre class="source-code">
Label: 0, Review: Not at all impressed...our server was not very happy to be there...food was very sub-par and it was ...</pre><pre class="source-code">
Label: 0, Review: I wish I would have read Megan P's review before I decided to cancel my dinner reservations because ...</pre><pre class="source-code">
Label: 1, Review: A large selection of food from all over the world. Great atmosphere and ambiance.  Quality of food i...</pre><pre class="source-code">
Label: 1, Review: I know, I know a review for Subway, come on.  But I have to say that the service at this subway is t...</pre><pre class="source-code">
Label: 1, Review: We came in for a pre-bachelor party madness meal and I have to say it was one of the best dining exp...</pre></li>
<li>Let’s examine some important statistics<a id="_idIndexMarker555"/> about our <span class="No-Break">training data:</span><pre class="source-code">
min_length, max_length, avg_length, total_positive, </pre><pre class="source-code">
    total_negative = dataset_insights(train_dataset)</pre><pre class="source-code">
# Display the results</pre><pre class="source-code">
print(f"Shortest Review Length: {min_length}")</pre><pre class="source-code">
print(f"Longest Review Length: {max_length}")</pre><pre class="source-code">
print(f"Average Review Length: {avg_length:.2f}")</pre><pre class="source-code">
print(f"Total Positive Reviews: {total_positive}")</pre><pre class="source-code">
print(f"Total Negative Reviews: {total_negative}")</pre><p class="list-inset">When we run the code, it returns <span class="No-Break">the following:</span></p><pre class="source-code">
Shortest Review Length: 1</pre><pre class="source-code">
Longest Review Length: 942</pre><pre class="source-code">
Average Review Length: 131.53</pre><pre class="source-code">
Total Positive Reviews: 1030</pre><pre class="source-code">
Total Negative Reviews: 970</pre></li>
<li>Let’s plot the distributions<a id="_idIndexMarker556"/> of our sampled <span class="No-Break">training data:</span><pre class="source-code">
plot_reviews(total_positive, total_negative)</pre><p class="list-inset">Here, we call the <strong class="source-inline">plot_reviews</strong> function and pass in the total number of positive and negative reviews from our sampled training data. When we run the code, we get the plot shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 10.4 – A distribution of reviews from our sampled training data" height="448" src="image/B18118_10_004.jpg" width="574"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – A distribution of reviews from our sampled training data</p>
<p class="list-inset">From the sampled training dataset, we see that our reviews are finely balanced. Therefore, we can proceed to train our model on this dataset. Let’s do <span class="No-Break">that now.</span></p>
<ol>
<li value="7">We define the key parameters for the tokenization, sequencing, and <span class="No-Break">training processes:</span><pre class="source-code">
# Define parameters</pre><pre class="source-code">
vocab_size = 10000</pre><pre class="source-code">
embedding_dim = 16</pre><pre class="source-code">
max_length = 132</pre><pre class="source-code">
trunc_type='post'</pre><pre class="source-code">
padding_type='post'</pre><pre class="source-code">
oov_tok = "&lt;OOV&gt;"</pre><pre class="source-code">
num_epochs = 10</pre><pre class="source-code">
# Build the Tokenizer</pre><pre class="source-code">
tokenizer = Tok<a id="_idTextAnchor245"/>enizer(num_words=vocab_size,</pre><pre class="source-code">
    oov_token=oov_tok)</pre><p class="list-inset">We set our vocabulary size to 10,000. This means the tokenizer will focus on the top 10,000 words in our dataset. When selecting the vocabulary size, there is a need to strike a balance between computational efficiency and capturing word diversity present within the dataset under consideration. If we increase the vocab size, we are likely to capture more nuances that can enrich our model’s understanding, but this will require more computational resources for training. Also, if we reduce the vocab size, training will be much faster; however, we will only capture a small portion of the linguistic variations present in <span class="No-Break">our dataset.</span></p><p class="list-inset">Next, we set the embedding<a id="_idIndexMarker557"/> dimension to 16. This means each word will be represented by a 16-dimensional vector. The choice of embedding dimension is usually based on empirical testing. Here, our choice of embedding dimension was based on computational efficiency. If we use higher dimensions such as 64 or 128, we are likely to capture more nuanced relationships between words; however, we will need more computational resources for training. When working with large datasets, you may wish to use higher dimensions for <span class="No-Break">better performance.</span></p><p class="list-inset">We set our max length to 132 words; we use this based on the average word length we got during our exploration of the first 2,000 reviews in our data. Reviews longer than 132 words will be truncated after the first 132 words are selected. Our choice of maximum length is to ensure we strike a decent compromise between computational efficiency and capturing the most important aspects of most of the reviews in our dataset. We set truncation and <strong class="source-inline">padding</strong> to <strong class="source-inline">post</strong>; this ensures that longer sentences are cut off at the end of a sequence and shorter sentences are padded with zeros at the end of the sequence. The key assumption here is that most of the important information we will find in a customer’s review is likely to be found at the beginning part of <span class="No-Break">the review.</span></p><p class="list-inset">Next, we set the OOV token<a id="_idIndexMarker558"/> to cater to OOV words that may occur in the test set but which the model did not see during training. Setting this parameter prevents our model from running into errors when handling unseen words. We also set the number of epochs that our model will train for to 10. Although we use 10 to test out our model, you may wish to train for longer and perhaps use callbacks to monitor the model’s performance during training on a <span class="No-Break">validation set.</span></p><p class="list-inset">With all our parameters defined, we can now instantiate our <strong class="source-inline">Tokenizer</strong> class, passing in <strong class="source-inline">num_words</strong> and <strong class="source-inline">oov_token</strong> <span class="No-Break">as arguments.</span></p></li>
<li>To reduce the processing time<a id="_idTextAnchor246"/>, we will make use of the 20,000 samples <span class="No-Break">for training:</span><pre class="source-code">
# Fetch and decode the training data</pre><pre class="source-code">
train_text = []</pre><pre class="source-code">
train_label = []</pre><pre class="source-code">
for example in train_dataset.take(20000):</pre><pre class="source-code">
    text, label = example</pre><pre class="source-code">
    train_text.append(text.numpy().decode('utf-8'))</pre><pre class="source-code">
    train_label.append(label.numpy())</pre><pre class="source-code">
# Convert labels to numpy array</pre><pre class="source-code">
train_labels = np.array(train_label)</pre><pre class="source-code">
# Fit the tokenizer on the training texts</pre><pre class="source-code">
tokenizer.fit_on_texts(train_text)</pre><pre class="source-code">
# Get the word index from the tokenizer</pre><pre class="source-code">
word_index = tokenizer.word_index</pre><pre class="source-code">
# Convert texts to sequences</pre><pre class="source-code">
train_seq<a id="_idTextAnchor247"/>uences = tokenizer.texts_to_sequences(</pre><pre class="source-code">
    train_text)</pre><p class="list-inset">Here, we train our model with the first 20,000 samples from the Yelp Polarity training dataset. We collect<a id="_idIndexMarker559"/> these reviews and their corresponding labels, and since the data is in the form of bytes, we use UTF-8 encoding to decode the string, after which we append the text and their labels to their respective lists. We convert the list of labels for easy manipulation using NumPy. After this, we fit the tokenizer on our selected training data and convert the text to <span class="No-Break">a sequence.</span></p></li>
<li>For testing purposes, we take 8,000 samples. The set of steps we carry out here is quite similar to those on the training set; however, we do not fit on text on the test set. This step is only for training purposes to help the neural network learn the word-to-index mapping in the <span class="No-Break">training set:</span><pre class="source-code">
# Fetch and decode the test data</pre><pre class="source-code">
test_text = []</pre><pre class="source-code">
test_label = []</pre><pre class="source-code">
for example in test_dataset.take(8000):</pre><pre class="source-code">
    text, label = example</pre><pre class="source-code">
    test_text.append(text.numpy().decode('utf-8'))</pre><pre class="source-code">
    test_label.append(label.numpy())</pre><pre class="source-code">
# Convert labels to numpy array</pre><pre class="source-code">
test_labels = np.array(test_label)</pre><pre class="source-code">
# Convert texts to sequences</pre><pre class="source-code">
test_s<a id="_idTextAnchor248"/>equences = tokenizer.texts_to_sequences(</pre><pre class="source-code">
    test_text)</pre><p class="list-inset">We take the first 8,000 samples from our test dataset. It is important to use the same tokenizer that was used to fit our training data here. This ensures that the word index mapping learned by the tokenizer during training is applied to the test set, and words not learned in the training set are replaced with the <span class="No-Break">OOV token.</span></p></li>
<li>The next step is to pad<a id="_idIndexMarker560"/> and truncate the sequences of integers representing the texts in the training and testing sets, ensuring that they all have the <span class="No-Break">same length:</span><pre class="source-code">
# Pad the sequences</pre><pre class="source-code">
train_padded = pad_sequences(train_sequences,</pre><pre class="source-code">
    maxlen=max_length, padding=padding_type,</pre><pre class="source-code">
    truncating=trunc_type)</pre><pre class="source-code">
test_padded = pad_sequences(test_sequences,</pre><pre class="source-code">
    maxlen=max_length, padding=padding_type,</pre><pre class="source-code">
    truncating=trunc_type)</pre><p class="list-inset">The output returned, <strong class="source-inline">train_padded</strong> and <strong class="source-inline">test_padded</strong>, is NumPy arrays of shape (<strong class="source-inline">num_sequences</strong> and <strong class="source-inline">maxlen</strong>). Now, every sequence that makes up these arrays is of the <span class="No-Break">same length.</span></p></li>
<li>We want to set up a validation<a id="_idIndexMarker561"/> set, which will help us track how our modeling process is going. To do this, we can use the <strong class="source-inline">train_test_split</strong> function <span class="No-Break">from scikit-learn:</span><pre class="source-code">
# Split the data into training and validation sets</pre><pre class="source-code">
train_padded, val_padded, train_labels,</pre><pre class="source-code">
    val_labels = train_test_split(train_padded,</pre><pre class="source-code">
        train_labels, test_size=0.2, random_state=42)</pre><p class="list-inset">Here, we split the data into training and validation sets, with 20 percent set as the <span class="No-Break">validation set.</span></p></li>
<li>Let’s proceed to build our sentiment <span class="No-Break">analysis model:</span><pre class="source-code">
# Define the model</pre><pre class="source-code">
model = tf.keras.Sequential([</pre><pre class="source-code">
    tf.keras.layers.Embedding(vocab_size,</pre><pre class="source-code">
        embedding_dim, input_length=max_length),</pre><pre class="source-code">
    tf.keras.layers.GlobalAveragePooling1D(),</pre><pre class="source-code">
    tf.keras.layers.Dense(24, activation='relu'),</pre><pre class="source-code">
    tf.keras.layers.Dense(1, activ<a id="_idTextAnchor249"/>ation='sigmoid')</pre><pre class="source-code">
# because it's binary classification</pre><pre class="source-code">
])</pre><p class="list-inset">We build our model with TensorFlow’s Keras API. Note<a id="_idIndexMarker562"/> that we have a new layer called the embedding layer, which is used to represent words in a dense vector space. This layer takes in the vocabulary size, the embedding dimension, and the max length as its parameters. In this experiment, we are training our word embeddings as a part of our model. We can also train this layer independently for the purpose of learning word embeddings. This can come in handy when we intend to use the same word embeddings across multiple models. In <a href="B18118_11.xhtml#_idTextAnchor267"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, <em class="italic">NLP with TensorFlow,</em> we will see how to apply a pretrained embedding layer from <span class="No-Break">TensorFlow Hub.</span></p><p class="list-inset">When we pass in a two-dimensional tensor<a id="_idIndexMarker563"/> of shape (<strong class="source-inline">batch_size</strong>, <strong class="source-inline">input_length</strong>), where each sample is an integer sequence, the embedding layer returns a three-dimensional tensor of shape (<strong class="source-inline">batch_size</strong>, <strong class="source-inline">input_length</strong>, <strong class="source-inline">embedding_dim</strong>). At the start of training, embedding vectors are randomly initialized. As we train the model, these vectors are adjusted, ensuring words with a similar context are clustered closely together within the embedding space. Instead of using discrete values, word embeddings make use of continuous values that our model can use to discern patterns and model intricate relationships with <span class="No-Break">the data.</span></p><p class="list-inset">The <strong class="source-inline">GlobalAveragePooling1D</strong> layer is applied to reduce the dimensionality of our data; it applies an average pooling operation. For example, if we apply <strong class="source-inline">GlobalAveragePooling1D</strong> to a sequence of words, it will return a summarized, single vector as output that can be fed into our fully connected layers for classification. Because we are performing binary classification, we use one neuron in our output layer and a sigmoid as our <span class="No-Break">activation function.</span></p></li>
<li>Now, we compile and fit our model. We pass in the loss as <strong class="source-inline">binary_crossentropy</strong> for the compilation step. We use Adam as our optimizer, and for our classification metric, we <span class="No-Break">use accuracy:</span><pre class="source-code">
# Compile the model</pre><pre class="source-code">
model.compile(loss='binary_crossentropy',</pre><pre class="source-code">
    optimizer='adam', metrics=['accuracy'])</pre></li>
<li>We fit the model for 10 epochs using our training data (<strong class="source-inline">train_padded</strong>) and labels (<strong class="source-inline">train_labels</strong>) and use the validation data to track <span class="No-Break">our experiment:</span><pre class="source-code">
# Train the model</pre><pre class="source-code">
history = model.fit(train_padded, train_labels,</pre><pre class="source-code">
    epochs=num_epochs, validation_data=(val_padded,</pre><pre class="source-code">
        val_labels))</pre><p class="list-inset">We report the results<a id="_idIndexMarker564"/> from the last <span class="No-Break">5 epochs:</span></p><pre class="source-code">
Epoch 6/10</pre><pre class="source-code">
625/625 [==============================] - 4s 7ms/step - loss: 0.1293 - accuracy: 0.9551 - val_loss: 0.3149 - val_accuracy: 0.8875</pre><pre class="source-code">
Epoch 7/10</pre><pre class="source-code">
625/625 [==============================] - 4s 6ms/step - loss: 0.1116 - accuracy: 0.9638 - val_loss: 0.3330 - val_accuracy: 0.8880</pre><pre class="source-code">
Epoch 8/10</pre><pre class="source-code">
625/625 [==============================] - 5s 9ms/step - loss: 0.0960 - accuracy: 0.9697 - val_loss: 0.3703 - val_accuracy: 0.8813</pre><pre class="source-code">
Epoch 9/10</pre><pre class="source-code">
625/625 [==============================] - 4s 6ms/step - loss: 0.0828 - accuracy: 0.9751 - val_loss: 0.3885 - val_accuracy: 0.8796</pre><pre class="source-code">
Epoch 10/10</pre><pre class="source-code">
625/625 [==============================] - 4s 6ms/step - loss: 0.0727 - accuracy: 0.9786 - val_loss: 0.4258 - val_accuracy: 0.8783</pre><p class="list-inset">The model reaches an accuracy of 0.9786 on training and a validation accuracy of 0.8783. This tells us there is an element of overfitting. Let’s see how our model will do on unseen data. To do this, let’s evaluate the model with our <span class="No-Break">test data.</span></p></li>
<li>We use the <strong class="source-inline">evaluate</strong> function<a id="_idIndexMarker565"/> to evaluate the <span class="No-Break">trained model:</span><pre class="source-code">
# Evaluate the model on the test set</pre><pre class="source-code">
results = model.evaluate(test_padded, test_labels)</pre><pre class="source-code">
print("Test Loss: ", results[0])</pre><pre class="source-code">
print("Test Accuracy: ", results[1])</pre><p class="list-inset">We pass in the test data (<strong class="source-inline">test_padded</strong>) and test labels (<strong class="source-inline">test_labels</strong>) and print <a id="_idTextAnchor250"/>out the loss and accuracy. The model reached an accuracy of 0.8783 on the <span class="No-Break">test set.</span></p></li>
<li>It is good practice to plot the loss and accuracy curves during training and validation, as it provides us with valuable insights into the learning process of the model and its performance. To do this, let’s construct a function <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">plot_history</strong></span><span class="No-Break">:</span><pre class="source-code">
def plot_history(history):</pre><pre class="source-code">
    plt.figure(figsize=(12, 4))</pre><pre class="source-code">
    # Plot training &amp; validation accuracy values</pre><pre class="source-code">
    plt.subplot(1, 2, 1)</pre><pre class="source-code">
    plt.plot(history.history['accuracy'])</pre><pre class="source-code">
    plt.plot(history.history['val_accuracy'])</pre><pre class="source-code">
    plt.title('Model accuracy')</pre><pre class="source-code">
    plt.ylabel('Accuracy')</pre><pre class="source-code">
    plt.xlabel('Epoch')</pre><pre class="source-code">
    plt.legend(['Train', 'Validation'],</pre><pre class="source-code">
        loc='upper left')</pre><pre class="source-code">
    # Plot training &amp; validation loss values</pre><pre class="source-code">
    plt.subplot(1, 2, 2)</pre><pre class="source-code">
    plt.plot(history.history['loss'])</pre><pre class="source-code">
    plt.plot(history.history['val_loss'])</pre><pre class="source-code">
    plt.title('Model loss')</pre><pre class="source-code">
    plt.ylabel('Loss')</pre><pre class="source-code">
    plt.xlabel('Epoch')</pre><pre class="source-code">
    plt.legend(['Train', 'Validation'],</pre><pre class="source-code">
        loc='upper right')</pre><pre class="source-code">
    plt.tight_layout()</pre><pre class="source-code">
    plt.show()</pre><p class="list-inset">This function takes in the <strong class="source-inline">history</strong> object and returns<a id="_idIndexMarker566"/> to us both the loss and accuracy curves. The <strong class="source-inline">plot_history</strong> function will create a figure with two subplots – the subplot on the left shows the training and validation accuracy per epoch, and the subplot on the right shows the training and validation loss <span class="No-Break">per epoch.</span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer155">
<img alt="Figure 10.5 – The loss and accuracy curves" height="384" src="image/B18118_10_005.jpg" width="1180"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – The loss and accuracy curves</p>
<p class="list-inset">From the plots in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.5</em>, we can see that the model’s accuracy on training increases per epoch; however, the validation accuracy begins to fall slightly around the end of the first epoch. The training loss also falls steadily per epoch, while the validation loss rises steadily over each epoch, thus <span class="No-Break">indicating overfitting.</span></p>
<ol>
<li value="17">Before we explore<a id="_idIndexMarker567"/> ways to fix overfitting in our case study, let’s try four new sentences and see how our model fares <span class="No-Break">on them:</span><pre class="source-code">
# New sentences</pre><pre class="source-code">
new_sentences = ["The restaurant was absolutely fantastic. The staff were kind and the food was delicious.",  # positive</pre><pre class="source-code">
    "I've had an incredible day at the beach, the weather was beautiful.",  # positive</pre><pre class="source-code">
    "The movie was a big disappointment. I wouldn't recommend it to anyone.",  # negative</pre><pre class="source-code">
    "I bought a new phone and it stopped working after a week. Terrible product."]  # negative</pre></li>
<li>In this example, we have the sentiments of each sentence indicated for reference. Let’s see how our trained model<a id="_idIndexMarker568"/> will perform on each of these <span class="No-Break">new sentences:</span><pre class="source-code">
# Preprocess the sentences in the same way as the training data</pre><pre class="source-code">
new_sequences = tokenizer.texts_to_sequences(</pre><pre class="source-code">
    new_sentences)</pre><pre class="source-code">
new_padded = pad_sequences(new_sequences,</pre><pre class="source-code">
    maxlen=max_length, padding=padding_type,</pre><pre class="source-code">
    truncating=trunc_type)</pre><pre class="source-code">
# Use the model to predict the sentiment of the new sentences</pre><pre class="source-code">
predictions = model.predict(new_padded)</pre><pre class="source-code">
# Print out the sequences and the corresponding predictions</pre><pre class="source-code">
for i in range(len(new_sentences)):</pre><pre class="source-code">
    print("Sequence:", new_sequences[i])</pre><pre class="source-code">
    print("Predicted sentiment (</pre><pre class="source-code">
        probability):", predictions[i])</pre><pre class="source-code">
    if predictions[i] &gt; 0.5:</pre><pre class="source-code">
        print("Interpretation: Positive sentiment")</pre><pre class="source-code">
    else:</pre><pre class="source-code">
        print("Interpretation: Negative sentiment")</pre><pre class="source-code">
    print("\n")</pre></li>
<li>Let’s print out the sequence corresponding to each sentence, along with the sentiment the <span class="No-Break">model predicted:</span><pre class="source-code">
1/1 [==============================] - 0s 21ms/step</pre><pre class="source-code">
Sequence: [2, 107, 7, 487, 533, 2, 123, 27, 290, 3, 2, 31, 7, 182]</pre><pre class="source-code">
Predicted sentiment (probability): [0.9689689]</pre><pre class="source-code">
Interpretation: Positive sentiment</pre><pre class="source-code">
Sequence: [112, 25, 60, 1251, 151, 26, 2, 3177, 2, 2079, 7, 634]</pre><pre class="source-code">
Predicted sentiment (probability): [0.9956489]</pre><pre class="source-code">
Interpretation: Positive sentiment</pre><pre class="source-code">
Sequence: [2, 1050, 7, 6, 221, 1174, 4, 454, 234, 9, 5, 528]</pre><pre class="source-code">
Predicted sentiment (probability): [0.43672907]</pre><pre class="source-code">
Interpretation: Negative sentiment</pre><pre class="source-code">
Sequence: [4, 764, 6, 161, 483, 3, 9, 695, 524, 83, 6, 393, 464, 1341]</pre><pre class="source-code">
Predicted sentiment (probability): [0.36306405]</pre><pre class="source-code">
Interpretation: Negative sentiment</pre></li>
</ol>
<p>Our sentiment analysis model<a id="_idIndexMarker569"/> was able to effectively predict the results correctly. What about if we want to visualize embedd<a href="https://projector.tensorflow.org">ings? TensorFlow has an embeddin</a>g projector, which can be accessed <span class="No-Break">at </span><a href="https://projector.tensorflow.org"><span class="No-Break">https://projector.tensorflow.org</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor251"/>Embedding visualization</h2>
<p>If we wish to visualize<a id="_idIndexMarker570"/> word embeddings from our trained model, we will need to extract the learned embeddings from the embedding layer and load them into the embedding projector provided by TensorBoard. Let’s examine how we can <span class="No-Break">do this:</span></p>
<ol>
<li>Extract the embedding <span class="No-Break">layer weights:</span><pre class="source-code">
weights = model.get_layer(</pre><pre class="source-code">
    'embedding').get_weights()[0]</pre><pre class="source-code">
vocab = tokenizer.word<a id="_idTextAnchor252"/>_index</pre><pre class="source-code">
print(weights.shape)</pre><pre class="source-code">
# shape: (vocab_size, embedding_dim)</pre><p class="list-inset">The first step is to retrieve the learned weights from our embedding layer after training. Next, we obtain the word index mapping that was generated during the tokenization process. If we apply a <strong class="source-inline">print</strong> statement, we can see the vocabulary size and the <span class="No-Break">embedding dimension.</span></p></li>
<li>Then, save the weights and vocabulary to disk. The TensorFlow Projector reads these file types and uses them to plot the vectors in 3D space, so we can <span class="No-Break">visualize them:</span><pre class="source-code">
out_v = io.open('vectors.tsv', 'w', encoding='utf-8')</pre><pre class="source-code">
out_m = io.open('metadata.tsv', 'w', encoding='utf-8')</pre><pre class="source-code">
for word, index in vocab.items():</pre><pre class="source-code">
    if index &lt; vocab_size:</pre><pre class="source-code">
        vec = weights[index]</pre><pre class="source-code">
        out_v.write('\t'.join([str(x) for x in vec]) + "\n")</pre><pre class="source-code">
        out_m.write(word + "\n")</pre><pre class="source-code">
out_v.close()</pre><pre class="source-code">
out_m.close()</pre></li>
<li>The next step is to save the embedding vectors<a id="_idIndexMarker571"/> and the vocabulary (words) as two separate <strong class="bold">tab-separated values</strong> (<strong class="bold">TSV</strong>) files called <strong class="source-inline">vecs.tsv</strong> and <strong class="source-inline">meta.tsv</strong>, respectively. When<a id="_idIndexMarker572"/> we run this code block, we see that we have two new files in our Google Colab notebook, as shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="Figure 10.6 – A screenshot showing the meta and vecs files" height="471" src="image/B18118_10_006.jpg" width="1286"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – A screenshot showing the meta and vecs files</p>
<ol>
<li value="4">Download the <span class="No-Break">files locally:</span><pre class="source-code">
try:</pre><pre class="source-code">
    from google.colab import files</pre><pre class="source-code">
    files.download('vectors.tsv')</pre><pre class="source-code">
    files.download('metadata.tsv')</pre><pre class="source-code">
except Exception:</pre><pre class="source-code">
    pass</pre><p class="list-inset">To download the required files from Google Colab to our local machine, run this code block. Note that you need to move these files from your server to your local machine if you work in a <span class="No-Break">cloud environment.</span><a href="https://projector.tensorflow.org/"/></p></li>
<li><a href="https://projector.tensorflow.org/">Visualize the embeddings. Open<span id="_idIndexMarker573"/> </a>the embedding projector <a id="_idIndexMarker574"/>using this link: <a href="https://projector.tensorflow.org/">https://projector.tensorflow.org/</a>. Then, you will have to click on the load button to load the <strong class="source-inline">vectors.tsv</strong> and <strong class="source-inline">metadata.tsv</strong> files you downloaded to your local machine. Once you successfully upload the files, the word embeddings will appear in 3D, as illustrated in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer157">
<img alt="Figure 10.7 – A snapshot of word embeddings" height="876" src="image/B18118_10_007.jpg" width="1460"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – A snapshot of word embe<a href="https://www.tensorflow.org/text/guide/word_embeddings?hl=en">ddings</a></p>
<p><a href="https://www.tensorflow.org/text/guide/word_embeddings?hl=en">To learn more about<span id="_idIndexMarker575"/> embedding visualization, see th</a>e documentation: <a href="https://www.tensorflow.org/text/guide/word_embeddings?hl=en">https://www.tensorflow.org/text/guide/word_embeddings?hl=en</a>. We have now seen how to visualize<a id="_idIndexMarker576"/> word embeddings. Now, let’s try to improve the performance of <span class="No-Break">our model.</span></p>
<h1 id="_idParaDest-186"><a id="_idTextAnchor253"/>Improving the performance of the model</h1>
<p>Earlier, we discussed some factors<a id="_idIndexMarker577"/> that we should consider as we designed our baseline architecture for sentiment analysis in this chapter. Also, in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Handling Overfitting,</em> we explored some foundational concepts to mitigate against overfitting. There, we looked at ideas such as early stopping and dropout regularization. To curb overfitting, let’s begin by tuning some of our model’s hyperparameters. To do this, let’s construct a function called <strong class="source-inline">sentiment_model</strong>. This function takes in three parameters – <strong class="source-inline">vocab_size</strong>, <strong class="source-inline">embedding_dim</strong>, and the size of the <span class="No-Break">training set.</span></p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor254"/>Increasing the size of the vocabulary</h2>
<p>One hyperparameter <a id="_idIndexMarker578"/>we may consider changing is the size of the vocabulary. Increasing the vocabulary size empowers the model to learn more unique words from our dataset. Let’s see how this will impact the performance of our base model. Here, we adjust <strong class="source-inline">vocab_size</strong> from <strong class="source-inline">10000</strong> to <strong class="source-inline">20000</strong>, while keeping the other <span class="No-Break">hyperparameters constant:</span></p>
<pre class="source-code">
# Increasing the vocab_size
vocab_size = 10000 #Change from 10000 to 20000
embedding_dim = 16
training_size = 20000
num_epochs=10
model_1, history_1 = sentiment_model(vocab_size,
    embedding_dim, training_size, num_epochs)</pre>
<p>The model reaches a test accuracy of 0.8749 compared to 0.8783, which was achieved by our base model. Here, increasing <strong class="source-inline">vocab_size</strong> had no positive impact on the performance of <span class="No-Break">our model.</span></p>
<p>When we use a larger vocabulary size, our model will learn more unique words, which could be a good idea, depending on the dataset and the use case. On the downside, more parameters and computational resource is required for us to efficiently train our model. Again, there is a greater risk <span class="No-Break">of overfitting.</span></p>
<p>In light of these issues, it is important to strike the right balance by ensuring we have a large enough <strong class="source-inline">vocab_size</strong> to capture the nuances in our data, without introducing the risk of overfitting at the same time. One strategy is to set a minimum frequency threshold, such that rare words that may lead to overfitting are excluded from our vocabulary. Another idea<a id="_idIndexMarker579"/> we can try is to adjust the dimensions of the embedding. Let’s discuss <span class="No-Break">that next.</span></p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor255"/>Adjusting the embedding dimension</h2>
<p>The embedding dimension<a id="_idIndexMarker580"/> refers to the size of the vector<a id="_idIndexMarker581"/> space in which words are represented. A high-dimension embedding<a id="_idIndexMarker582"/> has the ability to capture more nuanced relationships between words. However, it also increases the model complexity and may lead to overfitting, especially when working with small datasets. Let’s adjust <strong class="source-inline">embedding_dim</strong> from <strong class="source-inline">16</strong> to <strong class="source-inline">32</strong> while keeping other parameters constant and see what the impact will be on <span class="No-Break">our experiment:</span></p>
<pre class="source-code">
vocab_size = 10000
embedding_dim = 32 #Change from 16 to 32
train_size = 20000
num_epochs=10
model_2, history_2 = sentiment_model(vocab_size,
    embedding_dim, train_size, num_epochs)</pre>
<p>In 10 epochs, our new model with a larger embedding dimension reached an accuracy of 0.8720 on the test set. This falls short of our baseline model. Here, we see firsthand that an increase in the embedding dimension doesn’t always guarantee a better-performing model. When the embedding dimension is too small, it may fail to capture important relationships in our data. Conversely, an oversized embedding will lead to increased computation requirements and a greater risk of overfitting. It is important to note that a small embedding dimension suffices for simpler tasks or smaller datasets, while a larger embedding is an excellent choice for a large dataset. A pragmatic approach is to begin with a smaller embedding and gradually increase its size, while keeping an eye on the model’s performance during each iteration. Usually, the performance will improve, and at some point, diminishing returns will set in. When this happens, we stop training. Now, we can collect<a id="_idIndexMarker583"/> more data, increase the n<a id="_idTextAnchor256"/>umber of samples, and see what<a id="_idIndexMarker584"/> our results <span class="No-Break">look like.</span></p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor257"/>Collecting more data</h2>
<p>In <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Handling Overfitting,</em> we explored <a id="_idIndexMarker585"/>this option when handling overfitting. Collecting<a id="_idIndexMarker586"/> more data samples enables us to have a more diverse set of examples that our model can learn from. However, this process can be time-consuming. Also, more data may not help in cases where it is noisy or irrelevant. For our case study, let’s increase the training data size from 20,000 samples to <span class="No-Break">40,000 samples:</span></p>
<pre class="source-code">
vocab_size = 10000
embedding_dim = 16
train_size = 40000
model_3, history_3 = sentiment_model(vocab_size,
    embedding_dim, train_size)</pre>
<p>After 10 epochs of training, we see that our model’s performance improves to 0.8726 on our test set. Collecting more data can be a good strategy, as it may provide our model with a more diverse dataset; however, it didn’t work in this instance. So, let’s move on to other ideas; this time, let’s try <span class="No-Break">dropout regularization.</span></p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor258"/>Dropout regularization</h2>
<p>In <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Handling Overfitting,</em> we discussed dropout regularization, where<a id="_idIndexMarker587"/> we randomly dropped<a id="_idIndexMarker588"/> out a percentage of neurons from our model during training to break co-dependence among neurons in our model. Since we are dealing with a case of overfitting, let’s try out this technique. To implement dropout in our model, we can add a dropout layer, as <span class="No-Break">shown here:</span></p>
<pre class="source-code">
model_4  = tf.keras.Sequential([
    tf.keras.layers.Embedding(vocab_size, embedding_dim, 
    input_length=max_length),
    tf.keras.layers.GlobalAveragePooling1D(),
    tf.keras.layers.Dense(24, activation='relu'),
    tf.keras.layers.Dropout(0.5),
# Dropout layer with 50% dropout rate
    tf.keras.layers.Dense(1, activation='sigmoid')
])
# Compile the model
model_4.compile(loss='binary_crossentropy',
    optimizer='adam',metrics=['accuracy'])</pre>
<p>Here, we set our dropout rate to 50 percent, meaning we turn off half of the neurons during training. Our new model achieves an accuracy of 0.8830, which is marginally better than our baseline model. Dropout can help to enhance the robustness of our model by preventing co-dependence between neurons in it. However, we must apply dropout with caution. If we drop out too many neurons, our model becomes too simple and begins to underfit because it is unable to capture the underlying patterns in our data. Also, if we apply a low dropout value to our model, we may not achieve the desired regularization effect we hope for. It is a good<a id="_idIndexMarker589"/> idea to experiment with different dropout values to find the best <a id="_idIndexMarker590"/>balance between model complexity and generalization. Now, let’s try out <span class="No-Break">different optimizers.</span></p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor259"/>Trying a different optimizer</h2>
<p>While Adam is a good<a id="_idIndexMarker591"/> general-purpose<a id="_idIndexMarker592"/> optimizer, you might find that a different optimizer, such as SGD or RMSprop, works better for your specific task. Different optimizers might work better, depending on the task at hand. Let’s try out RMSprop for our use case and see how <span class="No-Break">it fares:</span></p>
<pre class="source-code">
# Initialize the optimizer
optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)
# Compile the model
model_7.compile(loss='binary_crossentropy',
    optimizer=optimizer, metrics=['accuracy'])
# Train the model
history_7 = model_7.fit(train_padded, train_labels,
    epochs=num_epochs, validation_data=(val_padded,
        val_labels))</pre>
<p>We achieved a test accuracy of 0.8920, beating our baseline model, using RMSprop as our optimizer. When choosing an appropriate optimizer, it is important to assess the key properties of your use case. For example, when working with large datasets, SGD is a more suitable choice than batch gradient descent, as SGD’s use of mini-batches reduces the computational cost. This attribute is useful when working with limited computation resources. It’s worth noting that if we have too many small batches while using SGD, it could lead to noisy updates; on the flip side, very large batch sizes could increase the <span class="No-Break">computational cost.</span></p>
<p>Adam is an excellent default optimizer <a id="_idIndexMarker593"/>for many deep learning<a id="_idIndexMarker594"/> use cases, as it combines the advantages of RMSprop and Momentum; however, when we deal with simple convex problems such as linear regression, SGD proves to be a better choice, due to Adam’s overcomp<a id="_idTextAnchor260"/>ensation in these scenarios. With this, we draw the curtain on <span class="No-Break">this chapter.</span></p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor261"/>Summary</h1>
<p>In this chapter, we explored the foundations of NLP. We began by looking at how to handle real-world text data, and we explored some preprocessing ideas, using tools such as Beautiful Soup, requests, and regular expressions. Then, we unpacked various ideas, such as tokenization, sequencing, and the use of word embedding to transform text data into vector representations, which not only preserved the sequential order of text data but also captured the relationships between words. We took a step further by building a sentiment analysis classifier using the Yelp Polarity dataset from the TensorFlow dataset. Finally, we performed a series of experiments with different hyperparameters in a bid to improve our base model’s performance and <span class="No-Break">overcome overfitting.</span></p>
<p>In the next chapter, we will introduce <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>) and see how they do things differently from the DNN we used in this chapter. We will put RNNs to the test as we will build a new classifier with them. We will also take things a step further by experimenting with pretrained embeddings, and finally, we will round off the NLP section by generating text in a fun exercise, using a dataset of children's stories. See <span class="No-Break">you there.</span></p>
<h1 id="_idParaDest-193"><a id="_idTextAnchor262"/>Questions</h1>
<p>Let’s test what we have learned in <span class="No-Break">this chapter.</span></p>
<ol>
<li>Using the test notebook, load the IMDB dataset <span class="No-Break">from TFDS.</span></li>
<li>Use a different embedding dimension and evaluate the model on the <span class="No-Break">test set.</span></li>
<li>Use a different vocabulary size and evaluate the model on the <span class="No-Break">test set.</span></li>
<li>Add more layers and evaluate the model on the <span class="No-Break">test set.</span></li>
<li>Use your best model to make predictions on the sample <span class="No-Break">sentences given.</span></li>
</ol>
<h1 id="_idParaDest-194"><a id="_idTextAnchor263"/>Further reading</h1>
<p>To learn more, you can check out the <span class="No-Break">following resources:</span></p>
<ul>
<li>Kapoor, A., Gulli, A. and Pal, S. (2020) <em class="italic">Deep Learning with TensorFlow and Keras, Third Edition</em>. Packt <span class="No-Break">Publishing Ltd.</span></li>
<li><em class="italic">Twitter Sentiment Classification using Distant Supervision</em> by Go et <span class="No-Break">al. (2009)</span></li>
<li><em class="italic">Embedding Projector: Interactive Visualization and Interpretation of Embeddings</em> by Smilkov et <span class="No-Break">al. (2016)</span></li>
<li><em class="italic">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</em> by Zhang et <span class="No-Break">al. (2016)</span></li>
</ul>
</div>
</div></body></html>