["```\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import models, transforms\n```", "```\nbatch_size = 50\n```", "```\n# training data\ntrain_data_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomVerticalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4821, 0.4465), (0.2470, \n    0.2435, 0.2616))\n])\n\ntrain_set = torchvision.datasets.CIFAR10(root='./data',\n                           train=True, download=True,\n                           transform=train_data_transform)\n\ntrain_loader = torch.utils.data.DataLoader(train_set,\n                           batch_size=batch_size,\n                           shuffle=True, num_workers=2)\n```", "```\nval_data_transform = transforms.Compose([\n    transforms.Resize(224),\n    transforms.ToTensor(),\n    transforms.Normalize((0.4914, 0.4821, 0.4465), (0.2470, 0.2435, \n    0.2616))\n])\n\nval_set = torchvision.datasets.CIFAR10(root='./data',\n                                  train=False, download=True,\n                                  transform=val_data_transform)\n\nval_order = torch.utils.data.DataLoader(val_set,\n                                  batch_size=batch_size,\n                                  shuffle=False, num_workers=2)\n```", "```\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n```", "```\ndef train_model(model, loss_function, optimizer, data_loader):\n    # set model to training mode\n    model.train()\n\n    current_loss = 0.0\n    current_acc = 0\n\n    # iterate over the training data\n    for i, (inputs, labels) in enumerate(data_loader):\n        # send the input/labels to the GPU\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(True):\n            # forward\n            outputs = model(inputs)\n            _, predictions = torch.max(outputs, 1)\n            loss = loss_function(outputs, labels)\n\n            # backward\n            loss.backward()\n            optimizer.step()\n\n        # statistics\n        current_loss += loss.item() * inputs.size(0)\n        current_acc += torch.sum(predictions == labels.data)\n\n    total_loss = current_loss / len(data_loader.dataset)\n    total_acc = current_acc.double() / len(data_loader.dataset)\n\n    print('Train Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, \n    total_acc))\n```", "```\ndef test_model(model, loss_function, data_loader):\n    # set model in evaluation mode\n    model.eval()\n\n    current_loss = 0.0\n    current_acc = 0\n\n    # iterate over  the validation data\n    for i, (inputs, labels) in enumerate(data_loader):\n        # send the input/labels to the GPU\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # forward\n        with torch.set_grad_enabled(False):\n            outputs = model(inputs)\n            _, predictions = torch.max(outputs, 1)\n            loss = loss_function(outputs, labels)\n\n        # statistics\n        current_loss += loss.item() * inputs.size(0)\n        current_acc += torch.sum(predictions == labels.data)\n\n    total_loss = current_loss / len(data_loader.dataset)\n    total_acc = current_acc.double() / len(data_loader.dataset)\n\n    print('Test Loss: {:.4f}; Accuracy: {:.4f}'.format(total_loss, \n    total_acc))\n\n    return total_loss, total_acc\n```", "```\ndef tl_feature_extractor(epochs=5):\n    # load the pretrained model\n    model = torchvision.models.resnet18(pretrained=True)\n\n    # exclude existing parameters from backward pass\n    # for performance\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # newly constructed layers have requires_grad=True by default\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, 10)\n\n    # transfer to GPU (if available)\n    model = model.to(device)\n\n    loss_function = nn.CrossEntropyLoss()\n\n    # only parameters of the final layer are being optimized\n    optimizer = optim.Adam(model.fc.parameters())\n\n    # train\n    test_acc = list()  # collect accuracy for plotting\n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch + 1, epochs))\n\n        train_model(model, loss_function, optimizer, train_loader)\n        _, acc = test_model(model, loss_function, val_order)\n        test_acc.append(acc)\n\n    plot_accuracy(test_acc)\n```", "```\ndef tl_fine_tuning(epochs=5):\n    # load the pretrained model\n    model = models.resnet18(pretrained=True)\n\n    # replace the last layer\n    num_features = model.fc.in_features\n    model.fc = nn.Linear(num_features, 10)\n\n    # transfer the model to the GPU\n    model = model.to(device)\n\n    # loss function\n    loss_function = nn.CrossEntropyLoss()\n\n    # We'll optimize all parameters\n    optimizer = optim.Adam(model.parameters())\n\n    # train\n    test_acc = list()  # collect accuracy for plotting\n    for epoch in range(epochs):\n        print('Epoch {}/{}'.format(epoch + 1, epochs))\n\n        train_model(model, loss_function, optimizer, train_loader)\n        _, acc = test_model(model, loss_function, val_order)\n        test_acc.append(acc)\n\n    plot_accuracy(test_acc)\n```", "```\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_datasets as tfds \n```", "```\nIMG_SIZE = 224\nBATCH_SIZE = 50\n\n```", "```\ndata, metadata = tfds.load('cifar10', with_info=True, as_supervised=True)\nraw_train, raw_test = data['train'].repeat(), data['test'].repeat()\n```", "```\ndef train_format_sample(image, label):\n    \"\"\"Transform data for training\"\"\"\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = (image / 127.5) - 1\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n\n    label = tf.one_hot(label, metadata.features['label'].num_classes)\n\n    return image, label\n\ndef test_format_sample(image, label):\n    \"\"\"Transform data for testing\"\"\"\n    image = tf.cast(image, tf.float32)\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    image = (image / 127.5) - 1\n\n    label = tf.one_hot(label, \n    metadata.features['label'].num_classes)\n\n    return image, label\n```", "```\n# assign transformers to raw data\ntrain_data = raw_train.map(train_format_sample)\ntest_data = raw_test.map(test_format_sample)\n\n# extract batches from the training set\ntrain_batches = train_data.shuffle(1000).batch(BATCH_SIZE)\ntest_batches = test_data.batch(BATCH_SIZE)\n```", "```\ndef build_fe_model():\n    # create the pretrained part of the network, excluding FC \n    layers\n    base_model = tf.keras.applications.ResNet50V2(input_shape=(IMG_SIZE,\n    IMG_SIZE, 3), include_top=False, weights='imagenet')\n\n    # exclude all model layers from training\n    base_model.trainable = False\n\n    # create new model as a combination of the pretrained net\n    # and one fully connected layer at the top\n    return tf.keras.Sequential([\n        base_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(\n            metadata.features['label'].num_classes,\n            activation='softmax')\n    ])\n```", "```\ndef build_ft_model():\n    # create the pretrained part of the network, excluding FC \n    layers\n    base_model = tf.keras.applications.ResNet50V2(input_shape=(IMG_SIZE, \n    IMG_SIZE, 3), include_top=False, weights='imagenet')\n\n    # Fine tune from this layer onwards\n    fine_tune_at = 100\n\n    # Freeze all the layers before the `fine_tune_at` layer\n    for layer in base_model.layers[:fine_tune_at]:\n        layer.trainable = False\n\n    # create new model as a combination of the pretrained net\n    # and one fully connected layer at the top\n    return tf.keras.Sequential([\n        base_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        tf.keras.layers.Dense(\n            metadata.features['label'].num_classes,\n            activation='softmax')\n    ])\n```", "```\ndef train_model(model, epochs=5):\n    # configure the model for training\n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    # train the model\n    history = model.fit(train_batches,\n                        epochs=epochs,\n                        steps_per_epoch=metadata.splits['train'].num_examples // BATCH_SIZE,\n                        validation_data=test_batches,\n                        validation_steps=metadata.splits['test'].num_examples // BATCH_SIZE,\n                        workers=4)\n\n    # plot accuracy\n    test_acc = history.history['val_accuracy']\n\n    plt.figure()\n    plt.plot(test_acc)\n    plt.xticks(\n        [i for i in range(0, len(test_acc))],\n        [i + 1 for i in range(0, len(test_acc))])\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.show()\n```"]