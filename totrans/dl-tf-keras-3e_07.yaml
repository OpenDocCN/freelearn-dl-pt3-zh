- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The book till now has focused on supervised learning and the models that learn
    via supervised learning. Starting from this chapter we will explore a less explored
    and more challenging area of unsupervised learning, self-supervised learning,
    and contrastive learning. In this chapter, we will delve deeper into some popular
    and useful unsupervised learning models. In contrast to supervised learning, where
    the training dataset consists of both the input and the desired labels, unsupervised
    learning deals with a case where the model is provided with only the input. The
    model learns the inherent input distribution by itself without any desired label
    guiding it. Clustering and dimensionality reduction are the two most commonly
    used unsupervised learning techniques. In this chapter, we will learn about different
    machine learning and neural network techniques for both. We will cover techniques
    required for clustering and dimensionality reduction, and go into the detail about
    Boltzmann machines, and finally, cover the implementation of the aforementioned
    techniques using TensorFlow. The concepts covered will be extended to build **Restricted
    Boltzmann Machines** (**RBMs**). The chapter will include:'
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Self-organizing maps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boltzmann machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RBMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp7](https://packt.link/dltfchp7).
  prefs: []
  type: TYPE_NORMAL
- en: Let us start with the most common and frequently used technique for dimensionality
    reduction, the principal component analysis method.
  prefs: []
  type: TYPE_NORMAL
- en: Principal component analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Principal component analysis** (**PCA**) is the most popular multivariate
    statistical technique for dimensionality reduction. It analyzes the training data
    consisting of several dependent variables, which are, in general, intercorrelated,
    and extracts important information from the training data in the form of a set
    of new orthogonal variables called principal components.'
  prefs: []
  type: TYPE_NORMAL
- en: We can perform PCA using two methods, either **eigen decomposition** or **singular
    value decomposition** (**SVD**).
  prefs: []
  type: TYPE_NORMAL
- en: 'PCA reduces the *n*-dimensional input data to *r*-dimensional input data, where
    *r<n*. In simple terms, PCA involves translating the origin and performing rotation
    of the axis such that one of the axes (principal axis) has the highest variance
    with data points. A reduced-dimensions dataset is obtained from the original dataset
    by performing this transformation and then dropping (removing) the orthogonal
    axes with low variance. Here, we employ the SVD method for PCA dimensionality
    reduction. Consider *X*, the *n*-dimensional data with *p* points, that is, *X*
    is a matrix of size *p × n*. From linear algebra we know that any real matrix
    can be decomposed using singular value decomposition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *U* and *V* are orthonormal matrices (that is, *U.U*^T *= V.V*^T *= 1*)
    of size *p × p* and *n × n* respectively. ![](img/B18331_07_002.png) is a diagonal
    matrix of size *p × n*. The *U* matrix is called the **left singular matrix**,
    and *V* the **right singular matrix**, and ![](img/B18331_07_002.png), the diagonal
    matrix, contains the singular values of *X* as its diagonal elements. Here we
    assume that the *X* matrix is centered. The columns of the *V* matrix are the
    principal components, and columns of ![](img/B18331_07_004.png) are the data transformed
    by principal components.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now to reduce the dimensions of the data from *n* to *k* (where *k < n*), we
    will select the first *k* columns of *U* and the upper-left *k × k* part of ![](img/B18331_07_002.png).
    The product of the two gives us our reduced-dimensions matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_006.png)'
  prefs: []
  type: TYPE_IMG
- en: The data *Y* obtained will be of reduced dimensions. Next, we implement PCA
    in TensorFlow 2.0.
  prefs: []
  type: TYPE_NORMAL
- en: PCA on the MNIST dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us now implement PCA in TensorFlow 2.0\. We will be definitely using TensorFlow;
    we will also need NumPy for some elementary matrix calculation, and Matplotlib,
    Matplotlib toolkits, and Seaborn for plotting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we load the MNIST dataset. Since we are doing dimension reduction using
    PCA, we do not need a test dataset or even labels; however, we are loading labels
    so that after reduction we can verify the PCA performance. PCA should cluster
    similar data points in one cluster; hence, if we see that the clusters formed
    using PCA are similar to our labels, it would indicate that our PCA works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we do PCA, we should preprocess the data. We first normalize it so that
    all data has values between 0 and 1, and then reshape the image from being a 28
    × 28 matrix to a 784-dimensional vector, and finally, center it by subtracting
    the mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our data is in the right format, we make use of TensorFlow’s powerful
    linear algebra (`linalg`) module to calculate the SVD of our training dataset.
    TensorFlow provides the function `svd()` defined in `tf.linalg` to perform this
    task. And then use the `diag` function to convert the sigma array (`s`, a list
    of singular values) to a diagonal matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This provides us with a diagonal matrix *s* of size 784 × 784; a left singular
    matrix *u* of size 60,000 × 784; and a right singular matrix *v* of size 784 ×
    784\. This is so because the argument `full_matrices` of the function `svd()`
    is by default set to `False`. As a result it does not generate the full *U* matrix
    (in this case, of size 60,000 × 60,000); instead, if input *X* is of size *m ×
    n*, it generates *U* of size *p = min(m,n)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The reduced-dimension data can now be generated by multiplying respective slices
    of *u* and *s*. We reduce our data from 784 to 3 dimensions; we can choose to
    reduce to any dimension less than 784, but we chose 3 here so that it is easier
    for us to visualize later. We make use of `tf.Tensor.getitem` to slice our matrices
    in the Pythonic way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'A comparison of the original and reduced data shape is done in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let us plot the data points in the three-dimensional space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '![Chart, scatter chart, surface chart  Description automatically generated](img/B18331_07_01.png)Figure
    7.1: Scatter plot of MNIST dataset after dimensionality reduction using PCA'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that the points corresponding to the same color and, hence, the
    same label are clustered together. We have therefore successfully used PCA to
    reduce the dimensions of MNIST images. Each original image was of size 28 × 28\.
    Using the PCA method we can reduce it to a smaller size. Normally for image data,
    dimensionality reduction is necessary. This is because images are large in size
    and contain a significant amount of redundant data.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Embedding API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TensorFlow also offers an Embedding API where one can find and visualize PCA
    and tSNE [1] clusters using TensorBoard. You can see the live PCA on MNIST images
    here: [http://projector.tensorflow.org](http://projector.tensorflow.org). The
    following image is reproduced for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_07_02.png)Figure
    7.2: A visualization of a principal component analysis, applied to the MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can process your data using TensorBoard. It contains a tool called **Embedding
    Projector** that allows one to interactively visualize embedding. The Embedding
    Projector tool has three panels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Panel**: It is located at the top left, and you can choose the data,
    labels, and so on in this panel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Projections Panel**: Available at the bottom left, you can choose the type
    of projections you want here. It offers three choices: PCA, t-SNE, and custom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inspector Panel**: On the right-hand side, here you can search for particular
    points and see a list of nearest neighbors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Graphical user interface, chart, scatter chart  Description automatically
    generated](img/B18331_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: Screenshot of the Embedding Projector tool'
  prefs: []
  type: TYPE_NORMAL
- en: PCA is a useful tool for visualizing datasets and for finding linear relationships
    between variables. It can also be used for clustering, outlier detection, and
    feature selection. Next, we will learn about the k-means algorithm, a method for
    clustering data.
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'K-means clustering, as the name suggests, is a technique to cluster data, that
    is, to partition data into a specified number of data points. It is an unsupervised
    learning technique. It works by identifying patterns in the given data. Remember
    the sorting hat of Harry Potter fame? What it is doing in the book is clustering—dividing
    new (unlabelled) students into four different clusters: Gryffindor, Ravenclaw,
    Hufflepuff, and Slytherin.'
  prefs: []
  type: TYPE_NORMAL
- en: Humans are very good at grouping objects together; clustering algorithms try
    to give a similar capability to computers. There are many clustering techniques
    available, such as hierarchical, Bayesian, or partitional. K-means clustering
    belongs to partitional clustering; it partitions data into *k* clusters. Each
    cluster has a center, called the centroid. The number of clusters *k* has to be
    specified by the user.
  prefs: []
  type: TYPE_NORMAL
- en: 'The k-means algorithm works in the following manner:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly choose *k* data points as the initial centroids (cluster centers).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Assign each data point to the closest centroid; there can be different measures
    to find closeness, the most common being the Euclidean distance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recompute the centroids using current cluster membership, such that the sum
    of squared distances decreases.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the last two steps until convergence is met.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the previous TensorFlow versions, the `KMeans` class was implemented in the
    `Contrib` module; however, the class is no longer available in TensorFlow 2.0\.
    Here we will instead use the advanced mathematical functions provided in TensorFlow
    2.0 to implement k-means clustering.
  prefs: []
  type: TYPE_NORMAL
- en: K-means in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To demonstrate k-means in TensorFlow, we will use randomly generated data in
    the code that follows. Our randomly generated data will contain 200 samples, and
    we will divide them into three clusters. We start by importing all the required
    modules, defining the variables, and determining the number of sample points (`points_n`),
    the number of clusters to be formed (`clusters_n`), and the number of iterations
    we will be doing (`iteration_n`). We also set the seed for a random number to
    ensure that our work is reproducible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we randomly generate data and from the data select three centroids randomly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now plot the points:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the scatter plot of all the points and the randomly selected three
    centroids in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Randomly generated data, from three randomly selected centroids,
    plotted'
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the function `closest_centroids()` to assign each point to the centroid
    it is closest to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We create another function `move_centroids()`. It recalculates the centroids
    such that the sum of squared distances decreases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we call these two functions iteratively for 100 iterations. We have chosen
    the number of iterations arbitrarily; you can increase and decrease it to see
    the effect:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us now visualize how the centroids have changed after 100 iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In *Figure 7.5*, you can see the final centroids after 100 iterations. We have
    also colored the points based on which centroid they are closest to. The yellow
    points correspond to one cluster (nearest the cross in its center), and the same
    is true for the purple and green cluster points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: Plot of the final centroids after 100 iterations'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the `plot` command works in `Matplotlib 3.1.1` or higher versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding code, we decided to limit the number of clusters to three,
    but in most cases with unlabelled data, one is never sure how many clusters exist.
    One can determine the optimal number of clusters using the elbow method. The method
    is based on the principle that we should choose the cluster number that reduces
    the **sum of squared error** (**SSE**) distance. If *k* is the number of clusters,
    then as *k* increases, the SSE decreases, with SSE = 0; when *k* is equal to the
    number of data points, each point is its own cluster. It is clear we do not want
    this as our number of clusters, so when we plot the graph between SSE and the
    number of clusters, we should see a kink in the graph, like the elbow of the hand,
    which is how the method gets its name – the elbow method. The following code calculates
    the sum of squared errors for our data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us use the elbow method now for finding the optimum number of clusters
    for our dataset. To do that we will start with one cluster, that is, all points
    belonging to a single cluster, and increase the number of clusters sequentially.
    In the code, we increase the clusters by one, with eleven being the maximum number
    of clusters. For each cluster number value, we use the code above to find the
    centroids (and hence the clusters) and find the SSE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.6* shows the different cluster values for the dataset. The kink is
    clearly visible when the number of clusters is four:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Plotting SSE against the number of clusters'
  prefs: []
  type: TYPE_NORMAL
- en: K-means clustering is very popular because it is fast, simple, and robust. It
    also has some disadvantages, the biggest being that the user has to specify the
    number of clusters. Second, the algorithm does not guarantee global optima; the
    results can change if the initial randomly chosen centroids change. Third, it
    is very sensitive to outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Variations in k-means
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the original k-means algorithm each point belongs to a specific cluster (centroid);
    this is called **hard clustering**. However, we can have one point belong to all
    the clusters, with a membership function defining how much it belongs to a particular
    cluster (centroid). This is called *fuzzy clustering* or *soft clustering*.
  prefs: []
  type: TYPE_NORMAL
- en: This variation was proposed in 1973 by J. C. Dunn and later improved upon by
    J. C. Bezdek in 1981\. Though soft clustering takes longer to converge, it can
    be useful when a point is in multiple classes, or when we want to know how similar
    a given point is to different clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The accelerated k-means algorithm was created in 2003 by Charles Elkan. He exploited
    the triangle inequality relationship (that is, a straight line is the shortest
    distance between two points). Instead of just doing all distance calculations
    at each iteration, he also kept track of the lower and upper bounds for distances
    between points and centroids.
  prefs: []
  type: TYPE_NORMAL
- en: In 2006, David Arthur and Sergei Vassilvitskii proposed the k-means++ algorithm.
    The major change they proposed was in the initialization of centroids. They showed
    that if we choose centroids that are distant from each other, then the k-means
    algorithm is less likely to converge on a suboptimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: Another alternative can be that at each iteration we do not use the entire dataset,
    instead using mini-batches. This modification was proposed by David Sculey in
    2010\. Now, that we have covered PCA and k-means, we move toward an interesting
    network called self-organized network or winner-take-all units.
  prefs: []
  type: TYPE_NORMAL
- en: Self-organizing maps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Both k-means and PCA can cluster the input data; however, they do not maintain
    a topological relationship. In this section, we will consider **Self-Organizing
    Maps** (**SOMs**), sometimes known as **Kohonen networks** or **Winner-Take-All
    Units** (**WTUs**). They maintain the topological relation. SOMs are a very special
    kind of neural network, inspired by a distinctive feature of the human brain.
    In our brain, different sensory inputs are represented in a topologically ordered
    manner. Unlike other neural networks, neurons are not all connected to each other
    via weights; instead, they influence each other’s learning. The most important
    aspect of SOM is that neurons represent the learned inputs in a topographic manner.
    They were proposed by Teuvo Kohonen [7] in 1982.
  prefs: []
  type: TYPE_NORMAL
- en: 'In SOMs, neurons are usually placed on the nodes of a (1D or 2D) lattice. Higher
    dimensions are also possible but are rarely used in practice. Each neuron in the
    lattice is connected to all the input units via a weight matrix. *Figure 7.7*
    shows a SOM with 6 × 8 (48 neurons) and 5 inputs. For clarity, only the weight
    vectors connecting all inputs to one neuron are shown. In this case, each neuron
    will have seven elements, resulting in a combined weight matrix of size 40 × 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: A self-organized map with 5 inputs and 48 neurons'
  prefs: []
  type: TYPE_NORMAL
- en: A SOM learns via competitive learning. It can be considered as a nonlinear generalization
    of PCA and, thus, like PCA, can be employed for dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to implement SOM, let’s first understand how it works. As a first
    step, the weights of the network are initialized either to some random value or
    by taking random samples from the input. Each neuron occupying a space in the
    lattice will be assigned specific locations. Now as an input is presented, the
    neuron with the least distance from the input is declared the winner (WTU). This
    is done by measuring the distance between the weight vectors (*W*) and input vectors
    (*X*) of all neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_007.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *d*[j] is the distance of the weights of neuron *j* from input *X*. The
    neuron with the lowest *d* value is the winner.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the weights of the winning neuron and its neighboring neurons are adjusted
    in a manner to ensure that the same neuron is the winner if the same input is
    presented next time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To decide which neighboring neurons need to be modified, the network uses a
    neighborhood function ![](img/B18331_07_008.png); normally, the Gaussian Mexican
    hat function is chosen as a neighborhood function. The neighborhood function is
    mathematically represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/B18331_07_010.png) is a time-dependent radius of the influence
    of a neuron and *d* is its distance from the winning neuron. Graphically, the
    function looks like a hat (hence its name), as you can see in *Figure 7.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: The “Gaussian Mexican hat” function, visualized in graph form'
  prefs: []
  type: TYPE_NORMAL
- en: Another important property of the neighborhood function is that its radius reduces
    with time. As a result, in the beginning, many neighboring neurons’ weights are
    modified, but as the network learns, eventually a few neurons’ weights (at times,
    only one or none) are modified in the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The change in weight is given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_011.png)'
  prefs: []
  type: TYPE_IMG
- en: The process is repeated for all the inputs for a given number of iterations.
    As the iterations progress, we reduce the learning rate and the radius by a factor
    dependent on the iteration number.
  prefs: []
  type: TYPE_NORMAL
- en: SOMs are computationally expensive and thus are not really useful for very large
    datasets. Still, they are easy to understand, and they can very nicely find the
    similarity between input data. Thus, they have been employed for image segmentation
    and to determine word similarity maps in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Colour mapping using a SOM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the interesting properties of the feature map of the input space generated
    by a SOM are:'
  prefs: []
  type: TYPE_NORMAL
- en: The feature map provides a good representation of the input space. This property
    can be used to perform vector quantization so that we may have a continuous input
    space, and using a SOM we can represent it in a discrete output space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature map is topologically ordered, that is, the spatial location of a
    neuron in the output lattice corresponds to a particular feature of the input.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The feature map also reflects the statistical distribution of the input space;
    the domain that has the largest number of input samples gets a wider area in the
    feature map.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These features of SOM make them the natural choice for many interesting applications.
    Here we use SOM for clustering a range of given R, G, and B pixel values to a
    corresponding color map. We start with the importing of modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The main component of the code is our class `WTU`. The class `__init__` function
    initializes various hyperparameters of our SOM, the dimensions of our 2D lattice
    (`m, n`), the number of features in the input (`dim`), the neighborhood radius
    (`sigma`), the initial weights, and the topographic information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The most important function of the class is the `training()` function, where
    we use the Kohonen algorithm as discussed before to find the winner units and
    then update the weights based on the neighborhood function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The `fit()` function is a helper function that calls the `training()` function
    and stores the centroid grid for easy retrieval:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then there are some more helper functions to find the winner and generate a
    2D lattice of neurons, and a function to map input vectors to the corresponding
    neurons in the 2D lattice:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need to normalize the input data, so we create a function to do
    so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us read the data. The data contains red, green, and blue channel values
    for different colors. Let us normalize them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us create our SOM and fit it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The fit function takes slightly longer to run, since our code is not optimized
    for performance but for explaining the concept. Now, let’s look at the result
    of the trained model. Let us run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the color map in the 2D neuron lattice:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: A plotted color map of the 2D neuron lattice'
  prefs: []
  type: TYPE_NORMAL
- en: You can see that neurons that win for similar colors are closely placed. Next,
    we move to an interesting architecture, the restricted Boltzmann machines.
  prefs: []
  type: TYPE_NORMAL
- en: Restricted Boltzmann machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RBM is a two-layered neural network—the first layer is called the **visible
    layer** and the second layer is called the **hidden layer**. They are called **shallow
    neural networks** because they are only two layers deep. They were first proposed
    in 1986 by Paul Smolensky (he called them Harmony Networks [1]) and later by Geoffrey
    Hinton who in 2006 proposed **Contrastive Divergence** (**CD**) as a method to
    train them. All neurons in the visible layer are connected to all the neurons
    in the hidden layer, but there is a **restriction**—no neuron in the same layer
    can be connected. All neurons in the RBM are binary by nature; they will either
    fire or not fire.
  prefs: []
  type: TYPE_NORMAL
- en: 'RBMs can be used for dimensionality reduction, feature extraction, and collaborative
    filtering. The training of RBMs can be divided into three parts: forward pass,
    backward pass, and then a comparison.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us delve deeper into the math. We can divide the operation of RBMs into
    two passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Forward pass**: The information at visible units (*V*) is passed via weights
    (*W*) and biases (*c*) to the hidden units (*h*[0]). The hidden unit may fire
    or not depending on the stochastic probability (![](img/B18331_07_010.png) is
    the stochastic probability), which is basically the sigmoid function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_013.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Backward pass**: The hidden unit representation (*h*[0]) is then passed back
    to the visible units through the same weights, *W*, but a different bias, *c*,
    where the model reconstructs the input. Again, the input is sampled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_014.png)'
  prefs: []
  type: TYPE_IMG
- en: These two passes are repeated for *k* steps or until the convergence [4] is
    reached. According to researchers, *k=1* gives good results, so we will keep *k
    = 1*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The joint configuration of the visible vector *V* and the hidden vector *h*
    has energy given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also associated with each visible vector *V* is free energy, the energy that
    a single configuration would need to have in order to have the same probability
    as all of the configurations that contain *V*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Using the contrastive divergence objective function, that is, *Mean(F(V*[original]*))
    - Mean(F(V*[reconstructed]*))*, the change in weights is given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_017.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/B18331_01_025.png) is the learning rate. Similar expressions exist
    for the biases *b* and *c*.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstructing images using an RBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us build an RBM in TensorFlow. The RBM will be designed to reconstruct
    handwritten digits. This is the first generative model that you are learning;
    in the upcoming chapters, we will learn a few more. We import the TensorFlow,
    NumPy, and Matplotlib libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a class `RBM`. The class `__init_()` function initializes the number
    of neurons in the visible layer (`input_size`) and the number of neurons in the
    hidden layer (`output_size`). The function initializes the weights and biases
    for both hidden and visible layers. In the following code, we have initialized
    them to zero. You can try with random initialization as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We define methods to provide the forward and backward passes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a function to generate random binary values. This is because both
    hidden and visible units are updated using stochastic probability, depending upon
    the input to each unit in the case of the hidden layer (and the top-down input
    to visible layers):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We will need functions to reconstruct the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To train the RBM created we define the `train()` function. The function calculates
    the positive and negative gradient terms of contrastive divergence and uses the
    weight update equation to update the weights and biases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that our class is ready, we instantiate an object of `RBM` and train it
    on the MNIST dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us plot the learning curve:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'In the figure below, you can see the learning curve of our RBM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: Learning curve for the RBM model'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we present the code to visualize the reconstructed images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'And the reconstructed images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_07_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: Image reconstruction using an RBM'
  prefs: []
  type: TYPE_NORMAL
- en: The top row is the input handwritten image, and the bottom row is the reconstructed
    image. You can see that the images look remarkably similar to the human handwritten
    digits. In the upcoming chapters, you will learn about models that can generate
    even more complex images such as artificial human faces.
  prefs: []
  type: TYPE_NORMAL
- en: Deep belief networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have a good understanding of RBMs and know how to train them using
    contrastive divergence, we can move toward the first successful deep neural network
    architecture, the **deep belief networks** (**DBNs**), proposed in 2006 by Hinton
    and his team in the paper *A fast learning algorithm for deep belief nets*. Before
    this model it was very difficult to train deep architectures, not just because
    of the limited computing resources, but also, as will be discussed in *Chapter
    8*, *Autoencoders*, because of the vanishing gradient problem. In DBNs it was
    first demonstrated how deep architectures can be trained via greedy layer-wise
    training.
  prefs: []
  type: TYPE_NORMAL
- en: In the simplest terms, DBNs are just stacked RBMs. Each RBM is trained separately
    using the contrastive divergence. We start with the training of the first RBM
    layer. Once it is trained, we train the second RBM layer. The visible units of
    the second RBM are now fed the output of the hidden units of the first RBM, when
    it is fed the input data. The procedure is repeated with each RBM layer addition.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us try stacking our `RBM` class. To make the DBN, we will need to define
    one more function in the `RBM` class; the output of the hidden unit of one RBM
    needs to feed into the next RBM:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can just use the `RBM` class to create a stacked RBM structure. In the
    following code we create an RBM stack: the first RBM will have 500 hidden units,
    the second will have 200 hidden units, and the third will have 50 hidden units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'For the first RBM, the MNIST data is the input. The output of the first RBM
    is then fed as input to the second RBM, and so on through the consecutive RBM
    layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Our DBN is ready. The three stacked RBMs are now trained using unsupervised
    learning. DBNs can also be trained using supervised training. To do so we need
    to fine-tune the weights of the trained RBMs and add a fully connected layer at
    the end. In their publication *Classification with Deep Belief Networks*, Hebbo
    and Kim show how they used a DBN for MNIST classification; it is a good introduction
    to the subject.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the major unsupervised learning algorithms. We went
    through algorithms best suited for dimension reduction, clustering, and image
    reconstruction. We started with the dimension reduction algorithm PCA, then we
    performed clustering using k-means and self-organized maps. After this we studied
    the restricted Boltzmann machine and saw how we can use it for both dimension
    reduction and image reconstruction. Next, we delved into stacked RBMs, that is,
    deep belief networks, and we trained a DBN consisting of three RBM layers on the
    MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore another model using an unsupervised learning
    paradigm – autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Smith, Lindsay. (2006). *A tutorial on Principal Component Analysis*: [http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Movellan, J. R. *Tutorial on Principal component Analysis*: [http://mplab.ucsd.edu/tutorials/pca.pdf](http://mplab.ucsd.edu/tutorials/pca.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'TensorFlow Projector: [http://projector.tensorflow.org/](http://projector.tensorflow.org/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition** (**SVD**) tutorial. MIT: [https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm](https://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Shlens, Jonathon. (2014). *A tutorial on principal component analysis*. arXiv
    preprint arXiv:1404.1100: [https://arxiv.org/abs/1404.1100](https://arxiv.org/abs/1404.1100)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Deep learning*. MIT
    press: [https://www.deeplearningbook.org](https://www.deeplearningbook.org)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kohonen, T. (1982). *Self-organized formation of topologically correct feature
    maps*. Biological cybernetics 43, no. 1: 59-69.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kanungo, Tapas, et al. (2002). *An Efficient k-Means Clustering Algorithm:
    Analysis and Implementation*. IEEE transactions on pattern analysis and machine
    intelligence 24.7: 881-892.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Ortega, Joaquín Pérez, et al. *Research issues on K-means Algorithm: An Experimental
    Trial Using Matlab*. CEUR Workshop Proceedings: Semantic Web and New Technologies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chen, K. (2009). *On Coresets for k-Median and k-Means Clustering in Metric
    and Euclidean Spaces and Their Applications.* SIAM Journal on Computing 39.3:
    923-947.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Determining the number of clusters in a data set*: [https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set](https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lloyd, S. P. (1982). *Least Squares Quantization in PCM*: [http://mlsp.cs.cmu.edu/courses/fall2010/class14/lloyd.pdf](http://mlsp.cs.cmu.edu/courses/fall2010/class14/lloyd.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Dunn, J. C. (1973-01-01). *A Fuzzy Relative of the ISODATA Process and Its
    Use in Detecting Compact Well-Separated Clusters*. Journal of Cybernetics. 3(3):
    32–57.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bezdek, James C. (1981). *Pattern Recognition with Fuzzy Objective Function
    Algorithms*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Peters, G., Crespo, F., Lingras, P., and Weber, R. (2013). *Soft clustering–Fuzzy
    and rough approaches and their extensions and derivatives*. International Journal
    of Approximate Reasoning 54, no. 2: 307-322.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sculley, D. (2010). *Web-scale k-means clustering*. In Proceedings of the 19th
    international conference on World wide web, pp. 1177-1178\. ACM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Smolensky, P. (1986). *Information Processing in Dynamical Systems: Foundations
    of Harmony Theory*. No. CU-CS-321-86\. COLORADO UNIV AT BOULDER DEPT OF COMPUTER
    SCIENCE.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Salakhutdinov, R., Mnih, A., and Hinton, G. (2007). *Restricted Boltzmann Machines
    for Collaborative Filtering*. Proceedings of the 24th international conference
    on Machine learning. ACM.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Hinton, G. (2010). *A Practical Guide to Training Restricted Boltzmann Machines*.
    Momentum 9.1: 926.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1831217224278819687.png)'
  prefs: []
  type: TYPE_IMG
