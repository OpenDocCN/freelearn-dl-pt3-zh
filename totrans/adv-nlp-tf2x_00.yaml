- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 2017 was a watershed moment for **Natural Language Processing** (**NLP**), with
    Transformer-and attention-based networks coming to the fore. The past few years
    have been as transformational for NLP as AlexNet was for computer vision in 2012\.
    Tremendous advances in NLP have been made, and we are now moving from research
    labs into applications.
  prefs: []
  type: TYPE_NORMAL
- en: These advances span the domains of **Natural Language Understanding** (**NLU**),
    **Natural Language Generation** (**NLG**), and **Natural Language Interaction**
    (**NLI**). With so much research in all of these domains, it can be a daunting
    task to understand the exciting developments in NLP.
  prefs: []
  type: TYPE_NORMAL
- en: This book is focused on cutting-edge applications in the fields of NLP, language
    generation, and dialog systems. It covers the concepts of pre-processing text
    using techniques such as tokenization, **parts-of-speech** (**POS**) tagging,
    and lemmatization using popular libraries such as Stanford NLP and spaCy. **Named
    Entity Recognition** (**NER**) models are built from scratch using **Bi-directional
    Long Short-Term Memory networks** (**BiLSTMs**), **Conditional Random Fields**
    (**CRFs**), and Viterbi decoding. Taking a very practical, application-focused
    perspective, the book covers key emerging areas such as generating text for use
    in sentence completion and text summarization, multi-modal networks that bridge
    images and text by generating captions for images, and managing the dialog aspects
    of chatbots. It covers one of the most important reasons behind recent advances
    of NLP – transfer learning and fine tuning. Unlabeled textual data is easily available
    but labeling this data is costly. This book covers practical techniques that can
    simplify the labeling of textual data.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the book, I hope you will have advanced knowledge of the tools,
    techniques, and deep learning architectures used to solve complex NLP problems.
    The book will cover encoder-decoder networks, **Long Short-Term Memory networks**
    (**LSTMs**) and BiLSTMs, CRFs, BERT, GPT-2, GPT-3, Transformers, and other key
    technologies using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Advanced TensorFlow techniques required for building advanced models are also
    covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Building custom models and layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building custom loss functions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing learning rate annealing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `tf.data` for loading data efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Checkpointing models to enable long training times (usually several days)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This book contains working code that can be adapted to your own use cases. I
    hope that you will even be able to do novel state-of-the-art research using the
    skills you'll gain as you progress through the book.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This book assumes that the reader has some familiarity with the basics of deep
    learning and the fundamental concepts of NLP. This book focuses on advanced applications
    and building NLP systems that can solve complex tasks. All kinds of readers will
    be able to follow the content of the book, but readers who can benefit the most
    from this book include:'
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate **Machine Learning** (**ML**) developers who are familiar with
    the basics of supervised learning and deep learning techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Professionals who already use TensorFlow/Python for purposes such as data science,
    ML, research, analysis, etc., and can benefit from a more solid understanding
    of advanced NLP techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Chapter 1*, *Essentials of NLP*, provides an overview of various topics in
    NLP such as tokenization, stemming, lemmatization, POS tagging, vectorization,
    etc. An overview of common NLP libraries like spaCy, Stanford NLP, and NLTK, with
    their key capabilities and use cases, will be provided. We will also build a simple
    classifier for spam.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 2*, *Understanding Sentiment in Natural Language with BiLSTMs*, covers
    the NLU use case of sentiment analysis with an overview of **Recurrent Neural
    Networks** (**RNNs**), LSTMs, and BiLSTMs, which are the basic building blocks
    of modern NLP models. We will also use `tf.data` for efficient use of CPUs and
    GPUs to speed up data pipelines and model training.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 3*, *Named Entity Recognition (NER) with BiLSTMs, CRFs, and Viterbi
    Decoding*, focuses on the key NLU problem of NER, which is a basic building block
    of task-oriented chatbots. We will build a custom layer for CRFs for improving
    the accuracy of NER and the Viterbi decoding scheme, which is often applied to
    a deep model to improve the quality of the output.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 4*, *Transfer Learning with BERT*, covers a number of important concepts
    in modern deep NLP such as types of transfer learning, pre-trained embeddings,
    an overview of Transformers, and BERT and its application in improving the sentiment
    analysis task introduced in *Chapter 2*, *Understanding Sentiment in Natural Language
    with BiLSTMs*.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 5*, *Generating Text with RNNs and GPT-2*, focuses on generating text
    with a custom character-based RNN and improving it with Beam Search. We will also
    cover the GPT-2 architecture and touch upon GPT-3.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 6*, *Text Summarization with Seq2seq Attention and Transformer Networks*,
    takes on the challenging task of abstractive text summarization. BERT and GPT
    are two halves of the full encoder-decoder model. We put them together to build
    a seq2seq model for summarizing news articles by generating headlines for them.
    How ROUGE metrics are used for the evaluation of summarization is also covered.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 7*, *Multi-Modal Networks and Image Captioning with ResNets and Transformers*,
    combines computer vision and NLP together to see if a picture is indeed worth
    a thousand words! We will build a custom Transformer model from scratch and train
    it to generate captions for images.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 8*, *Weakly Supervised Learning for Classification with Snorkel*,
    focuses on a key problem – labeling data. While NLP has a lot of unlabeled data,
    labeling it is quite an expensive task. This chapter introduces the `snorkel`
    library and shows how massive amounts of data can be quickly labeled.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 9*, *Building Conversational AI Applications with Deep Learning*,
    combines the various techniques covered throughout the book to show how different
    types of chatbots, such as question-answering or slot-filling bots, can be built.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 10*, *Installation and Setup Instructions for Code*, walks through
    all the instructions required to install and configure a system for running the
    code supplied with the book.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It would be a good idea to get a background on the basics of deep learning models
    and TensorFlow.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The use of a GPU is highly recommended. Some of the models, especially in the
    later chapters, are pretty big and complex. They may take hours or days to fully
    train on CPUs. RNNs are very slow to train without the use of GPUs. You can get
    access to free GPUs on Google Colab, and instructions for doing so are provided
    in the first chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code bundle for the book is hosted on GitHub at [https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2](https://github.com/PacktPublishing/Advanced-Natural-Language-Processing-with-TensorFlow-2).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781800200937_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781800200937_ColorImages.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    For example: "In the `num_capitals()` function, substitutions are performed for
    the capital letters in English."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see on
    the screen, for example, in menus or dialog boxes, also appear in the text like
    this. For example: "Select **System info** from the **Administration** panel."'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: If you have questions about any aspect of this book,
    mention the book title in the subject of your message and email Packt at `customercare@packtpub.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you could report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata),
    select your book, click on the **Errata Submission Form** link, and enter the
    details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Reviews
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Packt, please visit [packtpub.com](http://packtpub.com).
  prefs: []
  type: TYPE_NORMAL
