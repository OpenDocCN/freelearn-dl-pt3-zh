["```\ndef get_processed_tokens(text):\n'''\nGets Token List from a Review\n'''\nfiltered_text = re.sub(r'[^a-zA-Z0-9\\s]', '', text) #Removing Punctuations\nfiltered_text = filtered_text.split()\nfiltered_text = [token.lower() for token in filtered_text]\nreturn filtered_text\n```", "```\ndef tokenize_text(data_text, min_frequency =5):\n    '''\n    Tokenizes the reviews in the dataset. Filters non frequent tokens\n    '''\n    review_tokens = [get_processed_tokens(review) for review in   \n                     data_text] # Tokenize the sentences\n    token_list = [token for review in review_tokens for token in review]         \n    #Convert to single list\n    token_freq_dict = {token:token_list.count(token) for token in     \n    set(token_list)} # Get the frequency count of tokens\n    most_freq_tokens = [tokens for tokens in token_freq_dict if \n    token_freq_dict[tokens] >= min_frequency]\n    idx = range(len(most_freq_tokens))\n    token_idx = dict(zip(most_freq_tokens, idx))\n    return token_idx,len(most_freq_tokens)\n```", "```\ndef get_max(data):\n    '''\n    Get max length of the token\n    '''\n    tokens_per_review = [len(txt.split()) for txt in data]\n    return max(tokens_per_review)\n```", "```\n\ndef create_sequences(data_text,token_idx,max_tokens):\n    '''\n    Create sequences appropriate for GRU input\n    Input: reviews data, token dict, max_tokens\n    Output: padded_sequences of shape (len(data_text), max_tokens)\n    '''\n    review_tokens = [get_processed_tokens(review) for review in  \n                   data_text] # Tokenize the sentences \n    #Covert the tokens to their indexes \n    review_token_idx = map( lambda review: [token_idx[k] for k in review \n                           if k in token_idx.keys() ], review_tokens)\n    padded_sequences = pad_sequences(review_token_idx,maxlen=max_tokens)\n    return np.array(padded_sequences)\n```", "```\ndef define_model(num_tokens,max_tokens):\n    '''\n    Defines the model definition based on input parameters\n    '''\n    model = Sequential()\n    model.add(Embedding(input_dim=num_tokens,\n                    output_dim=EMBEDDING_SIZE,\n                    input_length=max_tokens,\n                    name='layer_embedding'))\n\n    model.add(GRU(units=16, name = \"gru_1\",return_sequences=True))\n    model.add(GRU(units=8, name = \"gru_2\",return_sequences=True))\n    model.add(GRU(units=4, name= \"gru_3\"))\n    model.add(Dense(1, activation='sigmoid',name=\"dense_1\"))\n    optimizer = Adam(lr=1e-3)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    print model.summary()\n    return model\n```", "```\ndef train_model(model,input_sequences,y_train):\n    '''\n    Train the model based on input parameters\n    '''\n\n    model.fit(input_sequences, y_train,\n          validation_split=VAL_SPLIT, epochs=EPOCHS, \n          batch_size=BATCH_SIZE)\n    return model\n```", "```\npip install tensorflowjs\n```", "```\nimport tensorflowjs as tfjs \ntfjs.converters.save_keras_model(model, OUTPUT_DIR)\n```", "```\n<script src=\"img/tfjs@0.8.0\"></script>\n```", "```\nasync function createModel()\n{\nconst model = await\ntf.loadModel('http://127.0.0.1:8000/model.json')\nreturn model\n}\nasync function loadDict()\n{\n await $.ajax({\n url: 'http://127.0.0.1:8000/token_index.csv',\n dataType: 'text',\n crossDomain : true}).done(success);\n}\nfunction success(data)\n{\n    var wd_idx = new Object();\n    lst = data.split(/\\r?\\n|\\r/)\n    for(var i = 0 ; i < lst.length ;i++){\n        key = (lst[i]).split(',')[0]\n        value = (lst[i]).split(',')[1]\n\n        if(key == \"\")\n            continue\n        wd_idx[key] = parseInt(value) \n    }\n\n    word_index = wd_idx\n}\n\nasync function init()\n{\n word_index = undefined\n console.log('Start loading dictionary')\n await loadDict()\n //console.log(word_index)\n console.log('Finish loading dictionary')\n console.log('Start loading model') \n model = await createModel()\n console.log('Finish loading model') \n}\n```", "```\nfunction process(txt)\n{\n out = txt.replace(/[^a-zA-Z0-9\\s]/, '')\n out = out.trim().split(/\\s+/)\n for (var i = 0 ; i < out.length ; i++)\n out[i] = out[i].toLowerCase()\n return out\n}\n\nfunction create_sequences(txt)\n{\n max_tokens = 40 \n tokens = []\n words = process(txt)\n seq = Array.from(Array(max_tokens), () => 0) \n start = max_tokens-words.length\n for(var i= 0 ; i< words.length ; i++)\n {\n     if (Object.keys(word_index).includes(words[i])){\n         seq[i+start] = word_index[words[i]]\n     } \n }\n return seq\n}\n```", "```\nasync function predict()\n{\n txt = document.getElementById(\"userInput\").value\n alert(txt);\n seq = create_sequences(txt) \n input = tf.tensor(seq)\n input = input.expandDims(0)\n pred = model.predict(input)\n document.getElementById(\"Sentiment\").innerHTML = pred;\n\n pred.print()\n}\n```"]