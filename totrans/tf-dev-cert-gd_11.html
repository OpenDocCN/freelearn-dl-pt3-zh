<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer165">
<p><a id="_idTextAnchor264"/><a id="_idTextAnchor265"/><a id="_idTextAnchor266"/></p>
<h1 class="chapter-number" id="_idParaDest-195"><a id="_idTextAnchor267"/>11</h1>
<h1 id="_idParaDest-196"><a id="_idTextAnchor268"/>NLP with TensorFlow</h1>
<p><a id="_idTextAnchor269"/>Text data is inherently sequential, defined by the order in which words occur. Words follow one another, building upon previous ideas and shaping those to come. Understanding the sequence of words and the context in which they are applied is straightforward for humans. However, this poses a significant challenge to feed-forward networks<a id="_idIndexMarker595"/> such as <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) and <a id="_idIndexMarker596"/>traditional <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>). These models treat text data as independent inputs; hence, they miss the interconnected nature and flow of language. For example, let’s take the sentence “<em class="italic">The cat, which is a mammal, likes to chase mice</em>." Humans immediately recognize the relationship between the cat and mice, as we process the entire sentence as a whole and not <span class="No-Break">individual units.</span></p>
<p>A <strong class="bold">recurrent neural network</strong> (<strong class="bold">RNN</strong>) is a<a id="_idIndexMarker597"/> type of neural network designed to handle sequential data such as text and time-series data. When working with text data, RNNs’ memory enables them to recall earlier parts of a sequence, aiding them to understand the context in which words are used in a text. For example, with a sentence such as “<em class="italic">As an archaeologist, John loves discovering ancient artifacts,</em>”  an RNN, in this context, can infer that an archaeologist will be excited about ancient artifacts, and in this sentence, the archaeologist <span class="No-Break">is John.</span></p>
<p>In this chapter, we will begin to explore the world of RNNs and take a look under the hood to understand how the inner mechanisms of RNNs work together to maintain a form of memory. We will explore the pros and cons of RNNs when working with text data, after which we<a id="_idIndexMarker598"/> will switch our attention to investigating its variants, such as <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>) and <strong class="bold">gated recurrent units</strong> (<strong class="bold">GRUs</strong>). Next, we <a id="_idIndexMarker599"/>will use the knowledge we have gained to build a multiclass text classifier. We will then explore the power of transfer learning in the <a id="_idIndexMarker600"/>field of <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>). Here, we will see how to apply pretrained word embeddings to our workflow. To close this chapter, we will use RNNs to build a child story generator; here, we will see RNNs in action as they generate <span class="No-Break">text data.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>The anatomy <span class="No-Break">of RNNs</span></li>
<li>Text classification <span class="No-Break">with RNNs</span></li>
<li>NLP with <span class="No-Break">transfer learning</span></li>
<li><span class="No-Break">Text generation</span></li>
</ul>
<h1 id="_idParaDest-197"><a id="_idTextAnchor270"/>Understanding sequential data processing – from traditional neural networks to RNNs and <a id="_idTextAnchor271"/>LSTMs</h1>
<p>In traditional neural<a id="_idIndexMarker601"/> networks, as we<a id="_idIndexMarker602"/> discussed earlier in this book, we see an arrangement of densely interconnected neurons, devoid of any form of memory. When we feed a sequence of data to these networks, it’s an all-or-nothing transaction – the entire sequence is processed at once and converted into a singular vector representation. This approach is quite different from how humans process and comprehend text data. When we read, we naturally analyze text word by word, understanding that important words – those that have the power to shift the entire message of a sentence – can be positioned anywhere within it. For example, let's consider the sentence “<em class="italic">I loved the movie, despite some critics.</em>” Here, the word “<em class="italic">despite</em>” is pivotal, altering the direction of the sentiment expressed in <span class="No-Break">the sentence.</span></p>
<p>RNNs don’t just consider the value of individual words through embeddings; they also take into account the sequence or relative order of these words. This ordering of words gives them meaning and allows humans to effectively communicate with one another. RNNs are unique in their ability to retain context from one timestamp (or one word in the case of a sentence) to the next, thereby preserving the sequential coherence of the input. For example, in the sentence “<em class="italic">I visited Rome last year, and I found the Colosseum fascinating,</em>” an RNN would understand that “<em class="italic">the Colosseum</em>” relates to “<em class="italic">Rome</em>” because of the sequence of words. However, there’s a catch – this context retention can fail in longer sentences where the distance between related <span class="No-Break">words increases.</span></p>
<p>This is precisely where gated variants of RNNs such as LSTM networks come in. LSTMs are designed with a special “cell state” architecture that enables them to manage and retain information over longer sequences. So, even in a lengthy sentence such as “<em class="italic">I visited Rome last year, experienced the rich culture, enjoyed the delicious food, met wonderful people, and I found the Colosseum fascinating,</em>” an LSTM could still link “<em class="italic">the Colosseum</em>” with “<em class="italic">Rome</em>,” understanding the broader context despite the length and <a id="_idIndexMarker603"/>complexity <a id="_idIndexMarker604"/>of the sentence. We have only scratched the surface. Let’s now examine the anatomy of these powerful networks. We will begin <span class="No-Break">with RNNs.</span></p>
<h1 id="_idParaDest-198">The anatomy<a id="_idTextAnchor272"/> of RNNs</h1>
<p>In the previous section, we<a id="_idIndexMarker605"/> talked about RNNs’ ability to handle sequential data; let’s drill down into how an RNN does this. The key differentiator between RNNs and feed-forward networks is their internal memory, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.1</em>, which enables RNNs to process input sequences while retaining information from previous steps. This attribute empowers RNNs to suitably exploit the temporal dependencies in sequences such as <span class="No-Break">text data.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer159">
<img alt="Figure 11.1 – The anatomy of an RNN" height="184" src="image/B18118_11_001.jpg" width="126"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – The anatomy <a id="_idTextAnchor273"/>of an RNN</p>
<p><span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em> shows a clearer picture of an RNN and its inner workings. Here, we can see a series of interconnected units through which data flows in a sequential fashion, one element at a time. As each unit processes the input data, it sends the output to the next unit in a similar fashion to how feed-forward networks work. The key difference lies in the feedback loop, which equips RNNs with the memory of previous inputs, empowering them with the ability to comprehend <span class="No-Break">entire sequences.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<img alt="Figure 11.2 – An expanded view of an RNN showing its operation across multiple timesteps" height="187" src="image/B18118_11_002.jpg" width="671"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – An expanded view of an RNN showing its operation across multiple timesteps</p>
<p>Let’s imagine we are dealing with sentences, and we want our RNN to learn something about their grammar. Each word in the sentence represents a time step, and at each one, the RNN considers the current word and also the “context” from the previous words (or steps). Let’s go over a sample sentence. Let’s say we have a sentence with five words – “<em class="italic">Barcelona is a nice city.</em>” This sentence has five time steps, one for each word. At time step 1, we feed the word “<em class="italic">Barcelona</em>” into our RNN. The network learns something about this word (in reality, it would learn from the word’s vector representation), produces an output, and also a hidden state capturing what it has learned, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em>. Now, we unroll the RNN to timestep 2. We input the next word, “<em class="italic">is</em>,” into our network, but we also input the hidden state from timestep 1. This hidden state represents the “memory” of the network, allowing the network to take into account what it has seen so far. The network produces a new output and a new <span class="No-Break">hidden state.</span></p>
<p>This process <a id="_idIndexMarker606"/>continues, with the RNN unrolling further for each word in the sentence. At each time step, the network takes the current word and the hidden state from the previous time step as input, producing an output and a new hidden state. When you “unroll” an RNN in this way, it may look like a deep feed-forward network with shared weights across each layer (since each timestep uses the same underlying RNN cell for its operations), but it’s more accurately a single network that’s being applied to each timestep, passing along the hidden state as it goes. RNNs have the ability to learn from and remember sequences of arbitrary lengths; however, they do have their own limitations. One key issue with RNNs is their struggle with capturing long-term dependencies due to the vanishing gradient problem. This happens because the influence of time steps over future time steps can diminish over long sequences, as a result of repeated multiplications during backpropagation, which makes the gradients exceedingly small and thus harder to learn from. To address this, we will apply more advanced versions of RNNs such as LSTM and GRU networks. These architectures apply gating mechanisms to control the flow of information in the network and make it easier for the model to learn long-term dependencies. Let’s examine these<a id="_idIndexMarker607"/> variants <span class="No-Break">of RNNs.</span></p>
<h2 id="_idParaDest-199">Variants of RNNs – LS<a id="_idTextAnchor274"/><a id="_idTextAnchor275"/><a id="_idTextAnchor276"/><a id="_idTextAnchor277"/>TM and GRU</h2>
<p>Let’s imagine we are <a id="_idIndexMarker608"/>working on a movie review <a id="_idIndexMarker609"/>classification project, and while inspecting our dataset, we find a sentence such as this one: “<em class="italic">The movie started off boring and slow, but it really picked up toward the end, and the climax was amazing.</em>” By examining the sentence, we see that the initial set of words used by the reviewer portrays a negative sentiment by using words such as “slow” and “boring,” but the sentiment takes a shift to a more positive one with the use of phrases such as “picked up” and “climax was amazing.” If we use a simple RNN for this task, due to its inherent limitation to retain information over longer sequences, it may misclassify the sentence by attaching undue importance to the earlier negative tone of <span class="No-Break">the review.</span></p>
<p>Conversely, LSTMs and GRUs are designed to handle long-term dependencies, which makes them effective in not just capturing the change in sentiment but also for other NLP tasks, such as machine translation, text summarization, and question answering, where they outshine their <span class="No-Break">simpler c<a id="_idTextAnchor278"/>ounterparts.</span></p>
<h3>LSTMs</h3>
<p>LSTMs are a specialized<a id="_idIndexMarker610"/> type of RNN designed <a id="_idIndexMarker611"/>to address the vanishing gradient problem, enabling LSTMs to effectively handle long-term dependencies in sequential data. To resolve this issue LSTM introduced a new structure called a memory cell, which essentially acts as an information carrier with the ability to preserve information over an extended period. Unlike standard RNNs, which feed information from one step to the next and tend to lose it over time, an LSTM with the aid of its memory cell can store and retrieve information from any point in the input sequence, irrespective of its length. Let’s examine how an LSTM decides what information it will store in its memory cell. An LSTM is made up of four main components, as shown in <span class="No-Break"><em class="italic">Figure 10</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer161">
<img alt="10.3 – LSTM architecture" height="1013" src="image/B18118_11_003.jpg" width="1426"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">10.3 – LSTM architecture</p>
<p>These components<a id="_idIndexMarker612"/> allow LSTMs to store and <a id="_idIndexMarker613"/>access information over long sequences. Let’s look at each of <span class="No-Break">the components:</span></p>
<ul>
<li><strong class="bold">Input gate</strong>: The input <a id="_idIndexMarker614"/>gate decides what new information will be stored in the memory cell. It is made up of a sigmoid and a tanh layer. The sigmoid layer produces output values between zero and one, representing the importance level of each value in the input, where zero means “not important at all” and one represents “very important.” The tanh layer generates a set of candidate values that can be added to the state, essentially suggesting what new information should be stored in the memory cell. The outputs of both layers are merged by performing element-wise multiplication. This element-wise operation produces an input modulation gate that effectively filters the new candidate values, by deciding which information is important enough to be stored in the <span class="No-Break">memory cell.</span></li>
<li><strong class="bold">Forget gate</strong>: This gate decides what information will be retained and what information will be discarded. It uses a sigmoid layer to return output values between zero and one. If a unit in the forget gate returns an output value close to zero, the LSTM will remove the information in the corresponding unit of the <span class="No-Break">cell state.</span></li>
<li><strong class="bold">Output gate</strong>: The output gate determines what the next hidden state should be. Like the other gates, it also uses a sigmoid function to decide which parts of the cell state make it to <span class="No-Break">the output.</span></li>
<li><strong class="bold">Cell state</strong>: The cell state is the “memory” of the LSTM cell. It is updated based on the output from<a id="_idIndexMarker615"/> the forget and input gates. It can remember information for use later in <span class="No-Break">the sequence.</span></li>
</ul>
<p>The gate mechanisms are paramount because they allow the LSTM to automatically learn appropriate context-dependent ways to read, write, and reset cells in the memory. These capabilities enable LSTMs to handle longer sequences, making them particularly useful for many complex <a id="_idIndexMarker616"/>sequential tasks where standard RNNs fall short, due to their inability to handle<a id="_idIndexMarker617"/> long-term dependencies, such as machine translation, text generation, time-series prediction, and <span class="No-Break">video analysis.</span></p>
<h3>Bidirectional Long Short-Term Memory (BiLSTM)</h3>
<p><strong class="bold">Bidirectional Long Short-Term Memory</strong> (<strong class="bold">BiLSTM</strong>) is an extension of traditional LSTM <a id="_idIndexMarker618"/>networks. However, unlike <a id="_idIndexMarker619"/>LSTMs, which process information in a sequential fashion from start to finish, BiLSTMs run two LSTMs simultaneously – one processes sequential data from the start to the end and the other from the end to the start, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer162">
<img alt="Figure 11.4 – The information flow in a BiLSTM" height="848" src="image/B18118_11_004.jpg" width="912"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – The information flow in a BiLSTM</p>
<p>By doing this, BiLSTMs can capture both the past and future context of each data point in the sequence. Because of BiLSTMs’ ability to comprehend the context from both directions in a sequence of data, they are well suited for tasks such as text generation, text<a id="_idIndexMarker620"/> classification, sentiment <a id="_idIndexMarker621"/>analysis, and machine translation. Now, let's <span class="No-Break">examine GRUs.</span></p>
<h3>GRUs</h3>
<p>In 2014, <em class="italic">Cho et al.</em> introduced <a id="_idIndexMarker622"/>the <strong class="bold">GRU</strong> architecture as a<a id="_idIndexMarker623"/> viable alternative to LSTMs. GRUs were designed to achieve two primary goals – one was to overcome the vanishing gradient issues that plagued traditional RNNs, and the other was to streamline LSTM architecture for increased computational efficiency while maintaining the ability to model long-term dependencies. Structurally, the GRU has two primary gates, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer163">
<img alt="Figure 11.5 – A GRU’s architecture" height="781" src="image/B18118_11_005.jpg" width="1005"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – A GRU’s architecture</p>
<p>One key difference between the GRU and LSTM is the absence of a separate cell state in GRUs; instead, they use a hidden state to transfer and manipulate information as well as streamline its <span class="No-Break">computational needs.</span></p>
<p>GRUs have two main gates, the update gate and the reset gate. Let’s <span class="No-Break">examine them:</span></p>
<ul>
<li><strong class="bold">Update gate</strong>: The update <a id="_idIndexMarker624"/>gate condenses the input and forget gates in LSTMs. It determines how much of the past information needs to be carried forward to the current state and which information needs <a id="_idIndexMarker625"/>to <span class="No-Break">be discarded.</span></li>
<li><strong class="bold">Reset gate</strong>: This gate defines how much of the past information should be forgotten. It helps the<a id="_idIndexMarker626"/> model evaluate the relative importance of new input against <span class="No-Break">past memory.</span></li>
</ul>
<p>Along with these two primary gates, a GRU introduces a “candidate hidden state.” This candidate hidden state combines the new input and the previous hidden state, and by doing so, it develops a preliminary version of the hidden state for the current time step. This candidate then plays an important role in determining the final hidden state, ensuring that the GRU retains relevant context from the past while accommodating new information. When deciding between LSTMs and GRUs, the choice is largely dependent on the specific application and the computational resources available. For some applications, the increased computational efficiency of GRU is more appealing. For example, in real-time processing, such as text-to-speech, or when working on tasks with short sequences, such as sentiment analysis of tweets, GRUs could prove to be an excellent choice in comparison <span class="No-Break">to LSTMs.</span></p>
<p>We have provided<a id="_idIndexMarker627"/> a high-level discussion on RNNs and their variants. Let’s now proceed to apply these new architectures to a real-world use case. Will they outperform the standard DNNs or CNNs? Let’s find out in a text classification <span class="No-Break">case study.</span></p>
<h1 id="_idParaDest-200"><a id="_idTextAnchor279"/>Text classification using the AG News dataset –  a comparative study</h1>
<p>The AG News<a id="_idIndexMarker628"/> dataset is a collection of more than 1 million <a id="_idIndexMarker629"/>news articles, collected from over 2,000 news sources by a news search engine called ComeToMyHead. The dataset is distributed across four categories – namely, world, sports, business, and science and technology – and it is available<a id="_idIndexMarker630"/> on <strong class="bold">TensorFlow Datasets</strong> (<strong class="bold">TFDS</strong>). The dataset is made up of 120,000 training samples (30,000 from each category), and the test set contains <span class="No-Break">7,600 examples.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">This experiment may take about an hour to run, due to the size of the dataset and the number of models; hence, it is important to ensure your notebook is GPU-enabled. Again, you could take a smaller subset to ensure your experiments run <span class="No-Break">much faster.</span></p>
<p>Let’s start building <span class="No-Break">our model:</span></p>
<ol>
<li>We will begin <a id="_idIndexMarker631"/>by <a id="_idIndexMarker632"/>loading the necessary libraries for <span class="No-Break">this experiment:</span><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import tensorflow_datasets as tfds</pre><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
from tensorflow.keras.preprocessing.text import Tokenizer</pre><pre class="source-code">
from tensorflow.keras.preprocessing.sequence import pad_sequences</pre><pre class="source-code">
from tensorflow.keras.utils import to_categorical</pre><pre class="source-code">
import tensorflow_hub as hub</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from tensorflow.keras.models import Sequential</pre><pre class="source-code">
from tensorflow.keras.layers import Embedding, SimpleRNN, Conv1D, GlobalMaxPooling1D, LSTM, GRU, Bidirectional, Dense, Flatten</pre></li>
</ol>
<p>These imports form the building blocks, enabling us to solve this text <span class="No-Break">classification problem.</span></p>
<ol>
<li value="2">Then, we load the AG News dataset <span class="No-Break">from TFDS:</span><pre class="source-code">
# Load the dataset</pre><pre class="source-code">
dataset, info = tfds.load('ag_news_subset',</pre><pre class="source-code">
    with_info=True, as_supervised=True)</pre><pre class="source-code">
train_dataset, test_dataset = datas<a id="_idTextAnchor280"/>et['train'],</pre><pre class="source-code">
    dataset['test']</pre></li>
</ol>
<p>We use this code to load our dataset from TFDS – the <strong class="source-inline">tfds.load</strong> function fetches and loads the AG News dataset. We set the <strong class="source-inline">with_info</strong> argument to <strong class="source-inline">True</strong>; this ensures the metadata of our dataset, such as the total number of samples and the version, is also collected. This metadata information is stored in the <strong class="source-inline">info</strong> variable. We also set <strong class="source-inline">as_supervised</strong> to <strong class="source-inline">True</strong>; we do this to ensure that data is loaded in input and label pairs, where the input is the news article and the label is the corresponding category. Then, we split the data int<a id="_idTextAnchor281"/>o a training set and a <span class="No-Break">test set.</span></p>
<ol>
<li value="3">Now, we need to <a id="_idIndexMarker633"/>prepare <a id="_idIndexMarker634"/>our data <span class="No-Break">for modeling:</span><pre class="source-code">
# Tokenize and pad the sequences</pre><pre class="source-code">
tokenizer = Tokenizer(num_words=20000,</pre><pre class="source-code">
    oov_token="&lt;OOV&gt;")</pre><pre class="source-code">
train_texts = [x[0].numpy().decode(</pre><pre class="source-code">
    'utf-8') for x in train_dataset]</pre><pre class="source-code">
tokenizer.fit_on_texts(train_texts)</pre><pre class="source-code">
sequences = tokenizer.texts_to_sequences(train_texts)</pre><pre class="source-code">
sequences = pad_sequences(sequences, padding='post')</pre></li>
</ol>
<p>Here, we perform data preparatory steps such as tokenization, sequencing, and padding, using TensorFlow’s Keras API. We initialize the tokenizer, which we use to convert our data from text to a sequence of integers. We set the <strong class="source-inline">num_words</strong> parameter to <strong class="source-inline">20000</strong>. This means we will only consider the top 20,000 occurring words in our dataset for tokenization; less frequently occurring words below this limit will be ignored. We set the <strong class="source-inline">oov_token="&lt;OOV&gt;"</strong> parameter to ensure we cater to unseen words that we may encounter during <span class="No-Break">model inferencing.</span></p>
<p>Then, we extract the training data and store it in the <strong class="source-inline">train_texts</strong> variable. We tokenize and transform our data into a sequence of integers by mapping numerical values to tokens, using the <strong class="source-inline">fit_on_texts</strong> and <strong class="source-inline">texts_to_sequences()</strong> methods respectively. We apply padding to each sequence to ensure that the data we will input into our models is of a consistent shape. We set <strong class="source-inline">padding</strong> to <strong class="source-inline">post</strong>; this will ensure padding is applied at the end of a sequence. We now have our data in a well-structured format, which we will feed into our deep-learning models for text <span class="No-Break">classification shortly.</span></p>
<ol>
<li value="4">Before we start modeling, we want to split our data into training and validation sets. We do this by<a id="_idIndexMarker635"/> splitting <a id="_idIndexMarker636"/>our training set into 80 percent for training and 20 percent <span class="No-Break">for validation:</span><pre class="source-code">
# Convert labels to one-hot encoding</pre><pre class="source-code">
train_labels = [label.numpy() for _, label in train_dataset]</pre><pre class="source-code">
train_labels = to_categorical(train_labels,</pre><pre class="source-code">
    num_classes=4)4  # assuming 4 classes</pre><pre class="source-code">
# Split the training set into training and validation sets</pre><pre class="source-code">
train_sequences, val_sequences, train_labels,</pre><pre class="source-code">
    val_labels = train_test_split(sequences,</pre><pre class="source-code">
        train_labels, test_size=0.2)</pre></li>
</ol>
<p>We convert our labels to one-hot encoding vectors, after which we split our training data using the <strong class="source-inline">train_test_split</strong> function from scikit-learn. We set our <strong class="source-inline">test_size</strong> to <strong class="source-inline">0.2</strong>; this means we will have 80 percent of our data for training and the remaining 20 percent for <span class="No-Break">validation purposes.</span></p>
<ol>
<li value="5">Let’s set the <strong class="source-inline">vocab_size</strong>, <strong class="source-inline">embedding_dim</strong>, and <span class="No-Break"><strong class="source-inline">max_length</strong></span><span class="No-Break"> parameters:</span><pre class="source-code">
vocab_size=20000</pre><pre class="source-code">
embedding_dim =64</pre><pre class="source-code">
max_length=sequences.shape[1]</pre></li>
</ol>
<p>We set <strong class="source-inline">vocab_size</strong> and <strong class="source-inline">embedding_dim</strong> to <strong class="source-inline">20000</strong> and <strong class="source-inline">64</strong>, respectively. When selecting your <strong class="source-inline">vocab_size</strong>, it is important to strike a good balance between computation efficiency, model complexity, and the ability to capture language nuances, while we use our embedding dimension to represent each word in our vocabulary by a 64-dimensional vector. The <strong class="source-inline">max_length</strong> parameter is set to match the longest tokenized and padded sequence in <span class="No-Break">our data.</span></p>
<ol>
<li value="6">We begin <a id="_idIndexMarker637"/>building <a id="_idIndexMarker638"/>our models, starting with <span class="No-Break">a DNN:</span><pre class="source-code">
# Define the DNN model</pre><pre class="source-code">
model_dnn = Sequential([</pre><pre class="source-code">
    Embedding(vocab_size, embedding_dim,</pre><pre class="source-code">
        input_length=max_length),</pre><pre class="source-code">
    Flatten(),</pre><pre class="source-code">
    tf.keras.layers.Dense(64, activation='relu'),</pre><pre class="source-code">
    Dense(16, activation='relu'),</pre><pre class="source-code">
    Dense(4, activation='softmax')</pre><pre class="source-code">
])</pre></li>
</ol>
<p>Using the <strong class="source-inline">Sequential</strong> API from TensorFlow, we build a DNN made up of an embedding layer, a flatten layer, two hidden layers, and an output layer for <span class="No-Break">multiclass classification.</span></p>
<ol>
<li value="7">Then, we build a <span class="No-Break">CNN architecture:</span><pre class="source-code">
# Define the CNN model</pre><pre class="source-code">
model_cnn = Sequential([</pre><pre class="source-code">
    Embedding(vocab_size, embedding_dim,</pre><pre class="source-code">
        input_length=max_length),</pre><pre class="source-code">
    Conv1D(128, 5, activation='relu'),</pre><pre class="source-code">
    GlobalMaxPooling1D(),</pre><pre class="source-code">
    tf.keras.layers.Dense(64, activation='relu'),</pre><pre class="source-code">
    Dense(4, activation='softmax')</pre><pre class="source-code">
])</pre></li>
</ol>
<p>We use a <strong class="source-inline">Conv1D</strong> layer made up of 128 filters (feature detectors) and a kernel size of <strong class="source-inline">5</strong>; this means it will consider five words at a time. Our architecture uses <strong class="source-inline">GlobalMaxPooling1D</strong> to downsample the output of the convolutional layer to the most significant features. We feed the output of the pooling layer into a fully connected layer <span class="No-Break">for classification.</span></p>
<ol>
<li value="8">Then, we build<a id="_idIndexMarker639"/> an <a id="_idIndexMarker640"/><span class="No-Break">LSTM model:</span><pre class="source-code">
# Define the LSTM model</pre><pre class="source-code">
model_lstm = Sequential([</pre><pre class="source-code">
    Embedding(vocab_size, embedding_dim,</pre><pre class="source-code">
        input_length=max_length),</pre><pre class="source-code">
    LSTM(32, return_sequences=True),</pre><pre class="source-code">
    LSTM(32),</pre><pre class="source-code">
    tf.keras.layers.Dense(64, activation='relu'),</pre><pre class="source-code">
    Dense(4, activation='softmax')</pre><pre class="source-code">
])</pre></li>
</ol>
<p>Our LSTM architecture is made up of two LSTM layers made up of 32 units each. In the first LSTM layer, we set <strong class="source-inline">return_sequences</strong> to <strong class="source-inline">True</strong>; this allows the first LSTM layer to pass the complete sequence it received as output to the next LSTM layer. The idea here is to allow the second LSTM layer access to the context of the entire sequence; this equips it with the ability to better understand and capture dependencies across the entire sequence. We then feed the output of the second LSTM layer into the fully connected layers to classify <span class="No-Break">our data.</span></p>
<ol>
<li value="9">For our final model, let’s use a <span class="No-Break">bidirectional LSTM:</span><pre class="source-code">
# Define the BiLSTM model</pre><pre class="source-code">
model_BiLSTM = Sequential([</pre><pre class="source-code">
    Embedding(vocab_size, embedding_dim,</pre><pre class="source-code">
        input_length=max_length),</pre><pre class="source-code">
    Bidirectional(LSTM(32, return_sequences=True)),</pre><pre class="source-code">
    Bidirectional(LSTM(16)),</pre><pre class="source-code">
    tf.keras.layers.Dense(64, activation='relu'),</pre><pre class="source-code">
    Dense(4, activation='softmax')])</pre></li>
</ol>
<p>Here, instead of <a id="_idIndexMarker641"/>the <a id="_idIndexMarker642"/>LSTM layers, two bidirectional LSTM layers are added. Note that the first layer also has <strong class="source-inline">return_sequences=True</strong> to return the full outputs to the next layer. Using a bidirectional wrapper allows each LSTM layer access to both past and future context when processing each element of the input sequence, providing additional contextual information when compared with a <span class="No-Break">unidirectional LSTM.</span></p>
<p>Stacking BiLSTM layers can help us build higher-level representations of the full sequence. The first BiLSTM extracts features by looking at the text from both directions while preserving the entire sequence. The second BiLSTM can then build on those features by further processing them. The final classification is carried out by the output layer in the fully connected layer. Our experimental models are now all set up, so let’s proceed with compiling and fitting <span class="No-Break">them next.</span></p>
<ol>
<li value="10">Let’s compile and fit all the models we have built <span class="No-Break">so far:</span><pre class="source-code">
models = [model_cnn, model_dnn, model_lstm,</pre><pre class="source-code">
    model_BiLSTM]</pre><pre class="source-code">
for model in models:</pre><pre class="source-code">
    model.compile(loss='categorical_crossentropy',</pre><pre class="source-code">
        optimizer='adam', metrics=['accuracy'])</pre><pre class="source-code">
    model.fit(train_sequences, train_labels,epochs=10,</pre><pre class="source-code">
        validation_data=(val_sequences, val_labels),</pre><pre class="source-code">
        verbose=False</pre></li>
</ol>
<p>We use a <strong class="source-inline">for</strong> loop to compile and fit all four models. We set <strong class="source-inline">verbose</strong> to <strong class="source-inline">False</strong>; this way, we don’t print the training information. We train for 10 epochs. Do expect this step to take a while, as we have a massive dataset and are experimenting with <span class="No-Break">four models.</span></p>
<ol>
<li value="11">Let’s evaluate <a id="_idIndexMarker643"/>our <a id="_idIndexMarker644"/>model on <span class="No-Break">unseen data:</span><pre class="source-code">
# Evaluate the model</pre><pre class="source-code">
test_texts = [x[0].numpy().decode(</pre><pre class="source-code">
    'utf-8') for x in test_dataset]</pre><pre class="source-code">
test_sequences = tokenizer.texts_to_sequences(</pre><pre class="source-code">
    test_texts)</pre><pre class="source-code">
test_sequences = pad_sequences(test_sequences,</pre><pre class="source-code">
    padding='post', maxlen=sequences.shape[1])</pre><pre class="source-code">
test_labels = [label.numpy() for _, label in test_dataset]</pre><pre class="source-code">
test_labels = to_categorical(test_labels,</pre><pre class="source-code">
    num_classes=4)</pre><pre class="source-code">
model_names = ["Model_CNN", "Model_DNN", "Model_LSTM",</pre><pre class="source-code">
    "Model_BiLSTM"]</pre><pre class="source-code">
for i, model in enumerate(models):</pre><pre class="source-code">
    loss, accuracy = model.evaluate(test_sequences,</pre><pre class="source-code">
        test_labels)</pre><pre class="source-code">
    print("Model Evaluation -", model_names[i])</pre><pre class="source-code">
    print("Loss:", loss)</pre><pre class="source-code">
    print("Accuracy:", accuracy)</pre><pre class="source-code">
    print()</pre></li>
</ol>
<p>To evaluate our model, we need to prepare our test data in the right fashion. We first extract our text data from our <strong class="source-inline">test_dataset</strong>, after which we tokenize the text using the tokenizer from our training process. The tokenized text is then converted into a sequence of integers, and padding is applied to ensure all sequences are of the same length as the longest sequence in our training <a id="_idIndexMarker645"/>data. Just<a id="_idIndexMarker646"/> like we did during training, we also one-hot-encode our test labels, and then we apply a <strong class="source-inline">for</strong> loop to iterate over each individual model, generating the test loss and accuracy for all our models. The output is <span class="No-Break">as follows:</span></p>
<pre class="source-code">
238/238 [==============================] - 1s 4ms/step - loss: 0.7756 - accuracy: 0.8989
Model Evaluation - Model_CNN
Loss: 0.7755934000015259
Accuracy: 0.8989473581314087
238/238 [==============================] - 1s 2ms/step - loss: 0.7091 - accuracy: 0.8896
Model Evaluation - Model_DNN
Loss: 0.7091193199157715
Accuracy: 0.8896052837371826
238/238 [==============================] - 2s 7ms/step - loss: 0.3211 - accuracy: 0.9008
Model Evaluation - Model_LSTM
Loss: 0.32113003730773926
Accuracy: 0.9007894992828369
238/238 [==============================] - 4s 10ms/step - loss: 0.5618 - accuracy: 0.8916
Model Evaluation - Model_BiLSTM
Loss: 0.5618014335632324
Accuracy: 0.8915789723396301</pre>
<p>From our returned results, we can see that our LSTM model achieved the highest accuracy (90.08%); other models performed quite well too. We can take this performance as a good starting point; we can also apply some of the ideas we used in <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, Handling Overfitting,</em> and <a href="B18118_10.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><em class="italic">, Introduction to Natural Language Processing,</em> to improve our <span class="No-Break">results here.</span></p>
<p>In <a href="B18118_10.xhtml#_idTextAnchor226"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Introduction to Natural Language Processing,</em> we talked about pretrained embeddings. These embeddings are trained on a large corpus of <a id="_idIndexMarker647"/>text<a id="_idIndexMarker648"/> data. Let’s see how we can leverage them; perhaps they can help us achieve a better result in <span class="No-Break">this case.</span></p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor282"/>Using pretrained embeddings</h1>
<p>In <a href="B18118_09.xhtml#_idTextAnchor210"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, <em class="italic">Transfer Learning,</em> we explored <a id="_idIndexMarker649"/>the concept of transfer learning. Here, we will revisit this concept as it relates to word embeddings. In all the models we have built up so far, we trained our word embeddings from scratch. Now, we will examine how to leverage pretrained embeddings that have been trained on massive amounts of text data, such as Word2Vec, GloVe, and FastText. Using these embeddings can be advantageous for <span class="No-Break">two reasons:</span></p>
<ul>
<li>Firstly, they are already trained on a massive and diverse set of data, so they have a rich understanding <span class="No-Break">of language.</span></li>
<li>Secondly, the training process is much faster, since we will skip training our own word embeddings from scratch. Instead, we can build our models on the information packed in these embeddings, focusing on the task <span class="No-Break">at hand.</span></li>
</ul>
<p>It is important to note that using pretrained embeddings isn’t always the right choice. For example, if you work on niche-based text data such as medical or legal data, sectors that apply a lot of domain-specific terminology may be underrepresented. When we use a pretrained embedding blindly for these use cases, they may lead to suboptimal performance. In these types of scenarios, you can either train your own embedding, which comes with an increased computational cost, or use a more balanced approach and fine-tune<a id="_idIndexMarker650"/> pretrained embeddings on your data. Let’s see how we can apply pretrained embeddings in <span class="No-Break">our workflow.</span></p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor283"/>Text classification using pretrained embedding</h2>
<p>To follow this <a id="_idIndexMarker651"/>experiment, you will need to <a id="_idIndexMarker652"/>use the second notebook in this chapter’s GitHub repository called <strong class="source-inline">modelling with pretrained embeddings</strong>. We will continue with the same dataset. This time, we will focus on using our best model with the GloVe pretrained embedding. We will use our best model (LSTM) from our initial round of experiments. <span class="No-Break">Let’s start:</span></p>
<ol>
<li> We will begin by importing the necessary libraries for <span class="No-Break">this experiment:</span><pre class="source-code">
import numpy as np</pre><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
from tensorflow.keras.models import Sequential</pre><pre class="source-code">
from tensorflow.keras.layers import Embedding, LSTM, Dense, Flatten</pre><pre class="source-code">
from tensorflow.keras.preprocessing.text import Tokenizer</pre><pre class="source-code">
from tensorflow.keras.preprocessing.sequence import pad_sequences</pre><pre class="source-code">
import tensorflow_datasets as tfds</pre></li>
</ol>
<p>Once we import our libraries, we will download our <span class="No-Break">pretrained embeddings.</span></p>
<ol>
<li value="2">Run the following commands to download the <span class="No-Break">pretrained embeddings:</span><pre class="source-code">
!wget http://nlp.stanford.edu/data/glove.6B.zip</pre><pre class="source-code">
!unzip glove.6B.zip -d glove.6B</pre></li>
</ol>
<p>We will download the GloVe 6B embedding files from the Stanford NLP website into our Colab notebooks, using the <strong class="source-inline">wget</strong> command. Then, we unzip the compressed <a id="_idIndexMarker653"/>files. We <a id="_idIndexMarker654"/>can see that this file contains different pretrained embeddings, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer164">
<img alt="Figure 11.6 – The directory displaying the GloVe 6B embeddings files" height="484" src="image/B18118_11_006.jpg" width="1273"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – The directory displaying the GloVe 6B embeddings files</p>
<p>The GloVe 6B embedding is made up of a 6 billion-token word embedding, trained by researchers at Stanford University and made publicly available to us all. For computational reasons, we will use the 50-dimensional vectors. You may wish to try out higher dimensions for richer representations, especially when working on tasks that require you to capture more complex semantic relationships, but also be mindful of the <span class="No-Break">compute required.</span></p>
<ol>
<li value="3">Then, we load the AG <span class="No-Break">News dataset:</span><pre class="source-code">
dataset, info = tfds.load('ag_news_subset',</pre><pre class="source-code">
    with_info=True, as_supervised=True)</pre><pre class="source-code">
train_dataset, test_dataset = dataset['train'],</pre><pre class="source-code">
    dataset['test']</pre></li>
</ol>
<p>We load our dataset and split it into training and <span class="No-Break">testing sets.</span></p>
<ol>
<li value="4">Then, we tokenize and sequence our <span class="No-Break">training set:</span><pre class="source-code">
tokenizer = Tokenizer(num_words=20000,</pre><pre class="source-code">
    oov_token="&lt;OOV&gt;")</pre><pre class="source-code">
train_texts = [x[0].numpy().decode(</pre><pre class="source-code">
    'utf-8') for x in train_dataset]</pre><pre class="source-code">
tokenizer.fit_on_texts(train_texts)</pre><pre class="source-code">
train_sequences = tokenizer.texts_to_sequences(</pre><pre class="source-code">
    train_texts)</pre><pre class="source-code">
train_sequences = pad_sequences(train_sequences,</pre><pre class="source-code">
    padding='post')</pre><pre class="source-code">
max_length = train_sequences.shape[1]</pre></li>
</ol>
<p>We prepare our<a id="_idIndexMarker655"/> data<a id="_idIndexMarker656"/> for modeling, just like our last experiment. We set the vocabulary size to 2,000 words and use <strong class="source-inline">OOV</strong> for out-of-vocabulary words. Then, we tokenize and pad the data to ensure consistency in the length <span class="No-Break">of it.</span></p>
<ol>
<li value="5">Then, we process our <span class="No-Break">test data:</span><pre class="source-code">
test_texts = [x[0].numpy().decode(</pre><pre class="source-code">
    'utf-8') for x in test_dataset]</pre><pre class="source-code">
test_sequences = tokenizer.texts_to_sequences(</pre><pre class="source-code">
    test_texts)</pre><pre class="source-code">
test_sequences = pad_sequences(test_sequences,</pre><pre class="source-code">
    padding='post', maxlen=max_length)</pre></li>
</ol>
<p>We process our testing data in a similar fashion to our training data. However, it is important to note that we do not apply <strong class="source-inline">fit_on_texts</strong> to our test set, ensuring that the tokenizer remains the same as the <span class="No-Break">training set.</span></p>
<ol>
<li value="6">Then, we set the <span class="No-Break">embedding parameters:</span><pre class="source-code">
vocab_size = len(tokenizer.word_index) + 1</pre><pre class="source-code">
embedding_dim = 50</pre></li>
</ol>
<p>We define the size of our vocabulary and we set the dimensionality of our embeddings to <strong class="source-inline">50</strong>. We use this because we are working with 50d <span class="No-Break">pretrained embeddings.</span></p>
<ol>
<li value="7">Apply the <a id="_idIndexMarker657"/>pretrained <a id="_idIndexMarker658"/><span class="No-Break">word embeddings:</span><pre class="source-code">
# Download GloVe embeddings and prepare embedding matrix</pre><pre class="source-code">
with open('/content/glove.6B/glove.6B.50d.txt', 'r', encoding='utf-8') as f:</pre><pre class="source-code">
    for line in f:</pre><pre class="source-code">
        values = line.split()</pre><pre class="source-code">
        word = values[0]</pre><pre class="source-code">
        if word in tokenizer.word_index:</pre><pre class="source-code">
            idx = tokenizer.word_index[word]</pre><pre class="source-code">
            embedding_matrix[idx] = np.array(</pre><pre class="source-code">
                values[1:], dtype=np.float32)</pre></li>
</ol>
<p>We access the <strong class="source-inline">glove.6B.50d.txt</strong> file and read it line by line. Each line contains a word and its corresponding embeddings. We cross-match words in the GloVe file with those in our own vocabulary, constructed with the Keras tokenizer. If there is a match, we take the corresponding word index from our own vocabulary and update our initially zero-initialized embedding matrix at that index with the GloVe embeddings. Conversely, words that do not match will remain as zero vectors in the matrix. We will use this embedding matrix to initialize the weight of our embedding layer shortly. We use the file path to the 50d embeddings, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.6</em>. You can do this by right-clicking on the specified file and copying the <span class="No-Break">file path.</span></p>
<ol>
<li value="8">Let’s <a id="_idIndexMarker659"/>build, compile, and<a id="_idIndexMarker660"/> train our <span class="No-Break">LSTM model:</span><pre class="source-code">
model_lstm = Sequential([</pre><pre class="source-code">
    Embedding(vocab_size, embedding_dim,</pre><pre class="source-code">
        input_length=max_length,</pre><pre class="source-code">
        weights=[embedding_matrix], trainable=False),</pre><pre class="source-code">
    LSTM(32, return_sequences=True),</pre><pre class="source-code">
    LSTM(32),</pre><pre class="source-code">
    Dense(64, activation='relu'),</pre><pre class="source-code">
    Dense(4, activation='softmax')</pre><pre class="source-code">
])</pre><pre class="source-code">
model_lstm.compile(optimizer='adam',</pre><pre class="source-code">
    loss='categorical_crossentropy',</pre><pre class="source-code">
    metrics=['accuracy'])</pre><pre class="source-code">
# Convert labels to one-hot encoding</pre><pre class="source-code">
train_labels = tf.keras.utils.to_categorical(</pre><pre class="source-code">
    [label.numpy() for _, label in train_dataset])</pre><pre class="source-code">
test_labels = tf.keras.utils.to_categorical(</pre><pre class="source-code">
    [label.numpy() for _, label in test_dataset])</pre><pre class="source-code">
model_lstm.fit(train_sequences, train_labels,</pre><pre class="source-code">
    epochs=10, validation_split=0.2)</pre></li>
</ol>
<p>While building our model, the key difference from our previous experiment is the embedding layer initialization. Here, we leverage our pretrained embedding matrix, and to ensure the weights remain unchanged, we set the trainable parameter to <strong class="source-inline">false</strong>. Everything else in our model’s architecture remains the same. Then, we compile and fit our model for <span class="No-Break">10 epochs.</span></p>
<ol>
<li value="9">Finally, we evaluate <span class="No-Break">our model:</span><pre class="source-code">
loss, accuracy = model_lstm.evaluate(test_sequences,</pre><pre class="source-code">
    test_labels)</pre><pre class="source-code">
print("Loss:", loss)</pre><pre class="source-code">
print("Accuracy:", accuracy)</pre></li>
</ol>
<p>We reached an accuracy of 89% on our test set, although we did not outperform our best model when we didn’t apply pretrained embeddings. Perhaps you may want to try out<a id="_idIndexMarker661"/> larger <a id="_idIndexMarker662"/>embedding dimensions from <strong class="source-inline">glove6B</strong> or other word embeddings to improve our result here. That would be a good exercise and is <span class="No-Break">well encouraged.</span></p>
<p>Now, it is time to move on to another exciting topic – text generation <span class="No-Break">with LSTM.</span></p>
<h1 id="_idParaDest-203"><a id="_idTextAnchor284"/>Using LSTMs to generate text</h1>
<p>We have explored <a id="_idIndexMarker663"/>LSTMs in text <a id="_idIndexMarker664"/>classification. Now, we will look at how to generate text that you would see in a novel, blog post, or children’s storybook, ensuring that is coherent and consistent with what we expect from these types of texts. LSTMs prove useful here, due to their ability to capture and remember intricate patterns for long sequences. When we train an LSTM on a large volume of text data, we allow it to learn the linguistic structure, style, and nuances. It can apply this to generate new sentences in line with the style and approach of the <span class="No-Break">training set.</span></p>
<p>Let’s imagine we are playing a word prediction game with our friends. The goal is to coin a story in which each friend comes up with a word to continue the story. To begin, we have a set of words, which we will call the seed, to set the tone for our story. From the seed sentence, each friend contributes a subsequent word until we have a complete story. We can also apply this idea to LSTMs – we feed a seed sentence into our model, and then we ask it to predict the next word, just like our word prediction game. This time, however, the game is played only by the LSTM, and we just need to specify the number of words it will play for. The result of each round of the game will serve as input to the LSTM for the next round, until we achieve our specified number <span class="No-Break">of words.</span></p>
<p>The question that comes to mind is, how does an LSTM know what word to predict next? This is where the <a id="_idIndexMarker665"/>concept of <strong class="bold">windowing</strong> comes into play. Let’s say we have a sample <a id="_idIndexMarker666"/>sentence<a id="_idIndexMarker667"/> such as “<em class="italic">I once had a dog called Jack.</em>” When we apply windowing with a window size of four to this sentence, we will have <span class="No-Break">the following:</span></p>
<ul>
<li>“<em class="italic">I once </em><span class="No-Break"><em class="italic">had a</em></span><span class="No-Break">”</span></li>
<li>“<em class="italic">once had </em><span class="No-Break"><em class="italic">a dog</em></span><span class="No-Break">”</span></li>
<li>“<em class="italic">had a </em><span class="No-Break"><em class="italic">dog called</em></span><span class="No-Break">”</span></li>
<li>“<em class="italic">a dog </em><span class="No-Break"><em class="italic">called Jack</em></span><span class="No-Break">”</span></li>
</ul>
<p>We can now split each of these sentences into input-output pairs. For example, in the first sentence, we will have “<em class="italic">I once had</em>” as the input, and the output will be “<em class="italic">a</em>.” We will apply the same approach to all the other sentences, thus giving us the following <span class="No-Break">input-output pairs:</span></p>
<ul>
<li>([“<em class="italic">I</em>”, “<em class="italic">once</em>”, “<span class="No-Break"><em class="italic">had</em></span><span class="No-Break">”], “</span><span class="No-Break"><em class="italic">a</em></span><span class="No-Break">”)</span></li>
<li>([“<em class="italic">once</em>”, “<em class="italic">had</em>”, “<span class="No-Break"><em class="italic">a</em></span><span class="No-Break">”], “</span><span class="No-Break"><em class="italic">dog</em></span><span class="No-Break">”)</span></li>
<li>([“<em class="italic">had</em>”, “<em class="italic">a</em>”, “<span class="No-Break"><em class="italic">dog</em></span><span class="No-Break">”], “</span><span class="No-Break"><em class="italic">called</em></span><span class="No-Break">”)</span></li>
<li>([“<em class="italic">a</em>”, “<em class="italic">dog</em>”, “<span class="No-Break"><em class="italic">called</em></span><span class="No-Break">”], “</span><span class="No-Break"><em class="italic">Jack</em></span><span class="No-Break">”)</span></li>
</ul>
<p>By using windowing, our LSTM focuses on the most recent set of words, which often holds the most relevant information to predict the next word in our text. Also, when we work with smaller, fixed-length sequences, it streamlines our training process and also optimizes our <a id="_idIndexMarker668"/>memory<a id="_idIndexMarker669"/> usage. Let’s see how we can apply this idea in our next <span class="No-Break">case study.</span></p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor285"/>Story generation using LSTMs</h2>
<p>In this case study, imagine <a id="_idIndexMarker670"/>you are a new NLP engineer working for a London-based start-up called Readrly. Your job is to build an AI storyteller for the company. You have been provided with a training dataset called <strong class="source-inline">stories.txt</strong>, which contains 30 sample stories. Your job is to train an LSTM to generate exciting children’s stories. Let’s return to our notebook and see how to make <span class="No-Break">this happen:</span></p>
<ol>
<li>As we did previously, we will begin by importing all the libraries required for <span class="No-Break">this task:</span><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
from tensorflow.keras.preprocessing.text import Tokenizer</pre><pre class="source-code">
from tensorflow.keras.preprocessing.sequence import pad_sequences</pre><pre class="source-code">
from tensorflow.keras.models import Sequential</pre><pre class="source-code">
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional</pre><pre class="source-code">
import numpy as np</pre></li>
<li>Then, we load the <span class="No-Break"><strong class="source-inline">stories.txt</strong></span><span class="No-Break"> dataset:</span><pre class="source-code">
text = open('stories.txt').read().lower()</pre></li>
</ol>
<p>We read our story file and convert all the contents to lowercase to ensure our data is in a consistent form, avoiding duplicate tokens generated from capitalized and non-capitalized versions of the <span class="No-Break">same word.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">This step helps us to reduce the vocabulary size by removing semantic nuances created by capitalization. However, this step should be carried out judiciously, as it may have a negative effect – for example, when we use the word “<em class="italic">march</em>.” If we lowercase the word, it will denote some form of walking, while with a capital M, it refers to the month of <span class="No-Break">the year.</span></p>
<ol>
<li value="3">Tokenize <span class="No-Break">the text:</span><pre class="source-code">
tokenizer = Tokenizer()</pre><pre class="source-code">
tokenizer.fit_on_texts([text])</pre><pre class="source-code">
total_words = len(tokenizer.word_index) + 1</pre></li>
</ol>
<p>We calculate the total number of unique words in our vocabulary. We add one to account for <span class="No-Break">out-of-vocabulary words.</span></p>
<ol>
<li value="4">Then, we <a id="_idIndexMarker671"/>convert the text <span class="No-Break">to sequences:</span><pre class="source-code">
input_sequences = []</pre><pre class="source-code">
for line in text.split('\n'):</pre><pre class="source-code">
    token_list = tokenizer.texts_to_sequences(</pre><pre class="source-code">
        [line])[0]</pre><pre class="source-code">
    for i in range(1, len(token_list)):</pre><pre class="source-code">
        n_gram_sequence = token_list[:i+1]</pre><pre class="source-code">
        input_sequences.append(n_gram_sequence)</pre></li>
</ol>
<p>In this step, we develop a dataset made up of <em class="italic">n</em>-gram sequences, where each entry in the “input sequence” is a sequence of words (that is, word numbers) that appear in the text. For every sequence of <em class="italic">n</em> words, <em class="italic">n</em>-1 words will be our input features, and the <em class="italic">n</em>-th word is the label that our model tries <span class="No-Break">to predict.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">An n-gram is a sequence of n words from a <span class="No-Break">given text.</span></p>
<p class="callout">For example, let’s say we have a sample sentence such as “<em class="italic">The dog played with the cat.</em>” Using a 2-gram (bigram) model on a sentence would split it into the <span class="No-Break">following bigrams:</span></p>
<p class="callout"> (“<em class="italic">The</em>”, “<em class="italic">dog</em>”), (“<em class="italic">dog</em>”, “<em class="italic">played</em>”), (“<em class="italic">played</em>”, “<em class="italic">with</em>”), (“<em class="italic">with</em>”, “<em class="italic">the</em>”), (“<span class="No-Break"><em class="italic">the</em></span><span class="No-Break">”, “</span><span class="No-Break"><em class="italic">cat</em></span><span class="No-Break">”)</span></p>
<p class="callout">Alternatively, if we use a 3-gram (trigram) model, it would split the sentence <span class="No-Break">into trigrams:</span></p>
<p class="callout"> (“<em class="italic">The</em>”, “<em class="italic">dog</em>”, “<em class="italic">played</em>”), (“<em class="italic">dog</em>”, “<em class="italic">played</em>”, “<em class="italic">with</em>”), (“<em class="italic">played</em>”, “<em class="italic">with</em>”, “<em class="italic">the</em>”), (“<em class="italic">with</em>”, “<span class="No-Break"><em class="italic">the</em></span><span class="No-Break">”, “</span><span class="No-Break"><em class="italic">cat</em></span><span class="No-Break">”)</span></p>
<p class="callout"><em class="italic">N</em>-gram models are common in NLP for text prediction, spelling correction, language modeling, and <span class="No-Break">feature extraction.</span></p>
<ol>
<li value="5">Then, we<a id="_idIndexMarker672"/> pad <span class="No-Break">the sequences:</span><pre class="source-code">
max_sequence_len = max([len(x) for x in input_sequences])</pre><pre class="source-code">
input_sequences = np.array(pad_sequences(</pre><pre class="source-code">
    input_sequences, maxlen=max_sequence_len,</pre><pre class="source-code">
    padding='pre'))</pre></li>
</ol>
<p>We use padding to ensure our input data is of a consistent format. We set <strong class="source-inline">padding</strong> to <strong class="source-inline">pre</strong> to ensure that our LSTM captures the most recent words at the end of each sequence; these words are relevant to predict the next word in <span class="No-Break">the sequence.</span></p>
<ol>
<li value="6">We now split the sequences into features <span class="No-Break">and labels:</span><pre class="source-code">
predictors, label = input_sequences[:,:-1],</pre><pre class="source-code">
    input_sequences[:,-1]</pre><pre class="source-code">
label = tf.keras.utils.to_categorical(label,</pre><pre class="source-code">
    num_classes=total_words)</pre></li>
</ol>
<p>Here, we one-hot-encode our labels to represent them as vectors. Then, we build <span class="No-Break">our model.</span></p>
<ol>
<li value="7">Create <span class="No-Break">the model:</span><pre class="source-code">
model = Sequential([</pre><pre class="source-code">
    Embedding(total_words, 200,</pre><pre class="source-code">
        input_length=max_sequence_len-1),</pre><pre class="source-code">
    Bidirectional(LSTM(200)),</pre><pre class="source-code">
    Dense(total_words, activation='softmax')</pre><pre class="source-code">
])</pre><pre class="source-code">
model.compile(loss='categorical_crossentropy',</pre><pre class="source-code">
    optimizer='adam', metrics=['accuracy'])</pre><pre class="source-code">
history = model.fit(predictors, label, epochs=300,</pre><pre class="source-code">
    verbose=0)</pre></li>
</ol>
<p>We build a text<a id="_idIndexMarker673"/> generation model using a bidirectional LSTM, as it has the ability to capture both past and future data points. Our embedding layer transforms each word’s numerical representation into a dense vector, with a dimension of 200 each. The next layer is the bidirectional LSTM layer with 200 units, after which we feed the output data into the dense layers. Then, we compile and fit the model for <span class="No-Break">300 epochs.</span></p>
<ol>
<li value="8">Create a function to make <span class="No-Break">the predictions:</span><pre class="source-code">
def generate_text(seed_text, next_words, model, max_sequence_len):</pre><pre class="source-code">
    for _ in range(next_words):</pre><pre class="source-code">
        token_list = tokenizer.texts_to_sequences(</pre><pre class="source-code">
            [seed_text])[0]</pre><pre class="source-code">
        token_list = pad_sequences([token_list],</pre><pre class="source-code">
            maxlen=max_sequence_len-1, padding='pre')</pre><pre class="source-code">
        # Get the predictions</pre><pre class="source-code">
        predictions = model.predict(token_list)</pre><pre class="source-code">
        # Get the index with the maximum prediction value</pre><pre class="source-code">
        predicted = np.argmax(predictions)</pre><pre class="source-code">
        output_word = ""</pre><pre class="source-code">
        for word,index in tokenizer.word_index.items():</pre><pre class="source-code">
            if index == predicted:</pre><pre class="source-code">
                output_word = word</pre><pre class="source-code">
                break</pre><pre class="source-code">
        seed_text += " " + output_word</pre><pre class="source-code">
    return seed_text</pre></li>
</ol>
<p>We construct <a id="_idIndexMarker674"/>the <strong class="source-inline">story_generator</strong> function. We start with the seed text, which we use to prompt the model to generate more text for our children’s stories. The seed text is transformed into its tokenized form, after which it is padded at the beginning to match the expected length of the input sequence, <strong class="source-inline">max_sequence_len-1</strong>. To predict the next word’s token, we use the <strong class="source-inline">predict</strong> method and apply <strong class="source-inline">np.argmax</strong> to select the most likely next word. The predicted token is then mapped back to its corresponding word, which is appended to the existing seed text. The process is repeated until we achieve the desired number of words (<strong class="source-inline">next_words</strong>), and the function returns the fully generated text (<strong class="source-inline">seed_text + </strong><span class="No-Break"><strong class="source-inline">next_words</strong></span><span class="No-Break">)</span></p>
<ol>
<li value="9">Let’s generate <span class="No-Break">the text:</span><pre class="source-code">
input_text= "In the hustle and bustle of ipoti"</pre><pre class="source-code">
print(generate_text(input_text, 50, model,</pre><pre class="source-code">
    max_sequence_len))</pre></li>
</ol>
<p>We define the seed text, which is the <strong class="source-inline">input_text</strong> variable here. Our choice of next words we want the model to generate is <strong class="source-inline">50</strong>, and we pass in our trained model and also <strong class="source-inline">max_sequence_len</strong>. When we run the code, it returns the <span class="No-Break">following output:</span></p>
<pre class="source-code">
In the hustle and bustle of ipoti the city the friends also learned about the wider context of the ancient world including the people who had lived and worshipped in the area they explored nearby archaeological sites and museums uncovering artifacts and stories that shed light on the lives and beliefs of those who had come before</pre>
<p>The sample of generated text indeed resembles something you might find in a children’s storybook. It continues the initial prompt coherently and creatively. While this <a id="_idIndexMarker675"/>example showcases the power of LSTMs as text generators, in this era, we leverage <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) for <a id="_idIndexMarker676"/>such applications. These models are trained on massive datasets, and they have a more sophisticated understanding of language; hence, they can <a id="_idTextAnchor286"/>generate more compelling stories when we prompt or fine-tune <span class="No-Break">them properly.</span></p>
<p>We are now at the end of this chapter on NLP. You should now have the requisite foundational ideas to build your own NLP projects with TensorFlow. All you have learned here will also help you effectively navigate the NLP section of the TensorFlow Developer certificate exam. You have come a long way, and you should give yourself credit. We have <a id="_idIndexMarker677"/>one more chapter to go. Before we proceed to the chapter on time series, let's summarize what we learned in <span class="No-Break">this chapter.</span></p>
<h1 id="_idParaDest-205"><a id="_idTextAnchor287"/>Summary</h1>
<p>In this chapter, we embarked on a voyage through the world of RNNs. We began by looking at the anatomy of RNNs and their variants, and we explored the task of classifying news articles using different model architectures. We took a step further by applying pretrained word embeddings to our best-performing model in our quest to improve it. Here, we learned how to apply pretrained word embeddings in our workflow. For our final challenge, we took on the task of building a text generator to generate <span class="No-Break">children’s stories.</span></p>
<p>In the next chapter, we will examine time series, explore its unique characteristics, and uncover various methods of building forecasting models. We will tackle a time-series problem, where we will master how to prepare, train, and evaluate <span class="No-Break">time-series data.</span></p>
<h1 id="_idParaDest-206"><a id="_idTextAnchor288"/>Questions</h1>
<p>Let’s test what we learned in <span class="No-Break">this chapter:</span></p>
<ol>
<li>Load the IMDB movies review dataset from TensorFlow and <span class="No-Break">preprocess it.</span></li>
<li>Build a CNN <span class="No-Break">movie classifier.</span></li>
<li>Build an LSTM <span class="No-Break">movie classifier.</span></li>
<li>Use the GloVe 6B embedding to apply a pretrained word embedding to <span class="No-Break">LSTM architecture.</span></li>
<li>Evaluate all the models, and save your <span class="No-Break">best-performing one.</span></li>
</ol>
<h1 id="_idParaDest-207"><a id="_idTextAnchor289"/>Further reading</h1>
<p>To learn more, you can check out the <span class="No-Break">following resources:</span></p>
<ul>
<li>Cho, K., et al. (2014). <em class="italic">Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation</em>. arXiv <span class="No-Break">preprint arXiv:1406.1078.</span></li>
<li>Hochreiter, S., &amp; Schmidhuber, J. (1997). <em class="italic">Long short-term memory</em>. Neural computation, <span class="No-Break">9(8), 1735–1780.</span></li>
<li>Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <em class="italic">Efficient Estimation of Word Representations in Vector Space</em>. arXiv <span class="No-Break">preprint arXiv:1301.3781.</span></li>
<li><em class="italic">The Unreasonable Effectiveness of Recurrent Neural </em><span class="No-Break"><em class="italic">Networks</em></span><span class="No-Break">: </span><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><span class="No-Break">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</span></a><span class="No-Break">.</span></li>
</ul>
</div>
</div>

<div id="sbo-rt-content"><div class="Content" id="_idContainer166">
<h1 id="_idParaDest-208" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor290"/>Part 4 – Time Series with TensorFlow</h1>
<p>In this part, you will learn to build time series forecasting applications with TensorFlow. You will understand how to perform preprocess and build models for time series data. In this part, you will also learn to generate forecast for <span class="No-Break">time series.</span></p>
<p>This section comprises the <span class="No-Break">following chapters:</span></p>
<ul>
<li><a href="B18118_12.xhtml#_idTextAnchor291"><em class="italic">Chapter 12</em></a>, <em class="italic">Introduction to Time Series, Sequences, and Predictions</em></li>
<li><a href="B18118_13.xhtml#_idTextAnchor318"><em class="italic">Chapter 13</em></a>, <em class="italic">Time Series, Sequence and Prediction with TensorFlow</em></li>
</ul>
</div>
<div>
<div id="_idContainer167">
</div>
</div>
<div>
<div id="_idContainer168">
</div>
</div>
<div>
<div id="_idContainer169">
</div>
</div>
<div>
<div id="_idContainer170">
</div>
</div>
<div>
<div id="_idContainer171">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer172">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer173">
</div>
</div>
<div>
<div id="_idContainer174">
</div>
</div>
<div>
<div id="_idContainer175">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer176">
</div>
</div>
</div></body></html>