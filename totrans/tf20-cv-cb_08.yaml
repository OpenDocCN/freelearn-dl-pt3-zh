- en: '*Chapter 8*: Fine-Grained Understanding of Images through Segmentation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image segmentation is one of the biggest areas of study in computer vision.
    It consists of simplifying the visual contents of an image by grouping together
    pixels that share one or more defining characteristics, such as location, color,
    or texture. As is the case with many other subareas of computer vision, image
    segmentation has been greatly boosted by deep neural networks, mainly in industries
    such as medicine and autonomous driving.
  prefs: []
  type: TYPE_NORMAL
- en: While it's great to classify the contents of an image, more often than not,
    it's not enough. What if we want to know exactly where an object is? What if we're
    interested in its shape? What if we need its contour? These fine-grained needs
    cannot be met with traditional classification techniques. However, as we'll discover
    in this chapter, we can frame an image segmentation problem in a very similar
    way to a regular classification project. How? Instead of labeling the image as
    a whole, we'll label each pixel! This is known as image segmentation and is what
    constitutes the recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a fully convolutional network for image segmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a U-Net from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a U-Net with transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmenting images using Mask-RCNN and TensorFlow Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to implement and experiment with the recipes in this chapter, it''s
    recommended that you have access to a GPU. If you have recourse to a cloud-based
    provider, such as AWS or FloydHub, that''s great, but keep the fees attached to
    them in mind as they might skyrocket if you''re not careful! In the *Getting ready*
    section of each recipe, you''ll find everything you''ll need to prepare for what
    lies ahead. The code for this chapter is available here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch8](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch8).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/2Na77IF](https://bit.ly/2Na77IF).'
  prefs: []
  type: TYPE_NORMAL
- en: Creating a fully convolutional network for image segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you were to create your first network for image segmentation while knowing
    that, at its core, segmenting is just pixel-wise classification, what would you
    do? You would probably take a battle-tested architecture and swap the final layers
    (usually fully connected ones) with convolutions in order to produce an output
    volume, instead of an output vector.
  prefs: []
  type: TYPE_NORMAL
- en: Well, that's exactly what we'll do in this recipe to build a **Fully Convolutional
    Network** (**FCN**) for image segmentation based on the famous **VGG16** network.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to install a couple of external libraries, starting with `tensorflow_docs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we need to install TensorFlow Datasets, `Pillow`, and `OpenCV`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Regarding the data, we will segment images from `the Oxford-IIIT Pet` dataset.
    The good news is that we''ll access it using `tensorflow-datasets`, so we don''t
    really need to do anything in that respect here. Each pixel in this dataset is
    classified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '1: The pixel belongs to a pet (cat or dog).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: The pixel belongs to the contour of a pet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3: The pixel belongs to the surroundings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some sample images from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Sample images from the Oxford-IIIT Pet dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.1 – Sample images from the Oxford-IIIT Pet dataset
  prefs: []
  type: TYPE_NORMAL
- en: Let's start implementing!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will normalize the images in the dataset to the range
    [0, 1]. Just for consistency''s sake, we''ll subtract one from each pixel in the
    mask so that they go from 0 all the way to 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `load_image()` function, which loads both the image and its mask,
    given a TensorFlow dataset element. We will seize the opportunity to resize the
    images to *256x256* here. Also, if the `train` flag is set to `True`, we can perform
    a bit of augmentation by randomly mirroring the image and its mask. Lastly, we
    must normalize the inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Implement the `FCN()` class, which encapsulates all the logic required to build,
    train, and evaluate our `RMSProp` as the optimizer and `SparseCategoricalCrossentropy`
    as the loss. Notice that `output_channels` is, by default, 3, because each pixel
    can be categorized into one of three classes. Also, notice that we are defining
    the path to the weights of the **VGG16** this model is based on. We'll use these
    weights to give our network a head start when training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, it''s time to define the architecture itself:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We started by defining the input and the first block of convolutions and max
    pooling layers. Now, define the second block of convolutions, this time with 128
    filters each:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The third block contains convolutions with 256 filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The fourth block uses convolutions with 512 filters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The fifth block is a repetition of block four, again with 512 filter-deep convolutions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The reason we''ve been naming the layers so far is so that we can match them
    with the pre-trained weights we''ll import next (notice `by_name=True`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`output`, in a traditional **VGG16** architecture, is comprised of fully connected
    layers. However, we''ll be replacing them with transposed convolutions. Notice
    we are connecting these layers to the output of the fifth block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a 1x1 convolution, followed by a transposed convolution, and connect
    it to the output of the fourth block (this is, indeed, a skip connection):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the output of the third block through a 1x1 convolution. Then, merge these
    three paths into one and pass them through a final transposed convolution. This
    will be activated with `Softmax`. This output constitutes the segmentation mask
    predicted by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s create a private helper method to plot the relevant training curves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `train()` method takes the training and validation datasets, as well as
    the number of epochs and training and validation steps to perform, in order to
    fit the model. It also saves the loss and accuracy plots to disk for later analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement `_process_mask()`, which is used to make the segmentation masks compatible
    with OpenCV. What this function does is create a three-channeled version of a
    grayscale mask and upscale the class values to the [0, 255] range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_save_image_and_masks()` helper method creates a mosaic of the original
    image, the ground truth mask, and the predicted segmentation mask, and then saves
    it to disk for later revision:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to pass the output volume produced by the network to a valid segmentation
    mask, we must take the index with the highest value at each pixel location. This
    corresponds to the most likely category for that pixel. The `_create_mask()` method
    does this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_save_predictions()` method uses the `_save_image_and_mask()` helper method,
    which we defined in *Step 18*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `evaluate()` method computes the accuracy of the **FCN** on the test set
    and generates predictions for a sample of images, which are then stored on disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download (or load, if cached) `Oxford IIIT Pet Dataset`, along with its metadata,
    using **TensorFlow Datasets**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the metadata to define the corresponding number of steps the network will
    take over the training and validation datasets. Also, define the batch and buffer
    sizes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the training and testing datasets'' pipelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the **FCN** and train it for 120 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, evaluate the network on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As shown in the following graph, the accuracy on the test set should be around
    84% (specifically, I got 84.47%):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Training and validation accuracy curves'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.2 – Training and validation accuracy curves
  prefs: []
  type: TYPE_NORMAL
- en: 'The training curves display a healthy behavior, meaning that the network did,
    indeed, learn. However, the true test is to visually assess the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.3 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding image, we can see that the mask that was produced by the network
    follows the shape of the ground truth segmentation. However, there''s an unsatisfying
    pixelated effect across the segments, as well as noise in the upper-right corner.
    Let''s take a look at another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Image86700.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.4 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding image, we can see a very deficient, spotty, and overall low-quality
    mask that proves that the network still needs a lot of improvement. This could
    be achieved by doing more fine-tuning and experimentation. However, in the next
    recipe, we'll discover a network that's best suited to performing image segmentation
    and capable of producing a really good mask with way less effort.
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss what we've just done in the *How it works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented an **FCN** for image segmentation. Even though
    we adapted a well-known architecture, **VGG16**, to our purposes, in reality,
    there are many different adaptations of **FCNs** that extend or modify other seminal
    architectures, such as **ResNet50**, **DenseNet**, and other variants of **VGG**.
  prefs: []
  type: TYPE_NORMAL
- en: What we need to remember is that `UpSampling2D()` with bilinear interpolation
    or `ConvTranspose2D()`). The achieved result is that instead of classifying the
    whole image with an output vector of probabilities, we produce an output volume
    that has the same dimensions as the input image, where each pixel contains a probability
    distribution of the classes it can belong to. Such an output volume of pixel-wise
    likelihood is known as a predicted segmentation mask.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about `Oxford IIIT Pet Dataset`, visit the official site
    here: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a U-Net from scratch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It's difficult to talk about image segmentation without mentioning **U-Net**,
    one of the seminal architectures when it comes to pixel-wise classification.
  prefs: []
  type: TYPE_NORMAL
- en: A **U-Net** is a composite network comprised of an encoder and a decoder, whose
    layers, as the name suggests, are arranged in a U shape. It's intended for fast
    and precise segmentation, and in this recipe, we'll implement one from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we''ll rely on several external libraries, such as TensorFlow
    Datasets, TensorFlow Docs, `Pillow`, and `OpenCV`. The good news is that we can
    easily install them all with `pip`. First, install `tensorflow_docs`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, install the remaining libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We will be using `Oxford-IIIT Pet Dataset` in this recipe. However, we don''t
    need to do anything at this stage since we''ll download it and manipulate it using
    `tensorflow-datasets`. In this dataset, the segmentation mask (an image where
    each location contains the class of the corresponding pixel in the original image)
    contains pixels categorized into three classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '1: The pixel belongs to a pet (cat or dog).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: The pixel belongs to the contour of a pet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3: The pixel belongs to the surroundings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some sample images from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Sample images from the Oxford-IIIT Pet dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.5 – Sample images from the Oxford-IIIT Pet dataset
  prefs: []
  type: TYPE_NORMAL
- en: Great! Let's start implementing!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to implement your own **U-Net** so that you can segment
    images of your own pets:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import all the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will normalize the images in the dataset. We must also
    normalize the masks so that the classes are numbered from 0 through 2, instead
    of from 1 through 3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load an image, given an element from a TensorFlow
    dataset data structure. Note that we resize both the image and the mask to *256x256*.
    Also, if the `train` flag is set to `True`, we perform augmentation by randomly
    mirroring the image and its mask. Finally, we normalize the inputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, let's define a class, `UNet()`, that will contain all the logic necessary
    to build, train, and evaluate our `RMSProp` as the optimizer and `SparseCategoricalCrossentropy`
    as the loss. Note that `output_channels` is, by default, `3`, because each pixel
    can be categorized into one of three classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s define the `_downsample()` helper method, which builds a downsampling
    block. This is a convolution that can be (optionally) batch normalized and that''s
    activated with `LeakyReLU`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Conversely, the `_upsample()` helper method expands its input through a transposed
    convolution, which is also batch normalized and `ReLU` activated (optionally,
    we can add a dropout layer to prevent overfitting):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Armed with `_downsample()` and `_upsample()`, we can iteratively build the
    full **U-Net** architecture. The encoding part of the network is just a stack
    of downsampling blocks, while the decoding portion is, as expected, comprised
    of a series of upsampling blocks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to shield the network against the vanishing gradient problem (a phenomenon
    where very deep networks forget what they''ve learned), we must add skip connections
    at every level:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output layer of the **U-Net** is a transposed convolution whose dimensions
    are the same as the input image''s, but it has as many channels as there are classes
    in the segmentation mask:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s define a `helper` method in order to plot the relevant training curves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `train()` method takes the training and validation datasets, as well as
    the number of epochs and training and validation steps to perform, in order to
    fit the model. It also saves the loss and accuracy plots to disk for later analysis:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a helper method named `_process_mask()`, which will be used to make
    the segmentation masks compatible with OpenCV. What this function does is create
    a three-channeled version of a grayscale mask and upscale the class values to
    the [0, 255] range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_save_image_and_masks()` helper method creates a mosaic of the original
    image, the ground truth mask, and the predicted segmentation mask, and saves it
    to disk for later revision:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to pass the output volume produced by the network to a valid segmentation
    mask, we must take the index of the highest value at each pixel location, which
    corresponds to the most likely category for that pixel. The `_create_mask()` method
    does this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_save_predictions()` method uses the `_save_image_and_mask()` helper method
    we defined in *Step 13*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `evaluate()` method computes the accuracy of the **U-Net** on the test
    set, and also generates predictions for a sample of images, which are then stored
    on disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download (or load, if cached) `Oxford IIIT Pet Dataset`, along with its metadata,
    using TensorFlow Datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the metadata to define the corresponding number of steps the network will
    go over for the training and validation datasets. Also, define the batch and buffer
    sizes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the training and testing datasets'' pipelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the **U-Net** and train it for 50 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Lastly, evaluate the network on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy on the test set should be around 83% (in my case, I got 83.49%):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Training and validation accuracy curves'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.6 – Training and validation accuracy curves
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that after about epoch 12, the gap between the training and
    validation accuracy curves slowly widens. This isn't a sign of overfitting, but
    an indication that we could do better. How does this accuracy translate to actual
    images?
  prefs: []
  type: TYPE_NORMAL
- en: 'Take a look at the following image, which shows the original image, the ground
    truth mask, and the produced mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.7 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that there''s a good resemblance between the ground truth
    mask (center) and the predicted one (right), although there is some noise, such
    as the small white region and the pronounced bump on the lower half of the dog''s
    silhouette, that could be cleaned up with more training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.8 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  prefs: []
  type: TYPE_NORMAL
- en: The preceding image clearly shows that the network could use more training or
    fine-tuning. This is because even though it gets the overall shape and location
    of the dog right, there's really too much noise for this mask to be usable in
    a real-world application.
  prefs: []
  type: TYPE_NORMAL
- en: Let's head over to the *How it works…* section to connect the dots.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented and trained a **U-Net** from scratch to segment
    the body and contour of household pets. As we saw, the network did learn, but
    still offers room for improvement.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to semantically segment the contents of an image is of paramount
    importance in several domains, such as in medicine, where what's more important
    than knowing if a condition, such as a malignant tumor, is present, is to determine
    the actual location, shape, and area of said ailment. The field of biomedicine
    is where **U-Net** made its debut. In 2015, it outperformed established methods
    for segmentation, such as sliding-windows convolutional networks, using far less
    data.
  prefs: []
  type: TYPE_NORMAL
- en: How does **U-Net** achieve such good results? As we learned in this recipe,
    the key is in its end-to-end nature, where both the encoder and decoder are comprised
    of convolutions that form a contracting path, whose job is to capture context
    and a symmetric expanding path, thereby enabling precise localization.
  prefs: []
  type: TYPE_NORMAL
- en: Both of the aforementioned paths can be as deep as needed, depending on the
    nature of the dataset. This depth customization is viable due to the presence
    of skip connections, which allow the gradients to flow farther down the network,
    thus preventing the vanishing gradient problem (this is similar to what **ResNet**
    does, as we learned in [*Chapter 2*](B14768_02_Final_JM_ePub.xhtml#_idTextAnchor053),
    *Performing Image Classification*).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next recipe, we''ll combine a very powerful concept with this implementation
    of `Oxford IIIT Pet Dataset`: transfer learning.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A great way to familiarize yourself with `Oxford IIIT Pet Dataset`, visit the
    official site here: [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we mentioned the vanishing gradient problem a few times, so
    it''s a good idea to understand the concept by reading this article: [https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a U-Net with transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a **U-Net** from scratch is a very good first step toward creating
    a performant image segmentation system. However, one of the biggest superpowers
    in deep learning that's applied to computer vision is being able to build solutions
    on top of the knowledge of other networks, which usually leads to faster and better
    results.
  prefs: []
  type: TYPE_NORMAL
- en: Image segmentation is no exception to this rule, and in this recipe, we'll implement
    a better segmentation network using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This recipe is very similar to the previous one (*Implementing a U-Net from
    scratch*), so we''ll only go into depth on the parts that are different. For a
    deeper explanation, I recommend that you complete the *Implementing a U-Net from
    scratch* recipe before attempting this one. As expected, the libraries we''ll
    need are the same as they were for that recipe, all of which can be installed
    using `pip`. Let''s start with `tensorflow_docs`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s set up the remaining dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we''ll work with `Oxford-IIIT Pet Dataset`, which can be accessed
    through `tensorflow-datasets`. Each pixel in this dataset falls within one of
    these classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '1: The pixel belongs to a pet (cat or dog).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2: The pixel belongs to the contour of a pet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3: The pixel belongs to the surroundings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following image shows two sample images from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – Sample images from the Oxford-IIIT Pet dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.9 – Sample images from the Oxford-IIIT Pet dataset
  prefs: []
  type: TYPE_NORMAL
- en: With that, we are good to go!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Complete these steps to implement a transfer learning-powered **U-Net**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the needed packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define an alias for `tf.data.experimental.AUTOTUNE`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will normalize the images and masks in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load an image and its corresponding mask, given
    an element from a TensorFlow Datasets data structure. Optionally, it should perform
    image mirroring on training images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Define `UNet()`, a container class for the logic necessary to build, train,
    and evaluate our transfer learning-aided `RMSProp` as the optimizer and `SparseCategoricalCrossentropy`
    as the loss. Notice that `output_channels` is, by default, `3`, because each pixel
    can be categorized into one of three classes. The encoder will be a pre-trained
    `MobileNetV2`. However, we'll only use a select group of layers, defined in `self.target_layers`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s define the `_upsample()` helper method, which builds an upsampling
    block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Armed with our pre-trained `MobileNetV2` and `_upsample()`, we can iteratively
    build the full `self.target_layers`, which are frozen (`down_stack.trainable =
    False`), meaning we only train the decoder or upsampling blocks of the architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we can add the skip connections to facilitate the flow of the gradient
    throughout the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output layer of the **U-Net** is a transposed convolution that has the
    same dimensions as the input image, but has as many channels as there are classes
    in the segmentation mask:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `_plot_model_history()`, a helper method that plots the relevant training
    curves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `train()` method, which is in charge of fitting the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `_process_mask()`, a helper method that makes the segmentation masks
    compatible with OpenCV:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `_save_image_and_masks()` helper method to create a visualization
    of the original image, along with the real and predicted masks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `_create_mask()`, which produces a valid segmentation mask from the
    network''s predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `_save_predictions()` method uses the `_save_image_and_mask()` helper method,
    which we defined in *Step 13*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `evaluate()` method computes the accuracy of the **U-Net** on the test
    set, while also generating predictions for a sample of images. These are then
    stored on disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Download (or load, if cached) `Oxford IIIT Pet Dataset`, along with its metadata,
    using TensorFlow Datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the metadata to define the corresponding number of steps the network will
    take over the training and validation datasets. Also, define the batch and buffer
    sizes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the training and testing datasets'' pipelines:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the **U-Net** and train it for 30 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the network on the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The accuracy on the test set should be close to 90% (in my case, I obtained
    90.78% accuracy):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Training and validation accuracy curves'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.10 – Training and validation accuracy curves
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy curves show that the network is not overfitting because both the
    training and validation plots follow the same trajectory, with a very thin gap.
    This also confirms that the knowledge the model is acquiring is transferrable
    and usable on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at some of the outputs from the network, starting with the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – The original image (left), the ground truth mask (center),
    and the predicted mask (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.11 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to *Figure 8.7* in the *Implementing a U-Net from scratch* recipe,
    in the preceding image, we can see that the **U-Net** produces a much cleaner
    result, with the background (gray pixels), contour (white pixels), and pet (black
    pixels) clearly separated and almost identical to the ground truth mask (center):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – The original image (left), the ground truth mask (center),
    and the predicted mask (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.12 – The original image (left), the ground truth mask (center), and
    the predicted mask (right)
  prefs: []
  type: TYPE_NORMAL
- en: The preceding image is a great improvement in comparison to *Figure 8.8* in
    the *Implementing a U-Net from scratch* recipe. This time, the predicted mask
    (right), although not perfect, presents less noise and is much closer to the actual
    segmentation mask (center).
  prefs: []
  type: TYPE_NORMAL
- en: We'll dig deeper in the *How it works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we made a small yet substantial change to the `MobileNetV2`
    trained on the massive `ImageNet` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The reason transfer learning worked so well in this context is that there are
    hundreds of classes in `ImageNet` focused on different breeds of cats and dogs,
    meaning the overlap with `Oxford IIIT Pet` is very substantial. However, if this
    wasn't the case, this doesn't mean we should drop transfer learning entirely!
    What we should do in that situation is fine-tune the encoder by making some (or
    all) of its layers trainable.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the knowledge encoded in `MobileNetV2`, we were able to bump the
    accuracy on the test set from 83% up to 90%, an impressive gain that translated
    into better, cleaner prediction masks, even on challenging examples.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read the original `Oxford IIIT Pet Dataset`, please go to [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/).
    To learn how to combat the vanishing gradient problem, read this article: [https://en.wikipedia.org/wiki/Vanishing_gradient_problem](https://en.wikipedia.org/wiki/Vanishing_gradient_problem).'
  prefs: []
  type: TYPE_NORMAL
- en: Segmenting images using Mask-RCNN and TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`COCO` dataset. This will help us perform out-of-the-box object detection and
    image segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we must install `Pillow` and **TFHub**, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to install the `cd` to a location of your preference and clone
    the `tensorflow/models` repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, install the **TensorFlow Object Detection API**, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to learn how to segment your images using **Mask-RCNN**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load an image into a NumPy array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will make predictions with **Mask-RCNN** and save the
    results to disk. Start by loading the image and passing it through the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert the results into `NumPy` arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract both the detection masks and boxes from the model output and convert
    them into tensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Reframe the box masks to image masks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a visualization of the detections and their boxes, scores, classes,
    and masks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save the result to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `COCO` dataset''s category index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load **Mask-RCNN** from **TFHub**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run `output` folder. Let''s review an easy one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 8.13– Single instance of segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.13– Single instance of segmentation
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can see that the network correctly detected and segmented the dog
    with 100% accuracy! Let''s try a more challenging one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Multiple instances of segmentation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.14 – Multiple instances of segmentation
  prefs: []
  type: TYPE_NORMAL
- en: 'This image is much more crowded than the previous one, and even then, the network
    correctly identified most of the objects in the scene (cars, people, trucks, and
    so on) – even occluded ones! However, the model fails in some circumstances, as
    shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – Segmentation with errors and redundancies'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_08_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8.15 – Segmentation with errors and redundancies
  prefs: []
  type: TYPE_NORMAL
- en: This time, the network correctly identified me and my dogs, as well as the coffee
    cup and the couch, but it threw duplicate and nonsensical detections, such as
    my leg being a person. This happened because I'm holding my dog, and parts of
    my body are disconnected in the photo, leading to incorrect or low confidence
    segmentations.
  prefs: []
  type: TYPE_NORMAL
- en: Let's head over to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we learned how to detect objects and perform image segmentation
    using one of the most powerful neural networks in existence: **Mask-RCNN**. Training
    such a model is not an easy task, let alone implementing it from scratch! Fortunately,
    thanks to **TensorFlow Hub**, we were able to use all its predicting power with
    just a few lines of code.'
  prefs: []
  type: TYPE_NORMAL
- en: We must take into consideration that this pre-trained model will work best on
    images containing objects the network has been trained on. More precisely, the
    more the images that we pass to `COCO`, the better the results will be. Nevertheless,
    a degree of tweaking and experimentation is always needed in order to achieve
    the best detections possible because, as we saw in the previous example, the network,
    although great, isn't perfect.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about the model we used here: [https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1](https://tfhub.dev/tensorflow/mask_rcnn/inception_resnet_v2_1024x1024/1).
    Also, reading the **Mask-RCNN** paper is a sound decision: [https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870).'
  prefs: []
  type: TYPE_NORMAL
