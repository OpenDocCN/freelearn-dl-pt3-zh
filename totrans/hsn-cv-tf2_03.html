<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Basics and Training a Model</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow</strong> is a numerical processing library used by researchers and machine learning practitioners. While you can perform any numerical operation with TensorFlow, it is mostly used to train and run deep neural networks. This chapter will introduce you to the core concepts of TensorFlow 2 and walk you through a simple example.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Getting started with TensorFlow 2 and Keras</li>
<li>Creating and training a simple computer vision model</li>
<li>TensorFlow and Keras core concepts</li>
<li>The TensorFlow ecosystem</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>Throughout this book, we will use TensorFlow 2. You can find detailed installation instructions for the different platforms at <a href="https://www.tensorflow.org/install/">https://www.tensorflow.org/install</a>.</p>
<p>If you plan on using your machine's GPU, make sure you install the corresponding version, <kbd>tensorflow-gpu</kbd>. It must be installed along with the CUDA Toolkit, a library provided by NVIDIA (<a href="https://developer.nvidia.com/cuda-zone">https://developer.nvidia.com/cuda-zone</a>).</p>
<p><span>Installation instructions are also available in the README on GitHub at <a href="https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter02">https://github.com/PacktPublishing/Hands-On-Computer-Vision-with-TensorFlow-2/tree/master/Chapter02</a>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with TensorFlow 2 and Keras</h1>
                </header>
            
            <article>
                
<p>Before detailing the core concepts of TensorFlow, we will start with a brief introduction of the framework and a basic example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing TensorFlow</h1>
                </header>
            
            <article>
                
<p>TensorFlow was originally developed at Google to allow researchers and developers to conduct machine learning research. It was originally defined as <em>an interface for expressing machine learning algorithms, and an implementation for executing such algorithms.</em></p>
<p class="mce-root">TensorFlow primarily offers to simplify the deployment of machine learning solutions on various platforms—computer CPUs, computer GPUs, mobile devices, and, more recently, in the browser. On top of that, TensorFlow offers many useful functions for creating machine learning models and running them at scale. In 2019, TensorFlow 2 was released with a focus on ease of use while maintaining good performance.</p>
<div class="packt_tip">An introduction to TensorFlow 1.0's concepts is available in <a href="59767fa2-b254-47a4-a39a-3f8c826490fa.xhtml">Appendix</a>, <em>Migrating from TensorFlow 1 to TensorFlow 2</em> of this book.</div>
<p class="mce-root">The library was open sourced in November 2015. Since then, it has been improved and used by users all around the world. It is considered one of the platforms of choice for research. It is also one of the most active deep learning frameworks in terms of GitHub activity.</p>
<p>TensorFlow can be used by beginners as well as experts. The TensorFlow API has different levels of complexity, allowing newcomers to start with a simple API and experts to create very complex models at the same time. Let's explore those different levels.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow's main architecture</h1>
                </header>
            
            <article>
                
<p>TensorFlow's architecture has several levels of abstraction. Let's first introduce the lowest layer and find our way to the uppermost layer:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/554cef8f-62a0-4b4c-abb8-9ab79075c69f.png" style="width:27.67em;height:10.50em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 2.1:</span> Diagram of the TensorFlow architecture</div>
<p>Most deep learning computations are coded in C++. To run operations on the GPU, TensorFlow uses a library developed by NVIDIA called <strong>CUDA</strong>. This is the reason you need to install CUDA if you want to exploit GPU capabilities and why you cannot use GPUs from another hardware manufacturer.</p>
<p>The Python <strong>low-level</strong> <strong>API</strong> then wraps the C++ sources. When you call a Python method in TensorFlow, it usually invokes C++ code behind the scenes. This wrapper layer allows users to work more quickly because Python is considered easier to use than C++ and does not require compilation. This Python wrapper makes it possible to perform extremely basic operations such as matrix multiplication and addition.</p>
<p>At the top sits the <strong>high-level API</strong>, made of two components—Keras and the Estimator API. <strong>Keras</strong> is a user-friendly, modular, and extensible wrapper for TensorFlow. We will introduce it in the next section. The <strong>Estimator API</strong> contains several pre-made components that allow you to build your machine learning model easily. You can consider them building blocks or templates.</p>
<div class="packt_tip">In deep learning, a <strong>model</strong> usually refers to a neural network that was trained on data. A model is composed of an architecture, matrix weights, and parameters.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing Keras</h1>
                </header>
            
            <article>
                
<p>First released in 2015, Keras was designed as an interface to enable fast experimentation with neural networks. As such, it relied on TensorFlow or <strong>Theano</strong> (another deep learning framework, now deprecated) to run deep learning operations. Known for its user-friendliness, it was the library of choice for beginners.</p>
<p class="mce-root">Since 2017, TensorFlow has integrated Keras fully, meaning that you can use it without installing anything other than TensorFlow. Throughout this book, we will rely on <kbd>tf.keras</kbd> instead of the standalone version of Keras. There are a few minor differences between the two versions, such as compatibility with TensorFlow's other modules and the way models are saved. For this reason, readers must make sure to use the correct version, as follows:</p>
<ul>
<li>In your code, import <kbd>tf.keras</kbd> and not <kbd>keras</kbd>.</li>
<li>Go through the <kbd>tf.keras</kbd> documentation on TensorFlow's website and not the <em>keras.io</em> documentation.</li>
<li>When using external Keras libraries, make sure they are compatible with <kbd>tf.keras</kbd>.</li>
<li>Some saved models might not be compatible between different versions of Keras.</li>
</ul>
<p class="mce-root">The two versions will continue to co-exist for the foreseeable future, and <kbd>tf.keras</kbd> will become more and more integrated with TensorFlow. To illustrate the power and simplicity of Keras, we will now use it to implement a simple neural network.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A simple computer vision model using Keras</h1>
                </header>
            
            <article>
                
<p>Before we delve into the core concepts of TensorFlow, let's start with a classical example of computer vision—digit recognition with the <strong><span>Modified National Institute of Standards and Technology</span></strong> (<strong>MNIST</strong>) dataset. The dataset was introduced in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data</h1>
                </header>
            
            <article>
                
<p>First, we import the data. It is made up of 60,000 images for the training set and 10,000 images for the test set:</p>
<pre>import tensorflow as tf<br/><br/><span>num_classes = 10<br/>img_rows, img_cols = 28, 28<br/>num_channels = 1<br/>input_shape = (img_rows, img_cols, num_channels)<br/><br/>(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()<br/>x_train, x_test = x_train / 255.0, x_test / 255.0</span></pre>
<div class="packt_tip">It is common practice to import TensorFlow with the alias <kbd>tf</kbd> for faster reading and typing. It is also common to use <kbd>x</kbd> to denote input data, and <kbd>y</kbd> to represent labels.</div>
<p>The <kbd>tf.keras.datasets</kbd> module provides quick access to download and instantiate a number of classical datasets. After importing the data using <kbd>load_data</kbd>, notice that we divide the array by <kbd>255.0</kbd> to get a number in the range [<em>0, 1</em>] instead of [<em>0, 255</em>]. It is common practice to normalize data, either in the [<em>0, 1</em>] range or in the [<em>-1, 1</em>] range.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p>We can now move on to building the actual model. We will use a very simple architecture composed of two <strong>fully connected</strong> (also called <strong>dense</strong>) layers. Before we explore the architecture, let's have a look at the code. As you can see, Keras code is very concise:</p>
<pre>model = tf.keras.models.Sequential()<br/>model.add(tf.keras.layers.Flatten())<br/>model.add(tf.keras.layers.Dense(128, activation='relu'))<br/>model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))</pre>
<p>Since our model is a linear stack of layers, we start by calling the <kbd>Sequential</kbd> function. We then add each layer one after the other. Our model is composed of two fully connected layers. We build it layer by layer:</p>
<ul>
<li><strong>Flatten</strong>: This will take the 2D matrix representing the image pixels and turn it into a 1D array. We need to do this before adding a fully connected layer. The <em>28</em> × <em>28</em> images are turned into a vector of size <em>784</em>.</li>
<li><strong>Dense</strong> of size <em>128</em>: This will turn the <em>784</em> pixel values into 128 activations using a weight matrix of size <em>128</em> × <em>784</em> and a bias matrix of size <em>128</em>. In total, this means <em>100,480</em> parameters.</li>
<li><strong>Dense</strong> of size <em>10</em>: This will turn the <em>128</em> activations into our final prediction. Notice that because we want probabilities to sum to <em>1</em>, we will use the <kbd>softmax</kbd> activation function.</li>
</ul>
<div class="packt_infobox">The <kbd>softmax</kbd> function takes the output of a layer and returns probabilities that sum up to <kbd>1</kbd>. It is the activation of choice for the last layer of a classification model.</div>
<p>Note that you can get a description of the model, the outputs, and their weights using <kbd>model.summary()</kbd>. Here is the output:</p>
<pre><strong>Model: "sequential"</strong><br/><strong>_________________________________________________________________</strong><br/><strong>Layer (type) Output Shape Param # </strong><br/><strong>=================================================================</strong><br/><strong>flatten_1 (Flatten) (None, 784) 0 </strong><br/><strong>_________________________________________________________________</strong><br/><strong>dense_1 (Dense) (None, 128) 100480 </strong><br/><strong>_________________________________________________________________</strong><br/><strong>dense_2 (Dense) (None, 10) 1290 </strong><br/><strong>=================================================================</strong><br/><strong>Total params: 101,770</strong><br/><strong>Trainable params: 101,770</strong><br/><strong>Non-trainable params: 0</strong></pre>
<p>With its architecture set and weights initialized, the model is now ready to be trained for the chosen task.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training the model</h1>
                </header>
            
            <article>
                
<p>Keras makes training extremely simple:</p>
<pre>model.compile(<span>optimizer</span>=<span>'sgd'</span><span>,<br/></span><span> </span><span>loss</span>=<span>'sparse_categorical_crossentropy'</span><span>,<br/></span><span> </span><span>metrics</span>=[<span>'accuracy'</span>])<br/><br/>model.fit(x_train<span>, </span>y_train<span>, </span><span>epochs</span>=<span>5</span><span>, </span><span>verbose</span>=<span>1</span><span>, </span><span>validation_data</span>=(x_test<span>, </span>y_test))</pre>
<p>Calling <kbd>.compile()</kbd> on the model we just created is a mandatory step. A few arguments must be specified:</p>
<ul>
<li><kbd>optimizer</kbd>: This is the component that will perform the gradient descent.</li>
<li><kbd>loss</kbd>: This is the metric we will optimize. In our case, we choose cross-entropy, just like in the previous chapter.</li>
<li><kbd>metrics</kbd>: These are additional metric functions evaluated during training to provide further visibility of the model's performance (unlike <kbd>loss</kbd>, they are not used in the optimization process).</li>
</ul>
<p>The Keras <kbd>loss</kbd> named <kbd>sparse_categorical_crossentropy</kbd> performs the same cross-entropy operation as <kbd>categorical_crossentropy</kbd>, but the former directly takes the ground truth labels as inputs, while the latter requires the ground truth labels to be <em>one-hot</em> encoded already before hand. Using the <kbd>sparse_...</kbd> loss thus saves us from manually having to transform the labels.</p>
<div class="packt_tip">Passing <kbd>'sgd'</kbd> to Keras is equivalent to passing <kbd>tf.keras.optimizers.SGD()</kbd>. The former option is easier to read, while the latter makes it possible to specify parameters such as a custom learning rate. The same goes for the loss, metrics, and most arguments passed to Keras methods.</div>
<p>Then, we call the <kbd>.fit()</kbd> method. It is very similar to the interface used in <strong>scikit-learn</strong>, another popular machine learning library. We will train for five epochs, meaning that we will iterate over the whole train dataset five times.</p>
<p>Notice that we set <kbd>verbose</kbd> to <kbd>1</kbd>. This will allow us to get a progress bar with the metrics we chose earlier, the loss, and the <strong>Estimated Time of Arrival</strong> (<strong>ETA</strong>). The ETA is an estimate of the remaining time before the end of the epoch. Here is what the progress bar looks like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/59872f2a-0192-4997-98c9-8bb6fd4c1b1a.png" style="width:43.67em;height:1.75em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 2.2:</span> Screenshot of the progress bar displayed by Keras in verbose mode</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance</h1>
                </header>
            
            <article>
                
<p>As described in <a href="3d1c879b-b6fa-4eee-b578-60b57a77ff33.xhtml">Chapter 1</a>, <em>Computer Vision and Neural Networks</em>, you will notice that our model is overfitting—training accuracy is greater than test accuracy. If we train the model for five epochs, we end up with an accuracy of 97% on the test set. This is about 2% better than in the previous chapter, where we achieved 95%. State-of-the-art algorithms attain 99.79% accuracy.</p>
<p>We followed three main steps:</p>
<ol>
<li><strong>Loading the data</strong>: In this case, the dataset was already available. During future projects, you may need additional steps to gather and clean the data.</li>
<li><strong>Creating the model</strong>: This step was made easy by using Keras—we defined the architecture of the model by adding sequential layers. Then, we selected a loss, an optimizer, and a metric to monitor.</li>
<li><strong>Training the model</strong>: Our model worked pretty well the first time. On more complex datasets, you will usually need to fine-tune parameters during training.</li>
</ol>
<p>The whole process was extremely simple thanks to Keras, the high-level API of TensorFlow. Behind this simple API, the library hides a lot of the complexity.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow 2 and Keras in detail</h1>
                </header>
            
            <article>
                
<p>We have introduced the general architecture of TensorFlow and trained our first model using Keras. Let's now walk through the main concepts of TensorFlow 2. We will explain several core concepts of TensorFlow that feature in this book, followed by some advanced notions. While we may not employ all of them in the remainder of the book, you might find it useful to understand some open source models that are available on GitHub or to get a deeper understanding of the library.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Core concepts</h1>
                </header>
            
            <article>
                
<p>Released in spring 2019, the new version of the framework is focused on simplicity and ease of use. In this section, we will introduce the concepts that TensorFlow relies on and cover how they evolved from version 1 to version 2.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing tensors</h1>
                </header>
            
            <article>
                
<p>TensorFlow takes its name from a mathematical object called a <strong>tensor</strong>. You can imagine tensors as N-dimensional arrays. A tensor could be a scalar, a vector, a 3D matrix, or an N-dimensional matrix.</p>
<p>A fundamental component of TensorFlow, the <kbd>Tensor</kbd> object is used to store mathematical values. It can contain fixed values (created using <kbd>tf.constant</kbd>) or changing values (created using <kbd>tf.Variable</kbd>).</p>
<div class="packt_infobox">In this book, <em>tensor</em> denotes the mathematical concept, while <em>Tensor</em> (with a capital <em>T</em>) corresponds to the TensorFlow object.</div>
<p>Each <kbd>Tensor</kbd> object has the following:</p>
<ul>
<li><strong>Type</strong>: <kbd>string</kbd>, <kbd>float32</kbd>, <kbd>float16</kbd>, or <kbd>int8</kbd>, among others.</li>
<li><strong>Shape</strong>: The dimensions of the data. For instance, the shape would be <kbd>()</kbd> for a scalar, <kbd>(n)</kbd> for a vector of size <em>n</em>, and <kbd>(n, m)</kbd> for a 2D matrix of size <em>n</em> × <em>m</em>.</li>
<li><strong>Rank</strong>: The number of dimensions, <em>0</em> for a scalar, <kbd>1</kbd> for a vector, and <em>2</em> for a 2D matrix.</li>
</ul>
<p>Some tensors can have partially unknown shapes. For instance, a model accepting images of variable sizes could have an input shape of <kbd>(None, None, 3)</kbd>. Since the height and the width of the images are not known in advance, the first two dimensions are set to <kbd>None</kbd>. However, the number of channels (<kbd>3</kbd>, corresponding to red, blue, and green) is known and is therefore set.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow graphs</h1>
                </header>
            
            <article>
                
<p>TensorFlow uses tensors as inputs as well as outputs. A component that transforms input into output is called an <strong>operation</strong>. A computer vision model is therefore composed of multiple operations.</p>
<p>TensorFlow represents these operations using a <strong>directed acyclic graph</strong> (<strong>DAC</strong>), also referred to as a <strong>graph</strong>. In TensorFlow 2, graph operations have disappeared under the hood to make the framework easier to use. Nevertheless, the graph concept remains important to understand how TensorFlow really works.</p>
<p class="CDPAlignLeft CDPAlign">When building the previous example using Keras, TensorFlow actually built a graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/12472b05-6c87-4dd4-a778-1c3344dd07f1.png" style="width:4.08em;height:12.08em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 2.3:</span> A simplified graph corresponding to our model. In practice, each node is composed of smaller operations (such as matrix multiplications and additions)</div>
<p>While very simple, this graph represents the different layers of our model in the form of operations. Relying on graphs has many advantages, allowing TensorFlow to do the following:</p>
<ul>
<li>Run part of the operations on the CPU and another part on the GPU</li>
<li>Run different parts of the graph on different machines in the case of a distributed model</li>
<li>Optimize the graph to avoid unnecessary operations, leading to better computational performance</li>
</ul>
<p>Moreover, the graph concept allows TensorFlow models to be portable. A single graph definition can be run on any kind of device.</p>
<p>In TensorFlow 2, graph creation is no longer handled by the user. While managing graphs used to be a complex task in TensorFlow 1, the new version greatly improves usability while still maintaining performance. In the next section, we will peek into the inner workings of TensorFlow and briefly explore how graphs are created.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing lazy execution to eager execution</h1>
                </header>
            
            <article>
                
<p>The main change in TensorFlow 2 is <strong>eager execution</strong>. Historically, TensorFlow 1 always used <strong>lazy execution</strong> by default. It is called <em>lazy</em> because operations are not run by the framework until asked specifically to do so.</p>
<p><span>Let's start with a very simple example to illustrate the difference between lazy and eager execution, summing the values of two vectors:<br/></span></p>
<div class="input">
<div class="inner_cell">
<div class="input_area">
<div class=" highlight hl-ipython2">
<pre><span class="n">import tensorflow as tf<br/><br/>a = tf.constant([1, 2, 3])<br/>b = tf.constant([0, 0, 1])<br/>c = tf.add(a, b)<br/><br/>print(c)</span></pre></div>
</div>
</div>
<div class="inner_cell">
<div class="input_area packt_tip">
<p>Note that <kbd>tf.add(a, b)</kbd> could be replaced by <kbd>a + b</kbd> since TensorFlow overloads many Python operators.</p>
</div>
</div>
</div>
<div class="input">
<p>The output of the previous code depends on the TensorFlow version. With TensorFlow 1 (where lazy execution is the default mode), the output would be this:</p>
<pre><span>Tensor("Add:0", shape=(3,), dtype=int32)</span></pre>
<p>However, with TensorFlow 2 (where eager execution is the default mode), you would get the following output:</p>
</div>
<pre class="input">tf.Tensor([1 2 4], shape=(3,), dtype=int32)</pre>
<p class="input"><span>In both cases, the output is a</span> Tensor<span>. In the second case, the operation has been run eagerly and we can observe directly that the Tensor contains the result (</span><kbd>[1 2 4]</kbd><span>). In the first case, the Tensor contains information about the addition operation (<kbd>Add:0</kbd>), but not the result of the operation.</span></p>
<div class="input packt_tip">
<p>In eager mode, you can access the value of a Tensor by calling the <kbd>.numpy()</kbd> method. In our example, calling <kbd>c.numpy()</kbd> returns <kbd>[1 2 4]</kbd> (as a NumPy array).</p>
</div>
<div class="input">
<div class="inner_cell">
<p>In TensorFlow 1, more code would be needed to compute the result, making the development process more complex. Eager execution makes code easier to debug (as developers can peak at the value of a Tensor at any time) and easier to develop. In the next section, we will detail the inner workings of TensorFlow and look at how it builds graphs.</p>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating graphs in TensorFlow 2</h1>
                </header>
            
            <article>
                
<p>We'll start with a simple example to illustrate graph creation and optimization:</p>
<pre>def compute(a, b, c):<br/>    d = a * b + c<br/>    e = a * b * c<br/>    return d, e</pre>
<p>Assuming <kbd>a</kbd>, <kbd>b</kbd>, and <kbd>c</kbd> are Tensor matrices, this code computes two new values: <kbd>d</kbd> and <kbd>e</kbd>. Using eager execution, TensorFlow would compute the value for <kbd>d</kbd> and then compute the value for <kbd>e</kbd>.</p>
<p>Using lazy execution, TensorFlow would create a graph of operations. Before running the graph to get the result, a <strong>graph optimizer</strong> would be run. To avoid computing <kbd>a * b</kbd> twice, the optimizer would <strong>cache</strong> the result and reuse it when necessary. For more complex operations, the optimizer could enable <strong>parallelism</strong> to make computation faster. Both techniques are important when running large and complex models.</p>
<p>As we saw, running in eager mode implies that every operation is run when defined. Therefore, such optimizations cannot be applied. Thankfully, TensorFlow includes a module to work around this—TensorFlow <strong>AutoGraph</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing TensorFlow AutoGraph and tf.function</h1>
                </header>
            
            <article>
                
<p>The TensorFlow AutoGraph module makes it easy to turn eager code into a graph, allowing automatic optimization. To do so, the easiest way is to add the <kbd>tf.function</kbd> decorator on top of your function:</p>
<pre>@tf.function<br/>def compute(a, b, c):<br/>    d = a * b + c<br/>    e = a * b * c<br/>    return d, e</pre>
<div class="packt_tip">A <strong>Python decorator</strong> is a concept that allows functions to be wrapped, adding functionalities or altering them. Decorators start with an <kbd>@</kbd> (the "at" symbol).</div>
<p>When we call the <kbd>compute</kbd> function for the first time, TensorFlow will transparently create the following graph:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/da22bed4-9aa1-4c0d-b5f3-18addb45a933.png" style="width:14.92em;height:22.17em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span>Figure 2.4:</span> The graph automatically generated by TensorFlow when calling the compute function for the first time</div>
<p>TensorFlow AutoGraph can convert most Python statements, such as <kbd>for</kbd> loops, <kbd>while</kbd> loops, <kbd>if</kbd> statements, and iterations. Thanks to graph optimizations, graph execution can sometimes be faster than eager code. More generally, AutoGraph should be used in the following scenarios:</p>
<ul>
<li>When the model needs to be exported to other devices</li>
<li>When performance is paramount and graph optimizations can lead to speed improvements</li>
</ul>
<p>Another advantage of graphs is their <strong>automatic differentiation</strong>. Knowing the full list of operations, TensorFlow can easily compute the gradient for each variable.</p>
<div class="packt_tip">Note that in order to compute the gradient, the operations need to be <strong>differentiable</strong>. Some of them, such as <kbd>tf.math.argmax</kbd>, are not. Using them in a <kbd>loss</kbd> function will most likely cause the automatic differentiation to fail. It is up to the user to make sure that the loss is differentiable.</div>
<p>However, since, in eager mode, each operation is independent from one another, automatic differentiation is not possible by default. Thankfully, TensorFlow 2 provides a way to perform automatic differentiation while still using eager mode—the <strong>gradient tape</strong>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Backpropagating errors using the gradient tape</h1>
                </header>
            
            <article>
                
<p>The gradient tape allows easy backpropagation in eager mode. To illustrate this, we will use a simple example. Let's assume that we want to solve the equation <em>A</em> × <em>X = B</em>, where <em>A</em> and <em>B</em> are constants. We want to find the value of <em>X</em> to solve the equation. To do so, we will try to minimize a simple loss, <em>abs(A × X - B)</em>.</p>
<p>In code, this translates to the following:</p>
<pre>A, B = tf.constant(3.0), tf.constant(6.0)<br/>X = tf.Variable(20.0) # In practice, we would start with a random value<br/>loss = tf.math.abs(A * X - B)</pre>
<p>Now, to update the value of <em>X</em>, we would like to compute the gradient of the loss with respect to <em>X</em>. However, when printing the content of the loss, we obtain the following:</p>
<pre class="mce-root">&lt;tf.Tensor: id=18525, shape=(), dtype=float32, numpy=54.0&gt;</pre>
<p class="mce-root">In eager mode, TensorFlow computed the result of the operation instead of storing the operation! With no information on the operation and its inputs, it would be impossible to automatically differentiate the <kbd>loss</kbd> operation.</p>
<p>That is where the gradient tape comes in handy. By running our loss computation in the context of <kbd>tf.GradientTape</kbd>, TensorFlow will automatically record all operations and allow us to replay them backward afterward:</p>
<pre>def train_step():<br/>    with tf.GradientTape() as tape:<br/>        loss = tf.math.abs(A * X - B)<br/>    dX = tape.gradient(loss, X)<br/>    <br/>    print('X = {:.2f}, dX = {:2f}'.format(X.numpy(), dX))<br/>    X.assign(X - dX)<br/><br/>for i in range(7):<br/>    train_step()</pre>
<p>The previous code defines a single training step. Every time <kbd>train_step</kbd> is called, the loss is computed in the context of the gradient tape. The context is then used to compute the gradient. The <em>X</em> variable is then updated. Indeed, we can see <em>X</em> converging toward the value that solves the equation:</p>
<pre class="mce-root"><strong> X = 20.00, dX = 3.000000</strong><br/><strong> X = 17.00, dX = 3.000000</strong><br/><strong> X = 14.00, dX = 3.000000</strong><br/><strong> X = 11.00, dX = 3.000000</strong><br/><strong> X = 8.00, dX = 3.000000</strong><br/><strong> X = 5.00, dX = 3.000000</strong><br/><strong> X = 2.00, dX = 0.000000</strong></pre>
<p class="mce-root">You will notice that in the very first example of this chapter, we did not make use of the gradient tape. This is because Keras models encapsulate training inside the <kbd>.fit()</kbd> function—there's no need to update the variables manually. Nevertheless, for innovative models or when experimenting, the gradient tape is a powerful tool that allows automatic differentiation without much effort. Readers can find a more practical use of the gradient tape in the regularization notebook of <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Keras models and layers</h1>
                </header>
            
            <article>
                
<p>In the first section of this chapter, we built a simple Keras Sequential model. The resulting <kbd>Model</kbd> object contains numerous useful methods and properties:</p>
<ul>
<li><kbd>.inputs</kbd> and <kbd>.outputs</kbd>: Provide access to the inputs and outputs of the model.</li>
<li><kbd>.layers</kbd>: Lists the model's layers as well as their shape.</li>
<li><kbd>.summary()</kbd>: Prints the architecture of the model.</li>
<li><kbd>.save()</kbd>: Saves the model, its architecture, and the current state of training. It is very useful for resuming training later on. Models can be instantiated from a file using <kbd>tf.keras.models.load_model()</kbd>.</li>
<li><kbd>.save_weights()</kbd>: Only saves the weights of the model.</li>
</ul>
<p>While there is only one type of Keras model object, they can be built in a variety of ways.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sequential and functional APIs</h1>
                </header>
            
            <article>
                
<p>Instead of employing the Sequential API, like at the beginning of this chapter, you can instead use the functional API:</p>
<pre>model_input = tf.keras.layers.Input(shape=input_shape)<br/>output = tf.keras.layers.Flatten()(model_input)<br/>output = tf.keras.layers.Dense(128, activation='relu')(output)<br/>output = tf.keras.layers.Dense(num_classes, activation='softmax')(output)<br/>model = tf.keras.Model(model_input, output)</pre>
<p>Notice that the code is slightly longer than it previously was. Nevertheless, the functional API is much more versatile and expressive than the Sequential API. The former allows for branching models (that is, for building architectures with multiple parallel layers for instance), while the latter can only be used for linear models. For even more flexibility, Keras also offers the possibility to subclass the <kbd>Model</kbd> class, as described in <a href="dd1d3406-d506-4690-bf13-e5e0584ea9d1.xhtml">Chapter 3</a>, <em>Modern Neural Networks</em>.</p>
<p>Regardless of how a <kbd>Model</kbd> object is built, it is composed of layers. A layer can be seen as a node that accepts one or several inputs and returns one or several outputs, similar to a TensorFlow operation. Its weights can be accessed using <kbd>.get_weights()</kbd> and set using <kbd>.set_weights()</kbd>. Keras provides pre-made layers for the most common deep learning operations. For more innovative or complex models, <kbd>tf.keras.layers.Layer</kbd> can also be subclassed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Callbacks</h1>
                </header>
            
            <article>
                
<p><strong>Keras callbacks</strong> are utility functions that you can pass to a Keras model's <kbd>.fit()</kbd> method to add functionality to its default behavior. Multiple callbacks can be defined, which will be called by Keras either before or after each batch iteration, each epoch, or the whole training procedure. Predefined Keras callbacks include the following:</p>
<ul>
<li><kbd>CSVLogger</kbd>: Logs training information in a CSV file.</li>
<li><kbd>EarlyStopping</kbd>: Stops training if the loss or a metric stops improving. It can be useful in avoiding overfitting.</li>
<li><kbd>LearningRateScheduler</kbd>: Changes the learning rate on each epoch according to a schedule.</li>
<li><kbd>ReduceLROnPlateau</kbd>: Automatically reduces the learning rate when the loss or a metric stops improving.</li>
</ul>
<p>It is also possible to create custom callbacks by subclassing <kbd>tf.keras.callbacks.Callback</kbd>, as demonstrated in later chapters and their code samples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced concepts</h1>
                </header>
            
            <article>
                
<p>In summary, the AutoGraph module, the <kbd>tf.function</kbd> decorator, and the gradient tape context make graph creation and management very simple—if not invisible. However, a lot of the complexity is hidden from the user. In this section, we will explore the inner workings of these modules.</p>
<div class="packt_tip">This section presents advanced concepts that are not required throughout the book, but it may be useful for you to understand more complex TensorFlow code. More impatient readers can skip this part and come back to it later.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How tf.function works</h1>
                </header>
            
            <article>
                
<p>As mentioned earlier, when calling a function decorated with <kbd>tf.function</kbd> for the first time, TensorFlow will create a graph corresponding to the function's operations. TensorFlow will then cache the graph so that the next time the function is called, graph creation will not be necessary.</p>
<p>To illustrate this, let's create a simple <kbd>identity</kbd> function:</p>
<pre>@tf.function<br/>def identity(x):<br/>  print('Creating graph !')<br/>  return x</pre>
<p>This function will print a message every time TensorFlow creates a graph corresponding to its operation. In this case, since TensorFlow is caching the graph, it will print something only the first time it is run:</p>
<pre>x1 = tf.random.uniform((10, 10))<br/>x2 = tf.random.uniform((10, 10))<br/><br/>result1 = identity(x1) # Prints 'Creating graph !'<br/>result2 = identity(x2) # Nothing is printed</pre>
<p>However, note that if we change the input type, TensorFlow will recreate a graph:</p>
<pre>x3 = tf.random.uniform((10, 10), dtype=tf.float16)<br/>result3 = identity(x3) # Prints 'Creating graph !'</pre>
<p>This behavior is explained by the fact that TensorFlow graphs are defined by their operations and the shapes and types of the tensors they receive as inputs. Therefore, when the input type changes, a new graph needs to be created. In TensorFlow vocabulary, when a <kbd>tf.function</kbd> function has defined input types, it becomes a <strong>concrete function</strong>.</p>
<p>To summarize, every time a decorated function is run for the first time, TensorFlow caches the graph corresponding to the input types and input shapes. If the function is run with inputs of a different type, TensorFlow will create a new graph and cache it.</p>
<p>Nevertheless, it might be useful to log information every time a concrete function is run and not just the first time. To do so, use <kbd>tf.print</kbd>:</p>
<pre>@tf.function<br/>def identity(x):<br/>  tf.print("Running identity")<br/>  return x</pre>
<p>Instead of printing information only the first time, this function will print <kbd>Running identity</kbd> every single time it is run.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Variables in TensorFlow 2</h1>
                </header>
            
            <article>
                
<p>To hold the model weights, TensorFlow uses <kbd>Variable</kbd> instances. In our Keras example, we can list the content of the model by accessing <kbd>model.variables</kbd>. It will return the list of all variables contained in our model:</p>
<pre>print([variable.name for variable in model.variables])<br/># Prints ['sequential/dense/kernel:0', 'sequential/dense/bias:0', 'sequential/dense_1/kernel:0', 'sequential/dense_1/bias:0']</pre>
<p>In our example, variable management (including naming) has been entirely handled by Keras. As we saw earlier, it is also possible to create our own variables:</p>
<pre>a = tf.Variable(3, name='my_var')<br/>print(a) # Prints &lt;tf.Variable 'my_var:0' shape=() dtype=int32, numpy=3&gt;</pre>
<p>Note that for large projects, it is recommended to name variables to clarify the code and ease debugging. To change the value of a variable, use the <kbd>Variable.assign</kbd> method:</p>
<pre>a.assign(a + 1)<br/>print(a.numpy()) # Prints 4</pre>
<p>Failing to use the <kbd>.assign()</kbd> method would create a new <kbd>Tensor</kbd> method:</p>
<pre>b = a + 1<br/>print(b) # Prints &lt;tf.Tensor: id=21231, shape=(), dtype=int32, numpy=4&gt;</pre>
<p>Finally, deleting the Python reference to a variable will remove the object itself from the active memory, releasing space for other variables to be created.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distribution strategies</h1>
                </header>
            
            <article>
                
<p>We trained a simple model on a very small dataset. When using larger models and datasets, more computing power is necessary—this often implies multiple servers. The <kbd>tf.distribute.Strategy</kbd> API defines how multiple machines communicate together to train a model efficiently.</p>
<p>Some of the strategies defined by TensorFlow are as follows:</p>
<ul>
<li><kbd>MirroredStrategy</kbd>: For training on multiple GPUs on a single machine. Model weights are kept in sync between each device.</li>
<li><kbd>MultiWorkerMirroredStrategy</kbd>: Similar to <kbd>MirroredStategy</kbd>, but for training on multiple machines.</li>
<li><kbd>ParameterServerStrategy</kbd>: For training on multiple machines. Instead of syncing the weights on each device, they are kept on a parameter server.</li>
<li><kbd>TPUStrategy</kbd>: For training on Google's <strong>Tensor Processing Unit</strong><span> (</span><strong>TPU</strong><span>)</span> chip.</li>
</ul>
<div class="packt_infobox">The TPU is a custom chip made by Google, similar to a GPU, designed specifically to run neural network computations. It is available through Google Cloud.</div>
<p>To use a distribution strategy, create and compile your model in its scope:</p>
<pre>mirrored_strategy = tf.distribute.MirroredStrategy()<br/>with mirrored_strategy.scope():<br/>  model = make_model() # create your model here<br/>  model.compile([...])</pre>
<p>Note that you will probably have to increase the batch size, as each device will now receive a small subset of each batch. Depending on your model, you may also have to change the learning rate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Estimator API</h1>
                </header>
            
            <article>
                
<p>We saw in the first part of this chapter that the Estimator API is a high-level alternative to the Keras API. Estimators simplify training, evaluation, prediction, and serving.</p>
<p>There are two types of Estimators. Pre-made Estimators are very simple models provided by TensorFlow, allowing you to quickly try out machine learning architectures. The second type is custom Estimators, which can be created using any model architecture.</p>
<p>Estimators handle all the small details of a model's life cycle—data queues, exception handling, recovering from failure, periodic checkpoints, and many more. While using Estimators was considered best practice in TensorFlow 1, in version 2, it is recommended to use the Keras API.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Available pre-made Estimators</h1>
                </header>
            
            <article>
                
<p>At the time of writing, the available pre-made Estimators are <kbd>DNNClassifier</kbd>, <kbd>DNNRegressor</kbd>, <kbd>LinearClassifier</kbd>, and <kbd>LinearRegressor</kbd>. Here, DNN stands for <strong>deep neural network</strong>. Combined Estimators based on both architectures are also available—<kbd>DNNLinearCombinedClassifier</kbd> and <kbd>DNNLinearCombinedRegressor</kbd>.</p>
<div class="packt_infobox">In machine learning, classification is the process of predicting a discrete category, while regression is the process of predicting a continuous number.</div>
<p><strong>Combined Estimators</strong>, also called <strong>deep-n-wide models</strong>, make use of a linear model (for memorization) and a deep model (for generalization). They are mostly used for recommendation or ranking models.</p>
<p>Pre-made Estimators are suitable for some machine learning problems. However, they are not suitable for computer vision problems, as there are no pre-made Estimators with convolutions, a powerful type of layer described in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a custom Estimator</h1>
                </header>
            
            <article>
                
<p>The easiest way to create an Estimator is to convert a Keras model. After the model has been compiled, call <kbd>tf.keras.estimator.model_to_estimator()</kbd>:</p>
<pre>estimator = tf.keras.estimator.model_to_estimator(model, model_dir='./estimator_dir')</pre>
<p>The <kbd>model_dir</kbd> argument allows you to specify a location where the checkpoints of the model will be saved. As mentioned earlier, Estimators will automatically save checkpoints for our models.</p>
<p>Training an Estimator requires the use of an <strong>input function</strong>—a function that returns data in a specific format. One of the accepted formats is a TensorFlow dataset. The dataset API is described in depth in <a href="337ec077-c215-4782-b56c-beae4d94d718.xhtml">Chapter 7</a>, <em>Training on Complex and Scarce Datasets</em>. For now, we'll define the following function, which returns the dataset defined in the first part of this chapter in the correct format, in batches of <em>32</em> samples:</p>
<pre>BATCH_SIZE = 32<br/>def train_input_fn():<br/>    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))<br/>    train_dataset = train_dataset.batch(BATCH_SIZE).repeat()<br/>    return train_dataset</pre>
<p>Once this function is defined, we can launch the training with the Estimator:</p>
<pre>estimator.train(train_input_fn, steps=len(x_train)//BATCH_SIZE)</pre>
<p>Just like Keras, the training part is very simple, as Estimators handle the heavy lifting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The TensorFlow ecosystem</h1>
                </header>
            
            <article>
                
<p>In addition to the main library, TensorFlow offers numerous tools that are useful for machine learning. While some of them are shipped with TensorFlow, others are grouped under <strong>TensorFlow Extended</strong> (<strong>TFX</strong>) and <strong>TensorFlow Addons</strong>. We will now introduce the most commonly used tools.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorBoard</h1>
                </header>
            
            <article>
                
<p>While the progress bar we used in the first example of this chapter displayed useful information, we might want to access more detailed graphs. TensorFlow provides a powerful tool for monitoring—<strong>TensorBoard</strong>. Installed by default with TensorFlow, it is also very easy to use when combined with Keras's callbacks:</p>
<pre>callbacks = [tf.keras.callbacks.TensorBoard('./logs_keras')]<br/>model.fit(x_train, y_train, epochs=5, verbose=1, validation_data=(x_test, y_test), callbacks=callbacks)</pre>
<p>In this updated code, we pass the TensorBoard callback to the <kbd>model.fit()</kbd> method. By default, TensorFlow will automatically write the loss and the metrics to the folder we specified. We can then launch TensorBoard from the command line:</p>
<pre><strong>$ tensorboard --logdir ./logs_keras</strong></pre>
<p>This command outputs a URL that we can then open to display the TensorBoard interface. In the <span class="packt_screen">Scalars</span> tab, we can find graphs displaying the loss and the accuracy:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/49b87d26-016f-4802-ae6b-13175e0ce6f3.png" style="width:42.17em;height:13.67em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 2.5:</span> Two graphs displayed by TensorBoard during training</div>
<p>As you will see in this book, training a deep learning model requires a lot of fine-tuning. Therefore, it is essential to monitor how your model is performing. TensorBoard allows you to do precisely this. The most common use case is to monitor the evolution of the loss of your model over time. But you can also do the following:</p>
<ul>
<li>Plot any metric (such as accuracy)</li>
<li>Display input and output images</li>
<li>Display the execution time</li>
<li>Draw your model's graph representation</li>
</ul>
<p>TensorBoard is very versatile, and there are many ways to use it. Each piece of information is stored in <kbd>tf.summary</kbd>—this can be scalars, images, histograms, or text. For instance, to log a scalar you might first create a summary writer and log information using the following:</p>
<pre>writer = tf.summary.create_file_writer('./model_logs')<br/>with writer.as_default():<br/>  tf.summary.scalar('custom_log', 10, step=3)</pre>
<p>In the preceding code, we specify the step—it could be the epoch number, the batch number, or custom information. It will correspond to the <em>x </em>axis in TensorBoard figures. TensorFlow also provides tools for generating aggregates. To manually log accuracy, you could use the following:</p>
<pre>accuracy = tf.keras.metrics.Accuracy()<br/>ground_truth, predictions = [1, 0, 1], [1, 0, 0] # in practice this would come from the model<br/>accuracy.update_state(ground_truth, predictions)<br/>tf.summary.scalar('accuracy', accuracy.result(), step=4)</pre>
<p>Other metrics are available, such as <kbd>Mean</kbd>, <kbd>Recall</kbd>, and <kbd>TruePositives</kbd>. While setting up the logging of metrics in TensorBoard may seem a bit complicated and time-consuming, it is an essential part of the TensorFlow toolkit. It will save you countless hours of debugging and manual logging.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Addons and TensorFlow Extended</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow Addons</strong> is a collection of extra functionalities gathered into a single repository (<a href="https://github.com/tensorflow/addons">https://github.com/tensorflow/addons</a>). It hosts some of the newer advancements in deep learning that are too unstable or not used by enough people to justify adding them to the main TensorFlow library. It also acts as a replacement for <kbd>tf.contrib</kbd>, which was removed from TensorFlow 1.</p>
<p><strong>TensorFlow Extended</strong> is an end-to-end machine learning platform for TensorFlow. It offers several useful tools:</p>
<ul>
<li><strong>TensorFlow Data Validation</strong>: A library for exploring and validating machine learning data. You can use it before even building your model.</li>
<li><strong>TensorFlow Transform</strong>: A library for preprocessing data. It allows you to make sure training and evaluation data are processed the same way.</li>
<li><strong>TensorFlow Model Analysis</strong>: <span>A</span> library for evaluating TensorFlow models.</li>
<li><strong>TensorFlow Serving</strong>: A serving system for machine learning models. Serving is the process of delivering predictions from a model, usually through a REST API:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/009f5850-47f5-4c19-af52-9861f8716370.png" style="width:43.00em;height:7.33em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 2.6:</span> End-to-end process of creating and using a deep learning model</div>
<p>As seen in <em>Figure 2.6</em>, these tools fulfill the goal of being end to end, covering every step of the process of building and using a deep learning model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow Lite and TensorFlow.js</h1>
                </header>
            
            <article>
                
<p>The main version of TensorFlow is designed for Windows, Linux, and Mac computers. To operate on other devices, a different version of TensorFlow is necessary. <strong>TensorFlow Lite</strong> is designed to run model predictions (inference) on mobile phones and embedded devices. It is composed of a converter transforming TensorFlow models to the required <kbd>.tflite</kbd> format and an interpreter that can be installed on mobile devices to run inferences.</p>
<p>More recently, <strong>TensorFlow.js</strong> (also referred to as <strong>tfjs</strong>) was developed to empower almost any web browser with deep learning. It does not require any installation from the user and can sometimes make use of the device's GPU acceleration. We detail the use of TensorFlow Lite and TensorFlow.js in <a href="e8935e55-c3b5-419e-a86a-43eba3ff4dad.xhtml">Chapter 9</a>, <em>Optimizing Models and Deploying on Mobile Devices</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Where to run your model</h1>
                </header>
            
            <article>
                
<p>As computer vision models process large amounts of data, they take a long time to train. Because of this, training on your local computer can take a considerable amount of time. You will also notice that creating efficient models requires a lot of iterations. Those two insights will drive your decision regarding where to train and run your models. In this section, we will compare the different options available to train and use your model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">On a local machine</h1>
                </header>
            
            <article>
                
<p>Coding your model on your computer is often the fastest way to get started. As you have access to a familiar environment, you can easily change your code as often as needed. However, personal computers, especially laptops, lack the computing power to train a computer vision model. Training on a GPU may be between 10 and 100 times faster than using a CPU. This is why it is recommended to use a GPU.</p>
<div class="packt_tip">Even if your computer has a GPU, only very specific models can run TensorFlow. Your GPU must be compatible with CUDA, NVIDIA's computing library. At the time of writing, the latest version of TensorFlow requires a CUDA compute capability of 3.5 or higher.</div>
<p>Some laptops are compatible with external GPU enclosures, but this defeats the purpose of a portable computer. Instead, a practical way is to run your model on a remote computer that has a GPU.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">On a remote machine</h1>
                </header>
            
            <article>
                
<p>Nowadays, you can rent powerful machines with GPUs by the hour. Pricing varies, depending on the GPU power and the provider. It usually costs around $1 per hour for a single GPU machine, with the price going down every day. If you commit to renting the machine for the month, you can get good computing power for around $100 per month. Considering the time you will save waiting for the model to train, it often makes economic sense to rent a remote machine.</p>
<p>Another option is to build your own deep learning server. Note that this requires investment and assembly, and that GPUs consume large amounts of electricity.</p>
<p>Once you have secured access to a remote machine, you have two options:</p>
<ul>
<li>Run Jupyter Notebook on the remote server. Jupyter Lab or Jupyter Notebook will then be accessible using your browser, anywhere on the planet. It is a very convenient way of performing deep learning.</li>
<li>Sync your local development folder and run your code remotely. Most IDEs have a feature to sync your local code with a remote server. This allows you to code in your favorite IDE while still enjoying a powerful machine.</li>
</ul>
<div class="packt_tip">Google Colab, based on Jupyter notebooks, allows you to run notebooks in the cloud for <em>free</em>. You can even enable GPU mode. Colab has limited storage space and a limit of 8 hours of consecutive running time. While it is the perfect tool for getting started or experimenting, it is not convenient for larger models.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">On Google Cloud</h1>
                </header>
            
            <article>
                
<p>To run TensorFlow on a remote machine, you will need to manage it yourself—installing the correct software, making sure it is up to date, and turning the server on and off. While it is still possible to do so for one machine, and you sometimes need to distribute the training among numerous GPUs, using Google Cloud ML to run TensorFlow allows you to focus on your model and not on operations.</p>
<p>You will find that Google Cloud ML is useful for the following:</p>
<ul>
<li>Training your model quickly thanks to elastic resources in the cloud</li>
<li>Looking for the best model parameters in the shortest amount of time possible using parallelization</li>
<li>Once your model is ready, serving predictions without having to run your own prediction server</li>
</ul>
<p>All the details for packaging, sending, and running your model are available in the Google Cloud ML documentation (<a href="https://cloud.google.com/ml-engine/docs/">https://cloud.google.com/ml-engine/docs/</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we started by training a basic computer vision model using the Keras API. We introduced the main concepts behind TensorFlow 2—tensors, graphs, AutoGraph, eager execution, and the gradient tape. We also detailed some of the more advanced concepts of the framework. We went through the main tools surrounding the use of deep learning with the library, from TensorBoard for monitoring, to TFX for preprocessing and model analysis. Finally, we covered where to run your model depending on your needs.</p>
<p>With these powerful tools in hand, you are now ready to discover modern computer vision models in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is Keras in relation to TensorFlow, and what is its purpose?</li>
<li>Why does TensorFlow use graphs, and how do you create them manually?</li>
<li>What is the difference between eager execution mode and lazy execution mode?</li>
<li>How do you log information in TensorBoard, and how do you display it?</li>
<li>What are the main differences between TensorFlow 1 and TensorFlow 2?</li>
</ol>


            </article>

            
        </section>
    </body></html>