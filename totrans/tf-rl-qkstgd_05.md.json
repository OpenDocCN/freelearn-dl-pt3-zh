["```\nimport tensorflow as tf\nimport numpy as np\nimport gym\nfrom gym import wrappers\n\nimport argparse\nimport pprint as pp\nimport sys\n\nfrom replay_buffer import ReplayBuffer\nfrom AandC import *\nfrom TrainOrTest import *\n```", "```\ndef train(args):\n\n    with tf.Session() as sess:\n\n        env = gym.make(args['env'])\n        np.random.seed(int(args['random_seed']))\n        tf.set_random_seed(int(args['random_seed']))\n        env.seed(int(args['random_seed']))\n        env._max_episode_steps = int(args['max_episode_len'])\n\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.shape[0]\n        action_bound = env.action_space.high\n\n        actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n                float(args['actor_lr']), float(args['tau']), int(args['minibatch_size']))\n\n        critic = CriticNetwork(sess, state_dim, action_dim,\n                 float(args['critic_lr']), float(args['tau']), float(args['gamma']), actor.get_num_trainable_vars())\n\n        trainDDPG(sess, env, args, actor, critic)\n\n        saver = tf.train.Saver()\n        saver.save(sess, \"ckpt/model\")\n        print(\"saved model \")\n```", "```\ndef test(args):\n\n    with tf.Session() as sess:\n\n        env = gym.make(args['env'])\n        np.random.seed(int(args['random_seed']))\n        tf.set_random_seed(int(args['random_seed']))\n        env.seed(int(args['random_seed']))\n        env._max_episode_steps = int(args['max_episode_len'])\n\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.shape[0]\n        action_bound = env.action_space.high\n\n        actor = ActorNetwork(sess, state_dim, action_dim, action_bound,\n                float(args['actor_lr']), float(args['tau']), int(args['minibatch_size']))\n\n        critic = CriticNetwork(sess, state_dim, action_dim,\n                 float(args['critic_lr']), float(args['tau']), float(args['gamma']), actor.get_num_trainable_vars())\n\n        saver = tf.train.Saver()\n        saver.restore(sess, \"ckpt/model\")\n\n        testDDPG(sess, env, args, actor, critic)\n```", "```\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='provide arguments for DDPG agent')\n\n    # agent parameters\n    parser.add_argument('--actor-lr', help='actor network learning rate', default=0.0001)\n    parser.add_argument('--critic-lr', help='critic network learning rate', default=0.001)\n    parser.add_argument('--gamma', help='discount factor for Bellman updates', default=0.99)\n    parser.add_argument('--tau', help='target update parameter', default=0.001)\n    parser.add_argument('--buffer-size', help='max size of the replay buffer', default=1000000)\n    parser.add_argument('--minibatch-size', help='size of minibatch', default=64)\n\n    # run parameters\n    parser.add_argument('--env', help='gym env', default='Pendulum-v0')\n    parser.add_argument('--random-seed', help='random seed', default=258)\n    parser.add_argument('--max-episodes', help='max num of episodes', default=250)\n    parser.add_argument('--max-episode-len', help='max length of each episode', default=1000)\n    parser.add_argument('--render-env', help='render gym env', action='store_true')\n    parser.add_argument('--mode', help='train/test', default='train')\n\n    args = vars(parser.parse_args())\n\n    pp.pprint(args)\n\n    if (args['mode'] == 'train'):\n      train(args)\n    elif (args['mode'] == 'test'):\n      test(args)\n```", "```\nimport tensorflow as tf\nimport numpy as np\nimport gym\nfrom gym import wrappers\nimport argparse\nimport pprint as pp\nimport sys\n\nfrom replay_buffer import ReplayBuffer\n```", "```\nwinit = tf.contrib.layers.xavier_initializer()\nbinit = tf.constant_initializer(0.01)\nrand_unif = tf.keras.initializers.RandomUniform(minval=-3e-3,maxval=3e-3)\nregularizer = tf.contrib.layers.l2_regularizer(scale=0.0)\n```", "```\nclass ActorNetwork(object):\n\n    def __init__(self, sess, state_dim, action_dim, action_bound, learning_rate, tau, batch_size):\n        self.sess = sess\n        self.s_dim = state_dim\n        self.a_dim = action_dim\n        self.action_bound = action_bound\n        self.learning_rate = learning_rate\n        self.tau = tau\n        self.batch_size = batch_size\n\n        # actor \n        self.state, self.out, self.scaled_out = self.create_actor_network(scope='actor')\n\n        # actor params\n        self.network_params = tf.trainable_variables()\n\n        # target network\n        self.target_state, self.target_out, self.target_scaled_out = self.create_actor_network(scope='act_target')\n        self.target_network_params = tf.trainable_variables()[len(self.network_params):]\n\n```", "```\n# update target using tau and 1-tau as weights\nself.update_target_network_params = \\\n                                   [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) + tf.multiply(self.target_network_params[i], 1\\. - self.tau))\n        for i in range(len(self.target_network_params))]\n\n# gradient (this is provided by the critic)\nself.action_gradient = tf.placeholder(tf.float32, [None, self.a_dim])\n\n# actor gradients\nself.unnormalized_actor_gradients = tf.gradients(\n    self.scaled_out, self.network_params, -self.action_gradient)\nself.actor_gradients = list(map(lambda x: tf.div(x, self.batch_size), self.unnormalized_actor_gradients))\n```", "```\n # adam optimization \n        self.optimize = tf.train.AdamOptimizer(self.learning_rate).apply_gradients(zip(self.actor_gradients, self.network_params))\n\n        # num trainable vars\n        self.num_trainable_vars = len(self.network_params) + len(self.target_network_params)\n```", "```\ndef create_actor_network(self, scope):\n      with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n        state = tf.placeholder(name='a_states', dtype=tf.float32, shape=[None, self.s_dim])\n\n        net = tf.layers.dense(inputs=state, units=400, activation=None, kernel_initializer=winit, bias_initializer=binit, name='anet1') \n        net = tf.nn.relu(net)\n\n        net = tf.layers.dense(inputs=net, units=300, activation=None, kernel_initializer=winit, bias_initializer=binit, name='anet2')\n        net = tf.nn.relu(net)\n\n        out = tf.layers.dense(inputs=net, units=self.a_dim, activation=None, kernel_initializer=rand_unif, bias_initializer=binit, name='anet_out')\n        out = tf.nn.tanh(out)\n        scaled_out = tf.multiply(out, self.action_bound)\n        return state, out, scaled_out\n```", "```\ndef train(self, state, a_gradient):\n        self.sess.run(self.optimize, feed_dict={self.state: state, self.action_gradient: a_gradient})\n\ndef predict(self, state):\n        return self.sess.run(self.scaled_out, feed_dict={\n            self.state: state})\n\ndef predict_target(self, state):\n        return self.sess.run(self.target_scaled_out, feed_dict={\n            self.target_state: state})\n\ndef update_target_network(self):\n        self.sess.run(self.update_target_network_params)\n\ndef get_num_trainable_vars(self):\n        return self.num_trainable_vars\n```", "```\nclass CriticNetwork(object):\n\n    def __init__(self, sess, state_dim, action_dim, learning_rate, tau, gamma, num_actor_vars):\n        self.sess = sess\n        self.s_dim = state_dim\n        self.a_dim = action_dim\n        self.learning_rate = learning_rate\n        self.tau = tau\n        self.gamma = gamma\n\n        # critic\n        self.state, self.action, self.out = self.create_critic_network(scope='critic')\n\n        # critic params\n        self.network_params = tf.trainable_variables()[num_actor_vars:]\n\n        # target Network\n        self.target_state, self.target_action, self.target_out = self.create_critic_network(scope='crit_target')\n\n        # target network params \n        self.target_network_params = tf.trainable_variables()[(len(self.network_params) + num_actor_vars):]\n```", "```\n# update target using tau and 1 - tau as weights\n        self.update_target_network_params = \\\n            [self.target_network_params[i].assign(tf.multiply(self.network_params[i], self.tau) \\\n            + tf.multiply(self.target_network_params[i], 1\\. - self.tau))\n                for i in range(len(self.target_network_params))]\n\n        # network target (y_i in the paper)\n        self.predicted_q_value = tf.placeholder(tf.float32, [None, 1])\n\n        # adam optimization; minimize L2 loss function\n        self.loss = tf.reduce_mean(tf.square(self.predicted_q_value - self.out))\n        self.optimize = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n\n        # gradient of Q w.r.t. action\n        self.action_grads = tf.gradients(self.out, self.action)\n```", "```\ndef create_critic_network(self, scope):\n        with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):\n           state = tf.placeholder(name='c_states', dtype=tf.float32, shape=[None, self.s_dim])\n           action = tf.placeholder(name='c_action', dtype=tf.float32, shape=[None, self.a_dim]) \n\n           net = tf.concat([state, action],1) \n\n           net = tf.layers.dense(inputs=net, units=400, activation=None, kernel_initializer=winit, bias_initializer=binit, name='cnet1') \n           net = tf.nn.relu(net)\n\n           net = tf.layers.dense(inputs=net, units=300, activation=None, kernel_initializer=winit, bias_initializer=binit, name='cnet2') \n           net = tf.nn.relu(net)\n\n           out = tf.layers.dense(inputs=net, units=1, activation=None, kernel_initializer=rand_unif, bias_initializer=binit, name='cnet_out')\n           return state, action, out\n```", "```\ndef train(self, state, action, predicted_q_value):\n        return self.sess.run([self.out, self.optimize], feed_dict={self.state: state, self.action: action, self.predicted_q_value: predicted_q_value})\n\ndef predict(self, state, action):\n        return self.sess.run(self.out, feed_dict={self.state: state, self.action: action})\n\ndef predict_target(self, state, action):\n        return self.sess.run(self.target_out, feed_dict={self.target_state: state, self.target_action: action})\n\ndef action_gradients(self, state, actions):\n        return self.sess.run(self.action_grads, feed_dict={self.state: state, self.action: actions})\n\n    def update_target_network(self):\n        self.sess.run(self.update_target_network_params)\n```", "```\nimport tensorflow as tf\nimport numpy as np\nimport gym\nfrom gym import wrappers\n\nimport argparse\nimport pprint as pp\nimport sys\n\nfrom replay_buffer import ReplayBuffer\nfrom AandC import *\n```", "```\ndef trainDDPG(sess, env, args, actor, critic):\n\n    sess.run(tf.global_variables_initializer())\n\n    # Initialize target networks\n    actor.update_target_network()\n    critic.update_target_network()\n\n    # Initialize replay memory\n    replay_buffer = ReplayBuffer(int(args['buffer_size']), int(args['random_seed']))\n\n    # start training on episodes \n    for i in range(int(args['max_episodes'])):\n\n        s = env.reset()\n\n        ep_reward = 0\n        ep_ave_max_q = 0\n\n        for j in range(int(args['max_episode_len'])):\n\n            if args['render_env']:\n                env.render()\n\n            a = actor.predict(np.reshape(s, (1, actor.s_dim))) \n\n            s2, r, terminal, info = env.step(a[0])\n\n            replay_buffer.add(np.reshape(s, (actor.s_dim,)), np.reshape(a, (actor.a_dim,)), r,\n                              terminal, np.reshape(s2, (actor.s_dim,)))\n```", "```\n# sample from replay buffer\n            if replay_buffer.size() > int(args['minibatch_size']):\n                s_batch, a_batch, r_batch, t_batch, s2_batch = \n                replay_buffer.sample_batch(int(args['minibatch                   \n                _size']))\n\n                # Calculate target q\n                target_q = critic.predict_target(s2_batch,  \n                           actor.predict_target(s2_batch))\n\n                y_i = []\n                for k in range(int(args['minibatch_size'])):\n                    if t_batch[k]:\n                        y_i.append(r_batch[k])\n                    else:\n                        y_i.append(r_batch[k] + critic.gamma * \n                                                target_q[k])\n```", "```\n# Update critic\n                predicted_q_value, _ = critic.train(s_batch, a_batch, np.reshape(y_i, (int(args['minibatch_size']), 1)))\n\n                ep_ave_max_q += np.amax(predicted_q_value)\n\n                # Update the actor policy using gradient\n                a_outs = actor.predict(s_batch)\n                grads = critic.action_gradients(s_batch, a_outs)\n                actor.train(s_batch, grads[0])\n\n                # update target networks\n                actor.update_target_network()\n                critic.update_target_network()\n```", "```\ns = s2\nep_reward += r\n\nif terminal:\n    print('| Episode: {:d} | Reward: {:d} | Qmax: {:.4f}'.format(i,        \n          int(ep_reward), (ep_ave_max_q / float(j))))\n    f = open(\"pendulum.txt\", \"a+\")\n    f.write(str(i) + \" \" + str(int(ep_reward)) + \" \" +    \n            str(ep_ave_max_q / float(j)) + '\\n') \n    break\n```", "```\ndef testDDPG(sess, env, args, actor, critic):\n\n    # test for max_episodes number of episodes\n    for i in range(int(args['max_episodes'])):\n\n        s = env.reset()\n\n        ep_reward = 0\n        ep_ave_max_q = 0\n\n        for j in range(int(args['max_episode_len'])):\n\n            if args['render_env']:\n                env.render()\n\n            a = actor.predict(np.reshape(s, (1, actor.s_dim))) \n\n            s2, r, terminal, info = env.step(a[0])\n\n            s = s2\n            ep_reward += r\n\n            if terminal:\n                print('| Episode: {:d} | Reward: {:d} |'.format(i, \n                      int(ep_reward)))\n                break\n```", "```\nfrom collections import deque\nimport random\nimport numpy as np\n\nclass ReplayBuffer(object):\n\n    def __init__(self, buffer_size, random_seed=258):\n        self.buffer_size = buffer_size\n        self.count = 0\n        self.buffer = deque()\n        random.seed(random_seed)\n```", "```\n    def add(self, s, a, r, t, s2):\n        experience = (s, a, r, t, s2)\n        if self.count < self.buffer_size: \n            self.buffer.append(experience)\n            self.count += 1\n        else:\n            self.buffer.popleft()\n            self.buffer.append(experience)\n\n    def size(self):\n        return self.count\n```", "```\n   def sample_batch(self, batch_size):\n        batch = []\n\n        if self.count < batch_size:\n            batch = random.sample(self.buffer, self.count)\n        else:\n            batch = random.sample(self.buffer, batch_size)\n\n        s_batch = np.array([_[0] for _ in batch])\n        a_batch = np.array([_[1] for _ in batch])\n        r_batch = np.array([_[2] for _ in batch])\n        t_batch = np.array([_[3] for _ in batch])\n        s2_batch = np.array([_[4] for _ in batch])\n\n        return s_batch, a_batch, r_batch, t_batch, s2_batch\n\n    def clear(self):\n        self.buffer.clear()\n        self.count = 0\n\n```", "```\npython ddpg.py\n```", "```\n\n{'actor_lr': 0.0001,\n 'buffer_size': 1000000,\n 'critic_lr': 0.001,\n 'env': 'Pendulum-v0',\n 'gamma': 0.99,\n 'max_episode_len': 1000,\n 'max_episodes': 250,\n 'minibatch_size': 64,\n 'mode': 'train',\n 'random_seed': 258,\n 'render_env': False,\n 'tau': 0.001}\n.\n.\n.\n2019-03-03 17:23:10.529725: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:300] kernel version seems to match DSO: 384.130.0\n| Episode: 0 | Reward: -7981 | Qmax: -6.4859\n| Episode: 1 | Reward: -7466 | Qmax: -10.1758\n| Episode: 2 | Reward: -7497 | Qmax: -14.0578\n```", "```\npython ddpg.py --mode test\n```", "```\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata = np.loadtxt('pendulum.txt')\n\nplt.plot(data[:,0], data[:,1])\nplt.xlabel('episode number', fontsize=12)\nplt.ylabel('episode reward', fontsize=12)\n#plt.show()\nplt.savefig(\"ddpg_pendulum.png\")\n```"]