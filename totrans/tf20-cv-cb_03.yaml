- en: '*Chapter 3*: Harnessing the Power of Pre-Trained Networks with Transfer Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite the undeniable power deep neural networks bring to computer vision,
    they are very complex to tune, train, and make performant. This difficulty comes
    from three main sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks start to pay off when we have sufficient data, but more
    often than not, this is not the case. Furthermore, data is expensive and, sometimes,
    impossible to expand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep neural networks contain a wide range of parameters that need tuning and
    can affect the overall performance of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning is very resource-intensive in terms of time, hardware, and effort.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not be dismayed! With **transfer learning**, we can save ourselves loads
    of time and effort by leveraging the rich amount of knowledge present in seminal
    architectures that have been pre-trained on gargantuan datasets, such as ImageNet.
    And the best part? Besides being such a powerful and useful tool, transfer learning
    is also easy to apply. We'll learn how to do this in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a feature extractor using a pre-trained network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a simple classifier on extracted features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spot-checking extractors and classifiers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using incremental learning to train a classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a network using the Keras API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a network using TFHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s highly encouraged that you have access to a GPU since transfer learning
    tends to be quite computationally heavy. In the *Getting ready* section of each
    recipe, you''ll receive specific instructions – if they''re needed – on how to
    install the dependencies for that recipe. You can find all the code for this chapter
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/39wR6DT](https://bit.ly/39wR6DT).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a feature extractor using a pre-trained network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the easiest ways to seize the power of transfer learning is to use pre-trained
    models as feature extractors. This way, we can combine both deep learning and
    machine learning, something that we normally cannot do, because traditional machine
    learning algorithms don't work with raw images. In this recipe, we'll implement
    a reusable `FeatureExtractor` class to produce a dataset of vectors from a set
    of input images, and then save it in the blazingly fast HDF5 format.
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready? Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You''ll need to install `Pillow` and `tqdm` (which we''ll use to display a
    nice progress bar). Fortunately, this is very easy with `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll be using the `Stanford Cars` dataset, which you can download here: [http://imagenet.stanford.edu/internal/car196/car_ims.tgz](http://imagenet.stanford.edu/internal/car196/car_ims.tgz).
    Decompress the data to a location of your preference. In this recipe, we assume
    the data is inside the `~/.keras/datasets` directory, under the name `car_ims`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sample images from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Sample images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Sample images
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll store the extracted features in HDF5 format, a binary, hierarchical
    protocol designed to store very large numerical datasets on disk, while keeping
    ease of access and computation on a row-wise level. You can read more about HDF5
    here: [https://portal.hdfgroup.org/display/HDF5/HDF5](https://portal.hdfgroup.org/display/HDF5/HDF5).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `FeatureExtractor` class and its constructor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to make sure the output path can be written:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s store the input parameter as object members:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`self.buffer` will contain a buffer of both instances and labels, while `self.current_index`
    will point to the next free location within the datasets in the inner HDF5 database.
    We''ll create this now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method that will extract features and labels from a list of image
    paths and store them in the `HDF5` database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After shuffling the image paths and their labels, as well as encoding and storing
    the latter, we''ll iterate over batches of images, passing them through the pre-trained
    network. Once we''ve done this, we''ll save the resulting features into the HDF5
    database (the helper methods we''ve used here will be defined shortly):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a private method that will add features and labels to the corresponding
    datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a private method that will flush the buffers to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a private method that will store the class labels in the HDF5 database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a private method that will close the HDF5 dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the paths to the images in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the output directory. We''ll create a dataset of rotated car images
    so that a potential classifier can learn how to correctly revert the photos back
    to their original orientation, by correctly predicting the rotation angle:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a copy of the dataset with random rotations performed on the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate `FeatureExtractor` while using a pre-trained `VGG16` network to
    extract features from the images in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the features and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After several minutes, there should be a file named `features.hdf5` in `~/.keras/datasets/car_ims_rotated`.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a reusable component in order to use pre-trained
    networks on ImageNet, such as `Logistic Regression` and `Support Vector Machines`.
  prefs: []
  type: TYPE_NORMAL
- en: Because image datasets tend to be too big to fit in memory, we resorted to the
    high-performance, user-friendly HDF5 format, which is perfect for storing large
    numeric data on disk, while also keeping the ease of access that's typical of
    `NumPy`. This means we can interact with HDF5 datasets *as if they were* regular
    `NumPy` arrays, making them compatible with the whole `SciPy` ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of `FeatureExtractor` is a hierarchical HDF5 file (think of it as
    a folder in a filesystem) containing three datasets: `features`, which contains
    the feature vectors, `labels`, which stores the encoded labels, and `label_names`,
    which holds the human-readable labels prior to encoding.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used `FeatureExtractor` to create a binary representation of a dataset
    of car images rotated 0º, 90º, 180º, or 270º.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the modified version of the `Stanford Cars` dataset we just worked
    on in future recipes in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information on the `Stanford Cars` dataset, you can visit the official
    page here: [https://ai.stanford.edu/~jkrause/cars/car_dataset.html](https://ai.stanford.edu/~jkrause/cars/car_dataset.html).
    To learn more about HDF5, head to the official HDF Group website: [https://www.hdfgroup.org/](https://www.hdfgroup.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: Training a simple classifier on extracted features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning algorithms are not properly equipped to work with tensors,
    which forbid them from learning directly from images. However, by using pre-trained
    networks as feature extractors, we close this gap, enabling us to access the power
    of widely popular, battle-tested algorithms such as **Logistic Regression**, **Decision
    Trees,** and **Support Vector Machines**.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll use the features we generated in the previous recipe (in
    HDF5 format) to train an image orientation detector to correct the degrees of
    rotation of a picture, to restore its original state.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned in the introduction to this reipce, we''ll use the `features.hdf5`
    dataset we generated in the previous recipe, which contains encoded information
    about rotated images from the `Stanford Cars` dataset. We assume the dataset is
    in the following location: `~/.keras/datasets/car_ims_rotated/features.hdf5`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some rotated samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Example of a car rotated 180º (left), and another rotated 90º
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Example of a car rotated 180º (left), and another rotated 90º (right)
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset in HDF5 format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because the dataset is too big, we''ll only work with 50% of the data. The
    following block splits both the features and labels in half:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take the first 80% of the data to train the model, and the remaining 20% to
    evaluate it later on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train a cross-validated `LogisticRegressionCV` will find the best `C` parameter
    using cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that `n_jobs=-1` means we'll use all available cores to find the best
    model in parallel. You can adjust this value based on the capacity of your hardware.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluate the model on the test set. We''ll compute a classification report
    to get a fine-grained view of the model''s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints the following report:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model does a good job of discriminating between the four classes, achieving
    an overall accuracy of 99% on the test set!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, close the HDF5 file to free up any resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We'll understand how this all works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We just trained a very simple **Logistic Regression** model to detect the degree
    of rotation in an image. To achieve this, we leveraged the rich and expressive
    features we extracted using a pre-trained **VGG16** network on ImageNet (for a
    deeper explanation, refer to the first recipe of this chapter).
  prefs: []
  type: TYPE_NORMAL
- en: Because this data is too big, and **scikit-learn**'s machine learning algorithms
    work with the full data in one go (more specifically, most of them cannot work
    in batches), we only used 50% of the features and labels, due to memory constraints.
  prefs: []
  type: TYPE_NORMAL
- en: After a couple of minutes, we obtained an incredible performance of 99% on the
    test set. Moreover, by analyzing the classification report, we can see that the
    model is very confident in its predictions, achieving an F1 score of at least
    0.99 in all four cases.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on how to extract features from pre-trained networks, refer
    to the *Implementing a feature extractor using a pre-trained network* recipe in
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Spot-checking extractors and classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, when we are tackling a new project, we are victims of the Paradox of
    Choice: we don''t know where or how to start due to the presence of so many options
    to choose from. Which feature extractor is the best? What''s the most performant
    model we can train? How should we pre-process our data?'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement a framework that will automatically spot-check
    feature extractors and classifiers. The goal is not to get the best possible model
    right away, but to narrow down our options so that we can focus on the most promising
    ones at a later stage.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we must install `Pillow` and `tqdm`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use a dataset called `17 Category Flower Dataset`, available here: [http://www.robots.ox.ac.uk/~vgg/data/flowers/17](http://www.robots.ox.ac.uk/~vgg/data/flowers/17).
    However, a curated version, organized into subfolders per class, can be downloaded
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip).
    Unzip it in a location of your preference. In this recipe, we assume the data
    is inside the `~/.keras/datasets` directory, under the name `flowers17`.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll reuse the `FeatureExtractor()` class we defined in the *Implementing
    a feature extractor using a pre-trained network* recipe, at the start of this
    chapter. Refer to it if you want to learn more about it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some example images from the dataset for this recipe, `17
    Category Flower Dataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Example images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Example images
  prefs: []
  type: TYPE_NORMAL
- en: With the preparation out of the way, let's get to it!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will allow us to spot-check several combinations of feature
    extractors and machine learning algorithms. Follow these steps to complete this
    recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the input size of all the feature extractors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will obtain a list of tuples of pre-trained networks,
    along with the dimensionality of the vectors they output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that returns a `dict` of machine learning models to spot-check:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the path to the dataset, as well as a list of all image paths:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the labels into memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define some variables in order to keep track of the spot-checking process.
    `final_report` will contain the accuracy of each classifier, trained on the features
    produced by different pre-trained networks. `best_model`, `best_accuracy`, and
    `best_features` will contain the name of the best model, its accuracy, and the
    name of the pre-trained network that produced the features, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate over each pre-trained network, using it to extract features from the
    images in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take 80% of the data to train, and 20% to test:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the extracted features in the current iteration, go over all the machine
    learning models, training them on the training set and evaluating them on the
    test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check if we have a new best model. If that''s the case, update the proper variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Store the results of this iteration in `final_report` and free the resources
    of the HDF5 file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update `final_report` with the information of the best model. Finally, write
    it to disk:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Examining the `final_report.json` file, we can see that the best model is a
    `PAClf` (`PassiveAggressiveClassifier`), which achieved an accuracy of 0.934 (93.4%)
    on the test set and was trained on the features we extracted from a **VGG19**
    network. You can check the full output here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/final_report.json](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/final_report.json).
    Let''s head over to the next section to study the project we completed in this
    recipe in more detail.'
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we developed a framework that automatically enabled us to spot-check
    40 different machine learning algorithms by using the features produced by five
    different pre-trained networks, resulting in 200 experiments. Leveraging the results
    of this approach, we found that the best model combination for this particular
    problem was a `PassiveAggressiveClassifier` trained on vectors produced by a **VGG19**
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we did not focus on achieving maximal performance, but rather on
    making an educated decision, based on hard evidence, on where to spend our time
    and resources if we were to optimize a classifier on this dataset. Now, we know
    that fine-tuning a **Passive Aggressive Classifier** will, most likely, pay off.
    How long would it have taken us to arrive at this conclusion? Hours or maybe days.
  prefs: []
  type: TYPE_NORMAL
- en: The power of letting the computer do the heavy lifting is that we don't have
    to guess and, at the same time, are free to spend our time on other tasks. It's
    great, isn't it?
  prefs: []
  type: TYPE_NORMAL
- en: Using incremental learning to train a classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the problems of traditional machine learning libraries, such as **scikit-learn**,
    is that they seldom offer the possibility to train models on high volumes of data,
    which, coincidentally, is the best type of data for deep neural networks. What
    good is having large amounts of data if we can't use it?
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is a way to circumvent this limitation, and it's called `creme`,
    to train a classifier on a dataset too big to fit in memory.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll leverage `creme`, an experimental library specifically
    designed to train machine learning models on huge datasets that are too big to
    fit in memory. To install `creme`, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use the `features.hdf5` dataset we generated in the *Implementing a
    feature extractor using a pre-trained network* recipe in this chapter, which contains
    encoded information about rotated images from the `Stanford Cars` dataset. We
    assume the dataset is in the following location: `~/.keras/datasets/car_ims_rotated/features.hdf5`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some sample images from this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Example of a car rotated 90º (left), and another rotated 0º
    (right)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Example of a car rotated 90º (left), and another rotated 0º (right)
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will guide us through how to incrementally train a classifier
    on big data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will save a dataset as a CSV file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll have one column for the class of each feature, and as many columns of
    elements in each feature vector. Next, let''s write the contents of the CSV file
    in batches, starting with the header:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the batch in this iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, write all the rows in the batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset in HDF5 format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the split index to separate the data into training (80%) and test (20%)
    chunks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write the training and test subsets to disk as CSV files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`creme` requires us to specify the type of each column in the CSV file as a
    `dict`. instance The following block specifies that `class` should be encoded
    as `int`, while the remaining columns, corresponding to the features, should be
    of the `float` type:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the following code, we are defining a `creme` pipeline, where each input
    will be standardized prior to being passed to the classifier. Because this is
    a multi-class problem, we need to wrap `LogisticRegression` with `OneVsRestClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define `Accuracy` as the target metric and create an iterator over the `train.csv`
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the classifier, one example at a time. Print the running accuracy every
    100 examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an iterator over the `test.csv` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model on the test set once more, one sample at a time:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After several minutes, we should have a model with around 99% accuracy on the
    test set. We'll look at this in more detail in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, even though we have massive amounts of data at our disposal, we are unable
    to use it all due to hardware or software limitations (in the *Training a simple
    classifier on extracted features* recipe, we had to use only 50%, because we couldn't
    keep it all in memory). However, with incremental learning (also known as online
    learning), we can train traditional machine learning models in batches, similar
    to what we can do with neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, in order to seize the totality of the feature vector from our
    `Stanford Cars` dataset, we had to write both the training and test sets into
    CSV files. Next, we trained `LogisticRegression` and wrapped it inside `OneVsRestClassifier`,
    which learned to detect the degrees of rotation in the feature vectors of the
    images. Finally, we achieved a very satisfying 99% accuracy on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a network using the Keras API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps one of the greatest advantages of transfer learning is its ability
    to seize the tailwind produced by the knowledge encoded in pre-trained networks.
    By simply swapping the shallower layers in one of these networks, we can obtain
    remarkable performance on new, unrelated datasets, even if our data is small.
    Why? Because the information in the bottom layers is virtually universal: It encodes
    basic forms and shapes that apply to almost any computer vision problem.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll fine-tune a pre-trained **VGG16** network on a tiny dataset,
    achieving an otherwise unlikely high accuracy score.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will need `Pillow` for this recipe. We can install it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll be using a dataset known as `17 Category Flower Dataset`, which is available
    here: [http://www.robots.ox.ac.uk/~vgg/data/flowers/17](http://www.robots.ox.ac.uk/~vgg/data/flowers/17).
    A version of it that''s been organized into subfolders per class can be found
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip).
    Download and decompress it in a location of your choosing. From now on, we''ll
    assume the data is in `~/.keras/datasets/flowers17`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some sample images from this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Example images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – Example images
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fine-tuning is easy! Follow these steps to complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will build a new network from a pre-trained model, where
    the top fully connected layers will be brand new and adapted to the problem at
    hand:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load the images and labels in the dataset as `NumPy`
    arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the image paths and extract the set of classes from them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the images and normalize them, one-hot encode the labels with `LabelBinarizer()`,
    and split the data into subsets for training (80%) and testing (20%):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a pre-trained `VGG16`, without the top layers. Specify an input
    shape of 256x256x3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Freeze all the layers in the base model. We are doing this because we don''t
    want to re-train them, but use their existing knowledge:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the full network with a new set of layers on top using `build_network()`
    (defined in *Step 3*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the batch size and a set of augmentations to be applied through `ImageDataGenerator()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Warm up the network. This means we''ll only train the new layers (the rest
    are frozen) for 20 epochs, using **RMSProp** with a learning rate of 0.001\. Finally,
    we''ll evaluate the network on the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the network has been warmed up, we''ll fine-tune the final layers
    of the base model, specifically from the 16th onward (remember, zero-indexing),
    along with the fully connected layers, for 50 epochs, using **SGD** with a learning
    rate of 0.001:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After warming up, the network achieved 81.6% accuracy on the test set. Then,
    when we fine-tuned it, after 50 epochs, the accuracy rose to 94.5% on the test
    set. We'll see how this all works in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We successfully harnessed the knowledge of a pre-trained **VGG16** on the massive
    ImageNet database. By replacing the top layers, which are fully connected and
    are in charge of the actual classification (the rest act as feature extractors),
    with our own set of deep layers suited to our problem, we managed to obtain a
    more than decent 94.5% accuracy on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: This result is a demonstration of the power of transfer learning, especially
    considering we only have 81 images per class in the dataset (81x17=1,377 in total),
    an insufficient amount for training a good performing deep learning model from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Although not always required, when fine-tuning networks, it is a good idea to
    first *warm up* the *head* (the fully connected layers at the top) to give them
    time to get accustomed to the features coming from the pre-trained networks.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about Keras pre-trained models here: https://www.tensorflow.org/api_docs/python/tf/keras/applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a network using TFHub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the easiest ways to fine-tune a network is to rely on the wealth of pre-trained
    models that live in **TensorFlow Hub** (**TFHub**). In this recipe, we'll fine-tune
    a **ResNetV1152** feature extractor to classify flowers from a very small dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will need `tensorflow-hub` and `Pillow` for this recipe. Both can be installed
    easily, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll use a dataset known as `17 Category Flower Dataset`, which can be accessed
    at [http://www.robots.ox.ac.uk/~vgg/data/flowers/17](http://www.robots.ox.ac.uk/~vgg/data/flowers/17).
    I encourage you to get a re-organized copy of the data here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip).
    Download and decompress it in a location of your choosing. From now on, we''ll
    assume the data is in `~/.keras/datasets/flowers17`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some sample images from this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Example images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Example images
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to successfully complete this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the random seed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will build a new network from a pre-trained model, where
    the top fully connected layer will be brand new and adapted to the number of categories
    in our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function that will load the images and labels in the dataset as `NumPy`
    arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the image paths and extract the set of classes from them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the images and normalize them, one-hot encode the labels with `LabelBinarizer()`,
    and split the data into subsets for training (80%) and testing (20%):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a pre-trained `KerasLayer()` class, indicating an input shape of
    256x256x3:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make the base model untrainable:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the full network while using the base model as a starting point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the batch size and a set of augmentations to be applied through `ImageDataGenerator()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the full model for 20 epochs and evaluate its performance on the test
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In a matter of minutes, we obtained a model with an accuracy of around 95.22%
    on the test set. Awesome, don't you think? Now, let's dive deeper.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We leveraged the knowledge encoded in the pre-trained `17 Category Flower Dataset`.
  prefs: []
  type: TYPE_NORMAL
- en: With just a quick top layer swap, we managed to obtain an impressive 95.22%
    accuracy on the test set, which is not a small feat, all constraints considered.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the *Fine-tuning a network using the Keras API* recipe, we didn't warm
    up the model's head this time. Again, this is not a hard rule, but yet another
    tool in our toolbox that we should try on a per-project basis.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about the pre-trained model we used in this recipe here:
    [https://tfhub.dev/google/imagenet/resnet_v1_152/feature_vector/4](https://tfhub.dev/google/imagenet/resnet_v1_152/feature_vector/4).'
  prefs: []
  type: TYPE_NORMAL
