- en: '*Chapter 3*: Harnessing the Power of Pre-Trained Networks with Transfer Learning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite the undeniable power deep neural networks bring to computer vision,
    they are very complex to tune, train, and make performant. This difficulty comes
    from three main sources:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Deep neural networks start to pay off when we have sufficient data, but more
    often than not, this is not the case. Furthermore, data is expensive and, sometimes,
    impossible to expand.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep neural networks contain a wide range of parameters that need tuning and
    can affect the overall performance of the model.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning is very resource-intensive in terms of time, hardware, and effort.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not be dismayed! With **transfer learning**, we can save ourselves loads
    of time and effort by leveraging the rich amount of knowledge present in seminal
    architectures that have been pre-trained on gargantuan datasets, such as ImageNet.
    And the best part? Besides being such a powerful and useful tool, transfer learning
    is also easy to apply. We'll learn how to do this in this chapter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to cover the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a feature extractor using a pre-trained network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a simple classifier on extracted features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spot-checking extractors and classifiers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using incremental learning to train a classifier
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a network using the Keras API
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning a network using TFHub
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's get started!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s highly encouraged that you have access to a GPU since transfer learning
    tends to be quite computationally heavy. In the *Getting ready* section of each
    recipe, you''ll receive specific instructions – if they''re needed – on how to
    install the dependencies for that recipe. You can find all the code for this chapter
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/39wR6DT](https://bit.ly/39wR6DT).'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a feature extractor using a pre-trained network
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the easiest ways to seize the power of transfer learning is to use pre-trained
    models as feature extractors. This way, we can combine both deep learning and
    machine learning, something that we normally cannot do, because traditional machine
    learning algorithms don't work with raw images. In this recipe, we'll implement
    a reusable `FeatureExtractor` class to produce a dataset of vectors from a set
    of input images, and then save it in the blazingly fast HDF5 format.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready? Let's get started!
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You''ll need to install `Pillow` and `tqdm` (which we''ll use to display a
    nice progress bar). Fortunately, this is very easy with `pip`:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We''ll be using the `Stanford Cars` dataset, which you can download here: [http://imagenet.stanford.edu/internal/car196/car_ims.tgz](http://imagenet.stanford.edu/internal/car196/car_ims.tgz).
    Decompress the data to a location of your preference. In this recipe, we assume
    the data is inside the `~/.keras/datasets` directory, under the name `car_ims`.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sample images from the dataset:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Sample images'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_001.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Sample images
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll store the extracted features in HDF5 format, a binary, hierarchical
    protocol designed to store very large numerical datasets on disk, while keeping
    ease of access and computation on a row-wise level. You can read more about HDF5
    here: [https://portal.hdfgroup.org/display/HDF5/HDF5](https://portal.hdfgroup.org/display/HDF5/HDF5).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the `FeatureExtractor` class and its constructor:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We need to make sure the output path can be written:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let''s store the input parameter as object members:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`self.buffer` will contain a buffer of both instances and labels, while `self.current_index`
    will point to the next free location within the datasets in the inner HDF5 database.
    We''ll create this now:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Define a method that will extract features and labels from a list of image
    paths and store them in the `HDF5` database:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After shuffling the image paths and their labels, as well as encoding and storing
    the latter, we''ll iterate over batches of images, passing them through the pre-trained
    network. Once we''ve done this, we''ll save the resulting features into the HDF5
    database (the helper methods we''ve used here will be defined shortly):'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Define a private method that will add features and labels to the corresponding
    datasets:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define a private method that will flush the buffers to disk:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define a private method that will store the class labels in the HDF5 database:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Define a private method that will close the HDF5 dataset:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Load the paths to the images in the dataset:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the output directory. We''ll create a dataset of rotated car images
    so that a potential classifier can learn how to correctly revert the photos back
    to their original orientation, by correctly predicting the rotation angle:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Create a copy of the dataset with random rotations performed on the images:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Instantiate `FeatureExtractor` while using a pre-trained `VGG16` network to
    extract features from the images in the dataset:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Extract the features and labels:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: After several minutes, there should be a file named `features.hdf5` in `~/.keras/datasets/car_ims_rotated`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a reusable component in order to use pre-trained
    networks on ImageNet, such as `Logistic Regression` and `Support Vector Machines`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: Because image datasets tend to be too big to fit in memory, we resorted to the
    high-performance, user-friendly HDF5 format, which is perfect for storing large
    numeric data on disk, while also keeping the ease of access that's typical of
    `NumPy`. This means we can interact with HDF5 datasets *as if they were* regular
    `NumPy` arrays, making them compatible with the whole `SciPy` ecosystem.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of `FeatureExtractor` is a hierarchical HDF5 file (think of it as
    a folder in a filesystem) containing three datasets: `features`, which contains
    the feature vectors, `labels`, which stores the encoded labels, and `label_names`,
    which holds the human-readable labels prior to encoding.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we used `FeatureExtractor` to create a binary representation of a dataset
    of car images rotated 0º, 90º, 180º, or 270º.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: We'll use the modified version of the `Stanford Cars` dataset we just worked
    on in future recipes in this chapter.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information on the `Stanford Cars` dataset, you can visit the official
    page here: [https://ai.stanford.edu/~jkrause/cars/car_dataset.html](https://ai.stanford.edu/~jkrause/cars/car_dataset.html).
    To learn more about HDF5, head to the official HDF Group website: [https://www.hdfgroup.org/](https://www.hdfgroup.org/).'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Training a simple classifier on extracted features
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning algorithms are not properly equipped to work with tensors,
    which forbid them from learning directly from images. However, by using pre-trained
    networks as feature extractors, we close this gap, enabling us to access the power
    of widely popular, battle-tested algorithms such as **Logistic Regression**, **Decision
    Trees,** and **Support Vector Machines**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll use the features we generated in the previous recipe (in
    HDF5 format) to train an image orientation detector to correct the degrees of
    rotation of a picture, to restore its original state.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As we mentioned in the introduction to this reipce, we''ll use the `features.hdf5`
    dataset we generated in the previous recipe, which contains encoded information
    about rotated images from the `Stanford Cars` dataset. We assume the dataset is
    in the following location: `~/.keras/datasets/car_ims_rotated/features.hdf5`.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some rotated samples:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Example of a car rotated 180º (left), and another rotated 90º
    (right)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_002.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – Example of a car rotated 180º (left), and another rotated 90º (right)
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this recipe:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Load the dataset in HDF5 format:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Because the dataset is too big, we''ll only work with 50% of the data. The
    following block splits both the features and labels in half:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Take the first 80% of the data to train the model, and the remaining 20% to
    evaluate it later on:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Train a cross-validated `LogisticRegressionCV` will find the best `C` parameter
    using cross-validation:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Notice that `n_jobs=-1` means we'll use all available cores to find the best
    model in parallel. You can adjust this value based on the capacity of your hardware.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Evaluate the model on the test set. We''ll compute a classification report
    to get a fine-grained view of the model''s performance:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This prints the following report:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The model does a good job of discriminating between the four classes, achieving
    an overall accuracy of 99% on the test set!
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, close the HDF5 file to free up any resources:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We'll understand how this all works in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We just trained a very simple **Logistic Regression** model to detect the degree
    of rotation in an image. To achieve this, we leveraged the rich and expressive
    features we extracted using a pre-trained **VGG16** network on ImageNet (for a
    deeper explanation, refer to the first recipe of this chapter).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Because this data is too big, and **scikit-learn**'s machine learning algorithms
    work with the full data in one go (more specifically, most of them cannot work
    in batches), we only used 50% of the features and labels, due to memory constraints.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: After a couple of minutes, we obtained an incredible performance of 99% on the
    test set. Moreover, by analyzing the classification report, we can see that the
    model is very confident in its predictions, achieving an F1 score of at least
    0.99 in all four cases.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on how to extract features from pre-trained networks, refer
    to the *Implementing a feature extractor using a pre-trained network* recipe in
    this chapter.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Spot-checking extractors and classifiers
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, when we are tackling a new project, we are victims of the Paradox of
    Choice: we don''t know where or how to start due to the presence of so many options
    to choose from. Which feature extractor is the best? What''s the most performant
    model we can train? How should we pre-process our data?'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will implement a framework that will automatically spot-check
    feature extractors and classifiers. The goal is not to get the best possible model
    right away, but to narrow down our options so that we can focus on the most promising
    ones at a later stage.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we must install `Pillow` and `tqdm`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We''ll use a dataset called `17 Category Flower Dataset`, available here: [http://www.robots.ox.ac.uk/~vgg/data/flowers/17](http://www.robots.ox.ac.uk/~vgg/data/flowers/17).
    However, a curated version, organized into subfolders per class, can be downloaded
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip).
    Unzip it in a location of your preference. In this recipe, we assume the data
    is inside the `~/.keras/datasets` directory, under the name `flowers17`.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we'll reuse the `FeatureExtractor()` class we defined in the *Implementing
    a feature extractor using a pre-trained network* recipe, at the start of this
    chapter. Refer to it if you want to learn more about it.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some example images from the dataset for this recipe, `17
    Category Flower Dataset`:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Example images'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_003.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Example images
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: With the preparation out of the way, let's get to it!
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will allow us to spot-check several combinations of feature
    extractors and machine learning algorithms. Follow these steps to complete this
    recipe:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the input size of all the feature extractors:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Define a function that will obtain a list of tuples of pre-trained networks,
    along with the dimensionality of the vectors they output:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define a function that returns a `dict` of machine learning models to spot-check:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Define the path to the dataset, as well as a list of all image paths:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Load the labels into memory:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Define some variables in order to keep track of the spot-checking process.
    `final_report` will contain the accuracy of each classifier, trained on the features
    produced by different pre-trained networks. `best_model`, `best_accuracy`, and
    `best_features` will contain the name of the best model, its accuracy, and the
    name of the pre-trained network that produced the features, respectively:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Iterate over each pre-trained network, using it to extract features from the
    images in the dataset:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Take 80% of the data to train, and 20% to test:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Using the extracted features in the current iteration, go over all the machine
    learning models, training them on the training set and evaluating them on the
    test set:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Check if we have a new best model. If that''s the case, update the proper variables:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Store the results of this iteration in `final_report` and free the resources
    of the HDF5 file:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Update `final_report` with the information of the best model. Finally, write
    it to disk:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Examining the `final_report.json` file, we can see that the best model is a
    `PAClf` (`PassiveAggressiveClassifier`), which achieved an accuracy of 0.934 (93.4%)
    on the test set and was trained on the features we extracted from a **VGG19**
    network. You can check the full output here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/final_report.json](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/final_report.json).
    Let''s head over to the next section to study the project we completed in this
    recipe in more detail.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we developed a framework that automatically enabled us to spot-check
    40 different machine learning algorithms by using the features produced by five
    different pre-trained networks, resulting in 200 experiments. Leveraging the results
    of this approach, we found that the best model combination for this particular
    problem was a `PassiveAggressiveClassifier` trained on vectors produced by a **VGG19**
    network.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Notice that we did not focus on achieving maximal performance, but rather on
    making an educated decision, based on hard evidence, on where to spend our time
    and resources if we were to optimize a classifier on this dataset. Now, we know
    that fine-tuning a **Passive Aggressive Classifier** will, most likely, pay off.
    How long would it have taken us to arrive at this conclusion? Hours or maybe days.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The power of letting the computer do the heavy lifting is that we don't have
    to guess and, at the same time, are free to spend our time on other tasks. It's
    great, isn't it?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Using incremental learning to train a classifier
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the problems of traditional machine learning libraries, such as **scikit-learn**,
    is that they seldom offer the possibility to train models on high volumes of data,
    which, coincidentally, is the best type of data for deep neural networks. What
    good is having large amounts of data if we can't use it?
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there is a way to circumvent this limitation, and it's called `creme`,
    to train a classifier on a dataset too big to fit in memory.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we''ll leverage `creme`, an experimental library specifically
    designed to train machine learning models on huge datasets that are too big to
    fit in memory. To install `creme`, execute the following command:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We''ll use the `features.hdf5` dataset we generated in the *Implementing a
    feature extractor using a pre-trained network* recipe in this chapter, which contains
    encoded information about rotated images from the `Stanford Cars` dataset. We
    assume the dataset is in the following location: `~/.keras/datasets/car_ims_rotated/features.hdf5`.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some sample images from this dataset:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Example of a car rotated 90º (left), and another rotated 0º
    (right)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_004.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – Example of a car rotated 90º (left), and another rotated 0º (right)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following steps will guide us through how to incrementally train a classifier
    on big data:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary packages:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Define a function that will save a dataset as a CSV file:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We''ll have one column for the class of each feature, and as many columns of
    elements in each feature vector. Next, let''s write the contents of the CSV file
    in batches, starting with the header:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Extract the batch in this iteration:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now, write all the rows in the batch:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Load the dataset in HDF5 format:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define the split index to separate the data into training (80%) and test (20%)
    chunks:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Write the training and test subsets to disk as CSV files:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`creme` requires us to specify the type of each column in the CSV file as a
    `dict`. instance The following block specifies that `class` should be encoded
    as `int`, while the remaining columns, corresponding to the features, should be
    of the `float` type:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In the following code, we are defining a `creme` pipeline, where each input
    will be standardized prior to being passed to the classifier. Because this is
    a multi-class problem, we need to wrap `LogisticRegression` with `OneVsRestClassifier`:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Define `Accuracy` as the target metric and create an iterator over the `train.csv`
    dataset:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Train the classifier, one example at a time. Print the running accuracy every
    100 examples:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Create an iterator over the `test.csv` file:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Evaluate the model on the test set once more, one sample at a time:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: After several minutes, we should have a model with around 99% accuracy on the
    test set. We'll look at this in more detail in the next section.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Often, even though we have massive amounts of data at our disposal, we are unable
    to use it all due to hardware or software limitations (in the *Training a simple
    classifier on extracted features* recipe, we had to use only 50%, because we couldn't
    keep it all in memory). However, with incremental learning (also known as online
    learning), we can train traditional machine learning models in batches, similar
    to what we can do with neural networks.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, in order to seize the totality of the feature vector from our
    `Stanford Cars` dataset, we had to write both the training and test sets into
    CSV files. Next, we trained `LogisticRegression` and wrapped it inside `OneVsRestClassifier`,
    which learned to detect the degrees of rotation in the feature vectors of the
    images. Finally, we achieved a very satisfying 99% accuracy on the test set.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a network using the Keras API
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Perhaps one of the greatest advantages of transfer learning is its ability
    to seize the tailwind produced by the knowledge encoded in pre-trained networks.
    By simply swapping the shallower layers in one of these networks, we can obtain
    remarkable performance on new, unrelated datasets, even if our data is small.
    Why? Because the information in the bottom layers is virtually universal: It encodes
    basic forms and shapes that apply to almost any computer vision problem.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll fine-tune a pre-trained **VGG16** network on a tiny dataset,
    achieving an otherwise unlikely high accuracy score.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will need `Pillow` for this recipe. We can install it as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We''ll be using a dataset known as `17 Category Flower Dataset`, which is available
    here: [http://www.robots.ox.ac.uk/~vgg/data/flowers/17](http://www.robots.ox.ac.uk/~vgg/data/flowers/17).
    A version of it that''s been organized into subfolders per class can be found
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip).
    Download and decompress it in a location of your choosing. From now on, we''ll
    assume the data is in `~/.keras/datasets/flowers17`.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个名为`17 Category Flower Dataset`的数据集，可以通过以下链接访问：[http://www.robots.ox.ac.uk/~vgg/data/flowers/17](http://www.robots.ox.ac.uk/~vgg/data/flowers/17)。该数据集的一个版本已经按照每个类的子文件夹进行组织，可以在此链接找到：[https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip)。下载并解压到您选择的位置。从现在起，我们假设数据存储在`~/.keras/datasets/flowers17`目录中。
- en: 'The following are some sample images from this dataset:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是来自该数据集的一些示例图像：
- en: '![Figure 3.5 – Example images'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.5 – 示例图像'
- en: '](img/B14768_03_005.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14768_03_005.jpg)'
- en: Figure 3.5 – Example images
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5 – 示例图像
- en: Let's begin!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: How to do it…
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'Fine-tuning is easy! Follow these steps to complete this recipe:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 微调很简单！按照以下步骤完成这个食谱：
- en: 'Import the necessary dependencies:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的依赖项：
- en: '[PRE55]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Set the random seed:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置随机种子：
- en: '[PRE56]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Define a function that will build a new network from a pre-trained model, where
    the top fully connected layers will be brand new and adapted to the problem at
    hand:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，从预训练模型构建一个新的网络，其中顶部的全连接层将是全新的，并且针对当前问题进行了调整：
- en: '[PRE57]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Define a function that will load the images and labels in the dataset as `NumPy`
    arrays:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将数据集中的图像和标签加载为`NumPy`数组：
- en: '[PRE58]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Load the image paths and extract the set of classes from them:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像路径并从中提取类集合：
- en: '[PRE59]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Load the images and normalize them, one-hot encode the labels with `LabelBinarizer()`,
    and split the data into subsets for training (80%) and testing (20%):'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载图像并对其进行归一化，使用`LabelBinarizer()`进行一热编码，并将数据拆分为训练集（80%）和测试集（20%）：
- en: '[PRE60]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Instantiate a pre-trained `VGG16`, without the top layers. Specify an input
    shape of 256x256x3:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个预训练的`VGG16`模型，去除顶部的全连接层。指定输入形状为256x256x3：
- en: '[PRE61]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Freeze all the layers in the base model. We are doing this because we don''t
    want to re-train them, but use their existing knowledge:'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 冻结基础模型中的所有层。我们这样做是因为我们不希望重新训练它们，而是使用它们已有的知识：
- en: '[PRE62]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Build the full network with a new set of layers on top using `build_network()`
    (defined in *Step 3*):'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`build_network()`（在*步骤 3*中定义）构建一个完整的网络，并在其上添加一组新层：
- en: '[PRE63]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Define the batch size and a set of augmentations to be applied through `ImageDataGenerator()`:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义批处理大小和一组要通过`ImageDataGenerator()`应用的增强方法：
- en: '[PRE64]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Warm up the network. This means we''ll only train the new layers (the rest
    are frozen) for 20 epochs, using **RMSProp** with a learning rate of 0.001\. Finally,
    we''ll evaluate the network on the test set:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预热网络。这意味着我们将只训练新添加的层（其余部分被冻结），训练20个周期，使用**RMSProp**优化器，学习率为0.001。最后，我们将在测试集上评估网络：
- en: '[PRE65]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Now that the network has been warmed up, we''ll fine-tune the final layers
    of the base model, specifically from the 16th onward (remember, zero-indexing),
    along with the fully connected layers, for 50 epochs, using **SGD** with a learning
    rate of 0.001:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，网络已经预热完毕，我们将微调基础模型的最终层，特别是从第16层开始（记住，索引从零开始），以及全连接层，训练50个周期，使用**SGD**优化器，学习率为0.001：
- en: '[PRE66]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: After warming up, the network achieved 81.6% accuracy on the test set. Then,
    when we fine-tuned it, after 50 epochs, the accuracy rose to 94.5% on the test
    set. We'll see how this all works in the next section.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在预热后，网络在测试集上的准确率达到了81.6%。然后，当我们进行了微调后，经过50个周期，测试集上的准确率提高到了94.5%。我们将在下一节看到这一过程的具体细节。
- en: How it works…
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的…
- en: We successfully harnessed the knowledge of a pre-trained **VGG16** on the massive
    ImageNet database. By replacing the top layers, which are fully connected and
    are in charge of the actual classification (the rest act as feature extractors),
    with our own set of deep layers suited to our problem, we managed to obtain a
    more than decent 94.5% accuracy on the test set.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地利用了在庞大的ImageNet数据库上预训练的**VGG16**模型的知识。通过替换顶部的全连接层，这些层负责实际分类（其余部分充当特征提取器），我们使用自己的一组深度层来适应当前问题，从而在测试集上获得了超过94.5%的不错准确率。
- en: This result is a demonstration of the power of transfer learning, especially
    considering we only have 81 images per class in the dataset (81x17=1,377 in total),
    an insufficient amount for training a good performing deep learning model from
    scratch.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Although not always required, when fine-tuning networks, it is a good idea to
    first *warm up* the *head* (the fully connected layers at the top) to give them
    time to get accustomed to the features coming from the pre-trained networks.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about Keras pre-trained models here: https://www.tensorflow.org/api_docs/python/tf/keras/applications.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning a network using TFHub
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the easiest ways to fine-tune a network is to rely on the wealth of pre-trained
    models that live in **TensorFlow Hub** (**TFHub**). In this recipe, we'll fine-tune
    a **ResNetV1152** feature extractor to classify flowers from a very small dataset.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will need `tensorflow-hub` and `Pillow` for this recipe. Both can be installed
    easily, like this:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We''ll use a dataset known as `17 Category Flower Dataset`, which can be accessed
    at [http://www.robots.ox.ac.uk/~vgg/data/flowers/17](http://www.robots.ox.ac.uk/~vgg/data/flowers/17).
    I encourage you to get a re-organized copy of the data here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip).
    Download and decompress it in a location of your choosing. From now on, we''ll
    assume the data is in `~/.keras/datasets/flowers17`.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some sample images from this dataset:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Example images'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_03_006.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Example images
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Follow these steps to successfully complete this recipe:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required packages:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Set the random seed:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Define a function that will build a new network from a pre-trained model, where
    the top fully connected layer will be brand new and adapted to the number of categories
    in our data:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Define a function that will load the images and labels in the dataset as `NumPy`
    arrays:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Load the image paths and extract the set of classes from them:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Load the images and normalize them, one-hot encode the labels with `LabelBinarizer()`,
    and split the data into subsets for training (80%) and testing (20%):'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Instantiate a pre-trained `KerasLayer()` class, indicating an input shape of
    256x256x3:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Make the base model untrainable:'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Build the full network while using the base model as a starting point:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define the batch size and a set of augmentations to be applied through `ImageDataGenerator()`:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Train the full model for 20 epochs and evaluate its performance on the test
    set:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: In a matter of minutes, we obtained a model with an accuracy of around 95.22%
    on the test set. Awesome, don't you think? Now, let's dive deeper.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We leveraged the knowledge encoded in the pre-trained `17 Category Flower Dataset`.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: With just a quick top layer swap, we managed to obtain an impressive 95.22%
    accuracy on the test set, which is not a small feat, all constraints considered.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the *Fine-tuning a network using the Keras API* recipe, we didn't warm
    up the model's head this time. Again, this is not a hard rule, but yet another
    tool in our toolbox that we should try on a per-project basis.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can read more about the pre-trained model we used in this recipe here:
    [https://tfhub.dev/google/imagenet/resnet_v1_152/feature_vector/4](https://tfhub.dev/google/imagenet/resnet_v1_152/feature_vector/4).'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
