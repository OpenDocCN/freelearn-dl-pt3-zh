- en: Predictive Maintenance for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In **Internet of Things** (**IoT**) devices, streaming data is generated for
    one event at a time. DL-based approaches can examine this data in order to diagnose
    the problem across the fleet in real time, and the future health of individual
    units can be predicted in order to enable on-demand maintenance. This strategy
    is known as **predictive** (or **condition-based**) **maintenance**. This approach
    is now emerging as one of the most promising and lucrative industrial applications
    of the IoT.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering these motivations, in this chapter, we will look at how to develop
    a DL solution for predictive maintenance for IoT using the **Turbofan Engine Degradation
    Simulation** dataset. The idea behind predictive maintenance is to determine whether
    the failure patterns of various types can be predictable. Furthermore, we will
    discuss how to collect data from IoT-enabled devices for the predictive maintenance.
    In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Predictive maintenance for IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developing a predictive maintenance application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training ML baselines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the LSTM model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FAQs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive maintenance for IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With advances in real-time data capture and streaming architecture, it is now
    possible to have real-time data monitoring, where an organization can gain real-time
    insight into individual components and all processes. Monitoring still requires
    active involvement and quick responses—for example, an oil well sensor that is
    indicating increased temperature or volume or a network traffic for bot-net activity
    or insider threats.
  prefs: []
  type: TYPE_NORMAL
- en: Let's consider a real-world example called **equipment failures in industrial
    engineering**, which is always considered a costly issue. Conducting preventative
    maintenance at regular intervals has always been the conventional strategy. Consequently,
    the schedules tend to be very unadventurous, which is often based on operator
    experience. This manual intervention has several downsides. Firstly, it tends
    to increase maintenance costs. Secondly, it's impossible to adapt such a setting
    to a highly complex or changing industrial scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting IoT data in an industrial setting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to **RT Insights**, a single jet engine could cost $16 million, and
    on a transatlantic flight it can consume 36,000 gallons of fuel. Today's airline
    fuel prices come to around $54,000 per trip, or more than $5,000 an hour. The
    majority of jet engines are gas turbine engines in which the thermal energy is
    converted into kinetic energy by expanding through nozzles, then into rotational
    mechanical energy in a spinning rotor. Such engines produce huge amounts of IoT
    data. Let's try to perceive how predictive maintenance with ML could help us to
    reduce the maintenance costs.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to collect the sensor data representing healthy and faulty
    operations under different operating conditions, for example, temperature, flow,
    and pressure. In a real-life scenario, those might be deployed in different environments
    and locations (suppose you are in Siberia at an operating temperature of -20 degree
    Celsius with high fluid viscosity, and another one in a Middle Eastern country
    with a temperature of 45 degree Celsius with high fluid viscosity).
  prefs: []
  type: TYPE_NORMAL
- en: 'Even though both of them are supposed to work normally, one of the engines
    might fail sooner because of different operating conditions. Unfortunately, without
    having enough data, there''s no further way to investigate the root cause of the
    failure. Once such a jet turbine engine is deployed, sensor data can be collected
    using streaming technologies in the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: Real sensor data from normal system operations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real sensor data from a system operating in a faulty condition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real sensor data from system failures (*run-to-failure* data)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, if we don''t have many such engines deployed, we won''t have much
    data, which would represent both healthy and faulty conditions and operations.
    There are two workarounds to overcome this data scarcity:'
  prefs: []
  type: TYPE_NORMAL
- en: Using historical data from similar/ the same engine, which might resemble the
    currently-deployed engines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, we can build a mathematical model of the engines and estimate their
    parameters from the available sensor data. Based on the statistical distribution
    and operating conditions, we can then generate failure data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we go with the second option, after generating the sensor data, we can combine
    them with the real sensor data to generate large-scale sensor data for the developing
    predictive maintenance model, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4910870-43c7-4ad2-86a0-fc496bcb2aa0.png)'
  prefs: []
  type: TYPE_IMG
- en: ML techniques for predictive maintenance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Deep learning** (**DL**) techniques can be applied to process the massive
    amount of IoT data and can be an appealing emerging alternative to classical machine
    learning algorithms. The idea is that when equipment is given with sensors and
    networked, a huge amount of sensor data is produced. In a more complex industrial
    setting, data from the sensor channels is quite noisy and fluctuates over time,
    but some of the data does not seem to change at all. This is more-or-less true
    for every industrial setting because the data produced in an IoT setting is a
    multivariate series of sensor measurements each with its own amount of noise containing
    many missing or uninformative values.'
  prefs: []
  type: TYPE_NORMAL
- en: A key step in predictive maintenance application development is identifying
    the **Condition Indicators** (**CIs**) and features from the collected sensor
    data, inspecting the behavior changes of CIs in a predictable way as the system
    degrades. Usually, CIs contain features that help distinguish normal and faulty
    operations and predict **Remaining Useful Life** (**RUL**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The RUL of an engine or machine is the expected life or usage time remaining
    before the engine requires repair or replacement. Consequently, predicting RUL
    from sensor data is key in many predictive-maintenance applications. In the following
    diagram, we can see that the peaks in the frequency data shifts to the left as
    the turbine engine degrades. Therefore, the peak frequency can serve as condition
    indicators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39ae6a54-b44e-4921-92c5-96cb0a42c3f0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'CIs can help us understand healthy and faulty operation in the turbine engine.
    However, they don''t tell us what parts need to be repaired or how much time remains
    until the failure occurs. We either identify the fault types before fixing or
    predict the RUL before the scheduled maintenance. For the former option, use the
    extracted CIs features to train an ML or DL model and identify the fault types,
    such as seal leakage, blocked inlet, or worn bearing, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cd8200d6-66a0-4393-b77d-bac7fb44f88b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the latter strategy, we can also train the ML/DL model to predict the trend
    that the pumps will continue to transition between these two states (current condition
    and the failure). A DL model can capture the relationships between CI features,
    and the degradation path of the turbine engine will help us to predict how much
    time we have until the next failure or when we should schedule maintenance, as
    depicted in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e543f232-57e9-49d5-9d28-cbc13f1dfcd0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, a stable model can be deployed in an industrial setting. The preceding
    steps can be summarized in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f413846e-0190-45cc-b5e9-346f65906fa6.png)'
  prefs: []
  type: TYPE_IMG
- en: Unfortunately, due to the lack of sensor data for predicting fault types, in
    the next section, we will see a hands-on example on predicting RUL using both
    ML and DL techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Example – PM for an aircraft gas turbine engine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To give a real-life glimpse into predictive maintenance, we will use the open
    source **Turbofan Engine Degradation Simulation** dataset, which was released
    in 2008 by the **Prognostics Center of Excellence** at NASA''s Ames research centre.
    The dataset can be downloaded from [https://ti.arc.nasa.gov/c/6/](https://ti.arc.nasa.gov/c/6/).
    We''re thankful to the authors of the following research for providing this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Turbofan Engine Degradation Simulation Data Set, A Saxena and K Goebel (2008), NASA
    Ames Prognostics Data Repository ([https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/)),
    NASA Ames Research Center, Moffett Field, CA.
  prefs: []
  type: TYPE_NORMAL
- en: Describing the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The dataset consists of sensor readings from a fleet of simulated aircraft
    gas turbine engines operating conditions as a multiple multivariate time series.
    The dataset consists of separate training and test sets. The testset is similar
    to the training set, except that each engine''s measurements are truncated some
    (unknown) amount of time before it fails. The data is provided as a ZIP-compressed
    text file with 26 columns of numbers. Each row represents a snapshot of data taken
    during a single operational cycle and each column represents a different variable.
    The columns correspond to the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Unit number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Time, in cycles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational setting 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational setting 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational setting 3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensor measurement 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensor measurement 2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sensor measurement 26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, the dataset has a vector of true RUL values for the data, which
    will be used as the ground truths for training the models.
  prefs: []
  type: TYPE_NORMAL
- en: Exploratory analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To give an idea of the sensor readings in areas such as the physical state
    of the engine (for example, with regard to the temperature of a component, the
    fan speed of the turbine, and so on) we decided to extract the first unit from
    the first dataset for all the sensors on a single engine. For this, we have written
    a script (see `make_dataset.py`) that gets all of the data files from the input
    directory. Then it parses a set of raw data files into a single DataFrame object
    and returns an aggregated representation of all files with the appropriate column
    names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To use this script, first copy all the files in the `data/raw/` directory,
    and then execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will generate three files—`train.csv`, `test.csv`, and `RUL.csv`—for
    the training set, testset, and labels, respectively. Now that our dataset is ready
    for exploratory analysis, we can now read each CSV file as a pandas DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, extract the first unit from the first dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we plot its sensor traces over time on a 7 * 3 = 21 plots grid to see
    all sensor channels. We have to plot the channel corresponding to this position:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As seen in the following diagram, data from the sensor channels is quite noisy
    and fluctuates over time, while other data does not seem to change at all. Each
    sensor''s life cycle is different in terms of the starting and ending value on
    the *x* axis:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb6ee94c-00be-4c6d-8df2-d6ca8e001d0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that each engine has a slightly different lifetime and failure pattern.
    Next, we can visualize the data from all the sensor channels against time for
    a random sample of 10 engines from the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment shows the following graph of random samples of 10
    units from the sensor reading from dataset 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aadcd010-d6a2-44dd-9850-d26e05de78e2.png)'
  prefs: []
  type: TYPE_IMG
- en: From the preceding graph, we can inspect that an engine's progress with respect
    to time is not quite aligned with the others. This is an impedance that does not
    allow us to compare the fifth cycle of one engine to the fifth cycle of another,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Inspecting failure modes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since it is already known when each engine in the training set will fail, we
    can compute a **time before failure** value at each time step, which can be defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Time before failure (TBF) = engine elapsed life at failure time (EEL) - total
    operating lifetime (TOL)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'This number can be considered as the countdown to failure for each engine,
    which allows us to align different engines'' data to a common end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The following shows the sensor channels in the same engines. The only difference
    is that the previous graph is plotted against the time before failure, where each
    engine ends at the same instant (*t=0*). It also gives us a common pattern across
    different engines, which shows that some sensor readings consistently rise or
    fall right before a failure, while others—for example, sensor 14—exhibit different
    failure behaviors<q>:</q>
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f855574e-b052-403b-9850-f700f005902c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This pattern is very common in many predictive maintenance problems: failure
    is often a confluence of different processes, and as a result, things in the real
    world are likely to exhibit multiple failure modes. Due to this unpredictable
    pattern of data, predicting the RUL is very challenging.'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction challenges
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As shown in the following diagram, after observing the engine''s sensor measurements
    and operating conditions for a certain amount of time (133 cycles in the diagram),
    the challenge is to predict the amount of time (in other words, the RUL) that
    the engine will continue to function before it fails:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8291ed7-5473-4b7f-bd0a-29eb8e31a583.png)'
  prefs: []
  type: TYPE_IMG
- en: However, making an incorrect prediction for an ML/DL model is basically underestimating
    the true RUL of a particular engine. This can bring the turbine engine to maintenance
    too early, when it could have operated for a bit longer without any issues arising.
    So, what would happen if our model were to overestimate the true RUL instead?
    In that case, we might allow a degrading aircraft to keep flying, and risk catastrophic
    engine failure. Clearly, the costs of these two outcomes would not be the same.
    Considering these challenges, in the next section, we will focus on using DL-based
    techniques for predicting RUL.
  prefs: []
  type: TYPE_NORMAL
- en: DL for predicting RLU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have discussed, we are trying to calculate the amount of time before an
    engine needs maintenance. What makes this dataset special is that the engines
    run all the way until failure, giving us the precise RLU information for every
    engine at every point in time.
  prefs: []
  type: TYPE_NORMAL
- en: Calculating cut-off times
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider the `FD004` dataset, that contains as much as 249 engines (`engine_no`)
    monitored over time (`time_in_cycles`). Each engine has `operational_settings`
    and `sensor_measurements` recorded for each cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'To train a model that will predict RUL, we can simulate real predictions by
    choosing a random point in the life of the engine and only using the data from
    before that point. We can create features with that restriction easily by using
    cut-off times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding function generates the cut-off times by sampling for both `cutoff_time`
    and `label`, which can be called as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding lines of code show the following RUL and cut-off time for five
    engines only:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e644b28-8122-42c1-be67-27fa8e49a973.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep feature synthesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Then, we generate the features using **Deep Feature Synthesis** (**DFS**).
    For this, we need to establish an entity set structure for our data. We can create
    an engines entity by normalizing the `engine_no` column in the raw data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block will generate the following statistics of the entity
    set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The `ft.dfs` function takes an entity set and stacks primitives such as `max`,
    `min`, and `last` exhaustively across entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: ML baselines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have generated the features, we can start training a first ML model
    called `RandomForestRegressor`. Then, we will gradually move to using DL using **Long
    Short-Term Memory** (**LSTM**) network. **Random forest** (**RF**) is an ensemble
    technique that builds several decision trees and integrates them together to get
    a more accurate and stable prediction. In general, a deeper tree signifies more
    complex decision rules and a better-fitted model for example the following image
    shows Decision tree for university admission data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4dfcb6f0-e778-4951-b1a7-df7b0f07c401.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Consequently, the deeper the tree, the more complex the decision rules and
    the better fitted the model is. This is a direct consequence of Random Forest.
    In other words, the final prediction based on the majority vote from a panel of
    independent juries is always better and more reliable than the best jury. The
    following diagram shows random forest and its assembling technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18820971-61e8-437d-bef1-7aab87e7e702.png)'
  prefs: []
  type: TYPE_IMG
- en: 'So, let''s get started by preparing the separate training set and test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, using the training set, we will check the following baselines:'
  prefs: []
  type: TYPE_NORMAL
- en: Always predict the median value of `y_train`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Always predict the RUL as if every engine has the median lifespan in `X_train`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will check those predictions by finding the mean of the absolute value of
    the errors called the **Mean Absolute Error** (**MAE**) using `RandomForestRegressor`
    from scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block should produce the following output showing the baseline
    `MAE` values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Making predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now we can use our created features to fit `RandomForestRegressor` to our data
    and see whether we can improve on the previous scores:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block should produce the following output showing the baseline
    MAE values and statistics about the engine recording cycles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to prepare both the features and label, which we can do using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The loaded data should have 41,214 recordings from 249 engines in which 21
    sensor measurements are used under three operational settings. Then, we have to
    prepare both the features and labels using the loaded data, which we can do using
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block should produce the following output showing the prediced
    MAE and baseline MEA values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: As seen, the predicted MAE value is lower than both baseline MAE values. Next,
    we try to improve the MAE even more using the LSTM network.
  prefs: []
  type: TYPE_NORMAL
- en: Improving MAE with LSTM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use the Keras-based LSTM network to predict RUL. However, for this,
    we first need to convert the data so that the LSTM model, which expects data in
    three-dimensional format, can consume it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have the data appropriate for the LSTM model, we can construct
    the LSTM network. For this, we have a fancy LSTM network that has only an LSTM
    layer followed by a dense layer, before we apply a dropout layer for better regularization.
    Then, we have another dense layer, before we project the output from this dense
    layer through to the activation layer using the linear activation function so
    that it outputs real-value outputs. We then use the SGD version, called `RMSProp`,
    which tries to optimize the **Mean Square Error** (**MSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we train the LSTM model with the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding lines of code should produce some logs, which give us an idea
    of whether the training and the validation losses are getting reduced across iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that the training has been finished, we can plot the training and validation
    loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block should produce the following graph, in which we can
    see that the validation loss drops below the training loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ea2b8246-0002-4817-9bda-7e6507f9b7a0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The model may be overfitting the training data. Measuring and plotting MAE
    during training may shed more light on this. Let''s take a look at the MAE on
    the testset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We should get an MAE of 38.32, which means the MAE error has been reduced a
    bit (whereas RF gave an MAE of 40.33), which is, however, still not convincing.
    There could be several reasons behind such a high MAE. For example, we do not
    have sufficient training data. Secondly, we used an inefficient method for generating
    the entity set. For the first problem, we can use all the dataset to train the
    model. However, we can also use other regularization techniques, such as a Gaussian
    Noise layer, by specifying the noise threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The Gaussian noise layer can be used as an input layer to add noise directly to
    input variables. This is the traditional use of noise as a regularization method
    in neural networks, which states that the noise can be added before or after the
    use of the activation function. It may make more sense to add this before activation,
    but, nevertheless, both options are possible. In our case, we added a Gaussian
    noise layer with a dropout of 0.2 after the LSTM layer and before the dense layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we have another Gaussian noise layer that adds noise to the linear output
    of a dense layer before a rectified linear activation function. Then, training
    the LSTM model with the same data with noise introduced should produce a slightly
    lower MAE value of around 35.25\. We can even inspect the plot showing the training
    and validation loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfbbf5d2-4100-4d57-b233-eac0fea3614e.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding diagram shows that the training loss and test loss are more or
    less the same, which indicates a better regularization of the model. Hence, the
    model performed better on the testset as well. However, the MAE can still be reduced
    using better quality features, perhaps. Let's explore this with a better feature
    generation technique.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised deep feature synthesis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will see how the entity set structures can contribute to improve the predictive
    accuracy. We will build custom primitives using time-series functions from the
    `tsfresh` library. Before that, we will make cut-off times by selecting a random
    one from the life of each engine. We are going to make five sets of cut-off times
    to use for cross-validation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block should show the cut-off time and the RUL values for
    five engines, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a06a9b2-7315-49ae-9a54-cc2bb226f2b7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we will use an unsupervised way of generating the entity set. As we can
    see, the values of operational settings `1`—`3` are continuous, but they create
    an implicit relation between different engines. Consequently, if two engines have
    a similar operational setting, the sensor measurements give a similar value. The
    idea is to apply the clustering technique through k-means to those settings. Then,
    we create a new entity from clusters with similar values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code segment generates an entity set showing the following relations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to changing our entity set structure, we are also going to use
    the complexity time-series primitive from the `tsfresh` package. Any function
    that takes in a pandas series and outputs a float can be converted into an aggregation
    primitive using the `make_agg_primitive` function, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Using this approach, we managed to generate 12 more features (previously, we
    had 290). Then, we built four more feature matrices with the same feature set
    but different cut-off times. This lets us test the pipeline multiple times before
    using it on test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, using the recursive feature elimination, we again model RF regressors
    so that the model picks only important features, so it makes better predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code block should produce the following output showing predicted
    MAE in each iteration and their average. Additionally, it shows the baseline MAE
    values and statistics about the engine recording cycles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Now let''s again try using LSTM to see whether we can reduce the MAE error:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding lines of code should produce the following diagram, in which
    the validation loss drops below the training loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63d36ef1-c495-4102-b75f-98b71db81b04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we can evaluate the model''s performance based on the MAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code block should produce an MAE of 52.40, which is lower than
    we experienced in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: FAQs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will cover some **frequently asked questions** (**FAQs**),
    which will help you to extend this application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Can we use other deep architectures to make predictions in similar IoT settings?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer**: Yes, using other deep architectures could be a viable option. For
    example, creating a convolutional-LSTM network by combining the predictive power
    of both CNN and LSTM layers has proven to be effective in many use cases, such
    as audio classification, **natural language processing** (**NLP**), and time-series
    forecasting.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sometimes we do not have enough IoT data to train the model flexibly. How
    can we increase the amount of training data?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer**: There are many ways to do this. For example, we can try to generate
    the training set by combining all the engines data. For this, the generated CSV
    files for both training, testing, and RUL would be helpful. Another example might
    be to try to extend the dataset by adding more samples.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Can I perform anomaly detection in an industrial setting?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer**: Yes, you can. In fact, this is very common in an industrial setting
    such as production fault identification, real-time time-series anomaly detection,
    predictive monitoring, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Where can I get data to perform other analytics in an IoT setting?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Answer**: Time-series data from some nominal state to a failed state from
    the **Prognostics Data Repository** can be used for the development of prognostic
    algorithms. See the following link to learn more about the dataset: [https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/](https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/).'
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have looked at how to develop a DL solution for predictive
    maintenance using IoT and the Turbofan Engine Degradation Simulation dataset.
    We started by discussing the exploratory analysis of the dataset before we modeled
    the predictive maintenance using one of the most popular tree-based ensemble techniques
    called **RF**, which uses features from the turbine engines as it is. Then, we
    saw how to improve the predictive accuracy using an LSTM network. The LSTM network
    indeed helps to reduce network errors. Nevertheless, we saw how to add a Gaussian
    noise layer to achieve generalization in the LSTM network, along with dropout.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the potential of DL techniques in all layers of IoT (including
    the sensors/sensing, gateway, and cloud layer) is important. Consequently, developing
    scalable and efficient solutions for IoT-enabled healthcare devices is no exception.
    In the next chapter, we will present a use case that exploits DL for data analysis
    in all potential stages of its life cycle.
  prefs: []
  type: TYPE_NORMAL
