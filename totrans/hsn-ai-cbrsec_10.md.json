["```\nIF amount > $1,000 AND buying_frequency > historical_buying_frequency THEN fraud_likelihood = 90%\n```", "```\nIF distance(new_transaction, last_transaction) > 1000 km AND time_range < 30 min THEN block_transaction\n```", "```\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import BaggingClassifier\n\nbagging = BaggingClassifier(\n            DecisionTreeClassifier(), \n            n_estimators=300,\n            max_samples=100, \n            bootstrap=True\n          )\n```", "```\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\n\nadaboost = AdaBoostClassifier(\n              DecisionTreeClassifier(),\n              n_estimators=300\n           )\n```", "```\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngradient_boost = GradientBoostingClassifier(\n                   max_depth=2, \n                   n_estimators=100, \n                   learning_rate=1.0,\n                   warm_start=True\n                 )\n```", "```\n# From the Imbalanced-Learn library documentation:\n# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.under_sampling.RandomUnderSampler.html\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.under_sampling import RandomUnderSampler \n\nX, y = make_classification(n_classes=2, class_sep=2,\n weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\nn_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\nprint('Original dataset shape %s' % Counter(y))\n\nrus = RandomUnderSampler(random_state=42)\nX_res, y_res = rus.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))\n```", "```\n# From the Imbalanced-Learn library documentation:\n# https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE \n\nX, y = make_classification(n_classes=2, class_sep=2,\n   weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n   n_features=20, n_clusters_per_class=1, n_samples=1000,    \n   random_state=10)\n\nprint('Original dataset shape %s' % Counter(y))\nOriginal dataset shape Counter({1: 900, 0: 100})\n\nsm = SMOTE(random_state=42)\nX_res, y_res = sm.fit_resample(X, y)\nprint('Resampled dataset shape %s' % Counter(y_res))\nResampled dataset shape Counter({0: 900, 1: 900})\n```", "```\n# Rename the dataframe to df\n\ndf = df_data_2\n```", "```\n\nfrom sklearn.model_selection import train_test_split\n\nx = df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\ny = df['Class']\n\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.30,\nrandom_state=0)\n```", "```\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nrfmodel = RandomForestClassifier()\nrfmodel.fit(xtrain,ytrain)\nypredrf = rfmodel.predict(xtest)\n\nprint('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))\n\nAccuracy : 0.999414\n```", "```\nfrom sklearn import ensemble\n\nparams = {'n_estimators': 500, 'max_depth': 3, 'subsample': 0.5,\n          'learning_rate': 0.01, 'min_samples_leaf': 1, 'random_state': 3}\n\nclf = ensemble.GradientBoostingClassifier(**params)\nclf.fit(xtrain, ytrain) \n\ny_pred = clf.predict(xtest) \n\nprint(\"Accuracy is :\")\nprint(metrics.accuracy_score(ytest, y_pred))\n\nAccuracy is : 0.998945085858\n```", "```\nfrom sklearn import metrics\nfrom xgboost.sklearn import XGBClassifier\n\nxgb_model = XGBClassifier()\n\nxgb_model.fit(xtrain, ytrain, eval_metric=['error'], eval_set=[((xtrain, ytrain)),(xtest, ytest)])\n\ny_pred = xgb_model.predict(xtest)  \n\nprint(\"Accuracy is :\")\nprint(metrics.accuracy_score(ytest, y_pred))\n\nAccuracy is : 0.999472542929\n```", "```\nfrom sklearn.metrics import roc_curve\n\nFPR, TPR, OPC = roc_curve(targets, probs)\n```", "```\n# Plotting Sensitivity\nplt.plot(OPC,TPR)\n```", "```\n# Plotting ROC curve\nplt.plot(FPR,TPR)\n```", "```\nfrom sklearn.metrics import auc\n\nAUC = auc(FPR, TPR)\n```", "```\nprint('classification report')\nprint(metrics.classification_report(ytest, ypredrf))\nprint('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))\nprint('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))\n\nclassification report\n             precision    recall  f1-score   support\n\n          0       1.00      1.00      1.00     17030\n          1       0.96      0.73      0.83        33\n\navg / total       1.00      1.00      1.00     17063\n\nAccuracy : 0.999414\nArea under the curve : 0.863607\n```", "```\nprint('classification report')\nprint(metrics.classification_report(ytest, y_pred))\nprint(\"Accuracy is :\")\nprint(metrics.accuracy_score(ytest, y_pred))\nprint('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))\n\nclassification report\n             precision    recall  f1-score   support\n\n          0       1.00      1.00      1.00     17030\n          1       0.74      0.70      0.72        33\n\navg / total       1.00      1.00      1.00     17063\n\nAccuracy is : 0.998945085858\nArea under the curve : 0.848250\n```", "```\nprint('classification report')\nprint(metrics.classification_report(ytest, y_pred))\nprint(\"Accuracy is :\")\nprint(metrics.accuracy_score(ytest, y_pred))\nprint('Area under the curve : %f' % (metrics.roc_auc_score(ytest, y_pred)))\n\nclassification report\n             precision    recall  f1-score   support\n\n          0       1.00      1.00      1.00     17030\n          1       0.93      0.79      0.85        33\n\navg / total       1.00      1.00      1.00     17063\n\nAccuracy is : 0.999472542929\nArea under the curve : 0.893881\n```", "```\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE \n\nx = df[['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']]\n\ny = df['Class']\n\n# Increase the fraud samples from 102 to 500\n\nsm = SMOTE(random_state=42,ratio={1:500})\nX_res, y_res = sm.fit_sample(x, y)\nprint('Resampled dataset shape {}'.format(Counter(y_res)))\n\nResampled dataset shape Counter({0: 56772, 1: 500})\n\n# Split the resampled data into train & test data with 70:30 mix\n\nxtrain, xtest, ytrain, ytest = train_test_split(X_res, y_res, test_size=0.30, random_state=0)\n\n# Random Forest Classifier on resampled data\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nrfmodel = RandomForestClassifier()\nrfmodel.fit(xtrain,ytrain)\n\nypredrf = rfmodel.predict(xtest)\n\nprint('classification report')\nprint(metrics.classification_report(ytest, ypredrf))\nprint('Accuracy : %f' % (metrics.accuracy_score(ytest, ypredrf)))\nprint('Area under the curve : %f' % (metrics.roc_auc_score(ytest, ypredrf)))\n\nclassification report\n             precision    recall  f1-score   support\n\n          0       1.00      1.00      1.00     17023\n          1       0.97      0.91      0.94       159\n\navg / total       1.00      1.00      1.00     17182\n\nAccuracy : 0.998952\nArea under the curve : 0.955857\n```"]