<html><head></head><body>
		<div>
			<div id="_idContainer027" class="Content">
			</div>
		</div>
		<div id="_idContainer028" class="Content">
			<h1 id="_idParaDest-43">2. <a id="_idTextAnchor043"/>Real-World Deep Learning: Predicting the Price of Bitcoin</h1>
		</div>
		<div id="_idContainer057" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter will help you to prepare data for a deep learning model, choose the right model architecture, use Keras—the default API of TensorFlow 2.0, and make predictions with the trained model. By the end of this chapter, you will have prepared a model to make predictions which we will explore in the upcoming chapters.</p>
			<h1 id="_idParaDest-44">Introduction<a id="_idTextAnchor044"/></h1>
			<p>Building on fundamental concepts from <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Neural Networks and Deep Learning</em>, let's now move on to a real-world scenario and identify whether we can build a deep learning model that predicts Bitcoin prices. </p>
			<p>We will learn the principles of preparing data for a deep learning model, and how to choose the right model architecture. We will use Keras—the default API of TensorFlow 2.0 and make predictions with the trained model. We will conclude this chapter by putting all these components together and building a bare bones, yet complete, first version of a deep learning application.</p>
			<p>Deep learning is a field that is undergoing intense research activity. Among other things, researchers are devoted to inventing new neural network architectures that can either tackle new problems or increase the performance of previously implemented architectures.</p>
			<p>In this chapter, we will study both old and new architectures. Older architectures have been used to solve a large array of problems and are generally considered the right choice when starting a new project. Newer architectures have shown great success in specific problems but are harder to generalize. The latter are interesting as references of what to explore next but are hardly a good choice when starting a project. </p>
			<p>The following topic discusses the details of these architectures and how to determine the best one for a particular problem statement.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor045"/>Choosing the Right Model Architecture</h1>
			<p>Considering the available architecture possibilities, there are two popular architectures that are often used as starting points for several applications: <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>) and <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>). These are foundational networks and should be considered starting points for most projects.</p>
			<p>We also include descriptions of another three networks, due to their relevance in the field: <strong class="bold">Long Short-Term Memory</strong> (<strong class="bold">LSTM</strong>) networks (an RNN variant); <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>); and <strong class="bold">Deep Reinforcement Learning</strong> (<strong class="bold">DRL</strong>). These latter architectures have shown great success in solving contemporary problems, however, they are slightly  difficult to use. The next section will cover the use of different types of architecture in different problems.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>Convolutional Neural Networks (CNNs)</h2>
			<p>CNNs have gained notoriety for working with problems that have a grid-like structure. They were originally created to classify images, but have been used in several other areas, ranging from speech recognition to self-driving vehicles.</p>
			<p>A CNN's essential insight is to use closely related data as an element of the training process, instead of only individual data inputs. This idea is particularly effective in the context of images, where a pixel located at the center is related to the ones located to its right and left. The name <strong class="bold">convolution</strong> is given to the mathematical representation of the following process:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B15911_02_01.jpg" alt="Figure 2.1: Illustration of the convolution process&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1: Illustration of the convolution process</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Image source: Volodymyr Mnih, <em class="italic">et al.</em></p>
			<p class="callout">You can find this image at: <a href="https://packt.live/3fivWLB">https://packt.live/3fivWLB</a></p>
			<p class="callout">For more information about deep reinforcement learning, refer to <em class="italic">Human-level Control through Deep Reinforcement Learning</em>. <em class="italic">February 2015</em>, <em class="italic">Nature</em>, available at <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a>.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Recurrent Neural Networks (RNNs)</h2>
			<p>A CNN works with a set of inputs that keeps altering the weights and biases of the network's respective layers and nodes. A known limitation of this approach is that its architecture ignores the sequence of these inputs when determining the changes to the network's weights and biases.</p>
			<p>RNNs were created precisely to address that problem. They are designed to work with sequential data. This means that at every epoch, layers can be influenced by the output of previous layers. The memory of previous observations in each sequence plays an important role in the evaluation of posterior observations.</p>
			<p>RNNs have had successful applications in speech recognition due to the sequential nature of that problem. Also, they are used for translation problems. Google Translate's current algorithm—Transformer, uses an RNN to translate text from one language to another. In late 2018, Google introduced another algorithm based on the Transformer algorithm called <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), which is currently state of the art in <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>). </p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information RNN applications, refer to the following:</p>
			<p class="callout"><em class="italic">Transformer: A Novel Neural Network Architecture for Language Understanding</em>, <em class="italic">Jakob Uszkoreit</em>, <em class="italic">Google Research Blog</em>, <em class="italic">August 2017</em>, available at <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html</a>.</p>
			<p class="callout"><em class="italic">BERT: Open Sourcing BERT: State-of-the-Art Pre-Training for Natural Language Processing</em>, available at <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html</a>.</p>
			<p>The following diagram illustrates how words in English are linked to words in French, based on where they appear in a sentence. RNNs are very popular in language translation problems:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B15911_02_02.jpg" alt="Figure 2.2: Illustration from distill.pub linking words in English and French&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2: Illustration from distill.pub linking words in English and French</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Image source: <a href="https://distill.pub/2016/augmented-rnns/">https://distill.pub/2016/augmented-rnns/</a></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>Long Short-Term Memory (LSTM) Networks</h2>
			<p><strong class="bold">LSTM</strong> networks are RNN variants created to address the vanishing gradient problem. This problem is caused by memory components that are too distant from the current step that receive lower weights due to their distance. LSTMs are a variant of RNNs that contain a memory component called a <strong class="bold">forget gate</strong>. This component can be used to evaluate how both recent and old elements affect the weights and biases, depending on where the observation is placed in a sequence.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The LSTM architecture was first introduced by Sepp Hochreiter and Jürgen Schmidhuber in 1997. Current implementations have had several modifications. For a detailed mathematical explanation of how each component of an LSTM works, refer to the article <em class="italic">Understanding LSTM Networks</em>, by Christopher Olah, August 2015, available at <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">https://colah.github.io/posts/2015-08-Understanding-LSTMs/</a>.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Generative Adversarial Networks</h2>
			<p><strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>) were invented in 2014 by Ian Goodfellow and his colleagues at the University of Montreal. GANs work based on the approach that, instead of having one neural network that optimizes weights and biases with the objective to minimize its errors, there should be two neural networks that compete against each other for that purpose. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information on GANs, refer to <em class="italic">Generative Adversarial Networks</em>, <em class="italic">Ian Goodfellow, et al.</em>, <em class="italic">arXiv</em>. <em class="italic">June 10, 2014</em>, available at 	<a href="https://arxiv.org/pdf/1406.2661.pdf">https://arxiv.org/pdf/1406.2661.pdf</a>.</p>
			<p>GANs generate new data (<em class="italic">fake</em> data) and a network that evaluates the likelihood of the data generated by the first network being <em class="italic">real</em> or <em class="italic">fake</em>. They compete because both learn: one learns how to better generate <em class="italic">fake</em> data, and the other learns how to distinguish whether the data presented is real. They iterate on every epoch until convergence. That is the point when the network that evaluates generated data cannot distinguish between <em class="italic">fake</em> and <em class="italic">real</em> data any further. </p>
			<p>GANs have been successfully used in fields where data has a clear topological structure. Originally, GANs were used to create synthetic images of objects, people's faces, and animals that were similar to real images of those things. You will see in the following image, used in the <strong class="bold">StarGAN</strong> project, that the expressions on the face change:</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B15911_02_03.jpg" alt="Figure 2.3: Changes in people's faces based on emotion, using GAN algorithms&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3: Changes in people's faces based on emotion, using GAN algorithms</p>
			<p>This domain of image creation is where GANs are used the most frequently, but applications in other domains occasionally appear in research papers.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Image source: StarGAN project, available at <a href="https://github.com/yunjey/StarGAN">https://github.com/yunjey/StarGAN</a>.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>Deep Reinforcement Learning (DRL)</h2>
			<p>The original DRL architecture was championed by DeepMind, a Google-owned artificial intelligence research organization based in the UK. The key idea of DRL networks is that they are unsupervised in nature and that they learn from trial and error, only optimizing for a reward function. </p>
			<p>That is, unlike other networks (which use a supervised approach to optimize incorrect predictions as compared to what are known to be correct ones), DRL networks do not know of a correct way of approaching a problem. They are simply given the rules of a system and are then rewarded every time they perform a function correctly. This process, which takes a very large number of iterations, eventually trains networks to excel in several tasks.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information about DRL, refer to <em class="italic">Human-Level Control through Deep Reinforcement Learning</em>, <em class="italic">Volodymyr Mnih et al.</em>, <em class="italic">February 2015</em>, <em class="italic">Nature</em>, available at: <a href="https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf">https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf</a>.</p>
			<p>DRL models gained popularity after DeepMind created AlphaGo—a system that plays the game Go better than professional players. DeepMind also created DRL networks that learn how to play video games at a superhuman level, entirely on their own.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information about DQN, look up the DQN that was created by DeepMind to beat Atari games. The algorithm uses a DRL solution to continuously increase its reward.</p>
			<p class="callout">Image source: <a href="https://keon.io/deep-q-learning/">https://keon.io/deep-q-learning/</a>.</p>
			<p>Here's a summary of neural network architectures and their applications:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B15911_02_04.jpg" alt="Figure 2.4: Different neural network architectures, data structures, and their successful applications&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4: Different neural network architectures, data structures, and their successful applications</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor051"/>Data Normalization</h2>
			<p>Before building a deep learning model, data normalization is an important step. Data normalization is a common practice in machine learning systems. For neural networks, researchers have proposed that normalization is an essential technique for training RNNs (and LSTMs), mainly because it decreases the network's training time and increases the network's overall performance.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information, refer to <em class="italic">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</em>, <em class="italic">Sergey Ioffe et al.</em>, <em class="italic">arXiv</em>, March 2015, available at <a href="https://arxiv.org/abs/1502.03167">https://arxiv.org/abs/1502.03167</a>.</p>
			<p>Which normalization technique works best depends on the data and the problem at hand. A few commonly used techniques are listed here:</p>
			<h3 id="_idParaDest-52"><a id="_idTextAnchor052"/>Z-Score</h3>
			<p>When data is normally distributed (that is, Gaussian), you can compute the distance between each observation as a standard deviation from its mean. This normalization is useful when identifying how distant the data points are from more likely occurrences in the distribution. The Z-score is defined by the following formula:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B15911_02_05.jpg" alt="Figure 2.5: Formula for Z-Score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5: Formula for Z-Score</p>
			<p>Here, <em class="italic">x</em><span class="subscript">i</span> is the <em class="italic">i</em><span class="superscript">th</span> observation, <img src="image/B15911_02_Formula_01.png" alt="1"/> is the mean, and <img src="image/B15911_02_Formula_02.png" alt="2"/> is the standard deviation of the series.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information, refer to the standard score article (<em class="italic">Z-Score: Definition, Formula, and Calculation</em>), available at <a href="https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/">https://www.statisticshowto.datasciencecentral.com/probability-and-statistics/z-score/</a>. </p>
			<h3 id="_idParaDest-53"><a id="_idTextAnchor053"/>Point-Relative Normalization</h3>
			<p>This normalization computes the difference in a given observation in relation to the first observation of the series. This kind of normalization is useful for identifying trends in relation to a starting point. The point-relative normalization is defined by:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B15911_02_06.jpg" alt="Figure 2.6: Formula for point-relative normalization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6: Formula for point-relative normalization</p>
			<p>Here, <em class="italic">o</em><span class="subscript">i</span> is the <em class="italic">i</em><span class="superscript">th</span> observation, and <em class="italic">o</em><span class="subscript">o</span> is the first observation of the series.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">For more information on making predictions, watch <em class="italic">How to Predict Stock Prices Easily – Intro to Deep Learning #7</em>, <em class="italic">Siraj Raval</em>, available on YouTube at <a href="https://www.youtube.com/watch?v=ftMq5ps503w">https://www.youtube.com/watch?v=ftMq5ps503w</a>.</p>
			<h3 id="_idParaDest-54"><a id="_idTextAnchor054"/>Maximum and Minimum Normalization</h3>
			<p>This type of normalization computes the distance between a given observation and the maximum and minimum values of the series. This is useful when working with series in which the maximum and minimum values are not outliers and are important for future predictions. This normalization technique can be applied with the following formula:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B15911_02_07.jpg" alt="Figure 2.7: Formula for calculating normalization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7: Formula for calculating normalization</p>
			<p>Here, <em class="italic">O</em><span class="subscript">i</span> is the <em class="italic">i</em><span class="superscript">th</span> observation, <em class="italic">O </em>represents a vector with all <em class="italic">O</em> values, and the functions <em class="italic">min (O)</em> and <em class="italic">max (O)</em> represent the minimum and maximum values of the series, respectively.</p>
			<p>During <em class="italic">Exercise</em><em class="italic"> 2.01</em>, <em class="italic">Exploring the Bitcoin Dataset and Preparing Data for a Model</em>, we will prepare available Bitcoin data to be used in our LSTM model. That includes selecting variables of interest, selecting a relevant period, and applying the preceding point-relative normalization technique.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Structuring Your Problem</h1>
			<p>Compared to researchers, practitioners spend much less time determining which architecture to choose when starting a new deep learning project. Acquiring data that represents a given problem correctly is the most important factor to consider when developing these systems, followed by an understanding of the dataset's inherent biases and limitations. When starting to develop a deep learning system, consider the following questions for reflection:</p>
			<ul>
				<li>Do I have the right data? This is the hardest challenge when training a deep learning model. First, define your problem with mathematical rules. Use precise definitions and organize the problem into either categories (classification problems) or a continuous scale (regression problems). Now, how can you collect data pertaining to those metrics?</li>
				<li>Do I have enough data? Typically, deep learning algorithms have shown to perform much better on large datasets than on smaller ones. Knowing how much data is necessary to train a high-performance algorithm depends on the kind of problem you are trying to address, but aim to collect as much data as you can.</li>
				<li>Can I use a pre-trained model? If you are working on a problem that is a subset of a more general application, but within the same domain. Consider using a pre-trained model. Pre-trained models can give you a head start on tackling the specific patterns of your problem, instead of the more general characteristics of the domain at large. A good place to start is the official TensorFlow repository (<a href="https://github.com/tensorflow/models">https://github.com/tensorflow/models</a>).</li>
			</ul>
			<p>When you structure your problem with such questions, you will have a sequential approach to any new deep learning project. The following is a representative flow chart of these questions and tasks:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B15911_02_08.jpg" alt="Figure 2.8: Decision tree of key reflection questions to be asked at the beginning of a deep learning project&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8: Decision tree of key reflection questions to be asked at the beginning of a deep learning project</p>
			<p>In certain circumstances, the data may simply not be available. Depending on the case, it may be possible to use a series of techniques to effectively create more data from your input data. This process is known as <strong class="bold">data augmentation</strong> and can be applied successfully when working with image recognition problems.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A good reference is the article <em class="italic">Classifying plankton with deep neural networks</em>, available at <a href="https://benanne.github.io/2015/03/17/plankton.html">https://benanne.github.io/2015/03/17/plankton.html</a>. The authors show a series of techniques for augmenting a small set of image data in order to increase the number of training samples the model has.</p>
			<p>Once the problem is well-structured, you will be able to start preparing the model.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor056"/>Jupyter Notebook</h2>
			<p>We will be using Jupyter Notebook to code in this section. Jupyter Notebooks provide Python sessions via a web browser that allows you to work with data interactively. They are a popular tool for exploring datasets. They will be used in exercises throughout this book.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor057"/>Exercise 2.01: Exploring the Bitcoin Dataset and Preparing Data for a Model</h2>
			<p>In this exercise, we will prepare the data and then pass it to the model. The prepared data will then be useful in making predictions as we move ahead in the chapter. Before preparing the data, we will do some visual analysis on it, such as looking at when the value of Bitcoin was at its highest and when the decline started.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We will be using a public dataset originally retrieved from the Yahoo Finance website (<a href="https://finance.yahoo.com/quote/BTC-USD/history/">https://finance.yahoo.com/quote/BTC-USD/history/</a>). The dataset has been slightly modified, as it has been provided alongside this chapter, and will be used throughout the rest of this book.</p>
			<p class="callout">The dataset can be downloaded from: <a href="https://packt.live/2Zgmm6r">https://packt.live/2Zgmm6r</a>.</p>
			<p>The following are the steps to complete this exercise:</p>
			<ol>
				<li>Using your Terminal, navigate to the <strong class="source-inline">Chapter02/Exercise2.01</strong> directory. Activate the environment created in the previous chapter and execute the following command to start a Jupyter Notebook instance:<p class="source-code">$ jupyter notebook</p><p>This should automatically open the Jupyter lab server in your browser. From there you can start a Jupyter Notebook.</p><p>You should see the following output or similar:</p><div id="_idContainer039" class="IMG---Figure"><img src="image/B15911_02_09.jpg" alt="Figure 2.9: Terminal image after starting a Jupyter lab instance&#13;&#10;"/></div><p class="figure-caption">Figure 2.9: Terminal image after starting a Jupyter lab instance</p></li>
				<li>Select the <strong class="source-inline">Exercise2.01_Exploring_Bitcoin_Dataset.ipynb</strong> file. This is a Jupyter Notebook file that will open in a new browser tab. The application will automatically start a new Python interactive session for you:<div id="_idContainer040" class="IMG---Figure"><img src="image/B15911_02_10.jpg" alt="Figure 2.10: Landing page of your Jupyter Notebook instance&#13;&#10;"/></div><p class="figure-caption">Figure 2.10: Landing page of your Jupyter Notebook instance</p></li>
				<li>Click the Jupyter Notebook file:<div id="_idContainer041" class="IMG---Figure"><img src="image/B15911_02_11.jpg" alt="Figure 2.11: Image of Jupyter Notebook&#13;&#10;"/></div><p class="figure-caption">Figure 2.11: Image of Jupyter Notebook</p></li>
				<li>Opening our Jupyter Notebook, consider the Bitcoin data made available with this chapter. The dataset <strong class="source-inline">data/bitcoin_historical_prices.csv</strong> (<a href="https://packt.live/2Zgmm6r">https://packt.live/2Zgmm6r</a>) contains the details of Bitcoin prices since early 2013. It contains eight variables, two of which (<strong class="source-inline">date</strong> and <strong class="source-inline">week</strong>) describe a time period of the data. These can be used as indices—and six others (<strong class="source-inline">open</strong>, <strong class="source-inline">high</strong>, <strong class="source-inline">low</strong>, <strong class="source-inline">close</strong>, <strong class="source-inline">volume</strong>, and <strong class="source-inline">market_capitalization</strong>) can be used to understand changes in the price and value of Bitcoin over time:<div id="_idContainer042" class="IMG---Figure"><img src="image/B15911_02_12.jpg" alt="Figure 2.12: Available variables (that is, columns) in the Bitcoin historical prices dataset&#13;&#10;"/></div><p class="figure-caption">Figure 2.12: Available variables (that is, columns) in the Bitcoin historical prices dataset</p></li>
				<li>Using the open Jupyter Notebook instance, consider the time series of two of those variables: <strong class="source-inline">close</strong> and <strong class="source-inline">volume</strong>. Start with those time series to explore price fluctuation patterns, that is, how the price of Bitcoin varied at different times in the historical data.</li>
				<li>Navigate to the open instance of the Jupyter Notebook, <strong class="source-inline">Exercise2.01_Exploring_Bitcoin_Dataset.ipynb</strong>. Now, execute all cells under the <strong class="source-inline">Introduction</strong> header. This will import the required libraries and import the dataset into memory:<div id="_idContainer043" class="IMG---Figure"><img src="image/B15911_02_13.jpg" alt="Figure 2.13: Output from the first cells of the notebook time-series plot &#13;&#10;of the closing price for Bitcoin from the close variable"/></div><p class="figure-caption">Figure 2.13: Output from the first cells of the notebook time-series plot of the closing price for Bitcoin from the close variable</p></li>
				<li>After the dataset has been imported into memory, move to the <strong class="source-inline">Exploration</strong> section. You will find a snippet of code that generates a time series plot for the <strong class="source-inline">close</strong> variable:<p class="source-code">bitcoin.set_index('date')['close'].plot(linewidth=2, \</p><p class="source-code">                                        figsize=(14, 4),\</p><p class="source-code">                                        color='#d35400')</p><p class="source-code">#plt.plot(bitcoin['date'], bitcoin['close'])</p><p>The output looks like: </p><div id="_idContainer044" class="IMG---Figure"><img src="image/B15911_02_14.jpg" alt="Figure 2.14: Time series plot of the closing price for Bitcoin from the close variable&#13;&#10;"/></div><p class="figure-caption">Figure 2.14: Time series plot of the closing price for Bitcoin from the close variable</p></li>
				<li>Reproduce this plot but using the <strong class="source-inline">volume</strong> variable in a new cell below this one. You will have most certainly noticed that price variables surge in 2017 and then the downfall starts:<p class="source-code">bitcoin.set_index('date')['volume'].plot(linewidth=2, \</p><p class="source-code">                                         figsize=(14, 4), \</p><p class="source-code">                                         color='#d35400')</p><div id="_idContainer045" class="IMG---Figure"><img src="image/B15911_02_15.jpg" alt="Figure 2.15: The total daily volume of Bitcoin coins &#13;&#10;"/></div><p class="figure-caption">Figure 2.15: The total daily volume of Bitcoin coins </p><p><em class="italic">Figure 2.15</em> shows that since 2017, Bitcoin transactions have significantly increased in the market. The total daily volume varies much more than daily closing prices.</p></li>
				<li>Execute the remaining cells in the Exploration section to explore the range from 2017 to 2018.<p>Fluctuations in Bitcoin prices have been increasingly commonplace in recent years. While those periods could be used by a neural network to understand certain patterns, we will be excluding older observations, given that we are interested in predicting future prices for not-too-distant periods. Filter the data after 2016 only. Navigate to the <strong class="source-inline">Preparing Dataset for a Model</strong> section. Use the pandas API to filter the data. Pandas provides an intuitive API for performing this operation.</p></li>
				<li>Extract recent data and save it into a variable:<p class="source-code">bitcoin_recent = bitcoin[bitcoin['date'] &gt;= '2016-01-04']</p><p>The <strong class="source-inline">bitcoin_recent</strong> variable now has a copy of our original Bitcoin dataset, but filtered to the observations that are newer or equal to January 4, 2016.</p><p>Normalize the data using the point-relative normalization technique described in the <em class="italic">Data Normalization</em> section in the Jupyter Notebook. You will only nor<a id="_idTextAnchor058"/>malize two variables—<strong class="source-inline">close</strong> and <strong class="source-inline">volume</strong>—because those are the variables that we are working to predict. </p></li>
				<li>Run the next cell in the notebook to ensure that we only keep the close and volume variables.<p>In the same directory containing this chapter, we have placed a script called <strong class="source-inline">normalizations.py</strong>. That script contains the three normalization techniques described in this chapter. We import that script into our Jupyter Notebook and apply the functions to our series.</p></li>
				<li>Navigate to the <strong class="source-inline">Preparing Dataset for a Model</strong> section. Now, use the <strong class="source-inline">iso_week</strong> variable to group daily observations from a given week using the pandas <strong class="source-inline">groupby()</strong> method. We can now apply the normalization function, <strong class="source-inline">normalizations.point_relative_normalization()</strong>, directly to the series within that week. We can store the normalization output as a new variable in the same pandas DataFrame using the following code:<p class="source-code">bitcoin_recent['close_point_relative_normalization'] = \</p><p class="source-code">bitcoin_recent.groupby('iso_week')['close']\</p><p class="source-code">.apply(lambda x: normalizations.point_relative_normalization(x))</p></li>
				<li>The <strong class="source-inline">close_point_relative_normalization</strong> variable now contains the normalized data for the <strong class="source-inline">close</strong> variable: <p class="source-code">bitcoin_recent.set_index('date')\</p><p class="source-code">['close_point_relative_normalization'].plot(linewidth=2, \</p><p class="source-code">                                            figsize=(14,4), \</p><p class="source-code">                                            color='#d35400')</p><p>This will result in the following output:</p><div id="_idContainer046" class="IMG---Figure"><img src="image/B15911_02_16.jpg" alt="Figure 2.16: Image of the Jupyter Notebook focusing on the section where the normalization function is applied&#13;&#10;"/></div><p class="figure-caption">Figure 2.16: Image of the Jupyter Notebook focusing on the section where the normalization function is applied</p></li>
				<li>Do the same with the <strong class="source-inline">volume</strong> variable (<strong class="source-inline">volume_point_relative_normalization</strong>). The normalized <strong class="source-inline">close</strong> variable contains an interesting variance pattern every week. We will be using that variable to train our LSTM model:<p class="source-code">bitcoin_recent.set_index('date')\</p><p class="source-code">                        ['volume_point_relative_normalization'].\</p><p class="source-code">                        plot(linewidth=2, \</p><p class="source-code">                        figsize=(14,4), \</p><p class="source-code">                        color='#d35400')</p><p>Your output should be as follows.</p><div id="_idContainer047" class="IMG---Figure"><img src="image/B15911_02_17.jpg" alt="Figure 2.17: Plot that displays the series from the normalized variable&#13;&#10;"/></div><p class="figure-caption">Figure 2.17: Plot that displays the series from the normalized variable</p></li>
				<li>In order to evaluate how well the model performs, you need to test its accuracy versus some other data. Do this by creating two datasets: a training set and a test set. You will use 90 percent of the dataset to train the LSTM model and 10 percent to evaluate its performance. Given that the data is continuous and in the form of a time series, use the last 10 percent of available weeks as a test set and the first 90 percent as a training set:<p class="source-code">boundary = int(0.9 * bitcoin_recent['iso_week'].nunique())</p><p class="source-code">train_set_weeks = bitcoin_recent['iso_week'].unique()[0:boundary]</p><p class="source-code">test_set_weeks = bitcoin_recent[~bitcoin_recent['iso_week']\</p><p class="source-code">                 .isin(train_set_weeks)]['iso_week'].unique()</p><p class="source-code">test_set_weeks</p><p class="source-code">train_set_weeks</p><p>This will display the following output:</p><div id="_idContainer048" class="IMG---Figure"><img src="image/B15911_02_18.jpg" alt="Figure 2.18: Output of the test set weeks&#13;&#10;"/></div><p class="figure-caption">Figure 2.18: Output of the test set weeks</p><div id="_idContainer049" class="IMG---Figure"><img src="image/B15911_02_19.jpg" alt="Figure 2.19: Using weeks to create a training set&#13;&#10;"/></div><p class="figure-caption">Figure 2.19: Using weeks to create a training set</p></li>
				<li>Create the separate datasets for each operation:<p class="source-code">train_dataset = bitcoin_recent[bitcoin_recent['iso_week']\</p><p class="source-code">                                             .isin(train_set_weeks)]</p><p class="source-code">test_dataset = bitcoin_recent[bitcoin_recent['iso_week'].\</p><p class="source-code">                                            isin(test_set_weeks)]</p></li>
				<li>Finally, navigate to the <strong class="source-inline">Storing Output</strong> section and save the filtered variable to disk, as follows:<p class="source-code">test_dataset.to_csv('data/test_dataset.csv', index=False)</p><p class="source-code">train_dataset.to_csv('data/train_dataset.csv', index=False)</p><p class="source-code">bitcoin_recent.to_csv('data/bitcoin_recent.csv', index=False)</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3ehbgCi">https://packt.live/3ehbgCi</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2ZdGq9s">https://packt.live/2ZdGq9s</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we explored the Bitcoin dataset and prepared it for a deep learning model.</p>
			<p>We learned that in 2017, the price of Bitcoin skyrocketed. This phenomenon took a long time to take place and may have been influenced by a number of external factors that this data alone doesn't explain (for instance, the emergence of other cryptocurrencies). After the great surge of 2017, we saw a great fall in the value of Bitcoin in 2018.</p>
			<p>We also used the point-relative normalization technique to process the Bitcoin dataset in weekly chunks. We do this to train an LSTM network to learn the weekly patterns of Bitcoin price changes so that it can predict a full week into the future.</p>
			<p>However, Bitcoin statistics show significant fluctuations on a weekly basis. Can we predict the price of Bitcoin in the future? What will the price be seven days from now? We will build a deep learning model to explore these questions in our next section using Keras.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor059"/>Using Keras as a TensorFlow Interface</h1>
			<p>We are using Keras because it simplifies the TensorFlow interface into general abstractions and, in TensorFlow 2.0, this is the default API in this version. In the backend, the computations are still performed in TensorFlow, but we spend less time worrying about individual components, such as variables and operations, and spend more time building the network as a computational unit. Keras makes it easy to experiment with different architectures and hyperparameters, moving more quickly toward a performant solution.</p>
			<p>As of TensorFlow 2.0.0, Keras is now officially distributed with TensorFlow as <strong class="source-inline">tf.keras</strong>. This suggests that Keras is now tightly integrated with TensorFlow and will likely continue to be developed as an open source tool for a long period of time. Components are an integral part when building models. Let's deep dive into this concept now.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor060"/>Model Components</h2>
			<p>As we saw in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Neural Networks and Deep Learning</em>, LSTM networks also have input, hidden, and output layers. Each hidden layer has an activation function that evaluates that layer's associated weights and biases. As expected, the network moves data sequentially from one layer to another and evaluates the results by the output at every iteration (that is, an epoch).</p>
			<p>Keras provides intuitive classes that represent each one of the components listed in the following table:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B15911_02_20.jpg" alt="Figure 2.20: Description of key components from the Keras API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.20: Description of key components from the Keras API</p>
			<p>We will be using these components to build a deep learning model.</p>
			<p>Keras' <strong class="source-inline">keras.models.Sequential()</strong> component represents a whole sequential neural network. This Python class can be instantiated on its own and have other components added to it subsequently.</p>
			<p>We are interested in building an LSTM network because those networks perform well with sequential data—and a time series is a kind of sequential data. Using Keras, the complete LSTM network would be implemented as follows:</p>
			<p class="source-code">from tensorflow.keras.models import Sequential</p>
			<p class="source-code">from tensorflow.keras.layers import LSTM</p>
			<p class="source-code">from tensorflow.keras.layers import Dense, Activation</p>
			<p class="source-code">model = Sequential()</p>
			<p class="source-code">model.add(LSTM(units=number_of_periods, \</p>
			<p class="source-code">               input_shape=(period_length, number_of_periods) \</p>
			<p class="source-code">               return_sequences=False), stateful=True)</p>
			<p class="source-code">model.add(Dense(units=period_length)) \</p>
			<p class="source-code">          model.add(Activation("linear"))</p>
			<p class="source-code">model.compile(loss="mse", optimizer="rmsprop")</p>
			<p>This implementation will be further optimized in <em class="italic">Chapter 3</em>, <em class="italic">Real-World Deep Learning with TensorFlow and Keras: Evaluating the Bitcoin Model</em>.</p>
			<p>Keras abstraction allows you to focus on the key elements that make a deep learning system more performant: determining the right sequence of components, how many layers and nodes to include, and which activation function to use. All of these choices are determined by either the order in which components are added to the instantiated <strong class="source-inline">keras.models</strong><strong class="source-inline">.Sequential()</strong> class or by parameters passed to each component instantiation (that is, <strong class="source-inline">Activation("linear")</strong>). The final <strong class="source-inline">model.compile()</strong> step builds the neural network using TensorFlow components.</p>
			<p>After the network is built, we train our network using the <strong class="source-inline">model.fit()</strong> method. This will yield a trained model that can be used to make predictions:</p>
			<p class="source-code">model.fit(X_train, Y_train,</p>
			<p class="source-code">          batch_size=32, epochs=epochs)</p>
			<p>The <strong class="source-inline">X_train</strong> and <strong class="source-inline">Y_train</strong> variables are, respectively, a set used for training and a smaller set used for evaluating the loss function (that is, testing how well the network predicts data). Finally, we can make predictions using the <strong class="source-inline">model.predict()</strong> method:</p>
			<p class="source-code">model.predict(x=X_train)</p>
			<p>The preceding steps cover the Keras paradigm for working with neural networks. Despite the fact that different architectures can be dealt with in very different ways, Keras simplifies the interface for working with different architectures by using three components – <strong class="source-inline">Network Architecture</strong>, <strong class="source-inline">Fit</strong>, and <strong class="source-inline">Predict</strong>:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B15911_02_21.jpg" alt="Figure 2.21: The Keras neural network paradigm&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.21: The Keras neural network paradigm</p>
			<p>The Keras neural network diagram comprises the following three steps:</p>
			<ul>
				<li>A neural network architecture</li>
				<li>Training a neural network (or <strong class="bold">Fit</strong>)</li>
				<li>Making predictions</li>
			</ul>
			<p>Keras allows much greater control within each of these steps. However, its focus is to make it as easy as possible for users to create neural networks in as little time as possible. That means that we can start with a simple model, and then add complexity to each one of the preceding steps to make that initial model perform better.</p>
			<p>We will take advantage of that paradigm during our upcoming exercise and chapters. In the next exercise, we will create the simplest LSTM network possible. Then, in <em class="italic">Chapter 3</em>, <em class="italic">Real-World Deep Learning: Evaluating the Bitcoin Model</em>, we will continuously evaluate and alter that network to make it more robust and performant.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor061"/>Exercise 2.02: Creating a TensorFlow Model Using Keras</h2>
			<p>In this notebook, we design and compile a deep learning model using Keras as an interface to TensorFlow. We will continue to modify this model in our next chapters and exercises by experimenting with different optimization techniques. However, the essential components of the model are designed entirely in this notebook:</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook and import the following libraries: <p class="source-code">import warnings</p><p class="source-code">warnings.filterwarnings("ignore", category=DeprecationWarning)</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow import keras</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import LSTM</p><p class="source-code">from tensorflow.keras.layers import Dense, Activation</p></li>
				<li>Our dataset contains daily observations and each observation influences a future observation. Also, we are interested in predicting a week—that is, 7 days—of Bitcoin prices in the future:<p class="source-code">period_length = 7</p><p class="source-code">number_of_periods = 208 - 21 - 1</p><p class="source-code">number_of_periods</p><p>We have calculated <strong class="source-inline">number_of_observations</strong> based on available weeks in our dataset. Given that we will be using last week to test the LSTM network on every epoch, we will use 208 – 21 – 1. You'll get: </p><p class="source-code">186</p></li>
				<li>Build the LSTM model using Keras. We have the batch size as one because we are passing the whole data in a single iteration. If data is big, then we can pass the data with multiple batches, That's why we used batch_input_shape:<p class="source-code">def build_model(period_length, number_of_periods, batch_size=1):</p><p class="source-code">    model = Sequential()</p><p class="source-code">    model.add(LSTM(units=period_length,\</p><p class="source-code">                   batch_input_shape=(batch_size, \</p><p class="source-code">                                      number_of_periods, \</p><p class="source-code">                                      period_length),\</p><p class="source-code">                   input_shape=(number_of_periods, \</p><p class="source-code">                                period_length),\</p><p class="source-code">                   return_sequences=False, stateful=False))</p><p class="source-code">    model.add(Dense(units=period_length))</p><p class="source-code">    model.add(Activation("linear"))</p><p class="source-code">    model.compile(loss="mse", optimizer="rmsprop")</p><p class="source-code">    return model</p><p>This should return a compiled Keras model that can be trained and stored in disk.</p></li>
				<li>Let's store the model on the output of the model to a disk:<p class="source-code">model = build_model(period_length=period_length, \</p><p class="source-code">                    number_of_periods=number_of_periods)</p><p class="source-code">model.save('bitcoin_lstm_v0.h5')</p><p>Note that the <strong class="source-inline">bitcoin_lstm_v0.h5</strong> model hasn't been trained yet. When saving a model without prior training, you effectively only save the architecture of the model. That same model can later be loaded by using Keras' <strong class="source-inline">load_model()</strong> function, as follows:</p><p class="source-code">model = keras.models.load_model('bitcoin_lstm_v0.h5')</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38KQI3Y">https://packt.live/38KQI3Y</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3fhEL89">https://packt.live/3fhEL89</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>This concludes the creation of our Keras model, which we can now use to make predictions.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You may encounter the following warning when loading the Keras library:</p>
			<p class="callout"><strong class="source-inline">Using TensorFlow backend</strong></p>
			<p class="callout">Keras can be configured to use another backend instead of TensorFlow (that is, Theano). In order to avoid this message, you can create a file called <strong class="source-inline">keras.json</strong> and configure its backend there. The correct configuration of that file depends on your system. Hence, it is recommended that you visit Keras' official documentation on the topic at <a href="https://keras.io/backend/">https://keras.io/backend/</a>.</p>
			<p>In this section, we have learned how to build a deep learning model using Keras—an interface for TensorFlow. We studied core components of Keras and used those components to build the first version of our Bitcoin price-predicting system based on an LSTM model.</p>
			<p>In our next section, we will discuss how to put all the components from this chapter together into a (nearly complete) deep learning system. That system will yield our very first predictions, serving as a starting point for future improvements.</p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor062"/>From Data Preparation to Modeling</h1>
			<p>This section focuses on the implementation aspects of a deep learning system. We will use the Bitcoin data from the <em class="italic">Choosing the Right Model Architecture</em> section, and the Keras knowledge from the preceding section, <em class="italic">Using Keras as a TensorFlow Interface</em>, to put both of these components together. This section concludes the chapter by building a system that reads data from a disk and feeds it into a model as a single piece of software.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor063"/>Training a Neural Network</h2>
			<p>Neural networks can take long periods of time to train. Many factors affect how long that process may take. Among them, three factors are commonly considered the most important:</p>
			<ul>
				<li>The network's architecture</li>
				<li>How many layers and neurons the network has</li>
				<li>How much data there is to be used in the training process</li>
			</ul>
			<p>Other factors may also greatly impact how long a network takes to train, but most of the optimization that a neural network can have when addressing a business problem comes from exploring those three.</p>
			<p>We will be using the normalized data from our previous section. Recall that we have stored the training data in a file called <strong class="source-inline">train_dataset.csv</strong>. </p>
			<p class="callout-heading">Note:</p>
			<p class="callout">You can download the training data by visiting this link: <a href="https://packt.live/2Zgmm6r">https://packt.live/2Zgmm6r</a>.</p>
			<p>We will load that dataset into memory using the <strong class="source-inline">pandas</strong> library for easy exploration:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">train = pd.read_csv('<strong class="bold">data/train_dataset.csv</strong>')</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Make sure you change the path (highlighted) based on where you have downloaded or saved the CSV file.</p>
			<p>You will see the output in a tabular form as follows:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B15911_02_22.jpg" alt="Figure 2.22: Table showing the first five rows of the training dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.22: Table showing the first five rows of the training dataset</p>
			<p>We will be using the series from the <strong class="source-inline">close_point_relative_normalization</strong> variable, which is a normalized series of the Bitcoin closing prices—from the <strong class="source-inline">close</strong> variable—since the beginning of 2016.</p>
			<p>The <strong class="source-inline">close_point_relative_normalization</strong> variable has been normalized on a weekly basis. Each observation from the week's period is made relative to the difference from the closing prices on the first day of the period. This normalization step is important and will help our network train faster:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B15911_02_23.jpg" alt="Figure 2.23: Plot that displays the series from the normalized variable.&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.23: Plot that displays the series from the normalized variable.</p>
			<p>This variable will be used to train our LSTM model.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor064"/>Reshaping Time Series Data</h2>
			<p>Neural networks typically work with vectors and tensors—both mathematical objects that organize data in a number of dimensions. Each neural network implemented in Keras will have either a vector or a tensor that is organized according to a specification as input. </p>
			<p>At first, understanding how to reshape the data into the format expected by a given layer can be confusing. To avoid confusion, it is advisable to start with a network with as few components as possible, and then add components gradually. Keras' official documentation (under the <strong class="source-inline">Layers</strong> section) is essential for learning about the requirements for each kind of layer.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The Keras official documentation is available at <a href="https://keras.io/layers/core/">https://keras.io/layers/core/</a>. That link takes you directly to the <strong class="source-inline">Layers</strong> section.</p>
			<p><strong class="bold">NumPy</strong> is a popular Python library used for performing numerical computations. It is used by the deep learning community to manipulate vectors and tensors and prepare them for deep learning systems.</p>
			<p>In particular, the <strong class="source-inline">numpy.reshape()</strong> method is very important when adapting data for deep learning models. That model allows for the manipulation of NumPy arrays, which are Python objects analogous to vectors and tensors.</p>
			<p>We'll now organize the prices from the <strong class="source-inline">close_point_relative_normalization</strong> variable using the weeks after 2016. We will create distinct groups containing 7 observations each (one for each day of the week) for a total of 208 complete weeks. We do that because we are interested in predicting the prices of a week's worth of trading.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We use the ISO standard to determine the beginning and the end of a week. Other kinds of organizations are entirely possible. This one is simple and intuitive to follow, but there is room for improvement.</p>
			<p>LSTM networks work with three-dimensional tensors. Each one of those dimensions represents an important property for the network. These dimensions are as follows:</p>
			<ul>
				<li><strong class="bold">Period length</strong>: The period length, that is, how many observations there are for a period</li>
				<li><strong class="bold">Number of periods</strong>: How many periods are available in the dataset</li>
				<li><strong class="bold">Number of features</strong>: The number of features available in the dataset</li>
			</ul>
			<p>Our data from the <strong class="source-inline">close_point_relative_normalization</strong> variable is currently a one-dimensional vector. We need to reshape it to match those three dimensions.</p>
			<p>We will be using a period of a week. So, our period length is 7 days (period length = 7). We have 208 complete weeks available in our data. We will be using the very last of those weeks to test our model against during its training period. That leaves us with 187 distinct weeks. Finally, we will be using a single feature in this network (number of features = 1); we will include more features in future versions.</p>
			<p>To reshape the data to match those dimensions, we will be using a combination of base Python properties and the <strong class="source-inline">reshape()</strong> method from the <strong class="source-inline">numpy</strong> library. First, we create the 186 distinct week groups with 7 days, each using pure Python:</p>
			<p class="source-code">group_size = 7</p>
			<p class="source-code">samples = list()</p>
			<p class="source-code">for i in range(0, len(data), group_size):</p>
			<p class="source-code">sample = list(data[i:i + group_size])</p>
			<p class="source-code">         if len(sample) == group_size:samples\</p>
			<p class="source-code">                           .append(np.array(sample)\</p>
			<p class="source-code">                           .reshape(group_size, 1).tolist())</p>
			<p class="source-code">data = np.array(samples)</p>
			<p>This piece of code creates distinct week groups. The resulting variable data is a variable that contains all the right dimensions. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Each Keras layer will expect its input to be organized in specific ways. However, Keras will reshape data accordingly, in most cases. Always refer to the Keras documentation on layers (<a href="https://keras.io/layers/core/">https://keras.io/layers/core/</a>) before adding a new layer.</p>
			<p>The Keras LSTM layer expects these dimensions to be organized in a specific order: the number of features, the number of observations, and the period length. Reshape the dataset to match that format:</p>
			<p class="source-code">X_train = data[:-1,:].reshape(1, 186, 7)</p>
			<p class="source-code">Y_validation = data[-1].reshape(1, 7)</p>
			<p>The preceding snippet also selects the very last week of our set as a validation set (via <strong class="source-inline">data[-1]</strong>). We will be attempting to predict the very last week in our dataset by using the preceding 76 weeks. The next step is to use those variables to fit our model:</p>
			<p class="source-code">model.fit(x=X_train, y=Y_validation, epochs=100)</p>
			<p>LSTMs are computationally expensive models. They may take up to 5 minutes to train with our dataset on a modern computer. Most of that time is spent at the beginning of the computation when the algorithm creates the full computation graph. The process gains speed after it starts training:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B15911_02_24.jpg" alt="Figure 2.24: Graph that shows the results of the loss function evaluated at each epoch&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.24: Graph that shows the results of the loss function evaluated at each epoch</p>
			<p class="callout-heading">Note</p>
			<p class="callout">This compares what the model predicted at each epoch, and then compares that with the real data using a technique called mean-squared error. This plot shows those results.</p>
			<p>At a glance, our network seems to perform very well; it starts with a very small error rate that continuously decreases. Now that we have lowered the error rate, let's move on to make some predictions.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor065"/>Making Predictions</h2>
			<p>After our network has been trained, we can proceed to make predictions. We will be making predictions for a future week beyond our time period.</p>
			<p>Once we have trained our model with the <strong class="source-inline">model.fit()</strong> method, making predictions is simple:</p>
			<p class="source-code">model.predict(x=X_train)</p>
			<p>We use the same data for making predictions as the data used for training (the <strong class="source-inline">X_train</strong> variable). If we have more data available, we can use that instead—given that we reshape it to the format the LSTM requires.</p>
			<h3 id="_idParaDest-65"><a id="_idTextAnchor066"/>Overfitting</h3>
			<p>When a neural network overfits a validation set, this means that it learns patterns present in the training set but is unable to generalize it to unseen data (for instance, the test set). In the next chapter, we will learn how to avoid overfitting and create a system for both evaluating our network and increasing its performance:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B15911_02_25.jpg" alt="Figure 2.25: Graph showing the weekly performance of Bitcoin&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.25: Graph showing the weekly performance of Bitcoin</p>
			<p>In the plot shown above, the horizontal axis represents the week number and the vertical axis represents the predicted performance of Bitcoin. Now that we have explored the data, prepared a model, and learned how to make predictions, let's put this knowledge into practice.</p>
			<h2 id="_idParaDest-66">Activity 2.01: Assembling a<a id="_idTextAnchor067"/> Deep Learning System</h2>
			<p>In this activity, we'll bring together all the essential pieces for building a basic deep learning system – the data, a model, and predictions:</p>
			<ol>
				<li value="1">Start a Jupyter Notebook.</li>
				<li>Load the training dataset into memory.</li>
				<li>Inspect the training set to see whether it is in the form period length, number of periods, or number of features. </li>
				<li>Convert the training set if it is not in the required format.</li>
				<li>Load the previously trained model.</li>
				<li>Train the model using your training dataset.</li>
				<li>Make a prediction on the training set.</li>
				<li>Denormalize the values and save the model.</li>
			</ol>
			<p>The final output will look as follows with the horizontal axis representing the number of days and the vertical axis represents the price of Bitcoin:</p>
			<p>6</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B15911_02_26.jpg" alt="Figure 2.26: Expected output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.26: Expected output</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found on page 136.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor068"/>Summary</h1>
			<p>In this chapter, we have assembled a complete deep learning system, from data to prediction. The model created in this activity requires a number of improvements before it can be considered useful. However, it serves as a great starting point from which we will continuously improve.</p>
			<p>The next chapter will explore techniques for measuring the performance of our model and will continue to make modifications until we reach a model that is both useful and robust.</p>
		</div>
		<div>
			<div id="_idContainer058" class="Content">
			</div>
		</div>
	</body></html>