- en: Emerging Neural Network Designs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll look at some emerging **Neural Network** (**NN**) designs.
    They haven't reached maturity yet, but hold potential for the future because they
    try to address fundamental limitations in existing DL algorithms. If one day any
    of these technologies prove successful and useful for practical applications,
    we might get one step closer to artificial general intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: One thing that we need to bear in mind is the nature of structured data. So
    far in this book, we've focused on processing either images or text—in other words,
    unstructured data. This is not a coincidence, because NNs excel in the seemingly
    complex task of finding structure in combinations of pixels or text sequences.
    On the other hand, ML algorithms, such as gradient boosted trees or random forests,
    seem to perform on a par with, or better than, NNs when it comes to structured
    data, such as social-network graphs or brain connections. In this chapter, we'll
    introduce graph NNs to deal with arbitrary structured graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Another NN limitation manifests itself with **Recurrent Networks** (**RNNs**).
    In theory, these are one of the most powerful NN models because they are Turing-complete,
    which means that an RNN can theoretically solve any computational problem. This
    is often not the case in practice. RNNs (even **Long Short-Term Memory **(**LSTM**))
    can struggle to carry information over extended periods of time. One possible
    solution is to extend the RNN with an external addressable memory. We'll look
    at how to do this in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The topics in this chapter are not detached from the rest of the topics in this
    book. In fact, we'll see that the new network architectures that we'll look at
    are based on many of the algorithms that we've already covered. These include
    convolutions, RNNs, and attention models, as well as others.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing graph NNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing memory-augmented NNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Graph NNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before learning about **graph NNs** (**GNNs**), let's look at why we need graph
    networks in the first place. We'll start by defining a graph, which is a set of
    objects (also known as **nodes** or **vertices**) where some pairs of objects
    have connections (or **edges**) between them.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll use several survey papers as resources, most notably
    *A* *Comprehensive Survey on Graph Neural Networks* ([https://arxiv.org/abs/1901.00596](https://arxiv.org/abs/1901.00596)),
    which contains some quotes and images.
  prefs: []
  type: TYPE_NORMAL
- en: 'A graph has the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: We'll represent the graph as ![](img/40da6b62-68d7-4d59-9247-d8e142246f89.png),
    where *V* is the set of nodes and *E* is the set of edges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The expression ![](img/88ec490a-336a-4f32-b501-138e471e5801.png) describes an
    edge between two nodes, ![](img/46829fd0-f93c-4710-909c-612208907d50.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An adjacency matrix, ![](img/2bd7df0f-0b56-4b25-aef4-9c8855ffcc90.png), where *n*
    is the number of graph nodes. This is written as ![](img/f3dd7e83-f3cd-4074-9873-d75fac7463d8.png) if
    an edge ![](img/96f0fb9d-25e2-4d01-bdcc-8bb2f14baa86.png) exists and ![](img/d6be8b1c-c944-47bc-9051-aab6d6c44840.png) if
    it doesn't.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphs can be **directed** when the edges have a direction and **undirected** when
    they don't. The adjacency matrix of an undirected graph is symmetric—that is ![](img/8a2dc410-555b-4395-a48d-6034f07430d0.png).
    The adjacency matrix of a directed graph is asymmetric—that is ![](img/0e4b2a68-8374-496e-b3ef-90c570166d30.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphs can be **cyclic** or **acyclic**. As the name suggests, a cyclic graph
    contains at least one cycle, which is a non-empty path of nodes where only the
    first and the last node are the same. Acyclic graphs don't contain cycles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both graph edges and nodes can have associated attributes, known as feature
    vectors. We'll denote the *d*-dimensional feature vector of node *v* with ![](img/0f8228cc-225b-44df-bd68-1449db642346.png).
    If a graph has *n* nodes, we can represent them as a matrix ![](img/1c5fd1ea-48cd-43c1-854d-e7b081d2ea18.png).
    Analogously, each edge attribute is a *c*-dimensional feature vector, expressed
    as ![](img/500c1356-80bc-4683-8bc8-afb9b4cd5109.png), where *v* and *u* are nodes.
    We can represent the set of edge attributes of a graph as a matrix ![](img/f22c4803-7135-40dc-ae0e-d6ab74e68ee5.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows a directed graph with five nodes and its corresponding
    adjacency matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4859621a-c3d2-4895-a357-11a006dfefe6.png)'
  prefs: []
  type: TYPE_IMG
- en: Directed graph with five nodes and its corresponding ![](img/e6a20be5-c2c0-46cf-82ef-5b766a62dc65.png) adjacency
    matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'A graph is a versatile data structure that lends itself well to the way data
    is organized in many real-world scenarios. The following is a nonexhaustive list
    of examples:'
  prefs: []
  type: TYPE_NORMAL
- en: We can use graphs to represent users in a social network (nodes) and their groups
    of friends (edges). In fact, this is what Facebook does with their social graph
    (*The Anatomy of the Facebook Social Graph, *[https://arxiv.org/abs/1111.4503](https://arxiv.org/abs/1111.4503)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can represent a molecule as a graph, where the nodes are atoms and the edges
    are the chemical bonds between them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can represent a street network (a classic example) as a graph, where the
    streets are edges and their intersections are nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In online commerce, we can represent both users and items as nodes and the relationships
    between them as edges.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let''s discuss the types of task we can solve with graphs. They fall
    broadly into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node-focused**: Classification and regression of individual nodes. For example,
    in the famous Zachary''s karate club problem ([https://en.wikipedia.org/wiki/Zachary%27s_karate_club](https://en.wikipedia.org/wiki/Zachary%27s_karate_club))
    we have a number of karate club members (nodes) and the friendships between them
    (edges). Initially, the club has a single instructor and all the members train
    as a group under that instructor. Later, the club splits into two groups with
    two separate instructors. Assuming that all but one club member opts to join one
    of the two groups, the goal is to determine which group will choose the last undecided
    member (classification), given its set of friendships with other members.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Edge-focused**: Classification and regression of individual edges of the
    graph. For example, we can predict how likely it is that two people in a social
    network know each other. In other words, the task is to determine whether an edge
    exists between two graph nodes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Graph-focused**: Classification and regression of full graphs. For example,
    given a molecule represented as a graph, we can predict whether the molecule is
    toxic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, let''s outline the main training frameworks of GNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Supervised**: All training data is labeled. We can apply supervised learning
    at node, edge, and graph level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised**: The goal here is to learn some form of graph embedding—for
    example, using autoencoders (we''ll discuss this scenario later in the chapter).
    We can apply unsupervised learning at node, edge, and graph level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semi-supervised**: This is usually applied at node level, where some graph
    nodes are labeled and some aren''t. Semi-supervised learning is especially suited
    for graphs because we can make the simple (but often true) assumption that neighboring
    nodes are likely to have the same labels. For example, say that we have two neighboring
    connected nodes. One of them contains an image of a car and the other contains
    an image of a truck. Let''s assume that the truck node is labeled as a vehicle
    while the car node is unlabeled. We can safely assume that the car node is also
    a vehicle because of its proximity to another vehicle node (the truck). There
    are multiple ways we can utilize this graph property in GNNs. We''ll outline two 
    of them (they are not mutually exclusive):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use this property implicitly by feeding the adjacency matrix of the graph as
    input to the network. The network will do its magic and hopefully infer that neighboring
    nodes are likely to have the same labels, thereby increasing the accuracy of the
    predictions thanks to the additional information. Most GNNs we'll discuss in this
    chapter use this mechanism.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label propagation**, where we can use labeled nodes as a seed for assigning
    labels to unlabeled ones based on their proximity to the labeled. We can do this
    in an iterative way as far as convergence by going through the following steps:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Start with the seed labels.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For all graph nodes (except the seed), assign a label based on the labels of
    their neighboring nodes. This step creates a new label configuration for the whole
    graph, where some of the nodes might need a new label, based on the modified neighbors'
    labels.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop label propagation if a convergence criterion is met; otherwise, repeat step
    2.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: We'll use this short introduction to graphs as a base for the next few sections,
    where we'll discuss various types of graph-focused NN model. The GNN arena is
    relatively new, and there is no outright perfect model resembling **convolutional
    networks** (**CNNs**) in computer vision. Instead, we have different models with
    various properties. Most of them fall into a few general categories, and there
    are attempts to create a framework that is generic enough to combine them all. This
    book doesn't aim to invent new models or model taxonomies but; instead, we'll
    introduce you to some existing ones.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent GNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We'll start this section by looking at **graph neural networks** (**GraphNNs**; see *The
    Graph Neural Network Model*, **[https://ieeexplore.ieee.org/document/4700287](https://ieeexplore.ieee.org/document/4700287)**).
    Although the authors of the paper abbreviated the model to GNN, we'll refer to
    it with the GraphNN acronym to avoid conflict with the GNN abbreviation, which
    is reserved for the general class of graph networks. This is one of the first
    GNN models to be proposed. It extends existing NNs to process graph-structured
    data. In the same way that we used the context of a word (that is, its surrounding
    words) to create embedding vectors ([Chapter 6](fe6a42c9-f18e-4c2b-9a82-99ec53e727ca.xhtml)*,
    Language Modeling*), we can use the neighboring graph nodes of a node to do the
    same. GraphNNs aim to create an *s*-dimensional vector state ![](img/e1c524a1-4bb1-4438-966b-00993d86d5af.png) of
    a node *v* based on the neighborhood of that node. In a similar way to language
    modeling, the vector state can serve as the input for other tasks, such as node
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The state of a node is updated by exchanging neighborhood information recurrently
    until a stable equilibrium is reached. Let''s denote the set of neighborhood nodes
    *v* with ![](img/3bd465a0-a7b2-44d0-9ca6-f316a4c15867.png) and a single node of
    that neighborhood with *u*. The hidden state of a node is recurrently updated
    using the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc73d220-66ee-4229-9e08-46567127e7c9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *f* is a parametric function (for example, a **feed-forward NN** (**FFNN**))
    and each state ![](img/e1889615-9da0-42d8-96b9-833d85ff49c1.png) is initialized
    randomly. The parametric function *f *takes as inputs the feature vector *![](img/8e00e0c7-e952-4459-9bda-debeb4a0f2bd.png)*
    of *v* , the feature vector ![](img/dda100a7-6d39-42b9-b482-ebfc6882f7e5.png) of
    its neighbor *u*, the feature vector ![](img/2123d1d9-f3ac-4968-b4fd-c8a1b4633c9e.png) of
    the edge connecting *u* and *v*, and the state vector ![](img/b40c18b0-cbc8-4505-9cf3-b9db3e3e3a66.png) of *u*
    at step *t-1*. In other words, *f* uses all known information about the neighborhood
    of *v*. The expression ![](img/161bd9cc-1ccb-4477-972e-908460c7624b.png) is a
    sum of the *f* applied over all neighboring nodes, which allows GraphNN to be
    independent of the number of neighbors and their ordering. The function *f* is
    the same (that is, has the same weights) for all steps of the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that we have an iterative (or recurrent) process, where the states at
    step *t* are based on the number of steps up to *t-1*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9e7c34c0-c84f-4979-b13c-02c9e6d344f3.png)'
  prefs: []
  type: TYPE_IMG
- en: The recurrent process of updating the feature vector states; the **Grec** recurrent
    layer is the same (that is, has the same weights) for all steps; Source: https://arxiv.org/abs/1901.00596
  prefs: []
  type: TYPE_NORMAL
- en: 'The process continues until a stable equilibrium is reached. For this to work,
    the function *f* must be a contraction mapping. Let''s clarify this: when applied
    to any two points (or values) A and B, a contraction mapping function *f* satisfies
    the condition ![](img/6a9b9e1e-bd66-4509-9081-1d2371d51fb5.png), where γ is a
    scalar value and ![](img/20f023c9-77a0-42d0-b88d-6a2f2359c604.png). In other words, the
    contraction mapping shrinks the distance between two points after mapping. This
    ensures that the system will converge (exponentially quickly) to the equilibrium
    state vector ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) for any initial
    value ![](img/3601a48a-7d0c-4dd3-98f6-112e93dd8b86.png). We can modify an NN to
    be a contracting function, but this goes beyond the scope of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have the hidden state, we can use it for tasks such as node classification.
    We can express this with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db4574c8-41af-44da-9fb4-7e90b7721985.png)'
  prefs: []
  type: TYPE_IMG
- en: In this equation, ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) is the state
    once an equilibrium is reached and *g* is a parametric function—for example, a
    fully connected layer with softmax activation for classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at how to train the GraphNN, given a set of training labels
    *t[i]* for some or all graph nodes and a mini-batch of size *m*. To train the
    GraphNN, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute ![](img/b0bb88a6-0596-4699-a87a-446972438586.png)and *o[v]* for all *m*
    nodes, following the recurrent process we just described.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the cost function (*t[i]* is the label of node *i*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bd326435-7a13-4c36-b3ac-25821841be2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Propagate the cost backward. Note that alternating the node state update of
    step 1 with the gradient propagation of the current step allows GraphNN to process
    cyclic graphs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the weights of the combined network *g(f)*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GraphNN has several limitations, one of which is that computing the equilibrium
    state vector ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) is not efficient.
    Furthermore, as we mentioned previously in this section, GraphNN uses the same
    parameters (weights) to update ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) over
    all steps *t*. In contrast, other NN models can use multiple stacked layers with
    different sets of weights, which makes it possible for us to capture the hierarchical
    structure of the data. It also allows us to compute ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) in
    a single forward pass. Finally, it's worth mentioning that, although computing ![](img/b0bb88a6-0596-4699-a87a-446972438586.png) is
    a recurrent process, GraphNN isn't a recurrent network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Gated Graph NN** model (**GGNN**, [https://arxiv.org/abs/1511.05493](https://arxiv.org/abs/1511.05493))
    tries to overcome these limitations with the help of **Gated Recurrent Unit**
    cells (or **GRU**; for more information, see [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml),
    *Understanding Recurrent Networks*) as a recurrent function. We can define GGNN
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c6865cb0-0fab-4627-b4b6-02ff5cb78f3d.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, ![](img/e46f22e8-6d32-4c56-8d73-4470554a49de.png).
    To clarify, GGNN updates the state based on its neighboring states ![](img/d3c1e172-c9e8-4106-a5b6-54d77730e785.png) of
    the same step *t* and its previous hidden state ![](img/75ceb0fc-8945-42a1-b824-4096c1bb42a0.png).
  prefs: []
  type: TYPE_NORMAL
- en: From a historical perspective, GraphNNs were one of the first GNN models. But
    as we mentioned, they have some limitations. In the next section, we'll discuss
    Convolutional Graph Networks, which are a more recent development.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Graph Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Convolutional Graph Networks** (**ConvGNN**) use a stack of special graph
    convolutional layers (Gconv*) to perform a convolution over the nodes of a graph
    when updating the state vectors. In a similar way to GraphNNs, the graph convolution
    takes the neighbors of a node and produces its vector representation ![](img/6f08cd10-d31e-4f38-ac59-8cabc213aad4.png).
    But whereas GraphNN uses the same layer (that is, the same set of weights) over
    all steps *t* of the computation of ![](img/bf2fd2ce-b2d5-48fa-8702-dbbeeea83486.png),
    ConvGNN uses different layers at every step. The difference between the two approaches
    is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/75832ae4-bfe8-484e-a504-9eefdee3a8bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Top: GraphNN uses the same Grec recurrent layer over all steps t; Bottom: GCN
    uses a different Gconv[*] layer for each step; Source: https://arxiv.org/abs/1901.00596'
  prefs: []
  type: TYPE_NORMAL
- en: 'With ConvGNN, the number of steps *t* is defined by the depth of the network.
    Although we will discuss this from a somewhat different perspective, ConvGNN behaves
    as a regular FFNN, but with graph convolutions. By stacking multiple layers, the
    final hidden representation of each node receives messages from a further neighborhood,
    as we can see in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/90f9c2fd-c6b9-4b89-a3af-73dd9243052f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Top: Node-level classification GraphCN; Bottom: Graph-level classification
    GraphCN. Source: https://arxiv.org/abs/1901.00596'
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram shows two scenarios:'
  prefs: []
  type: TYPE_NORMAL
- en: Node-level (top), where the output of each convolutional layer (including the
    last) is a vector for each node of the graph. We can perform node-level operations
    over these vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph-level (bottom), which alternates graph convolutions and pooling operations
    and ends with a readout layer, followed by several fully connected layers that
    summarize the whole graph to produce a single output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a high-level overview of ConvGNN, in the following section,
    we'll discuss graph convolutions (and after that, we'll talk about the readout
    and pooling layers).
  prefs: []
  type: TYPE_NORMAL
- en: Spectral-based convolutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are various types of graph convolutions (check out *A Comprehensive Survey
    on Graph Neural Networks*), but in this section, we'll discuss the algorithm from *Semi-Supervised
    Classification with Graph Convolutional Networks* ([https://arxiv.org/abs/1609.02907](https://arxiv.org/abs/1609.02907)).
    We'll denote this convolution with GCN to avoid confusion with the general ConvGNN
    notation, which refers to graph convolutional networks in general. GCN is a representative
    of the so-called **spectral-based** category of ConvGNNs. These algorithms define
    graph convolutions by introducing filters from the perspective of graph-signal
    processing, where the graph convolutional operation is interpreted as removing
    noises from graph signals.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the *Graph neural network* section*,* we defined the hidden node state ![](img/b1e6e909-4469-4c99-8193-9c922b7114c7.png) and
    noted that ![](img/f907d77e-60b9-436e-a154-6b69b18be076.png) in the case of GGNN.
    Let''s extend this notation by stacking the hidden vector states of all nodes
    in the graph in a matrix ![](img/b2332c0f-094d-4ecb-94e0-a1d354281c7d.png), where
    *n* is the total number of nodes in the graph and *d* is the size of the feature
    vectors. Each row of the matrix represents the hidden state of a single node.
    Then, we can define the generic formula for a single GCN layer at step *l+1* as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/145d7088-3fdd-4fae-ae22-91ca8ce6b26c.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **A** is the adjacency matrix, *f* is a nonlinear activation, such as
    ReLU, and ![](img/49f58e0e-95c2-4168-b77d-3e0cfa0058e4.png) (the feature vector
    matrix). Since ![](img/7562814c-e3e6-4725-83d2-ce4ee25ae57c.png) and ![](img/8630b436-b3bc-4ec0-89fe-8bc4fb72235b.png) have
    the same size, ![](img/95d3840b-a76f-4f57-b8bb-69b70ff60f2f.png) has the same
    dimensions as the node feature matrix **X** (see the *Graph neural networks *section). However, ![](img/05b65ee5-1076-4b17-a762-c2bd6b74bf16.png),
    where *z* is the size of the hidden state vector ![](img/150965c7-c90c-4a99-99be-972af915b99d.png) and
    is not necessarily the same as the initial *d*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s continue with a simplified but concrete version of the GCN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/181962b9-f896-49b5-9978-75461955b3af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/b3c301c4-c317-4e82-8ab4-3e9b17d1d5b5.png) is a weight matrix
    and σ is the sigmoid function. Since the adjacency matrix **A** represents the
    graph in matrix form, we can compute the output of the layer in a single operation.
    The ![](img/fa44df2d-508c-42aa-a875-20de6c4de0f9.png) operation allows each node
    to receive input from its neighboring nodes (it also allows GCN to work with both
    directed and undirected graphs). Let''s see how this works with an example. We''ll
    use the five-node graph that we introduced in the *Graph neural networks *section*.*
    For the sake of readability, we''ll assign a one-dimensional vector hidden state ![](img/712b0558-85ca-42f7-9e12-f4c66e78f352.png)
    for each node with a value equal to the node number![](img/ac37c382-7f41-405e-bd39-1cf4acf16e4d.png).
    Then we can compute the example with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1d830f5-de0c-4fcd-94f8-5876ec5da8cc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see how ![](img/2a88e6d6-53ea-4b02-977e-90196ff09f68.png) because it
    receives input from nodes 2, 3, and 5\. If ![](img/712b0558-85ca-42f7-9e12-f4c66e78f352.png) had
    more dimensions, then each cell of the output vector would be a sum of the corresponding
    cells of the state vectors of the input nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/72198d96-67d3-4f33-bf65-0a078dfcf0fd.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/a64b3256-e78f-4152-b6ae-e3ef4f263258.png) are the cells of the
    adjacency matrix.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this solution is elegant, it has two limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: Not all nodes receive input from their own previous state. In the preceding
    example, only node 2 takes input from itself because it has a loop edge (this
    is the edge that connects the node to itself). The solution to this problem is
    to artificially create loop edges for all nodes by setting all values along the
    main diagonal of the adjacency matrix to ones: ![](img/7ba61638-6168-435c-8cff-ba793c77c713.png).
    In this equation, **I** is the identity matrix, which has ones along the main
    diagonal and zeros in all other cells.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since **A** is not normalized, the state vectors of nodes with a large number
    of neighbors will change their scale in a different way compared to nodes with
    a smaller number of neighbors. We can see this in the preceding example, where ![](img/82a870e8-c31a-4587-a0b3-572fa5aa1099.png) is
    larger compared to the other nodes because node 4 has 3 nodes in its neighborhood.
    The solution to this problem is to normalize the adjacency matrix in such a way
    that the sum of all elements in one row is equal to 1: ![](img/d2340045-f977-47fe-98c8-f1f2c167e1f6.png).
    We can achieve this by multiplying **A** by the inverse degree matrix **D**^(-1).
    The degree matrix **D** is a diagonal matrix (that is, all other elements except
    the main diagonal are zeros) that contains information about the degree of each
    node. We refer to the number of neighbors of a node as a degree of that node.
    For example, the degree matrix of our example graph is the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/d6e2c432-11b2-4ea4-ade3-a9b3e6a13ff0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, **D**^(-1)**A** becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/441e83ce-02c3-40d5-bfac-0b6e443512a6.png)'
  prefs: []
  type: TYPE_IMG
- en: This mechanism assigns the same weight to each of the neighboring nodes. In
    practice, the authors of the paper discovered that using the symmetric normalization ![](img/9f5251dd-2d80-411a-b3ed-76f750f333c8.png)works
    better.
  prefs: []
  type: TYPE_NORMAL
- en: 'After we incorporate these two improvements, the final form of the GCN formula
    can be written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3baa1355-11c2-4958-9f68-0ce56e796f8c.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that the GCN we just described includes only the immediate neighborhood
    of the node as the context. Each stacked layer effectively increases the receptive
    field of the node beyond its immediate neighbors by 1\. The receptive field of
    the second layer of a ConvGNN includes the immediate neighbors, the receptive
    field of the second layer includes the nodes that are two hops away from the current
    node, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll look at the second major category of graph convolution
    operations, called spatial-based convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial-based convolutions with attention
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The second ConvGNN category is spatial-based methods, which take inspiration
    from the computer vision convolution ([Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*). We can think of an image as a graph,
    where each pixel is a node, directly connected to its neighboring pixels (the
    left-hand image in the following diagram). For example, if we use 3 × 3 as a filter,
    the neighborhood of each pixel consists of eight pixels. In the image convolution,
    this 3 × 3 weighted filter is applied over the 3 × 3 patch and the result is a
    weighted sum of the intensities of all nine pixels. Similarly, the spatial-based
    graph convolution convolves the representation of the central node with the representations
    of its neighbors to derive an updated representation for the central node, as
    illustrated in the right-hand image in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff69efd8-2240-4c09-91b1-0dc56b8601e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: 2D convolution over a pixel grid; Right: Spatial graph convolution. Source: https://arxiv.org/abs/1901.00596'
  prefs: []
  type: TYPE_NORMAL
- en: The generic spatial-based convolution is somewhat similar to the GCN in the
    sense that both operations rely on graph neighbors. The GCN uses the inverse degree
    matrix to assign weights to each neighbor. Spatial convolutions use the convolution
    filter for the same purpose. The main difference between the two is that in the
    case of GCNs, the weights are fixed and normalized, whereas the filter weights
    of the spatial convolution are learnable and not normalized. In some sense, we
    can think of the GCN as a spatial-based approach as well.
  prefs: []
  type: TYPE_NORMAL
- en: We'll continue this section with a specific type of spatial-based model called
    the **Graph Attention Network** (**GAT**) (for more information, go to [https://arxiv.org/abs/1710.10903](https://arxiv.org/abs/1710.10903)),
    which implements graph convolutions with a special graph self-attention layer.
    Instead of learning a convolutional filter or using the averaged adjacency matrix
    as a GCN, GAT uses the attention scores of the self-attention mechanism to assign
    weights to each of the neighboring nodes. The GAT layer is the main building block
    of graph attention networks, which consist of multiple stacked GAT layers. As
    with GCN, each additional layer increases the receptive field of the target node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to GCN, the GAT layer takes as input a set of node feature vectors
    ![](img/c8d92091-850b-429b-9887-f9951f2b976f.png) and outputs a different set
    of feature vectors ![](img/94b3890d-c654-4568-a6e0-7b28defceaa8.png), not necessarily
    of the same cardinality. Following the procedure we outlined in [Chapter 8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml), *Sequence-to-Sequence
    Models and Attention*, the operation starts by computing the alignment scores
    between the feature vectors ![](img/22e50d7b-c169-4fe9-9244-60ca93fa9882.png) and
    ![](img/25e3efe7-32af-43ac-8b81-e7d78773b0fd.png) of each two nodes of the neighborhood:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e6e85a94-9570-4d47-9407-50a8683c25e8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/dda45c47-d17b-4ed9-a7ed-3d28f1e198fb.png) is a weight matrix
    that transforms the input vectors to the cardinality of the output vectors and
    provides the necessary learnable parameters. The *f[a]* expression is a simple
    FFN with a single layer and LeakyReLU activation, which is parameterized by a
    weight vector ![](img/722ace34-34b7-4991-a8e3-5f3ab6c05625.png) and implements
    the additive attention mechanism:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7f67757-7c1b-422f-9386-391ea231573b.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/cfa05b0f-6474-45b1-9e53-9721753eb371.png) represents concatenation. If
    we don't impose any restrictions, each node will be able to attend to all other
    nodes of the graph, regardless of their proximity to the target node; however,
    we're only interested in the neighboring nodes. The authors of GAT propose to
    solve this by using masked attention, where the mask covers all nodes that are
    not immediate neighbors of the target node. We'll denote the immediate neighbors
    of node *i* with ![](img/aff2d979-9d86-46ce-a910-60474eb51ada.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we compute attention scores by using softmax. The following are the generic formula
    and the formula with *f[a]* (applied only over the immediate neighbors):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d88cf72d-7ec5-4a14-a934-b8d252571566.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once we have the attention scores, we can use them to compute the final output
    feature vector of each node (we referred to this as the context vector in [Chapter
    8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml), *Sequence-to-Sequence Models and
    Attention*), which is a weighted combination of the input feature vectors of all
    neighbors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78546e42-dca7-43b1-9697-59eab1457c33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, σ is the sigmoid function. The authors of the paper also found multihead
    attention to be beneficial to the performance of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88571404-92dd-4633-b741-4b6ee1978dcc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *k* is the index of each head (for a total of *K* heads), ![](img/8969baab-d817-478e-b2fe-06e80d6affc7.png) are
    the attention scores for each attention head, and ![](img/152f315b-ca65-41dc-a5b5-2d1e1a86a10b.png)is
    the weight matrix of each attention head. Since ![](img/2ce5f52e-452e-46cb-8bf7-2bdd2ed5d3a8.png) is
    a result of concatenation, its cardinality will be *k × z[l+1]*. Because of this,
    concatenation is not possible in the final attention layer of the network. To
    solve this, the authors of the paper suggest that you should average the outputs
    of the attention heads in the final layer (denoted with index *L*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/07cb2578-7a7a-48e3-ad4c-2f725c517613.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram shows a comparison between regular and multihead attention
    in the GAT context. In the left image, we can see the regular attention mechanism,
    applied between two nodes and *i* and *j*. In the right image, we can see the
    multihead attention with *k = 3* heads of node *1* with its neighborhood. The
    aggregated features are either concatenated (for all hidden GAT layers) or averaged
    (for the final GAT layer):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ec5f308-a34d-4a98-9ad7-2afb2b22536f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: Regular attention over two nodes; Right: Multihead attention of node
    1 with its neighborhood. Source: https://arxiv.org/abs/1710.10903'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have the output of the final GAT layer, we can use it as input to the
    next task-specific layers. For example, this could be a fully connected layer
    with softmax activation for node classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we conclude this section devoted to ConvGNNs, let''s discuss two final
    components that we haven''t addressed yet. The first is the readout layer that
    we introduced in the graph-level classification example at the beginning of the *Convolutional*
    *Graph* *Networks* section. It takes as input all the node states of the last
    graph convolutional layer **H**^((*L*)) and outputs a single vector that summarizes
    the whole graph. We can define it formally as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02793365-ff3a-479e-bef6-59c93a6c6798.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *G* represents the set of graph nodes and *R* is the readout function.
    There are various ways to implement it, but the simplest is to take the element-wise
    sum or mean of all node states.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next (and final) ConvGNN component we''ll look at is the pooling operation.
    Once again, there are various ways to use this, but one of the simplest is to
    use the same max/average pooling operations as we did in the computer vision convolutions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/20c1fb51-9f9a-46b6-a295-1e57bf879451.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *p* indicates the size of the pooling window. If the pooling window contains
    the whole graph, the pooling becomes similar to the readout.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion about ConvGNNs. In the next section, we'll discuss
    graph autoencoders, which provide a way to generate new graphs.
  prefs: []
  type: TYPE_NORMAL
- en: Graph autoencoders
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's have a quick recap of autoencoders, which we first introduced in [Chapter
    5](319c18b2-c733-402e-937c-ace912ff87ca.xhtml),*Generative Models*. An **autoencoder**
    is an FFN that tries to reproduce its input (more accurately, it tries to learn
    an identity function, ![](img/9b1a90cb-7d76-4d0a-b2ac-ab69e9a8f07e.png)). We can
    think of the autoencoder as a virtual composition of two components—the **encoder**,
    which maps the input data to the network's internal latent feature space (represented
    as vector *z*)*,* and the **decoder**, which tries to reconstruct the input from
    the network's internal data representation. We can train the autoencoder in an
    unsupervised way by minimizing a loss function (known as a **reconstruction error)**,
    which measures the distance between the original input and its reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: '**Graph autoencoders** (**GAE**) are similar to autoencoders, with the distinction
    that the encoder maps the graph nodes into the autoencoder latent feature space
    and then the decoder tries to reconstruct specific graph features from it. In
    this section, we''ll discuss a GAE variant, introduced in *Variational Graph Auto-Encoders*
    ([https://arxiv.org/abs/1611.07308](https://arxiv.org/abs/1611.07308)), which
    also outlines the variational version of GAE (**VGAE**). The following diagram shows
    an example GAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad20131f-4b1e-4cd7-b137-7c2e4b24725b.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of graph autoencoder. Source: https://arxiv.org/abs/1901.00596
  prefs: []
  type: TYPE_NORMAL
- en: 'The encoder is a GCN model that we defined in the *Spectral-based convolutions* section to
    compute a network embedding![](img/e799be66-a517-4c9e-b202-62de22c1ae57.png) for
    graph nodes, where the embedding for each of the *n* total nodes is a *d*-dimensional
    vector **z**. It takes as input the adjacency matrix **A** and the set of node
    feature vectors **X** (like the other GNN models we discussed in this chapter).
    The encoder is represented by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0753995-166a-4769-8d36-7d50faced583.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **W[1]** and **W[2]** are the learnable parameters (weights) of the two
    GCN graph convolutions, and *f* is a nonlinear activation function, like ReLU. The
    authors of the paper use two graph convolutional layers, although the proposed
    algorithm can work for any number of layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decoder tries to reconstruct the graph adjacency matrix ![](img/3f98fb9c-d5e9-454b-9459-6b3c28bfe9c7.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ccf67b6-cb38-47a2-b535-a03c5c14aa87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, σ is the sigmoid function. It first computes the dot (or inner) product
    between **Z** and its transpose: ![](img/c60a2b16-86cd-4a4a-b5ec-5a9ced360624.png). To
    clarify, this operation computes a dot product of the vector embedding *z[i]*
    of each node *i* and the vector embedding *z[j]* of every other node *j* of the
    graph, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f72a2b2-988f-4043-a076-8ad4bc89f926.png)'
  prefs: []
  type: TYPE_IMG
- en: As we mentioned in [Chapter 1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml),
    *The Nuts and Bolts of Neural Networks*, we can think of the dot product as a
    similarity measure between vectors. Therefore, ![](img/2921f759-7c77-4965-95a0-a5832062c067.png) measures
    the distance between every possible pair of nodes. These distances serve as a
    base for the reconstruction effort. After this, the decoder applies a nonlinear
    activation function and proceeds to reconstruct the graph adjacency matrix. We
    can train the GAE by minimizing the discrepancy between the real reconstructed
    adjacency matrices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s focus on **Variational Graph Autoencoders** (**VGAE**). Much like
    the **Variational Autoencoders** (**VAE**) we discussed in [Chapter 5](319c18b2-c733-402e-937c-ace912ff87ca.xhtml),
    *Generative Models*, the VGAE is a generative model that can generate new graphs
    (more specifically, new adjacency matrices). To understand this, let''s start
    with a short recap of VAEs. Unlike regular autoencoders, the VAE bottleneck layer
    won''t directly output latent state vectors. Instead, it will output two vectors,
    which describe the **mean** μ and the **variance** σ of the distribution of the
    latent vector **z**. We''ll use them to sample a random vector ε with the same
    dimensions as **z** from a Gaussian distribution. More specifically, we''ll shift
    ε by the latent distribution''s mean μ and scale it by the latent distribution''s
    variance σ:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8e54537-e06d-4f5f-aa11-7fccb06056bc.png)'
  prefs: []
  type: TYPE_IMG
- en: This technique is known as the **reparameterization** trick, and it allows the
    random vector to have the same mean and variance as the original dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can think of VGAE as a combination of GAE and VAE in the sense that it works
    with graph inputs (such as GAE) and follows the same principles to generate new
    data (like VAE). First, let''s focus on the encoder, which is split into two paths:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79b471f1-7bc7-43e7-ae07-ea9e44021ce1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the weights **W[0]** are shared between the paths, *![](img/37a71eea-c7d9-44c7-9af8-f0b2cc2217cc.png)* is
    the symmetrically normalized adjacency matrix, ***μ*** is the matrix of mean vectors ***μ**[i]*,
    and ***σ*** is the matrix of variances ***σ**[i]* of each graph node. Then, the
    encoder inference step for the full graph is defined as the inner product of the
    latent representations of all graph nodes *i*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b4675d4-526e-406a-9f32-a24fb0b4444b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this formula, *n* is the number of nodes in the graph and ![](img/bf7e4027-8056-49cc-ba8d-d2bdc0e9618e.png) represents the
    encoder approximation of the real probability distribution ![](img/9451f080-cacc-4328-a19d-760505c64b27.png),
    where *φ* is the network parameters (here, we have preserved the notation of [Chapter
    5](319c18b2-c733-402e-937c-ace912ff87ca.xhtml), *Generative Models*). The approximation
    is a Gaussian distribution with node-specific mean *μ[i]* and diagonal covariance
    values ![](img/2f51c5e2-c99d-47ca-bb62-40c4b90271e7.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0778279e-d32e-46ba-8829-8147a91ed5ae.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we define the generative step, which creates the new adjacency matrix.
    It is an inner product of the random latent vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/033b088c-576d-467d-b720-253c5476b649.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/9ca6cbb0-7521-41d1-a4d5-7c34c4c78dd7.png) indicates whether an
    edge exists between two nodes *i* and *j*, and ![](img/154fcf49-1337-4f63-982d-b95af38be050.png) represents
    the decoder approximation of the real probability ![](img/7daf605e-a4ff-4848-85e4-4628a726bec9.png). We
    can train the VGAE using the already familiar VAE cost:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98b1075f-740f-4e14-bae3-2ef89538d3d8.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the first term is the Kullback–Leibler divergence and the second is the
    reconstruction cost.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our description of GAE and VGAE. In the next section, we'll discuss
    yet another graph-learning paradigm, which makes it possible to mix structured
    and unstructured data as network inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Neural graph learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll describe the **Neural Graph Learning** paradigm (**NGL**)
    (for more information, see *Neural Graph Learning: Training Neural Networks Using
    Graphs* at [https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf](https://storage.googleapis.com/pub-tools-public-publication-data/pdf/bbd774a3c6f13f05bf754e09aa45e7aa6faa08a8.pdf)),
    which makes it possible to augment training based on unstructured data with structured
    signals. More specifically, we''ll discuss the **neural structured learning**
    framework (**NSL**) (for more information, go to [https://www.tensorflow.org/neural_structured_learning/](https://www.tensorflow.org/neural_structured_learning/)),
    which is based on TensorFlow 2.0 and implements these principles.'
  prefs: []
  type: TYPE_NORMAL
- en: To understand how NGL works, we'll use the CORA dataset ([https://relational.fit.cvut.cz/dataset/CORA](https://relational.fit.cvut.cz/dataset/CORA)),
    which consists of 2,708 scientific publications classified into 1 of 7 classes
    (this is the unstructured part of the dataset). The number of unique words in
    all publications (that is, the vocabulary) in the dataset is 1,433\. Each publication
    is described as a single **multihot** encoded vector. This is a vector of size
    1,433 (the same as the vocabulary), where the cell values are either 0 or 1\.
    If a publication contains the *i-*th word of the vocabulary, then the *i*th cell
    of the one-hot encoded vector of that publication is set to 1\. If the word is
    not present in the publication, the cell is set to 0\. This mechanism preserves
    information about the words present in an article, but not information about their
    order. The dataset also contains a directed graph of 5,429 citations, where the
    nodes are publications and the edges between them indicate whether publication
    *v* cites publication *u* (this is the structured part of the dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s focus on NGL itself, starting with the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3465ccd4-42ab-4e41-9ebb-38a0638e7460.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The NGL framework: green solid lines show the unstructured input data flow;
    yellow dashed lines show the structured signals data flow; Inspired by: https://www.tensorflow.org/neural_structured_learning/framework'
  prefs: []
  type: TYPE_NORMAL
- en: It acts as a kind of wrapper over the regular NN training framework, and it
    can be applied over any type of network, including FFN and RNN. For example, we
    can have a regular FFN, which takes as input the multihot encoded publication
    vector and tries to classify it to one of the 7 classes, using softmax output,
    as illustrated in the preceding diagram with green uninterrupted lines. NGL allows
    us to extend this network with structured data, offered by the citations, as illustrated
    by the yellow dashed lines.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at how this works. We start with the assumption that neighboring
    nodes in the graph are somewhat similar. We can transfer this assumption to the
    NN domain by saying that the embedding vector produced by the NN (the embedding
    is the output of the last hidden layer) of sample *i* should be somewhat similar
    to the embedding of sample *j*, provided that the two samples are neighbors in
    the associated graph. In our example, we can assume that the embedding vector
    of publication *i* should be similar to the embedding of publication *j*, provided
    that one of them cites the other (that is, they are neighbors in the graph of
    citations). In practice, we can implement this with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a dataset that contains both unstructured data (multihot-encoded
    publications) and structured data (the graph of citations).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build special types of composite training samples (organized in batches), where
    each composite sample consists of a single regular input sample (one multihot-encoded
    publication) and *K* of its neighboring samples (the multihot-encoded publications
    that cite or are cited by the initial sample).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feed the composite sample to the NN and produce embeddings for both the initial
    sample and its neighbors. Although the preceding diagram shows the two paths running
    in parallel, this is not the case. The diagram aims to illustrate that the network
    processes both the central sample and its neighbors, but the actual NN is not
    privy to this arrangement—it just takes all of the multihot-encoded inputs as
    part of a single batch and processes them. Instead, the NSL portion on top of
    the regular NN differentiates the two components.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute a special type of composite loss function composed of two parts: regular
    supervised loss and regularization neighbor loss, which uses a metric to measure
    the distance between the initial sample embedding and the embedding of its neighbors.
    The neighbor loss is the mechanism that allows us to augment unstructured training
    data with structured signals. The composite loss is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e4f50ba5-e319-41ab-8440-81d100e27716.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This formula has the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n* is the number of composite samples in the mini-batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/7ea30bdb-fb32-4c9b-9c37-4ebfca7b74f5.png) is the supervised loss function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*f[θ]* is the NN function with weights *θ.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*α* is a scalar parameter that determines the relative weight between the two
    loss components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e380c730-c4df-4fa8-b86d-afe275fc6ed0.png) is the set of graph neighbors
    of sample *x[i]*. Note that the neighbor loss iterates over all neighbors of all
    nodes of the graph (two sums).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/8dae52a2-49ba-4628-8a68-369127f9643c.png) is the weight of the graph
    edge between samples *i* and *j*. If the task doesn''t have a notion of weights,
    we can assume that all weights are 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/9820c380-205c-4127-8628-991725805d52.png) is the distance metric between
    the embedding vectors of samples *i* and *j*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the regularization nature of the neighbor loss, NGL is also referred
    to as **graph regularization**.
  prefs: []
  type: TYPE_NORMAL
- en: Propagate the error backward and update the network weights *θ*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have an overview of graph regularization, let's implement it.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing graph regularization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll implement graph regularization over the Cora dataset
    with the help of the NSL framework. This example is based on the tutorial available
    at [https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora](https://www.tensorflow.org/neural_structured_learning/tutorials/graph_keras_mlp_cora).
    Before we proceed with the implementation, we have to satisfy some prerequisites.
    First, we need TensorFlow 2.0 and the `neural-structured-learning` 1.1.0 package
    (available via `pip`).
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we satisfy these requirements, we can proceed with the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with the package imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll continue with some constant parameters of the program (hopefully the
    constant names and the comments speak for themselves):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The files under `TRAIN_DATA_PATH` and `TEST_DATA_PATH` contain the Cora dataset
    and labels, preprocessed in a TensorFlow-friendly format.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s load the dataset. This process is implemented by using two functions: `make_dataset`,
    which builds the whole dataset, and `parse_example`, which parses a single composite
    sample (`make_dataset` uses `parse_example` internally). We''ll start with `make_dataset`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `dataset.map(parse_example)` internally applies `parse_example` over
    all samples of the dataset. Let''s continue with the definition of `parse_example` ,
    starting from the declaration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The function creates the `feature_spec` dictionary that represents a kind of
    template for a single composite sample, which is later filled with actual data
    from the dataset. First, we fill `feature_spec` with the placeholder instances
    of `tf.io.FixedLenFeature` for `''words''`, which represents a multihot-encoded
    publication, and `''label''`, which represents the class of the publication (please
    bear in mind the indentation as this code is still part of `parse_example`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we iterate over the first `NUM_NEIGHBORS` neighbors and add their multihot
    vectors and edge weights to `feature_spec` under the `nbr_feature_key` and `nbr_weight_key`
    keys respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we populate the template with a real sample from the dataset with
    the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can instantiate the training and testing datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s implement the model, which is a simple FFN with two hidden layers
    and softmax as output. The model takes the multihot-encoded publication vector
    as input and outputs the publication class. It is independent of NSL and can be
    trained, in a simple supervised way, as a classification:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let''s instantiate the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We have all the ingredients that we need to use graph regularization. We''ll
    start by wrapping the `model` with the NSL wrapper:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We instantiate the `graph_reg_config` object (an instance of `nsl.configs.GraphRegConfig`)
    with the graph regularization parameters: `max_neighbors=NUM_NEIGHBORS` is the
    number of neighbors to use, `multiplier=0.1` is equivalent to the parameter α
    of the composite loss we introduced in the *Neural structured learning *section,
    and `distance_type=nsl.configs.DistanceType.L2` is the distance metric between
    the neighboring node embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can build a training framework and initiate the training for 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training is done, we can run the trained model over the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If everything goes alright, the output of the program should be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This concludes our discussion about GNN. As we mentioned, there are various
    types of GNNs, and we only included a small set here. If you are interested in
    learning more, I suggest that you refer to the survey paper we introduced at the
    beginning of the section or check out the following curated list of GNN-related
    papers at [https://github.com/thunlp/GNNPapers](https://github.com/thunlp/GNNPapers).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we'll discuss a new type of NN that uses external memory
    to store information.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing memory-augmented NNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've already seen the concept of memory (albeit in a strange form) in NNs—for
    example, the LSTM cell can add or delete information on its hidden cell state
    with the help of the input and the forget gates. Another example is the attention
    mechanism, where the set of vectors that represent the encoded source sequence
    can be viewed as external memory that is written to by the encoder and read from
    by the decoder. But this ability comes with some limitations. For one, the encoder
    can only write to a single memory location, which is the current element of the
    sequence. It also cannot update previously written vectors. On the other hand,
    the decoder can only read from the database, but cannot write to it.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll take the concept of memory one step further and look
    at **Memory-Augmented NNs** (**MANNs**), which resolve these limitations. This
    is a new class of algorithm and is still in its early stages, unlike the more
    mainstream types of NN, such as convolutional and RNNs, which have been around
    for decades. The first MANN network we'll discuss is the neural Turing machine.
  prefs: []
  type: TYPE_NORMAL
- en: Neural Turing machines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The concept of MANNs was first introduced with the concept of the **neural
    Turing machine** (**NTM**) (for more information, go to  [https://arxiv.org/abs/1410.5401](https://arxiv.org/abs/1410.5401)).
    The NTM has two components:'
  prefs: []
  type: TYPE_NORMAL
- en: A NN controller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An external memory, represented as a matrix ![](img/3a804a65-9cd9-4d40-ba70-790164f5d22b.png).
    The matrix contains *n* rows of *d*-dimensional vectors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram provides an overview of the NTM architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63d5e3d7-f148-42f6-b002-0023aba080ea.png)'
  prefs: []
  type: TYPE_IMG
- en: NTM Source: https://arxiv.org/abs/1410.5401
  prefs: []
  type: TYPE_NORMAL
- en: An NTM works in a sequential fashion (like an RNN), where the controller takes
    input vectors and produces output vectors in response. It also reads and writes
    to memory with the help of multiple parallel read/write heads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s focus on the reading operation, which is very similar to the attention
    mechanism we looked at in [Chapter 8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml),
    *Sequence-to-Sequence Models and Attention*. A read head always reads the full
    memory matrix, but it does so by attending to different memory vectors with different
    intensities. To do this, the read head emits an *n*-dimensional vector ![](img/6daa56ae-2570-4259-8817-ab17c4c9ce79.png) (at
    step *t*) with the following constraints:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1814d2d9-a6b2-452d-9a6d-b3bcc602ba3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The ![](img/6d2eb2e7-c39d-4d13-b8bb-caa46bb4c330.png) implements an attention
    mechanism, where each cell *i* of the vector indicates the weight of the *i*th
    memory vector (that is, the *i*th row of the matrix **M**) in forming the output.
    The output of a read operation at step *t* is a *d*-dimensional vector **r**[*t*],
    defined as the weighted sum of all memory vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b280f69c-ef47-48e3-b01b-3aab5fc3bbf2.png)'
  prefs: []
  type: TYPE_IMG
- en: This operation is similar to the soft attention mechanism we discussed in [Chapter
    8](0a021de6-b007-49bf-80e9-b7f6a72cbba7.xhtml),*Sequence-to-Sequence Models and
    Attention.* Soft attention (unlike hard attention) is differentiable, and this
    also true of this operation. In this way, the whole NTM (controller and memory)
    is a single differentiable system, which makes it possible to train it with gradient
    descent and backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s focus on the writing operation, which is composed of two steps:
    **erase** followed by an **add**. The write head emits the same type of attention
    vector ![](img/3f322f02-bf5f-4755-bf19-2a8c6a629a17.png) as the reading heads.
    It also emits another **erase** vector ![](img/16e529ef-8102-4f10-bd67-49b4debee77d.png),
    whose values are all within the (0, 1) range. We can define the erase operation at
    step *t* over a single row *i* of the memory as a function of these two vectors
    and the memory state at step *t-1*, *![](img/1615bdc5-cb7e-4add-9413-819093aa8950.png)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ac76018-5dec-4d02-a939-af1502e59eab.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, **1** is a *d*-dimensional vector of ones and the multiplication between ![](img/0a454eb0-2060-4036-8808-ec22c05940af.png) and
    the erase component is element-based. According to this formula, a memory location
    can be erased only if both the weight ![](img/b683581b-9d4f-4dbb-a5e9-c31f603969ac.png) and
    **e**[*t*] are nonzero. This mechanism can work with multiple attention heads
    writing in an arbitrary order, because multiplication is commutative.
  prefs: []
  type: TYPE_NORMAL
- en: 'The erase operation is followed by the add operation. The write head produces
    an **add** vector ![](img/0e7b6af0-ab80-48ed-ad62-7615fc08410f.png), which is
    added to the memory after the erase to produce the final memory state at step
    *t*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5634fb9-b32d-4620-9f0c-f6d4fb9ca6dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We are now familiar with read and write operations, but we still don''t know
    how to produce attention vectors ![](img/b68175cd-78ce-4ada-844c-77285e9aa0e9.png)(we''ll
    omit the superscript index, because the following descriptions apply for both
    read and write heads). NTM uses two complementary addressing mechanisms to do
    this: content-based and location-based.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with content-based addressing, where each head (both reading and
    writing) emits a key vector ![](img/199a165e-f567-4c4c-8c71-d4ffd10f4489.png).
    This vector is compared to each memory vector ![](img/1a7e81ac-d914-400e-98d8-a4e4705d0dc9.png) using
    the similarity measure ![](img/ccf35344-a461-4be5-8eb0-4adf5effc5ba.png). The
    NTM authors propose using cosine similarity, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff8af2ff-641e-4ad7-a7d7-1e4a2dee9576.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, we define a single cell of the content-based addressing vector as a softmax
    over the similarity results of all memory vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/45358260-1d54-47a0-8550-50668fbc10a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/25c1dc3b-42fa-4009-8b71-1ce5a3b112d2.png) is a scalar value key
    strength, which widens or narrows the scope of the focus. For small values of ![](img/68d864a1-6d82-4cb8-b3e8-8e99f90b6eab.png),
    the attention will diffuse over all memory vectors, and for large ![](img/fd2b11cb-dc95-4bf1-85f3-26254d747cc3.png),
    the attention will focus only on the most similar memory vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of NTM argue that in some problems, content-based attention is not
    enough, because the content of a variable can be arbitrary but its address has
    to be recognizable. They cite arithmetic problems as one such problem: two variables, *x*
    and *y,* can take on any two values, but the procedure *f(x, y) = x × y* should
    still be defined. A controller for this task could take the values of the variables
    *x* and *y*, store them in different addresses, then retrieve them and perform
    a multiplication algorithm. In this case, the variables are addressed by location,
    not by content, which brings us to the location-based addressing mechanism. It
    works with both random-access memory jumps and simple iterations across locations.
    It does this by shifting the attention weights one step forward or backward.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if the current weighting focuses entirely on a single location,
    a rotation of 1 would shift the focus to the next location. A negative shift would
    move the weighting in the opposite direction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Content and location addressing work in combination, as shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85a0d3f5-3abe-477a-ad36-8d35cf7cf6d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Flow diagram of the addressing mechanism. Source: https://arxiv.org/abs/1410.5401
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how it works step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: Content addressing produces the content addressing vector ![](img/24698595-cba6-4e49-a5cc-eaa5b7e39db7.png), based
    on the memory ![](img/0ca2bba2-b305-4317-be6d-f6fb3f8f4706.png), the key vector ![](img/9f276f5b-544b-4fcc-901c-381034e1ce43.png),
    and the key strength ![](img/05075069-9a60-4db0-a319-edb925e1bee5.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Interpolation** is the first of three steps in the location addressing mechanism,
    and it comes before the actual weight shifting. Each head (read or write) emits
    a scalar **interpolation gate** *g[t]* in the (0, 1) range. The *g[t]* dictates
    whether to preserve the weight ![](img/913e6859-677b-4e4a-a2a9-f201b25b0b42.png) produced
    by the head at step *t-1* or replace it with the content-based weight ![](img/24698595-cba6-4e49-a5cc-eaa5b7e39db7.png) of
    the current step *t.* The interpolation is defined as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1b1a66be-56db-4c99-8fbe-eba8c9f4d51a.png)'
  prefs: []
  type: TYPE_IMG
- en: If *g[t] = 0*, then we'll preserve the previous addressing vector completely.
    Alternatively, if *g[t] = 1*, we'll only use the content-based addressing vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is the **convolutional shift**, which takes interpolation attention
    ![](img/2695b5b4-f6e0-4ea2-ad94-4206e85b7c04.png)and determines how to shift it.
    Let''s assume that the head attention can shift forward (+1), backward (-1), or
    stay the same (0). Each head emits a shift weighting *s[t]* that defines a normalized
    distribution over the allowed shifts. In this case, *s[t]* will have three elements,
    which indicate the degree to which shifts of -1, 0, and 1 are performed. If we
    assume that the memory vector indices are 0-based (from 0 to *n-1*), then we can
    define the rotation of ![](img/f90f0bf5-bcad-43ac-a5d5-907197941f39.png) by *s[t]*
    as a circular convolution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/575ed1d9-39a1-487c-83c0-ec259ba0c411.png)'
  prefs: []
  type: TYPE_IMG
- en: Note that, although we iterate over all memory indices, *s[t]* will have nonzero
    values only at the allowed positions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final addressing step is the **sharpening** step. One side, effect of the
    ability to simultaneously shift with different degrees over multiple directions
    is that the attention might blur. For example, let''s say that we shift forward
    (+1) with a probability of 0.6, shift backward (-1) with a probability of 0.2,
    and don''t shift (0) with a probability of 0.2\. When we apply the shifting, the
    original focused attention will blur between the three locations. To solve this,
    the authors of NTM suggest that you modify each head to emit another scalar ![](img/d3f6fbce-c18b-4f0e-9827-0f14088edf05.png),
    which will sharpen the final results using the following formula:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/65574cfa-18eb-4cbe-94c2-3d48206dded0.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we know how addressing works, let's focus on the controller, where
    we can use either RNN (for example, LSTM) or FFN. The authors of NTM argue that
    an LSTM controller has internal memory, which is complementary to the external
    memory and also allows the controller to mix information from multiple time steps.
    However, in the context of NTM, an FFN controller can mimic an RNN one by reading
    and writing at the same memory location at every step. Additionally, the FFN is
    more transparent because its read/write pattern is easier to interpret than the
    internal RNN state.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of the paper illustrate how NTM works with several tasks, one of
    which is a copy operation where the NTM has to replicate the input sequence as
    output. The task illustrates the model''s ability to store and access information
    over long periods of time. The input sequence has a random length between 1 and
    20\. Each element of the sequence is a vector with eight binary elements (representing
    a single byte). First, the model takes the input sequence step by step until a
    special delimiter is reached. Then, it starts to generate the output sequence.
    No additional inputs are presented during the generation phase to ensure that
    the model can generate the entire sequence without intermediate assistance. The
    authors compare the performance of NTM- and LSTM-based models and note that NTM
    converges faster during training and can replicate longer sequences compared to
    LSTM. Based on these results, and after examining the interactions of the controller
    and the memory, they conclude that NTM doesn''t simply memorize the input sequence;
    instead, it learns a type of copy algorithm. We can describe the sequence of operations
    for the algorithm with the following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1caca29c-2202-4f57-b9d7-fe759aca2b8d.png)'
  prefs: []
  type: TYPE_IMG
- en: The NTM model learns a form of copy algorithm: source: https://arxiv.org/abs/1410.5401
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s focus on the copy algorithm from the perspective of the interaction
    between the controller and the memory, as illustrated in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ca3e380-8db5-43da-9134-41804d670ece.png)'
  prefs: []
  type: TYPE_IMG
- en: The controller/memory interaction during the copy algorithm; Source: https://arxiv.org/abs/1410.5401
  prefs: []
  type: TYPE_NORMAL
- en: The left column shows the input phase. The top-left image represents the input
    sequence of 8-bit binary vectors, the middle-left image represents the vectors
    added to the memory, and the bottom-left image represents the memory write attention
    weights at each step. The right column shows the output phase. The top-right image
    represents the generated output sequence of 8-bit binary vectors, the middle-right
    image represents the vectors read from the memory, and the bottom-right image
    represents the memory-read attention weights at each step. The bottom images illustrate
    incremental shifts of the head locations during write and read operations. Note
    that attention weights are clearly focused on a single memory location. At the
    same time, the input and output sequences read from the same location at each
    time step and the read vectors are equivalent to the write vectors. This indicates
    that each element of the input sequence is stored in a single memory location.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we conclude this section, let''s mention that the authors of NTM have
    released an improved memory network architecture called a **Differential Neural
    Computer** (**DNC**) (for more information, see *Hybrid computing using a neural
    network with dynamic external memory*, at [https://www.nature.com/articles/nature20101](https://www.nature.com/articles/nature20101)).
    The DNC introduces several improvements over NTM:'
  prefs: []
  type: TYPE_NORMAL
- en: The model only uses content-based addressing (as opposed to content and location
    in NTM).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model uses dynamic memory allocation by maintaining a list of available
    memory locations by adding locations to, and removing them from, a linked list
    (this is still differentiable). This mechanism allows the model to write new data
    only at locations that are marked as free.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The model uses temporal memory linkage by maintaining information about the
    order of the memory locations that the controller writes to, which allows it to
    store sequential data at different memory locations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our description of the NTM architecture. In the next section,
    we'll discuss an improvement to NTM introduced in the *One-shot Learning with
    Memory-Augmented Neural Networks* paper ([https://arxiv.org/abs/1605.06065](https://arxiv.org/abs/1605.06065)).
    We'll denote the improved architecture with MANN* to avoid confusion with the
    MANN acronym, which references the general class of memory networks.
  prefs: []
  type: TYPE_NORMAL
- en: MANN*
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The MANN* read operation is very similar to the NTM read operation, with the
    exception that it doesn''t include the key strength parameter ![](img/8c0a80bc-2768-4808-8e72-f461746e5822.png).
    On the other hand, MANN* introduces a new content-based write addressing mechanism
    called **Least Recently Used Access** (**LRUA**) as a replacement for the combined
    content/location NTM addressing mechanism. The LRUA write operation writes to
    either the least-used memory location or the most recently used one. There are
    two reasons for implementing this: to preserve recently stored information by
    writing new memories to the most rarely-used locations, and by writing new data
    to the last used location, the new information serves as a kind of update to the
    previously written state. But how does the model know which of the two options
    to use? The MANN* addressing mechanism interpolates between the two options by
    introducing a vector of usage weights ![](img/942f9145-7af4-4021-91b2-b8273dc3dbc9.png).
    These weights are updated at each time step by adding the usage weights ![](img/c3d2a1a3-ef5d-4dbe-a8b7-730b11c08666.png) at
    step *t-1* with the current read and write attention weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2cbd95cb-891b-4356-8f4d-3d5e2c75332b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the scalar γ is a decay parameter, which determines the balance between
    the two components of the equation. MANN* also introduces the least recently used
    weights vector ![](img/d164f3e8-dc60-4160-a78e-948c3fa5e6d3.png), where each element
    of the vector is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56e10e46-4b66-48f6-b857-36a016d72d29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, ![](img/fd4e3e1b-12e6-4f16-b340-f7cce5feb910.png) is the *n*th smallest
    element of the vector ![](img/069a6770-86b9-4e68-b663-c171e9c6bf2a.png) and *n*
    is equal to the number of memory reads. At this point, we can compute the write
    weights, which are an interpolation between the read weights and the least-recently
    used weights at step *t-1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e93623e-c9b1-448f-a4ef-dc45cd8afea3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, σ is the sigmoid function and α is a learnable scalar parameter, which
    indicates how to balance between the 2 input weights. Now, we can write new data
    to the memory, which is done in 2 steps: the first is for computing the least
    recently used location using the weights ![](img/c3d2a1a3-ef5d-4dbe-a8b7-730b11c08666.png).
    The second step is the actual writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96e22cde-65e1-45cc-92c2-bde6ce8ebecc.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/a9d608c2-aecf-41f2-9de9-0bdffed8b1aa.png) is the key vector we
    defined when we discussed NTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The MANN* paper goes into a bit more detail (compared to the original NTM paper)
    about the way the controller interacts with the input data and the read/write
    heads. The authors of the paper noted that their best performing models use LSTM (see
    [Chapter 7](379a4f7b-48da-40f2-99d6-ee57a7a5dcca.xhtml), *Understanding Recurrent*
    *Networks*) controllers. So the following is how the LSTM controller plugs into
    the MANN* system:'
  prefs: []
  type: TYPE_NORMAL
- en: The controller inputs at step *t* are the concatenated vectors ![](img/70297af4-5d9d-42f7-978d-154a789d8906.png),
    where ![](img/29facb26-b5e5-4791-b7d2-f46ea48f2edc.png) is the input data and ![](img/00d06425-4270-48fc-a1f8-38ad093018f3.png) is
    the system's output at step *t-1*. In classification tasks, the outputs ![](img/45e28d30-ccfd-4c93-9b00-134ae9ad11e1.png) are
    one-hot encoded class representations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The controller outputs at step *t* are the concatenated ![](img/0d1c22f1-5e92-42ed-a861-30e93b59c749.png),
    where ![](img/da198e0a-707e-4d3a-9631-8823a3723322.png) is the LSTM cell hidden
    state and ![](img/109f037d-5344-433d-a0b5-8e60125ed09c.png) is a result of the
    read operation. For classification tasks, we can use ![](img/2672393f-2e20-4718-8c23-0fb701375f61.png) as
    an input for a fully connected layer with softmax output, resulting in the expression ![](img/bc440539-668c-4312-b516-773202d903cc.png),
    where ![](img/0518e846-b423-4882-9564-61c3c2dca8c8.png) is the fully connected
    layer weights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key vector ![](img/36e80a51-1d5c-4419-b724-02754ea8fd03.png), which serves as
    a base for the attention weights of the read/write operations, is the LSTM cell
    state ![](img/71582670-adc7-4831-a821-11d6edc829bd.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our discussion of MANNs and, indeed, the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered two categories of emerging NN models—GNNs and MANNs.
    We started with a short introduction to graphs and then we looked at several different
    types of GNN, including GraphNN, graph convolutional networks, graph attention
    networks, and graph autoencoders. We concluded the graph section by looking at
    the NGL and we implemented an NGL example using the TensorFlow-based NSL framework.
    Then we focused on memory-augmented networks, where we looked at the NTM and MANN*
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll look at the emerging field of meta learning, which
    involves making ML algorithms learn to learn.
  prefs: []
  type: TYPE_NORMAL
