<html><head></head><body>
  <div id="_idContainer206">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-158" class="chapterTitle">Building Conversational AI Applications with Deep Learning</h1>
    <p class="normal">The art of conversation is considered a uniquely human trait. The ability of machines to have a dialog with humans has been a research topic for many years. Alan Turing proposed the now-famous Turing Test to see if a human could converse with another human and a machine through written messages, and identify each participant as machine or human correctly. In recent times, digital assistants such as Alexa by Amazon and Siri by Apple have made considerable strides in conversational AI. This chapter discusses different conversational agents and puts the techniques learned in the previous chapters into context. While there are several approaches to building conversational agents, we'll focus on the more recent deep learning approaches and cover the following topics:</p>
    <ul>
      <li class="bullet">Overview of conversational agents and their general architecture </li>
      <li class="bullet">An end-to-end pipeline for building a conversational agent</li>
      <li class="bullet">The architecture of different types of conversational agents, such as<ul>
          <li class="bullet-l2">Question-answering bots</li>
          <li class="bullet-l2">Slot-filling or task-oriented bots</li>
          <li class="bullet-l2">General conversation bots</li>
        </ul>
      </li>
    </ul>
    <p class="normal">We'll start with an overview of the general architecture of conversational agents.</p>
    <h1 id="_idParaDest-159" class="title">Overview of conversational agents</h1>
    <p class="normal">A <a id="_idIndexMarker674"/>conversational agent interacts with people using speech or text. Facebook Messenger would be an example of a text-based agent while Alexa and Siri are examples of agents that interact through speech. In either case, the agent needs to understand the user's intent and respond accordingly. Hence, the core part of the agent would be a <strong class="keyword">natural language understanding</strong> (<strong class="keyword">NLU</strong>) module. This module would interface with a <strong class="keyword">natural</strong> <strong class="keyword"><a id="_idIndexMarker675"/></strong><strong class="keyword">language generation </strong>(<strong class="keyword">NLG</strong>) module to supply a response back to the user. Voice agents differ from text-based agents in having an additional module that converts voice to text and vice versa. We can imagine the system having the following logical structure for a voice-activated agent:</p>
    <figure class="mediaobject"><img src="image/B16252_09_01.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.1: Conceptual architecture of a conversational AI system</p>
    <p class="normal">The main difference between a speech-based system and a text-based system is how the users communicate with the system. All the other parts to the right of the Speech Recognition and Generation section shown in <em class="italic">Figure 9.1</em> above are identical in both types of conversational AI systems.</p>
    <p class="normal">The user communicates with the agent using speech. The agent first converts speech to text. Many advancements have been made in the past few years in this area, and it is generally considered a solved problem for major languages like English.</p>
    <p class="normal">English is spoken in many countries across the globe, resulting in many different pronunciations and dialects. Consequently, companies like Apple develop various models for different accents, such as British English, Indian English, and Australian English. <em class="italic">Figure 9.2</em> below shows some English and French accents from the Siri control panel on an iPhone 11 running iOS 13.6. French, German, and some other languages also have multiple variants. Another way to do this could be by putting an accent and language classification model as the first step and then processing the input through the appropriate speech recognition model:</p>
    <figure class="mediaobject"><img src="image/B16252_09_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.2: Language variants in Siri for speech recognition</p>
    <p class="normal">For virtual assistants, there are specific <a id="_idIndexMarker676"/>models for wake word detection. The model's objective is to start the bot once it detects a wake word or phrase such as "OK Google." The wake word triggers the bot to listen to the utterances until the conversation is completed. Once the user's speech has been converted into words, it is easy to apply to various NLP techniques that we have seen in multiple chapters in this book. The breakdown of the elements shown inside the NLP box in <em class="italic">Figure 9.1</em> can be considered conceptual. Depending on the system and the task, these components may be different models or one end-to-end model. However, it is useful to think of the logical breakdown, as shown in the figure.</p>
    <p class="normal">Understanding the user's commands and the intent is a crucial part. Intent identification is essential for general-purpose systems like Amazon's Alexa or Apple's Siri, which serve multiple purposes. Specific dialogue management systems may be invoked based on the intent identified. The dialog management may invoke APIs provided by a fulfillment system. In a banking bot, the command may be to get the latest balances, and the fulfillment may be a banking system that retrieves the latest balance. The dialogue manager would process the balance and use an NLG system to convert the balance into a proper sentence. Note that some of these systems are built on rules-based systems and others use end-to-end deep learning. A question-answering system is an example of an end-to-end deep learning system where dialog management, and NLU are a single unit.</p>
    <p class="normal">There are different types of <a id="_idIndexMarker677"/>conversational AI applications. The most common ones are:</p>
    <ul>
      <li class="bullet">Task-oriented or slot-filling systems</li>
      <li class="bullet">Question-answering</li>
      <li class="bullet">Machine reading comprehension</li>
      <li class="bullet">Social or chit-chat bots</li>
    </ul>
    <p class="normal">Each of these types is described in the following sections.</p>
    <h2 id="_idParaDest-160" class="title">Task-oriented or slot-filling systems</h2>
    <p class="normal">Task-oriented systems<a id="_idIndexMarker678"/> are purpose-built to satisfy a specific task. Some examples of tasks are ordering a pizza, getting the latest balance of a bank account, calling a person, sending a text message, turning a light on, and so on. Most of the capabilities exposed by virtual assistants can be classified into this category. Once the user's intent has been identified, control is transferred to the model managing a specific intent to gather all the information to perform the task and manage the dialog with the user. NER and POS detection models form a crucial part of such systems. Imagine that the user needs to fill a form with some information, and the bot interacts with the user to find the required information to fulfill the task. Let's take the example of ordering a pizza. The table below shows a simplified example of the choices in this process:</p>
    <table id="table001-6">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Size</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Crust</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Toppings</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Delivery</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Quantity</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Small</p>
            <p class="Table-Column-Content--PACKT-">Medium</p>
            <p class="Table-Column-Content--PACKT-">Large</p>
            <p class="Table-Column-Content--PACKT-">XL</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Thin</p>
            <p class="Table-Column-Content--PACKT-">Regular</p>
            <p class="Table-Column-Content--PACKT-">Deep dish</p>
            <p class="Table-Column-Content--PACKT-">Gluten-free</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Cheese</p>
            <p class="Table-Column-Content--PACKT-">Jalapeno </p>
            <p class="Table-Column-Content--PACKT-">Pineapple</p>
            <p class="Table-Column-Content--PACKT-">Pepperoni</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Take-out</p>
            <p class="Table-Column-Content--PACKT-">Delivery</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">1</p>
            <p class="Table-Column-Content--PACKT-">2</p>
            <p class="Table-Column-Content--PACKT-">â€¦</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Here is a made-up example<a id="_idIndexMarker679"/> of a conversation with a bot:</p>
    <figure class="mediaobject"><img src="image/B16252_09_03.png" alt="Graphical user interface, text, application, chat or text message  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.3: A possible pizza-ordering bot conversation</p>
    <p class="normal">The bot tracks the information <a id="_idIndexMarker680"/>needed and keeps marking the information it has received from the person as the conversation progresses. Once the bot has all the information needed to complete the task, it can execute the task. Note that some steps, such as confirming the order or the customer asking for options for toppings, have been excluded for brevity.</p>
    <p class="normal">In today's world, solutions like <a id="_idIndexMarker681"/>Dialogflow, part of Google Cloud, and LUIS, part of Azure, simplify building such conversational agents to just the configuration. Let's see how a simple bot that implements a portion of the pizza-ordering task above can <a id="_idIndexMarker682"/>be implemented with Dialogflow. Note that this example has been kept small to simplify configuration and use the free tier of Dialogflow. The first step is to navigate to <a href="https://cloud.google.com/dialogflow"><span class="url">https://cloud.google.com/dialogflow</span></a>, which is the home page for this service. There are two version of Dialogflow â€“ Essentials or ES, and CX. CX is the advanced version with a lot more features and controls. Essentials is a simplified version with a free tier that is perfect for a bot's trial build. Scroll down on the page so that you can see the Dialogflow Essentials section and click on the <strong class="scree Text">Go to console</strong> link, as shown in <em class="italic">Figure 9.4</em> below:</p>
    <figure class="mediaobject"><img src="image/B16252_09_04.png" alt="A picture containing graphical user interface  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.4: Dialogflow console access</p>
    <p class="normal">Clicking on the console may <a id="_idIndexMarker683"/>require the authorization of the service, and you may need to<a id="_idIndexMarker684"/> log in with your Google Cloud account. Alternatively, you may navigate to <a href="http://dialogflow.cloud.google.com/#/agents"><span class="url">dialogflow.cloud.google.com/#/agents</span></a> to see a list of configured agents. This screen is shown in <em class="italic">Figure 9.5</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_09_05.png" alt="Graphical user interface, application, Teams  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.5: Agents configuration in Dialogflow</p>
    <p class="normal">A new agent can be created<a id="_idIndexMarker685"/> by clicking on the blue <strong class="scree Text">CREATE AGENT</strong> button on the top right. If you see a different interface, please check that you are using Dialogflow Essentials. You can also use this URL to get to the agents section: <a href="https://dialogflow.cloud.google.com/#/agents"><span class="url">https://dialogflow.cloud.google.com/#/agents</span></a>. This brings up the new agent configuration screen, shown in <em class="italic">Figure 9.6</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_09_06.png" alt="Graphical user interface, text, application, Teams  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.6: Creating a new agent</p>
    <p class="normal">Please note that this is not a<a id="_idIndexMarker686"/> comprehensive tutorial of Dialogflow, so we will be using several default values to illustrate the concept of building slot-filling bots. Hitting <strong class="scree Text">CREATE</strong> will build a new bot and load a screen, as shown in <em class="italic">Figure 9.7</em>. The main part of building the bot is to define intent. The main intent of our bot is to order pizza. Before we create an intent, we will configure a few entities:</p>
    <figure class="mediaobject"><img src="image/B16252_09_07.png" alt="Graphical user interface, text, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.7: A barebones agent ready for configuration</p>
    <p class="normal">These entities are the slots that the bot will fill out in conversation with the user. In this case, we will define two entities â€“ the crust of the pizza and the size of the pizza. Click on the <strong class="scree Text">+</strong> sign next to Entities on the left in the previous screenshot, and you'll see the following screen:</p>
    <figure class="mediaobject"><img src="image/B16252_09_08.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.8: Configuring options for the crust entity in Dialogflow</p>
    <p class="normal">The values on the left <a id="_idIndexMarker687"/>represent the values for the crust entity, and the multiple options or synonyms on the right are the terms the user can input or speak corresponding to each choice. We will configure four options corresponding to the table above. Another entity will be created for the size of the pizza. The configured entity looks like <em class="italic">Figure 9.9</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_09_09.png" alt="Graphical user interface, text, application, email  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.9: Configuration of the size entity</p>
    <p class="normal">Now we are ready to build the intent. Click <a id="_idIndexMarker688"/>on the <strong class="scree Text">+</strong> sign next to the <strong class="scree Text">Intents</strong> section on the left navigation bar. We will name this intent <code class="Code-In-Text--PACKT-">order</code>, as this intent will get the options for crust and size from the user. First, we need to specify a set of training phrases that will trigger this intent. Some examples of such training phrases can be "I would like to order pizza" or "Can I get a pizza?". <em class="italic">Figure 9.10</em> shows some of the configured training phrases for the intent:</p>
    <figure class="mediaobject"><img src="image/B16252_09_10.png" alt="Graphical user interface, text, application, email  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.10: Training phrases that trigger the ordering intent</p>
    <p class="normal">There is a lot of hidden <a id="_idIndexMarker689"/>machine learning and deep learning happening in this picture, simplified by Dialogflow. For example, the platform can process text input as well as speech. These training examples are indicative, and the actual phrasing does not need to match any of these expressions directly.</p>
    <p class="normal">The next step is to define the parameters we need from the user. We add an action with two parameters â€“ size and crust. Note that the <strong class="scree Text">ENTITY</strong> column links the parameter with the defined entities and their values. The <strong class="scree Text">VALUE</strong> column defines a variable name that can be used in future dialogue or for integration with APIs:</p>
    <figure class="mediaobject"><img src="image/B16252_09_11.png" alt="Graphical user interface, application, table  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.11: Required parameters for the order intent</p>
    <p class="normal">For each parameter, we <a id="_idIndexMarker690"/>need to specify some prompts that the agent will use to ask the user for the information. <em class="italic">Figure 9.12</em> below shows some example prompts for the size parameter. You may choose to configure your phrasings for the prompts:</p>
    <figure class="mediaobject"><img src="image/B16252_09_12.png" alt="Graphical user interface, text, application, email  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.12: Prompt options for the size parameter</p>
    <p class="normal">The last step in configuring the intent is configuring a response once the information is collected. This configuration is done in the <strong class="scree Text">Responses</strong> section and is shown in <em class="italic">Figure 9.13</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_09_13.png" alt="Graphical user interface, application, Teams  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 9.13: Response configuration for the order intent</p>
    <p class="normal">Note the use of <code class="Code-In-Text--PACKT-">$size.original</code> and <code class="Code-In-Text--PACKT-">$crust.original</code> in the response text. It uses the original terms used by the <a id="_idIndexMarker691"/>user while ordering when it repeats the order back. Finally, note that we set this intent as the end of the conversation as we have obtained all the information we needed to get. Our bot is ready to be trained and tested. Hit the blue <strong class="scree Text">Save</strong> button at the top of the page after you have configured the training phrases, action and parameters, and the responses. There is another section at the bottom called fulfilment. This allows connecting the intent with a web service to complete the intent. The bot can be tested using the right side. Note that though we configured only text, Dialogflow enables both text and voice interfaces. While we demonstrate the text interface here, you are encouraged to try the voice interface as well:</p>
    <figure class="mediaobject"><img src="image/B16252_09_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.14: An example of dialog showing the response processing and the variable being set</p>
    <p class="normal">Cloud-based solutions have made it quite easy to build task-oriented conversational agents for general uses. However, building an agent for a specialized domain like medical uses may require custom builds. Let's look at options for specific parts of such a system:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Intent identification</strong>: The <a id="_idIndexMarker692"/>simplest way to identify intent is to treat it as a classification problem. Given an utterance or input text, the model needs to classify it into several intents. Standard RNN-based architectures, like those seen in earlier chapters, can be used and adapted for this task.</li>
      <li class="bullet"><strong class="keyword">Slot tagging</strong>: Tagging slots <a id="_idIndexMarker693"/>used in a sentence to correspond to inputs can be treated as a sequence classification problem. This is similar to the approach used in the second chapter, where named entities were tagged in a sequence of text. Bi-directional RNN models are quite effective in this part.</li>
    </ul>
    <p class="normal">Different models can be developed for these parts, or they can be combined in one end-to-end model with a dialog manager. Dialog state tracking systems can be built by using a set of rules generated by experts or by using CRFs (see <em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>, for a detailed explanation). Recent approaches include a Neural Belief Tracker proposed by MrkÅ¡iÄ‡ et al. in 2017 in their paper titled <em class="italic">Neural Belief Tracker: Data-Driven Dialogue State Tracking</em>. This system takes three inputs:</p>
    <ol>
      <li class="numbered">The last system output</li>
      <li class="numbered">The last user utterance</li>
      <li class="numbered">A slot-value pair from the possible candidates for slots</li>
    </ol>
    <p class="normal">These three inputs are combined through the content model and semantic decoding model and fed to a binary decision (softmax) layer to produce a final output. Deep reinforcement learning is being used to optimize the dialog policy overall.</p>
    <p class="normal">In the NLG part, the most common approach is to define a set of templates <a id="_idIndexMarker694"/>that can be dynamically populated. This approach was shown in the preceding figure <em class="italic">Figure 9.13</em>. Neural methods, such as semantically controlled LSTM, as proposed by Wen et al. in their paper <em class="italic">Semantically Conditioned LSTM-based Natural Language Generation for Spoken Dialogue Systems</em> in 2015, are being actively researched.</p>
    <p class="normal">Now, let's move on to another interesting area of conversational agents â€“ question-answering and machine reading comprehension.</p>
    <h1 id="_idParaDest-161" class="title">Question-answering and MRC conversational agents</h1>
    <p class="normal">Bots can be trained to answer questions based on information contained in a <strong class="keyword">knowledge base</strong> (<strong class="keyword">KB</strong>). This setting<a id="_idIndexMarker695"/> is called the question-answering setting. Another related <a id="_idIndexMarker696"/>area is <strong class="keyword">machine reading comprehension</strong> or <strong class="keyword">MRC</strong>. In MRC, questions <a id="_idIndexMarker697"/>need to be answered with respect to a set of passages or documents provided with the query. Both of these areas are seeing a lot of startup activity and innovation. A very large number of business use cases can be enabled with both of these types of conversational agents. Passing the financial report to a bot and answering questions such as the increase in revenue given the financial report would be an example of MRC. Organizations have large digital caches of information, with new information pouring in every day. Building such agents empowers knowledge workers to process and parse large amounts of information quickly. Startups like Pryon are delivering conversational AI agents that merge, ingest, and adapt a myriad of structured and unstructured data into unified knowledge domains that end users can ask natural language questions as a way to discover information.</p>
    <p class="normal">KBs typically consist of subject-predicate-object triples. The subject and object are entities, while the predicate indicates a relationship between them. The KB can be represented as a knowledge graph, where objects and subjects are nodes connected by predicate edges. A big challenge is the maintenance of such knowledge bases and graphs in real life. Most deep NLP approaches are focused on determining whether a given subject-predicate-object triplet is true or not. The problem is reduced to a binary classification through this reformulation. There are several approaches, including the use of BERT models, which can solve the classification problem. The key here is to learn an embedding of the KB and then frame queries on top of this embedding. Dat Nguyen's survey paper, titled <em class="italic">A survey of embedding models of entities and relationships for knowledge graph completion</em>, provides an excellent overview of various topics for a deeper dive. We focus on MRC for the rest of this section now.</p>
    <p class="normal">MRC is a<a id="_idIndexMarker698"/> challenging task as the objective is to answer any set of questions about a given set of passages or documents. These passages are not known in advance and may be of variable length. The most common research dataset used for evaluating models is the <strong class="keyword">Stanford Question Answering Dataset</strong> or <strong class="keyword">SQuAD</strong>, as it is commonly called. The dataset has 100,000 <a id="_idIndexMarker699"/>questions for different Wikipedia articles. The objective of the model is to output the span of text from the article that answers the question. A more challenging dataset has been published by Microsoft based on Bing queries. This dataset is called the <strong class="keyword">MAchine Reading COmprehension</strong> or <strong class="keyword">MARCO</strong> dataset. This dataset has over 1 million anonymized questions, with over <a id="_idIndexMarker700"/>8.8 million passages extracted from over 3.5 million documents. Some of the questions in this dataset may not be answerable based on the passages, which is not the case with the SQuAD dataset, which makes this a challenging dataset. The second challenging aspect of MARCO as compared to SQuAD is that MARCO requires the generation of an answer by combining information from multiple passages, whereas SQuAD requires marking the span from the given passage.</p>
    <p class="normal">BERT and its variants such as <em class="italic">ALBERT: A Lite BERT for Self-supervised Learning of Language Representations</em> published at ICLR 2020 form the basis of most competitive baselines today. BERT architecture is well suited to this task as it allows passing in two pieces of input text separated by a [SEP] token. The BERT paper evaluated their language model on a number of tasks, including performance on the SQuAD task. Question tokens formed the first part of<a id="_idIndexMarker701"/> the pair, and the passage/document formed the second part of the pair. The output tokens corresponding to the second part, the passage, are scored to represent whether the token represents the start of the span or the end of the span. </p>
    <p class="normal">A high-level depiction of the architecture is shown in <em class="italic">Figure 9.15</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_09_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.15: BERT fine-tuning approach for SQuAD question answering</p>
    <p class="normal">A multi-modal aspect of question <a id="_idIndexMarker702"/>answering is Visual QA, which was briefly introduced in <em class="chapterRef">Chapter 7</em>, <em class="italic">Multi-modal Networks and Image Captioning with ResNets and Transformer</em>. Analogous architectures to the one proposed for image captioning, which can take images as well as text tokens, are used for solving this challenge.</p>
    <p class="normal">The setting for QA above is called single turn because the user presents a question with a passage from where the question needs to be answered. However, people have conversations with a back and forth dialog. Such a setting is called multi-turn dialog. A follow-up question may have context from a previous question or answer in the conversation. One of the challenges in a multi-turn dialog is coreference resolution. Consider the following dialog:</p>
    <p class="normal"><strong class="keyword">Person</strong>: Can you tell me the balance in my account #XYZ?</p>
    <p class="normal"><strong class="keyword">Bot</strong>: Your balance is $NNN.</p>
    <p class="normal"><strong class="keyword">Person</strong>: Can you transfer $MM to account #ABC from <em class="italic">that</em> account?</p>
    <p class="normal">"that" in the second instruction refers to account #XYZ, which was mentioned in the first question from the person. This is called coreference resolution. In a multi-turn conversation, resolving references could be quite complicated based on the distance between the references. Several strides have been made in this area with respect to general conversation bots, which we'll cover next.</p>
    <h1 id="_idParaDest-162" class="title">General conversational agents</h1>
    <p class="normal">Seq2seq models provide the best<a id="_idIndexMarker703"/> inspiration for learning multi-turn general conversations. A useful mental model is that of machine translation. Similar to the machine translation problem, the response to the previous question can be thought of as a translation of that input into a different language â€“ the response. Encoding more context into a conversation can be achieved by passing in a sliding window of the previous conversation turns instead of just the last question/statement. The term open-domain is often used to describe bots in this area as the domain of the conversation is not fixed. The bot should be able to discuss a wide variety of topics. There are several issues that are their own research topics.</p>
    <p class="normal">Lack of personality or blandness is one such problem. The dialog is very dry. As an example, we have seen the use of a temperature hyperparameter to adjust the predictability of the response in previous chapters. Conversational agents have a high propensity to generate "I don't know" responses due to a lack of specificity in the dialog. A variety of techniques, including GANs, can be used to address this. The <em class="italic">Personalizing Dialogue Agents</em> paper authored by Zhang et al. from Facebook outlines some of the approaches used to address this problem.</p>
    <p class="normal">Two recent examples that highlight the state of the art of writing human-like comments come from Google and Facebook. Google published a paper titled <em class="italic">Towards a Human-like Open-Domain Chatbot</em>, with a chatbot named Meena with over 2.6 billion parameters. The core model is a seq2seq model using an <strong class="keyword">Evolved Transformer</strong> (<strong class="keyword">ET</strong>) block for encoding and decoding. The model architecture has one ET block in the encoder and 13 ET <a id="_idIndexMarker704"/>block in the decoder. ET block was discovered through <strong class="keyword">neural architecture search</strong> (<strong class="keyword">NAS</strong>) on top of the Transformer architecture. A new human evaluation metric called <strong class="keyword">Sensibleness and Specificity Average</strong> (<strong class="keyword">SSA</strong>) was proposed in the paper. The current literature has a variety of different metrics being proposed for the evaluation of such open-domain chatbots with little standardization.</p>
    <p class="normal">Another example of an open-domain chatbot is described by Facebook on <a href="https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/"><span class="url">https://ai.facebook.com/blog/state-of-the-art-open-source-chatbot/</span></a>. This paper builds on several years of research and combines the work on personalization, empathy, and KBs into a blended model called BlenderBot. Similar to Google's research, different datasets and benchmarks are used to train this chatbot. The code for the bot has been shared on <a href="https://parl.ai/projects/recipes/"><span class="url">https://parl.ai/projects/recipes/</span></a>. ParlAI, by Facebook research, provides several models for chatbots through <a href="https://github.com/facebookresearch/ParlAI"><span class="url">https://github.com/facebookresearch/ParlAI</span></a>.</p>
    <p class="normal">This is a very hot area<a id="_idIndexMarker705"/> of active research with a lot of action happening in it. Comprehensive coverage of this topic would take a book of its own. Hopefully, you have learned many techniques in this book that can be combined to build amazing conversational agents. Let's wrap up.</p>
    <h1 id="_idParaDest-163" class="title">Summary</h1>
    <p class="normal">We discussed the various types of conversational agents, such as task-oriented, question-answering, machine reading comprehension, and general chit-chat bots. Building a conversational AI system is a very challenging task with many layers, and it is an area of active research and development. The material covered earlier in the book can also help in building various parts of chatbots.</p>
    <h1 id="_idParaDest-164" class="title">Epilogue</h1>
    <p class="normal">First, let me congratulate you on reaching the end of the book. I hope this book helped you get a grounding in advanced NLP models. The main challenge facing a book such as this is that it will likely be obsolete by the time it reaches the press. The key thing is that new developments are based on past developments; for example, the Evolved Transformer is based on the Transformer architecture. Knowledge of all the models presented in the book will give you a solid foundation and significantly cut down the amount of time you need to spend to understand a new development. A set of influential and important papers for each chapter have also been made available in the GitHub repository. I am excited to see what you will discover and build next!</p>
  </div>
</body></html>