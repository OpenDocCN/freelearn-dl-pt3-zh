["```\nimport tensorflow as tf\nimport numpy as np\nfrom tf_agents.environments import py_environment, tf_environment, tf_py_environment, utils, wrappers, suite_gym\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import trajectory,time_step as ts\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.networks import q_network\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.metrics import tf_metrics, py_metrics\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.utils import common\nfrom tf_agents.drivers import py_driver, dynamic_episode_driver\nfrom tf_agents.utils import common\nimport matplotlib.pyplot as plt \n```", "```\nclass GridWorldEnv(py_environment.PyEnvironment):\n# the _init_ contains the specifications for action and observation\n    def __init__(self):\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n        self._observation_spec = array_spec.BoundedArraySpec(\n            shape=(4,), dtype=np.int32, minimum=[0,0,0,0],                            maximum=[5,5,5,5], name='observation')\n        self._state=[0,0,5,5] #represent the (row, col, frow, fcol) of the player and the finish\n        self._episode_ended = False\n    def action_spec(self):\n        return self._action_spec\n    def observation_spec(self):\n        return self._observation_spec\n# once the same is over, we reset the state\n    def _reset(self):\n        self._state=[0,0,5,5]\n        self._episode_ended = False\n        return ts.restart(np.array(self._state, dtype=np.int32))\n# the _step function handles the state transition by applying an action to the current state to obtain a new one\n    def _step(self, action):\n        if self._episode_ended:\n            return self.reset()\n        self.move(action)\n        if self.game_over():\n            self._episode_ended = True\n        if self._episode_ended:\n            if self.game_over():\n                reward = 100\n            else:\n                reward = 0\n            return ts.termination(np.array(self._state, dtype=np.int32),             reward)\n        else:\n            return ts.transition(\n                np.array(self._state, dtype=np.int32), reward=0,                 discount=0.9)\n    def move(self, action):\n        row, col, frow, fcol = self._state[0],self._state[1],self._        state[2],self._state[3]\n        if action == 0: #down\n            if row - 1 >= 0:\n                self._state[0] -= 1\n        if action == 1: #up\n            if row + 1 < 6:\n                self._state[0] += 1\n        if action == 2: #left\n            if col - 1 >= 0:\n                self._state[1] -= 1\n        if action == 3: #right\n            if col + 1 < 6:\n                self._state[1] += 1\n    def game_over(self):\n        row, col, frow, fcol = self._state[0],self._state[1],self._        state[2],self._state[3]\n        return row==frow and col==fcol\ndef compute_avg_return(environment, policy, num_episodes=10):\n    total_return = 0.0\n    for _ in range(num_episodes):\n        time_step = environment.reset()\n        episode_return = 0.0\n        while not time_step.is_last():\n            action_step = policy.action(time_step)\n            time_step = environment.step(action_step.action)\n            episode_return += time_step.reward\n            total_return += episode_return\n    avg_return = total_return / num_episodes\n    return avg_return.numpy()[0]\ndef collect_step(environment, policy):\n    time_step = environment.current_time_step()\n    action_step = policy.action(time_step)\n    next_time_step = environment.step(action_step.action)\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n    # Add trajectory to the replay buffer\n    replay_buffer.add_batch(traj) \n```", "```\n# parameter settings\nnum_iterations = 10000  \ninitial_collect_steps = 1000  \ncollect_steps_per_iteration = 1  \nreplay_buffer_capacity = 100000  \nfc_layer_params = (100,)\nbatch_size = 128 # \nlearning_rate = 1e-5  \nlog_interval = 200  \nnum_eval_episodes = 2  \neval_interval = 1000 \n```", "```\ntrain_py_env = wrappers.TimeLimit(GridWorldEnv(), duration=100)\neval_py_env = wrappers.TimeLimit(GridWorldEnv(), duration=100)\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env) \n```", "```\nq_net = q_network.QNetwork(\n        train_env.observation_spec(),\n        train_env.action_spec(),\n        fc_layer_params=fc_layer_params)\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate) \n```", "```\ntrain_step_counter = tf.compat.v2.Variable(0) \n```", "```\ntf_agent = dqn_agent.DqnAgent(\n        train_env.time_step_spec(),\n        train_env.action_spec(),\n        q_network=q_net,\n        optimizer=optimizer,\n        td_errors_loss_fn = common.element_wise_squared_loss,\n        train_step_counter=train_step_counter)\ntf_agent.initialize()\neval_policy = tf_agent.policy\ncollect_policy = tf_agent.collect_policy \n```", "```\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n        data_spec = tf_agent.collect_data_spec,\n        batch_size = train_env.batch_size,\n        max_length = replay_buffer_capacity)\nprint(\"Batch Size: {}\".format(train_env.batch_size))\nreplay_observer = [replay_buffer.add_batch]\ntrain_metrics = [\n            tf_metrics.NumberOfEpisodes(),\n            tf_metrics.EnvironmentSteps(),\n            tf_metrics.AverageReturnMetric(),\n            tf_metrics.AverageEpisodeLengthMetric(),\n] \n```", "```\ndataset = replay_buffer.as_dataset(\n            num_parallel_calls=3,\n            sample_batch_size=batch_size,\n    num_steps=2).prefetch(3) \n```", "```\ndriver = dynamic_step_driver.DynamicStepDriver(\n            train_env,\n            collect_policy,\n            observers=replay_observer + train_metrics,\n    num_steps=1)\niterator = iter(dataset)\nprint(compute_avg_return(eval_env, tf_agent.policy, num_eval_episodes))\ntf_agent.train = common.function(tf_agent.train)\ntf_agent.train_step_counter.assign(0)\nfinal_time_step, policy_state = driver.run() \n```", "```\nepisode_len = []\nstep_len = []\nfor i in range(num_iterations):\n    final_time_step, _ = driver.run(final_time_step, policy_state)\n    experience, _ = next(iterator)\n    train_loss = tf_agent.train(experience=experience)\n    step = tf_agent.train_step_counter.numpy()\n    if step % log_interval == 0:\n        print('step = {0}: loss = {1}'.format(step, train_loss.loss))\n        episode_len.append(train_metrics[3].result().numpy())\n        step_len.append(step)\n        print('Average episode length: {}'.format(train_metrics[3].                                                  result().numpy()))\n    if step % eval_interval == 0:\n        avg_return = compute_avg_return(eval_env, tf_agent.policy,                                        num_eval_episodes)\n        print('step = {0}: Average Return = {1}'.format(step, avg_return)) \n```", "```\nstep = 200: loss = 0.27092617750167847 Average episode length: 96.5999984741211 step = 400: loss = 0.08925052732229233 Average episode length: 96.5999984741211 step = 600: loss = 0.04888586699962616 Average episode length: 96.5999984741211 step = 800: loss = 0.04527277499437332 Average episode length: 96.5999984741211 step = 1000: loss = 0.04451741278171539 Average episode length: 97.5999984741211 step = 1000: Average Return = 0.0 step = 1200: loss = 0.02019939199090004 Average episode length: 97.5999984741211 step = 1400: loss = 0.02462056837975979 Average episode length: 97.5999984741211 step = 1600: loss = 0.013112186454236507 Average episode length: 97.5999984741211 step = 1800: loss = 0.004257255233824253 Average episode length: 97.5999984741211 step = 2000: loss = 78.85380554199219 Average episode length: 100.0 step = 2000:\nAverage Return = 0.0 step = 2200: loss = 0.010012316517531872 Average episode length: 100.0 step = 2400: loss = 0.009675763547420502 Average episode length: 100.0 step = 2600: loss = 0.00445540901273489 Average episode length: 100.0 step = 2800: loss = 0.0006154756410978734 \n```", "```\nplt.plot(step_len, episode_len)\nplt.xlabel('Episodes')\nplt.ylabel('Average Episode Length (Steps)')\nplt.show() \n```", "```\n!sudo apt-get install -y xvfb ffmpeg\n!pip install gym\n!pip install 'imageio==2.4.0'\n!pip install PILLOW\n!pip install pyglet\n!pip install pyvirtualdisplay\n!pip install tf-agents\nfrom __future__ import absolute_import, division, print_function\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\nimport pyvirtualdisplay\nimport tensorflow as tf\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\ntf.compat.v1.enable_v2_behavior()\n# Set up a virtual display for rendering OpenAI gym environments.\ndisplay = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start() \n```", "```\nnum_iterations = 20000 \ninitial_collect_steps = 100  \ncollect_steps_per_iteration = 1  \nreplay_buffer_max_length = 100000  \n# parameters of the neural network underlying at the core of an agent\nbatch_size = 64  \nlearning_rate = 1e-3  \nlog_interval = 200  \nnum_eval_episodes = 10  \neval_interval = 1000 \n```", "```\ndef compute_avg_return(environment, policy, num_episodes=10):\n  total_return = 0.0\n  for _ in range(num_episodes):\n    time_step = environment.reset()\n    episode_return = 0.0\n    while not time_step.is_last():\n      action_step = policy.action(time_step)\n      time_step = environment.step(action_step.action)\n      episode_return += time_step.reward\n    total_return += episode_return\n  avg_return = total_return / num_episodes\n  return avg_return.numpy()[0] \n```", "```\ndef collect_step(environment, policy, buffer):\n  time_step = environment.current_time_step()\n  action_step = policy.action(time_step)\n  next_time_step = environment.step(action_step.action)\n  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n  # Add trajectory to the replay buffer\n  buffer.add_batch(traj)\ndef collect_data(env, policy, buffer, steps):\n  for _ in range(steps):\n    collect_step(env, policy, buffer) \n```", "```\ndef embed_mp4(filename):\n  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n  video = open(filename,'rb').read()\n  b64 = base64.b64encode(video)\n  tag = '''\n  <video width=\"640\" height=\"480\" controls>\n    <source src=\"img/mp4;base64,{0}\" type=\"video/mp4\">\n  Your browser does not support the video tag.\n  </video>'''.format(b64.decode())\n  return IPython.display.HTML(tag)\ndef create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n  filename = filename + \".mp4\"\n  with imageio.get_writer(filename, fps=fps) as video:\n    for _ in range(num_episodes):\n      time_step = eval_env.reset()\n      video.append_data(eval_py_env.render())\n      while not time_step.is_last():\n        action_step = policy.action(time_step)\n        time_step = eval_env.step(action_step.action)\n        video.append_data(eval_py_env.render())\n  return embed_mp4(filename) \n```", "```\nenv_name = 'CartPole-v0'\nenv = suite_gym.load(env_name)\nenv.reset() \n```", "```\ntrain_py_env = suite_gym.load(env_name)\neval_py_env = suite_gym.load(env_name)\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env) \n```", "```\nfc_layer_params = (100,)\nq_net = q_network.QNetwork(\n    train_env.observation_spec(),\n    train_env.action_spec(),\n    fc_layer_params=fc_layer_params)\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\ntrain_step_counter = tf.Variable(0) \n```", "```\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\nagent.initialize() \n```", "```\neval_policy = agent.policy\ncollect_policy = agent.collect_policy \n```", "```\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec()) \n```", "```\nexample_environment = tf_py_environment.TFPyEnvironment(\n    suite_gym.load('CartPole-v0'))\ntime_step = example_environment.reset() \n```", "```\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_max_length) \n```", "```\ncollect_data(train_env, random_policy, replay_buffer, initial_collect_steps) \n```", "```\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=3, \n    sample_batch_size=batch_size, \n    num_steps=2).prefetch(3)\niterator = iter(dataset) \n```", "```\nagent.train = common.function(agent.train)\n# Reset the train step\nagent.train_step_counter.assign(0)\n# Evaluate the agent's policy once before training.\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\nreturns = [avg_return]\nfor _ in range(num_iterations):\n  # Collect a few steps using collect_policy and save to the replay buffer.\n  collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n  # Sample a batch of data from the buffer and update the agent's network.\n  experience, unused_info = next(iterator)\n  train_loss = agent.train(experience).loss\n  step = agent.train_step_counter.numpy()\n  if step % log_interval == 0:\n    print('step = {0}: loss = {1}'.format(step, train_loss))\n  if step % eval_interval == 0:\n    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n    returns.append(avg_return) \n```", "```\nstep = 200: loss = 4.396056175231934\nstep = 400: loss = 7.12950325012207\nstep = 600: loss = 19.0213623046875\nstep = 800: loss = 45.954856872558594\nstep = 1000: loss = 35.900394439697266\nstep = 1000: Average Return = 21.399999618530273\nstep = 1200: loss = 60.97482681274414\nstep = 1400: loss = 8.678962707519531\nstep = 1600: loss = 13.465248107910156\nstep = 1800: loss = 42.33995056152344\nstep = 2000: loss = 42.936370849609375\nstep = 2000: Average Return = 21.799999237060547 \n```", "```\ncreate_policy_eval_video(random_policy, \"random-agent\") \n```", "```\ncreate_policy_eval_video(agent.policy, \"trained-agent\") \n```", "```\n!pip install tf-agents\nimport abc\nimport numpy as np\nimport tensorflow as tf\nfrom tf_agents.agents import tf_agent\nfrom tf_agents.drivers import driver\nfrom tf_agents.environments import py_environment\nfrom tf_agents.environments import tf_environment\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.policies import tf_policy\nfrom tf_agents.specs import array_spec\nfrom tf_agents.specs import tensor_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.trajectories import policy_step\ntf.compat.v1.reset_default_graph()\ntf.compat.v1.enable_resource_variables()\ntf.compat.v1.enable_v2_behavior()\nnest = tf.compat.v2.nest\nfrom tf_agents.bandits.agents import lin_ucb_agent\nfrom tf_agents.bandits.environments import stationary_stochastic_py_environment as sspe\nfrom tf_agents.bandits.metrics import tf_metrics\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nimport matplotlib.pyplot as plt \n```", "```\nbatch_size = 2\nnum_iterations = 100 \nsteps_per_loop = 1 \n```", "```\ndef context_sampling_fn(batch_size):\n  def _context_sampling_fn():\n    return np.random.randint(0, 2, [batch_size, 2]).astype(np.float32)\n  return _context_sampling_fn \n```", "```\nclass CalculateReward(object):\n\n    \"\"\"A class that acts as linear reward function when called.\"\"\"\n    def __init__(self, theta, sigma):\n        self.theta = theta\n        self.sigma = sigma\n    def __call__(self, x):\n        mu = np.dot(x, self.theta)\n        #return np.random.normal(mu, self.sigma)\n        return (mu > 0) + 0 \n```", "```\narm0_param = [2, -1]\narm1_param = [1, -1] \narm2_param = [-1, 1] \narm3_param = [ 0, 0] \narm0_reward_fn = CalculateReward(arm0_param, 1)\narm1_reward_fn = CalculateReward(arm1_param, 1)\narm2_reward_fn = CalculateReward(arm2_param, 1)\narm3_reward_fn = CalculateReward(arm3_param, 1) \n```", "```\ndef compute_optimal_reward(observation):\n    expected_reward_for_arms = [\n      tf.linalg.matvec(observation, tf.cast(arm0_param, dtype=tf.float32)),\n      tf.linalg.matvec(observation, tf.cast(arm1_param, dtype=tf.float32)),\n      tf.linalg.matvec(observation, tf.cast(arm2_param, dtype=tf.float32)),\n      tf.linalg.matvec(observation, tf.cast(arm3_param, dtype=tf.float32))\n    ]\n    optimal_action_reward = tf.reduce_max(expected_reward_for_arms, axis=0)\n\n    return optimal_action_reward \n```", "```\nenvironment = tf_py_environment.TFPyEnvironment(\n    sspe.StationaryStochasticPyEnvironment(\n        context_sampling_fn(batch_size),\n        [arm0_reward_fn, arm1_reward_fn, arm2_reward_fn, arm3_reward_fn],\n        batch_size=batch_size)) \n```", "```\nobservation_spec = tensor_spec.TensorSpec([2], tf.float32)\ntime_step_spec = ts.time_step_spec(observation_spec)\naction_spec = tensor_spec.BoundedTensorSpec(\n    dtype=tf.int32, shape=(), minimum=0, maximum=2)\nagent = lin_ucb_agent.LinearUCBAgent(time_step_spec=time_step_spec,\n                                     action_spec=action_spec) \n```", "```\nregret_metric = tf_metrics.RegretMetric(compute_optimal_reward) \n```", "```\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.policy.trajectory_spec,\n    batch_size=batch_size,\n    max_length=steps_per_loop)\nobservers = [replay_buffer.add_batch, regret_metric]\ndriver = dynamic_step_driver.DynamicStepDriver(\n    env=environment,\n    policy=agent.collect_policy,\n    num_steps=steps_per_loop * batch_size,\n    observers=observers)\nregret_values = []\nfor _ in range(num_iterations):\n    driver.run()\n    loss_info = agent.train(replay_buffer.gather_all())\n    replay_buffer.clear()\n    regret_values.append(regret_metric.result()) \n```", "```\nplt.plot(regret_values)\nplt.ylabel('Average Regret')\nplt.xlabel('Number of Iterations') \n```"]