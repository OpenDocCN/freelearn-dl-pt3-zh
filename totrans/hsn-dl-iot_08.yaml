- en: Physiological and Psychological State Detection in IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Human physiological and psychological states can provide very useful information
    about a person's activity and emotions. This information can be used in many application
    domains, including smart homes, smart cars, entertainment, education, rehabilitation
    and health support, sports, and industrial manufacturing, to improve existing
    services and/or offer new services. Many IoT applications incorporate sensors
    and processors for human pose estimation or activity and emotion recognition.
    However, the detection of activities or emotions based on the sensor data is a
    challenging task. In recent years, DL-based approaches have become a popular and
    effective way to address this challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter presents DL-based human physiological and psychological state
    detection techniques for IoT applications in general. The first part of this chapter
    will briefly describe different IoT applications and their physiological and psychological
    state detection-based decision making. In addition, it will briefly discuss two
    IoT applications and their physiological and psychological state detection-based
    implementations in a real-world scenario. In the second part of the chapter, we
    will present the DL-based implementations of the two IoT applications. In this
    chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: IoT-based human physiological and psychological state detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case one: IoT-based remote progress monitoring of physiotherapy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of IoT-based remote progress monitoring of physiotherapy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use case two: the smart classroom'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementation of the smart classroom
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning for human activity and emotion detection in IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM and CNNs and transfer learning for HAR/FER in IoT applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data preprocessing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation of the models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IoT-based human physiological and psychological state detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In recent years, human physiological and psychological state detection have
    been used in many application domains to improve existing services and/or offer
    new services. IoT, combined with DL techniques, can be used in applications to
    detect human physiological and psychological states. The following diagram highlights
    a few key applications of these detection approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78f2e175-fada-4f76-a8be-67400c6d4590.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, we will learn in detail about the two state detection variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Physiological state detection**: Physiological state or activity detection
    are useful tools in many applications, including assisted living for vulnerable
    people, such as the elderly, and in remote physical therapy/rehabilitation systems.
    In assisted living for elderly people, falls among older people are detrimental
    to the health of the victim because of the associated risk of physical injury.
    A fall can also have financial consequences, due to the medical costs and need
    for hospitalization. Moreover, falls can also reduce the person''s life expectancy,
    especially in the case of a **long-lie**. It is also worth noting that the medical
    expenses linked with falls are extremely high. For example, the yearly cost of
    falls in the US alone is expected to reach US $67 billion by 2020\. In this context,
    automated and remote fall detection using DL-supported IoT applications can address
    the challenge, thus improving the quality of life for elderly people and minimizing
    the associated costs. Another key area of human activity detection applications
    is the remote physical therapy monitoring system. This is the first use case for
    this chapter, and we will present an overview of it in the next section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Psychological state detection: **Facial expressions are good reflections
    of human psychological states, and they are important factors in human communication
    that help us to understand the intentions of humans. Generally, we can infer the
    emotional states of other people, such as joy, sadness, and anger, by analyzing
    their facial expressions and vocal tone. Forms of non-verbal communication make
    up two-thirds of all human interactions. Facial expressions, in their imparted
    emotional meaning, are one of the main non-verbal interpersonal communication
    channels. Hence, facial expression-based emotion detection can be useful in understanding
    people''s behavior. It can, therefore, help to improve existing services and/or
    new services, including personalized customer services. IoT applications, such
    as smart healthcare, smart education, and security and safety, can improve their
    services through DL-based emotion detection or sentiment analysis. For example,
    in a smart classroom, a teacher can analyze the students’ sentiment in real time
    or quasi real time to offer personalized and/or group-orientated teaching. This
    will improve their learning experience.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case one – remote progress monitoring of physiotherapy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Physical therapy is a big part of healthcare. There is a huge gap between the
    demand for physical therapy and our ability to deliver that therapy. Most countries
    in the world are still greatly dependent on the one-to-one patient-therapist interaction
    (which is the gold standard), but it is not a scalable solution and not cost-effective
    for either patients or healthcare providers. In addition, most existing therapies
    and their updates rely on average data instead of an individual’s unique data,
    and sometimes this data is qualitative (for example, *Yes, I did what you told
    me to do*) rather than quantitative. This is a challenge regarding effective therapy.
    Finally, many people—especially elderly people—are living with **multiple chronic
    conditions **(**MCC**), and these conditions are generally treated separately.
    This can result in suboptimal care or even cases where these conditions can conflict
    with each other. For example, in the case of a patient with diabetes and back
    pain: a diabetic carer may recommend walking, whereas a back pain carer may forbid
    it. In this context, IoT is already changing healthcare. It can address most of
    these challenges with the support of machine learning/deep learning and data analysis
    tools, and offer effective physiotherapy by providing real-time or quasi real-time
    information.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of use case one
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Progress monitoring is a key challenge in traditional therapy. An IoT-based
    therapy can solve the progress monitoring issue. The following diagram briefly
    presents how the IoT-based remote physiotherapy monitoring system will work:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bfb73806-d56e-4a24-9626-2f6359fab343.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One of the key components of this application is the activity monitoring of
    the subject (patient) that will help the therapist remotely observe how the patient is
    complying with the suggested therapy, and whether they are making progress. As
    shown in the preceding diagram, the IoT-based remote physiotherapy monitoring
    system consists of four main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensors and patient-side computing platform**: For this use case, we are
    considering two sensors: an accelerometer and a gyroscope. Both of them measure
    three-dimensional readings linked with the subject''s activities. For these sensors,
    we can use dedicated sensors or a smartphone''s sensors (these sensors are embedded
    within most smartphones). For the client-side computing platform, we can consider
    Raspberry Pi for the dedicated sensors and the smartphone (if we are using the
    smartphone sensors). The sensors need to be properly placed in order to measure
    signals correctly. The sensors can be used for continuous or event-wise (such
    as during exercise) monitoring of the subject’s activities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Care providers and therapists**: Care providers, such as hospitals with doctors
    and medical/healthcare databases, are connected through a cloud platform/HealthCloud.
    The main care provider for the therapy use case is a therapist, and hospitals/doctors
    will offer support to the therapist when required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DL-based human activity detection**: In this phase, the edge-computing device
    will be installed with an app. The installed app on the smartphone or Raspberry
    Pi will be loaded with a pretrained human activity detection and classification
    model. Once the accelerometer and gyroscope detect any signal, they send it to
    the smartphone or Raspberry Pi for processing and detection using the DL model,
    and finally inform the therapist for their feedback or intervention if necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HealthCloud for model learning**: The HealthCloud is a cloud computing platform
    mainly designed for healthcare-related services. This will train the selected
    DL model in human activity detection and classification using reference datasets.
    This learned model will be preinstalled in the smartphone or Raspberry Pi.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use case two — IoT-based smart classroom
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The higher education dropout rate is increasing worldwide. For example, dropout
    rates among UK university students have increased for the third consecutive year.
    Three of the top eight reasons for these dropouts are:'
  prefs: []
  type: TYPE_NORMAL
- en: Lack of quality time with teachers and counselors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demotivating school environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of student support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the key challenges in addressing these issues is knowing the students
    (such as knowing whether a student is following a topic or not) and delivering
    lectures/tutorials and other support accordingly. One potential approach is to
    know the emotions of the students, which is challenging in a large classroom,
    computer lab, or in e-learning environments. The use of technologies (including
    IoT with the support of DL models) can help to recognize emotion using facial
    expression and/or speech. The second use case of this chapter aims at increasing
    student performance in the classroom by detecting emotions and managing the lecture/lab
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation of use case two
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram shows a simplified implementation of an IoT-based smart
    classroom application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4e9c55a8-bd4c-4f57-a31c-fe7911ee824b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The facial-expression-based emotion analysis implementation consists of three
    main elements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensors and computing platform: **For this use case, we need at least one
    CCTV camera that can cover the classroom and be connected to the computing platform
    wirelessly or via a concealed cable in the walls. The lecturer''s computer in
    the classroom can work as the computing platform. The computer will continuously
    process the video signals and convert them into images for the image-based facial
    expression analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facial expression-based emotion detection**: The lecturer''s computer will
    be installed with an app. The installed app will be loaded with a pretrained facial
    expression-based detection and classification model. Once the DL model receives
    the facial images of the students, it identifies their emotions (such as happy/unhappy/confused)
    regarding a lecture and notifies the lecturer to take the necessary action.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Desktop or server for model learning:** The lecturer''s computer will be
    connected to a university server or cloud computing platform, and this will train/retrain
    the model for facial expression-based emotion recognition and classification using
    reference datasets. This learned model will be preinstalled in the lecturer''s
    PC in the classroom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of the following sections will describe the implementation of the DL-based
    human activity and emotion recognition needed for the aforementioned use cases. All
    of the necessary codes are available in the chapter's code folder.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning for human activity and emotion detection in IoT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is important to understand the working principle of an accelerometer- and
    gyroscope-based human activity detection system, and of a facial expression-based
    emotion detection system, before discussing the useful deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic human activity recognition system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Automatic **human activity recognition** (**HAR**) systems detect human activity,
    based on raw accelerometer and gyroscope signals. The following diagram shows
    a schematic of a DL-based HAR that consists of three different phases. They are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: IoT deployment or instrumentation of the subject or person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature extraction and model development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activity classification/identification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/e3327277-f93f-46f8-9279-7cebe6cdef39.png)'
  prefs: []
  type: TYPE_IMG
- en: Generally, classical HAR approaches mainly rely on heuristic hand-crafted feature
    extraction methods, which is a complex process and not well suited for resource-constrained
    IoT devices. More recent DL-based HAR approaches perform the feature extraction
    automatically, and they can work well on resource-constrained IoT devices. Most
    HAR approaches consider six different activities, including walking, running,
    sitting, standing, climbing upstairs, and coming downstairs. These activities
    exhibit differences in accelerometer and gyroscope signals, and a classifier exploits
    the differences to identify the current activity—which could form a part of physiotherapy
    (such as running).
  prefs: []
  type: TYPE_NORMAL
- en: Automated human emotion detection system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Automated **human emotion recognition** (**HER**) can be done by using either
    one of the following signals/inputs from the subject (human) or a combination
    thereof:'
  prefs: []
  type: TYPE_NORMAL
- en: Facial expression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Speech/audio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This chapter considers the **facial expression recognition** (**FER**)-based
    HER. A DL-based automated FER consists of three main steps: preprocessing, deep
    feature learning, and classification. The following diagram highlights these main
    steps in an FER-based HER.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image processing for facial expression analysis requires preprocessing, since
    the different types of emotions (such as anger, disgust, fear, happiness, sadness,
    surprise, and neutral) have subtle differences. Variations in input images that
    are irrelevant to FER, including different backgrounds, illuminations, and head
    poses, can be removed through preprocessing to improve the model prediction/classification
    accuracy. Face alignment, data augmentation, and image normalization are a few
    of the key preprocessing techniques. Most open source datasets for FER are not
    sufficient to generalize the FER approach. Data augmentation is essential in order to
    improve an existing dataset in terms of FER. Face alignment and image normalization
    are useful for improving individual images. The final stage of the FER DL pipeline
    is for the DL algorithm to learn and classify the features, hence emotions. Most
    image recognition DL algorithms, including CNN and RNN, are suitable for the final
    stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2569a659-4081-4324-a81a-9958f6c121e1.png)'
  prefs: []
  type: TYPE_IMG
- en: Deep learning models for HAR and emotion detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Generally, human activity recognition systems use accelerometer and gyroscope
    signals, which are time series data. Sometimes, the recognition process uses a
    combination of time series and spatial data. In this context, **Recurrent Neural
    Network** (**RNN**) and LSTM are potential candidates for the former type of signals
    because of their capability to incorporate temporal features of input during evolution.
    On the other hand, CNNs are good for spatial aspects of accelerometer and gyroscope
    signals. Hence, a combination or hybrid of CNNs and LSTMs/RNNs is ideal for the
    former type of signals. We will use an LSTM model for the HAR use case as it can address
    the temporal aspects of human activities.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike HAR systems, FER-based human emotion detection systems generally rely
    on facial expressions images, which rely on the local or spatial correlations
    between the pixel values of the images. Any DL model that works well for image
    recognition is fine for an FER task and, equally, for emotion detection. A number
    of deep learning algorithms or models have been used for image recognition, and
    the **deep belief network** (**DBN**) and CNNs are the top two candidates. In
    this chapter, we are considering CNNs because of their performance in image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: LSTM, CNNs, and transfer learning for HAR/FER in IoT applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LSTM is the widely used DL model for HAR—including in IoT-based HAR—because
    its memory capacity can deal better with time series data (such as HAR data) than
    other models, including CNN. The LSTM implementation of HAR can support transfer
    learning and is suitable for resource-constrained IoT devices. Generally, FER
    relies on image processing, and the CNN is the best model for image processing.
    Therefore, we implement use case two (FER) using a CNN model. In [Chapter 3](b28129e7-3bd1-4f83-acf7-4567e5198efb.xhtml), *Image
    Recognition in IoT*, we presented an overview of two popular implementations of
    the CNN (such as incentive V3 and Mobilenets) and their corresponding transfer
    learning. In the following paragraphs, we briefly present an overview of the baseline
    LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTM is an extension of RNNs. Many variants of LSTM are proposed, and they
    follow the baseline LSTM. The following is a schematic diagram of the baseline
    LSTM:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fee37e7b-6974-4ddf-bc1c-d04d9a769793.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding diagram, LSTM mainly consists of two components. They
    have a memory cell or neuron, and each cell or neuron has a multiplicative forget
    gate, read gate, and write gate. These gates control the access to memory cells/neurons
    and prevent them from being disturbed by irrelevant inputs. These gates are controlled
    through 0/1 or off/on. For example, if the forget gate is on/1, the neuron/cell
    writes its data to itself, and if the gate is off/0, the neuron forgets its last
    content. Other gates are controlled in a similar fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike RNNs, LSTMs use forget gates to actively control the cell/neuron states
    and ensure they do not degrade. Importantly, LSTM models perform better than RNN
    models in the case of data that has a long dependency in time. Many IoT applications,
    such as human activity recognition and disaster prediction based on environmental
    monitoring, exhibit this long-time dependency.
  prefs: []
  type: TYPE_NORMAL
- en: As the FER considered for use case two is based on image processing, CNNs are
    the best choice. CNN has different implementations, including a simple CNN, two
    versions of Mobilenets, and Incentive3\. Use case two will explore a simple CNN
    and Mobilenet V1 for the FER part of the implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Data collection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data collection for HAR and/FER is a challenging task for many reasons, including
    privacy. As a result, open source quality datasets are limited in number. For
    the HAR implementation in use case one, we are using a very popular and open source
    **Wireless Sensor Data Mining** (**WISDM**) lab dataset . The dataset consists
    of 54,901 samples collected from 36 different subjects. For privacy reasons, usernames
    are masked with ID numbers from 1-36\. The data was collected for six different
    activities undertaken by the subjects: standing, sitting, jogging, walking, going
    downstairs, and climbing upstairs. The dataset contains three-axis accelerometer
    data with more than 200 time steps for each sample. The following screenshot is
    a sample of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/96d959f8-1fb5-4f89-9718-7e82fb7f64bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For the FER-based emotion detection in use case two, we used two different
    datasets. The first one is the popular and open source FER2013 dataset. This dataset
    contains 48 x 48 pixel grayscale images of human faces. These images are preprocessed
    and ready to be used directly for training and validation. The images can be classified
    into seven categories (*0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise,*
    and *6=Neutral*). The dataset in CSV format contains information about pixel values
    of the face images rather than the images. The following screenshot shows a few
    values of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9bcdb674-6626-49b7-ae3d-0f0898470dd7.png)'
  prefs: []
  type: TYPE_IMG
- en: The split ratio between the training and the validation dataset is *80:20*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also prepared a dataset through Google search, particularly for Mobilenet
    V1\. The dataset is not a big one, as it consists of five classes of emotions,
    and each of those consists of more than 100 images. These images are not preprocessed.
    The following diagram shows a folder view of the prepared dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54928366-063a-44fa-ac2b-1d5051415c06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For data collection (each class of the dataset), we can follow a four step
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Search:** Use any browser (we used Chrome), go to Google, and search the
    appropriate word combination for the class/emotion (such as *angry human*) in
    Google images.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Image URL gatherings**: This step utilizes a few lines of JavaScript code
    to gather the image URLs. The gathered URLs can be used in Python to download
    the images. To do that, select the JavaScript console (assuming you will use the
    Chrome web browser, but you can use Firefox as well) by clicking the View | Developer
    | JavaScript console (in macOS) and customize and control**Google Chrome** | **More
    tools** | **Developer tools** (Windows OS). Once you have selected the JavaScript
    console, this will enable you to execute JavaScript in a REPL-like manner. Now,
    do the following in order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Scroll down the page until you have found all images relevant to your query.
    From there, you need to grab the URLs for the images.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Switch back to the JavaScript console and then copy and paste the following
    JavaScript snippet into the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet will pull down the jQuery JavaScript library, and
    now you can use a CSS selector to grab a list of URLs using the following snippet:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, write the URLs to a file (one per line) using the following snippet:'
  prefs:
  - PREF_OL
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Once you execute the preceding code snippet, you will have a file named `emotion_images_urls.txt` in
    your default download directory.
  prefs: []
  type: TYPE_NORMAL
- en: '**Downloading the images**: Now, you are ready to download the images running `download_images.py`
    (available in the code folder of the chapter) on the previously downloaded `emotion_images_urls.txt`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '**Exploration**:Once we have downloaded the images, we need to explore the
    images in order to delete the irrelevant ones. We can do this through a bit of
    manual inspection. After that, we need to resize and crop match our requirements.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data exploration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will examine in more detail the datasets that we will be
    using:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HAR dataset**:The dataset is a text file that consists of the different subjects
    accelerations for each of the six activities. We can do a data distribution check
    for the dataset as it is not easy to perceive the data distribution by looking
    at the text file only. The following graph summarizes the breakdown for the training
    set:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ^(![](img/6bccc879-56a3-4d13-8030-66e1b6500f92.png))
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see from the preceding graph, the training dataset consists of more
    walking and jogging data than the other four activities. This is good for the
    DL model, since walking and jogging are moving activities, where the range of acceleration
    data could be wide. To visualize this, we have explored activity-wise acceleration
    measurements/data for 200 time steps for each activity. The following screenshot represents
    200 time step acceleration measurements for sitting:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fb95debf-8537-4d23-a147-f39438dd5467.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot represents 200 time step acceleration measurements
    for standing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/de028247-dee1-43f9-9457-481c235ff5ed.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot represents 200 time step acceleration measurements
    for walking:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c2dedc6-73ea-4372-9891-cf093c734c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following screenshot represents 200 time step acceleration measurements
    for jogging:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/464b6e30-9e87-493c-b45e-e71b4af961b6.png)'
  prefs: []
  type: TYPE_IMG
- en: It is clear from the preceding diagrams that walking and jogging activities
    are busier than the other activities as they reflect the user's movements.
  prefs: []
  type: TYPE_NORMAL
- en: '**FER dataset**:We need to convert the FER2013 dataset pixel values of the
    face images into actual images to explore them. We can use the following code
    to convert the pixel values to images:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can execute the previous code using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the diagram, we can run the following code for image exploration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce a figure similar to the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bff0196b-d563-496a-ad52-b2a3ad85bdb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see in the preceding image, the FER dataset is preprocessed well.
    On the other hand, the second dataset (we named it FER2019) is not preprocessed,
    including the image sizes, as can be seen in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/738621cb-f6c4-4c56-941f-f477391992e9.png)'
  prefs: []
  type: TYPE_IMG
- en: Data preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data preprocessing is an essential step for a deep learning pipeline. The HAR
    and FER2013 datasets are preprocessed well. However, the downloaded image files
    for the second dataset of use case two are not preprocessed. As shown in the preceding
    image, the images are not uniform in size or pixels and the dataset is not large
    in size; hence, they require data augmentation. Popular augmentation techniques
    are flip, rotation, scale, crop, translation, and Gaussian noise. Many tools are
    available for each of these activities. You can use the tools or write their own
    script to do the data augmentation. A useful tool is **Augmentor**, a Python library
    for machine learning. We can install the tool in our Python and use it for augmentation.
    The following code (`data_augmentation.py`) is a simple data augmentation process
    that executes flipping, rotation, cropping, and resizing of the input images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following image presents two original images and their augmented samples
    (3 out of 25 samples):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/16b0faa7-a06b-4825-b765-cda89f838ecd.png)'
  prefs: []
  type: TYPE_IMG
- en: As shown in the preceding image, the augmented images are uniform in size, flipped,
    and rotated.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are two key issues to be noted during the training image set
    preparation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data size**: We need to collect at least 100 images for each class to train
    a model that works well. The more we can gather, the better the likely accuracy
    of the trained model. However, one-shot learning (an object categorization technique)
    can work using fewer than 100 training samples. We also made sure that the images
    are a good representation of what our application will actually face in real implementation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data heterogeneity***:* Data collected for training should be heterogeneous.
    For example, images for FER should be from a diverse range of skin tones or different
    views of the same expressions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we mentioned earlier, we are using LSTM for use case one and two implementations
    of CNN (simple CNN and Mobilenet V1) for use case two. All of these DL implementations
    support transfer learning for both use cases that do not require training from
    scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Use case one
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We consider a stacked LSTM, which is a popular DL model for sequence prediction,
    including time series problems. A stacked LSTM architecture consists of two or
    more LSTM layers. We implemented the HAR for use case one, using a two-layered
    stacked LSTM architecture. The following diagram presents a two-layered LSTM,
    where the first layer provides a sequence of outputs instead of a single value
    output to the second LSTM layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/06b140d6-c70c-442a-93af-5194e6d1018f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can train and test the model by running the `LSTM -HAR.py` code, available
    in the `use-case-1` folder (after making the necessary changes to your setup,
    such as the `data` directory):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Use case two
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We used two different architectures of CNN for the FER-based emotion detection
    in the smart classroom. The first one is a simple CNN architecture. To train the
    model on the FER2013 dataset, we need to run `CNN-FER2013.py`, which is available
    in the chapter''s `use-case-2` code folder, or use the notebook. To run in all
    default settings of `CNN-FER2013.py` *(*after making any necessary changes to
    your setup, such as the `data` directory)*,* we need to run the following in the
    Command Prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The training and testing of the model on the FER2103 dataset could take a few
    hours. The following diagram, generated from the TensorBoard log files, presents
    the network used for use case two:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2e49dc9c-ed05-4ac9-9e21-002a00a9b6e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can retrain Mobilenet V1 on FER2019 by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we run the preceding commands, they will generate the retrain models (`retrained_graph.pb`)
    and label text (`retrained_labels.txt` ) in the given directory. This will also
    store the model''s summary information in a directory. The summary information
    (the `--summaries_dir` argument with `retrain_logs` as the default value) can
    be used by the TensorBoard to visualize different aspects of the models, including
    the networks and their performance graphs. If we type the following command into
    the Terminal or command window, it will run the TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the TensorBoard is running, navigate your web browser to `localhost:6006` to
    view the TensorBoard and the network of the corresponding model. The following
    diagram presents a network for the Mobilenet V1 architecture used in the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d39947f7-a751-4270-9fbf-2630e6d6610b.png)'
  prefs: []
  type: TYPE_IMG
- en: Model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can evaluate the models in three different aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning/(re)training time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage requirement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance (accuracy)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of training time, in a desktop (Intel Xenon CPU E5-1650 v3@3.5 GHz
    and 32 GB RAM) with GPU support, LSTM on the HAR dataset, CNN on FER2013, and
    Mobilenet V1 on the FER2019 dataset, it took less than an hour to train/retrain
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The storage requirement of a model is an essential consideration in resource-constrained
    IoT devices. The following diagram presents the storage requirements for the three
    models we tested for the two use cases. As shown in the diagram, the simple CNN
    takes up only 2.6 MB, smaller than one sixth of the Mobilenet V1 (17.1 MB). Also,
    the LSTM for the HAR took up 1.6 MB (not in the diagram) of storage. In terms
    of storage requirements, all the models are fine to be deployed in many resource-constrained
    IoT devices, including Raspberry Pi or smartphones:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d70ca87d-aa69-4dec-8ae4-9e6c18547674.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we have evaluated the performance of the models. Two levels of performance
    evaluation can be executed for the use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset-wide evaluation or testing has been done during the retraining phase
    in the desktop PC platform/server side.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Individual activity signals for human activity and facial images for emotion
    detection were tested or evaluated in the Raspberry Pi 3 environment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model performance (use case one)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following graph presents the progressive training and test accuracy of
    the LSTM model against the HAR dataset. As we can see from the graph, training
    accuracy is close to 1.0, or 100%, and test accuracy is above .90, or 90%. With
    this test accuracy, we believe that the LSTM model can detect human activities
    in most cases, including whether the subject is doing the assigned physiotherapy
    activities:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12ffe603-6af5-4dc5-890b-bfa2f92e8b97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram is a confusion matrix of the model against the HAR test
    dataset. As seen in the diagram, the model gets confused between **Downstairs** and
    **Upstairs**, and **Sitting** and **Standing** activities, as they have very limited
    or zero mobility, which means there is no significant acceleration to differentiate
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ad58033-786b-41ef-8e67-55988260b906.png)'
  prefs: []
  type: TYPE_IMG
- en: Model performance (use case two)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following screenshot shows the training and validation performance of the
    simple CNN model on the FER2013 dataset. The accuracy of this dataset is not great
    (training–.83, and validation–.63), but the test or validation accuracy should
    be able to detect the distinctive and necessary emotions (such as happy, sad,
    and confused) for the smart classroom:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cee139c1-5676-4c9b-9bb3-34509c430bd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The following diagram is a confusion matrix of the model against the FER2013
    test dataset. As expected, the model is showing confusion for all the expressions
    (such as 156 angry expressions being detected as sad expressions). This is one
    of the applications of deep learning where further research is needed to improve
    performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35f1d314-5857-4ec4-a487-ac1da3c460f6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For use case two, we have tested Mobilenet V1\. The following diagrams shows
    the overall performance of model Mobilenet V1 on the FER2019 dataset. As we can
    see from the figure, this is showing better training accuracy, but no improvement
    in validation and test accuracy. One potential reason for this could be the size
    and quality of the data, since, after data augmentation, every sample may not
    contain a facial expression image. Further preprocessing that includes manual
    inspection may improve data quality and the model''s performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d2bf23a-5c49-470b-9ab9-a9120a05c56c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/53844e93-cfa2-40ec-bd76-5c109ec4c24d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to test the model on an individual image, and transfer the learning
    of the model, we need to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Export the trained model (such as `fer2013_trained.hdf5`) and the `label_image.py`
    file (image classifier) into a Raspberry Pi (installed with TensorFlow)/smartphone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Run the image classifier (do not forget to update the `test_image` path) using
    the following command:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This will produce the test result for your test image.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Automatic human physiological and psychological state detection is becoming
    a popular means by which people can learn a person's physical and mental state
    to interact and react accordingly. There are many applications within smart education,
    healthcare, and entertainment where these state detection techniques can be useful.
    Machine learning and DL algorithms are essential for these detection techniques.
    In the first part of this chapter, we briefly described different IoT applications
    using human physiological and psychological state detection. We also briefly discussed
    two potential use cases of IoT where DL algorithms can be useful in human physiological
    and psychological state detection. The first use case considers an IoT-based remote
    physiotherapy progress monitoring system. The second use case is an IoT-based
    smart classroom application that uses facial expressions of the students to know
    their feedback. In the second part of the chapter, we briefly discussed the data
    collection process for the use cases, and we discussed the rationale behind selecting
    LSTM for the HAR and CNNs for the FER. The remainder of the chapter described
    all of the necessary components of the DL pipeline for these models and their
    results.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key challenges in IoT applications is security. Many IoT applications,
    such as driverless cars, connected healthcare, and smart grid, are mission-critical
    applications. Security is an essential element for these and many other IoT applications.
    In the next chapter, we will discuss security in IoT applications, and show how
    deep learning can be used for IoT security solutions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'K. Rapp, C. Becker, I.D. Cameron, H.H. König, and G. Büchele, *Epidemiology
    of falls in residential aged care: analysis of more than 70,000 falls from residents
    of Bavarian nursing homes*, J. Am. Med. Dir. Assoc. 13 (2) (2012) 187.e1–187.e6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Centers for disease control and prevention. *Cost of Falls Among Older Adults*,
    2014\. [http://www.cdc.gov/homeandrecreationalsafety/falls/fallcost.html](http://www.cdc.gov/homeandrecreationalsafety/falls/fallcost.html)
    (accessed 14.04.19).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: M. S. Hossain and G. Muhammad, *Emotion-Aware Connected Healthcare Big Data
    Towards 5G*, in IEEE Internet of Things Journal, vol. 5, no. 4, pp. 2399-2406,
    Aug. 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'M. A. Razzaque, Muta Tah Hira, and Mukta Dira. 2017\. Q*oS in Body Area Networks:
    A Survey. ACM Trans*. Sen. Netw. 13, 3, Article 25 (August 2017), 46 pages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nigel Bosch, Sidney K. D'Mello, Ryan S. Baker, Jaclyn Ocumpaugh, Valerie Shute,
    Matthew Ventura, Lubin Wang, and Weinan Zhao. 2016\. Detecting student emotions
    in computer-enabled classrooms. In *Proceedings of the Twenty-Fifth International
    Joint Conference on Artificial Intelligence*(IJCAI'16), Gerhard Brewka (Ed.).
    AAAI Press 4125-4129.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Isabel Sagenmüller, *Student retention: 8 reasons people drop out of higher
    education*, [https://www.u-planner.com/en-us/blog/student-retention-8-reasons-people-drop-out-of-higher-education](https://www.u-planner.com/en-us/blog/student-retention-8-reasons-people-drop-out-of-higher-education).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nikki Bardsley, *Drop-out rates among university students increases for third
    consecutive year*, [https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year](https://www.fenews.co.uk/featured-article/24449-drop-out-rates-among-university-students-increases-for-third-consecutive-year).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S. Hochreiter and J. Schmidhuber, *Long Short-Term Memory*, neural computation,
    vol. 9, no. 8, pp. 1735–1780, 1997.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.cis.fordham.edu/wisdm/dataset.php](http://www.cis.fordham.edu/wisdm/dataset.php).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'I. Goodfellow, D. Erhan, PL Carrier, A. Courville, M. Mirza, B. Hamner, W.
    Cukierski, Y. Tang, DH Lee, Y. Zhou, C. Ramaiah, F. Feng, R. Li, X. Wang, D. Athanasakis,
    J. Shawe-Taylor, M. Milakov, J. Park, R. Ionescu, M. Popescu, C. Grozea, J. Bergstra,
    J. Xie, L. Romaszko, B. Xu, Z. Chuang, and Y. Bengio., *Challenges in Representation
    Learning: A report on three machine learning contests*. arXiv 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
