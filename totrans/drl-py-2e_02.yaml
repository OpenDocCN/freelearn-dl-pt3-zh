- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Guide to the Gym Toolkit
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: OpenAI is an **artificial intelligence** (**AI**) research organization that
    aims to build **artificial general intelligence** (**AGI**). OpenAI provides a
    famous toolkit called Gym for training a reinforcement learning agent.
  prefs: []
  type: TYPE_NORMAL
- en: Let's suppose we need to train our agent to drive a car. We need an environment
    to train the agent. Can we train our agent in the real-world environment to drive
    a car? No, because we have learned that reinforcement learning (RL) is a trial-and-error
    learning process, so while we train our agent, it will make a lot of mistakes
    during learning. For example, let's suppose our agent hits another vehicle, and
    it receives a negative reward. It will then learn that hitting other vehicles
    is not a good action and will try not to perform this action again. But we cannot
    train the RL agent in the real-world environment by hitting other vehicles, right?
    That is why we use simulators and train the RL agent in the simulated environments.
  prefs: []
  type: TYPE_NORMAL
- en: There are many toolkits that provide a simulated environment for training an
    RL agent. One such popular toolkit is Gym. Gym provides a variety of environments
    for training an RL agent ranging from classic control tasks to Atari game environments.
    We can train our RL agent to learn in these simulated environments using various
    RL algorithms. In this chapter, first, we will install Gym and then we will explore
    various Gym environments. We will also get hands-on with the concepts we have
    learned in the previous chapter by experimenting with the Gym environment.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the book, we will use the Gym toolkit for building and evaluating
    reinforcement learning algorithms, so in this chapter, we will make ourselves
    familiar with the Gym toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up our machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing Anaconda and Gym
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the Gym environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating an episode in the Gym environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring more Gym environments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cart-Pole balancing with the random agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An agent playing the Tennis game
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up our machine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn how to install several dependencies that are
    required for running the code used throughout the book. First, we will learn how
    to install Anaconda and then we will explore how to install Gym.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Anaconda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Anaconda is an open-source distribution of Python. It is widely used for scientific
    computing and processing large volumes of data. It provides an excellent package
    management environment, and it supports Windows, Mac, and Linux operating systems.
    Anaconda comes with Python installed, along with popular packages used for scientific
    computing such as NumPy, SciPy, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: To download Anaconda, visit [https://www.anaconda.com/download/](https://www.anaconda.com/download/),
    where you will see an option for downloading Anaconda for different platforms.
    If you are using Windows or macOS, you can directly download the graphical installer
    according to your machine architecture and install Anaconda using the graphical
    installer.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using Linux, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Terminal and type the following command to download Anaconda:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After downloading, we can install Anaconda using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the successful installation of Anaconda, we need to create a virtual environment.
    What is the need for a virtual environment? Say we are working on project A, which
    uses NumPy version 1.14, and project B, which uses NumPy version 1.13\. So, to
    work on project B we either downgrade NumPy or reinstall NumPy. In each project,
    we use different libraries with different versions that are not applicable to
    the other projects. Instead of downgrading or upgrading versions or reinstalling
    libraries every time for a new project, we use a virtual environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The virtual environment is just an isolated environment for a particular project
    so that each project can have its own dependencies and will not affect other projects.
    We will create a virtual environment using the following command and name our
    environment `universe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that we use Python version 3.6\. Once the virtual environment is created,
    we can activate it using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Now that we have learned how to install Anaconda and create a virtual
    environment, in the next section, we will learn how to install Gym.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Gym toolkit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to install the Gym toolkit. Before going
    ahead, first, let''s activate our virtual environment, `universe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, install the following dependencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can install Gym directly using `pip`. Note that throughout the book, we
    will use Gym version 0.15.4\. We can install Gym using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also install Gym by cloning the Gym repository as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Common error fixes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Just in case, if you get any of the following errors while installing Gym,
    the following commands will help:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Failed building wheel for pachi-py** or **failed building wheel for pachi-py
    atari-py**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Failed building wheel for mujoco-py**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**error: command ''gcc'' failed with exit status 1**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have successfully installed Gym, in the next section, let's kickstart
    our hands-on reinforcement learning journey.
  prefs: []
  type: TYPE_NORMAL
- en: Creating our first Gym environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned that Gym provides a variety of environments for training a reinforcement
    learning agent. To clearly understand how the Gym environment is designed, we
    will start with the basic Gym environment. After that, we will understand other
    complex Gym environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s introduce one of the simplest environments called the Frozen Lake environment.
    *Figure 2.1* shows the Frozen Lake environment. As we can observe, in the Frozen
    Lake environment, the goal of the agent is to start from the initial state **S**
    and reach the goal state **G**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: The Frozen Lake environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding environment, the following apply:'
  prefs: []
  type: TYPE_NORMAL
- en: '**S** denotes the starting state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F** denotes the frozen state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H** denotes the hole state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G** denotes the goal state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, the agent has to start from state **S** and reach the goal state **G**.
    But one issue is that if the agent visits state **H**, which is the hole state,
    then the agent will fall into the hole and die as shown in *Figure 2.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: The agent falls down a hole'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we need to make sure that the agent starts from **S** and reaches **G**
    without falling into the hole state **H** as shown in *Figure 2.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: The agent reaches the goal state'
  prefs: []
  type: TYPE_NORMAL
- en: Each grid box in the preceding environment is called a state, thus we have 16
    states (**S** to **G**) and we have 4 possible actions, which are *up*, *down*,
    *left*, and *right*. We learned that our goal is to reach the state **G** from
    **S** without visiting **H**. So, we assign +1 reward for the goal state **G**
    and 0 for all other states.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have learned how the Frozen Lake environment works. Now, to train our
    agent in the Frozen Lake environment, first, we need to create the environment
    by coding it from scratch in Python. But luckily we don't have to do that! Since
    Gym provides various environments, we can directly import the Gym toolkit and
    create a Frozen Lake environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will learn how to create our Frozen Lake environment using Gym. Before
    running any code, make sure that you have activated our virtual environment `universe`.
    First, let''s import the Gym library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can create a Gym environment using the `make` function. The `make`
    function requires the environment id as a parameter. In Gym, the id of the Frozen
    Lake environment is `FrozenLake-v0`. So, we can create our Frozen Lake environment
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the environment, we can see how our environment looks like using
    the `render` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code renders the following environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Gym''s Frozen Lake environment'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, the Frozen Lake environment consists of 16 states (**S**
    to **G**) as we learned. The state **S** is highlighted indicating that it is
    our current state, that is, the agent is in the state **S**. So whenever we create
    an environment, an agent will always begin from the initial state, which in our
    case is state **S**.
  prefs: []
  type: TYPE_NORMAL
- en: That's it! Creating the environment using Gym is that simple. In the next section,
    we will understand more about the Gym environment by relating all the concepts
    we have learned in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned that the reinforcement learning environment
    can be modeled as a **Markov decision process** (**MDP**) and an MDP consists
    of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**States**: A set of states present in the environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: A set of actions that the agent can perform in each state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transition probability**: The transition probability is denoted by ![](img/B15558_02_001.png).
    It implies the probability of moving from a state *s* to the state ![](img/B15558_02_002.png)
    while performing an action *a*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward function**: The reward function is denoted by ![](img/B15558_02_003.png).
    It implies the reward the agent obtains moving from a state *s* to the state ![](img/B15558_02_004.png)
    while performing an action *a*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now understand how to obtain all the above information from the Frozen
    Lake environment we just created using Gym.
  prefs: []
  type: TYPE_NORMAL
- en: States
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A state space consists of all of our states. We can obtain the number of states
    in our environment by just typing `env.observation_space` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'It implies that we have 16 discrete states in our state space starting from
    state **S** to **G**. Note that, in Gym, the states will be encoded as a number,
    so the state **S** will be encoded as 0, state **F** will be encoded as 1, and
    so on as *Figure 2.5* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Sixteen discrete states'
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We learned that the action space consists of all the possible actions in the
    environment. We can obtain the action space by using `env.action_space`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'It shows that we have `4` discrete actions in our action space, which are *left*,
    *down*, *right*, and *up*. Note that, similar to states, actions also will be
    encoded into numbers as shown in *Table 2.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.1: Four discrete actions'
  prefs: []
  type: TYPE_NORMAL
- en: Transition probability and reward function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's look at how to obtain the transition probability and the reward function.
    We learned that in the stochastic environment, we cannot say that by performing
    some action *a*, the agent will always reach the next state ![](img/B15558_02_004.png)
    exactly because there will be some randomness associated with the stochastic environment,
    and by performing an action *a* in the state *s*, the agent reaches the next state
    ![](img/B15558_02_004.png) with some probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we are in state 2 (**F**). Now, if we perform action 1 (*down*)
    in state 2, we can reach state 6 as shown in *Figure 2.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image62187.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: The agent performing a down action from state 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Frozen Lake environment is a stochastic environment. When our environment
    is stochastic, we won''t always reach state 6 by performing action 1 (*down*)
    in state 2; we also reach other states with some probability. So when we perform
    an action 1 (*down*) in state 2, we reach state 1 with probability 0.33333, we
    reach state 6 with probability 0.33333, and we reach state 3 with probability
    0.33333 as shown in *Figure 2.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Transition probability of the agent in state 2'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, in a stochastic environment we reach the next states with some
    probability. Now, let's learn how to obtain this transition probability using
    the Gym environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can obtain the transition probability and the reward function by just typing
    `env.P[state][action]`. So, to obtain the transition probability of moving from
    state **S** to the other states by performing the action *right*, we can type
    `env.P[S][right]`. But we cannot just type state **S** and action *right* directly
    since they are encoded as numbers. We learned that state **S** is encoded as 0
    and the action *right* is encoded as 2, so, to obtain the transition probability
    of state **S** by performing the action *right*, we type `env.P[0][2]` as the
    following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The above code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'What does this imply? Our output is in the form of `[(transition probability,
    next state, reward, Is terminal state?)]`. It implies that if we perform an action
    2 (*right*) in state 0 (**S**) then:'
  prefs: []
  type: TYPE_NORMAL
- en: We reach state 4 (**F**) with probability 0.33333 and receive 0 reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach state 1 (**F**) with probability 0.33333 and receive 0 reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach the same state 0 (**S**) with probability 0.33333 and receive 0 reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2.8* shows the transition probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Transition probability of the agent in state 0'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, when we type `env.P[state][action]`, we get the result in the form of
    `[(transition probability, next state, reward, Is terminal state?)]`. The last
    value is Boolean and tells us whether the next state is a terminal state. Since
    4, 1, and 0 are not terminal states, it is given as false.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of `env.P[0][2]` is shown in *Table 2.2* for more clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.2: Output of env.P[0][2]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this with one more example. Let''s suppose we are in state
    3 (**F**) as *Figure 2.9* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: The agent in state 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we perform action 1 (*down*) in state 3 (**F**). Then the transition probability
    of state 3 (**F**) by performing action 1 (*down*) can be obtained as the following
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'As we learned, our output is in the form of `[(transition probability, next
    state, reward, Is terminal state?)]`. It implies that if we perform action 1 (*down*)
    in state 3 (**F**) then:'
  prefs: []
  type: TYPE_NORMAL
- en: We reach state 2 (**F**) with probability 0.33333 and receive 0 reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach state 7 (**H**) with probability 0.33333 and receive 0 reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach the same state 3 (**F**) with probability 0.33333 and receive 0 reward.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2.10* shows the transition probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Transition probabilities of the agent in state 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of `env.P[3][1]` is shown in *Table 2.3* for more clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.3: Output of env.P[3][1]'
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, in the second row of our output, we have `(0.33333, 7, 0.0,
    True)`, and the last value here is marked as `True`. It implies that state 7 is
    a terminal state. That is, if we perform action 1 (*down*) in state 3 (**F**)
    then we reach state 7 (**H**) with 0.33333 probability, and since 7 (**H**) is
    a hole, the agent dies if it reaches state 7 (**H**). Thus 7(**H**) is a terminal
    state and so it is marked as `True`.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have learned how to obtain the state space, action space, transition
    probability, and the reward function using the Gym environment. In the next section,
    we will learn how to generate an episode.
  prefs: []
  type: TYPE_NORMAL
- en: Generating an episode in the Gym environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned that the agent-environment interaction starting from an initial state
    until the terminal state is called an episode. In this section, we will learn
    how to generate an episode in the Gym environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we initialize the state by resetting our environment; resetting
    puts our agent back to the initial state. We can reset our environment using the
    `reset()` function as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Action selection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order for the agent to interact with the environment, it has to perform
    some action in the environment. So, first, let''s learn how to perform an action
    in the Gym environment. Let''s suppose we are in state 3 (**F**) as *Figure 2.11*
    shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: The agent is in state 3 in the Frozen Lake environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we need to perform action 1 (*down*) and move to the new state 7 (**H**).
    How can we do that? We can perform an action using the `step` function. We just
    need to input our action as a parameter to the `step` function. So, we can perform
    action 1 (*down*) in state 3 (**F**) using the `step` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s render our environment using the `render` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in *Figure 2.12*, the agent performs action 1 (*down*) in state 3
    (**F**) and reaches the next state 7 (**H**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.12: The agent in state 7 in the Frozen Lake environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that whenever we make an action using `env.step()`, it outputs a tuple
    containing 4 values. So, when we take action 1 (*down*) in state 3 (**F**) using
    `env.step(1)`, it gives the output as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'As you might have guessed, it implies that when we perform action 1 (*down*)
    in state 3 (**F**):'
  prefs: []
  type: TYPE_NORMAL
- en: We reach the next state 7 (**H**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent receives the reward `0.0.`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the next state 7 (**H**) is a terminal state, it is marked as `True`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach the next state 7 (**H**) with a probability of 0.33333.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we can just store this information as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus:'
  prefs: []
  type: TYPE_NORMAL
- en: '`next_state` represents the next state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward` represents the obtained reward.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`done` implies whether our episode has ended. That is, if the next state is
    a terminal state, then our episode will end, so `done` will be marked as `True`
    else it will be marked as `False`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info`â€”Apart from the transition probability, in some cases, we also obtain
    other information saved as info, which is used for debugging purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also sample action from our action space and perform a random action
    to explore our environment. We can sample an action using the `sample` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have sampled an action from our action space, then we perform our
    sampled action using our step function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have learned how to select actions in the environment, let's see
    how to generate an episode.
  prefs: []
  type: TYPE_NORMAL
- en: Generating an episode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now let's learn how to generate an episode. The episode is the agent environment
    interaction starting from the initial state to the terminal state. The agent interacts
    with the environment by performing some action in each state. An episode ends
    if the agent reaches the terminal state. So, in the Frozen Lake environment, the
    episode will end if the agent reaches the terminal state, which is either the
    hole state (**H**) or goal state (**G**).
  prefs: []
  type: TYPE_NORMAL
- en: Let's understand how to generate an episode with the random policy. We learned
    that the random policy selects a random action in each state. So, we will generate
    an episode by taking random actions in each state. So for each time step in the
    episode, we take a random action in each state and our episode will end if the
    agent reaches the terminal state.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s set the number of time steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For each time step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomly select an action by sampling from the action space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the selected action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'If the next state is the terminal state, then break. This implies that our
    episode ends:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will print something similar to *Figure 2.13*. Note that
    you might get a different result each time you run the preceding code since the
    agent is taking a random action in each time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can observe from the following output, on each time step, the agent takes
    a random action in each state and our episode ends once the agent reaches the
    terminal state. As *Figure 2.13* shows, in time step 4, the agent reaches the
    terminal state **H,** and so the episode ends:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.13: Actions taken by the agent in each time step'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of generating one episode, we can also generate a series of episodes
    by taking some random action in each state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we can generate an episode by selecting a random action in each state
    by sampling from the action space. But wait! What is the use of this? Why do we
    even need to generate an episode?
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we learned that an agent can find the optimal policy
    (that is, the correct action in each state) by generating several episodes. But
    in the preceding example, we just took random actions in each state over all the
    episodes. How can the agent find the optimal policy? So, in the case of the Frozen
    Lake environment, how can the agent find the optimal policy that tells the agent
    to reach state **G** from state **S** without visiting the hole states **H**?
  prefs: []
  type: TYPE_NORMAL
- en: This is where we need a reinforcement learning algorithm. Reinforcement learning
    is all about finding the optimal policy, that is, the policy that tells us what
    action to perform in each state. We will learn how to find the optimal policy
    by generating a series of episodes using various reinforcement learning algorithms
    in the upcoming chapters. In this chapter, we will focus on getting acquainted
    with the Gym environment and various Gym functionalities as we will be using the
    Gym environment throughout the course of the book.
  prefs: []
  type: TYPE_NORMAL
- en: So far we have understood how the Gym environment works using the basic Frozen
    Lake environment, but Gym has so many other functionalities and also several interesting
    environments. In the next section, we will learn about the other Gym environments
    along with exploring the functionalities of Gym.
  prefs: []
  type: TYPE_NORMAL
- en: More Gym environments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore several interesting Gym environments, along
    with exploring different functionalities of Gym.
  prefs: []
  type: TYPE_NORMAL
- en: Classic control environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gym provides environments for several classic control tasks such as Cart-Pole
    balancing, swinging up an inverted pendulum, mountain car climbing, and so on.
    Let''s understand how to create a Gym environment for a Cart-Pole balancing task.
    The Cart-Pole environment is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.14: Cart-Pole example'
  prefs: []
  type: TYPE_NORMAL
- en: 'Cart-Pole balancing is one of the classical control problems. As shown in *Figure
    2.14*, the pole is attached to the cart and the goal of our agent is to balance
    the pole on the cart, that is, the goal of our agent is to keep the pole standing
    straight up on the cart as shown in *Figure 2.15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.15: The goal is to keep the pole straight up'
  prefs: []
  type: TYPE_NORMAL
- en: So the agent tries to push the cart left and right to keep the pole standing
    straight on the cart. Thus our agent performs two actions, which are pushing the
    cart to the left and pushing the cart to the right, to keep the pole standing
    straight on the cart. You can also check out this very interesting video, [https://youtu.be/qMlcsc43-lg](https://youtu.be/qMlcsc43-lg),
    which shows how the RL agent balances the pole on the cart by moving the cart
    left and right.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to create the Cart-Pole environment using Gym. The environment
    id of the Cart-Pole environment in Gym is `CartPole-v0`, so we can just use our
    `make` function to create the Cart-Pole environment as shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the environment, we can view our environment using the `render`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also close the rendered environment using the `close` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: State space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's look at the state space of our Cart-Pole environment. Wait! What
    are the states here? In the Frozen Lake environment, we had 16 discrete states
    from **S** to **G**. But how can we describe the states here? Can we describe
    the state by cart position? Yes! Note that the cart position is a continuous value.
    So, in this case, our state space will be continuous values, unlike the Frozen
    Lake environment where our state space had discrete values (**S** to **G**).
  prefs: []
  type: TYPE_NORMAL
- en: 'But with just the cart position alone we cannot describe the state of the environment
    completely. So we include cart velocity, pole angle, and pole velocity at the
    tip. So we can describe our state space by an array of values as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that all of these values are continuous, that is:'
  prefs: []
  type: TYPE_NORMAL
- en: The value of the cart position ranges from `-4.8` to `4.8`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the cart velocity ranges from `-Inf` to `Inf` ( ![](img/B15558_02_007.png)
    to ![](img/B15558_02_008.png) ).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the pole angle ranges from `-0.418` radians to `0.418` radians.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the pole velocity at the tip ranges from `-Inf` to `Inf`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thus, our state space contains an array of continuous values. Let''s learn
    how we can obtain this from Gym. In order to get the state space, we can just
    type `env.observation_space` as shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '`Box` implies that our state space consists of continuous values and not discrete
    values. That is, in the Frozen Lake environment, we obtained the state space as
    `Discrete(16)`, which shows that we have 16 discrete states (**S** to **G**).
    But now we have our state space denoted as `Box(4,)`, which implies that our state
    space is continuous and consists of an array of 4 values.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s reset our environment and see how our initial state space
    will look like. We can reset the environment using the `reset` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Note that here the state space is randomly initialized and so we will get different
    values every time we run the preceding code.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the preceding code implies that our initial state space consists
    of an array of 4 values that denote the cart position, cart velocity, pole angle,
    and pole velocity at the tip, respectively. That is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.16: Initial state space'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we obtain the maximum and minimum values of our state space? We
    can obtain the maximum values of our state space using `env.observation_space.high`
    and the minimum values of our state space using `env.observation_space.low`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s look at the maximum value of our state space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'It implies that:'
  prefs: []
  type: TYPE_NORMAL
- en: The maximum value of the cart position is `4.8`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We learned that the maximum value of the cart velocity is `+Inf`, and we know
    that infinity is not really a number, so it is represented using the largest positive
    real value `3.4028235e+38`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The maximum value of the pole angle is `0.418` radians.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The maximum value of the pole velocity at the tip is `+Inf`, so it is represented
    using the largest positive real value `3.4028235e+38`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Similarly, we can obtain the minimum value of our state space as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'It states that:'
  prefs: []
  type: TYPE_NORMAL
- en: The minimum value of the cart position is `-4.8`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We learned that the minimum value of the cart velocity is `-Inf`, and we know
    that infinity is not really a number, so it is represented using the largest negative
    real value `-3.4028235e+38`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The minimum value of the pole angle is `-0.418` radians.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The minimum value of the pole velocity at the tip is `-Inf`, so it is represented
    using the largest negative real value `-3.4028235e+38`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Action space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's look at the action space. We already learned that in the Cart-Pole
    environment we perform two actions, which are pushing the cart to the left and
    pushing the cart to the right, and thus the action space is discrete since we
    have only two discrete actions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get the action space, we can just type `env.action_space` as the
    following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can observe, `Discrete(2)` implies that our action space is discrete,
    and we have two actions in our action space. Note that the actions will be encoded
    into numbers as shown in *Table 2.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.4: Two possible actions'
  prefs: []
  type: TYPE_NORMAL
- en: Cart-Pole balancing with random policy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's create an agent with the random policy, that is, we create the agent that
    selects a random action in the environment and tries to balance the pole. The
    agent receives a +1 reward every time the pole stands straight up on the cart.
    We will generate over 100 episodes, and we will see the return (sum of rewards)
    obtained over each episode. Let's learn this step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create our Cart-Pole environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of episodes and number of time steps in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'For each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the return to `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'For each step in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomly select an action by sampling from the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the randomly selected action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'If the next state is a terminal state then end the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'For every 10 episodes, print the return (sum of rewards):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will output the sum of rewards obtained over every 10 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus, we have learned about one of the interesting and classic control problems
    called Cart-Pole balancing and how to create the Cart-Pole balancing environment
    using Gym. Gym provides several other classic control environments as shown in
    *Figure 2.17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.17: Classic control environments'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also do some experimentation by creating any of the above environments
    using Gym. We can check all the classic control environments offered by Gym here:
    [https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control).'
  prefs: []
  type: TYPE_NORMAL
- en: Atari game environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Are you a fan of Atari games? If yes, then this section will interest you. Atari
    2600 is a video game console from a game company called Atari. The Atari game
    console provides several popular games, which include Pong, Space Invaders, Ms.
    Pac-Man, Break Out, Centipede, and many more. Training our reinforcement learning
    agent to play Atari games is an interesting as well as challenging task. Often,
    most of the RL algorithms will be tested out on Atari game environments to evaluate
    the accuracy of the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to create the Atari game environment using
    Gym. Gym provides about 59 Atari game environments including Pong, Space Invaders,
    Air Raid, Asteroids, Centipede, Ms. Pac-Man, and so on. Some of the Atari game
    environments provided by Gym are shown in *Figure 2.18* to keep you excited:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.18: Atari game environments'
  prefs: []
  type: TYPE_NORMAL
- en: In Gym, every Atari game environment has 12 different variants. Let's understand
    this with the Pong game environment. The Pong game environment will have 12 different
    variants as explained in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: General environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pong-v0 and Pong-v4**: We can create a Pong environment with the environment
    id as Pong-v0 or Pong-v4\. Okay, what about the state of our environment? Since
    we are dealing with the game environment, we can just take the image of our game
    screen as our state. But we can''t deal with the raw image directly so we will
    take the pixel values of our game screen as the state. We will learn more about
    this in the upcoming section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pong-ram-v0 and Pong-ram-v4**: This is similar to Pong-v0 and Pong-v4, respectively.
    However, here, the state of the environment is the RAM of the Atari machine, which
    is just the 128 bytes instead of the game screen''s pixel values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deterministic environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**PongDeterministic-v0 and PongDeterministic-v4**: In this type, as the name
    suggests, the initial position of the game will be the same every time we initialize
    the environment, and the state of the environment is the pixel values of the game
    screen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pong-ramDeterministic-v0 and Pong-ramDeterministic-v4**: This is similar
    to PongDeterministic-v0 and PongDeterministic-v4, respectively, but here the state
    is the RAM of the Atari machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No frame skipping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**PongNoFrameskip-v0 and PongNoFrameskip-v4**: In this type, no game frame
    is skipped; all game screens are visible to the agent and the state is the pixel
    value of the game screen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pong-ramNoFrameskip-v0 and Pong-ramNoFrameskip-v4**: This is similar to PongNoFrameskip-v0
    and PongNoFrameskip-v4, but here the state is the RAM of the Atari machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus in the Atari environment, the state of our environment will be either the
    game screen or the RAM of the Atari machine. Note that similar to the Pong game,
    all other Atari games have the id in the same fashion in the Gym environment.
    For example, suppose we want to create a deterministic Space Invaders environment;
    then we can just create it with the id `SpaceInvadersDeterministic-v0`. Say we
    want to create a Space Invaders environment with no frame skipping; then we can
    create it with the id `SpaceInvadersNoFrameskip-v0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check out all the Atari game environments offered by Gym here: [https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari).'
  prefs: []
  type: TYPE_NORMAL
- en: State and action space
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's explore the state space and action space of the Atari game environments
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: State space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, let's understand the state space of the Atari games in the
    Gym environment. Let's learn this with the Pong game. We learned that in the Atari
    environment, the state of the environment will be either the game screen's pixel
    values or the RAM of the Atari machine. First, let's understand the state space
    where the state of the environment is the game screen's pixel values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a Pong environment with the `make` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: Here, the game screen is the state of our environment. So, we will just take
    the image of the game screen as the state. However, we can't deal with the raw
    images directly, so we will take the pixel values of the image (game screen) as
    our state. The dimension of the image pixel will be `3` containing the image height,
    image width, and the number of the channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the state of our environment will be an array containing the pixel values
    of the game screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the pixel values range from 0 to 255\. In order to get the state
    space, we can just type `env.observation_space` as the following shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: This indicates that our state space is a 3D array with a shape of [`210`,`160`,`3`].
    As we've learned, `210` denotes the height of the image, `160` denotes the width
    of the image, and `3` represents the number of channels.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can reset our environment and see how the initial state space
    looks like. We can reset the environment using the reset function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will print an array representing the initial game screen's
    pixel value.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a Pong environment where the state of our environment is
    the RAM of the Atari machine instead of the game screen''s pixel value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the state space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'This implies that our state space is a 1D array containing 128 values. We can
    reset our environment and see how the initial state space looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: Note that this applies to all Atari games in the Gym environment, for example,
    if we create a space invaders environment with the state of our environment as
    the game screen's pixel value, then our state space will be a 3D array with a
    shape of `Box(210, 160, 3)`. However, if we create the Space Invaders environment
    with the state of our environment as the RAM of Atari machine, then our state
    space will be an array with a shape of `Box(128,)`.
  prefs: []
  type: TYPE_NORMAL
- en: Action space
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s now explore the action space. In general, the Atari game environment
    has 18 actions in the action space, and the actions are encoded from 0 to 17 as
    shown in *Table 2.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.5: Atari game environment actions'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that all the preceding 18 actions are not applicable to all the Atari
    game environments and the action space varies from game to game. For instance,
    some games use only the first six of the preceding actions as their action space,
    and some games use only the first nine of the preceding actions as their action
    space, while others use all of the preceding 18 actions. Let''s understand this
    with an example using the Pong game:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: The code shows that we have `6` actions in the Pong action space, and the actions
    are encoded from `0` to `5`. So the possible actions in the Pong game are noop
    (no action), fire, up, right, left, and down.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at the action space of the Road Runner game. Just in case you
    have not come across this game before, the game screen looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.19: The Road Runner environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the action space of the Road Runner game:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: This shows us that the action space in the Road Runner game includes all 18
    actions.
  prefs: []
  type: TYPE_NORMAL
- en: An agent playing the Tennis game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, let's explore how to create an agent to play the Tennis game.
    Let's create an agent with a random policy, meaning that the agent will select
    an action randomly from the action space and perform the randomly selected action.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create our Tennis environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s view the Tennis environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will display the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.20: The Tennis game environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the number of episodes and the number of time steps in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'For each episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the return to `0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the state by resetting the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'For each step in the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Render the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Randomly select an action by sampling from the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Perform the randomly selected action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'If the next state is a terminal state, then end the episode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'For every 10 episodes, print the return (sum of rewards):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will output the return (sum of rewards) obtained over every
    10 episodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: Recording the game
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have just learned how to create an agent that randomly selects an action
    from the action space and plays the Tennis game. Can we also record the game played
    by the agent and save it as a video? Yes! Gym provides a wrapper class, which
    we can use to save the agent's gameplay as video.
  prefs: []
  type: TYPE_NORMAL
- en: To record the game, our system should support FFmpeg. FFmpeg is a framework
    used for processing media files. So before moving ahead, make sure that your system
    provides FFmpeg support.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can record our game using the `Monitor` wrapper as the following code shows.
    It takes three parameters: the environment; the directory where we want to save
    our recordings; and the force option. If we set `force = False`, it implies that
    we need to create a new directory every time we want to save new recordings, and
    when we set `force = True`, old recordings in the directory will be cleared out
    and replaced by new recordings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We just need to add the preceding line of code after creating our environment.
    Let''s take a simple example and see how the recordings work. Let''s make our
    agent randomly play the Tennis game for a single episode and record the agent''s
    gameplay as a video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the episode ends, we will see a new directory called **recording** and
    we can find the video file in MP4 format in this directory, which has our agent''s
    gameplay as shown in *Figure 2.21*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.21: The Tennis gameplay'
  prefs: []
  type: TYPE_NORMAL
- en: Other environments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from the classic control and the Atari game environments we've discussed,
    Gym also provides several different categories of the environment. Let's find
    out more about them.
  prefs: []
  type: TYPE_NORMAL
- en: Box2D
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Box2D is the 2D simulator that is majorly used for training our agent to perform
    continuous control tasks, such as walking. For example, Gym provides a Box2D environment
    called `BipedalWalker-v2`, which we can use to train our agent to walk. The `BipedalWalker-v2`
    environment is shown in *Figure 2.22*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.22: The Bipedal Walker environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check out several other Box2D environments offered by Gym here: [https://gym.openai.com/envs/#box2d](https://gym.openai.com/envs/#box2d).'
  prefs: []
  type: TYPE_NORMAL
- en: MuJoCo
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Mujoco** stands for **Multi-Joint dynamics with Contact** and is one of the
    most popular simulators used for training our agent to perform continuous control
    tasks. For example, MuJoCo provides an interesting environment called `HumanoidStandup-v2`,
    which we can use to train our agent to stand up. The `HumanoidStandup-v2` environment
    is shown in *Figure 2.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.23: The Humanoid Stand Up environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check out several other Mujoco environments offered by Gym here: [https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco).'
  prefs: []
  type: TYPE_NORMAL
- en: Robotics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gym provides several environments for performing goal-based tasks for the fetch
    and shadow hand robots. For example, Gym provides an environment called `HandManipulateBlock-v0`,
    which we can use to train our agent to orient a box using a robotic hand. The
    `HandManipulateBlock-v0` environment is shown in *Figure 2.24*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.24: The Hand Manipulate Block environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check out the several robotics environments offered by Gym here: [https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics).'
  prefs: []
  type: TYPE_NORMAL
- en: Toy text
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Toy text is the simplest text-based environment. We already learned about one
    such environment at the beginning of this chapter, which is the Frozen Lake environment.
    We can check out other interesting toy text environments offered by Gym here:
    [https://gym.openai.com/envs/#toy_text](https://gym.openai.com/envs/#toy_text).'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using our RL agent to play games, can we make use of our agent to
    solve some interesting problems? Yes! The algorithmic environment provides several
    interesting problems like copying a given sequence, performing addition, and so
    on. We can make use of the RL agent to solve these problems by learning how to
    perform computation. For instance, Gym provides an environment called `ReversedAddition-v0`,
    which we can use to train our agent to add multiple digit numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the algorithmic environments offered by Gym here: [https://gym.openai.com/envs/#algorithmic](https://gym.openai.com/envs/#algorithmic).'
  prefs: []
  type: TYPE_NORMAL
- en: Environment synopsis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have learned about several types of Gym environment. Wouldn''t it be nice
    if we could have information about all the environments in a single place? Yes!
    The Gym wiki provides a description of all the environments with their environment
    id, state space, action space, and reward range in a table: [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check all the available environments in Gym using the `registry.all()`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will print all the available environments in Gym.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in this chapter, we have learned about the Gym toolkit and also several
    interesting environments offered by Gym. In the upcoming chapters, we will learn
    how to train our RL agent in a Gym environment to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by understanding how to set up our machine by installing
    Anaconda and the Gym toolkit. We learned how to create a Gym environment using
    the `gym.make()` function. Later, we also explored how to obtain the state space
    of the environment using `env.observation_space` and the action space of the environment
    using `env.action_space`. We then learned how to obtain the transition probability
    and reward function of the environment using `env.P`. Following this, we also
    learned how to generate an episode using the Gym environment. We understood that
    in each step of the episode we select an action using the `env.step()` function.
  prefs: []
  type: TYPE_NORMAL
- en: We understood the classic control methods in the Gym environment. We learned
    about the continuous state space of the classic control environments and how they
    are stored in an array. We also learned how to balance a pole using a random agent.
    Later, we learned about interesting Atari game environments, and how Atari game
    environments are named in Gym, and then we explored their state space and action
    space. We also learned how to record the agent's gameplay using the wrapper class,
    and at the end of the chapter, we discovered other environments offered by Gym.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to find the optimal policy using two
    interesting algorithms called value iteration and policy iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s evaluate our newly gained knowledge by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is the use of a Gym toolkit?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we create an environment in Gym?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we obtain the action space of the Gym environment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we visualize the Gym environment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name some classic control environments offered by Gym.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we generate an episode using the Gym environment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the state space of Atari Gym environments?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we record the agent's gameplay?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following resources for more information:'
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about Gym, go to [http://gym.openai.com/docs/](http://gym.openai.com/docs/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also check out the Gym repository to understand how Gym environments
    are coded: [https://github.com/openai/gym](https://github.com/openai/gym).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
