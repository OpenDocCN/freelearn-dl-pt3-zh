- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: A Guide to the Gym Toolkit
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gym 工具包指南
- en: OpenAI is an **artificial intelligence** (**AI**) research organization that
    aims to build **artificial general intelligence** (**AGI**). OpenAI provides a
    famous toolkit called Gym for training a reinforcement learning agent.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI 是一个 **人工智能** (**AI**) 研究组织，旨在构建 **人工通用智能** (**AGI**)。OpenAI 提供了一个著名的工具包，叫做
    Gym，用于训练强化学习代理。
- en: Let's suppose we need to train our agent to drive a car. We need an environment
    to train the agent. Can we train our agent in the real-world environment to drive
    a car? No, because we have learned that reinforcement learning (RL) is a trial-and-error
    learning process, so while we train our agent, it will make a lot of mistakes
    during learning. For example, let's suppose our agent hits another vehicle, and
    it receives a negative reward. It will then learn that hitting other vehicles
    is not a good action and will try not to perform this action again. But we cannot
    train the RL agent in the real-world environment by hitting other vehicles, right?
    That is why we use simulators and train the RL agent in the simulated environments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要训练我们的代理驾驶一辆汽车。我们需要一个环境来训练代理。我们能在现实世界的环境中训练代理驾驶汽车吗？不能，因为我们已经知道强化学习（RL）是一个试错学习过程，所以在训练代理时，它会在学习过程中犯很多错误。例如，假设我们的代理撞到另一辆车，并获得了负奖励。它将学到撞击其他车辆不是一个好的动作，并会尝试避免再次执行这个动作。但我们不能通过让
    RL 代理撞车来训练它在现实环境中驾驶对吧？这就是为什么我们使用模拟器，并在模拟环境中训练 RL 代理的原因。
- en: There are many toolkits that provide a simulated environment for training an
    RL agent. One such popular toolkit is Gym. Gym provides a variety of environments
    for training an RL agent ranging from classic control tasks to Atari game environments.
    We can train our RL agent to learn in these simulated environments using various
    RL algorithms. In this chapter, first, we will install Gym and then we will explore
    various Gym environments. We will also get hands-on with the concepts we have
    learned in the previous chapter by experimenting with the Gym environment.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多工具包提供了一个模拟环境，用于训练强化学习（RL）代理。一个流行的工具包是 Gym。Gym 提供了多种环境，用于训练 RL 代理，从经典控制任务到
    Atari 游戏环境应有尽有。我们可以通过各种 RL 算法训练我们的 RL 代理，使其在这些模拟环境中学习。在本章中，首先，我们将安装 Gym，然后我们将探索各种
    Gym 环境。我们还将通过在 Gym 环境中进行实验，实践我们在上一章学到的概念。
- en: Throughout the book, we will use the Gym toolkit for building and evaluating
    reinforcement learning algorithms, so in this chapter, we will make ourselves
    familiar with the Gym toolkit.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用 Gym 工具包来构建和评估强化学习算法，因此在本章中，我们将熟悉 Gym 工具包。
- en: 'In this chapter, we will learn about the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习以下主题：
- en: Setting up our machine
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置我们的机器
- en: Installing Anaconda and Gym
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Anaconda 和 Gym
- en: Understanding the Gym environment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Gym 环境
- en: Generating an episode in the Gym environment
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Gym 环境中生成一个回合
- en: Exploring more Gym environments
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索更多 Gym 环境
- en: Cart-Pole balancing with the random agent
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机代理的倒立摆平衡
- en: An agent playing the Tennis game
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一名代理玩网球游戏
- en: Setting up our machine
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置我们的机器
- en: In this section, we will learn how to install several dependencies that are
    required for running the code used throughout the book. First, we will learn how
    to install Anaconda and then we will explore how to install Gym.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何安装一些运行全书代码所需的依赖项。首先，我们将学习如何安装 Anaconda，然后我们将探索如何安装 Gym。
- en: Installing Anaconda
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 Anaconda
- en: Anaconda is an open-source distribution of Python. It is widely used for scientific
    computing and processing large volumes of data. It provides an excellent package
    management environment, and it supports Windows, Mac, and Linux operating systems.
    Anaconda comes with Python installed, along with popular packages used for scientific
    computing such as NumPy, SciPy, and so on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda 是一个开源的 Python 发行版，广泛用于科学计算和处理大量数据。它提供了一个优秀的包管理环境，支持 Windows、Mac 和 Linux
    操作系统。Anaconda 自带 Python，并且包括许多用于科学计算的流行包，如 NumPy、SciPy 等。
- en: To download Anaconda, visit [https://www.anaconda.com/download/](https://www.anaconda.com/download/),
    where you will see an option for downloading Anaconda for different platforms.
    If you are using Windows or macOS, you can directly download the graphical installer
    according to your machine architecture and install Anaconda using the graphical
    installer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载Anaconda，请访问[https://www.anaconda.com/download/](https://www.anaconda.com/download/)，你将在该页面上看到适用于不同平台的Anaconda下载选项。如果你使用的是Windows或macOS，你可以根据你的机器架构直接下载图形安装程序，并使用图形安装程序安装Anaconda。
- en: 'If you are using Linux, follow these steps:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是Linux，请按照以下步骤操作：
- en: 'Open the Terminal and type the following command to download Anaconda:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并输入以下命令以下载Anaconda：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'After downloading, we can install Anaconda using the following command:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载完成后，我们可以使用以下命令安装Anaconda：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After the successful installation of Anaconda, we need to create a virtual environment.
    What is the need for a virtual environment? Say we are working on project A, which
    uses NumPy version 1.14, and project B, which uses NumPy version 1.13\. So, to
    work on project B we either downgrade NumPy or reinstall NumPy. In each project,
    we use different libraries with different versions that are not applicable to
    the other projects. Instead of downgrading or upgrading versions or reinstalling
    libraries every time for a new project, we use a virtual environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功安装Anaconda之后，我们需要创建一个虚拟环境。为什么需要虚拟环境呢？假设我们正在进行项目A，该项目使用NumPy版本1.14，而项目B使用NumPy版本1.13。那么，要在项目B中工作，我们要么降级NumPy，要么重新安装NumPy。在每个项目中，我们使用的是不同版本的库，这些库在其他项目中不可用。为了避免每次新项目都需要降级、升级版本或重新安装库，我们使用虚拟环境。
- en: 'The virtual environment is just an isolated environment for a particular project
    so that each project can have its own dependencies and will not affect other projects.
    We will create a virtual environment using the following command and name our
    environment `universe`:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟环境是为特定项目创建的一个隔离环境，使得每个项目可以拥有自己独立的依赖项，并且不会影响其他项目。我们将使用以下命令创建一个名为`universe`的虚拟环境：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that we use Python version 3.6\. Once the virtual environment is created,
    we can activate it using the following command:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用的是Python版本3.6。虚拟环境创建完成后，我们可以使用以下命令激活它：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That's it! Now that we have learned how to install Anaconda and create a virtual
    environment, in the next section, we will learn how to install Gym.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在我们已经学会了如何安装Anaconda并创建虚拟环境，在接下来的章节中，我们将学习如何安装Gym。
- en: Installing the Gym toolkit
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Gym工具包
- en: 'In this section, we will learn how to install the Gym toolkit. Before going
    ahead, first, let''s activate our virtual environment, `universe`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何安装Gym工具包。在继续之前，首先让我们激活虚拟环境`universe`：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, install the following dependencies:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，安装以下依赖项：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can install Gym directly using `pip`. Note that throughout the book, we
    will use Gym version 0.15.4\. We can install Gym using the following command:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`pip`直接安装Gym。请注意，本书中将使用Gym版本0.15.4。我们可以使用以下命令安装Gym：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can also install Gym by cloning the Gym repository as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过克隆Gym仓库来安装Gym，如下所示：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Common error fixes
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见错误修复
- en: 'Just in case, if you get any of the following errors while installing Gym,
    the following commands will help:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在安装Gym时遇到以下任何错误，以下命令将有所帮助：
- en: '**Failed building wheel for pachi-py** or **failed building wheel for pachi-py
    atari-py**:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建pachi-py轮文件失败**或**构建pachi-py atari-py轮文件失败**：'
- en: '[PRE8]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Failed building wheel for mujoco-py**:'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建mujoco-py轮文件失败**：'
- en: '[PRE9]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**error: command ''gcc'' failed with exit status 1**:'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误：命令''gcc''以退出状态1失败**：'
- en: '[PRE10]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that we have successfully installed Gym, in the next section, let's kickstart
    our hands-on reinforcement learning journey.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功安装了Gym，在接下来的章节中，让我们开始我们的强化学习实践之旅。
- en: Creating our first Gym environment
  id: totrans-48
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的第一个Gym环境
- en: We have learned that Gym provides a variety of environments for training a reinforcement
    learning agent. To clearly understand how the Gym environment is designed, we
    will start with the basic Gym environment. After that, we will understand other
    complex Gym environments.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解到，Gym提供了多种环境用于训练强化学习代理。为了清楚地理解Gym环境的设计，我们将从基本的Gym环境开始。之后，我们将理解其他复杂的Gym环境。
- en: 'Let''s introduce one of the simplest environments called the Frozen Lake environment.
    *Figure 2.1* shows the Frozen Lake environment. As we can observe, in the Frozen
    Lake environment, the goal of the agent is to start from the initial state **S**
    and reach the goal state **G**:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_01.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: The Frozen Lake environment'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding environment, the following apply:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '**S** denotes the starting state'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F** denotes the frozen state'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H** denotes the hole state'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**G** denotes the goal state'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, the agent has to start from state **S** and reach the goal state **G**.
    But one issue is that if the agent visits state **H**, which is the hole state,
    then the agent will fall into the hole and die as shown in *Figure 2.2*:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_02.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: The agent falls down a hole'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we need to make sure that the agent starts from **S** and reaches **G**
    without falling into the hole state **H** as shown in *Figure 2.3*:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_03.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: The agent reaches the goal state'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Each grid box in the preceding environment is called a state, thus we have 16
    states (**S** to **G**) and we have 4 possible actions, which are *up*, *down*,
    *left*, and *right*. We learned that our goal is to reach the state **G** from
    **S** without visiting **H**. So, we assign +1 reward for the goal state **G**
    and 0 for all other states.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have learned how the Frozen Lake environment works. Now, to train our
    agent in the Frozen Lake environment, first, we need to create the environment
    by coding it from scratch in Python. But luckily we don't have to do that! Since
    Gym provides various environments, we can directly import the Gym toolkit and
    create a Frozen Lake environment.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we will learn how to create our Frozen Lake environment using Gym. Before
    running any code, make sure that you have activated our virtual environment `universe`.
    First, let''s import the Gym library:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we can create a Gym environment using the `make` function. The `make`
    function requires the environment id as a parameter. In Gym, the id of the Frozen
    Lake environment is `FrozenLake-v0`. So, we can create our Frozen Lake environment
    as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After creating the environment, we can see how our environment looks like using
    the `render` function:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code renders the following environment:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_04.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Gym''s Frozen Lake environment'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, the Frozen Lake environment consists of 16 states (**S**
    to **G**) as we learned. The state **S** is highlighted indicating that it is
    our current state, that is, the agent is in the state **S**. So whenever we create
    an environment, an agent will always begin from the initial state, which in our
    case is state **S**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: That's it! Creating the environment using Gym is that simple. In the next section,
    we will understand more about the Gym environment by relating all the concepts
    we have learned in the previous chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the environment
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous chapter, we learned that the reinforcement learning environment
    can be modeled as a **Markov decision process** (**MDP**) and an MDP consists
    of the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '**States**: A set of states present in the environment.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Actions**: A set of actions that the agent can perform in each state.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transition probability**: The transition probability is denoted by ![](img/B15558_02_001.png).
    It implies the probability of moving from a state *s* to the state ![](img/B15558_02_002.png)
    while performing an action *a*.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reward function**: The reward function is denoted by ![](img/B15558_02_003.png).
    It implies the reward the agent obtains moving from a state *s* to the state ![](img/B15558_02_004.png)
    while performing an action *a*.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's now understand how to obtain all the above information from the Frozen
    Lake environment we just created using Gym.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: States
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A state space consists of all of our states. We can obtain the number of states
    in our environment by just typing `env.observation_space` as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code will print:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'It implies that we have 16 discrete states in our state space starting from
    state **S** to **G**. Note that, in Gym, the states will be encoded as a number,
    so the state **S** will be encoded as 0, state **F** will be encoded as 1, and
    so on as *Figure 2.5* shows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_05.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Sixteen discrete states'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Actions
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We learned that the action space consists of all the possible actions in the
    environment. We can obtain the action space by using `env.action_space`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code will print:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It shows that we have `4` discrete actions in our action space, which are *left*,
    *down*, *right*, and *up*. Note that, similar to states, actions also will be
    encoded into numbers as shown in *Table 2.1*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_06.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.1: Four discrete actions'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Transition probability and reward function
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's look at how to obtain the transition probability and the reward function.
    We learned that in the stochastic environment, we cannot say that by performing
    some action *a*, the agent will always reach the next state ![](img/B15558_02_004.png)
    exactly because there will be some randomness associated with the stochastic environment,
    and by performing an action *a* in the state *s*, the agent reaches the next state
    ![](img/B15558_02_004.png) with some probability.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s suppose we are in state 2 (**F**). Now, if we perform action 1 (*down*)
    in state 2, we can reach state 6 as shown in *Figure 2.6*:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Image62187.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: The agent performing a down action from state 2'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'Our Frozen Lake environment is a stochastic environment. When our environment
    is stochastic, we won''t always reach state 6 by performing action 1 (*down*)
    in state 2; we also reach other states with some probability. So when we perform
    an action 1 (*down*) in state 2, we reach state 1 with probability 0.33333, we
    reach state 6 with probability 0.33333, and we reach state 3 with probability
    0.33333 as shown in *Figure 2.7*:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_08.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Transition probability of the agent in state 2'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, in a stochastic environment we reach the next states with some
    probability. Now, let's learn how to obtain this transition probability using
    the Gym environment.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: 'We can obtain the transition probability and the reward function by just typing
    `env.P[state][action]`. So, to obtain the transition probability of moving from
    state **S** to the other states by performing the action *right*, we can type
    `env.P[S][right]`. But we cannot just type state **S** and action *right* directly
    since they are encoded as numbers. We learned that state **S** is encoded as 0
    and the action *right* is encoded as 2, so, to obtain the transition probability
    of state **S** by performing the action *right*, we type `env.P[0][2]` as the
    following shows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The above code will print:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'What does this imply? Our output is in the form of `[(transition probability,
    next state, reward, Is terminal state?)]`. It implies that if we perform an action
    2 (*right*) in state 0 (**S**) then:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: We reach state 4 (**F**) with probability 0.33333 and receive 0 reward.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach state 1 (**F**) with probability 0.33333 and receive 0 reward.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach the same state 0 (**S**) with probability 0.33333 and receive 0 reward.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2.8* shows the transition probability:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_09.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Transition probability of the agent in state 0'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: Thus, when we type `env.P[state][action]`, we get the result in the form of
    `[(transition probability, next state, reward, Is terminal state?)]`. The last
    value is Boolean and tells us whether the next state is a terminal state. Since
    4, 1, and 0 are not terminal states, it is given as false.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of `env.P[0][2]` is shown in *Table 2.2* for more clarity:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_10.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.2: Output of env.P[0][2]'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this with one more example. Let''s suppose we are in state
    3 (**F**) as *Figure 2.9* shows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_11.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: The agent in state 3'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we perform action 1 (*down*) in state 3 (**F**). Then the transition probability
    of state 3 (**F**) by performing action 1 (*down*) can be obtained as the following
    shows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code will print:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'As we learned, our output is in the form of `[(transition probability, next
    state, reward, Is terminal state?)]`. It implies that if we perform action 1 (*down*)
    in state 3 (**F**) then:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: We reach state 2 (**F**) with probability 0.33333 and receive 0 reward.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach state 7 (**H**) with probability 0.33333 and receive 0 reward.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach the same state 3 (**F**) with probability 0.33333 and receive 0 reward.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2.10* shows the transition probability:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_12.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Transition probabilities of the agent in state 3'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The output of `env.P[3][1]` is shown in *Table 2.3* for more clarity:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_13.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.3: Output of env.P[3][1]'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: As we can observe, in the second row of our output, we have `(0.33333, 7, 0.0,
    True)`, and the last value here is marked as `True`. It implies that state 7 is
    a terminal state. That is, if we perform action 1 (*down*) in state 3 (**F**)
    then we reach state 7 (**H**) with 0.33333 probability, and since 7 (**H**) is
    a hole, the agent dies if it reaches state 7 (**H**). Thus 7(**H**) is a terminal
    state and so it is marked as `True`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have learned how to obtain the state space, action space, transition
    probability, and the reward function using the Gym environment. In the next section,
    we will learn how to generate an episode.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Generating an episode in the Gym environment
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We learned that the agent-environment interaction starting from an initial state
    until the terminal state is called an episode. In this section, we will learn
    how to generate an episode in the Gym environment.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we begin, we initialize the state by resetting our environment; resetting
    puts our agent back to the initial state. We can reset our environment using the
    `reset()` function as shown as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Action selection
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order for the agent to interact with the environment, it has to perform
    some action in the environment. So, first, let''s learn how to perform an action
    in the Gym environment. Let''s suppose we are in state 3 (**F**) as *Figure 2.11*
    shows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_14.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: The agent is in state 3 in the Frozen Lake environment'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we need to perform action 1 (*down*) and move to the new state 7 (**H**).
    How can we do that? We can perform an action using the `step` function. We just
    need to input our action as a parameter to the `step` function. So, we can perform
    action 1 (*down*) in state 3 (**F**) using the `step` function as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, let''s render our environment using the `render` function:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As shown in *Figure 2.12*, the agent performs action 1 (*down*) in state 3
    (**F**) and reaches the next state 7 (**H**):'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_15.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.12: The agent in state 7 in the Frozen Lake environment'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that whenever we make an action using `env.step()`, it outputs a tuple
    containing 4 values. So, when we take action 1 (*down*) in state 3 (**F**) using
    `env.step(1)`, it gives the output as:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As you might have guessed, it implies that when we perform action 1 (*down*)
    in state 3 (**F**):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: We reach the next state 7 (**H**).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent receives the reward `0.0.`
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the next state 7 (**H**) is a terminal state, it is marked as `True`.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We reach the next state 7 (**H**) with a probability of 0.33333.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we can just store this information as:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Thus:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '`next_state` represents the next state.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reward` represents the obtained reward.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`done` implies whether our episode has ended. That is, if the next state is
    a terminal state, then our episode will end, so `done` will be marked as `True`
    else it will be marked as `False`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`info`—Apart from the transition probability, in some cases, we also obtain
    other information saved as info, which is used for debugging purposes.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`info`—除了转移概率外，在某些情况下，我们还会获得其他信息，并将其保存为info，这些信息用于调试目的。'
- en: 'We can also sample action from our action space and perform a random action
    to explore our environment. We can sample an action using the `sample` function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以从我们的动作空间中采样动作，并执行一个随机动作以探索我们的环境。我们可以使用`sample`函数采样一个动作：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'After we have sampled an action from our action space, then we perform our
    sampled action using our step function:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在从我们的动作空间中采样了一个动作后，我们使用我们的步进函数执行采样的动作：
- en: '[PRE28]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now that we have learned how to select actions in the environment, let's see
    how to generate an episode.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学会了如何在环境中选择动作，让我们看看如何生成一个回合。
- en: Generating an episode
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成一个回合
- en: Now let's learn how to generate an episode. The episode is the agent environment
    interaction starting from the initial state to the terminal state. The agent interacts
    with the environment by performing some action in each state. An episode ends
    if the agent reaches the terminal state. So, in the Frozen Lake environment, the
    episode will end if the agent reaches the terminal state, which is either the
    hole state (**H**) or goal state (**G**).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们学习如何生成一个回合。回合是智能体与环境的交互，从初始状态开始，到终止状态结束。智能体通过在每个状态下执行一些动作与环境进行交互。如果智能体到达终止状态，回合结束。因此，在Frozen
    Lake环境中，如果智能体到达终止状态，即坑洞状态（**H**）或目标状态（**G**），回合就会结束。
- en: Let's understand how to generate an episode with the random policy. We learned
    that the random policy selects a random action in each state. So, we will generate
    an episode by taking random actions in each state. So for each time step in the
    episode, we take a random action in each state and our episode will end if the
    agent reaches the terminal state.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解如何使用随机策略生成一个回合。我们了解到，随机策略在每个状态下选择一个随机动作。因此，我们将通过在每个状态下采取随机动作来生成一个回合。所以在回合的每个时间步长中，我们在每个状态下采取一个随机动作，如果智能体到达终止状态，回合就会结束。
- en: 'First, let''s set the number of time steps:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们设置时间步长的数量：
- en: '[PRE29]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For each time step:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个时间步长：
- en: '[PRE30]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Randomly select an action by sampling from the action space:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 通过从动作空间中采样，随机选择一个动作：
- en: '[PRE31]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Perform the selected action:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 执行选择的操作：
- en: '[PRE32]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If the next state is the terminal state, then break. This implies that our
    episode ends:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果下一个状态是终止状态，则退出。这意味着我们的回合结束：
- en: '[PRE33]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The preceding code will print something similar to *Figure 2.13*. Note that
    you might get a different result each time you run the preceding code since the
    agent is taking a random action in each time step.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将打印出类似于*图2.13*的内容。请注意，每次运行上述代码时，你可能会得到不同的结果，因为智能体在每个时间步长中都在执行一个随机动作。
- en: 'As we can observe from the following output, on each time step, the agent takes
    a random action in each state and our episode ends once the agent reaches the
    terminal state. As *Figure 2.13* shows, in time step 4, the agent reaches the
    terminal state **H,** and so the episode ends:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如下输出所示，在每个时间步长上，智能体在每个状态下执行一个随机动作，并且一旦智能体到达终止状态，我们的回合就结束。如*图2.13*所示，在第4个时间步长，智能体到达了终止状态**H**，因此回合结束：
- en: '![](img/B15558_02_16.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B15558_02_16.png)'
- en: 'Figure 2.13: Actions taken by the agent in each time step'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13：智能体在每个时间步长所采取的动作
- en: 'Instead of generating one episode, we can also generate a series of episodes
    by taking some random action in each state:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不仅可以生成一个回合，还可以通过在每个状态下执行一些随机动作来生成一系列回合：
- en: '[PRE35]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Thus, we can generate an episode by selecting a random action in each state
    by sampling from the action space. But wait! What is the use of this? Why do we
    even need to generate an episode?
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过从动作空间中采样，在每个状态下选择一个随机动作来生成一个回合。但等等！这样做有什么用？我们为什么需要生成一个回合？
- en: In the previous chapter, we learned that an agent can find the optimal policy
    (that is, the correct action in each state) by generating several episodes. But
    in the preceding example, we just took random actions in each state over all the
    episodes. How can the agent find the optimal policy? So, in the case of the Frozen
    Lake environment, how can the agent find the optimal policy that tells the agent
    to reach state **G** from state **S** without visiting the hole states **H**?
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们了解到，智能体可以通过生成多个回合来找到最优策略（即在每个状态下选择正确的动作）。但是在前面的例子中，我们仅仅是在所有回合中在每个状态下采取了随机动作。那么智能体怎么能找到最优策略呢？那么，在Frozen
    Lake环境中，智能体如何找到最优策略，告诉智能体如何从状态**S**到达状态**G**，而不经过坑洞状态**H**呢？
- en: This is where we need a reinforcement learning algorithm. Reinforcement learning
    is all about finding the optimal policy, that is, the policy that tells us what
    action to perform in each state. We will learn how to find the optimal policy
    by generating a series of episodes using various reinforcement learning algorithms
    in the upcoming chapters. In this chapter, we will focus on getting acquainted
    with the Gym environment and various Gym functionalities as we will be using the
    Gym environment throughout the course of the book.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: So far we have understood how the Gym environment works using the basic Frozen
    Lake environment, but Gym has so many other functionalities and also several interesting
    environments. In the next section, we will learn about the other Gym environments
    along with exploring the functionalities of Gym.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: More Gym environments
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore several interesting Gym environments, along
    with exploring different functionalities of Gym.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Classic control environments
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gym provides environments for several classic control tasks such as Cart-Pole
    balancing, swinging up an inverted pendulum, mountain car climbing, and so on.
    Let''s understand how to create a Gym environment for a Cart-Pole balancing task.
    The Cart-Pole environment is shown below:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_17.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.14: Cart-Pole example'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Cart-Pole balancing is one of the classical control problems. As shown in *Figure
    2.14*, the pole is attached to the cart and the goal of our agent is to balance
    the pole on the cart, that is, the goal of our agent is to keep the pole standing
    straight up on the cart as shown in *Figure 2.15*:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_18.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.15: The goal is to keep the pole straight up'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: So the agent tries to push the cart left and right to keep the pole standing
    straight on the cart. Thus our agent performs two actions, which are pushing the
    cart to the left and pushing the cart to the right, to keep the pole standing
    straight on the cart. You can also check out this very interesting video, [https://youtu.be/qMlcsc43-lg](https://youtu.be/qMlcsc43-lg),
    which shows how the RL agent balances the pole on the cart by moving the cart
    left and right.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to create the Cart-Pole environment using Gym. The environment
    id of the Cart-Pole environment in Gym is `CartPole-v0`, so we can just use our
    `make` function to create the Cart-Pole environment as shown below:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'After creating the environment, we can view our environment using the `render`
    function:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can also close the rendered environment using the `close` function:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: State space
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's look at the state space of our Cart-Pole environment. Wait! What
    are the states here? In the Frozen Lake environment, we had 16 discrete states
    from **S** to **G**. But how can we describe the states here? Can we describe
    the state by cart position? Yes! Note that the cart position is a continuous value.
    So, in this case, our state space will be continuous values, unlike the Frozen
    Lake environment where our state space had discrete values (**S** to **G**).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'But with just the cart position alone we cannot describe the state of the environment
    completely. So we include cart velocity, pole angle, and pole velocity at the
    tip. So we can describe our state space by an array of values as shown as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that all of these values are continuous, that is:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The value of the cart position ranges from `-4.8` to `4.8`.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the cart velocity ranges from `-Inf` to `Inf` ( ![](img/B15558_02_007.png)
    to ![](img/B15558_02_008.png) ).
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the pole angle ranges from `-0.418` radians to `0.418` radians.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The value of the pole velocity at the tip ranges from `-Inf` to `Inf`.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Thus, our state space contains an array of continuous values. Let''s learn
    how we can obtain this from Gym. In order to get the state space, we can just
    type `env.observation_space` as shown as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding code will print:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '`Box` implies that our state space consists of continuous values and not discrete
    values. That is, in the Frozen Lake environment, we obtained the state space as
    `Discrete(16)`, which shows that we have 16 discrete states (**S** to **G**).
    But now we have our state space denoted as `Box(4,)`, which implies that our state
    space is continuous and consists of an array of 4 values.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s reset our environment and see how our initial state space
    will look like. We can reset the environment using the `reset` function:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The preceding code will print:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Note that here the state space is randomly initialized and so we will get different
    values every time we run the preceding code.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the preceding code implies that our initial state space consists
    of an array of 4 values that denote the cart position, cart velocity, pole angle,
    and pole velocity at the tip, respectively. That is:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_19.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.16: Initial state space'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we obtain the maximum and minimum values of our state space? We
    can obtain the maximum values of our state space using `env.observation_space.high`
    and the minimum values of our state space using `env.observation_space.low`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s look at the maximum value of our state space:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The preceding code will print:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'It implies that:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: The maximum value of the cart position is `4.8`.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We learned that the maximum value of the cart velocity is `+Inf`, and we know
    that infinity is not really a number, so it is represented using the largest positive
    real value `3.4028235e+38`.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The maximum value of the pole angle is `0.418` radians.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The maximum value of the pole velocity at the tip is `+Inf`, so it is represented
    using the largest positive real value `3.4028235e+38`.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Similarly, we can obtain the minimum value of our state space as:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The preceding code will print:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'It states that:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: The minimum value of the cart position is `-4.8`.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We learned that the minimum value of the cart velocity is `-Inf`, and we know
    that infinity is not really a number, so it is represented using the largest negative
    real value `-3.4028235e+38`.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The minimum value of the pole angle is `-0.418` radians.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The minimum value of the pole velocity at the tip is `-Inf`, so it is represented
    using the largest negative real value `-3.4028235e+38`.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Action space
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's look at the action space. We already learned that in the Cart-Pole
    environment we perform two actions, which are pushing the cart to the left and
    pushing the cart to the right, and thus the action space is discrete since we
    have only two discrete actions.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to get the action space, we can just type `env.action_space` as the
    following shows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The preceding code will print:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As we can observe, `Discrete(2)` implies that our action space is discrete,
    and we have two actions in our action space. Note that the actions will be encoded
    into numbers as shown in *Table 2.4*:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_20.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.4: Two possible actions'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Cart-Pole balancing with random policy
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's create an agent with the random policy, that is, we create the agent that
    selects a random action in the environment and tries to balance the pole. The
    agent receives a +1 reward every time the pole stands straight up on the cart.
    We will generate over 100 episodes, and we will see the return (sum of rewards)
    obtained over each episode. Let's learn this step by step.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create our Cart-Pole environment:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Set the number of episodes and number of time steps in the episode:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'For each episode:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Set the return to `0`:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'For each step in the episode:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Render the environment:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Randomly select an action by sampling from the environment:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Perform the randomly selected action:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Update the return:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'If the next state is a terminal state then end the episode:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'For every 10 episodes, print the return (sum of rewards):'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Close the environment:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The preceding code will output the sum of rewards obtained over every 10 episodes:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Thus, we have learned about one of the interesting and classic control problems
    called Cart-Pole balancing and how to create the Cart-Pole balancing environment
    using Gym. Gym provides several other classic control environments as shown in
    *Figure 2.17*:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_21.png)'
  id: totrans-298
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.17: Classic control environments'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also do some experimentation by creating any of the above environments
    using Gym. We can check all the classic control environments offered by Gym here:
    [https://gym.openai.com/envs/#classic_control](https://gym.openai.com/envs/#classic_control).'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Atari game environments
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Are you a fan of Atari games? If yes, then this section will interest you. Atari
    2600 is a video game console from a game company called Atari. The Atari game
    console provides several popular games, which include Pong, Space Invaders, Ms.
    Pac-Man, Break Out, Centipede, and many more. Training our reinforcement learning
    agent to play Atari games is an interesting as well as challenging task. Often,
    most of the RL algorithms will be tested out on Atari game environments to evaluate
    the accuracy of the algorithm.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to create the Atari game environment using
    Gym. Gym provides about 59 Atari game environments including Pong, Space Invaders,
    Air Raid, Asteroids, Centipede, Ms. Pac-Man, and so on. Some of the Atari game
    environments provided by Gym are shown in *Figure 2.18* to keep you excited:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_22.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.18: Atari game environments'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: In Gym, every Atari game environment has 12 different variants. Let's understand
    this with the Pong game environment. The Pong game environment will have 12 different
    variants as explained in the following sections.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: General environment
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pong-v0 and Pong-v4**: We can create a Pong environment with the environment
    id as Pong-v0 or Pong-v4\. Okay, what about the state of our environment? Since
    we are dealing with the game environment, we can just take the image of our game
    screen as our state. But we can''t deal with the raw image directly so we will
    take the pixel values of our game screen as the state. We will learn more about
    this in the upcoming section.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pong-ram-v0 and Pong-ram-v4**: This is similar to Pong-v0 and Pong-v4, respectively.
    However, here, the state of the environment is the RAM of the Atari machine, which
    is just the 128 bytes instead of the game screen''s pixel values.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deterministic environment
  id: totrans-310
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**PongDeterministic-v0 and PongDeterministic-v4**: In this type, as the name
    suggests, the initial position of the game will be the same every time we initialize
    the environment, and the state of the environment is the pixel values of the game
    screen.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pong-ramDeterministic-v0 and Pong-ramDeterministic-v4**: This is similar
    to PongDeterministic-v0 and PongDeterministic-v4, respectively, but here the state
    is the RAM of the Atari machine.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No frame skipping
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**PongNoFrameskip-v0 and PongNoFrameskip-v4**: In this type, no game frame
    is skipped; all game screens are visible to the agent and the state is the pixel
    value of the game screen.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pong-ramNoFrameskip-v0 and Pong-ramNoFrameskip-v4**: This is similar to PongNoFrameskip-v0
    and PongNoFrameskip-v4, but here the state is the RAM of the Atari machine.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thus in the Atari environment, the state of our environment will be either the
    game screen or the RAM of the Atari machine. Note that similar to the Pong game,
    all other Atari games have the id in the same fashion in the Gym environment.
    For example, suppose we want to create a deterministic Space Invaders environment;
    then we can just create it with the id `SpaceInvadersDeterministic-v0`. Say we
    want to create a Space Invaders environment with no frame skipping; then we can
    create it with the id `SpaceInvadersNoFrameskip-v0`.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check out all the Atari game environments offered by Gym here: [https://gym.openai.com/envs/#atari](https://gym.openai.com/envs/#atari).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: State and action space
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's explore the state space and action space of the Atari game environments
    in detail.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: State space
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this section, let's understand the state space of the Atari games in the
    Gym environment. Let's learn this with the Pong game. We learned that in the Atari
    environment, the state of the environment will be either the game screen's pixel
    values or the RAM of the Atari machine. First, let's understand the state space
    where the state of the environment is the game screen's pixel values.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create a Pong environment with the `make` function:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Here, the game screen is the state of our environment. So, we will just take
    the image of the game screen as the state. However, we can't deal with the raw
    images directly, so we will take the pixel values of the image (game screen) as
    our state. The dimension of the image pixel will be `3` containing the image height,
    image width, and the number of the channel.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, the state of our environment will be an array containing the pixel values
    of the game screen:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Note that the pixel values range from 0 to 255\. In order to get the state
    space, we can just type `env.observation_space` as the following shows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The preceding code will print:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: This indicates that our state space is a 3D array with a shape of [`210`,`160`,`3`].
    As we've learned, `210` denotes the height of the image, `160` denotes the width
    of the image, and `3` represents the number of channels.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can reset our environment and see how the initial state space
    looks like. We can reset the environment using the reset function:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The preceding code will print an array representing the initial game screen's
    pixel value.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create a Pong environment where the state of our environment is
    the RAM of the Atari machine instead of the game screen''s pixel value:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let''s look at the state space:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The preceding code will print:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'This implies that our state space is a 1D array containing 128 values. We can
    reset our environment and see how the initial state space looks like:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Note that this applies to all Atari games in the Gym environment, for example,
    if we create a space invaders environment with the state of our environment as
    the game screen's pixel value, then our state space will be a 3D array with a
    shape of `Box(210, 160, 3)`. However, if we create the Space Invaders environment
    with the state of our environment as the RAM of Atari machine, then our state
    space will be an array with a shape of `Box(128,)`.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Action space
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s now explore the action space. In general, the Atari game environment
    has 18 actions in the action space, and the actions are encoded from 0 to 17 as
    shown in *Table 2.5*:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_23.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
- en: 'Table 2.5: Atari game environment actions'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that all the preceding 18 actions are not applicable to all the Atari
    game environments and the action space varies from game to game. For instance,
    some games use only the first six of the preceding actions as their action space,
    and some games use only the first nine of the preceding actions as their action
    space, while others use all of the preceding 18 actions. Let''s understand this
    with an example using the Pong game:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'The preceding code will print:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The code shows that we have `6` actions in the Pong action space, and the actions
    are encoded from `0` to `5`. So the possible actions in the Pong game are noop
    (no action), fire, up, right, left, and down.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now look at the action space of the Road Runner game. Just in case you
    have not come across this game before, the game screen looks like this:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_24.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.19: The Road Runner environment'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see the action space of the Road Runner game:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The preceding code will print:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: This shows us that the action space in the Road Runner game includes all 18
    actions.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: An agent playing the Tennis game
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, let's explore how to create an agent to play the Tennis game.
    Let's create an agent with a random policy, meaning that the agent will select
    an action randomly from the action space and perform the randomly selected action.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s create our Tennis environment:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Let''s view the Tennis environment:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The preceding code will display the following:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_25.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.20: The Tennis game environment'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the number of episodes and the number of time steps in the episode:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'For each episode:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Set the return to `0`:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Initialize the state by resetting the environment:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'For each step in the episode:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Render the environment:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Randomly select an action by sampling from the environment:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Perform the randomly selected action:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Update the return:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'If the next state is a terminal state, then end the episode:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'For every 10 episodes, print the return (sum of rewards):'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Close the environment:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The preceding code will output the return (sum of rewards) obtained over every
    10 episodes:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: Recording the game
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We have just learned how to create an agent that randomly selects an action
    from the action space and plays the Tennis game. Can we also record the game played
    by the agent and save it as a video? Yes! Gym provides a wrapper class, which
    we can use to save the agent's gameplay as video.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: To record the game, our system should support FFmpeg. FFmpeg is a framework
    used for processing media files. So before moving ahead, make sure that your system
    provides FFmpeg support.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 'We can record our game using the `Monitor` wrapper as the following code shows.
    It takes three parameters: the environment; the directory where we want to save
    our recordings; and the force option. If we set `force = False`, it implies that
    we need to create a new directory every time we want to save new recordings, and
    when we set `force = True`, old recordings in the directory will be cleared out
    and replaced by new recordings:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  id: totrans-400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'We just need to add the preceding line of code after creating our environment.
    Let''s take a simple example and see how the recordings work. Let''s make our
    agent randomly play the Tennis game for a single episode and record the agent''s
    gameplay as a video:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Once the episode ends, we will see a new directory called **recording** and
    we can find the video file in MP4 format in this directory, which has our agent''s
    gameplay as shown in *Figure 2.21*:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_26.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.21: The Tennis gameplay'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: Other environments
  id: totrans-406
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Apart from the classic control and the Atari game environments we've discussed,
    Gym also provides several different categories of the environment. Let's find
    out more about them.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: Box2D
  id: totrans-408
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Box2D is the 2D simulator that is majorly used for training our agent to perform
    continuous control tasks, such as walking. For example, Gym provides a Box2D environment
    called `BipedalWalker-v2`, which we can use to train our agent to walk. The `BipedalWalker-v2`
    environment is shown in *Figure 2.22*:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_27.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.22: The Bipedal Walker environment'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check out several other Box2D environments offered by Gym here: [https://gym.openai.com/envs/#box2d](https://gym.openai.com/envs/#box2d).'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: MuJoCo
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Mujoco** stands for **Multi-Joint dynamics with Contact** and is one of the
    most popular simulators used for training our agent to perform continuous control
    tasks. For example, MuJoCo provides an interesting environment called `HumanoidStandup-v2`,
    which we can use to train our agent to stand up. The `HumanoidStandup-v2` environment
    is shown in *Figure 2.23*:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_28.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.23: The Humanoid Stand Up environment'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check out several other Mujoco environments offered by Gym here: [https://gym.openai.com/envs/#mujoco](https://gym.openai.com/envs/#mujoco).'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: Robotics
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gym provides several environments for performing goal-based tasks for the fetch
    and shadow hand robots. For example, Gym provides an environment called `HandManipulateBlock-v0`,
    which we can use to train our agent to orient a box using a robotic hand. The
    `HandManipulateBlock-v0` environment is shown in *Figure 2.24*:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_02_29.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.24: The Hand Manipulate Block environment'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check out the several robotics environments offered by Gym here: [https://gym.openai.com/envs/#robotics](https://gym.openai.com/envs/#robotics).'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: Toy text
  id: totrans-423
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Toy text is the simplest text-based environment. We already learned about one
    such environment at the beginning of this chapter, which is the Frozen Lake environment.
    We can check out other interesting toy text environments offered by Gym here:
    [https://gym.openai.com/envs/#toy_text](https://gym.openai.com/envs/#toy_text).'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instead of using our RL agent to play games, can we make use of our agent to
    solve some interesting problems? Yes! The algorithmic environment provides several
    interesting problems like copying a given sequence, performing addition, and so
    on. We can make use of the RL agent to solve these problems by learning how to
    perform computation. For instance, Gym provides an environment called `ReversedAddition-v0`,
    which we can use to train our agent to add multiple digit numbers.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the algorithmic environments offered by Gym here: [https://gym.openai.com/envs/#algorithmic](https://gym.openai.com/envs/#algorithmic).'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Environment synopsis
  id: totrans-428
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have learned about several types of Gym environment. Wouldn''t it be nice
    if we could have information about all the environments in a single place? Yes!
    The Gym wiki provides a description of all the environments with their environment
    id, state space, action space, and reward range in a table: [https://github.com/openai/gym/wiki/Table-of-environments](https://github.com/openai/gym/wiki/Table-of-environments).'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check all the available environments in Gym using the `registry.all()`
    method:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: The preceding code will print all the available environments in Gym.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in this chapter, we have learned about the Gym toolkit and also several
    interesting environments offered by Gym. In the upcoming chapters, we will learn
    how to train our RL agent in a Gym environment to find the optimal policy.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-434
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started the chapter by understanding how to set up our machine by installing
    Anaconda and the Gym toolkit. We learned how to create a Gym environment using
    the `gym.make()` function. Later, we also explored how to obtain the state space
    of the environment using `env.observation_space` and the action space of the environment
    using `env.action_space`. We then learned how to obtain the transition probability
    and reward function of the environment using `env.P`. Following this, we also
    learned how to generate an episode using the Gym environment. We understood that
    in each step of the episode we select an action using the `env.step()` function.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: We understood the classic control methods in the Gym environment. We learned
    about the continuous state space of the classic control environments and how they
    are stored in an array. We also learned how to balance a pole using a random agent.
    Later, we learned about interesting Atari game environments, and how Atari game
    environments are named in Gym, and then we explored their state space and action
    space. We also learned how to record the agent's gameplay using the wrapper class,
    and at the end of the chapter, we discovered other environments offered by Gym.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to find the optimal policy using two
    interesting algorithms called value iteration and policy iteration.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-438
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s evaluate our newly gained knowledge by answering the following questions:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: What is the use of a Gym toolkit?
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we create an environment in Gym?
  id: totrans-441
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we obtain the action space of the Gym environment?
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we visualize the Gym environment?
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name some classic control environments offered by Gym.
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we generate an episode using the Gym environment?
  id: totrans-445
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the state space of Atari Gym environments?
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we record the agent's gameplay?
  id: totrans-447
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-448
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check out the following resources for more information:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: To learn more about Gym, go to [http://gym.openai.com/docs/](http://gym.openai.com/docs/).
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can also check out the Gym repository to understand how Gym environments
    are coded: [https://github.com/openai/gym](https://github.com/openai/gym).'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
