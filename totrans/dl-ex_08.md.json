["```\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom urllib.request import urlretrieve\nfrom os.path import isfile, isdir\nfrom tqdm import tqdm\nimport tarfile\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder\n\nimport pickle\nimport tensorflow as tf\n```", "```\ncifar10_batches_dir_path = 'cifar-10-batches-py'\n\ntar_gz_filename = 'cifar-10-python.tar.gz'\n\nclass DLProgress(tqdm):\n    last_block = 0\n\n    def hook(self, block_num=1, block_size=1, total_size=None):\n        self.total = total_size\n        self.update((block_num - self.last_block) * block_size)\n        self.last_block = block_num\n\nif not isfile(tar_gz_filename):\n    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Python Images Batches') as pbar:\n        urlretrieve(\n            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n            tar_gz_filename,\n            pbar.hook)\n\nif not isdir(cifar10_batches_dir_path):\n    with tarfile.open(tar_gz_filename) as tar:\n        tar.extractall()\n        tar.close()\n```", "```\n# Defining a helper function for loading a batch of images\ndef load_batch(cifar10_dataset_dir_path, batch_num):\n\n    with open(cifar10_dataset_dir_path + '/data_batch_' + str(batch_num), mode='rb') as file:\n        batch = pickle.load(file, encoding='latin1')\n\n    input_features = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n    target_labels = batch['labels']\n\n    return input_features, target_labels\n```", "```\n#Defining a function to show the stats for batch ans specific sample\ndef batch_image_stats(cifar10_dataset_dir_path, batch_num, sample_num):\n\n    batch_nums = list(range(1, 6))\n\n    #checking if the batch_num is a valid batch number\n    if batch_num not in batch_nums:\n        print('Batch Num is out of Range. You can choose from these Batch nums: {}'.format(batch_nums))\n        return None\n\n    input_features, target_labels = load_batch(cifar10_dataset_dir_path, batch_num)\n\n    #checking if the sample_num is a valid sample number\n    if not (0 <= sample_num < len(input_features)):\n        print('{} samples in batch {}. {} is not a valid sample number.'.format(len(input_features), batch_num, sample_num))\n        return None\n\n    print('\\nStatistics of batch number {}:'.format(batch_num))\n    print('Number of samples in this batch: {}'.format(len(input_features)))\n    print('Per class counts of each Label: {}'.format(dict(zip(*np.unique(target_labels, return_counts=True)))))\n\n    image = input_features[sample_num]\n    label = target_labels[sample_num]\n    cifar10_class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n    print('\\nSample Image Number {}:'.format(sample_num))\n    print('Sample image - Minimum pixel value: {} Maximum pixel value: {}'.format(image.min(), image.max()))\n    print('Sample image - Shape: {}'.format(image.shape))\n    print('Sample Label - Label Id: {} Name: {}'.format(label, cifar10_class_names[label]))\n    plt.axis('off')\n    plt.imshow(image)\n```", "```\n# Explore a specific batch and sample from the dataset\nbatch_num = 3\nsample_num = 6\nbatch_image_stats(cifar10_batches_dir_path, batch_num, sample_num)\n```", "```\n\nStatistics of batch number 3:\nNumber of samples in this batch: 10000\nPer class counts of each Label: {0: 994, 1: 1042, 2: 965, 3: 997, 4: 990, 5: 1029, 6: 978, 7: 1015, 8: 961, 9: 1029}\n\nSample Image Number 6:\nSample image - Minimum pixel value: 30 Maximum pixel value: 242\nSample image - Shape: (32, 32, 3)\nSample Label - Label Id: 8 Name: ship\n```", "```\n#Normalize CIFAR-10 images to be in the range of [0,1]\n\ndef normalize_images(images):\n\n    # initial zero ndarray\n    normalized_images = np.zeros_like(images.astype(float))\n\n    # The first images index is number of images where the other indices indicates\n    # hieight, width and depth of the image\n    num_images = images.shape[0]\n\n    # Computing the minimum and maximum value of the input image to do the normalization based on them\n    maximum_value, minimum_value = images.max(), images.min()\n\n    # Normalize all the pixel values of the images to be from 0 to 1\n    for img in range(num_images):\n        normalized_images[img,...] = (images[img, ...] - float(minimum_value)) / float(maximum_value - minimum_value)\n\n    return normalized_images\n```", "```\n#encoding the input images. Each image will be represented by a vector of zeros except for the class index of the image \n# that this vector represents. The length of this vector depends on number of classes that we have\n# the dataset which is 10 in CIFAR-10\n\ndef one_hot_encode(images):\n\n    num_classes = 10\n\n    #use sklearn helper function of OneHotEncoder() to do that\n    encoder = OneHotEncoder(num_classes)\n\n    #resize the input images to be 2D\n    input_images_resized_to_2d = np.array(images).reshape(-1,1)\n    one_hot_encoded_targets = encoder.fit_transform(input_images_resized_to_2d)\n\n    return one_hot_encoded_targets.toarray()\n```", "```\ndef preprocess_persist_data(cifar10_batches_dir_path, normalize_images, one_hot_encode):\n\n    num_batches = 5\n    valid_input_features = []\n    valid_target_labels = []\n\n    for batch_ind in range(1, num_batches + 1):\n\n        #Loading batch\n        input_features, target_labels = load_batch(cifar10_batches_dir_path, batch_ind)\n        num_validation_images = int(len(input_features) * 0.1)\n\n        # Preprocess the current batch and perisist it for future use\n        input_features = normalize_images(input_features[:-num_validation_images])\n        target_labels = one_hot_encode( target_labels[:-num_validation_images])\n\n        #Persisting the preprocessed batch\n        pickle.dump((input_features, target_labels), open('preprocess_train_batch_' + str(batch_ind) + '.p', 'wb'))\n\n        # Define a subset of the training images to be used for validating our model\n        valid_input_features.extend(input_features[-num_validation_images:])\n        valid_target_labels.extend(target_labels[-num_validation_images:])\n\n    # Preprocessing and persisting the validationi subset\n    input_features = normalize_images( np.array(valid_input_features))\n    target_labels = one_hot_encode(np.array(valid_target_labels))\n\n    pickle.dump((input_features, target_labels), open('preprocess_valid.p', 'wb'))\n\n    #Now it's time to preporcess and persist the test batche\n    with open(cifar10_batches_dir_path + '/test_batch', mode='rb') as file:\n        test_batch = pickle.load(file, encoding='latin1')\n\n    test_input_features = test_batch['data'].reshape((len(test_batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n    test_input_labels = test_batch['labels']\n\n    # Normalizing and encoding the test batch\n    input_features = normalize_images( np.array(test_input_features))\n    target_labels = one_hot_encode(np.array(test_input_labels))\n\n    pickle.dump((input_features, target_labels), open('preprocess_test.p', 'wb'))\n\n# Calling the helper function above to preprocess and persist the training, validation, and testing set\npreprocess_persist_data(cifar10_batches_dir_path, normalize_images, one_hot_encode)\n```", "```\n# Load the Preprocessed Validation data\nvalid_input_features, valid_input_labels = pickle.load(open('preprocess_valid.p', mode='rb'))\n```", "```\n\n# Defining the model inputs\ndef images_input(img_shape):\n return tf.placeholder(tf.float32, (None, ) + img_shape, name=\"input_images\")\n\ndef target_input(num_classes):\n\n target_input = tf.placeholder(tf.int32, (None, num_classes), name=\"input_images_target\")\n return target_input\n\n#define a function for the dropout layer keep probability\ndef keep_prob_input():\n return tf.placeholder(tf.float32, name=\"keep_prob\")\n```", "```\n# Applying a convolution operation to the input tensor followed by max pooling\ndef conv2d_layer(input_tensor, conv_layer_num_outputs, conv_kernel_size, conv_layer_strides, pool_kernel_size, pool_layer_strides):\n\n input_depth = input_tensor.get_shape()[3].value\n weight_shape = conv_kernel_size + (input_depth, conv_layer_num_outputs,)\n\n #Defining layer weights and biases\n weights = tf.Variable(tf.random_normal(weight_shape))\n biases = tf.Variable(tf.random_normal((conv_layer_num_outputs,)))\n\n #Considering the biase variable\n conv_strides = (1,) + conv_layer_strides + (1,)\n\n conv_layer = tf.nn.conv2d(input_tensor, weights, strides=conv_strides, padding='SAME')\n conv_layer = tf.nn.bias_add(conv_layer, biases)\n\n conv_kernel_size = (1,) + conv_kernel_size + (1,)\n\n pool_strides = (1,) + pool_layer_strides + (1,)\n pool_layer = tf.nn.max_pool(conv_layer, ksize=conv_kernel_size, strides=pool_strides, padding='SAME')\n return pool_layer\n```", "```\n#Flatten the output of max pooling layer to be fing to the fully connected layer which only accepts the output\n# to be in 2D\ndef flatten_layer(input_tensor):\nreturn tf.contrib.layers.flatten(input_tensor)\n```", "```\n#Define the fully connected layer that will use the flattened output of the stacked convolution layers\n#to do the actuall classification\ndef fully_connected_layer(input_tensor, num_outputs):\n return tf.layers.dense(input_tensor, num_outputs)\n```", "```\n#Defining the output function\ndef output_layer(input_tensor, num_outputs):\n    return  tf.layers.dense(input_tensor, num_outputs)\n```", "```\ndef build_convolution_net(image_data, keep_prob):\n\n # Applying 3 convolution layers followed by max pooling layers\n conv_layer_1 = conv2d_layer(image_data, 32, (3,3), (1,1), (3,3), (3,3)) \n conv_layer_2 = conv2d_layer(conv_layer_1, 64, (3,3), (1,1), (3,3), (3,3))\n conv_layer_3 = conv2d_layer(conv_layer_2, 128, (3,3), (1,1), (3,3), (3,3))\n\n# Flatten the output from 4D to 2D to be fed to the fully connected layer\n flatten_output = flatten_layer(conv_layer_3)\n\n# Applying 2 fully connected layers with drop out\n fully_connected_layer_1 = fully_connected_layer(flatten_output, 64)\n fully_connected_layer_1 = tf.nn.dropout(fully_connected_layer_1, keep_prob)\n fully_connected_layer_2 = fully_connected_layer(fully_connected_layer_1, 32)\n fully_connected_layer_2 = tf.nn.dropout(fully_connected_layer_2, keep_prob)\n\n #Applying the output layer while the output size will be the number of categories that we have\n #in CIFAR-10 dataset\n output_logits = output_layer(fully_connected_layer_2, 10)\n\n #returning output\n return output_logits\n```", "```\n#Using the helper function above to build the network\n\n#First off, let's remove all the previous inputs, weights, biases form the previous runs\ntf.reset_default_graph()\n\n# Defining the input placeholders to the convolution neural network\ninput_images = images_input((32, 32, 3))\ninput_images_target = target_input(10)\nkeep_prob = keep_prob_input()\n\n# Building the models\nlogits_values = build_convolution_net(input_images, keep_prob)\n\n# Name logits Tensor, so that is can be loaded from disk after training\nlogits_values = tf.identity(logits_values, name='logits')\n\n# defining the model loss\nmodel_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits_values, labels=input_images_target))\n\n# Defining the model optimizer\nmodel_optimizer = tf.train.AdamOptimizer().minimize(model_cost)\n\n# Calculating and averaging the model accuracy\ncorrect_prediction = tf.equal(tf.argmax(logits_values, 1), tf.argmax(input_images_target, 1))\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='model_accuracy')\ntests.test_conv_net(build_convolution_net)\n```", "```\n#Define a helper function for kicking off the training process\ndef train(session, model_optimizer, keep_probability, in_feature_batch, target_batch):\nsession.run(model_optimizer, feed_dict={input_images: in_feature_batch, input_images_target: target_batch, keep_prob: keep_probability})\n```", "```\n#Defining a helper funcitno for print information about the model accuracy and it's validation accuracy as well\ndef print_model_stats(session, input_feature_batch, target_label_batch, model_cost, model_accuracy):\n\n    validation_loss = session.run(model_cost, feed_dict={input_images: input_feature_batch, input_images_target: target_label_batch, keep_prob: 1.0})\n    validation_accuracy = session.run(model_accuracy, feed_dict={input_images: input_feature_batch, input_images_target: target_label_batch, keep_prob: 1.0})\n\n    print(\"Valid Loss: %f\" %(validation_loss))\n    print(\"Valid accuracy: %f\" % (validation_accuracy))\n```", "```\n# Model Hyperparameters\nnum_epochs = 100\nbatch_size = 128\nkeep_probability = 0.5\n```", "```\n# Splitting the dataset features and labels to batches\ndef batch_split_features_labels(input_features, target_labels, train_batch_size):\n    for start in range(0, len(input_features), train_batch_size):\n        end = min(start + train_batch_size, len(input_features))\n        yield input_features[start:end], target_labels[start:end]\n\n#Loading the persisted preprocessed training batches\ndef load_preprocess_training_batch(batch_id, batch_size):\n    filename = 'preprocess_train_batch_' + str(batch_id) + '.p'\n    input_features, target_labels = pickle.load(open(filename, mode='rb'))\n\n    # Returning the training images in batches according to the batch size defined above\n    return batch_split_features_labels(input_features, target_labels, train_batch_size)\n```", "```\nprint('Training on only a Single Batch from the CIFAR-10 Dataset...')\nwith tf.Session() as sess:\n\n # Initializing the variables\n sess.run(tf.global_variables_initializer())\n\n # Training cycle\n for epoch in range(num_epochs):\n batch_ind = 1\n\n for batch_features, batch_labels in load_preprocess_training_batch(batch_ind, batch_size):\n train(sess, model_optimizer, keep_probability, batch_features, batch_labels)\n\n print('Epoch number {:>2}, CIFAR-10 Batch Number {}: '.format(epoch + 1, batch_ind), end='')\n print_model_stats(sess, batch_features, batch_labels, model_cost, accuracy)\n\nOutput:\n.\n.\n.\nEpoch number 85, CIFAR-10 Batch Number 1: Valid Loss: 1.490792\nValid accuracy: 0.550000\nEpoch number 86, CIFAR-10 Batch Number 1: Valid Loss: 1.487118\nValid accuracy: 0.525000\nEpoch number 87, CIFAR-10 Batch Number 1: Valid Loss: 1.309082\nValid accuracy: 0.575000\nEpoch number 88, CIFAR-10 Batch Number 1: Valid Loss: 1.446488\nValid accuracy: 0.475000\nEpoch number 89, CIFAR-10 Batch Number 1: Valid Loss: 1.430939\nValid accuracy: 0.550000\nEpoch number 90, CIFAR-10 Batch Number 1: Valid Loss: 1.484480\nValid accuracy: 0.525000\nEpoch number 91, CIFAR-10 Batch Number 1: Valid Loss: 1.345774\nValid accuracy: 0.575000\nEpoch number 92, CIFAR-10 Batch Number 1: Valid Loss: 1.425942\nValid accuracy: 0.575000\n\nEpoch number 93, CIFAR-10 Batch Number 1: Valid Loss: 1.451115\nValid accuracy: 0.550000\nEpoch number 94, CIFAR-10 Batch Number 1: Valid Loss: 1.368719\nValid accuracy: 0.600000\nEpoch number 95, CIFAR-10 Batch Number 1: Valid Loss: 1.336483\nValid accuracy: 0.600000\nEpoch number 96, CIFAR-10 Batch Number 1: Valid Loss: 1.383425\nValid accuracy: 0.575000\nEpoch number 97, CIFAR-10 Batch Number 1: Valid Loss: 1.378877\nValid accuracy: 0.625000\nEpoch number 98, CIFAR-10 Batch Number 1: Valid Loss: 1.343391\nValid accuracy: 0.600000\nEpoch number 99, CIFAR-10 Batch Number 1: Valid Loss: 1.319342\nValid accuracy: 0.625000\nEpoch number 100, CIFAR-10 Batch Number 1: Valid Loss: 1.340849\nValid accuracy: 0.525000\n```", "```\nmodel_save_path = './cifar-10_classification'\n\nwith tf.Session() as sess:\n # Initializing the variables\n sess.run(tf.global_variables_initializer())\n\n # Training cycle\n for epoch in range(num_epochs):\n\n # iterate through the batches\n num_batches = 5\n\n for batch_ind in range(1, num_batches + 1):\n for batch_features, batch_labels in load_preprocess_training_batch(batch_ind, batch_size):\n train(sess, model_optimizer, keep_probability, batch_features, batch_labels)\n\n print('Epoch number{:>2}, CIFAR-10 Batch Number {}: '.format(epoch + 1, batch_ind), end='')\n print_model_stats(sess, batch_features, batch_labels, model_cost, accuracy)\n\n # Save the trained Model\n saver = tf.train.Saver()\n save_path = saver.save(sess, model_save_path)\n\nOutput:\n.\n.\n.\nEpoch number94, CIFAR-10 Batch Number 5: Valid Loss: 0.316593\nValid accuracy: 0.925000\nEpoch number95, CIFAR-10 Batch Number 1: Valid Loss: 0.285429\nValid accuracy: 0.925000\nEpoch number95, CIFAR-10 Batch Number 2: Valid Loss: 0.347411\nValid accuracy: 0.825000\nEpoch number95, CIFAR-10 Batch Number 3: Valid Loss: 0.232483\nValid accuracy: 0.950000\nEpoch number95, CIFAR-10 Batch Number 4: Valid Loss: 0.294707\nValid accuracy: 0.900000\nEpoch number95, CIFAR-10 Batch Number 5: Valid Loss: 0.299490\nValid accuracy: 0.975000\nEpoch number96, CIFAR-10 Batch Number 1: Valid Loss: 0.302191\nValid accuracy: 0.950000\nEpoch number96, CIFAR-10 Batch Number 2: Valid Loss: 0.347043\nValid accuracy: 0.750000\nEpoch number96, CIFAR-10 Batch Number 3: Valid Loss: 0.252851\nValid accuracy: 0.875000\nEpoch number96, CIFAR-10 Batch Number 4: Valid Loss: 0.291433\nValid accuracy: 0.950000\nEpoch number96, CIFAR-10 Batch Number 5: Valid Loss: 0.286192\nValid accuracy: 0.950000\nEpoch number97, CIFAR-10 Batch Number 1: Valid Loss: 0.277105\nValid accuracy: 0.950000\nEpoch number97, CIFAR-10 Batch Number 2: Valid Loss: 0.305842\nValid accuracy: 0.850000\nEpoch number97, CIFAR-10 Batch Number 3: Valid Loss: 0.215272\nValid accuracy: 0.950000\nEpoch number97, CIFAR-10 Batch Number 4: Valid Loss: 0.313761\nValid accuracy: 0.925000\nEpoch number97, CIFAR-10 Batch Number 5: Valid Loss: 0.313503\nValid accuracy: 0.925000\nEpoch number98, CIFAR-10 Batch Number 1: Valid Loss: 0.265828\nValid accuracy: 0.925000\nEpoch number98, CIFAR-10 Batch Number 2: Valid Loss: 0.308948\nValid accuracy: 0.800000\nEpoch number98, CIFAR-10 Batch Number 3: Valid Loss: 0.232083\nValid accuracy: 0.950000\nEpoch number98, CIFAR-10 Batch Number 4: Valid Loss: 0.298826\nValid accuracy: 0.925000\nEpoch number98, CIFAR-10 Batch Number 5: Valid Loss: 0.297230\nValid accuracy: 0.950000\nEpoch number99, CIFAR-10 Batch Number 1: Valid Loss: 0.304203\nValid accuracy: 0.900000\nEpoch number99, CIFAR-10 Batch Number 2: Valid Loss: 0.308775\nValid accuracy: 0.825000\nEpoch number99, CIFAR-10 Batch Number 3: Valid Loss: 0.225072\nValid accuracy: 0.925000\nEpoch number99, CIFAR-10 Batch Number 4: Valid Loss: 0.263737\nValid accuracy: 0.925000\nEpoch number99, CIFAR-10 Batch Number 5: Valid Loss: 0.278601\nValid accuracy: 0.950000\nEpoch number100, CIFAR-10 Batch Number 1: Valid Loss: 0.293509\nValid accuracy: 0.950000\nEpoch number100, CIFAR-10 Batch Number 2: Valid Loss: 0.303817\nValid accuracy: 0.875000\nEpoch number100, CIFAR-10 Batch Number 3: Valid Loss: 0.244428\nValid accuracy: 0.900000\nEpoch number100, CIFAR-10 Batch Number 4: Valid Loss: 0.280712\nValid accuracy: 0.925000\nEpoch number100, CIFAR-10 Batch Number 5: Valid Loss: 0.278625\nValid accuracy: 0.950000\n```", "```\n#A helper function to visualize some samples and their corresponding predictions\ndef display_samples_predictions(input_features, target_labels, samples_predictions):\n\n num_classes = 10\n\n cifar10_class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\nlabel_binarizer = LabelBinarizer()\n label_binarizer.fit(range(num_classes))\n label_inds = label_binarizer.inverse_transform(np.array(target_labels))\n\nfig, axies = plt.subplots(nrows=4, ncols=2)\n fig.tight_layout()\n fig.suptitle('Softmax Predictions', fontsize=20, y=1.1)\n\nnum_predictions = 4\n margin = 0.05\n ind = np.arange(num_predictions)\n width = (1\\. - 2\\. * margin) / num_predictions\n\nfor image_ind, (feature, label_ind, prediction_indicies, prediction_values) in enumerate(zip(input_features, label_inds, samples_predictions.indices, samples_predictions.values)):\n prediction_names = [cifar10_class_names[pred_i] for pred_i in prediction_indicies]\n correct_name = cifar10_class_names[label_ind]\n\naxies[image_ind][0].imshow(feature)\n axies[image_ind][0].set_title(correct_name)\n axies[image_ind][0].set_axis_off()\n\naxies[image_ind][1].barh(ind + margin, prediction_values[::-1], width)\n axies[image_ind][1].set_yticks(ind + margin)\n axies[image_ind][1].set_yticklabels(prediction_names[::-1])\n axies[image_ind][1].set_xticks([0, 0.5, 1.0])\n```", "```\ntest_batch_size = 64\nsave_model_path = './cifar-10_classification'\n#Number of images to visualize\nnum_samples = 4\n\n#Number of top predictions\ntop_n_predictions = 4\n\n#Defining a helper function for testing the trained model\ndef test_classification_model():\n\n input_test_features, target_test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n loaded_graph = tf.Graph()\nwith tf.Session(graph=loaded_graph) as sess:\n\n # loading the trained model\n model = tf.train.import_meta_graph(save_model_path + '.meta')\n model.restore(sess, save_model_path)\n\n# Getting some input and output Tensors from loaded model\n model_input_values = loaded_graph.get_tensor_by_name('input_images:0')\n model_target = loaded_graph.get_tensor_by_name('input_images_target:0')\n model_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n model_logits = loaded_graph.get_tensor_by_name('logits:0')\n model_accuracy = loaded_graph.get_tensor_by_name('model_accuracy:0')\n\n # Testing the trained model on the test set batches\n test_batch_accuracy_total = 0\n test_batch_count = 0\n\n for input_test_feature_batch, input_test_label_batch in batch_split_features_labels(input_test_features, target_test_labels, test_batch_size):\n test_batch_accuracy_total += sess.run(\n model_accuracy,\n feed_dict={model_input_values: input_test_feature_batch, model_target: input_test_label_batch, model_keep_prob: 1.0})\n test_batch_count += 1\n\nprint('Test set accuracy: {}\\n'.format(test_batch_accuracy_total/test_batch_count))\n\n# print some random images and their corresponding predictions from the test set results\n random_input_test_features, random_test_target_labels = tuple(zip(*random.sample(list(zip(input_test_features, target_test_labels)), num_samples)))\n\n random_test_predictions = sess.run(\n tf.nn.top_k(tf.nn.softmax(model_logits), top_n_predictions),\n feed_dict={model_input_values: random_input_test_features, model_target: random_test_target_labels, model_keep_prob: 1.0})\n\n display_samples_predictions(random_input_test_features, random_test_target_labels, random_test_predictions)\n\n#Calling the function\ntest_classification_model()\n\nOutput:\nINFO:tensorflow:Restoring parameters from ./cifar-10_classification\nTest set accuracy: 0.7540007961783439\n```"]