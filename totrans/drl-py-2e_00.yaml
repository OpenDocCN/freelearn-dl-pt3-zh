- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With significant enhancement in the quality and quantity of algorithms in recent
    years, this second edition of *Hands-On Reinforcement Learning with Python* has
    been revamped into an example-rich guide to learning state-of-the-art **reinforcement
    learning** (**RL**) and deep RL algorithms with TensorFlow 2 and the OpenAI Gym
    toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to exploring RL basics and foundational concepts such as the Bellman
    equation, Markov decision processes, and dynamic programming, this second edition
    dives deep into the full spectrum of value-based, policy-based, and actor-critic
    RL methods. It explores state-of-the-art algorithms such as DQN, TRPO, PPO and
    ACKTR, DDPG, TD3, and SAC in depth, demystifying the underlying math and demonstrating
    implementations through simple code examples.
  prefs: []
  type: TYPE_NORMAL
- en: The book has several new chapters dedicated to new RL techniques including distributional
    RL, imitation learning, inverse RL, and meta RL. You will learn to leverage Stable
    Baselines, an improvement of OpenAI's baseline library, to implement popular RL
    algorithms effortlessly. The book concludes with an overview of promising approaches
    such as meta-learning and imagination augmented agents in research.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're a machine learning developer with little or no experience with neural
    networks interested in artificial intelligence and want to learn about reinforcement
    learning from scratch, this book is for you. Basic familiarity with linear algebra,
    calculus, and Python is required. Some experience with TensorFlow would be a plus.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Chapter 1*, *Fundamentals of Reinforcement Learning*, helps you build a strong
    foundation on RL concepts. We will learn about the key elements of RL, the Markov
    decision process, and several important fundamental concepts such as action spaces,
    policies, episodes, the value function, and the Q function. At the end of the
    chapter, we will learn about some of the interesting applications of RL and we
    will also look into the key terms and terminologies frequently used in RL.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 2*, *A Guide to the Gym Toolkit*, provides a complete guide to OpenAI''s
    Gym toolkit. We will understand several interesting environments provided by Gym
    in detail by implementing them. We will begin our hands-on RL journey from this
    chapter by implementing several fundamental RL concepts using Gym.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 3*, *The Bellman Equation and Dynamic Programming*, will help us understand
    the Bellman equation in detail with extensive math. Next, we will learn two interesting
    classic RL algorithms called the value and policy iteration methods, which we
    can use to find the optimal policy. We will also see how to implement value and
    policy iteration methods for solving the Frozen Lake problem.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 4*, *Monte Carlo Methods*, explains the model-free method, Monte Carlo.
    We will learn what prediction and control tasks are, and then we will look into
    Monte Carlo prediction and Monte Carlo control methods in detail. Next, we will
    implement the Monte Carlo method to solve the blackjack game using the Gym toolkit.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 5*, *Understanding Temporal Difference Learning*, deals with one of
    the most popular and widely used model-free methods called **Temporal Difference**
    (**TD**) learning. First, we will learn how the TD prediction method works in
    detail, and then we will explore the on-policy TD control method called SARSA
    and the off-policy TD control method called Q learning in detail. We will also
    implement TD control methods to solve the Frozen Lake problem using Gym.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 6*, *Case Study – The MAB Problem*, explains one of the classic problems
    in RL called the **multi-armed bandit** (**MAB**) problem. We will start the chapter
    by understanding what the MAB problem is and then we will learn about several
    exploration strategies such as epsilon-greedy, softmax exploration, upper confidence
    bound, and Thompson sampling methods for solving the MAB problem in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 7*, *Deep Learning Foundations*, helps us to build a strong foundation
    on deep learning. We will start the chapter by understanding how artificial neural
    networks work. Then we will learn several interesting deep learning algorithms,
    such as recurrent neural networks, LSTM networks, convolutional neural networks,
    and generative adversarial networks.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 8*, *A Primer on TensorFlow*, deals with one of the most popular deep
    learning libraries called TensorFlow. We will understand how to use TensorFlow
    by implementing a neural network to recognize handwritten digits. Next, we will
    learn to perform several math operations using TensorFlow. Later, we will learn
    about TensorFlow 2.0 and see how it differs from the previous TensorFlow versions.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 9*, *Deep Q Network and Its Variants*, enables us to kick-start our
    deep RL journey. We will learn about one of the most popular deep RL algorithms
    called the **Deep Q Network** (**DQN**). We will understand how DQN works step
    by step along with the extensive math. We will also implement a DQN to play Atari
    games. Next, we will explore several interesting variants of DQN, called Double
    DQN, Dueling DQN, DQN with prioritized experience replay, and DRQN.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 10*, *Policy Gradient Method*, covers policy gradient methods. We
    will understand how the policy gradient method works along with the detailed derivation.
    Next, we will learn several variance reduction methods such as policy gradient
    with reward-to-go and policy gradient with baseline. We will also understand how
    to train an agent for the Cart Pole balancing task using policy gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 11*, *Actor-Critic Methods – A2C and A3C*, deals with several interesting
    actor-critic methods such as advantage actor-critic and asynchronous advantage
    actor-critic. We will learn how these actor-critic methods work in detail, and
    then we will implement them for a mountain car climbing task using OpenAI Gym.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 12*, *Learning DDPG, TD3, and SAC*, covers state-of-the-art deep RL
    algorithms such as deep deterministic policy gradient, twin delayed DDPG, and
    soft actor, along with step by step derivation. We will also learn how to implement
    the DDPG algorithm for performing the inverted pendulum swing-up task using Gym.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 13*, *TRPO, PPO, and ACKTR Methods*, deals with several popular policy
    gradient methods such as TRPO and PPO. We will dive into the math behind TRPO
    and PPO step by step and understand how TRPO and PPO helps an agent find the optimal
    policy. Next, we will learn to implement PPO for performing the inverted pendulum
    swing-up task. At the end, we will learn about the actor-critic method called
    actor-critic using Kronecker-Factored trust region in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 14*, *Distributional Reinforcement Learning*, covers distributional
    RL algorithms. We will begin the chapter by understanding what distributional
    RL is. Then we will explore several interesting distributional RL algorithms such
    as categorical DQN, quantile regression DQN, and distributed distributional DDPG.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 15*, *Imitation Learning and Inverse RL*, explains imitation and inverse
    RL algorithms. First, we will understand how supervised imitation learning, DAgger,
    and deep Q learning from demonstrations work in detail. Next, we will learn about
    maximum entropy inverse RL. At the end of the chapter, we will learn about generative
    adversarial imitation learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 16*, *Deep Reinforcement Learning with Stable Baselines*, helps us
    to understand how to implement deep RL algorithms using a library called Stable
    Baselines. We will learn what Stable Baselines is and how to use it in detail
    by implementing several interesting Deep RL algorithms such as DQN, A2C, DDPG
    TRPO, and PPO.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 17*, *Reinforcement Learning Frontiers*, covers several interesting
    avenues in RL, such as meta RL, hierarchical RL, and imagination augmented agents
    in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You need the following software for this book:'
  prefs: []
  type: TYPE_NORMAL
- en: Anaconda
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any web browser
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can download the example code files for this book from your account at [http://www.packtpub.com](http://www.packtpub.com).
    If you purchased this book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the code files by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in or register at [http://www.packtpub.com](http://www.packtpub.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **SUPPORT** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Code Downloads & Errata**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the name of the book in the **Search** box and follow the on-screen instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  prefs: []
  type: TYPE_NORMAL
- en: WinRAR / 7-Zip for Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipeg / iZip / UnRarX for Mac
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7-Zip / PeaZip for Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-with-Python).
    We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781839210686_ColorImages.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    For example: "`epsilon_greedy` computes the optimal policy."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see on
    the screen, for example, in menus or dialog boxes, also appear in the text like
    this. For example: "The **Markov Reward Process** (**MRP**) is an extension of
    the Markov chain with the reward function."'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: Email `feedback@packtpub.com`, and mention the book''s
    title in the subject of your message. If you have questions about any aspect of
    this book, please email us at `questions@packtpub.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book we would
    be grateful if you would report this to us. Please visit, [http://www.packtpub.com/submit-errata](http://www.packtpub.com/submit-errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packtpub.com` with a
    link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [http://authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Reviews
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Packt, please visit [packtpub.com](http://packtpub.com).
  prefs: []
  type: TYPE_NORMAL
