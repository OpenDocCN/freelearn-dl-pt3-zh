<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Playing Pacman Using Deep Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning refers to a paradigm where an agent learns from environment feedback by virtue of receiving observations and rewards in return for actions it takes. The following diagram captures the feedback-based learning loop of reinforcement learning:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1041 image-border" src="assets/185ca50f-32f0-4e67-9d9b-200eb4c15771.png" style="width:25.25em;height:15.83em;"/></p>
<p class="CDPAlignLeft CDPAlign">Although mostly applied to learn how to play games, reinforcement learning has also been successfully applied in digital advertising, stock trading, self-driving cars, and industrial robots.</p>
<p>In this chapter, we will use reinforcement learning to create a PacMan game and learn about reinforcement learning in the process. We will cover the following topics: </p>
<ul>
<li>Reinforcement learning</li>
<li>Reinforcement learning  versus supervised and unsupervised learning</li>
<li>Components of reinforcement learning</li>
<li>OpenAI Gym</li>
<li>A PacMan game in OpenAI Gym</li>
<li>DQN for deep reinforcement learning:
<ul>
<li>Q Learning</li>
<li>Deep Q Network</li>
</ul>
</li>
<li>Applying DQN to a PacMan game</li>
</ul>
<p class="mce-root">Let's get started!</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is a type of machine learning in which an agent learns from the environment. The agent takes actions and, as a result of the actions, the environment returns observations and rewards. From the observation and rewards, the agent learns the policy and takes further actions, thus continuing the sequence of actions, observations, and rewards. In the long run, the agent has to learn the policy such that, when it takes actions based on the policy, it does so in such a way as to maximize the long-term rewards. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning versus supervised and unsupervised learning</h1>
                </header>
            
            <article>
                
<p>Machine learning solutions can be of three major types: supervised learning, unsupervised learning, and reinforcement learning. So how is reinforcement learning different from the other two types?</p>
<ol>
<li><strong>Supervised learning</strong>: I<span>n supervised learning, the agent learns the model from a training dataset consisting of features and labels. The two most common types of supervised learning problems are regression and classification. Regression refers to predicting the future values based on the model, and classification refers to predicting the categories of the input values.</span></li>
<li><strong>Unsupervised learning</strong>: I<span>n unsupervised learning, the agent learns the model from a training dataset consisting of only features. The two most common types of unsupervised learning problems are dimensionality reduction and clustering. Dimensionality reduction refers to reducing the number of features or dimensions in a dataset without altering its natural distribution. Clustering refers to dividing the input data into multiple groups, thus producing clusters or segments.</span></li>
</ol>
<ol start="3">
<li><strong>Reinforcement learning</strong>: In reinforcement learning, the agent starts with an initial model and then continuously learns the model based on feedback from the environment. An RL agent updates the model by applying supervised or unsupervised learning techniques on a sequence of actions, observations, and rewards. The agent only learns from a reward signal, not from a loss function as in other <span>machine learning</span> approaches. The agent receives the feedback after it has already taken the action, while, in other ML approaches, the feedback is provided at the time of training in terms of loss or error. The data is not i.i.d. (independent and identically distributed) because it depends on previous actions taken, while in other ML approaches data is i.i.d.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Components of Reinforcement Learning</h1>
                </header>
            
            <article>
                
<p>In any RL formalization, we talk in terms of a <strong>state space</strong> and an <strong>action space</strong>. Action space is a set of finite numbers of actions that can be taken by the agent, represented by <em>A</em>. State space is a finite set of states that the environment can be in, represented by <em>S</em>.</p>
<p>The goal of the agent is to learn a policy, denoted by <img class="fm-editor-equation" src="assets/2d80d9e2-ee59-47b1-90c6-5050457942b1.png" style="width:0.92em;height:1.00em;"/>. A <strong>policy</strong> can be <strong>deterministic</strong> or <strong>stochastic</strong>. A policy basically represents the model, using which the agent to  select the best action to take. Thus, the policy maps the rewards and observations received from the environment to actions.</p>
<p>When an agent follows a policy, it results in a sequence of state, action, reward, state, and so on. This sequence is known as a <strong>trajectory</strong> or an <strong>episode</strong>.</p>
<p>An important component of reinforcement learning formalizations is the <strong>return</strong>. The return is the estimate of the total long-term reward. Generally, the return can be represented by the following formula:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8d82b310-321c-4ff0-82b6-6ca9742f366b.png" style="width:7.67em;height:4.42em;"/></p>
<p>Here <img class="fm-editor-equation" src="assets/77a691d7-b626-4fd0-98fc-5acebc7745d2.png" style="width:0.83em;height:1.25em;"/> is a discount factor with values between (0,1), and <img class="fm-editor-equation" src="assets/48e152e1-7cdc-4d4d-9d55-68e496294f47.png" style="width:1.25em;height:1.17em;"/> is the reward at time step <em>t</em>. The discount factor represents how much importance should be given to the reward at later time steps. If <img class="fm-editor-equation" src="assets/5187e85e-81f4-4c40-81a5-a70743fec3cd.png" style="width:0.83em;height:1.25em;"/> is 0 then only the rewards from the next action are considered, and if it is 1 then the future rewards have the same weight as the rewards from the next action.</p>
<p>However, since it is difficult to compute the value of the return, hence it is estimated with <strong>state-value</strong> or <strong>action-value</strong> functions. We shall talk about action-value functions further in the q-learning section in this chapter.</p>
<p class="mce-root"/>
<p>For simulating our agent which will play the PacMan game, we shall be using the OpenAI Gym. Let's learn about OpenAI Gym now.</p>
<div class="packt_infobox"><span>You can follow along with the code in the</span> <span>Jupyter Notebook </span><span> </span><kbd>ch-14_Reinforcement_Learning</kbd><span> in the code b</span><span>undle of this book.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenAI Gym </h1>
                </header>
            
            <article>
                
<p>OpenAI Gym is a Python-based toolkit for the<span> </span>development<span> </span>of reinforcement learning algorithms. It provides more than 700 open source contributed environments at the time of writing this book. Custom environments for OpenAI can also be created. OpenAI Gym provides a unified interface for working with reinforcement learning environments and takes care of running the simulation, while the user of OpenAI can<span> </span>focus<span> </span>on designing and implementing the reinforcement learning algorithms.</p>
<div class="packt_infobox"><span>The original research paper on OpenAI Gym is available at the following link: </span><a href="http://arxiv.org/abs/1606.01540">http://arxiv.org/abs/1606.01540</a><span>.</span></div>
<p>Let's take a look at the following steps to learn how to install and explore OpenAI Gym:</p>
<ol start="1">
<li>Install OpenAI Gym using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>pip3 install gym</strong></pre>
<div class="packt_infobox">If the preceding command does not work, then refer to the following link for further help with installation: <a href="https://github.com/openai/gym#installation">https://github.com/openai/gym#installation</a>. </div>
<ol start="2">
<li>Print the number of available environments in the OpenAI Gym with the following code:</li>
</ol>
<pre style="padding-left: 60px">all_env = list(gym.envs.registry.all())<br/>print('Total Environments in Gym version {} : {}'<br/>    .format(gym.__version__,len(all_env)))<br/><br/></pre>
<p class="mce-root" style="padding-left: 60px">The preceding code generates the following output:</p>
<pre class="mce-root" style="padding-left: 60px"><strong>Total Environments in Gym version 0.10.5 : 797</strong></pre>
<p class="mce-root"/>
<ol start="3">
<li>Print the list of all environments, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">for e in list(all_env):<br/>    print(e)</pre>
<p style="padding-left: 60px">The partial list from the output is as follows:</p>
<pre style="padding-left: 60px"><strong>EnvSpec(Copy-v0) EnvSpec(RepeatCopy-v0) EnvSpec(ReversedAddition-v0) EnvSpec(ReversedAddition3-v0) EnvSpec(DuplicatedInput-v0) EnvSpec(Reverse-v0) EnvSpec(CartPole-v0) EnvSpec(CartPole-v1) EnvSpec(MountainCar-v0) EnvSpec(MountainCarContinuous-v0) EnvSpec(Pendulum-v0)</strong></pre>
<p>Each environment, represented by the<span> </span><kbd>env</kbd><span> </span>object, has a standardized interface:</p>
<ul>
<li>An<span> </span><kbd>env</kbd><span> </span>object can be created with the <kbd>env.make(&lt;game-id-string&gt;)</kbd><span> </span>function by passing the <kbd>id</kbd> string.</li>
<li>Each<span> </span><kbd>env</kbd><span> </span>object contains the following main functions:
<ul style="padding-left: 2px">
<li>The<span> </span><kbd>step()</kbd> function takes an action object as an argument and returns four objects:
<ul style="padding-left: 1px">
<li><kbd>observation</kbd>: An object implemented by the environment, representing the observation of the environment.</li>
<li><kbd>reward</kbd>: A signed float value indicating the gain (or loss) from the previous action.</li>
<li><kbd>done</kbd>: A Boolean value representing whether or not the scenario is finished.</li>
<li><kbd>info</kbd>: A Python dictionary object representing the diagnostic information.</li>
</ul>
</li>
<li>The<span> </span><kbd>render()</kbd><span> </span>function creates a visual representation of the environment.</li>
<li>The <kbd>reset()</kbd><span> </span>function resets the environment to the original state.</li>
</ul>
</li>
<li>Each<span> </span><kbd>env</kbd><span> </span>object comes with well-defined actions and observations, represented by<span> </span><kbd>action_space</kbd><span> </span>and<span> </span><kbd>observation_space</kbd>.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Pacman game in OpenAI Gym </h1>
                </header>
            
            <article>
                
<p>In this chapter, we will use the PacMan game as an example, known as <strong>MsPacman-v0</strong>. Let's explore this game a bit further:</p>
<ol>
<li>Create the<span> </span><kbd>env</kbd><span> </span>object with the standard<span> </span><kbd>make</kbd><span> </span>function, as shown in the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>env=gym.make('MsPacman-v0')</strong></pre>
<ol start="2">
<li>Let's print the action space of the game with the following code:</li>
</ol>
<pre style="padding-left: 60px">print(env.action_space)</pre>
<p style="padding-left: 60px">The preceding code generates the following output:</p>
<pre class="mce-root" style="padding-left: 60px"><strong>Discrete(9)</strong></pre>
<div class="packt_infobox"><kbd>Discrete 9</kbd> refers to the nine actions, such as up, down, left, and right.</div>
<ol start="3">
<li>We can now see the observation space, as shown in the following example:</li>
</ol>
<pre style="padding-left: 60px">print(env.observation_space)</pre>
<p style="padding-left: 60px">The preceding code generates the following output:</p>
<pre class="mce-root" style="padding-left: 60px"><strong>Box(210, 160, 3)</strong></pre>
<p style="padding-left: 60px">Thus, the observation space has three color channels and is of size 210 x 160. The observation space gets rendered as in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1042 image-border" src="assets/a5d91e17-360f-4dec-97be-4aad1eb1ad46.png" style="width:27.92em;height:35.58em;"/></p>
<ol start="4">
<li>The number of episodes is the number of game plays. We shall set it to one, for now, indicating that we just want to play the game once. Since every episode is stochastic, in actual production runs you will run over several episodes and calculate the average values of the rewards. Let's run the game for one episode while randomly selecting one of the actions during the gameplay with the following code:</li>
</ol>
<pre style="padding-left: 90px">import time<br/><br/>frame_time = 1.0 / 15 # seconds<br/>n_episodes = 1<br/><br/>for i_episode in range(n_episodes):<br/>    t=0<br/>    score=0<br/>    then = 0<br/>    done = False<br/>    env.reset()<br/>    while not done:<br/>        now = time.time()<br/>        if frame_time &lt; now - then:<br/>            action = env.action_space.sample()<br/>            observation, reward, done, info = env.step(action)<br/>            score += reward<br/>            env.render()<br/>            then = now<br/>            t=t+1<br/>    print('Episode {} finished at t {} with score {}'.format(i_episode,<br/>                                                             t,score))</pre>
<p style="padding-left: 60px">We then get the following output:</p>
<pre style="padding-left: 60px">Episode 0 finished at t 551 with score 100.0</pre>
<ol start="5">
<li>Now, let's run, the game <kbd>500</kbd> times and see what maximum, minimum, and average scores we get. This is demonstrated in the following example:</li>
</ol>
<pre style="padding-left: 60px">import time<br/>import numpy as np<br/><br/>frame_time = 1.0 / 15 # seconds<br/>n_episodes = 500<br/><br/>scores = []<br/>for i_episode in range(n_episodes):<br/>    t=0<br/>    score=0<br/>    then = 0<br/>    done = False<br/>    env.reset()<br/>    while not done:<br/>        now = time.time()<br/>        if frame_time &lt; now - then:<br/>            action = env.action_space.sample()<br/>            observation, reward, done, info = env.step(action)<br/>            score += reward<br/>            env.render()<br/>            then = now<br/>            t=t+1<br/>    scores.append(score)<br/>    #print("Episode {} finished at t {} with score {}".format(i_episode,t,score))<br/>print('Average score {}, max {}, min {}'.format(np.mean(scores),<br/>                                          np.max(scores),<br/>                                          np.min(scores)<br/>                                         ))</pre>
<p style="padding-left: 60px">The preceding code generates the following output:</p>
<pre class="mce-root" style="padding-left: 60px"><strong>Average 219.46, max 1070.0, min 70.0</strong></pre>
<p>Randomly picking an action and applying it is probably not the best strategy. There are many algorithms for finding solutions to make the agent learn from playing the game and apply the best actions. In this chapter, we shall apply Deep Q Network for learning from the game. The reader is encouraged to explore other algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DQN for deep reinforcement learning</h1>
                </header>
            
            <article>
                
<p>The <strong>Deep Q Networks</strong> (<strong>DQN</strong>) are based on Q-learning. In this section, we will explain both of them before we implement the DQN in Keras to play the PacMan game.</p>
<ul>
<li><strong>Q-learning</strong>: In Q-learning, the agent learns the action-value function, also known as the Q-function. The <em>Q</em> function denoted with <em>q(s</em><span>,</span><em>a)</em> <span>is used to estimate the long-term value of taking an action</span> <em>a</em><span> when the agent is in state</span> <em>s</em><span>. The <em>Q</em> function maps the state-action pairs to the estimates of long-term values, as shown in the following equation:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/eab73493-acce-4360-8164-a86d11c95447.png" style="width:8.58em;height:1.42em;"/></p>
<p style="padding-left: 90px">Thus, under a policy, the <em>q</em>-value function can be written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/28dd49bb-5f0b-43ea-96dc-6a94209695cb.png" style="width:15.33em;height:1.75em;"/></p>
<p style="padding-left: 90px">The <em>q</em> function can be recursively written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3041f8dd-a123-4a77-bcf0-0206b13f832a.png" style="width:28.25em;height:1.75em;"/></p>
<p style="padding-left: 90px">The expectation can be expanded as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c71a670c-3b18-47a3-8821-2c15ac30b20e.png" style="width:36.75em;height:3.67em;"/></p>
<p style="padding-left: 90px">An optimal <em>q</em> function is the one that returns the maximum value, and an optimal policy is the one that applies the optimal <em>q</em> function. The optimal <em>q</em> function can be written as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f4c59cf4-9276-49c5-b00a-950c70652361.png" style="width:31.25em;height:3.33em;"/></p>
<p style="padding-left: 60px">This equation represents the <strong>Bellman Optimality Equation</strong>. Since directly solving this equation is difficult, Q-learning is one of the methods used to approximate the value of this function.</p>
<p style="padding-left: 60px">Thus, in Q-learning, a model is built that can predict this value, given the state and action. Generally, this model is in the form of a table that contains all the possible combinations of state <em>s</em> and action <em>a</em>, and the expected value from that combination. However, for situations with a large number of state-action combinations, this table becomes cumbersome to maintain. The DQN helps to overcome this shortcoming of table-based Q-learning.</p>
<ul>
<li>The DQN: In DQN, instead of tables, a neural network model is built that learns from the state-action-reward-next state tuples and predicts the approximate q-value based on the state and action provided. Since the sequence of states-action-rewards is correlated in time, deep learning faces the challenge, since, in deep learning, the input samples need to be i.i.d. Thus, in DQN algorithms, <strong>experience replay</strong> is used to alleviate that. In the experience replay<span>, the previous actions and their results are sampled randomly to train the network.</span></li>
</ul>
<p>The basic Deep Q-learning algorithm is as follows:</p>
<ol>
<li>Start the play in its initial state</li>
<li>Select to explore or exploit</li>
<li>If you selected exploit, then predict the action with the neural network and take the predicted action</li>
<li>If you selected explore, then randomly select an action</li>
<li>Record the previous state, action, rewards, and next state in the experience buffer</li>
<li>Update the <kbd>q_values</kbd> using <kbd>bellman</kbd> function</li>
<li>Train the neural network with <kbd>states</kbd>, <kbd>actions</kbd>, and <kbd>q_values</kbd></li>
<li>Repeat from <em>step 2</em></li>
</ol>
<p>To improve the performance, and implement experience replay, one of the things you can do is to randomly select the training data in <em>step 7</em>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying DQN to a game</h1>
                </header>
            
            <article>
                
<p>So far, we have randomly picked an action<span> </span>and<span> </span>applied it to the game. Now, let's apply DQN for selecting actions for playing the PacMan game.</p>
<ol>
<li>We define the <kbd>q_nn</kbd> policy function as follows:</li>
</ol>
<pre style="padding-left: 60px">def policy_q_nn(obs, env):<br/>    # Exploration strategy - Select a random action<br/>    if np.random.random() &lt; explore_rate:<br/>        action = env.action_space.sample()<br/>    # Exploitation strategy - Select the action with the highest q<br/>    else:<br/>        action = np.argmax(q_nn.predict(np.array([obs])))<br/>    return action</pre>
<ol start="2">
<li><span>Next, we modify the <kbd>episode</kbd> function to incorporate calculation of <kbd>q_values</kbd> and <kbd>train</kbd> the neural network on the sampled experience buffer. This is shown in the following code:</span></li>
</ol>
<pre style="padding-left: 60px">def episode(env, policy, r_max=0, t_max=0):<br/><br/>    # create the empty list to contain game memory<br/>    #memory = deque(maxlen=1000)<br/>    <br/>    # observe initial state<br/>    obs = env.reset()<br/>    state_prev = obs<br/>    #state_prev = np.ravel(obs) # replaced with keras reshape[-1]<br/>    <br/>    # initialize the variables<br/>    episode_reward = 0<br/>    done = False<br/>    t = 0<br/>    <br/>    while not done:<br/>        <br/>        action = policy(state_prev, env)<br/>        obs, reward, done, info = env.step(action)<br/>        state_next = obs<br/>        #state_next = np.ravel(obs) # replaced with keras reshape[-1]<br/>        <br/>                                             <br/>        # add the state_prev, action, reward, state_new, done to memory<br/>        memory.append([state_prev,action,reward,state_next,done])<br/>                           <br/>        <br/>        # Generate and update the q_values with <br/>        # maximum future rewards using bellman function:<br/>        states = np.array([x[0] for x in memory])<br/>        states_next = np.array([np.zeros(n_shape) if x[4] else x[3] for x in memory])<br/>        <br/>        q_values = q_nn.predict(states)<br/>        q_values_next = q_nn.predict(states_next)<br/>        <br/>        for i in range(len(memory)):<br/>            state_prev,action,reward,state_next,done = memory[i]<br/>            if done:<br/>                q_values[i,action] = reward<br/>            else:<br/>                best_q = np.amax(q_values_next[i])<br/>                bellman_q = reward + discount_rate * best_q<br/>                q_values[i,action] = bellman_q<br/>        <br/>        # train the q_nn with states and q_values, same as updating the q_table<br/>        q_nn.fit(states,q_values,epochs=1,batch_size=50,verbose=0)<br/>    <br/>        state_prev = state_next<br/>        <br/>        episode_reward += reward<br/>        if r_max &gt; 0 and episode_reward &gt; r_max:<br/>            break<br/>        t+=1<br/>        if t_max &gt; 0 and t == t_max:<br/>            break<br/>    return episode_reward</pre>
<ol start="3">
<li>Define an <kbd>ex</kbd><kbd>periment</kbd> function that will run for a specific number of episodes; each episode runs until the game is lost, namely when <kbd>done</kbd><span> </span>is<span> </span><kbd>True</kbd>. We use<span> </span><kbd>rewards_max</kbd><span> </span>to indicate when to break out of the loop as we do not wish to run the experiment forever, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px"># experiment collect observations and rewards for each episode<br/>def experiment(env, policy, n_episodes,r_max=0, t_max=0):<br/>    <br/>    rewards=np.empty(shape=[n_episodes])<br/>    for i in range(n_episodes):<br/>        val = episode(env, policy, r_max, t_max)<br/>        #print('episode:{}, reward {}'.format(i,val))<br/>        rewards[i]=val<br/>            <br/>    print('Policy:{}, Min reward:{}, Max reward:{}, Average reward:{}'<br/>        .format(policy.__name__,<br/>              np.min(rewards),<br/>              np.max(rewards),<br/>              np.mean(rewards)))</pre>
<ol start="4">
<li>Create a simple MLP network with the following code:</li>
</ol>
<pre style="padding-left: 60px">from collections import deque <br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Flatten<br/><br/># build the Q-Network<br/>model = Sequential()<br/>model.add(Flatten(input_shape = n_shape))<br/>model.add(Dense(512, activation='relu',name='hidden1'))<br/>model.add(Dense(9, activation='softmax', name='output'))<br/>model.compile(loss='categorical_crossentropy',optimizer='adam')<br/>model.summary()<br/>q_nn = model</pre>
<p class="mce-root" style="padding-left: 60px">The preceding code generates the following output:</p>
<pre class="mce-root" style="padding-left: 60px">_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
flatten_3 (Flatten)          (None, 100800)            0         
_________________________________________________________________
hidden1 (Dense)              (None, 8)                 806408    
_________________________________________________________________
output (Dense)               (None, 9)                 81        
=================================================================
Total params: 806,489
Trainable params: 806,489
Non-trainable params: 0
_________________________________________________________________</pre>
<ol start="5">
<li>Create an empty list to contain the game memory and define other hyperparameters and run the experiment for one episode, as shown in the following example:</li>
</ol>
<pre style="padding-left: 90px"># Hyperparameters<br/><br/>discount_rate = 0.9<br/>explore_rate = 0.2<br/>n_episodes = 1<br/><br/># create the empty list to contain game memory<br/>memory = deque(maxlen=1000)<br/><br/>experiment(env, policy_q_nn, n_episodes)</pre>
<p style="padding-left: 60px">The result we get is as follows:</p>
<pre style="padding-left: 60px"><strong>Policy:policy_q_nn, Min reward:490.0, Max reward:490.0, Average reward:490.0</strong></pre>
<p style="padding-left: 60px">That is definitely an improvement in our case, but, in your case, it might be different. In this case, our game has only learned from a limited memory and only from game replay in one episode.</p>
<ol start="6">
<li>Now, run it for <kbd>100</kbd> episodes, as shown in the following example:</li>
</ol>
<pre style="padding-left: 60px"># Hyperparameters<br/><br/>discount_rate = 0.9<br/>explore_rate = 0.2<br/>n_episodes = 100<br/><br/># create the empty list to contain game memory<br/>memory = deque(maxlen=1000)<br/><br/>experiment(env, policy_q_nn, n_episodes)</pre>
<p style="padding-left: 60px">We get the following results:</p>
<pre style="padding-left: 60px"><strong>Policy:policy_q_nn, Min reward:70.0, Max reward:580.0, Average reward:270.5</strong></pre>
<p style="padding-left: 60px">Thus we see that, on average, the results did not improve, although we reached a high max reward. Tuning the network architecture, features, and hyperparameters might produce better results. We would encourage <span><span>you</span></span> to modify the code. As an example, instead of MLP, you can use the simple one-layer convolutional network, as follows:</p>
<pre style="padding-left: 60px">from collections import deque <br/>from tensorflow.keras.models import Sequential<br/>from tensorflow.keras.layers import Dense, Flatten<br/>from tensorflow.keras.layers import Conv2D, MaxPooling2D<br/><br/># build the CNN Q-Network<br/>model = Sequential()<br/>model.add(Conv2D(16, kernel_size=(5, 5), <br/>                 strides=(1, 1),<br/>                 activation='relu',<br/>                 input_shape=n_shape))<br/>model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))<br/>model.add(Flatten())<br/>model.add(Dense(512, activation='relu',name='hidden1'))<br/>model.add(Dense(9, activation='softmax', name='output'))<br/>model.compile(loss='categorical_crossentropy',optimizer='adam')<br/>model.summary()<br/>q_nn = model<br/></pre>
<p><span>The preceding code displays the network summary as follows:</span></p>
<pre class="mce-root"><strong>_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_4 (Conv2D)            (None, 206, 156, 16)      1216      
_________________________________________________________________
max_pooling2d_4 (MaxPooling2 (None, 103, 78, 16)       0         
_________________________________________________________________
flatten_8 (Flatten)          (None, 128544)            0         
_________________________________________________________________
hidden1 (Dense)              (None, 512)               65815040  
_________________________________________________________________
output (Dense)               (None, 9)                 4617      
=================================================================
Total params: 65,820,873
Trainable params: 65,820,873
Non-trainable params: 0</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned what reinforcement learning is. <span>Reinforcement learning is an advanced technique that you will find is often used to solve complex problems. </span>We learned about OpenAI Gym, a framework that provides an environment for simulating many popular games in order to implement and practice reinforcement learning algorithms. We touched on deep reinforcement learning concepts, and we encourage you to explore books (mentioned in the further reading) specifically written about reinforcement learning to learn deeply about the theories and concepts.</p>
<p>We learned how to play the PacMan game in OpenAI Gym. We implemented DQN and used it to learn to play the PacMan game. We only used an MLP network to keep things simple, but, for complex examples, you may end up using complex CNN, RNN, or Sequence-to-Sequence models.</p>
<p><span>In the next chapter, we shall learn about future opportunities in the fields of machine learning and TensorFlow.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further Reading</h1>
                </header>
            
            <article>
                
<ul>
<li>Deep Reinforcement Learning Hands-On by Maxim Lapan, Packt Publications</li>
<li>Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</li>
<li>Statistical Reinforcement Learning: Modern Machine Learning Approaches by Masashi Sugiyama</li>
<li>Algorithms for reinforcement learning by Csaba Szepesvari</li>
</ul>


            </article>

            
        </section>
    </body></html>