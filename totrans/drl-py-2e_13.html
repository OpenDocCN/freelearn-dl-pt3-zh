<html><head></head><body>
  <div id="_idContainer2272">
    <h1 class="chapterNumber">13</h1>
    <h1 id="_idParaDest-339" class="chapterTitle">TRPO, PPO, and ACKTR Methods</h1>
    <p class="normal">In this chapter, we will learn two interesting state-of-art policy gradient algorithms: trust region policy optimization and proximal policy optimization. Both of these algorithms act as an improvement to the policy gradient algorithm (REINFORCE with baseline) we learned in <em class="chapterRef">Chapter 10</em>,<em class="italic"> Policy Gradient Method.</em></p>
    <p class="normal">We begin the chapter by understanding the <strong class="keyword">Trust Region Policy Optimization</strong> (<strong class="keyword">TRPO</strong>) method and how it acts as an improvement to the policy gradient method. Later we will understand several essential math concepts that are required to understand TRPO. Following this, we will learn how to design and solve the TRPO objective function. At the end of the section, we will understand how the TRPO algorithm works step by step.</p>
    <p class="normal">Moving on, we will learn about <strong class="keyword">Proximal Policy Optimization </strong>(<strong class="keyword">PPO</strong>). We will understand how PPO works and how it acts as an improvement to the TRPO algorithm in detail. We will also learn two types of PPO algorithm called PPO-clipped and PPO-penalty.</p>
    <p class="normal">At the end of the chapter, we will learn about an interesting actor-critic method called the <strong class="keyword">Actor-Critic using Kronecker-Factored Trust Region</strong> (<strong class="keyword">ACKTR</strong>) method, which uses Kronecker factorization to approximate the second-order derivative. We will explore how ACKTR works and how it uses the trust region in its update rule.</p>
    <p class="normal">In this chapter, we will learn the following topics:</p>
    <ul>
      <li class="bullet">Trust region policy optimization</li>
      <li class="bullet">Designing the TRPO objective function</li>
      <li class="bullet">Solving the TRPO objective function </li>
      <li class="bullet">Proximal policy optimization</li>
      <li class="bullet">The PPO algorithm</li>
      <li class="bullet">Actor-critic using Kronecker-factored trust region</li>
    </ul>
    <h1 id="_idParaDest-340" class="title">Trust region policy optimization</h1>
    <p class="normal">TRPO is one of the most popularly used algorithms in deep <a id="_idIndexMarker1156"/>reinforcement learning. TRPO is a policy gradient algorithm and it acts as an improvement to the policy gradient with baseline method we learned in <em class="chapterRef">Chapter 10</em>,<em class="italic"> Policy Gradient Method</em>. We learned that policy gradient is an on-policy method, meaning that on every iteration, we improve the same policy with which we are generating trajectories. On every iteration, we update the parameter of our network and try to find the improved policy. The update rule for updating the parameter <img src="../Images/B15558_09_042.png" alt="" style="height: 1.11em;"/> of our network is given as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_11_005.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_10_113.png" alt="" style="height: 1.11em;"/> is the gradient and <img src="../Images/B15558_07_025.png" alt="" style="height: 0.93em;"/> is known as the step size or learning rate. If the step size is large then there will be a large policy update, and if it is small then there will be a small update in the policy. How can we find an optimal step size? In the policy gradient method, we keep the step size small and so on every iteration there will be a small improvement in the policy.</p>
    <p class="normal">But what happens if we take a large step on every iteration? Let's suppose we have a policy <img src="../Images/B15558_04_099.png" alt="" style="height: 0.84em;"/> parameterized by <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>. So, on every iteration, updating <img src="../Images/B15558_09_118.png" alt="" style="height: 1.11em;"/> implies that we are improving our policy. If the step size is large, then the policy on every iteration varies greatly, meaning the old policy (the policy used in the previous iteration) and the new policy (the policy used in the current iteration) vary greatly. Since we are using a parametrized policy, it implies that if we make a large update (large step size) then the parameter of the old policy and the new policy vary heavily, and this leads to a problem called model collapse.</p>
    <p class="normal">This is the reason that in the policy gradient method, instead of taking larger steps and updating the parameter of our network, we take small steps and update the parameter to keep the old policy and new policy close. But how can we improve this? </p>
    <p class="normal">Can we take a larger step along with keeping the old and new policies close so that it won't affect our model performance and also helps us to learn quickly? Yes, this problem is solved by TRPO.</p>
    <p class="normal">TRPO tries to make a large policy update while imposing a constraint that the old policy and the new policy should not vary too much. Okay, what is this constraint? But first, how can we measure and understand if the old policy and new policy are changing greatly? Here is where we <a id="_idIndexMarker1157"/>use a measure called the <strong class="keyword">Kullback-Leibler </strong>(<strong class="keyword">KL</strong>) divergence. The KL divergence is ubiquitous in reinforcement learning. It tells <a id="_idIndexMarker1158"/>us how two probability distributions are different from each other. So, we can use the KL divergence to understand if our old policy and new policy vary greatly or not. TRPO adds a constraint that the KL divergence between the old policy and the new policy should be less than or equal to some constant <img src="../Images/B15558_09_135.png" alt="" style="height: 1.11em;"/>. That is, when we make a policy update, the old policy and the new policy should not vary more than some constant. This constraint is called the trust region constraint.</p>
    <p class="normal">Thus, TRPO tries to make a large policy update while imposing the constraint that the parameter of the old policy and the new policy should be within the trust region. Note that in the policy gradient method, we use a parameterized policy. Thus, keeping the parameter of the old policy and the new policy within the trust region implies that the old and new policies are within the trust region.</p>
    <p class="normal">TRPO guarantees monotonic policy improvement; that is, it guarantees that there will always be a policy improvement on every iteration. This is the fundamental idea behind the TRPO algorithm. </p>
    <p class="normal">To understand how exactly TRPO works, we should understand the math behind TRPO. TRPO has pretty heavy math. But worry not! It will be simple if we understand the fundamental math concepts required to understand TRPO. So, before diving into the TRPO algorithm, first, we will understand several essential math concepts that are required to understand TRPO. Then we will learn how to design a TRPO objective function with the trust region constraint, and finally, we will see how to solve the TRPO objective function.</p>
    <h2 id="_idParaDest-341" class="title">Math essentials</h2>
    <p class="normal">Before <a id="_idIndexMarker1159"/>understanding how TRPO works, first, we will understand the following important math concepts:</p>
    <ul>
      <li class="bullet">The Taylor series</li>
      <li class="bullet">The trust region method</li>
      <li class="bullet">The conjugate gradient method</li>
      <li class="bullet">Lagrange multipliers</li>
      <li class="bullet">Importance sampling </li>
    </ul>
    <h3 id="_idParaDest-342" class="title">The Taylor series</h3>
    <p class="normal">The Taylor series <a id="_idIndexMarker1160"/>is a series of infinite terms and it is <a id="_idIndexMarker1161"/>used for approximating a function. Let's say we have a function <em class="italic">f</em>(<em class="italic">x</em>) centered at <em class="italic">x</em> = <em class="italic">a</em>; we can approximate it using an infinite sum of polynomial terms as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_009.png" alt="" style="height: 2.31em;"/></figure>
    <p class="normal">The preceding equation can be represented in sigma notation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_010.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">So for each term in the Taylor series, we calculate the <em class="italic">n</em><sup class="" style="font-style: italic;">th</sup> order derivative, divide them by <em class="italic">n</em>!, and multiply by (<em class="italic">x</em> – <em class="italic">a</em>)<sup class="" style="font-style: italic;">n</sup>. </p>
    <p class="normal">Let's understand how exactly the Taylor series approximates a function with an example. Let's say we have an exponential function <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup> as shown in <em class="italic">Figure 13.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.1: Exponential function</p>
    <p class="normal">Can we <a id="_idIndexMarker1162"/>approximate the exponential function <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup> using the Taylor series? We <a id="_idIndexMarker1163"/>know that the Taylor series is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_011.png" alt="" style="height: 2.31em;"/></figure>
    <p class="normal">Here, the function <em class="italic">f</em>(<em class="italic">x</em>) we want to approximate is <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup>, that is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_012.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Say our function <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup> is centered at <em class="italic">x</em> = <em class="italic">a</em>, first, let's calculate the derivatives of the function up to 3 orders. The derivative of the exponential function is the function itself, so we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_013.png" alt="" style="height: 3.71em;"/></figure>
    <p class="normal">Substituting the preceding terms in the equation (1), we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_014.png" alt="" style="height: 2.31em;"/></figure>
    <p class="normal">Let's <a id="_idIndexMarker1164"/>suppose <em class="italic">a</em> = 0; then our equation becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_015.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">We <a id="_idIndexMarker1165"/>know that <em class="italic">e</em><sup class="Superscript--PACKT-">0</sup> =1; thus, the Taylor series of the exponential function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_016.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">It implies that the sum of the terms on the right-hand side approximates the exponential function <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup>. Let's understand this with the help of a plot. Let's take only the terms till the 0<sup class="" style="font-style: italic;">th</sup> order derivative from the Taylor series (equation 2), that is, <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup> = 1, and plot them:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.2: Taylor series approximation till the 0<sup class="" style="font-style: italic;">th</sup> order derivative</p>
    <p class="normal">As we <a id="_idIndexMarker1166"/>can observe from the preceding plot, just taking the 0<sup class="" style="font-style: italic;">th</sup> order derivative, we <a id="_idIndexMarker1167"/>are far away from the actual function <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup>. That is, our approximation is not good. So, let's take the sum of terms till the 1<sup class="" style="font-style: italic;">st</sup> order derivative from the Taylor series (equation 2), that is, <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup> = 1 + <em class="italic">x</em>, and plot them: </p>
    <figure class="mediaobject"><img src="../Images/B15558_13_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.3: Taylor series approximation till the 1<sup class="" style="font-style: italic;">st</sup> order derivative</p>
    <p class="normal">As we can observe from the preceding plot, including the terms till the 1<sup class="" style="font-style: italic;">st</sup> order derivative from the Taylor series gets us closer to the actual function <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup>. So, let's take the sum of terms till the 2<sup class="" style="font-style: italic;">nd</sup> order derivative from the Taylor series (equation 2), that is, <img src="../Images/B15558_13_017.png" alt="" style="height: 2.4em;"/>, and plot them. As we can observe from the following plot our approximation <a id="_idIndexMarker1168"/>gets better and we reach closer to the actual function <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.4: Taylor series approximation till the 2<sup class="" style="font-style: italic;">nd</sup> order derivative</p>
    <p class="normal">Now, let's <a id="_idIndexMarker1169"/>take the sum of terms till the 3<sup class="" style="font-style: italic;">rd</sup> order derivative from the Taylor series, that is, <img src="../Images/B15558_13_018.png" alt="" style="height: 2.4em;"/>, and plot them:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.5: Taylor series approximation till the 3<sup class="" style="font-style: italic;">rd</sup> order derivative</p>
    <p class="normal">By looking <a id="_idIndexMarker1170"/>at the preceding graph, we can understand that our approximation is far better after including the sum of terms till the 3<sup class="" style="font-style: italic;">rd</sup> order derivative. As you <a id="_idIndexMarker1171"/>might have guessed, adding more and more terms in the Taylor series makes our approximation of <em class="italic">e</em><sup class="" style="font-style: italic;">x</sup> better. Thus, using the Taylor series, we can approximate any function.</p>
    <p class="normal">The Taylor <a id="_idIndexMarker1172"/>polynomial till the first degree is called <strong class="keyword">linear approximation.</strong> In linear approximation, we calculate the Taylor series only till the first-order derivative. Thus, the linear approximation (first-order) of the function <em class="italic">f</em>(<em class="italic">x</em>) around the point <em class="italic">a</em> can be given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_019.png" alt="" style="height: 2.31em;"/></figure>
    <p class="normal">We can denote our first-order derivative by <img src="../Images/B15558_13_020.png" alt="" style="height: 1.11em;"/>, so we can just replace <img src="../Images/B15558_13_021.png" alt="" style="height: 1.2em;"/> by <img src="../Images/B15558_13_022.png" alt="" style="height: 1.11em;"/> and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_023.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">The Taylor <a id="_idIndexMarker1173"/>polynomial till the second degree is called <strong class="keyword">quadratic approximation</strong>. In quadratic approximation, we calculate the Taylor series only till the second-order derivative. Thus, the quadratic approximation (second-order) of the function <em class="italic">f</em>(<em class="italic">x</em>) around the point <em class="italic">a</em> can be given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_024.png" alt="" style="height: 2.31em;"/></figure>
    <p class="normal">We can <a id="_idIndexMarker1174"/>denote our first-order derivative by <img src="../Images/B15558_13_020.png" alt="" style="height: 1.11em;"/> and second-order derivative by <img src="../Images/B15558_13_026.png" alt="" style="height: 1.2em;"/>; so, we can just replace <img src="../Images/B15558_13_021.png" alt="" style="height: 1.2em;"/> with <img src="../Images/B15558_13_020.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_13_029.png" alt="" style="height: 1.2em;"/> with <img src="../Images/B15558_13_026.png" alt="" style="height: 1.2em;"/> and <a id="_idIndexMarker1175"/>rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_031.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">A Hessian is a second-order derivative, so we can denote <img src="../Images/B15558_13_026.png" alt="" style="height: 1.2em;"/> by <em class="italic">H</em>(<em class="italic">a</em>) and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_033.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Thus, to <a id="_idIndexMarker1176"/>summarize, a <strong class="keyword">linear approximation</strong> of the function <em class="italic">f</em>(<em class="italic">x</em>) is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_034.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">The<strong class="keyword"> quadratic approximation</strong> of <a id="_idIndexMarker1177"/>the function <em class="italic">f</em>(<em class="italic">x</em>) is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_035.png" alt="" style="height: 2.22em;"/></figure>
    <h3 id="_idParaDest-343" class="title">The trust region method</h3>
    <p class="normal">Let's say <a id="_idIndexMarker1178"/>we have a function <em class="italic">f</em>(<em class="italic">x</em>) and we need to find the minimum of the function. Let's suppose it is difficult to find the minimum <a id="_idIndexMarker1179"/>of the function <em class="italic">f</em>(<em class="italic">x</em>). So, what we can do is that we can use the Taylor series and approximate the given function <em class="italic">f</em>(<em class="italic">x</em>) and try to find the minimum value using the approximated function. Let's represent the approximated function with <img src="../Images/B15558_13_036.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">Say we use the quadratic approximation, we learned that with the quadratic approximation, we calculate the Taylor series only till the second-order derivative. Thus, the quadratic approximation (second-order) of the given function <em class="italic">f</em>(<em class="italic">x</em>) around the region <em class="italic">a</em> can be given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_037.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">So, we can just use the approximated function <img src="../Images/B15558_13_038.png" alt="" style="height: 1.29em;"/> and compute the minimum value. But wait! What if our approximated function <img src="../Images/B15558_13_036.png" alt="" style="height: 1.29em;"/> is inaccurate at a particular point, say <em class="italic">a</em>*, and if <em class="italic">a</em>* is optimal, then we miss out on finding the optimal value.</p>
    <p class="normal">So, we will introduce a new constraint called the trust region constraint. The trust region implies the <a id="_idIndexMarker1180"/>region where our actual function <em class="italic">f</em>(<em class="italic">x</em>) and approximated function <img src="../Images/B15558_13_036.png" alt="" style="height: 1.29em;"/> are close together. So, we can say that our approximation will be <a id="_idIndexMarker1181"/>accurate if our approximated function <img src="../Images/B15558_13_036.png" alt="" style="height: 1.29em;"/> is in the trust region.</p>
    <p class="normal">For instance, as shown in <em class="italic">Figure 13.6</em>, our approximated function <img src="../Images/B15558_13_036.png" alt="" style="height: 1.29em;"/> is in the trust region and thus our approximation will be accurate since the approximated function <img src="../Images/B15558_13_036.png" alt="" style="height: 1.29em;"/> is closer to the actual function <em class="italic">f</em>(<em class="italic">x</em>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.6: Approximated function is in the trust region</p>
    <p class="normal">But when <img src="../Images/B15558_13_044.png" alt="" style="height: 1.29em;"/> is not in the trust region, then our approximation will not be accurate since the approximated function <img src="../Images/B15558_13_036.png" alt="" style="height: 1.29em;"/> is far from the actual function <em class="italic">f</em>(<em class="italic">x</em>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.7: Approximated function is not in the trust region</p>
    <p class="normal">Thus, we <a id="_idIndexMarker1182"/>need to make sure that our approximated <a id="_idIndexMarker1183"/>function stays in the trust region so that it will be close to the actual function.</p>
    <h3 id="_idParaDest-344" class="title">The conjugate gradient method</h3>
    <p class="normal">The conjugate <a id="_idIndexMarker1184"/>gradient method <a id="_idIndexMarker1185"/>is an iterative method used to solve a system of linear equations. It is also used to solve the optimization problem. The conjugate gradient method is used when a system is of the form:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_046.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Where <em class="italic">A</em> is the positive definite, square, and symmetric matrix, <em class="italic">x</em> is the vector we want to find, and <em class="italic">b</em> is the known vector. Let's consider the following quadratic function:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_047.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">When <em class="italic">A</em> is the positive semi-definite matrix; finding the minimum of this function is equal to solving the system <em class="italic">Ax</em> = <em class="italic">b</em>. Just like gradient descent, conjugate gradient descent also tries to find the minimum of the function; however, the search direction of conjugate gradient descent will be different from gradient descent, and conjugate gradient descent attains convergence in <em class="italic">N</em> iterations. Let's understand how conjugate gradient descent differs from gradient descent with the help of a contour plot. </p>
    <p class="normal">First, let's <a id="_idIndexMarker1186"/>look at the contour plot of gradient descent. As <a id="_idIndexMarker1187"/>we can see in the following plot, in order to find the minimum value of a function, gradient descent takes several search directions and we get a zigzag pattern of directions:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.8: Contour plot of gradient descent</p>
    <p class="normal">Unlike the gradient descent method, in the conjugate gradient descent, the search direction is orthogonal to the previous search direction as shown in <em class="italic">Figure 13.9</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.9: Contour plot of conjugate gradient descent</p>
    <p class="normal">So, using <a id="_idIndexMarker1188"/>conjugate gradient descent, we can solve a system <a id="_idIndexMarker1189"/>of the form <em class="italic">Ax</em> = <em class="italic">b</em>.</p>
    <h3 id="_idParaDest-345" class="title">Lagrange multipliers </h3>
    <p class="normal">Let's say <a id="_idIndexMarker1190"/>we have a function <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">x</em><sup class="Superscript--PACKT-">2</sup>: how do <a id="_idIndexMarker1191"/>we find the minimum of the function? We can find the minimum of the function by finding a point where the gradient of the function is zero. The gradient of the function <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">x</em><sup class="Superscript--PACKT-">2</sup> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_048.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">When <em class="italic">x</em> = 0, the gradient of the function is zero; that is, <img src="../Images/B15558_13_049.png" alt="" style="height: 1.11em;"/> when <em class="italic">x</em> = 0. So, we can say that the minimum of the function <em class="italic">f</em>(<em class="italic">x</em>) = <em class="italic">x</em><sup class="Superscript--PACKT-">2</sup> is at <em class="italic">x</em> = 0. The problem we just saw is called the unconstrained optimization problem. </p>
    <p class="normal">Consider a case where we have a constraint—say we need to minimize the function <em class="italic">f</em>(<em class="italic">x</em>) subject to the constraint that <em class="italic">g</em>(<em class="italic">x</em>) = 1, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_050.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Now, how can we solve this problem? That is, how can we find the minimum of the function <em class="italic">f</em>(<em class="italic">x</em>) while satisfying the constraint <em class="italic">g</em>(<em class="italic">x</em>)? We can find the minimum value when the gradient of the objective function <em class="italic">f</em>(<em class="italic">x</em>) and the gradient of the constraint <em class="italic">g</em>(<em class="italic">x</em>) point in the same direction. That is, we can find the minimum value when the gradient of <em class="italic">f</em>(<em class="italic">x</em>) and the gradient of <em class="italic">g</em>(<em class="italic">x</em>) are parallel or antiparallel to each other:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_051.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Although <a id="_idIndexMarker1192"/>the gradients of <em class="italic">f</em>(<em class="italic">x</em>) and <em class="italic">g</em>(<em class="italic">x</em>) point in the same direction, their magnitude will not be the same. So, we will just multiply the gradient of <em class="italic">g</em>(<em class="italic">x</em>) by a <a id="_idIndexMarker1193"/>variable called <img src="../Images/B15558_13_052.png" alt="" style="height: 1.11em;"/> as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_053.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_13_054.png" alt="" style="height: 1.11em;"/> is known as the Lagrange multiplier. So, we can rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_055.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Solving the preceding equation implies that we find the minimum of the function <em class="italic">f</em>(<em class="italic">x</em>) along with satisfying the constraint <em class="italic">g</em>(<em class="italic">x</em>). So, we can rewrite our objective function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_056.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">The gradient of the preceding function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_057.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">We can find the minimum value when <img src="../Images/B15558_13_058.png" alt="" style="height: 1.11em;"/>. Lagrange multipliers are widely used for solving constrained optimization problems.</p>
    <p class="normal">Let's understand this with one more example. Say we want to find the minimum of the function <img src="../Images/B15558_13_059.png" alt="" style="height: 1.2em;"/> subject to the constraint <img src="../Images/B15558_13_060.png" alt="" style="height: 1.2em;"/>, as the following shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_061.png" alt="" style="height: 3.07em;"/></figure>
    <p class="normal">We can <a id="_idIndexMarker1194"/>rewrite our objective function with the constraint multiplied <a id="_idIndexMarker1195"/>by the Lagrange multiplier as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_062.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Solving for <img src="../Images/B15558_13_063.png" alt="" style="height: 1.11em;"/>, we can find the minimum of the function <img src="../Images/B15558_13_059.png" alt="" style="height: 1.2em;"/> along with satisfying the constraint that <img src="../Images/B15558_13_065.png" alt="" style="height: 1.2em;"/>.</p>
    <h3 id="_idParaDest-346" class="title">Importance sampling </h3>
    <p class="normal">Let's recap <a id="_idIndexMarker1196"/>the importance sampling <a id="_idIndexMarker1197"/>method we learned in <em class="chapterRef">Chapter 4</em>,<em class="italic"> Monte Carlo Methods</em>. Say we want to compute the expectation of a function <em class="italic">f</em>(<em class="italic">x</em>) where the value of <em class="italic">x</em> is sampled from the distribution <em class="italic">p</em>(<em class="italic">x</em>), that is, <img src="../Images/B15558_13_066.png" alt="" style="height: 1.11em;"/>; we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_067.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">Can we approximate the expectation of a function <em class="italic">f</em>(<em class="italic">x</em>)? We learned that using the Monte Carlo method, we can approximate the expectation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_068.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">That is, using the Monte Carlo method, we sample <em class="italic">x</em> from the distribution <em class="italic">p</em>(<em class="italic">x</em>) for <em class="italic">N</em> times and compute the average of <em class="italic">f(x)</em> to approximate the expectation.</p>
    <p class="normal">Instead of <a id="_idIndexMarker1198"/>using the Monte Carlo method, we can also use importance sampling to approximate the expectation. In the importance <a id="_idIndexMarker1199"/>sampling method, we estimate the expectation using a different distribution <em class="italic">q</em>(<em class="italic">x</em>); that is, instead of sampling <em class="italic">x</em> from <em class="italic">p</em>(<em class="italic">x</em>) we use a different distribution <em class="italic">q</em>(<em class="italic">x</em>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_069.png" alt="" style="height: 5.73em;"/></figure>
    <p class="normal">The ratio <img src="../Images/B15558_04_146.png" alt="" style="height: 2.51em;"/> is called the importance sampling ratio or the importance correction. </p>
    <p class="normal">Now that we have understood the several important math prerequisites, we will learn how the TRPO algorithm works in the next section.</p>
    <h2 id="_idParaDest-347" class="title">Designing the TRPO objective function</h2>
    <p class="normal">At the <a id="_idIndexMarker1200"/>beginning of the chapter, we learned that TRPO tries to make a large policy update while imposing the constraint that the parameter of the old policy and the new policy should stay within the trust region. In this section, we will learn how to design the TRPO objective function along with the trust region constraint so that the old policy and the new policy will not vary very much.</p>
    <p class="normal">This section will be pretty dense and optional. If you are not interested in math you can directly navigate to the section <em class="italic">Solving the TRPO objective function</em>, where we learn how to solve the TRPO objective function step by step. </p>
    <p class="normal">Let us say we have a policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/>; we can express the expected discounted return <img src="../Images/B15558_13_072.png" alt="" style="height: 0.93em;"/> following the policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/> as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_074.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">We know that in the policy gradient method, on every iteration, we keep on improving the policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/>. Say we updated our old policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> and have a new policy <img src="../Images/B15558_13_077.png" alt="" style="height: 1.11em;"/>; then, we can express the <a id="_idIndexMarker1201"/>expected discounted return <img src="../Images/B15558_13_078.png" alt="" style="height: 0.93em;"/>, following the new policy <img src="../Images/B15558_13_079.png" alt="" style="height: 1.11em;"/> in terms of advantage over the old policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/>, as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_081.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">As we can notice from the preceding equation, the expected return following the new policy <img src="../Images/B15558_13_082.png" alt="" style="height: 1.11em;"/>, that is, <img src="../Images/B15558_13_083.png" alt="" style="height: 1.11em;"/>, is just the sum of the expected return following the old policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/>, that is, <img src="../Images/B15558_13_085.png" alt="" style="height: 1.11em;"/>, and the expected discounted advantage of the old policy <img src="../Images/B15558_13_086.png" alt="" style="height: 1.11em;"/>. That is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_12.png" alt="" style="height:7em;"/></figure>
    <p class="normal">But, why are <a id="_idIndexMarker1202"/>we using the advantage of the old policy? Because we are measuring how good the new policy <img src="../Images/B15558_13_087.png" alt="" style="height: 1.11em;"/> is with respect to the average performance of the old policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">We can simplify the equation (2) and replace the sum over time steps with the sum over states and actions as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_089.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_13_090.png" alt="" style="height: 1.11em;"/> is the discounted visitation frequency of the new policy. We already learned that the expected return of the new policy <img src="../Images/B15558_13_091.png" alt="" style="height: 1.11em;"/> is obtained by adding the expected return of the old policy <img src="../Images/B15558_13_092.png" alt="" style="height: 1.11em;"/> and the advantage of the old policy <img src="../Images/B15558_13_093.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">In the preceding equation (3), if the advantage <img src="../Images/B15558_13_094.png" alt="" style="height: 1.11em;"/> is always positive, then it means that our policy is improving and we have better <img src="../Images/B15558_13_095.png" alt="" style="height: 1.11em;"/>. That is, if the advantage <img src="../Images/B15558_13_094.png" alt="" style="height: 1.11em;"/> is always <img src="../Images/B15558_13_097.png" alt="" style="height: 1.11em;"/>, then we will always have an improvement in our policy.</p>
    <p class="normal">However, equation (3) is difficult to optimize, so we approximate <img src="../Images/B15558_13_098.png" alt="" style="height: 1.11em;"/> by a local approximate <img src="../Images/B15558_13_099.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_100.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">As you <a id="_idIndexMarker1203"/>may notice, unlike equation (3), in equation (4) we use <img src="../Images/B15558_13_101.png" alt="" style="height: 1.11em;"/> instead of <img src="../Images/B15558_13_102.png" alt="" style="height: 1.11em;"/>. That is, we use a discounted visitation frequency of the old policy <img src="../Images/B15558_13_103.png" alt="" style="height: 1.11em;"/> instead of the new policy <img src="../Images/B15558_13_090.png" alt="" style="height: 1.11em;"/>. But why do we have to do that? Because we already have trajectories sampled from the old policy, so it is easier to obtain <img src="../Images/B15558_13_103.png" alt="" style="height: 1.11em;"/> than <img src="../Images/B15558_13_090.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">A surrogate function is a function that is an approximate of the objective function; so, we can call <img src="../Images/B15558_13_107.png" alt="" style="height: 1.11em;"/> a surrogate function since it is the local approximate of our objective function <img src="../Images/B15558_13_098.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Thus, <img src="../Images/B15558_13_109.png" alt="" style="height: 1.11em;"/> is the local approximate of our objective <img src="../Images/B15558_13_091.png" alt="" style="height: 1.11em;"/>. We need to make sure that our local approximate is accurate. Remember how, in the <em class="italic">The trust region method</em> section, we learned that the local approximation of the function will be accurate if it is in the trust region? So, our local approximate <img src="../Images/B15558_13_111.png" alt="" style="height: 1.11em;"/> will be accurate if it is in the trust region. Thus, while updating the values of <img src="../Images/B15558_13_112.png" alt="" style="height: 1.11em;"/>, we need to make sure that it remains in the trust region; that is, the policy updates should remain in the trust region.</p>
    <p class="normal">So, when we <a id="_idIndexMarker1204"/>update the old policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> to a new policy <img src="../Images/B15558_13_079.png" alt="" style="height: 1.11em;"/>, we just need to ensure that the new policy update stays within the trust region. In order to do that, we have to measure how far our new policy is from the old policy, so, we use the KL divergence to measure this:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_115.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Therefore, while updating the policy, we check the KL divergence between the policy updates and make sure that our policy updates are within the trust region. To satisfy this KL constraint, Kakade and Langford introduced a new policy updating scheme called conservative policy iteration and derived the following lower bound:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_116.png" alt="" style="height: 3.71em;"/></figure>
    <p class="normal">As we can observe, in the preceding equation, we have the KL divergence as the penalty term and <em class="italic">C</em> is the penalty coefficient.</p>
    <p class="normal">Now, our <a id="_idIndexMarker1205"/>surrogate objective function (4) along with the penalized KL term is written as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_117.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Maximizing the surrogate function <img src="../Images/B15558_13_118.png" alt="" style="height: 1.2em;"/> improves our true objective function <img src="../Images/B15558_13_098.png" alt="" style="height: 1.11em;"/> and guarantees a monotonic improvement in the policy. The preceding objective <a id="_idIndexMarker1206"/>function is known as <strong class="keyword">KL penalized objective.</strong></p>
    <h3 id="_idParaDest-348" class="title">Parameterizing the policies</h3>
    <p class="normal">We learned <a id="_idIndexMarker1207"/>that maximizing the surrogate objective function maximizes our true objective function. We know that in the policy gradient method, we use a parameterized policy; that is, we use a function approximator like a neural network parameterized by some parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> and learn the optimal policy.</p>
    <p class="normal">We parameterize the old policy with <img src="../Images/B15558_10_037.png" alt="" style="height: 1.11em;"/> as <img src="../Images/B15558_13_122.png" alt="" style="height: 0.93em;"/> and the new policy with <img src="../Images/B15558_12_330.png" alt="" style="height: 1.11em;"/> as <img src="../Images/B15558_13_124.png" alt="" style="height: 0.84em;"/>. So, we can rewrite our equation (5) in terms of parameterized policies as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_125.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">As shown in the preceding equation, we are using the max KL divergence between the old and new policies, that is, <img src="../Images/B15558_13_126.png" alt="" style="height: 1.4em;"/>. It is difficult to optimize our objective with the max KL term, so instead of using max KL, we can take the average KL divergence <img src="../Images/B15558_13_127.png" alt="" style="height: 1.4em;"/> and rewrite our surrogate objective as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_128.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">The issue <a id="_idIndexMarker1208"/>with the preceding objective function is that when we substitute the value of the penalty coefficient <em class="italic">C</em> as <img src="../Images/B15558_13_129.png" alt="" style="height: 2.4em;"/>, it reduces the step size, and it takes us a lot of time to attain convergence.</p>
    <p class="normal">So, we can redefine our surrogate objective function as a constrained objective function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_130.png" alt="" style="height: 3.07em;"/></figure>
    <p class="normal">The preceding equation implies that we maximize our surrogate objective function <img src="../Images/B15558_13_131.png" alt="" style="height: 1.11em;"/> while maintaining the constraint that the KL divergence between the old policy <img src="../Images/B15558_13_132.png" alt="" style="height: 0.93em;"/> and new policy <img src="../Images/B15558_10_111.png" alt="" style="height: 0.84em;"/> is less than or equal to a constant <img src="../Images/B15558_09_135.png" alt="" style="height: 1.11em;"/>, and it ensures that our old policy and <a id="_idIndexMarker1209"/>the new policy will not vary very much. The preceding objective function is called the <strong class="keyword">KL-constrained objective.</strong></p>
    <h3 id="_idParaDest-349" class="title">Sample-based estimation</h3>
    <p class="normal">In the <a id="_idIndexMarker1210"/>previous section, we learned how to frame our objective function as a KL-constrained objective with parameterized policies. In this section, we will learn how to simplify our objective function.</p>
    <p class="normal">We learned that our KL constrained objective function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_135.png" alt="" style="height: 3.07em;"/></figure>
    <p class="normal">From equation (4), substituting the value of <img src="../Images/B15558_13_136.png" alt="" style="height: 1.11em;"/> with <img src="../Images/B15558_13_137.png" alt="" style="height: 2.69em;"/> in the preceding equation, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_138.png" alt="" style="height: 4.27em;"/></figure>
    <p class="normal">Now we will see how we can simplify equation (9) by getting rid of the two summations using sampling.</p>
    <p class="normal">The first sum <img src="../Images/B15558_13_139.png" alt="" style="height: 2.69em;"/> expresses the summation over state visitation frequency; we can replace it by sampling states from state visitation as <img src="../Images/B15558_13_140.png" alt="" style="height: 1.2em;"/>. Then, our equation becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_141.png" alt="" style="height: 3.24em;"/></figure>
    <p class="normal">Next, we replace the sum over actions <img src="../Images/B15558_13_142.png" alt="" style="height: 2.69em;"/> with an importance sampling estimator. Let <em class="italic">q</em> be the sampling distribution, and <em class="italic">a</em> is sampled from <em class="italic">q</em>, that is, <img src="../Images/B15558_13_143.png" alt="" style="height: 0.93em;"/>. Then, we can rewrite our preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_144.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Replacing <a id="_idIndexMarker1211"/>the sampling distribution <em class="italic">q</em> with <img src="../Images/B15558_13_145.png" alt="" style="height: 0.93em;"/>, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_146.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">Thus, our equation (9) becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_147.png" alt="" style="height: 4.73em;"/></figure>
    <p class="normal">In the next section, we will learn how to solve the preceding objective function to find the optimal policy.</p>
    <h2 id="_idParaDest-350" class="title">Solving the TRPO objective function</h2>
    <p class="normal">In the <a id="_idIndexMarker1212"/>previous section, we learned that the TRPO objective function is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_147.png" alt="" style="height: 4.73em;"/></figure>
    <p class="normal">The preceding equation implies that we try to find the policy that gives the maximum return along with the constraint that the KL divergence between the old and new policies should be less than or equal to <img src="../Images/B15558_09_135.png" alt="" style="height: 1.11em;"/>. This KL constraint makes sure that our new policy is not too far away from the old policy.</p>
    <p class="normal">For notation <a id="_idIndexMarker1213"/>brevity, let us represent our objective with <img src="../Images/B15558_13_150.png" alt="" style="height: 1.11em;"/> and the KL constraint with <img src="../Images/B15558_13_151.png" alt="" style="height: 1.11em;"/> and rewrite the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_152.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">By maximizing our objective function <img src="../Images/B15558_13_153.png" alt="" style="height: 1.11em;"/>, we can find the optimal policy. We can maximize the objective <img src="../Images/B15558_13_154.png" alt="" style="height: 1.11em;"/> by calculating gradients with respect to <img src="../Images/B15558_09_056.png" alt="" style="height: 1.11em;"/> and update the parameter using gradient ascent as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_156.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_13_157.png" alt="" style="height: 1.11em;"/> is the search direction (gradient) and <img src="../Images/B15558_13_158.png" alt="" style="height: 0.93em;"/> is the backtracking coefficient.</p>
    <p class="normal">That is, to update the parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>, we perform the two following steps:</p>
    <ul>
      <li class="bullet">First, we compute the search direction <img src="../Images/B15558_13_160.png" alt="" style="height: 1.11em;"/> using the Taylor series approximation</li>
      <li class="bullet">Next, we perform the line search in the computed search direction <img src="../Images/B15558_13_161.png" alt="" style="height: 1.11em;"/> by finding the value of <img src="../Images/B15558_09_143.png" alt="" style="height: 0.93em;"/> using the backtracking line search method</li>
    </ul>
    <p class="normal">We will learn what the backtracking coefficient is and how exactly the backtracking line search method works in the <em class="italic">Performing a line search in the search direction </em>section. Okay, but why do we have to perform these two steps? If you look at our objective function (10), we have a constrained optimization problem. Our constraint here is that while updating the parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/>, we need to make sure that our parameter updates are within the <a id="_idIndexMarker1214"/>trust region; that is, the KL divergence between the old and new parameters should be less than or equal to <img src="../Images/B15558_13_164.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Thus, performing these two steps and updating our parameter helps us satisfy the KL constraint and also guarantees monotonic improvement. Let's get into details and learn how exactly the two steps work. </p>
    <h3 id="_idParaDest-351" class="title">Computing the search direction</h3>
    <p class="normal">It is difficult to <a id="_idIndexMarker1215"/>optimize our objective function (10) directly, so first, we approximate our function using the Taylor series. We approximate the surrogate objective function <img src="../Images/B15558_13_154.png" alt="" style="height: 1.11em;"/> using linear approximation and we approximate our constraint <img src="../Images/B15558_13_166.png" alt="" style="height: 1.11em;"/> using quadratic approximation. </p>
    <p class="normal">To better understand the upcoming steps, recap <em class="italic">The Taylor series </em>from the <em class="italic">Math essentials</em> section.</p>
    <p class="normal">The<strong class="keyword"> linear approximation</strong> of <a id="_idIndexMarker1216"/>our objective function at a point <img src="../Images/B15558_13_167.png" alt="" style="height: 1.11em;"/> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_168.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">We represent the gradient <img src="../Images/B15558_13_169.png" alt="" style="height: 1.11em;"/> with <em class="italic">g</em>, so the preceding equation becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_170.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">While solving <a id="_idIndexMarker1217"/>the preceding equation, the value of <img src="../Images/B15558_13_171.png" alt="" style="height: 1.11em;"/> becomes zero, so we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_172.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">The<strong class="keyword"> quadratic approximation</strong> of our <a id="_idIndexMarker1218"/>constraint at point <img src="../Images/B15558_13_173.png" alt="" style="height: 1.11em;"/> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_174.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Where <em class="italic">H</em> is the second-order derivative, that is, <img src="../Images/B15558_13_175.png" alt="" style="height: 1.29em;"/>. In the preceding equation, the first term <img src="../Images/B15558_13_176.png" alt="" style="height: 1.11em;"/> becomes zero as the KL divergence between two identical distributions is zero, and the first-order derivative <img src="../Images/B15558_13_177.png" alt="" style="height: 1.11em;"/> becomes zero at <img src="../Images/B15558_13_178.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">So, our final equation becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_179.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Substituting (11) and (12) in the equation (10), we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_180.png" alt="" style="height: 3.98em;"/></figure>
    <p class="normal">Note that <a id="_idIndexMarker1219"/>in the preceding equation, <img src="../Images/B15558_13_181.png" alt="" style="height: 1.11em;"/> represents the parameter of the old policy and <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> represents the parameter of the new policy.</p>
    <p class="normal">As we can observe, in equation (13) we have a constrained optimization problem. How can we solve this? We can solve this using the Lagrange multiplier. </p>
    <p class="normal">Thus, using the Lagrange multiplier <img src="../Images/B15558_13_054.png" alt="" style="height: 1.11em;"/>, we can rewrite our objective function (13) as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_184.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">For notation brevity, let <em class="italic">s</em> represent <img src="../Images/B15558_13_185.png" alt="" style="height: 1.11em;"/>, so we can rewrite equation (14) as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_186.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Our goal is to find the optimal parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/>. So, we need to calculate the gradient of the preceding function and update our parameter using gradient ascent as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_188.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_06_030.png" alt="" style="height: 1.11em;"/> is the learning rate and <em class="italic">s</em> is the gradient.<strong class="keyword"> </strong>Now we will look at how to determine the learning rate <img src="../Images/B15558_13_190.png" alt="" style="height: 1.11em;"/> and the gradient <em class="italic">s</em>.</p>
    <p class="normal">First, we <a id="_idIndexMarker1220"/>compute <em class="italic">s</em>. Calculating the derivative of the objective function <em class="italic">L</em> given in equation (15) with respect to gradient <em class="italic">s</em>, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_191.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Thus, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_192.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal"><img src="../Images/B15558_13_193.png" alt="" style="height: 1.11em;"/> is just our Lagrange multiplier and it will not affect our gradient, so we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_194.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Thus, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_195.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">However, computing the value of <em class="italic">s</em> directly in this way is not optimal. This is because in the preceding equation, we have <img src="../Images/B15558_13_196.png" alt="" style="height: 1.2em;"/>, which implies the inverse of the second-order derivative. Computing the second-order derivative and its inverse is a expensive task. So, we need to find a better way to compute <em class="italic">s</em>; how we do that?</p>
    <p class="normal">From (17), we learned that:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_197.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">From the <a id="_idIndexMarker1221"/>preceding equation, we can observe the equation is in the form of <em class="italic">Ax</em> = <em class="italic">B</em>. Thus, using conjugate gradient descent, we can approximate the value of <em class="italic">s</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_198.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Thus, our update equation becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_199.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Where the value of <img src="../Images/B15558_13_200.png" alt="" style="height: 1.2em;"/> is computed using conjugated gradient descent.</p>
    <p class="normal">Now that we have calculated the gradient, we need to determine the learning rate <img src="../Images/B15558_06_030.png" alt="" style="height: 1.11em;"/>. We need to keep in mind that our update should be within the trust region, so while calculating the value of <img src="../Images/B15558_09_152.png" alt="" style="height: 1.11em;"/>, we need to maintain the KL constraint.</p>
    <p class="normal">In equation (18), we learned that our update rule is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_203.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">By rearranging the terms, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_204.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">From equation (13), we can write our KL constraint as: </p>
    <figure class="mediaobject"><img src="../Images/B15558_13_205.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Substituting (19) in the preceding equation, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_206.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">The <a id="_idIndexMarker1222"/>preceding equation can be solved as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_207.png" alt="" style="height: 3.62em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_13_208.png" alt="" style="height: 6.02em;"/></figure>
    <p class="normal">Thus, we can substitute the preceding value of the learning rate <img src="../Images/B15558_06_030.png" alt="" style="height: 1.11em;"/> in the equation (18) and rewrite our parameter update as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_210.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Where the value of <img src="../Images/B15558_13_211.png" alt="" style="height: 1.2em;"/> is computed using conjugated gradient descent.</p>
    <p class="normal">Thus, we <a id="_idIndexMarker1223"/>have computed the search direction using the Taylor series approximation and the Lagrange multiplier:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_13.png" alt="" style="height:7em;"/></figure>
    <p class="normal">In the next section, let's learn how to perform a line search.</p>
    <h3 id="_idParaDest-352" class="title">Performing a line search in the search direction</h3>
    <p class="normal">In order <a id="_idIndexMarker1224"/>to make sure that our policy updates satisfy the KL constraint, we use a backtracking line search method. So, our update equation becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_212.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Okay, what does this mean? What's that new parameter <img src="../Images/B15558_13_158.png" alt="" style="height: 0.93em;"/> doing there? It is called the backtracking coefficient and the value of <img src="../Images/B15558_13_158.png" alt="" style="height: 0.93em;"/> ranges from 0 to 1. It helps us to take a large step to update our parameter. That is, we can set <img src="../Images/B15558_09_143.png" alt="" style="height: 0.93em;"/> to a high value and make a large update. However, we need to make sure that we are maximizing our objective <img src="../Images/B15558_13_216.png" alt="" style="height: 1.11em;"/> along with satisfying our constraint <img src="../Images/B15558_13_217.png" alt="" style="height: 1.11em;"/>. </p>
    <p class="normal">So, we just try for different values of <em class="italic">j</em> from 0 to <em class="italic">N</em> and compute <img src="../Images/B15558_09_106.png" alt="" style="height: 1.11em;"/> as <img src="../Images/B15558_13_212.png" alt="" style="height: 3.51em;"/>. If <img src="../Images/B15558_13_216.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_13_217.png" alt="" style="height: 1.11em;"/> for some values of <em class="italic">j</em>, then we just stop and update our parameter as <img src="../Images/B15558_13_212.png" alt="" style="height: 3.51em;"/>.</p>
    <p class="normal">The <a id="_idIndexMarker1225"/>following steps provide clarity on how the backtracking line search method works:</p>
    <ol>
      <li class="numbered">For iterations <em class="italic">j</em> = 0, 1, 2, 3, . . . , <em class="italic">N</em>:<ol>
          <li class="numbered-l2">Compute <img src="../Images/B15558_13_212.png" alt="" style="height: 3.51em;"/></li>
          <li class="numbered-l2">If <img src="../Images/B15558_13_224.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_13_225.png" alt="" style="height: 1.11em;"/> then:
        <ol>
          <li class="numbered-l2" value="1">Update <img src="../Images/B15558_13_212.png" alt="" style="height: 3.51em;"/></li>
          <li class="numbered-l2">Break</li>
        </ol>
      </li>
    </ol></li>
        </ol>
    <p class="normal">Thus, our final parameter update rule of TRPO is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_212.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">In the <a id="_idIndexMarker1226"/>next section, we will learn how exactly the TRPO algorithm works by using the preceding update rule.</p>
    <h2 id="_idParaDest-353" class="title">Algorithm – TRPO</h2>
    <p class="normal">TRPO acts <a id="_idIndexMarker1227"/>as an improvement to the policy gradient algorithm we learned in <em class="chapterRef">Chapter 10</em>,<em class="italic"> Policy Gradient Method</em>. It ensures that we can take large steps and update our parameter along with maintaining the constraint that our old policy and the new policy should not vary very much. The TRPO update rule is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_212.png" alt="" style="height: 3.51em;"/></figure>
    <p class="normal">Now, let's look at the algorithm of TRPO and see exactly how TRPO uses the preceding update rule and finds the optimal policy. Before going ahead, let's recap how we computed gradient in the policy gradient method. In the policy gradient method, we computed the gradient <em class="italic">g</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_229.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Where <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub> is the reward-to-go. The reward-to-go is the sum of the rewards of the trajectory starting from a state <em class="italic">s</em> and action <em class="italic">a</em>; it is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_230.png" alt="" style="height: 3.42em;"/></figure>
    <p class="normal">Isn't the reward-to-go similar to something we learned about earlier? Yes! If you recall, we learned that the Q function is the sum of rewards of the trajectory starting from the state <em class="italic">s</em> and action <em class="italic">a</em>. So, we can just replace the reward-to-go with the Q function and write our gradient as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_231.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">In the preceding equation, we have a difference between the Q function and the value function. We learned that the advantage function is the difference between the Q function and <a id="_idIndexMarker1228"/>the value function and hence we can rewrite our gradient with the advantage function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_232.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Now, let's look at the algorithm of TRPO. Remember that TRPO is the policy gradient method, so unlike actor-critic methods, here, first we generate <em class="italic">N</em> number of trajectories and then we update the parameter of the policy and value network.</p>
    <p class="normal">The steps involved in the TRPO are given as follows:</p>
    <ol>
      <li class="numbered" value="1">Initialize the policy network parameter <img src="../Images/B15558_13_233.png" alt="" style="height: 1.11em;"/> and value network parameter <img src="../Images/B15558_13_234.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Generate <em class="italic">N</em> number of trajectories <img src="../Images/B15558_10_058.png" alt="" style="height: 1.67em;"/> following the policy <img src="../Images/B15558_10_111.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">Compute the return (reward-to-go) <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub></li>
      <li class="numbered">Compute the advantage value <em class="italic">A</em><sub class="" style="font-style: italic;">t</sub></li>
      <li class="numbered">Compute the policy gradients:<figure class="mediaobject"><img src="../Images/B15558_13_232.png" alt="" style="height: 3.33em;"/></figure>
      </li>
      <li class="numbered">Compute <img src="../Images/B15558_13_238.png" alt="" style="height: 1.2em;"/> using the conjugate gradient method</li>
      <li class="numbered">Update the policy network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> using the update rule:<figure class="mediaobject"><img src="../Images/B15558_13_240.png" alt="" style="height: 3.51em;"/></figure>
      </li>
      <li class="numbered">Compute the mean squared error of the value network: <figure class="mediaobject"><img src="../Images/B15558_10_149.png" alt="" style="height: 3.33em;"/></figure>
      </li>
      <li class="numbered">Update the value network parameter <img src="../Images/B15558_13_234.png" alt="" style="height: 1.11em;"/> using gradient descent as <img src="../Images/B15558_13_243.png" alt="" style="height: 1.2em;"/></li>
      <li class="numbered">Repeat <a id="_idIndexMarker1229"/>steps 2 to 9 for several iterations</li>
    </ol>
    <p class="normal">Now that we have understood how TRPO works, in the next section, we will learn another interesting algorithm called proximal policy optimization. </p>
    <h1 id="_idParaDest-354" class="title">Proximal policy optimization</h1>
    <p class="normal">In the <a id="_idIndexMarker1230"/>previous section, we learned how TRPO works. We learned that TRPO keeps the policy updates in the trust region by imposing a constraint that the KL divergence between the old and new policy should be less than or equal to <img src="../Images/B15558_13_244.png" alt="" style="height: 1.11em;"/>. The problem with the TRPO method is that it is difficult to implement and is computationally expensive. So, now we will learn one of the most popular and state-of-the-art policy gradient algorithms called <strong class="keyword">Proximal Policy Optimization</strong> (<strong class="keyword">PPO</strong>).</p>
    <p class="normal">PPO improves upon the TRPO algorithm and is simple to implement. Similar to TRPO, PPO ensures <a id="_idIndexMarker1231"/>that the policy updates are in the trust region. But unlike TRPO, PPO does not use any constraints in the objective function. Going forward, we will learn how exactly PPO works and how PPO ensures that the policy updates are in the trust region. </p>
    <p class="normal">There are two different types of PPO algorithm:</p>
    <ul>
      <li class="bullet"><strong class="keyword">PPO-clipped –</strong> In the <a id="_idIndexMarker1232"/>PPO-clipped method, in order to ensure that the policy updates are in the trust region (that the new policy is not far away from the old policy), PPO adds a new function called the clipping function, which ensures that the new and old policies are not far away from each other.</li>
      <li class="bullet"><strong class="keyword">PPO-penalty –</strong> In the PPO-penalty method, we modify our objective function by converting <a id="_idIndexMarker1233"/>the KL constraint term to a penalty term and update the penalty coefficient adaptively during training by ensuring that the policy updates are in the trust region.</li>
    </ul>
    <p class="normal">We will now look into the preceding two types of PPO algorithm in detail. </p>
    <h2 id="_idParaDest-355" class="title">PPO with a clipped objective </h2>
    <p class="normal">First, let us <a id="_idIndexMarker1234"/>recall the objective function of TRPO. We learned that the TRPO objective function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_245.png" alt="" style="height: 4.36em;"/></figure>
    <p class="normal">It implies that we try to maximize our policy along with the constraint that the old policy and the new policy stays within the trust region, that is, the KL divergence between the old policy and new policy should be less than or equal to <img src="../Images/B15558_13_246.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Let us take only the objective without the constraint and write the PPO objective function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_247.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">In the preceding equation, the term <img src="../Images/B15558_13_248.png" alt="" style="height: 2.69em;"/> implies the probability ratio, that is, the ratio of the new policy to the old policy. Let us denote this using <img src="../Images/B15558_13_249.png" alt="" style="height: 1.11em;"/> and write the PPO objective function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_250.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">If we update <a id="_idIndexMarker1235"/>the policy using the preceding objective function then the policy updates will not be in the trust region. So, to ensure that our policy updates are in the trust region (that the new policy is not far from the old policy), we modify our objective function by adding a new function called the clipping function and rewrite our objective function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_251.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">The preceding function implies that we take the minimum of two terms: one is <img src="../Images/B15558_13_252.png" alt="" style="height: 1.11em;"/> and the other is <img src="../Images/B15558_13_253.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">We know that the first term <img src="../Images/B15558_13_254.png" alt="" style="height: 1.11em;"/> is basically our objective, see equation (20), and the second term is called the clipped objective. Thus, our final objective function is just the minimum of the unclipped and clipped objectives. But what's the use of this? How does adding this clipped objective help us in keeping our new policy not far away from the old policy?</p>
    <p class="normal">Let's understand this by taking a closer look:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_251.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">We know that the first term (unclipped objective) is just given by equation (20). So, let's take a look into the second term, the clipped objective. It is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_256.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">By looking at the preceding term, we can say that we are clipping the probability ratio <img src="../Images/B15558_13_257.png" alt="" style="height: 1.11em;"/> in the range of <img src="../Images/B15558_13_258.png" alt="" style="height: 1.2em;"/>. But why do we have to clip <img src="../Images/B15558_13_259.png" alt="" style="height: 1.11em;"/>? This can be explained <a id="_idIndexMarker1236"/>by considering two cases of the advantage function—when the advantage is positive and when it is negative.</p>
    <p class="normal"><strong class="keyword">Case 1: When the advantage is positive</strong></p>
    <p class="normal">When the advantage is positive, <img src="../Images/B15558_13_260.png" alt="" style="height: 1.11em;"/>, then it means that the corresponding action should be preferred over the average of all other actions. So, we can increase the value of <img src="../Images/B15558_13_261.png" alt="" style="height: 1.11em;"/> for that action so that it will have a greater chance of being selected. However, while increasing the value of <img src="../Images/B15558_13_257.png" alt="" style="height: 1.11em;"/>, we should not increase it too much that it goes far away from the old policy. So, to prevent this, we clip <img src="../Images/B15558_13_261.png" alt="" style="height: 1.11em;"/> at <img src="../Images/B15558_13_264.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal"><em class="italic">Figure 13.10</em> shows how we increase the value of <img src="../Images/B15558_13_261.png" alt="" style="height: 1.11em;"/> when the advantage is positive and how we clip it at <img src="../Images/B15558_13_266.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.10: Value of <img src="../Images/B15558_13_267.png" alt="" style="height: 1.11em;"/> when the advantage is positive </p>
    <p class="normal"><strong class="keyword">Case 2: When the advantage is negative</strong></p>
    <p class="normal">When the <a id="_idIndexMarker1237"/>advantage is negative, <img src="../Images/B15558_13_268.png" alt="" style="height: 1.11em;"/>, then it means that the corresponding action should not be preferred over the average of all other actions. So, we can decrease the value of <img src="../Images/B15558_13_257.png" alt="" style="height: 1.11em;"/> for that action so that it will have a lower chance of being selected. However, while decreasing the value of <img src="../Images/B15558_13_257.png" alt="" style="height: 1.11em;"/>, we should not decrease it too much that it goes far away from the old policy. So, in order to prevent that, we clip <img src="../Images/B15558_13_257.png" alt="" style="height: 1.11em;"/> at <img src="../Images/B15558_13_272.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal"><em class="italic">Figure 13.11</em> shows how <a id="_idIndexMarker1238"/>we decrease the value of <img src="../Images/B15558_13_273.png" alt="" style="height: 1.11em;"/> when the advantage is negative and how we clip it at <img src="../Images/B15558_13_272.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 13.11: Value of <img src="../Images/B15558_13_275.png" alt="" style="height: 1.11em;"/> when the advantage is negative</p>
    <p class="normal">The value of <img src="../Images/B15558_13_276.png" alt="" style="height: 0.93em;"/> is usually set to 0.1 or 0.2. Thus, we learned that the clipped objective keeps our policy updates close to the old policy by clipping at <img src="../Images/B15558_13_266.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_04_123.png" alt="" style="height: 1.11em;"/> based on the advantage function. So, our final objective function takes the minimum value of the unclipped and clipped objectives as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_251.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Now that <a id="_idIndexMarker1239"/>we have learned how the PPO algorithm with a clipped objective works, let's look into the algorithm in the next section. </p>
    <h3 id="_idParaDest-356" class="title">Algorithm – PPO-clipped</h3>
    <p class="normal">The steps <a id="_idIndexMarker1240"/>involved in the PPO-clipped algorithm are given as follows:</p>
    <ol>
      <li class="numbered" value="1">Initialize the policy network parameter <img src="../Images/B15558_09_002.png" alt="" style="height: 1.11em;"/> and value network parameter <img src="../Images/B15558_12_213.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Collect <em class="italic">N</em> number of trajectories <img src="../Images/B15558_10_058.png" alt="" style="height: 1.67em;"/> following the policy <img src="../Images/B15558_10_120.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">Compute the return (reward-to-go) <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub></li>
      <li class="numbered">Compute the gradient of the objective function <img src="../Images/B15558_13_284.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Update the policy network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> using gradient ascent as <img src="../Images/B15558_13_286.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Compute the mean squared error of the value network:<figure class="mediaobject"><img src="../Images/B15558_10_149.png" alt="" style="height: 3.33em;"/></figure>
      </li>
      <li class="numbered">Compute the gradient of the value network <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/></li>
      <li class="numbered">Update the value network parameter <img src="../Images/B15558_13_289.png" alt="" style="height: 1.11em;"/> using gradient descent as <img src="../Images/B15558_10_150.png" alt="" style="height: 1.2em;"/></li>
      <li class="numbered">Repeat steps 2 to 8 for several iterations</li>
    </ol>
    <h2 id="_idParaDest-357" class="title">Implementing the PPO-clipped method</h2>
    <p class="normal">Let's <a id="_idIndexMarker1241"/>implement the PPO-clipped method for the swing-up pendulum task. The code used in this section is adapted from one of the very good PPO implementations (<a href="https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/12_Proxima"><span class="url">https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/tree/master/contents/12_Proximal_Policy_Optimization</span></a>) by Morvan.</p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">'ignore'</span>)
<span class="hljs-keyword">import</span> tensorflow.compat.v1 <span class="hljs-keyword">as</span> tf
tf.disable_v2_behavior()
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> gym
</code></pre>
    <h3 id="_idParaDest-358" class="title">Creating the Gym environment</h3>
    <p class="normal">Let's <a id="_idIndexMarker1242"/>create a pendulum environment using Gym:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'Pendulum-v0'</span>).unwrapped
</code></pre>
    <p class="normal">Get the state shape of the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">state_shape = env.observation_space.shape[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">Get the action shape of the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">action_shape = env.action_space.shape[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">Note that <a id="_idIndexMarker1243"/>the pendulum is a continuous environment and thus our action space consists of continuous values. So, we get the bound of our action space:</p>
    <pre class="programlisting code"><code class="hljs-code">action_bound = [env.action_space.low, env.action_space.high]
</code></pre>
    <p class="normal">Set the epsilon value that is used in the clipped objective:</p>
    <pre class="programlisting code"><code class="hljs-code">epsilon = <span class="hljs-number">0.2</span>
</code></pre>
    <h3 id="_idParaDest-359" class="title">Defining the PPO class</h3>
    <p class="normal">Let's define <a id="_idIndexMarker1244"/>a class called <code class="Code-In-Text--PACKT-">PPO</code> where we will <a id="_idIndexMarker1245"/>implement the PPO algorithm. For a clear understanding, let's take a look into the code line by line:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">PPO</span><span class="hljs-class">(</span><span class="hljs-params">object</span><span class="hljs-class">):</span>
</code></pre>
    <h4 class="title">Defining the init method</h4>
    <p class="normal">First, let's <a id="_idIndexMarker1246"/>define the <code class="Code-In-Text--PACKT-">init</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Start the TensorFlow session:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess = tf.Session()
</code></pre>
    <p class="normal">Define the placeholder for the state:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.state_ph = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, state_shape], <span class="hljs-string">'state'</span>)
</code></pre>
    <p class="normal">Now, let's build the value network that returns the value of a state:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'value'</span>):
            layer1 = tf.layers.dense(self.state_ph, <span class="hljs-number">100</span>, tf.nn.relu)
            self.v = tf.layers.dense(layer1, <span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Define the placeholder for the Q value:</p>
    <pre class="programlisting code"><code class="hljs-code">            self.Q = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">'discounted_r'</span>)
</code></pre>
    <p class="normal">Define the advantage value as the difference between the Q value and the state value:</p>
    <pre class="programlisting code"><code class="hljs-code">            self.advantage = self.Q - self.v
</code></pre>
    <p class="normal">Compute <a id="_idIndexMarker1247"/>the loss of the value network:</p>
    <pre class="programlisting code"><code class="hljs-code">            self.value_loss = tf.reduce_mean(tf.square(self.advantage))
</code></pre>
    <p class="normal">Train the value network by minimizing the loss using the Adam optimizer:</p>
    <pre class="programlisting code"><code class="hljs-code">            self.train_value_nw = tf.train.AdamOptimizer(<span class="hljs-number">0.002</span>).minimize(self.value_loss)
</code></pre>
    <p class="normal">Now, we obtain the new policy and its parameter from the policy network:</p>
    <pre class="programlisting code"><code class="hljs-code">        pi, pi_params = self.build_policy_network(<span class="hljs-string">'pi'</span>, trainable=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Obtain the old policy and its parameter from the policy network:</p>
    <pre class="programlisting code"><code class="hljs-code">        oldpi, oldpi_params = self.build_policy_network(<span class="hljs-string">'oldpi'</span>, trainable=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">Sample an action from the new policy:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'sample_action'</span>):
            self.sample_op = tf.squeeze(pi.sample(<span class="hljs-number">1</span>), axis=<span class="hljs-number">0</span>) 
</code></pre>
    <p class="normal">Update the parameters of the old policy:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'update_oldpi'</span>):
            self.update_oldpi_op = [oldp.assign(p) <span class="hljs-keyword">for</span> p, oldp <span class="hljs-keyword">in</span> zip(pi_params, oldpi_params)]
</code></pre>
    <p class="normal">Define the placeholder for the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.action_ph = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, action_shape], <span class="hljs-string">'action'</span>)
</code></pre>
    <p class="normal">Define the placeholder for the advantage:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.advantage_ph = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">'advantage'</span>)
</code></pre>
    <p class="normal">Now, let's define our surrogate objective function of the policy network:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'loss'</span>):
            <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'surrogate'</span>):
</code></pre>
    <p class="normal">We learned <a id="_idIndexMarker1248"/>that the objective of the policy network is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_251.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">First, let's define the ratio <img src="../Images/B15558_13_257.png" alt="" style="height: 1.11em;"/> as <img src="../Images/B15558_13_248.png" alt="" style="height: 2.69em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">                ratio = pi.prob(self.action_ph) / oldpi.prob(self.action_ph)
</code></pre>
    <p class="normal">Define the objective by multiplying the ratio <img src="../Images/B15558_13_257.png" alt="" style="height: 1.11em;"/> and the advantage value <em class="italic">A</em><sub class="" style="font-style: italic;">t</sub>:</p>
    <pre class="programlisting code"><code class="hljs-code">                objective = ratio * self.advantage_ph
</code></pre>
    <p class="normal">Define the objective function with the clipped and unclipped objectives:</p>
    <pre class="programlisting code"><code class="hljs-code">                L = tf.reduce_mean(tf.minimum(objective, tf.clip_by_value(ratio, <span class="hljs-number">1.</span>-epsilon, <span class="hljs-number">1.</span>+ epsilon)*self.advantage_ph))
</code></pre>
    <p class="normal">Now, we can compute the gradient and maximize the objective function using gradient ascent. However, instead of doing that, we can convert the preceding maximization objective into the minimization objective by just adding a negative sign. So, we can denote the loss of the policy network as:</p>
    <pre class="programlisting code"><code class="hljs-code">            self.policy_loss = -L
</code></pre>
    <p class="normal">Train the policy network by minimizing the loss using the Adam optimizer:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'train_policy'</span>):
            self.train_policy_nw = tf.train.AdamOptimizer(<span class="hljs-number">0.001</span>).minimize(self.policy_loss)
</code></pre>
    <p class="normal">Initialize <a id="_idIndexMarker1249"/>all the TensorFlow variables:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess.run(tf.global_variables_initializer())
</code></pre>
    <h4 class="title">Defining the train function</h4>
    <p class="normal">Now, let's <a id="_idIndexMarker1250"/>define the <code class="Code-In-Text--PACKT-">train</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">train</span><span class="hljs-function">(</span><span class="hljs-params">self, state, action, reward</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Update the old policy:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess.run(self.update_oldpi_op)
</code></pre>
    <p class="normal">Compute the advantage value:</p>
    <pre class="programlisting code"><code class="hljs-code">        adv = self.sess.run(self.advantage, {self.state_ph: state, self.Q: reward})
</code></pre>
    <p class="normal">Train the policy network:</p>
    <pre class="programlisting code"><code class="hljs-code">        [self.sess.run(self.train_policy_nw, {self.state_ph: state, self.action_ph: action, self.advantage_ph: adv}) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)]
</code></pre>
    <p class="normal">Train the value network:</p>
    <pre class="programlisting code"><code class="hljs-code">        [self.sess.run(self.train_value_nw, {self.state_ph: state, self.Q: reward}) <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> range(<span class="hljs-number">10</span>)]
</code></pre>
    <h4 class="title">Building the policy network</h4>
    <p class="normal">We define <a id="_idIndexMarker1251"/>a function called <code class="Code-In-Text--PACKT-">build_policy_network</code> for building the policy network. Note that our action space is continuous here, so our policy network returns the mean and variance of the action as an output and then we generate a normal distribution using this mean and variance and select an action by sampling from this normal distribution: </p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_policy_network</span><span class="hljs-function">(</span><span class="hljs-params">self, name, trainable</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">with</span> tf.variable_scope(name):
</code></pre>
    <p class="normal">Define the layer of the network:</p>
    <pre class="programlisting code"><code class="hljs-code">            layer = tf.layers.dense(self.state_ph, <span class="hljs-number">100</span>, tf.nn.relu, trainable=trainable)
</code></pre>
    <p class="normal">Compute the mean:</p>
    <pre class="programlisting code"><code class="hljs-code">            mu = <span class="hljs-number">2</span> * tf.layers.dense(layer, action_shape, tf.nn.tanh, trainable=trainable)
</code></pre>
    <p class="normal">Compute the standard deviation:</p>
    <pre class="programlisting code"><code class="hljs-code">            sigma = tf.layers.dense(layer, action_shape, tf.nn.softplus, trainable=trainable)
</code></pre>
    <p class="normal">Compute <a id="_idIndexMarker1252"/>the normal distribution:</p>
    <pre class="programlisting code"><code class="hljs-code">            norm_dist = tf.distributions.Normal(loc=mu, scale=sigma)
</code></pre>
    <p class="normal">Get the parameters of the policy network:</p>
    <pre class="programlisting code"><code class="hljs-code">        params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=name)
        <span class="hljs-keyword">return</span> norm_dist, params
</code></pre>
    <h4 class="title">Selecting the action</h4>
    <p class="normal">Let's define <a id="_idIndexMarker1253"/>a function called <code class="Code-In-Text--PACKT-">select_action</code> for selecting the action:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">select_action</span><span class="hljs-function">(</span><span class="hljs-params">self, state</span><span class="hljs-function">):</span>
        state = state[np.newaxis, :]
</code></pre>
    <p class="normal">Sample an action from the normal distribution generated by the policy network:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = self.sess.run(self.sample_op, {self.state_ph: state})[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">We clip the action so that it lies within the action bounds and then we return the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = np.clip(action, action_bound[<span class="hljs-number">0</span>], action_bound[<span class="hljs-number">1</span>])
        <span class="hljs-keyword">return</span> action
</code></pre>
    <h4 class="title">Computing the state value</h4>
    <p class="normal">We define <a id="_idIndexMarker1254"/>a function called <code class="Code-In-Text--PACKT-">get_state_value</code> to obtain the value of the state computed by the value network:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">get_state_value</span><span class="hljs-function">(</span><span class="hljs-params">self, state</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">if</span> state.ndim &lt; <span class="hljs-number">2</span>: state = state[np.newaxis, :]
        <span class="hljs-keyword">return</span> self.sess.run(self.v, {self.state_ph: state})[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>]
</code></pre>
    <h3 id="_idParaDest-360" class="title">Training the network</h3>
    <p class="normal">Now, let's <a id="_idIndexMarker1255"/>start training the network. First, let's create an object for our PPO class:</p>
    <pre class="programlisting code"><code class="hljs-code">ppo = PPO()
</code></pre>
    <p class="normal">Set the number of episodes:</p>
    <pre class="programlisting code"><code class="hljs-code">num_episodes = <span class="hljs-number">1000</span>
</code></pre>
    <p class="normal">Set the number of time steps in each episode:</p>
    <pre class="programlisting code"><code class="hljs-code">num_timesteps = <span class="hljs-number">200</span>
</code></pre>
    <p class="normal">Set the discount factor, <img src="../Images/B15558_03_190.png" alt="" style="height: 0.93em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">gamma = <span class="hljs-number">0.9</span>
</code></pre>
    <p class="normal">Set the batch size:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">32</span>
</code></pre>
    <p class="normal">For each episode:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_episodes):
</code></pre>
    <p class="normal">Initialize the state by resetting the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = env.reset()
</code></pre>
    <p class="normal">Initialize the lists for holding the states, actions, and rewards obtained in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    episode_states, episode_actions, episode_rewards = [], [], []
</code></pre>
    <p class="normal">Initialize the return:</p>
    <pre class="programlisting code"><code class="hljs-code">    Return = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">For every step:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
</code></pre>
    <p class="normal">Render the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        env.render()
</code></pre>
    <p class="normal">Select the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = ppo.select_action(state)
</code></pre>
    <p class="normal">Perform <a id="_idIndexMarker1256"/>the selected action:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state, reward, done, _ = env.step(action)
</code></pre>
    <p class="normal">Store the state, action, and reward in the list:</p>
    <pre class="programlisting code"><code class="hljs-code">        episode_states.append(state)
        episode_actions.append(action)
        episode_rewards.append((reward+<span class="hljs-number">8</span>)/<span class="hljs-number">8</span>)
</code></pre>
    <p class="normal">Update the state to the next state:</p>
    <pre class="programlisting code"><code class="hljs-code">        state = next_state
</code></pre>
    <p class="normal">Update the return:</p>
    <pre class="programlisting code"><code class="hljs-code">        Return += reward
</code></pre>
    <p class="normal">If we reached the batch size or if we reached the final step of the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> (t+<span class="hljs-number">1</span>) % batch_size == <span class="hljs-number">0</span> <span class="hljs-keyword">or</span> t == num_timesteps<span class="hljs-number">-1</span>:
</code></pre>
    <p class="normal">Compute the value of the next state:</p>
    <pre class="programlisting code"><code class="hljs-code">            v_s_ = ppo.get_state_value(next_state)
</code></pre>
    <p class="normal">Compute the Q value as <img src="../Images/B15558_13_296.png" alt="" style="height: 1.2em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">            discounted_r = []
            <span class="hljs-keyword">for</span> reward <span class="hljs-keyword">in</span> episode_rewards[::<span class="hljs-number">-1</span>]:
                v_s_ = reward + gamma * v_s_
                discounted_r.append(v_s_)
            discounted_r.reverse()
</code></pre>
    <p class="normal">Stack the episodic states, actions, and rewards:</p>
    <pre class="programlisting code"><code class="hljs-code">            es, ea, er = np.vstack(episode_states), np.vstack(episode_actions), np.array(discounted_r)[:, np.newaxis]
</code></pre>
    <p class="normal">Train the network:</p>
    <pre class="programlisting code"><code class="hljs-code">            ppo.train(es, ea, er)
</code></pre>
    <p class="normal">Empty <a id="_idIndexMarker1257"/>the lists: </p>
    <pre class="programlisting code"><code class="hljs-code">            episode_states, episode_actions, episode_rewards = [], [], []
</code></pre>
    <p class="normal">Print the return for every 10 episodes:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> i %<span class="hljs-number">10</span> ==<span class="hljs-number">0</span>:
         print(<span class="hljs-string">"Episode:{}, Return: {}"</span>.format(i,Return))
</code></pre>
    <p class="normal">Now that we have learned how PPO with a clipped objective works and how to implement it, in the next section we will learn about another interesting type of PPO algorithm called PPO with a penalized objective.</p>
    <h2 id="_idParaDest-361" class="title">PPO with a penalized objective</h2>
    <p class="normal">In the <a id="_idIndexMarker1258"/>PPO-penalty method, we convert the constraint term into a penalty term. First, let us recall the objective function of TRPO. We learned that the TRPO objective function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_245.png" alt="" style="height: 4.36em;"/></figure>
    <p class="normal">In the PPO-penalty method, we can rewrite the preceding objective by converting the KL constraint term into a penalty term as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_298.png" alt="" style="height: 3.16em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_06_030.png" alt="" style="height: 1.11em;"/> is called the penalty coefficient.</p>
    <p class="normal">Let <img src="../Images/B15558_13_300.png" alt="" style="height: 1.4em;"/> and let <img src="../Images/B15558_13_301.png" alt="" style="height: 1.11em;"/> be the target KL divergence; then, we set the <a id="_idIndexMarker1259"/>value of <img src="../Images/B15558_09_151.png" alt="" style="height: 1.11em;"/> adaptively as:</p>
    <ul>
      <li class="bullet">If <em class="italic">d</em> is greater than or equal to <img src="../Images/B15558_13_303.png" alt="" style="height: 1.11em;"/>, then we set <img src="../Images/B15558_13_304.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet">If <em class="italic">d</em> is less than or equal to <img src="../Images/B15558_13_305.png" alt="" style="height: 1.11em;"/>, then we set <img src="../Images/B15558_13_306.png" alt="" style="height: 1.11em;"/></li>
    </ul>
    <p class="normal">We can understand how exactly this works by looking into the PPO-penalty algorithm in the next section.</p>
    <h3 id="_idParaDest-362" class="title">Algorithm – PPO-penalty</h3>
    <p class="normal">The steps <a id="_idIndexMarker1260"/>involved in the PPO-penalty algorithm are:</p>
    <ol>
      <li class="numbered" value="1">Initialize the policy network parameter <img src="../Images/B15558_09_008.png" alt="" style="height: 1.11em;"/> and the value network parameter <img src="../Images/B15558_13_308.png" alt="" style="height: 1.11em;"/>, and initialize the penalty coefficient <img src="../Images/B15558_13_309.png" alt="" style="height: 1.11em;"/> and the target KL divergence <img src="../Images/B15558_13_310.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For iterations <img src="../Images/B15558_13_311.png" alt="" style="height: 1.11em;"/>:<ol>
          <li class="numbered-l2">Collect <em class="italic">N</em> number of trajectories following the policy <img src="../Images/B15558_10_111.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Compute the return (reward-to-go) <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub></li>
          <li class="numbered-l2">Compute <img src="../Images/B15558_13_313.png" alt="" style="height: 3.16em;"/></li>
          <li class="numbered-l2">Compute the gradient of the objective function <img src="../Images/B15558_13_284.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Update the policy network parameter <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> using gradient ascent as <img src="../Images/B15558_13_316.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">If <em class="italic">d</em> is greater than or equal to <img src="../Images/B15558_13_317.png" alt="" style="height: 1.11em;"/>, then we set <img src="../Images/B15558_13_318.png" alt="" style="height: 1.11em;"/>; if <em class="italic">d</em> is less than or equal to <img src="../Images/B15558_13_319.png" alt="" style="height: 1.11em;"/>, then we set <img src="../Images/B15558_13_306.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the mean squared error of the value network:<figure class="mediaobject"><img src="../Images/B15558_13_321.png" alt="" style="height: 3.33em;"/></figure>
          </li>
          <li class="numbered-l2">Compute <a id="_idIndexMarker1261"/>the gradients of the value network <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update the value network parameter <img src="../Images/B15558_13_234.png" alt="" style="height: 1.11em;"/> using gradient descent as <img src="../Images/B15558_10_150.png" alt="" style="height: 1.2em;"/></li>
        </ol>
      </li>
    </ol>
    <p class="normal">Thus, we <a id="_idIndexMarker1262"/>learned how PPO-clipped and PPO-penalized objectives work. In general, PPO with a clipped objective is used more often than the PPO method with a penalized objective.</p>
    <p class="normal">In the next section, we will learn another interesting algorithm called ACKTR.</p>
    <h1 id="_idParaDest-363" class="title">Actor-critic using Kronecker-factored trust region </h1>
    <p class="normal">ACKTR, as the name suggests, is the actor-<a id="_idIndexMarker1263"/>critic algorithm based on the Kronecker factorization and trust region. </p>
    <p class="normal">We know that the actor-critic architecture consists of the actor and critic networks, where the role of the actor is to produce a policy and the role of the critic is to evaluate the policy produced by the actor network. We learned that in the actor network (policy network), we compute gradients and update the parameter of the actor network using gradient ascent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_10_027.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Instead of updating our actor network parameter using the preceding update rule, we can also update it by computing the natural gradients as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_326.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where <em class="italic">F</em> is called the Fisher information matrix. Thus, the natural gradient is just the product of the inverse of the Fisher matrix and standard gradient:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_14.png" alt="" style="height:5em;"/></figure>
    <p class="normal">The use of the natural gradient is that it guarantees a monotonic improvement in the policy. However, updating the actor network (policy network) parameter using the preceding update rule is a computationally expensive task, because computing the Fisher information matrix and then taking its inverse is a computationally expensive task. So, to avoid this tedious computation, we can just approximate the value of <img src="../Images/B15558_13_327.png" alt="" style="height: 1.2em;"/> using a Kronecker-factored approximation. Once we approximate <img src="../Images/B15558_13_327.png" alt="" style="height: 1.2em;"/> using a Kronecker-factored approximation, then we can just update our policy network parameter <a id="_idIndexMarker1264"/>using the natural gradient update rule given in equation (21), and while updating the policy network parameter, we also ensure that the policy updates are in the trust region so that the new policy is not far from the old policy. This is the main idea behind the ACKTR algorithm.</p>
    <p class="normal">Now that we have a basic understanding of what ACKTR is, let us understand how this works exactly in detail. First, we will understand what Kronecker factorization is, then we will learn how it is used in the actor-critic setting, and later we will learn how to incorporate the trust region in the policy updates. </p>
    <p class="normal">Before going ahead, let's learn several math concepts that are required to understand ACKTR. </p>
    <h2 id="_idParaDest-364" class="title">Math essentials</h2>
    <p class="normal">To <a id="_idIndexMarker1265"/>understand how Kronecker factorization works, we will learn the following important concepts:</p>
    <ul>
      <li class="bullet">Block matrix</li>
      <li class="bullet">Block diagonal matrix</li>
      <li class="bullet">The Kronecker product</li>
      <li class="bullet">The vec operator</li>
      <li class="bullet">Properties of the Kronecker product</li>
    </ul>
    <h3 id="_idParaDest-365" class="title">Block matrix</h3>
    <p class="normal">A block matrix <a id="_idIndexMarker1266"/>is defined as a matrix that can be broken <a id="_idIndexMarker1267"/>down into submatrices called blocks, or we can say a block matrix is formed by a set of submatrices or blocks. For instance, let's consider a block matrix <em class="italic">A</em> as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_329.png" alt="" style="height: 4.18em;"/></figure>
    <p class="normal">The matrix <em class="italic">A</em> can be broken into four <img src="../Images/B15558_13_330.png" alt="" style="height: 1.11em;"/> submatrices as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_331.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Now, we can simply write our block matrix <em class="italic">A</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_332.png" alt="" style="height: 2.22em;"/></figure>
    <h3 id="_idParaDest-366" class="title">Block diagonal matrix</h3>
    <p class="normal">A block diagonal <a id="_idIndexMarker1268"/>matrix is a block matrix <a id="_idIndexMarker1269"/>that consists of a square matrix on the diagonals, and off-diagonal elements are set to 0. A block diagonal matrix <em class="italic">A</em> is represented as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_333.png" alt="" style="height: 4.53em;"/></figure>
    <p class="normal">Where the diagonals <img src="../Images/B15558_13_334.png" alt="" style="height: 1.11em;"/> are the square matrices.</p>
    <p class="normal">An example of a block diagonal matrix is shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_335.png" alt="" style="height: 6.49em;"/></figure>
    <p class="normal">As we <a id="_idIndexMarker1270"/>can see, the diagonals are basically the square matrix <a id="_idIndexMarker1271"/>and off-diagonal elements are set to zero:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_15.png" alt="" style="height:11em;"/></figure>
    <p class="normal">Thus, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_336.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Now, we can simply denote our block diagonal matrix <em class="italic">A</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_337.png" alt="" style="height: 3.42em;"/></figure>
    <h3 id="_idParaDest-367" class="title">The Kronecker product</h3>
    <p class="normal">The Kronecker <a id="_idIndexMarker1272"/>product is an operation performed <a id="_idIndexMarker1273"/>between two matrices. The Kronecker product is not the same as matrix multiplication. When we perform the Kronecker product between two matrices, it will output the block matrix. The Kronecker product is denoted by <img src="../Images/B15558_13_338.png" alt="" style="height: 1.11em;"/>. Let us say we have a matrix <em class="italic">A</em> of order <img src="../Images/B15558_13_339.png" alt="" style="height: 0.93em;"/> and a matrix <em class="italic">B</em> of order <img src="../Images/B15558_13_340.png" alt="" style="height: 0.93em;"/>; the Kronecker product of matrices <em class="italic">A</em> and <em class="italic">B</em> is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_341.png" alt="" style="height: 4.62em;"/></figure>
    <p class="normal">This implies that we multiply every element in matrix <em class="italic">A</em> by matrix <em class="italic">B</em>. Let us understand this with an example. Say we have two matrices <em class="italic">A</em> and <em class="italic">B</em> as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_342.png" alt="" style="height: 2.13em;"/></figure>
    <p class="normal">Then the Kronecker product of matrices <em class="italic">A</em> and <em class="italic">B</em> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_16.png" alt="" style="height:25em;"/></figure>
    <h3 id="_idParaDest-368" class="title">The vec operator</h3>
    <p class="normal">The vec <a id="_idIndexMarker1274"/>operator creates a column vector by stacking all <a id="_idIndexMarker1275"/>the columns in a matrix below one another. For instance, let's consider a matrix <em class="italic">A</em> as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_343.png" alt="" style="height: 2.13em;"/></figure>
    <p class="normal">Applying the vec operator on <em class="italic">A</em> stacks all the columns in the matrix one below the other as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_344.png" alt="" style="height: 4.44em;"/></figure>
    <h3 id="_idParaDest-369" class="title">Properties of the Kronecker product</h3>
    <p class="normal">The Kronecker <a id="_idIndexMarker1276"/>product has several <a id="_idIndexMarker1277"/>useful properties; these include:</p>
    <ul>
      <li class="bullet"><img src="../Images/B15558_13_345.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet"><img src="../Images/B15558_13_346.png" alt="" style="height: 1.2em;"/></li>
      <li class="bullet"><img src="../Images/B15558_13_347.png" alt="" style="height: 1.2em;"/></li>
    </ul>
    <p class="normal">Now that we have learned several important concepts, let's understand what Kronecker factorization is. </p>
    <h2 id="_idParaDest-370" class="title">Kronecker-Factored Approximate Curvature (K-FAC) </h2>
    <p class="normal">Let's <a id="_idIndexMarker1278"/>suppose we have a neural network parametrized by <img src="../Images/B15558_09_054.png" alt="" style="height: 1.11em;"/> and we train the neural network using gradient descent. We can write our update rule, including the natural gradient, as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_349.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where <em class="italic">F</em> is the Fisher information matrix. The problem is that computing <em class="italic">F</em> and finding its inverse is an expensive task. So to avoid that, we use a Kronecker-factored approximation to approximate the value of <img src="../Images/B15558_13_350.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Let's learn how we approximate <img src="../Images/B15558_13_350.png" alt="" style="height: 1.2em;"/> using Kronecker factors. Say our network has <img src="../Images/B15558_13_352.png" alt="" style="height: 1.11em;"/> layers and the weight of the network is represented by <img src="../Images/B15558_09_123.png" alt="" style="height: 1.11em;"/>. Thus, <img src="../Images/B15558_13_354.png" alt="" style="height: 1.11em;"/> denotes the weights of the layers <img src="../Images/B15558_13_355.png" alt="" style="height: 1.11em;"/> respectively. Let <img src="../Images/B15558_13_356.png" alt="" style="height: 1.11em;"/> denote the output distribution of the network, and we will use the negative log-likelihood as the loss function <em class="italic">J</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_357.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Then, the Fisher information matrix can be written as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_358.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">K-FAC <a id="_idIndexMarker1279"/>approximates the Fisher information matrix, <em class="italic">F</em>, as a block diagonal matrix where each block refers to the gradients of the loss with respect to the weights of a particular layer. For example, the block <em class="italic">F</em><sub class="Subscript--PACKT-">1</sub> denotes the gradients of the loss with respect to the weights of layer 1. The block <em class="italic">F</em><sub class="Subscript--PACKT-">2</sub> denotes the gradients of the loss with respect to the weights of layer 2. The block <em class="italic">F</em><sub class="" style="font-style: italic;">l</sub> denotes the gradients of the loss with respect to the weights of layer <em class="italic">l</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_359.png" alt="" style="height: 6.67em;"/></figure>
    <p class="normal">That is, <img src="../Images/B15558_13_360.png" alt="" style="height: 1.11em;"/>, where:</p>
    <ul>
      <li class="bullet"><img src="../Images/B15558_13_361.png" alt="" style="height: 1.76em;"/></li>
      <li class="bullet"><img src="../Images/B15558_13_362.png" alt="" style="height: 1.76em;"/></li>
      <li class="bullet"><img src="../Images/B15558_13_363.png" alt="" style="height: 1.76em;"/></li>
      <li class="bullet"><img src="../Images/B15558_13_364.png" alt="" style="height: 1.76em;"/></li>
    </ul>
    <p class="normal">As we can observe, each block <em class="italic">F</em><sub class="Subscript--PACKT-">1</sub> to <em class="italic">F</em><sub class="" style="font-style: italic;">L</sub> contains the derivatives of loss <em class="italic">J</em> with respect to the weights of the corresponding layer. Okay, how can we compute each block? That is, how can the values in the preceding block diagonal matrix be computed? </p>
    <p class="normal">To understand this, let's just take one block, say, <em class="italic">F</em><sub class="" style="font-style: italic;">l</sub>, and learn how it is computed. Let's take a layer <em class="italic">l</em>. Let <em class="italic">a</em> be the input activation vector, let <img src="../Images/B15558_13_365.png" alt="" style="height: 1.11em;"/> be the weights of the layer, and let <em class="italic">s</em> be the output pre-activation vector, and it can be sent to the next layer <em class="italic">l</em> + 1.</p>
    <p class="normal">We know <a id="_idIndexMarker1280"/>that in the neural network, we multiply the activation vector by weights and send that to the next layer; so, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_366.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">We can approximate the block <em class="italic">F</em><sub class="" style="font-style: italic;">l</sub> corresponding to layer <em class="italic">l</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_367.png" alt="" style="height: 1.76em;"/></figure>
    <p class="normal">The preceding equation <em class="italic">F</em><sub class="" style="font-style: italic;">l</sub> denotes the gradient of the loss with respect to the weights of layer <em class="italic">l</em>.</p>
    <p class="normal">From (22), the partial derivative of the loss function <em class="italic">J</em> with respect to weights <img src="../Images/B15558_13_368.png" alt="" style="height: 1.11em;"/> in layer <em class="italic">l</em> can be written as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_369.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Substituting (24) in (23), we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_370.png" alt="" style="height: 3.89em;"/></figure>
    <p class="normal">The preceding <a id="_idIndexMarker1281"/>equation implies that <em class="italic">F</em><sub class="" style="font-style: italic;">l</sub> is just the expected value of the Kronecker product. So, we can rewrite it as the Kronecker product of the expected value; that is, <em class="italic">F</em><sub class="" style="font-style: italic;">l</sub> can be approximated as the Kronecker product of the expected value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_371.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Let <img src="../Images/B15558_13_372.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_13_373.png" alt="" style="height: 1.2em;"/>. We can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_374.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">This is known as Kronecker factorization and <em class="italic">A</em> and <em class="italic">S</em> are called the Kronecker factors. Now that we have learned how to compute the block <em class="italic">F</em><sub class="" style="font-style: italic;">l</sub>, let's learn how to update the weights <img src="../Images/B15558_13_368.png" alt="" style="height: 1.11em;"/> of the layer <em class="italic">l</em>.</p>
    <p class="normal">The update rule for updating the weights <img src="../Images/B15558_13_376.png" alt="" style="height: 1.11em;"/> of the layer <em class="italic">l</em> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_377.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Let <img src="../Images/B15558_13_378.png" alt="" style="height: 1.29em;"/>. We can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_379.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Let's see how to compute the value of <img src="../Images/B15558_13_380.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_378.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Applying <a id="_idIndexMarker1282"/>the vec operator on both sides, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_382.png" alt="" style="height: 1.4em;"/></figure>
    <p class="normal">From (25), we can substitute the value of <em class="italic">F</em><sub class="" style="font-style: italic;">l</sub> and write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_383.png" alt="" style="height: 1.4em;"/></figure>
    <p class="normal">Using the properties <img src="../Images/B15558_13_384.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_13_347.png" alt="" style="height: 1.2em;"/>, we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_386.png" alt="" style="height: 1.4em;"/></figure>
    <p class="normal">As you may observe, we have computed the value of <img src="../Images/B15558_13_387.png" alt="" style="height: 1.11em;"/> without expensive computation of the inverse of the Fisher information matrix using Kronecker factors. Now, using the value of <img src="../Images/B15558_13_388.png" alt="" style="height: 1.11em;"/> we just derived, we can update the weights <img src="../Images/B15558_13_389.png" alt="" style="height: 1.11em;"/> of the layer <em class="italic">l</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_379.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">In a nutshell, K-FAC approximates the Fisher information matrix as a block diagonal matrix where <a id="_idIndexMarker1283"/>each block contains the derivatives. Then, each block is approximated as the Kronecker product of two matrices, which is known as Kronecker factorization.</p>
    <p class="normal">Thus, we have learned how to approximate the natural gradient using Kronecker factors. In the next section, we will learn how to apply this in an actor-critic setting. </p>
    <h2 id="_idParaDest-371" class="title">K-FAC in actor-critic</h2>
    <p class="normal">We <a id="_idIndexMarker1284"/>know that in the actor-critic method, we have actor and critic networks. The role of the actor is to produce the policy and the role of the critic is to evaluate the policy <a id="_idIndexMarker1285"/>produced by the actor network. </p>
    <p class="normal">First, let's take a look at the actor network. In the actor network, our goal is to find the optimal policy. So, we try to find the optimal parameter <img src="../Images/B15558_09_087.png" alt="" style="height: 1.11em;"/> with which we can obtain the optimal policy. We compute gradients and update the parameter of the actor network using gradient ascent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_10_027.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Instead of updating the actor network parameter using the preceding update rule, we can also update the parameter of the actor network by computing the natural gradients as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_393.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">But computing <img src="../Images/B15558_13_350.png" alt="" style="height: 1.2em;"/> is an expensive task. So, we can use Kronecker factorization for approximating the value of <img src="../Images/B15558_13_350.png" alt="" style="height: 1.2em;"/>. We can define the Fisher information matrix for the actor network <em class="italic">F</em> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_396.png" alt="" style="height: 1.76em;"/></figure>
    <p class="normal">Now, just as we learned in the previous section, we can approximate the Fisher information matrix as a block diagonal matrix where each block contains the derivatives, and <a id="_idIndexMarker1286"/>then we can approximate each block as the Kronecker product of two matrices.</p>
    <p class="normal">Let <img src="../Images/B15558_13_397.png" alt="" style="height: 1.2em;"/>. We can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_156.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">The <a id="_idIndexMarker1287"/>value of <img src="../Images/B15558_13_399.png" alt="" style="height: 1.11em;"/> can be computed using Kronecker factorization as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_400.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">This is exactly the same as what we learned in the previous section.</p>
    <p class="normal">Now, let's look at the critic network. We know that the critic evaluates the policy produced by the actor network by estimating the Q function. So, we train the critic by minimizing the mean squared error between the target value and predicted value. </p>
    <p class="normal">We minimize the loss using gradient descent and update the critic network parameter <img src="../Images/B15558_12_213.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_10_150.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> is the standard first-order gradient.</p>
    <p class="normal">Instead of using the first-order gradient, can we use the second-order gradient and update the critic network parameter <img src="../Images/B15558_10_148.png" alt="" style="height: 1.11em;"/>, similar to what we did with the actor? Yes, in settings like least squares (MSE), we can use an algorithm called the Gauss-Newton method for finding the second-order derivative. You can learn more about the Gauss-Newton <a id="_idIndexMarker1288"/>method here: <a href="http://www.seas.ucla.edu/~vandenbe/236C/lectures/gn.pdf"><span class="url">http://www.seas.ucla.edu/~vandenbe/236C/lectures/gn.pdf</span></a>. Let's represent our error as <img src="../Images/B15558_13_405.png" alt="" style="height: 1.11em;"/>. According to the Gauss-Newton method, the update rule for updating the critic network parameter <img src="../Images/B15558_13_406.png" alt="" style="height: 1.11em;"/> is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_407.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Where <em class="italic">G</em> is called the Gauss-Newton matrix, and it is given as <img src="../Images/B15558_13_408.png" alt="" style="height: 1.2em;"/>, and <em class="italic">J</em> is the Jacobian <a id="_idIndexMarker1289"/>matrix. (A Jacobian matrix is a matrix that contains a first-order partial derivative for a vector-valued function.)</p>
    <p class="normal">If you <a id="_idIndexMarker1290"/>look at the preceding equation, computing <img src="../Images/B15558_13_409.png" alt="" style="height: 1.2em;"/> is equivalent to computing the <img src="../Images/B15558_13_350.png" alt="" style="height: 1.2em;"/> we saw in the actor network. That is, computing the inverse of the Gauss-Newton matrix is equivalent to computing the inverse of the Fisher information matrix. So, we can use the Kronecker factor (K-FAC) to approximate the value of <img src="../Images/B15558_13_409.png" alt="" style="height: 1.2em;"/> just like we approximated the value of <img src="../Images/B15558_13_412.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Instead of applying K-FAC to the actor and critic separately, we can also apply them in a shared mode. As specified in the paper <strong class="keyword">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</strong> by Yuhuai Wu, Elman Mansimov, Shun Liao, Roger Grosse, Jimmy Ba (<a href="https://arxiv.org/pdf/1708.05144.pdf"><span class="url">https://arxiv.org/pdf/1708.05144.pdf</span></a>), "<em class="italic">We can have a single architecture where both actor and critic share the lower layer representations but they have different output layers</em>."</p>
    <p class="normal">In a nutshell, in the ACKTR method, we update the parameters of the actor and critic networks by computing the second-order derivatives. Since computing the second-order derivative <a id="_idIndexMarker1291"/>is an expensive task, we use a method called Kronecker-factored approximation to approximate the second-order derivative.</p>
    <p class="normal">In the <a id="_idIndexMarker1292"/>next section, we will learn how to incorporate the trust region into our update rule so that our new and old policy updates will not be too far apart.</p>
    <h2 id="_idParaDest-372" class="title">Incorporating the trust region</h2>
    <p class="normal">We learned <a id="_idIndexMarker1293"/>that we can update the parameter of our network with a natural gradient as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_13_413.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">In the previous section, we learned how we can use K-FAC to approximate the <img src="../Images/B15558_13_350.png" alt="" style="height: 1.2em;"/> matrix. While updating the policy, we need to make sure that our policy updates are in the trust region; that is, our new policy should not be too far away from the old policy. So to ensure this, we can choose the step size <img src="../Images/B15558_09_143.png" alt="" style="height: 0.93em;"/> as <img src="../Images/B15558_13_416.png" alt="" style="height: 3.89em;"/>, where <img src="../Images/B15558_09_143.png" alt="" style="height: 0.93em;"/> and the trust region radius <img src="../Images/B15558_09_135.png" alt="" style="height: 1.11em;"/> are the hyperparameters, as mentioned in the ACKTR paper (refer to the <em class="italic">Further reading</em> section). Updating our network parameters with this step size ensures that our policy updates are in the trust region.</p>
    <h1 id="_idParaDest-373" class="title">Summary</h1>
    <p class="normal">We started off the chapter by understanding what TRPO is and how it acts as an improvement to the policy gradient algorithm. We learned that when the new policy and old policy vary greatly then it causes model collapse.</p>
    <p class="normal">So in TRPO, we make a policy update while imposing the constraint that the parameters of the old and new policies should stay within the trust region. We also learned that TRPO guarantees monotonic policy improvement; that is, it guarantees that there will always be a policy improvement on every iteration.</p>
    <p class="normal">Later, we learned about the PPO algorithm, which acts as an improvement to the TRPO algorithm. We learned about two types of PPO algorithm: PPO-clipped and PPO-penalty. In the PPO-clipped method, in order to ensure that the policy updates are in the trust region, PPO adds a new function called the clipping function that ensures the new and old policies are not far away from each other. In the PPO-penalty method, we modify our objective function by converting the KL constraint term to a penalty term and update the penalty coefficient adaptively during training by ensuring that the policy updates are in the trust region.</p>
    <p class="normal">At the end of the chapter, we learned about ACKTR. In the ACKTR method, we update the parameters of the actor and critic networks by computing the second-order derivative. Since computing the second-order derivative is an expensive task, we use a method called Kronecker-factored approximation to approximate the second-order derivatives, and while updating the policy network parameter, we also ensure that the policy updates are in the trust region so that the new policy is not far from the old policy.</p>
    <p class="normal">In the next chapter, we will learn about several interesting distributional reinforcement learning algorithms.</p>
    <h1 id="_idParaDest-374" class="title">Questions</h1>
    <p class="normal">Let's evaluate our understanding of the algorithms we learned in this chapter. Try answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">What is a trust region?</li>
      <li class="numbered">Why is TRPO useful?</li>
      <li class="numbered">How does the conjugate gradient method differ from gradient descent?</li>
      <li class="numbered">What is the update rule of TRPO?</li>
      <li class="numbered">How does PPO differ from TRPO?</li>
      <li class="numbered">Explain the PPO-clipped method.</li>
      <li class="numbered">What is Kronecker factorization? </li>
    </ol>
    <h1 id="_idParaDest-375" class="title">Further reading</h1>
    <p class="normal">For more information, refer to the following papers:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Trust Region Policy Optimization </strong>by <em class="italic">John Schulman</em>, <em class="italic">Sergey Levine, Philipp Moritz</em>, <em class="italic">Michael I. Jordan</em>, <em class="italic">Pieter Abbeel</em>, <a href="https://arxiv.org/pdf/1502.05477.pdf"><span class="url">https://arxiv.org/pdf/1502.05477.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Proximal Policy Optimization Algorithms</strong> by <em class="italic">John Schulman</em>, <em class="italic">Filip Wolski</em>, <em class="italic">Prafulla Dhariwal</em>, <em class="italic">Alec Radford</em>, <em class="italic">Oleg Klimov</em>, <a href="https://arxiv.org/pdf/1707.06347.pdf"><span class="url">https://arxiv.org/pdf/1707.06347.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Scalable trust-region method for deep reinforcement learning using Kronecker-factored approximation</strong> by<strong class="keyword"> </strong><em class="italic">Yuhuai Wu</em>, <em class="italic">Elman Mansimov</em>, <em class="italic">Shun Liao</em>, <em class="italic">Roger Grosse</em>, <em class="italic">Jimmy Ba</em>, <a href="https://arxiv.org/pdf/1708.05144.pdf"><span class="url">https://arxiv.org/pdf/1708.05144.pdf</span></a></li>
    </ul>
  </div>
</body></html>