- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement learning** (**RL**) is a popular and promising branch of artificial
    intelligence that involves making smarter models and agents that can automatically
    determine ideal behavior based on changing requirements. *Reinforcement Learning
    Algorithms with Python* will help you master RL algorithms and understand their
    implementation as you build self-learning agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Starting with an introduction to the tools, libraries, and setup needed to work
    in the RL environment, this book covers the building blocks of RL and delves into
    value-based methods such as the application of Q-learning and SARSA algorithms.
    You'll learn how to use a combination of Q-learning and neural networks to solve
    complex problems. Furthermore, you'll study policy gradient methods, TRPO, and
    PPO, to improve performance and stability, before moving on to the DDPG and TD3
    deterministic algorithms. This book also covers how imitation learning techniques
    work and how Dagger can teach an agent to fly. You'll discover evolutionary strategies
    and black-box optimization techniques. Finally, you'll get to grips with exploration
    approaches such as UCB and UCB1 and develop a meta-algorithm called ESBAS.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the book, you'll have worked with key RL algorithms to overcome
    challenges in real-world applications, and you'll be part of the RL research community.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you are an AI researcher, deep learning user, or anyone who wants to learn
    RL from scratch, this book is for you. You'll also find this RL book useful if
    you want to learn about the advancements in the field. Working knowledge of Python
    is necessary.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Chapter 1](0469636f-09ff-417a-84ec-3fc0e4b4be08.xhtml), *The Landscape of
    Reinforcement Learning*, gives you an insight into RL. It describes the problems
    that RL is good at solving and the applications where RL algorithms are already
    adopted. It also introduces the tools, the libraries, and the setup needed for
    the completion of the projects in the following chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 2](716e2db3-37c0-4f2c-8397-81c0c812c80e.xhtml),* Implementing RL Cycle
    and OpenAI Gym*, describes the main cycle of the RL algorithms, the toolkit used
    to develop the algorithms, and the different types of environments. You will be
    able to develop a random agent using the OpenAI Gym interface to play CartPole
    using random actions. You will also learn how to use the OpenAI Gym interface
    to run other environments.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 3](f2414b11-976a-4410-92d8-89ee54745d99.xhtml), *Solving Problems
    with Dynamic Programming*, introduces to you the core ideas, terminology, and
    approaches of RL. You will learn about the main blocks of RL and develop a general
    idea about how RL algorithms can be created to solve a problem. You will also
    learn the differences between model-based and model-free algorithms and the categorization
    of reinforcement learning algorithms. Dynamic programming will be used to solve
    the game FrozenLake.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 4](6364d5e9-40e9-4991-a88f-1ce5b0f0a6e9.xhtml), *Q-Learning and SARSA
    Applications*, talks about value-based methods, in particular Q-learning and SARSA,
    two algorithms that differ from dynamic programming and scale well on large problems.
    To become confident with these algorithms, you will apply them to the FrozenLake
    game and study the differences from dynamic programming.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 5](b2fa8158-6d3c-469a-964d-a800942472ca.xhtml), *Deep Q-Networks*,
    describes how neural networks and **convolutional neural networks** (**CNNs**)
    in particular are applied to Q-learning. You''ll learn why the combination of
    Q-learning and neural networks produces incredible results and how its use can
    open the door to a much larger variety of problems. Furthermore, you''ll apply
    the DQN to an Atari game using the OpenAI Gym interface.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 6](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml), *Learning Stochastic
    and PG Optimization*, introduces a new family of model-free algorithms: policy
    gradient methods. You will learn the differences between policy gradient and value-based
    methods, and you''ll learn about their strengths and weaknesses. Then you will
    implement the REINFORCE and Actor-Critic algorithms to solve a new game called
    LunarLander.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 7](4148a47d-a2c7-44da-ba6d-76ae4d8bec2c.xhtml), *TRPO and PPO Implementation*,
    proposes a modification of policy gradient methods using new mechanisms to control
    the improvement of the policy. These mechanisms are used to improve the stability
    and convergence of the policy gradient algorithms. In particular you''ll learn
    and implement two main policy gradient methods that use these techniques, namely
    TRPO and PPO. You will implement them on RoboSchool, an environment with a continuous
    action space.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 8](d902d278-a1f5-438a-9ddd-6d9d665f2fa2.xhtml), *DDPG and TD3 Applications*,
    introduces a new category of algorithms called deterministic policy algorithms
    that combine both policy gradient and Q-learning. You will learn about the underlying
    concepts and implement DDPG and TD3, two deep deterministic algorithms, on a new
    environment.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 9](7e8448cf-7b74-4f07-99bd-da8f98f4505c.xhtml), *Model-Based RL*, illustrates
    RL algorithms that learn the model of the environment to plan future actions,
    or, to learn a policy. You will be taught how they work, their strengths, and
    why they are preferred in many situations. To master them, you will implement
    a model-based algorithm on Roboschool.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 10](e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml), *Imitation Learning
    with the DAgger Algorithm*, explains how imitation learning works and how it can
    be applied and adapted to a problem. You will learn about the most well-known
    imitation learning algorithm, DAgger. To become confident with it, you will implement
    it to speed up the learning process of an agent on FlappyBird.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 11](dab022a7-3243-4e45-9f91-39a82df3a248.xhtml), *Understanding Black-Box
    Optimization Algorithms*, explores evolutionary algorithms, a class of black-box
    optimization algorithms that don''t rely on backpropagation. These algorithms
    are gaining interest because of their fast training and easy parallelization across
    hundreds or thousands of cores. This chapter provides a theoretical and practical
    background of these algorithms by focusing particularly on the Evolution Strategy
    algorithm, a type of evolutionary algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 12](800cfc13-07b5-4b59-a8d9-b93ea3320237.xhtml), *Developing ESBAS
    Algorithm*, introduces the important exploration-exploitation dilemma, which is
    specific to RL. The dilemma is demonstrated using the multi-armed bandit problem
    and is solved using approaches such as UCB and UCB1\. Then, you will learn about
    the problem of algorithm selection and develop a meta-algorithm called ESBAS.
    This algorithm uses UCB1 to select the most appropriate RL algorithm for each
    situation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 13](719f001d-db10-47a4-98c5-fe95666f7c32.xhtml), *Practical Implementations
    to Resolve RL Challenges*, takes a look at the major challenges in this field
    and explains some practices and methods to overcome them. You will also learn
    about some of the challenges of applying RL to real-world problems, future developments
    of deep RL, and their social impact in the world.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working knowledge of Python is necessary. Knowledge of RL and the various tools
    used for it will also be beneficial.
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from your account at [www.packt.com](http://www.packt.com).
    If you purchased this book elsewhere, you can visit [www.packtpub.com/support](https://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the code files by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in or register at [www.packt.com](http://www.packt.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the Support tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Code Downloads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the name of the book in the Search box and follow the onscreen instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  prefs: []
  type: TYPE_NORMAL
- en: WinRAR/7-Zip for Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipeg/iZip/UnRarX for Mac
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7-Zip/PeaZip for Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python). In
    case there's an update to the code, it will be updated on the existing GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at **[https://github.com/PacktPublishing/](https://github.com/PacktPublishing/)**.
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)[.](http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`CodeInText`: Indicates code words in text, database table names, folder names,
    filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles.
    Here is an example: "In this book, we use Python 3.7, but all versions above 3.5
    should work. We also assume that you''ve already installed `numpy` and `matplotlib`."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Any command-line input or output is written as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '**Bold**: Indicates a new term, an important word, or words that you see onscreen.
    For example, words in menus or dialog boxes appear in the text like this. Here
    is an example: "In **reinforcement learning** (**RL**), the algorithm is called
    the agent, and it learns from the data provided by an environment."'
  prefs: []
  type: TYPE_NORMAL
- en: Warnings or important notes appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Tips and tricks appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: If you have questions about any aspect of this book,
    mention the book title in the subject of your message and email us at `customercare@packtpub.com`.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](https://www.packtpub.com/support/errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at `copyright@packt.com` with a link
    to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: Reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Packt, please visit [packt.com](http://www.packt.com/).
  prefs: []
  type: TYPE_NORMAL
