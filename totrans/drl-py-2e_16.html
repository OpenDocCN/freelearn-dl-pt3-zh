<html><head></head><body>
  <div id="_idContainer2807">
    <h1 class="chapterNumber">16</h1>
    <h1 id="_idParaDest-425" class="chapterTitle">Deep Reinforcement Learning with Stable Baselines</h1>
    <p class="normal">So far, we have learned various deep <strong class="keyword">reinforcement learning</strong> (<strong class="keyword">RL</strong>) algorithms. Wouldn't it be nice if we had a library to easily implement a deep RL algorithm? Yes! There are various libraries available to easily build a deep RL algorithm.</p>
    <p class="normal">One such popular deep RL library is OpenAI Baselines. OpenAI Baselines provides an efficient implementation of many deep RL algorithms, which makes them easier to use. However, OpenAI Baselines does not provide good documentation. So, we will look at the fork of OpenAI Baselines called <strong class="keyword">Stable Baselines</strong>.</p>
    <p class="normal">Stable Baselines is an improved implementation of OpenAI Baselines. Stable Baselines is easier to use and it also includes state-of-the-art deep RL algorithms along with several useful features. We can use Stable Baselines for quickly prototyping the RL model. </p>
    <p class="normal">Let's start off the chapter by installing Stable Baselines, and then we will learn how to create our first agent using the library. Next, we will learn about vectorized environments. Going further, we will learn to implement several deep RL algorithms using Stable Baselines along with exploring various functionalities of baselines. </p>
    <p class="normal">In this chapter, we will learn the following topics:</p>
    <ul>
      <li class="bullet">Installing Stable Baselines</li>
      <li class="bullet">Creating our first agent with Stable Baselines</li>
      <li class="bullet">Multiprocessing with vectorized environments</li>
      <li class="bullet">Playing Atari games with DQN and its variants</li>
      <li class="bullet">Lunar lander using A2C</li>
      <li class="bullet">Swinging up a pendulum using DDPG</li>
      <li class="bullet">Training an agent to walk using TRPO</li>
      <li class="bullet">Implementing GAIL </li>
    </ul>
    <p class="normal">Let's begin the chapter by installing Stable Baselines. </p>
    <h1 id="_idParaDest-426" class="title">Installing Stable Baselines</h1>
    <p class="normal">First, let's <a id="_idIndexMarker1442"/>install the dependencies: </p>
    <pre class="programlisting con"><code class="hljs-con">sudo apt-get update &amp;&amp; sudo apt-get install cmake libopenmpi-dev zlib1g-dev
</code></pre>
    <p class="normal">Several deep RL algorithms require MPI to run, so, let's install MPI:</p>
    <pre class="programlisting con"><code class="hljs-con">sudo pip install mpi4py
</code></pre>
    <p class="normal">Now, we can install Stable Baselines through <code class="Code-In-Text--PACKT-">pip</code>:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install stable-baselines[mpi]
</code></pre>
    <p class="normal">Note that currently, Stable Baselines works only with TensorFlow version 1.x. So, make sure you are running the Stable Baselines experiment with TensorFlow 1.x.</p>
    <p class="normal">Now that we have installed Stable Baselines, let's see how to create our first agent using it. </p>
    <h1 id="_idParaDest-427" class="title">Creating our first agent with Stable Baselines</h1>
    <p class="normal">Now, let's <a id="_idIndexMarker1443"/>build our first deep RL algorithm using Stable Baselines. Let's create a simple agent using a <strong class="keyword">Deep Q Network</strong> (<strong class="keyword">DQN</strong>) for <a id="_idIndexMarker1444"/>the mountain car climbing task. We know that in the <a id="_idIndexMarker1445"/>mountain car climbing task, a car is placed between two mountains and the goal of the agent is to drive up the mountain on the right. </p>
    <p class="normal">First, let's import <code class="Code-In-Text--PACKT-">gym</code> and <code class="Code-In-Text--PACKT-">DQN</code> from <code class="Code-In-Text--PACKT-">stable_baselines</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">from</span> stable_baselines <span class="hljs-keyword">import</span> DQN
</code></pre>
    <p class="normal">Create a mountain car environment:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'MountainCar-v0'</span>)
</code></pre>
    <p class="normal">Now, let's instantiate our agent. As we can observe in the following code, we are passing <code class="Code-In-Text--PACKT-">MlpPolicy</code>, which implies that our network is a multilayer perceptron:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = DQN(<span class="hljs-string">'MlpPolicy'</span>, env, learning_rate=<span class="hljs-number">1e-3</span>)
</code></pre>
    <p class="normal">Now, let's <a id="_idIndexMarker1446"/>train the agent by specifying the <a id="_idIndexMarker1447"/>number of time steps we want to train: </p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">That's it. Building a DQN agent and training it is that simple.</p>
    <h2 id="_idParaDest-428" class="title">Evaluating the trained agent</h2>
    <p class="normal">We can <a id="_idIndexMarker1448"/>also evaluate the trained agent by looking at the mean rewards using <code class="Code-In-Text--PACKT-">evaluate_policy</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> stable_baselines.common.evaluation <span class="hljs-keyword">import</span> evaluate_policy
</code></pre>
    <p class="normal">In the following code, <code class="Code-In-Text--PACKT-">agent</code> is the trained agent, <code class="Code-In-Text--PACKT-">agent.get_env()</code> gets the environment we trained our agent in, and <code class="Code-In-Text--PACKT-">n_eval_episodes</code> represents the number of episodes we need to evaluate our agent:</p>
    <pre class="programlisting code"><code class="hljs-code">mean_reward, n_steps = evaluate_policy(agent, agent.get_env(), n_eval_episodes=<span class="hljs-number">10</span>)
</code></pre>
    <h2 id="_idParaDest-429" class="title">Storing and loading the trained agent</h2>
    <p class="normal">With <a id="_idIndexMarker1449"/>Stable Baselines, we can also save and load our trained agent to and from disk.</p>
    <p class="normal">We can <a id="_idIndexMarker1450"/>save the agent as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.save(<span class="hljs-string">"DQN_mountain_car_agent"</span>)
</code></pre>
    <p class="normal">After saving, we can load the agent as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = DQN.load(<span class="hljs-string">"DQN_mountain_car_agent"</span>)
</code></pre>
    <h2 id="_idParaDest-430" class="title">Viewing the trained agent </h2>
    <p class="normal">After <a id="_idIndexMarker1451"/>training, we can also have a look at how our trained agent performs in the environment. </p>
    <p class="normal">Initialize the state:</p>
    <pre class="programlisting code"><code class="hljs-code">state = env.reset()
</code></pre>
    <p class="normal">For 5,000 steps:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(<span class="hljs-number">5000</span>):
</code></pre>
    <p class="normal">Predict the action to perform in the given state using our trained agent:</p>
    <pre class="programlisting code"><code class="hljs-code">    action, _ = agent.predict(state)
</code></pre>
    <p class="normal">Perform <a id="_idIndexMarker1452"/>the predicted action:</p>
    <pre class="programlisting code"><code class="hljs-code">    next_state, reward, done, info = env.step(action)
</code></pre>
    <p class="normal">Update <code class="Code-In-Text--PACKT-">state</code> to the current state:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = next_state
</code></pre>
    <p class="normal">Render the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">    env.render()
</code></pre>
    <p class="normal">Now, we can see how our trained agent performs in the environment:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.1: Agent learning to climb mountain</p>
    <h2 id="_idParaDest-431" class="title">Putting it all together</h2>
    <p class="normal">Now, let's <a id="_idIndexMarker1453"/>look at the final code combining everything we learned so far:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#import the libraries</span>
<span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">from</span> stable_baselines <span class="hljs-keyword">import</span> DQN
<span class="hljs-keyword">from</span> stable_baselines.common.evaluation <span class="hljs-keyword">import</span> evaluate_policy
<span class="hljs-comment">#create the gym environment</span>
env = gym.make(<span class="hljs-string">'MountainCar-v0'</span>)
<span class="hljs-comment">#instantiate the agent</span>
agent = DQN(<span class="hljs-string">'MlpPolicy'</span>, env, learning_rate=<span class="hljs-number">1e-3</span>)
<span class="hljs-comment">#train the agent</span>
agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
<span class="hljs-comment">#evaluate the agent</span>
mean_reward, n_steps = evaluate_policy(agent, agent.get_env(), n_eval_episodes=<span class="hljs-number">10</span>)
<span class="hljs-comment">#save the trained agent</span>
agent.save(<span class="hljs-string">"DQN_mountain_car_agent"</span>)
<span class="hljs-comment">#view the trained agent</span>
state = env.reset()
<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(<span class="hljs-number">5000</span>):
    action, _ = agent.predict(state)
    next_state, reward, done, info = env.step(action)
    state = next_state
    env.render()
</code></pre>
    <p class="normal">Now that we have a basic idea of how to use Stable Baselines, let's explore it in detail. </p>
    <h1 id="_idParaDest-432" class="title">Vectorized environments </h1>
    <p class="normal">One of the <a id="_idIndexMarker1454"/>very interesting and useful features of Stable Baselines is that we can train our agent in multiple independent environments either in separate processes (using <strong class="keyword">SubprocVecEnv</strong>) or in the same process (using <strong class="keyword">DummyVecEnv</strong>). </p>
    <p class="normal">For example, say we are training our agent in a cart pole balancing environment – instead of training our agent only in a single cart pole balancing environment, we can train our agent in the multiple cart pole balancing environments. </p>
    <p class="normal">We generally train our agent in a single environment per step but now we can train our agent in <a id="_idIndexMarker1455"/>multiple environments per step. This helps our agent to learn more quickly. Now, our state, action, reward, and done will be in the form of a vector since we are training our agent in multiple environments. So, we call this a vectorized environment. </p>
    <p class="normal">There are two types of vectorized environment offered by Stable Baselines:</p>
    <ul>
      <li class="bullet">SubprocVecEnv</li>
      <li class="bullet">DummyVecEnv</li>
    </ul>
    <h2 id="_idParaDest-433" class="title">SubprocVecEnv</h2>
    <p class="normal">In the <a id="_idIndexMarker1456"/>subproc vectorized environment, we <a id="_idIndexMarker1457"/>run each environment in a <strong class="keyword">separate </strong>process (taking advantage of multiprocessing). Now, let's see how to create the subproc vectorized environment. </p>
    <p class="normal">First, let's import <code class="Code-In-Text--PACKT-">SubprocVecEnv</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> stable_baselines.common.vec_env <span class="hljs-keyword">import</span> SubprocVecEnv
<span class="hljs-keyword">from</span> stable_baselines.common <span class="hljs-keyword">import</span> set_global_seeds
</code></pre>
    <p class="normal">Next, we create a function called <code class="Code-In-Text--PACKT-">make_env</code> for initializing our environment:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">make_env</span><span class="hljs-function">(</span><span class="hljs-params">env_name, rank, seed=</span><span class="hljs-number">0</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">_init</span><span class="hljs-function">():</span>
        env = gym.make(env_name)
        env.seed(seed + rank)
        <span class="hljs-keyword">return</span> env
    set_global_seeds(seed)
    <span class="hljs-keyword">return</span> _init
</code></pre>
    <p class="normal">Then, we can create the subproc vectorized environment as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">env_name = <span class="hljs-string">'Pendulum-v0'</span>
num_process = <span class="hljs-number">2</span>
env = SubprocVecEnv([make_env(env_name, i) <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_process)])
</code></pre>
    <h2 id="_idParaDest-434" class="title">DummyVecEnv</h2>
    <p class="normal">In the <a id="_idIndexMarker1458"/>dummy vectorized environment, we run each environment in sequence on the current Python process. It does not support multiprocessing. Now, let's see how to create the dummy vectorized environment. </p>
    <p class="normal">First, let's import <code class="Code-In-Text--PACKT-">DummyVecEnv</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> stable_baselines.common.vec_env <span class="hljs-keyword">import</span> DummyVecEnv
</code></pre>
    <p class="normal">Next, we can create the dummy vectorized environment as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">env_name = <span class="hljs-string">'Pendulum-v0'</span>
env = DummyVecEnv([<span class="hljs-keyword">lambda</span>: gym.make(env_name)])
</code></pre>
    <p class="normal">Now that we have learned to train the agent in multiple independent environments using vectorized environments, in the next section, we will see how to integrate custom environments into Stable Baselines.</p>
    <h1 id="_idParaDest-435" class="title">Integrating custom environments</h1>
    <p class="normal">We can also <a id="_idIndexMarker1459"/>use Stable Baselines to train an agent in our own environment. While creating our own environment, we need to make sure that our custom environment follows the Gym interface. That is, our environment should include methods such as <code class="Code-In-Text--PACKT-">step</code>, <code class="Code-In-Text--PACKT-">reset</code>, <code class="Code-In-Text--PACKT-">render</code>, and so on.</p>
    <p class="normal">Suppose the name of our custom environment is <code class="Code-In-Text--PACKT-">CustomEnv</code>. First, we instantiate our custom environment as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">env = CustomEnv()
</code></pre>
    <p class="normal">Next, we can train our agent in the custom environment as usual:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = DQN(<span class="hljs-string">'MlpPolicy'</span>, env, learning_rate=<span class="hljs-number">1e-3</span>)
agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">That's it. In the next section, let's learn how to play Atari games using a DQN and its variants. </p>
    <h1 id="_idParaDest-436" class="title">Playing Atari games with a DQN and its variants </h1>
    <p class="normal">Now, let's <a id="_idIndexMarker1460"/>learn how to create a DQN to play Atari games <a id="_idIndexMarker1461"/>with Stable Baselines. First, let's import the necessary modules:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> stable_baselines <span class="hljs-keyword">import</span> DQN
</code></pre>
    <p class="normal">Since <a id="_idIndexMarker1462"/>we are dealing with Atari <a id="_idIndexMarker1463"/>games, we can use a convolutional neural network instead of a vanilla neural network. So, we use <code class="Code-In-Text--PACKT-">CnnPolicy</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> stable_baselines.deepq.policies <span class="hljs-keyword">import</span> CnnPolicy
</code></pre>
    <p class="normal">We learned that we preprocess the game screen before feeding it to the agent. With Stable Baselines, we don't have to preprocess manually; instead, we can make use of the <code class="Code-In-Text--PACKT-">make_atari</code> module, which takes care of preprocessing the game screen:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> stable_baselines.common.atari_wrappers <span class="hljs-keyword">import</span> make_atari
</code></pre>
    <p class="normal">Now, let's create an Atari game environment. Let's create the Ice Hockey game environment:</p>
    <pre class="programlisting code"><code class="hljs-code">env = make_atari(<span class="hljs-string">'IceHockeyNoFrameskip-v4'</span>)
</code></pre>
    <p class="normal">Instantiate the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = DQN(CnnPolicy, env, verbose=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Train the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">After training the agent, we can have a look at how our trained agent performs in the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">state = env.reset()
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    action, _ = agent.predict(state)
    next_state, reward, done, info = env.step(action)
    state = next_state
    env.render()
</code></pre>
    <p class="normal">The preceding code displays how our trained agent plays the ice hockey game:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.2: Agent playing the Ice Hockey game</p>
    <h2 id="_idParaDest-437" class="title">Implementing DQN variants</h2>
    <p class="normal">We just <a id="_idIndexMarker1464"/>learned how to implement DQN using Stable Baselines. Now, let's see how to implement the variants of DQN, such as double DQN, DQN with prioritized experience replay, and dueling DQN. Implementing DQN variants is very simple with Baselines.</p>
    <p class="normal">First, we define our keyword arguments as follows: </p>
    <pre class="programlisting code"><code class="hljs-code">kwargs = {<span class="hljs-string">"double_q"</span>: <span class="hljs-literal">True</span>, <span class="hljs-string">"prioritized_replay"</span>: <span class="hljs-literal">True</span>, <span class="hljs-string">"policy_kwargs"</span>: dict(dueling=<span class="hljs-literal">True</span>)}
</code></pre>
    <p class="normal">Now, while instantiating our agent, we just need to pass the keyword arguments: </p>
    <pre class="programlisting code"><code class="hljs-code">agent = DQN(CnnPolicy, env, verbose=<span class="hljs-number">1</span>, **kwargs)
</code></pre>
    <p class="normal">Then, we can train the agent as usual:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">That's it! Now <a id="_idIndexMarker1465"/>we have the dueling double DQN with prioritized experience replay. In the next section, we will learn how to play the lunar lander <a id="_idIndexMarker1466"/>game using the <strong class="keyword">Advantage Actor-Critic Algorithm</strong> (<strong class="keyword">A2C</strong>).</p>
    <h1 id="_idParaDest-438" class="title">Lunar lander using A2C</h1>
    <p class="normal">Let's learn <a id="_idIndexMarker1467"/>how to implement A2C with Stable Baselines for the lunar landing task. In the lunar lander environment, our agent drives <a id="_idIndexMarker1468"/>the space vehicle, and the goal of the agent is to land correctly on the landing pad. If our agent (lander) lands away from the landing pad, then it loses the reward, and the episode will get terminated if the agent crashes or comes to rest. The action space of the environment includes four discrete actions, which are: do nothing, fire left orientation engine, fire main engine, and fire right orientation engine. Now, let's see how to train the agent using A2C to correctly land on the landing pad. </p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">from</span> stable_baselines.common.policies <span class="hljs-keyword">import</span> MlpPolicy
<span class="hljs-keyword">from</span> stable_baselines.common.vec_env <span class="hljs-keyword">import</span> DummyVecEnv
<span class="hljs-keyword">from</span> stable_baselines.common.evaluation <span class="hljs-keyword">import</span> evaluate_policy
<span class="hljs-keyword">from</span> stable_baselines <span class="hljs-keyword">import</span> A2C
</code></pre>
    <p class="normal">Create the lunar lander environment using Gym:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'LunarLander-v2'</span>)
</code></pre>
    <p class="normal">Let's use the dummy vectorized environment. We learned that in the dummy vectorized environment, we run each environment in the same process:</p>
    <pre class="programlisting code"><code class="hljs-code">env = DummyVecEnv([<span class="hljs-keyword">lambda</span>: env])
</code></pre>
    <p class="normal">Create the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = A2C(MlpPolicy, env, ent_coef=<span class="hljs-number">0.1</span>, verbose=<span class="hljs-number">0</span>)
</code></pre>
    <p class="normal">Train the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">After training, we can evaluate our agent by looking at the mean rewards:</p>
    <pre class="programlisting code"><code class="hljs-code">mean_reward, n_steps = evaluate_policy(agent, agent.get_env(), n_eval_episodes=<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">We can <a id="_idIndexMarker1469"/>also have a look at how our trained agent performs in the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">state = env.reset()
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    action, _states = agent.predict(state)
    next_state, reward, done, info = env.step(action)
    state = next_state
    env.render()
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1470"/>preceding code will show how well our trained agent lands on the landing pad:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.3: Agent playing the lunar lander game</p>
    <h2 id="_idParaDest-439" class="title">Creating a custom network </h2>
    <p class="normal">In the <a id="_idIndexMarker1471"/>previous section, we learned how to create an A2C using Stable Baselines. Instead of using the default network, can we customize the network architecture? Yes! With Stable Baselines, we can also use our own custom architecture. Let's see how to do that. </p>
    <p class="normal">First, let's import the feedforward policy (feedforward network):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> stable_baselines.common.policies <span class="hljs-keyword">import</span> FeedForwardPolicy
</code></pre>
    <p class="normal">Now, we can define our custom policy (custom network) as shown in the following snippet. As we can observe in the following code, we are passing <code class="Code-In-Text--PACKT-">net_arch=[dict(pi=[128, 128, 128], vf=[128, 128, 128])]</code>, which specifies our network architecture. <code class="Code-In-Text--PACKT-">pi</code> represents the architecture of the policy network and <code class="Code-In-Text--PACKT-">vf</code> represents the architecture of value network: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">CustomPolicy</span><span class="hljs-class">(</span><span class="hljs-params">FeedForwardPolicy</span><span class="hljs-class">):</span>
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, *args, **kargs</span><span class="hljs-function">):</span>
        super(CustomPolicy, self).__init__(*args, **kargs,
                                           net_arch=[dict(pi=[<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>], vf=[<span class="hljs-number">128</span>, <span class="hljs-number">128</span>, <span class="hljs-number">128</span>])], feature_extraction=<span class="hljs-string">"mlp"</span>)
</code></pre>
    <p class="normal">We can instantiate the agent with the custom policy as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = A2C(CustomPolicy, <span class="hljs-string">'LunarLander-v2'</span>, verbose=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Now, we can train the agent as usual:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">That's it. Similarly, we can create our own custom network. In the next section, let's learn <a id="_idIndexMarker1472"/>how to perform the <em class="italic">inverted pendulum swing-up</em> task using the <strong class="keyword">Deep Deterministic Policy Gradient (DDPG)</strong> algorithm.</p>
    <h1 id="_idParaDest-440" class="title">Swinging up a pendulum using DDPG</h1>
    <p class="normal">Let's <a id="_idIndexMarker1473"/>learn how to implement the DDPG for the inverted pendulum swing-up task using Stable Baselines. First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> stable_baselines.ddpg.policies <span class="hljs-keyword">import</span> MlpPolicy
<span class="hljs-keyword">from</span> stable_baselines.common.noise <span class="hljs-keyword">import</span> NormalActionNoise, OrnsteinUhlenbeckActionNoise, AdaptiveParamNoiseSpec
<span class="hljs-keyword">from</span> stable_baselines <span class="hljs-keyword">import</span> DDPG
</code></pre>
    <p class="normal">Create the pendulum environment using Gym:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'Pendulum-v0'</span>)
</code></pre>
    <p class="normal">Get the number of actions:</p>
    <pre class="programlisting code"><code class="hljs-code">n_actions = env.action_space.shape[<span class="hljs-number">-1</span>]
</code></pre>
    <p class="normal">We know that in DDPG, instead of selecting the action directly, we add some noise using the Ornstein-Uhlenbeck process to ensure exploration. So, we create the action noise as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">action_noise = OrnsteinUhlenbeckActionNoise(mean=np.zeros(n_actions), sigma=float(<span class="hljs-number">0.5</span>) * np.ones(n_actions))
</code></pre>
    <p class="normal">Instantiate the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = DDPG(MlpPolicy, env, verbose=<span class="hljs-number">1</span>, param_noise=<span class="hljs-literal">None</span>, action_noise=action_noise)
</code></pre>
    <p class="normal">Train the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">After training the agent, we can also look at how our trained agent swings up the pendulum by rendering the environment. Can we also look at the computational graph of DDPG? Yes! In the next section, we will learn how to do that.</p>
    <h2 id="_idParaDest-441" class="title">Viewing the computational graph in TensorBoard</h2>
    <p class="normal">With <a id="_idIndexMarker1474"/>Stable Baselines, it is easier to view the computational graph of our model in TensorBoard. In order to that, we just need to pass the directory where we will store <a id="_idIndexMarker1475"/>our log files while instantiating the agent, as shown here:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = DDPG(MlpPolicy, env, verbose=<span class="hljs-number">1</span>, param_noise=<span class="hljs-literal">None</span>, action_noise=action_noise, tensorboard_log=<span class="hljs-string">"logs"</span>)
</code></pre>
    <p class="normal">Then, we <a id="_idIndexMarker1476"/>can train the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">After training, open the terminal and type the following command to run TensorBoard:</p>
    <pre class="programlisting code"><code class="hljs-code">tensorboard --logdir logs
</code></pre>
    <p class="normal">As we can observe, we can now see the computational graph of the DDPG model (agent):</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.4: Computational graph of DDPG</p>
    <p class="normal">From <em class="italic">Figure 16.4</em>, we <a id="_idIndexMarker1477"/>can understand <a id="_idIndexMarker1478"/>how the <a id="_idIndexMarker1479"/>DDPG computational graph is generated just as we learned in <em class="chapterRef">Chapter 12</em>, <em class="italic">Learning DDPG, TD3, and SAC</em>.</p>
    <p class="normal">Now, let's expand and look into the model node for more clarity:</p>
    <p class="normal"> <img src="../Images/B15558_16_05.png" alt=""/></p>
    <p class="packt_figref">Figure 16.5: Computational graph of DDPG</p>
    <p class="normal">As we <a id="_idIndexMarker1480"/>can observe from <em class="italic">Figure 16.5</em>, our model includes the policy (actor) and Q (critic) network. </p>
    <p class="normal">Now <a id="_idIndexMarker1481"/>that we have learned how to use Stable <a id="_idIndexMarker1482"/>Baselines to implement DDPG for the inverted pendulum swing-up task, in the next section we will learn how to implement TRPO using Stable Baselines. </p>
    <h1 id="_idParaDest-442" class="title">Training an agent to walk using TRPO </h1>
    <p class="normal">In this <a id="_idIndexMarker1483"/>section, let's learn how to train the agent to walk <a id="_idIndexMarker1484"/>using <strong class="keyword">Trust Region Policy Optimization </strong>(<strong class="keyword">TRPO</strong>).<strong class="keyword"> </strong>Let's <a id="_idIndexMarker1485"/>use the MuJoCo <a id="_idIndexMarker1486"/>environment for training the agent. <strong class="keyword">MuJoCo</strong> stands for <strong class="keyword">Multi-Joint dynamics with Contact</strong> and is one of the most popular simulators used for training agents to perform continuous control tasks.</p>
    <p class="normal">Note that MuJoCo is a proprietary physics engine, so we need to acquire a license to use it. Also, MuJoCo offers a free 30-day trial period. Installing MuJoCo requires a specific set of steps. So, in the next section, we will see how to install the MuJoCo environment.</p>
    <h2 id="_idParaDest-443" class="title">Installing the MuJoCo environment</h2>
    <p class="normal">First, in <a id="_idIndexMarker1487"/>your home directory, create a new hidden folder called <code class="Code-In-Text--PACKT-">.mujoco</code>. Next, go to the MuJoCo website (<a href="https://www.roboti.us/"><span class="url">https://www.roboti.us/</span></a>) and download MuJoCo according to your operating system. As shown in <em class="italic">Figure 16.6</em>, MuJoCo provides support for Windows, Linux, and macOS: </p>
    <figure class="mediaobject"><img src="../Images/B15558_16_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.6: Different MuJoCo versions</p>
    <p class="normal">If you are using Linux, then you can download the zip file named <code class="Code-In-Text--PACKT-">mujoco200 linux</code>. After downloading the zip file, unzip the file and rename it to <code class="Code-In-Text--PACKT-">mujoco200</code>. Now, copy the <code class="Code-In-Text--PACKT-">mujoco200</code> folder and place the folder inside the <code class="Code-In-Text--PACKT-">.mujoco</code> folder in your home directory. </p>
    <p class="normal">As <em class="italic">Figure 16.7</em> shows, now in our home directory, we have a <code class="Code-In-Text--PACKT-">.mujoco</code> folder, and inside the <code class="Code-In-Text--PACKT-">.mujoco</code> folder, we have a <code class="Code-In-Text--PACKT-">mujoco200</code> folder:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.7: Installing MuJoCo</p>
    <p class="normal">Now, we need to obtain a trial license. First, go to <a href="https://www.roboti.us/license.html"><span class="url">https://www.roboti.us/license.html</span></a> and register for the trial license, as <em class="italic">Figure 16.8</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.8: Registering for the trial license</p>
    <p class="normal">To register, we also need the computer id. As <em class="italic">Figure 16.8</em> shows, to the right of the <strong class="keyword">Computer id</strong> field, we have the name of different platforms. Now, just click on your operating system and you <a id="_idIndexMarker1488"/>will obtain the relevant executable <code class="Code-In-Text--PACKT-">getid</code> file. For instance, if you are using Linux, then you will obtain a file named <code class="Code-In-Text--PACKT-">getid_linux</code>.</p>
    <p class="normal">After downloading the <code class="Code-In-Text--PACKT-">getid_linux</code> file, run the following command on your terminal:</p>
    <pre class="programlisting con"><code class="hljs-con">chmod +x getid_linux
</code></pre>
    <p class="normal">Then, run the following command:</p>
    <pre class="programlisting con"><code class="hljs-con">./getid_linux
</code></pre>
    <p class="normal">The preceding command will display your computer id. After getting the computer id, fill in the form and register to obtain a license. Once you click on the <strong class="keyword">Submit </strong>button, you will get an email from Roboti LLC Licensing.</p>
    <p class="normal">From the email, download the file named <code class="Code-In-Text--PACKT-">mjkey.txt</code>. Next, place the <code class="Code-In-Text--PACKT-">mjkey.txt</code> file in the <code class="Code-In-Text--PACKT-">.mujoco</code> folder. As <em class="italic">Figure 16.9</em> shows, now our <code class="Code-In-Text--PACKT-">.mujoco</code> hidden folder contains the <code class="Code-In-Text--PACKT-">mjkey.txt</code> file and a folder named <code class="Code-In-Text--PACKT-">mujoco200</code>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.9: Installing MuJoCo</p>
    <p class="normal">Next, open your terminal and run the following command to edit the <code class="Code-In-Text--PACKT-">bashrc</code> file:</p>
    <pre class="programlisting con"><code class="hljs-con">nano ~/.bashrc
</code></pre>
    <p class="normal">Copy the following line to the <code class="Code-In-Text--PACKT-">bashrc</code> file and make sure to replace the username text with your own username:</p>
    <pre class="programlisting con"><code class="hljs-con">export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/home/username/.mujoco/mujoco200/bin
</code></pre>
    <p class="normal">Next, save the file and exit the nano editor. Now, run the following command on your terminal:</p>
    <pre class="programlisting con"><code class="hljs-con">source ~/.bashrc
</code></pre>
    <p class="normal">Well <a id="_idIndexMarker1489"/>done! We are almost there. Now, clone the MuJoCo GitHub repository:</p>
    <pre class="programlisting con"><code class="hljs-con">git clone https://github.com/openai/mujoco-py.git
</code></pre>
    <p class="normal">Navigate to the <code class="Code-In-Text--PACKT-">mujoco-py</code> folder:</p>
    <pre class="programlisting con"><code class="hljs-con">cd mujoco-py
</code></pre>
    <p class="normal">Update the packages:</p>
    <pre class="programlisting con"><code class="hljs-con">sudo apt-get update
</code></pre>
    <p class="normal">Install the dependencies:</p>
    <pre class="programlisting con"><code class="hljs-con">sudo apt-get install libgl1-mesa-dev libgl1-mesa-glx libosmesa6-dev python3-pip python3-numpy python3-scipy
</code></pre>
    <p class="normal">Finally, install MuJoCo:</p>
    <pre class="programlisting con"><code class="hljs-con">pip3 install -r requirements.txt
sudo python3 setup.py install
</code></pre>
    <p class="normal">To test the successful installation of MuJoCo, let's run a Humanoid agent by taking a random action in the environment. So, create the following Python file named <code class="Code-In-Text--PACKT-">mujoco_test.py</code> with the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
env = gym.make(<span class="hljs-string">'Humanoid-v2'</span>)
env.reset()
<span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(<span class="hljs-number">1000</span>):
  env.render()
  env.step(env.action_space.sample())
env.close()
</code></pre>
    <p class="normal">Next, open the terminal and run the Python file:</p>
    <pre class="programlisting con"><code class="hljs-con">python mujoco_test.py 
</code></pre>
    <p class="normal">The <a id="_idIndexMarker1490"/>preceding code will render the <code class="Code-In-Text--PACKT-">Humanoid</code> environment as <em class="italic">Figure 16.10 shows</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.10: Humanoid environment</p>
    <p class="normal">Now that we have successfully installed MuJoCo, let's start implementing TRPO to train our agent to walk in the next section.</p>
    <h2 id="_idParaDest-444" class="title">Implementing TRPO</h2>
    <p class="normal">Import <a id="_idIndexMarker1491"/>the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">from</span> stable_baselines.common.policies <span class="hljs-keyword">import</span> MlpPolicy
<span class="hljs-keyword">from</span> stable_baselines.common.vec_env <span class="hljs-keyword">import</span> DummyVecEnv, VecNormalize
<span class="hljs-keyword">from</span> stable_baselines <span class="hljs-keyword">import</span> TRPO
<span class="hljs-keyword">from</span> stable_baselines.common.vec_env <span class="hljs-keyword">import</span> VecVideoRecorder
</code></pre>
    <p class="normal">Create a vectorized <code class="Code-In-Text--PACKT-">Humanoid</code> environment using <code class="Code-In-Text--PACKT-">DummyVecEnv</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">env = DummyVecEnv([<span class="hljs-keyword">lambda</span>: gym.make(<span class="hljs-string">"Humanoid-v2"</span>)])
</code></pre>
    <p class="normal">Normalize the states (observations):</p>
    <pre class="programlisting code"><code class="hljs-code">env = VecNormalize(env, norm_obs=<span class="hljs-literal">True</span>, norm_reward=<span class="hljs-literal">False</span>,
                   clip_obs=<span class="hljs-number">10.</span>)
</code></pre>
    <p class="normal">Instantiate the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = TRPO(MlpPolicy, env)
</code></pre>
    <p class="normal">Train the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">250000</span>)
</code></pre>
    <p class="normal">After training the agent, we can see how our trained agent learned to walk by rendering the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">state = env.reset()
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    action, _ = agent.predict(state)
    next_state, reward, done, info = env.step(action)
    state = next_state
    env.render()
</code></pre>
    <p class="normal">Save the whole code used in this section in a Python file called <code class="Code-In-Text--PACKT-">trpo.py</code> and then open the terminal and run the file:</p>
    <pre class="programlisting con"><code class="hljs-con">python trpo.py
</code></pre>
    <p class="normal">We can see how our trained agent learned to walk in <em class="italic">Figure 16.11</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.11: Agent learning to walk using TRPO</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Always use the terminal to run the program that uses the MuJoCo environment. </p>
    </div>
    <p class="normal">That's it. In <a id="_idIndexMarker1492"/>the next section, we will learn how to record our trained agent's actions as a video.</p>
    <h2 id="_idParaDest-445" class="title">Recording the video</h2>
    <p class="normal">In the <a id="_idIndexMarker1493"/>previous section, we trained our agent to learn to walk using TRPO. Can we also record a video of our trained agent? Yes! With Stable Baselines, we can easily record a video of our agent using the <code class="Code-In-Text--PACKT-">VecVideoRecorder</code> module.</p>
    <p class="normal">Note that to record the video, we need the <code class="Code-In-Text--PACKT-">ffmpeg</code> package installed in our machine. If it is not installed, then install it using the following set of commands:</p>
    <pre class="programlisting con"><code class="hljs-con">sudo add-apt-repository ppa:mc3man/trusty-media
sudo apt-get update
sudo apt-get dist-upgrade
sudo apt-get install ffmpeg
</code></pre>
    <p class="normal">Now, let's import the <code class="Code-In-Text--PACKT-">VecVideoRecorder</code> module:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> stable_baselines.common.vec_env <span class="hljs-keyword">import</span> VecVideoRecorder
</code></pre>
    <p class="normal">Define a function called <code class="Code-In-Text--PACKT-">record_video</code> for recording the video:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">record_video</span><span class="hljs-function">(</span><span class="hljs-params">env_name, agent, video_length=</span><span class="hljs-number">500</span><span class="hljs-params">, prefix=</span><span class="hljs-string">''</span><span class="hljs-params">, video_folder=</span><span class="hljs-string">'videos/'</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Create the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">    env = DummyVecEnv([<span class="hljs-keyword">lambda</span>: gym.make(env_name)])
</code></pre>
    <p class="normal">Instantiate <a id="_idIndexMarker1494"/>the video recorder:</p>
    <pre class="programlisting code"><code class="hljs-code">    env = VecVideoRecorder(env, video_folder=video_folder,
        record_video_trigger=<span class="hljs-keyword">lambda</span> step: step == <span class="hljs-number">0</span>, video_length=video_length, name_prefix=prefix)
</code></pre>
    <p class="normal">Select actions in the environment using our trained agent where the number of time steps is set to the video length:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = env.reset()
    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(video_length):
        action, _ = agent.predict(state)
        next_state, reward, done, info = env.step(action)
        state = next_state
    env.close()
</code></pre>
    <p class="normal">That's it! Now, let's call our <code class="Code-In-Text--PACKT-">record_video</code> function. Note that we are passing the environment name, our trained agent, the length of the video, and the name of our video file: </p>
    <pre class="programlisting code"><code class="hljs-code">record_video(<span class="hljs-string">'Humanoid-v2'</span>, agent, video_length=<span class="hljs-number">500</span>, prefix=<span class="hljs-string">'Humanoid_walk_TRPO'</span>)
</code></pre>
    <p class="normal">Now, we will have a new file called <code class="Code-In-Text--PACKT-">Humanoid_walk_TRPO-step-0-to-step-500.mp4</code> in the <code class="Code-In-Text--PACKT-">videos</code> folder:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.12: Recorded video</p>
    <p class="normal">In this <a id="_idIndexMarker1495"/>way, we can record our trained agent's action. In the next section, we will learn how to implement PPO using Stable Baselines.</p>
    <h1 id="_idParaDest-446" class="title">Training a cheetah bot to run using PPO</h1>
    <p class="normal">In this <a id="_idIndexMarker1496"/>section, let's learn how to train the 2D <a id="_idIndexMarker1497"/>cheetah bot to run <a id="_idIndexMarker1498"/>using <strong class="keyword">Proximal Policy Optimization</strong> (<strong class="keyword">PPO</strong>). First, import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">from</span> stable_baselines.common.policies <span class="hljs-keyword">import</span> MlpPolicy
<span class="hljs-keyword">from</span> stable_baselines.common.vec_env <span class="hljs-keyword">import</span> DummyVecEnv, VecNormalize
<span class="hljs-keyword">from</span> stable_baselines <span class="hljs-keyword">import</span> PPO2
</code></pre>
    <p class="normal">Create a vectorized environment using <code class="Code-In-Text--PACKT-">DummyVecEnv</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">env = DummyVecEnv([<span class="hljs-keyword">lambda</span>: gym.make(<span class="hljs-string">"HalfCheetah-v2"</span>)])
</code></pre>
    <p class="normal">Normalize the state:</p>
    <pre class="programlisting code"><code class="hljs-code">env = VecNormalize(env,norm_obs=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Instantiate the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = PPO2(MlpPolicy, env)
</code></pre>
    <p class="normal">Train the agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">250000</span>)
</code></pre>
    <p class="normal">After training, we can see how our trained cheetah bot learned to run by rendering the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">state = env.reset()
<span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
    action, _ = agent.predict(state)
    next_state, reward, done, info = env.step(action)
    state = next_state
    env.render()
</code></pre>
    <p class="normal">Save the whole code used in this section in a Python file called <code class="Code-In-Text--PACKT-">ppo.py</code> and then open the terminal and run the file:</p>
    <pre class="programlisting con"><code class="hljs-con">python ppo.py
</code></pre>
    <p class="normal">We can see how our trained cheetah bot learned to run, as <em class="italic">Figure 16.13</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.13: 2D cheetah bot learning to run</p>
    <h2 id="_idParaDest-447" class="title">Making a GIF of a trained agent</h2>
    <p class="normal">In the <a id="_idIndexMarker1499"/>previous section, we learned how to train the cheetah bot to run using PPO. Can we also create a GIF file of our trained agent? Yes! Let's see how to do that. </p>
    <p class="normal">First, import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> imageio
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">Initialize the list for storing images:</p>
    <pre class="programlisting code"><code class="hljs-code">images = []
</code></pre>
    <p class="normal">Initialize the state by resetting the environment, where <code class="Code-In-Text--PACKT-">agent</code> is the agent we trained in the previous section:</p>
    <pre class="programlisting code"><code class="hljs-code">state = agent.env.reset()
</code></pre>
    <p class="normal">Render the environment and get the image:</p>
    <pre class="programlisting code"><code class="hljs-code">img = agent.env.render(mode=<span class="hljs-string">'rgb_array'</span>)
</code></pre>
    <p class="normal">For every step in the environment, save the image:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">500</span>):
    images.append(img)
    action, _ = agent.predict(state)
    next_state, reward, done ,info = agent.env.step(action)
    state = next_state
    img = agent.env.render(mode=<span class="hljs-string">'rgb_array'</span>)
</code></pre>
    <p class="normal">Create the GIF file as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">imageio.mimsave(<span class="hljs-string">'HalfCheetah.gif'</span>, [np.array(img) <span class="hljs-keyword">for</span> i, img <span class="hljs-keyword">in</span> enumerate(images) <span class="hljs-keyword">if</span> i%<span class="hljs-number">2</span> == <span class="hljs-number">0</span>], fps=<span class="hljs-number">29</span>)
</code></pre>
    <p class="normal">Now, we <a id="_idIndexMarker1500"/>will have a new file called <code class="Code-In-Text--PACKT-">HalfCheetah.gif</code>, as <em class="italic">Figure 16.14 </em>shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_16_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 16.14: GIF of the trained agent</p>
    <p class="normal">In this <a id="_idIndexMarker1501"/>way, we can obtain a GIF of our trained agent. In the next section, we will learn how to implement GAIL using Stable Baselines.</p>
    <h1 id="_idParaDest-448" class="title">Implementing GAIL</h1>
    <p class="normal">In this <a id="_idIndexMarker1502"/>section, let's <a id="_idIndexMarker1503"/>explore how to implement <strong class="keyword">Generative Adversarial Imitation Learning </strong>(<strong class="keyword">GAIL</strong>) with Stable Baselines. In <em class="chapterRef">Chapter 15</em>,<em class="italic"> Imitation Learning and Inverse RL</em>, we learned that we use the generator to generate the state-action pair in a way that the discriminator is not able to distinguish whether the state-action pair is generated using the expert policy or the agent policy. We train the generator to generate a policy similar to an expert policy using TRPO, while the discriminator is a classifier and it is optimized using Adam. </p>
    <p class="normal">To implement GAIL, we need expert trajectories so that our generator learns to mimic the expert trajectory. Okay, so how can we obtain the expert trajectory? First, we use the TD3 algorithm to generate expert trajectories and then create an expert dataset. Then, using this expert dataset, we train our GAIL agent. Note that instead of using TD3, we can also use any other algorithm for generating expert trajectories. </p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">from</span> stable_baselines <span class="hljs-keyword">import</span> GAIL, TD3
<span class="hljs-keyword">from</span> stable_baselines.gail <span class="hljs-keyword">import</span> ExpertDataset, generate_expert_traj
</code></pre>
    <p class="normal">Instantiate the TD3 agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent = TD3(<span class="hljs-string">'MlpPolicy'</span>, <span class="hljs-string">'MountainCarContinuous-v0'</span>, verbose=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Generate the expert trajectories:</p>
    <pre class="programlisting code"><code class="hljs-code">generate_expert_traj(agent, <span class="hljs-string">'expert_traj'</span>, n_timesteps=<span class="hljs-number">100</span>, n_episodes=<span class="hljs-number">20</span>)
</code></pre>
    <p class="normal">Create the expert dataset using the expert trajectories:</p>
    <pre class="programlisting code"><code class="hljs-code">dataset = ExpertDataset(expert_path=<span class="hljs-string">'expert_traj.npz'</span>, traj_limitation=<span class="hljs-number">10</span>, verbose=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Instantiate the GAIL agent with the expert dataset (expert trajectories):</p>
    <pre class="programlisting code"><code class="hljs-code">agent = GAIL(<span class="hljs-string">'MlpPolicy'</span>, <span class="hljs-string">'MountainCarContinuous-v0'</span>, dataset, verbose=<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">Train <a id="_idIndexMarker1504"/>the GAIL agent:</p>
    <pre class="programlisting code"><code class="hljs-code">agent.learn(total_timesteps=<span class="hljs-number">25000</span>)
</code></pre>
    <p class="normal">After training, we can also render the environment and see how our trained agent performs in the environment. That's it, implementing GAIL using Stable Baselines is that simple.</p>
    <h1 id="_idParaDest-449" class="title">Summary</h1>
    <p class="normal">We started the chapter by understanding what Stable Baselines is and how to install it. Then, we learned to create our first agent with Stable Baselines using a DQN. We also learned how to save and load an agent. Next, we learned how to create multiple independent environments using vectorization. We also learned about two types of vectorized environment called SubprocVecEnv and DummyVecEnv. </p>
    <p class="normal">We learned that in SubprocVecEnv, we run each environment in a different process, whereas in DummyVecEnv, we run each environment in the same process. </p>
    <p class="normal">Later, we learned how to implement a DQN and its variants to play Atari games using Stable Baselines. Next, we learned how to implement A2C and also how to create a custom policy network. Moving on, we learned how to implement DDPG and also how to view the computational graph in TensorBoard.</p>
    <p class="normal">Going further, we learned how to set up the MuJoCo environment and how to train an agent to walk using TRPO. We also learned how to record a video of a trained agent. Next, we learned how to implement PPO and how to make a GIF of a trained agent. At the end of the chapter, we learned how to implement generative adversarial imitation learning using Stable Baselines.</p>
    <h1 id="_idParaDest-450" class="title">Questions</h1>
    <p class="normal">Let's put our knowledge of Stable Baselines to the test. Try answering the following questions:</p>
    <ol>
      <li class="numbered">What is Stable Baselines?</li>
      <li class="numbered">How do you store and load a trained agent?</li>
      <li class="numbered">What is a vectorized environment?</li>
      <li class="numbered">What is the difference between the SubprocVecEnv and DummyVecEnv environments?</li>
      <li class="numbered">How do you visualize a computational graph in TensorBoard?</li>
      <li class="numbered">How do you record a video of a trained agent?</li>
    </ol>
    <h1 id="_idParaDest-451" class="title">Further reading</h1>
    <p class="normal">To learn more, check the following resource:</p>
    <ul>
      <li class="bullet">Check out the Stable Baselines documentation, available at <a href="https://stable-baselines.readthedocs.io/en/master/index.html"><span class="url">https://stable-baselines.readthedocs.io/en/master/index.html</span></a></li>
    </ul>
  </div>
</body></html>