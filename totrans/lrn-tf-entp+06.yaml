- en: 'Chapter 4:'
  prefs: []
  type: TYPE_NORMAL
- en: Reusable Models and Scalable Data Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you will learn different ways of using scalable data ingestion
    pipelines with pre-made model elements in TensorFlow Enterprise's high-level API's.
    These options provide the flexibility to suit different requirements or styles
    for building, training, and deploying models. Armed with this knowledge, you will
    be able to make informed choices and understand trade-offs among different model
    development approaches. The three major approaches are TensorFlow Hub, the TensorFlow
    Estimators API, and the TensorFlow Keras API.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Hub is a library of open source machine learning models. TensorFlow
    Estimators and `tf.keras` APIs are wrappers that can be regarded as high-level
    elements that can be configured and reused as building blocks in a model. In terms
    of the amount of coding required, TensorFlow Hub models require the least amount
    of extra coding, while Estimator and Keras APIs are building blocks at a lower
    level, and therefore more coding is involved when using either Estimator or Keras
    APIs. But in any case, all three approaches really made TensorFlow much easier
    to learn and use. We are going to spend the next few sections of this chapter
    learning how these approaches work with scalable data ingestion pipelines from
    cloud storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the help of an example, we will learn to use *TensorFlow datasets* and
    *TensorFlow I/O* as a means to ingest large amounts of data without reading it
    into the JupyterLab runtime memory. We will cover the following topics in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorFlow Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying models from TensorFlow Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging TensorFlow Keras API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with TensorFlow Estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of these three approaches (**TensorFlow Hub, the Estimators API, and the Keras
    API**), TensorFlow Hub stands out from the other two. It is a library for open
    source machine learning models. The main purpose of TensorFlow Hub is to enable
    model reusability through transfer learning. Transfer learning is a very practical
    and convenient technique in deep learning modeling development. The hypothesis
    is that as a well-designed model (peer reviewed and made famous by publications)
    learned patterns in features during the training process, the model learned to
    generalize these patterns, and such generalization can be applied to new data.
    Therefore, we do not need to retrain the model again when we have new training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take human vision as an example. The content of what we see can be decomposed
    from simple to sophisticated patterns in the order of lines, edges, shapes, layers,
    and finally a pattern. As it turns out, this is how a computer vision model recognizes
    human faces. If we imagine a multilayer perceptron model, in the beginning, the
    layers learn patterns in lines, then shapes, and as we go into deep layers, we
    see that what's learned is the facial patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Since the hierarchy of the same pattern can be used to classify other images,
    we can reuse the architecture of the model (from a repository such as TensorFlow
    Hub) and append a final classification layer for our own purposes. We will utilize
    this approach of transfer learning in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Applying models from TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow Hub contains many reusable models. For example, in image classification
    tasks, there are pretrained models such as Inception V3, ResNet of different versions,
    as well as feature vectors available. In this chapter, we will take a look at
    how to load and use a ResNet feature vector model for image classification of
    our own images. The images are five types of flowers: daisy, dandelion, roses,
    sunflowers, and tulips. We will use the `tf.keras` API to get these images for
    our use:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may use Google Cloud AI Platform''s JupyterLab environment for this work.
    Once you are in the AI Platform''s JupyterLab environment, you may start by importing
    the necessary modules and download the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You may find the flower images in the runtime instance of your JupyterLab at
    `/home/jupyter/.keras/datasets/flower_photos`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In a new cell, we may run the following command to take a look at the directory
    structure for our data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding command will return the following structure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Inside each folder, you will find colored images in `.jpg` format with different
    widths and heights. Before using any pre-built model, it is important to find
    out about the required data shape at the entry point of the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are going to use the ResNet V2 feature vector that was pretrained with imagenet
    as our base model. The URL to this model is [https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4](https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The documentation there indicates the expected height and width of the image
    at the entry point to be `224`. Let''s go ahead and specify these parameters as
    well as the batch size for training:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have understood the dimension of the image expected as model input,
    we are ready to deal with the next step, which is about how to ingest our training
    images.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a generator to feed image data at scale
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A convenient method to ingest data into the model is by a generator. A Pythonic
    generator is an iterator that goes through the data directory and passes batches
    of data to the model. When a generator is used to cycle through our training data,
    we do not have to load the entire image collection at one time and worry about
    memory constraints in our compute node. Rather, we send a batch of images at one
    time. Therefore, the use of the Python generator is more efficient for the compute
    node's memory than passing all the data as a huge NumPy array.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow provides APIs and workflows to create such generators specific for
    the TensorFlow model''s consumption. At a high level, it follows this process:'
  prefs: []
  type: TYPE_NORMAL
- en: It creates an object with the `ImageDataGenerator` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It uses this object to invoke the `flow_from_directory` function to create a
    TensorFlow generator.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As a result, this generator knows the directory where the training data is stored.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with images, there are a few parameters about the data that we
    need to specify for the generator. The input images'' color values are preferred
    to be between `0` and `1`. Therefore, we have to normalize our image by dividing
    it by a rescale factor with a value of `255`, which is the maximum pixel value
    in an RGB image of `.jpg` format. We may also hold on to 20% of the data for validation.
    This is known as the validation split factor. We also need to specify the standard
    image size that conforms to ResNet, an interpolation method that converts images
    of any size into this size, and the amount of data in a batch (batch size). The
    necessary steps are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Organize these factors as tuples. These factors are specified as input keywords
    for either `ImageDataGenerator` or `flow_from_directory`. We may pass these parameters
    and their values as tuples to these functions. The tuple will be unpacked when
    the function is executed. These parameters are held in these dictionaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As seen in the preceding lines of code, `datagen_kwargs` goes to `ImageDataGenerator`,
    while `dataflow_kwargs` goes to `flow_from_directory`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Pass the tuples to `ImageGenerator`. These tuples encapsulate all these factors.
    Now we will pass these tuples into the generator, as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And you will see the output with number of images and classes in the cross
    validation data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For training data, if you wish, you may consider the option for data augmentation.
    If so, then we may set these parameters in `ImageDataGenerator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These parameters help transform original images into different orientations.
    This is a typical technique to add more training data to improve accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For now, let''s not bother with that, so we set `do_data_augmentation = False`,
    as shown in the following code. You may set it to `True` if you wish. Suggested
    augmentation parameters are provided:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon executing the preceding code, you will receive the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our generators for validation data and training data correctly identified the
    directory and were able to identify the number of classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As with all classification tasks, labels are converted to integer indices.
    The generator maps labels with `train_generator.class_indices`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can easily map indices back to labels by creating a reverse lookup, also
    in the form of a dictionary. This can be done by reversing the key-value pairs
    as we iterated `through labels_idx`, where the key is the index and the values
    are flower types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we learned how to implement `ImageGenerator` for both training
    and validation data. We leveraged optional input parameters to rescale and normalize
    our images. We also learned to retrieve the ground truth label mapping so that
    we may decode the model prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will learn to implement transfer learning by reusing ResNet feature
    vectors for our own image classification task.
  prefs: []
  type: TYPE_NORMAL
- en: Reusing pretrained ResNet feature vectors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we are ready to construct the model. We will use the `tf.keras.sequential`
    API. It consists of three layers—input, ResNet, and a dense layer—as the classification
    output. We also have the choice between fine-tuning and retraining the ResNet
    (this requires longer training time). The code for defining the model architecture
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll begin by defining the parameters, as shown in the following lines of
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will construct the model with the help of the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s build the model with the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: ResNet requires RGB layers to be separated as a third dimension. Therefore,
    we need to add an input layer that takes on `input_shape` of `[224, 224, 3]`.
    Also, since we have five types of flowers, this is a multiclass classification.
    We need a dense layer with softmax activation for outputting probability for each
    label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We may confirm the model architecture with the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon executing the preceding line of code, you will see the sequence of the
    three layers and their expected output shape:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This shows the model is very simple in its structure. It consists of the ResNet
    feature vector layer that we downloaded from TensorFlow Hub, followed by a classification
    head with five nodes (there are five flower classes in our image collection).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have wrapped the ResNet feature vector with proper input and output
    layers, we are ready to set up the training workflow. To begin, we need to compile
    the model, where we specify the optimizer (in this case, we select `loss` function.
    The optimizer leverages the gradient decent algorithm to continuously seek weights
    and biases to minimize the `loss` function. Since this is a multiclass classification
    problem, it needs to be categorical cross-entropy.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a deeper discussion, see *TensorFlow 2.0 Quick Start Guide*, by *Tony Holroyd*,
    published by *Packt Publishing*. You can refer to [*Chapter 4*](B16070_04_Final_JM_ePub.xhtml#_idTextAnchor101)
    *Supervised Machine Learning Using TensorFlow 2*, and the section entitled *Logistic
    regression*, concerning loss functions and optimizers. This is how we define an
    optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'And since we want to output probability for each class, we set `from_logits
    = True`, We also would like the model not to become overconfident, so we set `label_smoothing
    = 0.1` as a regularization to penalize extremely high probability. We may define
    a `loss` function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to configure the model for training. This is accomplished by defining
    the `loss` function and optimizer as part of the model''s training process, as
    the training process needs to know what the `loss` function is to optimize for,
    and what optimizer to use. To compile the model with the optimizer and `loss`
    function specified, execute the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The outcome is a model architecture that is ready to be used for training.
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For model training, we will use the `tf.keras.fit` function. We are only going
    to train for five epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'And the training result should be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: At each epoch, the `loss` function value and accuracy on training data is provided.
    Since we have cross-validation data provided, the model is also tested with a
    validation dataset at the end of each training epoch. The `loss` function and
    accuracy measurement are provided at each epoch by the Fit API. This is the standard
    output for each training run.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth mentioning that when the preceding code is executed in AI Notebook
    using the Nvidia Tesla T4 Graphics Processing Unit (GPU) and a basic driver node
    of 4 CPUs at 15 GB RAM, the total training time is just a little over 2 minutes,
    whereas if this training process was executed in the same driver node without
    a GPU, it could take more than 30 minutes to complete the training process.
  prefs: []
  type: TYPE_NORMAL
- en: GPUs are well suited for deep learning model training because it can process
    multiple computations in parallel. A GPU achieves parallel processing through
    a large number of cores. This translates to large memory bandwidth and faster
    gradient computation of all trainable parameters in the deep learning architecture
    than otherwise would be the case in a CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Scoring with test images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we may test the model using test (holdout) images. In this example, I uploaded
    five flower images, and we need to convert them all to the shape of `[224, 224]`
    and normalize pixel values to `[0, 1]`. As common practice, test images are stored
    separately from training and cross-validation images. Therefore, it is typical
    to have a different file path to test images:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We are going to download some test images for these types of flowers. The images
    are partitioned into training, validation, and test images at the following link:
    [https://dataverse.harvard.edu/api/access/datafile/4159750](https://dataverse.harvard.edu/api/access/datafile/4159750)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'So, in the next cell, you may use `wget` to download it to your notebook:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, unzip it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a data generator instance for test data. Since our `train_datagen` already
    knows how to do that, we may reuse this object. Make sure you specify the `working_dir`
    directory as a file path to where our test images reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s take a note of the label index:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output indicates the relative position of each label in the array of probability:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s also define a helper function that plots the images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s take a look at the test images and their corresponding labels (ground
    truth):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output for the test images is shown as follows. In the first three rows,
    one-hot encoding is `1` at the first position. This corresponds to `daisy` according
    to `test_generator.class_indices`, whereas for the last two rows, `1` is at the
    last position, indicating the last two images are of `tulips`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And we may plot these images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.1 – Test image examples; the first three are daisies, and the last
    two are tulips'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_4.1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.1 – Test image examples; the first three are daisies, and the last
    two are tulips
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the model to make predictions on these images, execute the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the prediction is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This output is a NumPy array of probabilities for each class in each image.
    Each row corresponds to an image, and consists of five probabilities for each
    label. The first three rows have the highest probability in the first position,
    and the last two rows have the highest probability in the last position. This
    means the model predicted that the first three images would be daisies, and the
    last two images tulips, according to the mapping provided by `test_generator.class_indices`.
  prefs: []
  type: TYPE_NORMAL
- en: It would also be helpful if we could output these results in a more readable
    format such as a CSV file, with filenames of the test images and their respective
    predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s map the probability magnitude with respect to the position and define
    a label reference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write a helper function to map position with respect to the actual label:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can map the position of each observation''s highest probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And we can take a look at `predicted_idx`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This means that in the first three images, the maximum probability occurs at
    position `0`, which corresponds to `daisy` according to `test_generator.class_indices`.
    By the same token, the last two images have the maximum probability occurring
    at position `4`, which corresponds to `tulips`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, apply the helper function to each row of the prediction output, and insert
    test image filenames (`test_generator.filenames`) alongside the prediction in
    a nicely formatted pandas DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results should look similar to the following diagram. Now you may save
    the pandas DataFrame to any format of your choice, such as a CSV file or a pickle:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Prediction output in a DataFrame format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Prediction output in a DataFrame format
  prefs: []
  type: TYPE_NORMAL
- en: This completes the demonstration of using a pretrained model from TensorFlow
    Hub, applying it to our own data, retraining the model, and making the prediction.
    We also saw how to leverage generators to ingest training data in batches to the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Hub sits at the highest level of model reusability. There, you will
    find many open source models already built for consumption via a technique known
    as transfer learning. In this chapter, we built a regression model using the `tf.keras`
    API. Building a model this way (custom) is actually not a straightforward task.
    Often, you will spend a lot of time experimenting with different model parameters
    and architectures. If your need falls into classification or regression problems
    that are compatible with pre-built open source models, then TensorFlow Hub is
    the one-stop shop for finding the classification or regression model for your
    data. However, for these pre-built models, you still need to investigate the data
    structure required for the input layer and provide a final output layer for your
    purpose. However, reusing these pre-built models in TensorFlow Hub will save time
    in building and debugging your own model architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to see the TensorFlow Keras API, which is
    the newest high-level API that provides many reusable models.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the TensorFlow Keras API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras is a deep learning API that wraps around machine learning libraries such
    as TensorFlow, Theano, and Microsoft Cognitive Toolkit (also known as CNTK). Its
    popularity as a standalone API stems from the succinct style of the model construction
    process. As of 2018, TensorFlow added Keras as a high-level API moving forward,
    and it is now known as `tf.keras`. Starting with the TensorFlow 2.0 distribution
    released in 2019, `tf.keras` has become the official high-level API.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras` excels at modeling sophisticated deep learning architecture that
    contains `tf.keras` dense layer to build a regression model with tabular data
    from BigQuery.'
  prefs: []
  type: TYPE_NORMAL
- en: Data acquisition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to use a publicly available dataset from Google Cloud as the working
    data for this example:'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is our table of interest:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You may find it in BigQuery:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – BigQuery portal and table selection'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_4.3.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.3 – BigQuery portal and table selection
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s take a look at the data by running the following query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding query helps us to retrieve 1,000 random rows of the table: `data.covid19_geotab_mobility_impact.us_border_volumes`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'And this is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Table columns in us_border_volumes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_4.4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Table columns in us_border_volumes
  prefs: []
  type: TYPE_NORMAL
- en: Solving a data science problem with the us_border_volumes table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The output consists of rows that are randomly selected from the table being
    queried. Your output will consist of different values.
  prefs: []
  type: TYPE_NORMAL
- en: In the `us_border_volumes` table, each record represents a truck's entry or
    exit at a USA border point. The attributes in each record are `trip_direction`,
    `day_type`, `day_of_week`, `date`, `avg_crossing_duration`, `percent_of_normal_volume`,
    `avg_crossing_duration_truck`, and `percent_of_nortal_volume_truck`. We would
    like to build a model that predicts how long it would take for a truck to cross
    a border, given these features.
  prefs: []
  type: TYPE_NORMAL
- en: Selecting features and a target for model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here is an example problem that we will use to demonstrate how to leverage a
    TensorFlow I/O pipeline to provide training data to a model.
  prefs: []
  type: TYPE_NORMAL
- en: Let's set this problem up as a regression problem with this data. We are going
    to build a regression model to predict the average time it takes for a truck to
    cross the border (`avg_crossing_duration_truck`). Other columns (except `date`)
    are the features.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the rest of this example, we are going to use the Google AI Platform JupyterLab
    notebook with TensorFlow Enterprise 2.1 distribution. You may reuse the project
    ID from [*Chapter 2*](B16070_02_Final_JM_ePub.xhtml#_idTextAnchor061), *Running
    TensorFlow Enterprise in Google AI Platform*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having identified the data source, we are going to build a streaming workflow
    to feed training data to the model. This is different from reading the table as
    a pandas DataFrame in the Python runtime. We want to stream the training data
    by batches rather than using up all the memory allocated for the Python runtime.
    Therefore, we are going to use TensorFlow I/O for streaming the training data
    from BigQuery:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with the following code to import the necessary libraries and
    set up environmental variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a session to read from BigQuery:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have just selected fields of interest from the table in BigQuery. Now that
    the table is read as a dataset, we need to designate each column as either features
    or target. Let''s use this helper function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we apply this function to each row of the training dataset. This is essentially
    a transformation of the dataset, because we are applying a function that splits
    a dataset into a tuple of two dictionaries—features and target:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And now we will shuffle the dataset and batch it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we identified a table from BigQuery, identified the feature
    and target columns, convert the table to a TensorFlow dataset, shuffled it, and
    batched it. This is a common technique that is preferred when you are not sure
    if data size presents a problem in terms of memory usage.
  prefs: []
  type: TYPE_NORMAL
- en: For the next section, we are going to look at the `tf.keras` API and how to
    use it to build and train a model.
  prefs: []
  type: TYPE_NORMAL
- en: Input to a model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'So far, we have taken care of specifying features and the target in the training
    dataset. Now, we need to specify each feature as either categorical or numeric.
    This requires us to set up TensorFlow''s `feature_columns` object. The `feature_columns`
    object is the input to the model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each categorical column, we need to keep track of the possible categories.
    This is done through a helper function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we can create the `feature_columns` object (which is really a Python
    list) with the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that the target column is not in the `feature_columns`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now all we have to do is create a layer that creates the input to the model.
    The first layer is the feature columns'' input to the model, which is a multilayer
    perceptron as defined by a series of reusable `Dense` layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we created a flow to ingest our dataset into the model's feature
    layer. During this process, for categorical columns, we have to one-hot encode
    it as these columns are not numeric. We then build a model architecture with the
    `tf.keras` API.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to compile this model and launch the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Model training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before the model can be used, we need to compile it. Since this is a regression
    model, we may specify `loss` function, and for training metrics, we will track
    MSE as well as **mean-absolute-error** (**MAE**):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compile the model with the proper `loss` function and metrics used in the regression
    task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the model is trained, we may create a sample test dataset with two observations.
    The test data has to be in a dictionary format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To score this test sample, execute the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This indicates the predicted average number of waiting minutes for a truck to
    cross the border (`avg_crossing_duration_truck`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We just learned how to reuse the `tf.keras` dense layer and the sequential API,
    and integrate it with a data input pipeline driven by the use of datasets for
    streaming, and `feature_column` objects for feature encoding.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras` is a high-level API that provides another set of reusable elements
    specifically for deep learning problems. If your solution requires deep learning
    techniques, then `tf.keras` is the recommended starting point.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we are going to take a look at another high-level API known
    as TensorFlow Estimators. Before the `tf.keras` API became a first-class citizen
    in TensorFlow and in the 1.x TensorFlow distribution, TensorFlow Estimators was
    the only high-level API available.
  prefs: []
  type: TYPE_NORMAL
- en: So, in the next section, we will take a look at how it works.
  prefs: []
  type: TYPE_NORMAL
- en: Working with TensorFlow Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: TensorFlow estimators are also reusable components. The Estimators are higher-level
    APIs that enable users to build, train, and deploy machine learning models. It
    has several pre-made models that can save users from the hassle of creating computational
    graphs or sessions. This makes it easier for users to try different model architectures
    quickly with limited code changes. The Estimators are not specifically dedicated
    to deep learning models in the same way as `tf.keras`. Therefore, you will not
    find a lot of pre-made deep learning models available. If you need to work with
    deep learning frameworks, then the `tf.keras` API is the right choice to get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we are going to set up the same regression problem and build
    a regression model. The source of data is the same one we used in streaming training
    data, which is available through Google Cloud''s BigQuery:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: This is the same BigQuery table (*Figure 4.4*) that we used for the `tf.keras`
    section. See *Figure 4.4* for some randomly extracted rows of this table.
  prefs: []
  type: TYPE_NORMAL
- en: Just as we did in the previous section using the `tf.keras` API, here we want
    to build a linear regression model using TensorFlow Estimators to predict the
    average time it takes for a truck to cross the border (`avg_crossing_duration_truck`).
    Other columns (except `date`) are the features.
  prefs: []
  type: TYPE_NORMAL
- en: The pattern of using the TensorFlow Estimators API to build and train a model
    is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Create an `estimator` object by invoking an estimator (that is, for a pre-made
    estimator such as a linear regressor) and specify the `feature_columns` object,
    so the model knows what data types to expect in the feature data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `estimator` object to `call .train()` and pass an input function to
    it. This input function is responsible for parsing training data and the label.
    Since we are setting up a regression problem, let''s use the pre-made linear regression
    estimator as an example. This is the common pattern for training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, the following is observed:'
  prefs: []
  type: TYPE_NORMAL
- en: First, an instance of a linear regressor, `linear_est`, is created with a `feature_columns`
    object. This object provides an annotation regarding each feature (numeric or
    categorical data types). `model_dir` is the specified directory to save the model
    by checkpoints.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next in the code is `linear_est.train(input_fn)`. This instance invokes the
    `train()` method to start the training process. The `train()` method takes on
    a function, `input_fn`. This is responsible for streaming, formatting, and sending
    the training data by batches into the model. We will take a look at how to construct
    `input_fn`. In other words, TensorFlow Estimators separates data annotation from
    the data ingestion process for training workflows.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data pipeline for TensorFlow Estimators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like `tf.keras`, TensorFlow Estimators can leverage the streaming data pipeline
    when running in the TensorFlow Enterprise environment, such as the Google Cloud
    AI Platform. In this section, as an example, we are going to see how to stream
    training data (from a table in BigQuery) into a TensorFlow Estimator model.
  prefs: []
  type: TYPE_NORMAL
- en: Below are the steps to building a BigQuery data pipeline for TensorFlow Estimator's
    consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we start with the `import` operations for the requisite libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we specify a few parameters for our table of interest in BigQuery. Make
    sure you specify your own `PROJECT_ID`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will specify the input function for the training process. This input
    function will handle the read operation, data annotation, transformation, and
    separation of the target from features by means of the `transform_row` function.
    These are exactly the identical operations seen in the previously described `tf.keras`
    example in the *Leveraging TensorFlow Keras API* section. The only difference
    is that we now wrap all of the code as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are still inside `input_fn`. Continue on with `input_fn`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We also reorganized how we specify features and the target in our data with
    the `transform_row` function inside `input_fn`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This concludes the entire `input_fn`. At this point, `input_fn` returns a dataset
    read from `us_border_volumes`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Just as we discussed in the `tf.keras` example in the *Leveraging TensorFlow
    Keras API* section, we also need to build a `feature_columns` object for feature
    annotation. We can reuse the same code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s set up a directory to save the model checkpoints:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following command to make the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Launch the training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This completes the model training process.
  prefs: []
  type: TYPE_NORMAL
- en: Since the `estimator` model expects input to be a function, in order to use
    the `estimator` for scoring, I have to pass into it a function that takes the
    test data, formats it, and feeds it to the model just like the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The input function for training basically did two things:'
  prefs: []
  type: TYPE_NORMAL
- en: It queried the table and got the dataset representation of the table.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It transformed the data by separating the target from the features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In terms of the scoring situation here, we do not need to worry about this.
    We just need to get a dataset representation of the test data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can reuse the same test data as shown in the *Model training* section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a helper function to convert `test_samples` to a dataset with the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next step involves scoring the test data with the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let''s print the prediction as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, we iterate through the model output, which is a dictionary.
    To refer to the values associated with the model output, we need to access it
    by the key named `prediction`. For readability, we convert it to a list and print
    it out as a string. It shows the first truck is predicted to cross the border
    in `23.87` minutes, while the second truck is predicted to cross the border in
    `13.62` minutes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: TensorFlow Estimators was the only high-level API before `tf.keras` became an
    official part of TensorFlow distribution. While it contains many pre-made modules,
    such as linear regressors and different flavors of classifiers, it lacks the support
    for some of the commonplace deep learning modules, including CNN, LSTM, and GRU.
    But if your need can be addressed with non-deep learning regressors or classifiers,
    then TensorFlow Estimators is a good starting point. It also integrates with the
    data ingestion pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have seen how the three major sources of reusable model
    elements can integrate with the scalable data pipeline. Through TensorFlow datasets
    and TensorFlow I/O APIs, training data is streamed into the model training process.
    This enables models to be trained without having to deal with the compute node's
    memory.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow Hub sits at the highest level of model reusability. There, you will
    find many open source models already built for consumption via a technique known
    as transfer learning. In this chapter, we built a regression model using the `tf.keras`
    API. Building a model this way (custom) is actually not a straightforward task.
    Often, you will spend a lot of time experimenting with different model parameters
    and architectures. If your need can be addressed by means of pre-built open source
    models, then TensorFlow Hub is the place. However, for these pre-built models,
    you still need to investigate the data structure required for the input layer,
    and provide a final output layer for your purpose. However, reusing these pre-built
    models in TensorFlow Hub will save time in building and debugging your own model
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '`tf.keras` is a high-level API that provides another set of reusable elements
    specifically for deep learning problems. If your solution requires deep learning
    techniques, then `tf.keras` is the recommended starting point. With the help of
    an example, we have seen how a multilayer perceptron can be built quickly with
    the `tf.keras` API and integrated with the TensorFlow I/O module that streams
    training data.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to take up what we learned about `tf.keras`
    and TensorFlow Hub here, and leverage Google Cloud AI Platform to run our model
    training routine as a cloud training job.
  prefs: []
  type: TYPE_NORMAL
