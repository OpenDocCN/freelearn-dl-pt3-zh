["```\n    import tensorflow as tf\n    ```", "```\n    import numpy as np\n    ```", "```\n    import matplotlib.pyplot as plt\n    ```", "```\n    (x_train,y_train),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()\n    ```", "```\n    len(x_train), len(x_test)\n    ```", "```\n(60000, 10000)\n```", "```\n    # Reshape the images(batch_size, height, width, channels)\n    ```", "```\n    x_train = x_train.reshape(x_train.shape[0],\n    ```", "```\n        28, 28, 1).astype('float32')\n    ```", "```\n    x_test = x_test.reshape(x_test.shape[0],\n    ```", "```\n        28, 28, 1).astype('float32')\n    ```", "```\n    # Normalize the pixel values\n    ```", "```\n    x_train /= 255\n    ```", "```\n    x_test /= 255\n    ```", "```\n# Convert the labels to one hot encoding format\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n```", "```\n    # Build the Sequential model\n    ```", "```\n    model = tf.keras.models.Sequential()\n    ```", "```\n    # Add convolutional layer\n    ```", "```\n    model.add(tf.keras.layers.Conv2D(64,kernel_size=(3,3),\n    ```", "```\n        activation='relu',\n    ```", "```\n        input_shape=(28, 28, 1)))\n    ```", "```\n    # Add max pooling layer\n    ```", "```\n    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n    ```", "```\n    # Flatten the data\n    ```", "```\n    model.add(tf.keras.layers.Flatten())\n    ```", "```\n    # Add fully connected layer\n    ```", "```\n    model.add(tf.keras.layers.Dense(128,\n    ```", "```\n                                    activation='relu'))\n    ```", "```\n    # Apply softmax\n    ```", "```\n    model.add(tf.keras.layers.Dense(10,\n    ```", "```\n                                    activation='softmax'))\n    ```", "```\n    # Compile and fit the model\n    ```", "```\n    model.compile(loss='categorical_crossentropy',\n    ```", "```\n                  optimizer='adam', metrics=['accuracy'])\n    ```", "```\n    model.fit(x_train, y_train, epochs=10,\n    ```", "```\n              validation_split=0.2)\n    ```", "```\nEpoch 6/10\n1500/1500 [==============================] - 5s 3ms/step - loss: 0.1267 - accuracy: 0.9532 - val_loss: 0.2548 - val_accuracy: 0.9158\nEpoch 7/10\n1500/1500 [==============================] - 5s 4ms/step - loss: 0.1061 - accuracy: 0.9606 - val_loss: 0.2767 - val_accuracy: 0.9159\nEpoch 8/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0880 - accuracy: 0.9681 - val_loss: 0.2957 - val_accuracy: 0.9146\nEpoch 9/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0697 - accuracy: 0.9749 - val_loss: 0.3177 - val_accuracy: 0.9135\nEpoch 10/10\n1500/1500 [==============================] - 6s 4ms/step - loss: 0.0588 - accuracy: 0.9785 - val_loss: 0.3472 - val_accuracy: 0.9133\n```", "```\n    model.summary()\n    ```", "```\nModel: \"sequential\"\n______________________________________________________\n Layer (type)             Output Shape         Param #\n======================================================\n conv2d (Conv2D)          (None, 26, 26,64)    640\n max_pooling2d (MaxPooling2D  (None,13,13,64)  0    )\n flatten (Flatten)            (None, 10816)    0\n dense (Dense)                (None, 128)      1384576\n dense_1 (Dense)              (None, 10)       1290\n======================================================\nTotal params: 1,386,506\nTrainable params: 1,386,506\nNon-trainable params: 0\n_________________________________________________________________\n```", "```\n    # Evaluate the model\n    ```", "```\n    score = model.evaluate(x_test, y_test)\n    ```", "```\n    import os\n    ```", "```\n    import pathlib\n    ```", "```\n    import matplotlib.pyplot as plt\n    ```", "```\n    import matplotlib.image as mpimg\n    ```", "```\n    import random\n    ```", "```\n    import numpy as np\n    ```", "```\n    from PIL import Image\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    from tensorflow import keras\n    ```", "```\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    ```", "```\n    for dirpath, dirnames, filenames in os.walk(root_dir):\n    ```", "```\n        print(f\"Directory: {dirpath}\")\n    ```", "```\n        print(f\"Number of images: {len(filenames)}\")\n    ```", "```\n        print()\n    ```", "```\n    def retrieve_labels(train_dir, test_dir, val_dir):\n    ```", "```\n        # Retrieve labels from training directory\n    ```", "```\n        train_labels = os.listdir(train_dir)\n    ```", "```\n        print(f\"Training labels: {train_labels}\")\n    ```", "```\n        print(f\"Number of training labels: {len(train_labels)}\")\n    ```", "```\n        print()\n    ```", "```\n        # Retrieve labels from test directory\n    ```", "```\n        test_labels = os.listdir(test_dir)\n    ```", "```\n        print(f\"Test labels: {test_labels}\")\n    ```", "```\n        print(f\"Number of test labels: {len(test_labels)}\")\n    ```", "```\n        print()\n    ```", "```\n        # Retrieve labels from validation directory\n    ```", "```\n        val_labels = os.listdir(val_dir)\n    ```", "```\n        print(f\"Validation labels: {val_labels}\")\n    ```", "```\n        print(f\"Number of validation labels: {len(val_labels)}\")\n    ```", "```\n        print()\n    ```", "```\n    train_dir = \"/content/drive/MyDrive/weather dataset/train\"\n    ```", "```\n    test_dir = \"/content/drive/MyDrive/weather dataset/test\"\n    ```", "```\n    val_dir = \"/content/drive/MyDrive/weather dataset/validation\"\n    ```", "```\n    retrieve_labels(train_dir, test_dir, val_dir)\n    ```", "```\nTraining labels: ['cloud', 'shine', 'rain', 'sunrise']\nNumber of training labels: 4\nTest labels: ['sunrise', 'shine', 'cloud', 'rain']\nNumber of test labels: 4\nValidation labels: ['shine', 'sunrise', 'cloud', 'rain']\nNumber of validation labels: 4\n```", "```\n    def view_random_images(target_dir, num_images):\n    ```", "```\n      \"\"\"\n    ```", "```\n      View num_images random images from the subdirectories of target_dir as a subplot.\n    ```", "```\n      \"\"\"\n    ```", "```\n      # Get list of subdirectories\n    ```", "```\n        subdirs = [d for d in os.listdir(\n    ```", "```\n            target_dir) if os.path.isdir(os.path.join(\n    ```", "```\n                target_dir, d))]\n    ```", "```\n      # Select num_images random subdirectories\n    ```", "```\n        random.shuffle(subdirs)\n    ```", "```\n        selected_subdirs = subdirs[:num_images]\n    ```", "```\n      # Create a subplot\n    ```", "```\n        fig, axes = plt.subplots(1, num_images, figsize=(15,9))\n    ```", "```\n        for i, subdir in enumerate(selected_subdirs):\n    ```", "```\n          # Get list of images in subdirectory\n    ```", "```\n            image_paths = [f for f in os.listdir(\n    ```", "```\n                os.path.join(target_dir, subdir))]\n    ```", "```\n          # Select a random image\n    ```", "```\n            image_path = random.choice(image_paths)\n    ```", "```\n          # Load image\n    ```", "```\n            image = plt.imread(os.path.join(target_dir,\n    ```", "```\n                subdir, image_path))\n    ```", "```\n          # Display image in subplot\n    ```", "```\n            axes[i].imshow(image)\n    ```", "```\n            axes[i].axis(\"off\")\n    ```", "```\n            axes[i].set_title(subdir)\n    ```", "```\n        print(f\"Shape of image: {image.shape}\")    \n    ```", "```\n        #width,height, colour chDNNels\n    ```", "```\n        plt.show()\n    ```", "```\n    view_random_images(target_dir=\"/content/drive/MyDrive/weather dataset/train/\", num_images=4)\n    ```", "```\n# Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization)\ntrain_datagen = ImageDataGenerator(rescale=1./255)\nvalid_datagen = ImageDataGenerator(rescale=1./255)\ntest_datagen = ImageDataGenerator(rescale=1./255)\n```", "```\n# Import data from directories and turn it into batches\ntrain_data = train_datagen.flow_from_directory(train_dir,\n    batch_size=64, # number of images to process at a time\n    target_size=(224,224), # convert all images to be 224 x 224\n    class_mode=\"categorical\")\nvalid_data = valid_datagen.flow_from_directory(val_dir,\n    batch_size=64,\n    target_size=(224,224),\n    class_mode=\"categorical\")\ntest_data = test_datagen.flow_from_directory(test_dir,\n    batch_size=64,\n    target_size=(224,224),\n    class_mode=\"categorical\")\n```", "```\nmodel_1 = tf.keras.models.Sequential([\n    tf.keras.layers.Conv2D(filters=16,\n        kernel_size=3, # can also be (3, 3)\n        activation=\"relu\",\n        input_shape=(224, 224, 3)),\n        #(height, width, colour channels)\ntf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Conv2D(32, 3, activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Conv2D(64, 3, activation=\"relu\"),\n    tf.keras.layers.MaxPool2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(1050, activation=\"relu\"),\n    tf.keras.layers.Dense(4, activation=\"softmax\")\n    # binary activation output\n])\n# Compile the model\nmodel_1.compile(loss=\"CategoricalCrossentropy\",\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=[\"accuracy\"])\n# Fit the model\nhistory_1 = model_1.fit(train_data,\n    epochs=10,\n    validation_data=valid_data,\n    )\n```", "```\nEpoch 6/10\n13/13 [==============================] - 8s 622ms/step - loss: 0.1961 - accuracy: 0.9368 - val_loss: 0.2428 - val_accuracy: 0.8994\nEpoch 7/10\n13/13 [==============================] - 8s 653ms/step - loss: 0.1897 - accuracy: 0.9241 - val_loss: 0.2967 - val_accuracy: 0.9218\nEpoch 8/10\n13/13 [==============================] - 8s 613ms/step - loss: 0.1093 - accuracy: 0.9671 - val_loss: 0.3447 - val_accuracy: 0.8939\nEpoch 9/10\n13/13 [==============================] - 8s 604ms/step - loss: 0.1756 - accuracy: 0.9381 - val_loss: 0.6276 - val_accuracy: 0.8324\nEpoch 10/10\n13/13 [==============================] - 8s 629ms/step - loss: 0.1472 - accuracy: 0.9418 - val_loss: 0.2633 - val_accuracy: 0.9106\n```", "```\nModel: \"sequential\"\n___________________________________________________________\n Layer (type)                 Output Shape         Param #\n===========================================================\n conv2d (Conv2D)              (None, 222, 222, 16) 448\n max_pooling2d (MaxPooling2D) (None, 111, 111, 16) 0\n conv2d_1 (Conv2D)            (None, 109, 109, 32) 4640\n max_pooling2d_1 (MaxPooling  (None, 54, 54, 32)   0\n 2D)\n conv2d_2 (Conv2D)            (None, 52, 52, 64)   18496\n max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)   0\n 2D)\n flatten (Flatten)            (None, 43264)        0\n dense (Dense)                (None, 1050)         45428250\n dense_1 (Dense)              (None, 4)            4204\n===========================================================\nTotal params: 45,456,038\nTrainable params: 45,456,038\nNon-trainable params: 0\n___________________________________________________________\n```", "```\nmodel_1.evaluate(test_data)\n```", "```\nmodel_2 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(224, 224, 3)),\n    tf.keras.layers.Dense(1200, activation='relu'),\n    tf.keras.layers.Dense(600, activation='relu'),\n    tf.keras.layers.Dense(300, activation='relu'),\n    tf.keras.layers.Dense(4, activation='softmax')\n])\n# Compile the model\nmodel_2.compile(loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=[\"accuracy\"])\n# Fit the model\nhistory_2 = model_2.fit(train_data,\n    epochs=10,\n    validation_data=valid_data)\n```", "```\nEpoch 6/10\n13/13 [==============================] - 8s 625ms/step - loss: 2.2083 - accuracy: 0.6953 - val_loss: 0.9884 - val_accuracy: 0.7933\nEpoch 7/10\n13/13 [==============================] - 8s 606ms/step - loss: 2.7116 - accuracy: 0.6435 - val_loss: 2.0749 - val_accuracy: 0.6704\nEpoch 8/10\n13/13 [==============================] - 8s 636ms/step - loss: 2.8324 - accuracy: 0.6877 - val_loss: 1.7241 - val_accuracy: 0.7430\nEpoch 9/10\n13/13 [==============================] - 8s 599ms/step - loss: 1.8597 - accuracy: 0.6890 - val_loss: 1.1507 - val_accuracy: 0.7877\nEpoch 10/10\n13/13 [==============================] - 8s 612ms/step - loss: 1.0902 - accuracy: 0.7813 - val_loss: 0.9915 - val_accuracy: 0.7486\n```", "```\nmodel_3 = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(224, 224, 3)),\n    tf.keras.layers.Dense(1000, activation='relu'),\n    tf.keras.layers.Dense(500, activation='relu'),\n    tf.keras.layers.Dense(500, activation='relu'),\n    tf.keras.layers.Dense(4, activation='softmax')\n])\n# Compile the model\nmodel_3.compile(loss='categorical_crossentropy',\n    optimizer=tf.keras.optimizers.Adam(),\n    metrics=[\"accuracy\"])\n# Fit the model\nhistory_3 = model_3.fit(train_data,\n    epochs=10,\n    validation_data=valid_data)\n```", "```\nEpoch 6/10\n13/13 [==============================] - 9s 665ms/step - loss: 1.6911 - accuracy: 0.6814 - val_loss: 0.5861 - val_accuracy: 0.7877\nEpoch 7/10\n13/13 [==============================] - 8s 606ms/step - loss: 0.7309 - accuracy: 0.7952 - val_loss: 0.5100 - val_accuracy: 0.8268\nEpoch 8/10\n13/13 [==============================] - 8s 572ms/step - loss: 0.6797 - accuracy: 0.7863 - val_loss: 0.9520 - val_accuracy: 0.7263\nEpoch 9/10\n13/13 [==============================] - 8s 632ms/step - loss: 0.7430 - accuracy: 0.7724 - val_loss: 0.5220 - val_accuracy: 0.7933\nEpoch 10/10\n13/13 [==============================] - 8s 620ms/step - loss: 0.5845 - accuracy: 0.7737 - val_loss: 0.5881 - val_accuracy: 0.7765\n```", "```\ndef evaluate_models(models, model_names,test_data):\n    # Initialize lists for the results\n    losses = []\n    accuracies = []\n    # Iterate over the models\n    for model in models:\n        # Evaluate the model\n        loss, accuracy = model.evaluate(test_data)\n        losses.append(loss)\n        accuracies.append(accuracy)\n       # Convert the results to percentages\n    losses = [round(loss * 100, 2) for loss in losses]\n    accuracies = [round(accuracy * 100, 2) for accuracy in accuracies]\n    # Create a dataframe with the results\n    results = pd.DataFrame({\"Model\": model_names,\n        \"Loss\": losses,\n        \"Accuracy\": accuracies})\n    return results\n```", "```\n# Define the models and model names\nmodels = [model_1, model_2, model_3]\nmodel_names = [\"Model 1\", \"Model 2\", \"Model 3\"]\n# Evaluate the models\nresults = evaluate_models(models, model_names,test_data)\n# Display the results\nresults\n```", "```\ndef plot_loss_accuracy(history_1):\n  # Extract the loss and accuracy history for both training and validation data\n    loss = history_1.history['loss']\n    val_loss = history_1.history['val_loss']\n    acc = history_1.history['accuracy']\n    val_acc = history_1.history['val_accuracy']\n  # Create subplots\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 6))\n  # Plot the loss history\n    ax1.plot(loss, label='Training loss')\n    ax1.plot(val_loss, label='Validation loss')\n    ax1.set_title('Loss history')\n    ax1.set_xlabel('Epoch')\n    ax1.set_ylabel('Loss')\n    ax1.legend()\n  # Plot the accuracy history\n    ax2.plot(acc, label='Training accuracy')\n    ax2.plot(val_acc, label='Validation accuracy')\n    ax2.set_title('Accuracy history')\n    ax2.set_xlabel('Epoch')\n    ax2.set_ylabel('Accuracy')\n    ax2.legend()\n    plt.show()\n```", "```\n# Lets plot the training and validation loss and accuracy\nplot_loss_accuracy(history_1)\n```"]