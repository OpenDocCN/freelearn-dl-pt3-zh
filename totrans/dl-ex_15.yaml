- en: Face Generation and Handling Missing Labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The list of interesting applications that we can use GANs for is endless. In
    this chapter, we are going to demonstrate another promising application of GANs,
    which is face generation based on the CelebA database. We'll also demonstrate
    how to use GANs for semi-supervised learning setups where we've got a poorly labeled
    dataset with some missing labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Face generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semi-supervised learning with generative adversarial networks (GANs)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Face generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As we mentioned in the previous chapter, the Generator and Discriminator consist
    of a **Deconvolutional Network** (**DNN**: [https://www.quora.com/How-does-a-deconvolutional-neural-network-work](https://www.quora.com/How-does-a-deconvolutional-neural-network-work))
    and **Convolutional Neural Network** (**CNN**: [http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)):'
  prefs: []
  type: TYPE_NORMAL
- en: CNN is a a type of neural network that encodes hundreds of pixels of an image
    into a vector of small dimensions (z), which is a summary of the image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DNN is a network that learns some filters to recover the original image from
    z
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, the discriminator will output one or zero to indicate whether the input
    image is from the actual dataset or generated by the generator. On the other side,
    the generator will try to replicate images similar to the original dataset based
    on the latent space z, which might follow a Gaussian distribution. So, the goal
    of the discriminator is to correctly discriminate between the real images, and
    the goal of the generator is to learn the distribution of the original dataset
    and hence fool the discriminator so that it makes a wrong decision.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll try to teach the generator to learn human face image
    distribution so that it can generate realistic faces.
  prefs: []
  type: TYPE_NORMAL
- en: Generating human-like faces is crucial for most graphics companies, who are
    always looking for new faces for their applications, and it gives us a clue of
    how artificial intelligence is really close to achieving realism in generating
    artificial faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we''ll be using the CelebA dataset. The CelebFaces Attributes
    Dataset (CelebA)  is a large-scale face attributes dataset with about 200K celebrity
    images, each with 40 attribute annotations. There are lots of pose variations
    covered by the dataset, as well as background clutter, so CelebA is very diverse
    and well annotated. it includes:'
  prefs: []
  type: TYPE_NORMAL
- en: 10,177 identities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 202,599 face images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Five landmark locations and 40 binary attribute annotations per image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can use this dataset for many computer vision applications other than face
    generation, such as face recognition and localization, or face attribute detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'This figure shows how the generator error, or learning human face distribution,
    gets close to realism during the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/26446f87-864b-4267-95b8-a4980fd70a11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: GANs for generating new faces from a celebrity images dataset'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will define some helper functions that will help us to
    download the CelebA dataset. We''ll start off by importing their required packages
    for this implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we are going to use the utils script to download the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Exploring the Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The CelebA dataset contains over 200k annotated celebrity images. Since we are
    going to use GANs to generate similar images, it is worth looking at a bunch of
    images from the dataset and see how they look. In this section, we are going to
    define some helper functions for visualizing a bunch of images from the CelebA
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use the `utils` script to display some images from the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1972d296-3172-438f-8dd0-4b700ba8c968.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Plotting a sample of images from CelebA dataset'
  prefs: []
  type: TYPE_NORMAL
- en: The main focus of this computer vision task is to use GANs for generating images
    similar two the ones in the celebrity dataset, so we'll need to focus on the face
    part of the images. To focus on the face part of an image, we are going to remove
    the parts of the image that don't include a celebrity face.
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s start off by building the core of our implementation, which is
    the computational graph; it will mainly include the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Model inputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discriminator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model losses
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we are going to implement a helper function that we'll define
    the model input placeholders which will responsible for feeding the data the the
    computational graph.
  prefs: []
  type: TYPE_NORMAL
- en: 'The functions should be able to create three main placeholders:'
  prefs: []
  type: TYPE_NORMAL
- en: Actual input images from the dataset which will have the dimensions of (batch
    size, input image width, input image height, number of channels)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The latent space Z, which will be used by the generator for generating fake
    images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning rate placeholder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The helper function will return a tuple of these three input placeholders.
    So, let''s go ahead and define this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Next up, we need to implement the discriminator part of the network, which will
    be used to judge whether the incoming input is coming from the real dataset or
    generated by the generator. Again, we'll use the TensorFlow feature of `tf.variable_scope`
    to prefix some variables with discriminator so that we can retrieve and reuse
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s define the function which will return the binary output of the discriminator
    as well as the logit values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it's time to implement the second part of the network that will be trying
    to replicate the original input images using the latent space `z`. We'll be using
    `tf.variable_scope` for this function as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s define the function which will return a generated image by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Model losses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now comes the tricky part, which we covered in the previous chapter, which is
    to calculate the losses of the discriminator and the generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s define this function, which will make use of the `generator` and
    `discriminator` functions that were defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Model optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, before training our model, we need to implement the optimization criteria
    for this task. We will use the naming conventions that we used previously to retrieve
    the trainable parameters for the discriminator and the generator and train them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Training the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it's time to train the model and see how the generator will be able to
    fool, to some extent, the discriminator, by generating images very close to the
    original CelebA dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'But first, let''s define a helper function that will display some generated
    images by the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will use the helper functions that we have defined before to build
    the model inputs, loss, and optimization criteria. We stack them together and
    start training our model based on the CelebA dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Kick off the training process, which might take some time depending on your
    host machine specs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/2d5b7d29-193a-4b1d-898d-8f1a4345ca3e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Sample generated output at this point of training'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/5b21242d-61d1-45ba-90be-a766f2d984fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Sample generated output at this point of training'
  prefs: []
  type: TYPE_NORMAL
- en: 'After some time of training, you should get something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e78748d2-24a1-4507-9a53-c202659576b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Sample generated output at this point of training'
  prefs: []
  type: TYPE_NORMAL
- en: Semi-supervised learning with Generative Adversarial Networks (GANs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With that in mind, semi-supervised learning is a technique in which both labeled
    and unlabeled data is used to train a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: This type of classifier takes a tiny portion of labeled data and a much larger
    amount of unlabeled data (from the same domain). The goal is to combine these
    sources of data to train a **Deep Convolution Neural Network** (**DCNN**) to learn
    an inferred function capable of mapping a new datapoint to its desirable outcome.
  prefs: []
  type: TYPE_NORMAL
- en: In this frontier, we present a GAN model to classify street view house numbers
    using a very small labeled training set. In fact, the model uses roughly 1.3%
    of the original SVHN training labels i.e. 1000 (one thousand) labeled examples.
    We use some of the techniques described in the paper *Improved Techniques for
    Training GANs from OpenAI* ([https://arxiv.org/abs/1606.03498](https://arxiv.org/abs/1606.03498)).
  prefs: []
  type: TYPE_NORMAL
- en: Intuition
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building a GAN for generating images, we trained both   the generator and
    the discriminator at the same time. After training, we can discard the discriminator
    because we only used it for training the generator.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7b223ba3-8c25-4477-9a17-f04fa287296b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Semi-supervised learning GAN architecture for an 11 class classification
    problem'
  prefs: []
  type: TYPE_NORMAL
- en: In semi-supervised learning, we need to transform the discriminator into a multi-class
    classifier. This new model has to be able to generalize well on the test set,
    even though we do not have many labeled examples for training. Additionally, this
    time, by the end of training, we can actually throw away the generator. Note that
    the roles changed. Now, the generator is only used for helping the discriminator
    during training. Putting it differently, the generator acts as a different source
    of information from which the discriminator gets raw, unlabeled training data.
    As we will see, this unlabelled data is key to improving the discriminator's performance.
    Also, for a regular image generation GAN, the discriminator has only one role.
    Compute the probability of whether its inputs are real or not — let's call it
    the GAN problem.
  prefs: []
  type: TYPE_NORMAL
- en: However, to turn the discriminator into a semi-supervised classifier, besides
    the GAN problem, the discriminator also has to learn the probabilities of each
    of the original dataset classes. In other words, for each input image, the discriminator
    has to learn the probabilities of it being a one, two, three, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that for an image generation GAN discriminator, we have a single sigmoid
    unit output. This value represents the probability of an input image being real
    (value close to 1), or fake (value near 0). In other words, from the discriminator's
    point of view, values close to 1 mean that the samples are likely to come from
    the training set. Likewise, value near 0 mean a higher chance that the samples
    come from the generator network. By using this probability, the discriminator
    is able to send a signal back to the generator. This signal allows the generator
    to adapt its parameters during training, making it possible to improve its capabilities
    of creating realistic images.
  prefs: []
  type: TYPE_NORMAL
- en: We have to convert the discriminator (from the previous GAN) into an 11 class
    classifier. To do that, we can turn its sigmoid output into a softmax with 11
    class outputs, the first 10 for the individual class probabilities of the SVHN
    dataset (zero to nine), and the 11th class for all the fake images that come from
    the generator.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if we set the 11th class probability to 0, then the sum of the first
    10 probabilities represents the same probability computed using the sigmoid function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to set up the losses in such a way that the discriminator
    can do both:'
  prefs: []
  type: TYPE_NORMAL
- en: Help the generator learn to produce realistic images. To do that, we have to
    instruct the discriminator to distinguish between real and fake samples.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the generator’s images, along with the labeled and unlabeled training data,
    to help classify the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To summarize, the discriminator has three different sources of training data:'
  prefs: []
  type: TYPE_NORMAL
- en: Real images with labels. These are image label pairs like in any regular supervised
    classification problem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real images without labels. For those, the classifier only learns that these
    images are real.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Images from the generator. To use these ones, the discriminator learns to classify
    as fake.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The combination of these different sources of data will make the classifier
    able to learn from a broader perspective. That, in turn, allows the model to perform
    inference much more precisely than it would be if only using the 1,000 labeled
    examples for training.
  prefs: []
  type: TYPE_NORMAL
- en: Data analysis and preprocessing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this task, we will be using SVHN dataset, which is an abbreviation for
    Street View House Numbers by Stanford ([http://ufldl.stanford.edu/housenumbers/](http://ufldl.stanford.edu/housenumbers/)).
    So, let''s start the implementation by importing the required packages for this
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Next up, we are going to define a helper class to download the SVHN dataset
    (remember that you need to manually create the `input_data_dir` first):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s get a sense of what these images look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a75e45c4-fe45-4bf0-b1a6-52d1ddbdeef6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Sample images from the SVHN dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next up, we need to scale our images to be between -1 and 1, and this will
    be necessary since we are going to use the `tanh()` function, which will squash
    the output values of the generator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Building the model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will build all the bits and pieces that are necessary for
    our test, so let's start off by defining the inputs that will be used to feed
    data to the computational graph.
  prefs: []
  type: TYPE_NORMAL
- en: Model inputs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First off, we are going to define the model inputs function, which will create
    the model input placeholders to be used for feeding data to the computational
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Generator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to implement the first core part of the GAN network.
    The architecture and implementation of this part will follow the original DCGAN
    paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Discriminator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, it's time to build the second core piece of the GAN network, which is the
    discriminator. In previous implementations, we said that the discriminator will
    produce a binary output that represents whether the input image is from the real
    dataset (1) or it's generated by the generator (0). The scenario is different
    here, so the discriminator will now be a multi-class classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go ahead and build up the discriminator part of the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Instead of applying a fully connected layer at the end, we are going to perform
    so-called **global average pooling** (**GAP**), which takes the average over the
    spatial dimensions of a feature vector; this will produce a squashed tensor to
    only a single value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, suppose that after a stack of convolutions, we get an output tensor
    of shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To apply global average pooling, we calculate the average value on the [8x8]
    tensor slice. This operation will result in a tensor which is the following shape:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'That can be reshaped to:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After applying the global average pooling, we add a fully connected layer that
    will output the final logits. These have the shape of:'
  prefs: []
  type: TYPE_NORMAL
- en: '`[BATCH_SIZE, NUM_CLASSES]`'
  prefs: []
  type: TYPE_NORMAL
- en: 'which will represent the scores for each class. To get these scores for probability,
    we are going to use the `softmax` activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: And finally the discriminator function will look like this,
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Model losses
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it''s time to define the model losses. First off, the discriminator loss
    will be divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: One which will represent the GAN problem, which is the unsupervised loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second one will compute the individual actual class probabilities, which
    is the supervised loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the discriminator's unsupervised loss, it has to discriminate between actual
    training images and the generated images by the generator.
  prefs: []
  type: TYPE_NORMAL
- en: As for a regular GAN, half of the time, the discriminator will get unlabeled
    images from the training set as an input and the other half, fake, unlabeled images
    from the generator.
  prefs: []
  type: TYPE_NORMAL
- en: For the second part of the discriminator loss, which is the supervised loss,
    we need to build upon the logits from the discriminator. So, we will use the softmax
    cross entropy since it's a multi classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the *Enhanced Techniques for Training GANs* paper, we should
    use feature matching for the generator loss. As the authors describe:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Feature matching is the concept of penalizing the mean absolute error between
    the average value of some set of features on the training data and the average
    values of that set of features on the generated samples. To do that, we take some
    set of statistics (the moments) from two different sources and force them to be
    similar. First, we take the average of the features extracted from the discriminator
    when a real training minibatch is being processed. Second, we compute the moments
    in the same way, but now for when a minibatch composed of fake images that come
    from the generator was being analyzed by the discriminator. Finally, with these
    two sets of moments, the generator loss is the mean absolute difference between
    them. In other words, as the paper emphasizes: We train the generator to match
    the expected values of the features on an intermediate layer of the discriminator."'
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, the model loss function will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Model optimizer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s define the model optimizer, which is pretty much similar to the
    ones that we defined before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Model training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, let''s go ahead and kick off the training process after putting it
    all together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Don''t forget to create a directory called checkpoints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, at `Epoch 24`, you should get something close to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/16442734-420c-4f3e-ad7e-f233a4280e9a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Sample images created by the generator network using the feature
    matching loss'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/4e8ab204-3482-4502-ad4e-2610c537b0e0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Train versus Test accuracy over the training process'
  prefs: []
  type: TYPE_NORMAL
- en: Although feature matching loss performs well on the task of semi-supervised
    learning, the images produced by the generator are not as good as the ones created
    in the previous chapter. But this implementation was mainly introduced to demonstrate
    how we can use GANs for semi-supervised learning setups.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, many researchers consider unsupervised learning as the missing link
    in general AI systems. To overcome these obstacles, attempts to solve established
    problems using less labeled data is key. In this scenario, GANs pose a real alternative
    for learning complicated tasks with less labeled samples. Yet, the performance
    gap between supervised and semi-supervised learning is still far from being equal.
    We can certainly expect this gap to become shorter as new approaches come into
    play.
  prefs: []
  type: TYPE_NORMAL
