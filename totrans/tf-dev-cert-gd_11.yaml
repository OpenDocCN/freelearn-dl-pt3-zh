- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: NLP with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text data is inherently sequential, defined by the order in which words occur.
    Words follow one another, building upon previous ideas and shaping those to come.
    Understanding the sequence of words and the context in which they are applied
    is straightforward for humans. However, this poses a significant challenge to
    feed-forward networks such as **convolutional neural networks** (**CNNs**) and
    traditional **deep neural networks** (**DNNs**). These models treat text data
    as independent inputs; hence, they miss the interconnected nature and flow of
    language. For example, let’s take the sentence “*The cat, which is a mammal, likes
    to chase mice*." Humans immediately recognize the relationship between the cat
    and mice, as we process the entire sentence as a whole and not individual units.
  prefs: []
  type: TYPE_NORMAL
- en: A **recurrent neural network** (**RNN**) is a type of neural network designed
    to handle sequential data such as text and time-series data. When working with
    text data, RNNs’ memory enables them to recall earlier parts of a sequence, aiding
    them to understand the context in which words are used in a text. For example,
    with a sentence such as “*As an archaeologist, John loves discovering ancient
    artifacts,*” an RNN, in this context, can infer that an archaeologist will be
    excited about ancient artifacts, and in this sentence, the archaeologist is John.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will begin to explore the world of RNNs and take a look
    under the hood to understand how the inner mechanisms of RNNs work together to
    maintain a form of memory. We will explore the pros and cons of RNNs when working
    with text data, after which we will switch our attention to investigating its
    variants, such as **long short-term memory** (**LSTM**) and **gated recurrent
    units** (**GRUs**). Next, we will use the knowledge we have gained to build a
    multiclass text classifier. We will then explore the power of transfer learning
    in the field of **natural language processing** (**NLP**). Here, we will see how
    to apply pretrained word embeddings to our workflow. To close this chapter, we
    will use RNNs to build a child story generator; here, we will see RNNs in action
    as they generate text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The anatomy of RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text classification with RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLP with transfer learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text generation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding sequential data processing – from traditional neural networks
    to RNNs and LSTMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In traditional neural networks, as we discussed earlier in this book, we see
    an arrangement of densely interconnected neurons, devoid of any form of memory.
    When we feed a sequence of data to these networks, it’s an all-or-nothing transaction
    – the entire sequence is processed at once and converted into a singular vector
    representation. This approach is quite different from how humans process and comprehend
    text data. When we read, we naturally analyze text word by word, understanding
    that important words – those that have the power to shift the entire message of
    a sentence – can be positioned anywhere within it. For example, let's consider
    the sentence “*I loved the movie, despite some critics.*” Here, the word “*despite*”
    is pivotal, altering the direction of the sentiment expressed in the sentence.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs don’t just consider the value of individual words through embeddings; they
    also take into account the sequence or relative order of these words. This ordering
    of words gives them meaning and allows humans to effectively communicate with
    one another. RNNs are unique in their ability to retain context from one timestamp
    (or one word in the case of a sentence) to the next, thereby preserving the sequential
    coherence of the input. For example, in the sentence “*I visited Rome last year,
    and I found the Colosseum fascinating,*” an RNN would understand that “*the Colosseum*”
    relates to “*Rome*” because of the sequence of words. However, there’s a catch
    – this context retention can fail in longer sentences where the distance between
    related words increases.
  prefs: []
  type: TYPE_NORMAL
- en: This is precisely where gated variants of RNNs such as LSTM networks come in.
    LSTMs are designed with a special “cell state” architecture that enables them
    to manage and retain information over longer sequences. So, even in a lengthy
    sentence such as “*I visited Rome last year, experienced the rich culture, enjoyed
    the delicious food, met wonderful people, and I found the Colosseum fascinating,*”
    an LSTM could still link “*the Colosseum*” with “*Rome*,” understanding the broader
    context despite the length and complexity of the sentence. We have only scratched
    the surface. Let’s now examine the anatomy of these powerful networks. We will
    begin with RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: The anatomy of RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we talked about RNNs’ ability to handle sequential
    data; let’s drill down into how an RNN does this. The key differentiator between
    RNNs and feed-forward networks is their internal memory, as shown in *Figure 11**.1*,
    which enables RNNs to process input sequences while retaining information from
    previous steps. This attribute empowers RNNs to suitably exploit the temporal
    dependencies in sequences such as text data.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – The anatomy of an RNN](img/B18118_11_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – The anatomy of an RNN
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 11**.2* shows a clearer picture of an RNN and its inner workings. Here,
    we can see a series of interconnected units through which data flows in a sequential
    fashion, one element at a time. As each unit processes the input data, it sends
    the output to the next unit in a similar fashion to how feed-forward networks
    work. The key difference lies in the feedback loop, which equips RNNs with the
    memory of previous inputs, empowering them with the ability to comprehend entire
    sequences.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – An expanded view of an RNN showing its operation across multiple
    timesteps](img/B18118_11_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – An expanded view of an RNN showing its operation across multiple
    timesteps
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine we are dealing with sentences, and we want our RNN to learn something
    about their grammar. Each word in the sentence represents a time step, and at
    each one, the RNN considers the current word and also the “context” from the previous
    words (or steps). Let’s go over a sample sentence. Let’s say we have a sentence
    with five words – “*Barcelona is a nice city.*” This sentence has five time steps,
    one for each word. At time step 1, we feed the word “*Barcelona*” into our RNN.
    The network learns something about this word (in reality, it would learn from
    the word’s vector representation), produces an output, and also a hidden state
    capturing what it has learned, as illustrated in *Figure 11**.2*. Now, we unroll
    the RNN to timestep 2\. We input the next word, “*is*,” into our network, but
    we also input the hidden state from timestep 1\. This hidden state represents
    the “memory” of the network, allowing the network to take into account what it
    has seen so far. The network produces a new output and a new hidden state.
  prefs: []
  type: TYPE_NORMAL
- en: This process continues, with the RNN unrolling further for each word in the
    sentence. At each time step, the network takes the current word and the hidden
    state from the previous time step as input, producing an output and a new hidden
    state. When you “unroll” an RNN in this way, it may look like a deep feed-forward
    network with shared weights across each layer (since each timestep uses the same
    underlying RNN cell for its operations), but it’s more accurately a single network
    that’s being applied to each timestep, passing along the hidden state as it goes.
    RNNs have the ability to learn from and remember sequences of arbitrary lengths;
    however, they do have their own limitations. One key issue with RNNs is their
    struggle with capturing long-term dependencies due to the vanishing gradient problem.
    This happens because the influence of time steps over future time steps can diminish
    over long sequences, as a result of repeated multiplications during backpropagation,
    which makes the gradients exceedingly small and thus harder to learn from. To
    address this, we will apply more advanced versions of RNNs such as LSTM and GRU
    networks. These architectures apply gating mechanisms to control the flow of information
    in the network and make it easier for the model to learn long-term dependencies.
    Let’s examine these variants of RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Variants of RNNs – LSTM and GRU
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s imagine we are working on a movie review classification project, and
    while inspecting our dataset, we find a sentence such as this one: “*The movie
    started off boring and slow, but it really picked up toward the end, and the climax
    was amazing.*” By examining the sentence, we see that the initial set of words
    used by the reviewer portrays a negative sentiment by using words such as “slow”
    and “boring,” but the sentiment takes a shift to a more positive one with the
    use of phrases such as “picked up” and “climax was amazing.” If we use a simple
    RNN for this task, due to its inherent limitation to retain information over longer
    sequences, it may misclassify the sentence by attaching undue importance to the
    earlier negative tone of the review.'
  prefs: []
  type: TYPE_NORMAL
- en: Conversely, LSTMs and GRUs are designed to handle long-term dependencies, which
    makes them effective in not just capturing the change in sentiment but also for
    other NLP tasks, such as machine translation, text summarization, and question
    answering, where they outshine their simpler counterparts.
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LSTMs are a specialized type of RNN designed to address the vanishing gradient
    problem, enabling LSTMs to effectively handle long-term dependencies in sequential
    data. To resolve this issue LSTM introduced a new structure called a memory cell,
    which essentially acts as an information carrier with the ability to preserve
    information over an extended period. Unlike standard RNNs, which feed information
    from one step to the next and tend to lose it over time, an LSTM with the aid
    of its memory cell can store and retrieve information from any point in the input
    sequence, irrespective of its length. Let’s examine how an LSTM decides what information
    it will store in its memory cell. An LSTM is made up of four main components,
    as shown in *Figure 10**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![10.3 – LSTM architecture](img/B18118_11_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 10.3 – LSTM architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'These components allow LSTMs to store and access information over long sequences.
    Let’s look at each of the components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input gate**: The input gate decides what new information will be stored
    in the memory cell. It is made up of a sigmoid and a tanh layer. The sigmoid layer
    produces output values between zero and one, representing the importance level
    of each value in the input, where zero means “not important at all” and one represents
    “very important.” The tanh layer generates a set of candidate values that can
    be added to the state, essentially suggesting what new information should be stored
    in the memory cell. The outputs of both layers are merged by performing element-wise
    multiplication. This element-wise operation produces an input modulation gate
    that effectively filters the new candidate values, by deciding which information
    is important enough to be stored in the memory cell.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forget gate**: This gate decides what information will be retained and what
    information will be discarded. It uses a sigmoid layer to return output values
    between zero and one. If a unit in the forget gate returns an output value close
    to zero, the LSTM will remove the information in the corresponding unit of the
    cell state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output gate**: The output gate determines what the next hidden state should
    be. Like the other gates, it also uses a sigmoid function to decide which parts
    of the cell state make it to the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cell state**: The cell state is the “memory” of the LSTM cell. It is updated
    based on the output from the forget and input gates. It can remember information
    for use later in the sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gate mechanisms are paramount because they allow the LSTM to automatically
    learn appropriate context-dependent ways to read, write, and reset cells in the
    memory. These capabilities enable LSTMs to handle longer sequences, making them
    particularly useful for many complex sequential tasks where standard RNNs fall
    short, due to their inability to handle long-term dependencies, such as machine
    translation, text generation, time-series prediction, and video analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Long Short-Term Memory (BiLSTM)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Bidirectional Long Short-Term Memory** (**BiLSTM**) is an extension of traditional
    LSTM networks. However, unlike LSTMs, which process information in a sequential
    fashion from start to finish, BiLSTMs run two LSTMs simultaneously – one processes
    sequential data from the start to the end and the other from the end to the start,
    as illustrated in *Figure 11**.4*.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – The information flow in a BiLSTM](img/B18118_11_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – The information flow in a BiLSTM
  prefs: []
  type: TYPE_NORMAL
- en: By doing this, BiLSTMs can capture both the past and future context of each
    data point in the sequence. Because of BiLSTMs’ ability to comprehend the context
    from both directions in a sequence of data, they are well suited for tasks such
    as text generation, text classification, sentiment analysis, and machine translation.
    Now, let's examine GRUs.
  prefs: []
  type: TYPE_NORMAL
- en: GRUs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2014, *Cho et al.* introduced the **GRU** architecture as a viable alternative
    to LSTMs. GRUs were designed to achieve two primary goals – one was to overcome
    the vanishing gradient issues that plagued traditional RNNs, and the other was
    to streamline LSTM architecture for increased computational efficiency while maintaining
    the ability to model long-term dependencies. Structurally, the GRU has two primary
    gates, as illustrated in *Figure 11**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – A GRU’s architecture](img/B18118_11_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – A GRU’s architecture
  prefs: []
  type: TYPE_NORMAL
- en: One key difference between the GRU and LSTM is the absence of a separate cell
    state in GRUs; instead, they use a hidden state to transfer and manipulate information
    as well as streamline its computational needs.
  prefs: []
  type: TYPE_NORMAL
- en: 'GRUs have two main gates, the update gate and the reset gate. Let’s examine
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Update gate**: The update gate condenses the input and forget gates in LSTMs.
    It determines how much of the past information needs to be carried forward to
    the current state and which information needs to be discarded.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reset gate**: This gate defines how much of the past information should be
    forgotten. It helps the model evaluate the relative importance of new input against
    past memory.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Along with these two primary gates, a GRU introduces a “candidate hidden state.”
    This candidate hidden state combines the new input and the previous hidden state,
    and by doing so, it develops a preliminary version of the hidden state for the
    current time step. This candidate then plays an important role in determining
    the final hidden state, ensuring that the GRU retains relevant context from the
    past while accommodating new information. When deciding between LSTMs and GRUs,
    the choice is largely dependent on the specific application and the computational
    resources available. For some applications, the increased computational efficiency
    of GRU is more appealing. For example, in real-time processing, such as text-to-speech,
    or when working on tasks with short sequences, such as sentiment analysis of tweets,
    GRUs could prove to be an excellent choice in comparison to LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: We have provided a high-level discussion on RNNs and their variants. Let’s now
    proceed to apply these new architectures to a real-world use case. Will they outperform
    the standard DNNs or CNNs? Let’s find out in a text classification case study.
  prefs: []
  type: TYPE_NORMAL
- en: "Text classification using the AG News dataset – \La comparative study"
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The AG News dataset is a collection of more than 1 million news articles, collected
    from over 2,000 news sources by a news search engine called ComeToMyHead. The
    dataset is distributed across four categories – namely, world, sports, business,
    and science and technology – and it is available on **TensorFlow Datasets** (**TFDS**).
    The dataset is made up of 120,000 training samples (30,000 from each category),
    and the test set contains 7,600 examples.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This experiment may take about an hour to run, due to the size of the dataset
    and the number of models; hence, it is important to ensure your notebook is GPU-enabled.
    Again, you could take a smaller subset to ensure your experiments run much faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start building our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by loading the necessary libraries for this experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These imports form the building blocks, enabling us to solve this text classification
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we load the AG News dataset from TFDS:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use this code to load our dataset from TFDS – the `tfds.load` function fetches
    and loads the AG News dataset. We set the `with_info` argument to `True`; this
    ensures the metadata of our dataset, such as the total number of samples and the
    version, is also collected. This metadata information is stored in the `info`
    variable. We also set `as_supervised` to `True`; we do this to ensure that data
    is loaded in input and label pairs, where the input is the news article and the
    label is the corresponding category. Then, we split the data into a training set
    and a test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need to prepare our data for modeling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we perform data preparatory steps such as tokenization, sequencing, and
    padding, using TensorFlow’s Keras API. We initialize the tokenizer, which we use
    to convert our data from text to a sequence of integers. We set the `num_words`
    parameter to `20000`. This means we will only consider the top 20,000 occurring
    words in our dataset for tokenization; less frequently occurring words below this
    limit will be ignored. We set the `oov_token="<OOV>"` parameter to ensure we cater
    to unseen words that we may encounter during model inferencing.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we extract the training data and store it in the `train_texts` variable.
    We tokenize and transform our data into a sequence of integers by mapping numerical
    values to tokens, using the `fit_on_texts` and `texts_to_sequences()` methods
    respectively. We apply padding to each sequence to ensure that the data we will
    input into our models is of a consistent shape. We set `padding` to `post`; this
    will ensure padding is applied at the end of a sequence. We now have our data
    in a well-structured format, which we will feed into our deep-learning models
    for text classification shortly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start modeling, we want to split our data into training and validation
    sets. We do this by splitting our training set into 80 percent for training and
    20 percent for validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We convert our labels to one-hot encoding vectors, after which we split our
    training data using the `train_test_split` function from scikit-learn. We set
    our `test_size` to `0.2`; this means we will have 80 percent of our data for training
    and the remaining 20 percent for validation purposes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s set the `vocab_size`, `embedding_dim`, and `max_length` parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We set `vocab_size` and `embedding_dim` to `20000` and `64`, respectively. When
    selecting your `vocab_size`, it is important to strike a good balance between
    computation efficiency, model complexity, and the ability to capture language
    nuances, while we use our embedding dimension to represent each word in our vocabulary
    by a 64-dimensional vector. The `max_length` parameter is set to match the longest
    tokenized and padded sequence in our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin building our models, starting with a DNN:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using the `Sequential` API from TensorFlow, we build a DNN made up of an embedding
    layer, a flatten layer, two hidden layers, and an output layer for multiclass
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we build a CNN architecture:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use a `Conv1D` layer made up of 128 filters (feature detectors) and a kernel
    size of `5`; this means it will consider five words at a time. Our architecture
    uses `GlobalMaxPooling1D` to downsample the output of the convolutional layer
    to the most significant features. We feed the output of the pooling layer into
    a fully connected layer for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we build an LSTM model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our LSTM architecture is made up of two LSTM layers made up of 32 units each.
    In the first LSTM layer, we set `return_sequences` to `True`; this allows the
    first LSTM layer to pass the complete sequence it received as output to the next
    LSTM layer. The idea here is to allow the second LSTM layer access to the context
    of the entire sequence; this equips it with the ability to better understand and
    capture dependencies across the entire sequence. We then feed the output of the
    second LSTM layer into the fully connected layers to classify our data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our final model, let’s use a bidirectional LSTM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, instead of the LSTM layers, two bidirectional LSTM layers are added. Note
    that the first layer also has `return_sequences=True` to return the full outputs
    to the next layer. Using a bidirectional wrapper allows each LSTM layer access
    to both past and future context when processing each element of the input sequence,
    providing additional contextual information when compared with a unidirectional
    LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking BiLSTM layers can help us build higher-level representations of the
    full sequence. The first BiLSTM extracts features by looking at the text from
    both directions while preserving the entire sequence. The second BiLSTM can then
    build on those features by further processing them. The final classification is
    carried out by the output layer in the fully connected layer. Our experimental
    models are now all set up, so let’s proceed with compiling and fitting them next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s compile and fit all the models we have built so far:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use a `for` loop to compile and fit all four models. We set `verbose` to
    `False`; this way, we don’t print the training information. We train for 10 epochs.
    Do expect this step to take a while, as we have a massive dataset and are experimenting
    with four models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s evaluate our model on unseen data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To evaluate our model, we need to prepare our test data in the right fashion.
    We first extract our text data from our `test_dataset`, after which we tokenize
    the text using the tokenizer from our training process. The tokenized text is
    then converted into a sequence of integers, and padding is applied to ensure all
    sequences are of the same length as the longest sequence in our training data.
    Just like we did during training, we also one-hot-encode our test labels, and
    then we apply a `for` loop to iterate over each individual model, generating the
    test loss and accuracy for all our models. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: From our returned results, we can see that our LSTM model achieved the highest
    accuracy (90.08%); other models performed quite well too. We can take this performance
    as a good starting point; we can also apply some of the ideas we used in [*Chapter
    8*](B18118_08.xhtml#_idTextAnchor186)*, Handling Overfitting,* and [*Chapter 10*](B18118_10.xhtml#_idTextAnchor226)*,
    Introduction to Natural Language Processing,* to improve our results here.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B18118_10.xhtml#_idTextAnchor226), *Introduction to Natural
    Language Processing,* we talked about pretrained embeddings. These embeddings
    are trained on a large corpus of text data. Let’s see how we can leverage them;
    perhaps they can help us achieve a better result in this case.
  prefs: []
  type: TYPE_NORMAL
- en: Using pretrained embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 9*](B18118_09.xhtml#_idTextAnchor210), *Transfer Learning,* we
    explored the concept of transfer learning. Here, we will revisit this concept
    as it relates to word embeddings. In all the models we have built up so far, we
    trained our word embeddings from scratch. Now, we will examine how to leverage
    pretrained embeddings that have been trained on massive amounts of text data,
    such as Word2Vec, GloVe, and FastText. Using these embeddings can be advantageous
    for two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, they are already trained on a massive and diverse set of data, so they
    have a rich understanding of language.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secondly, the training process is much faster, since we will skip training our
    own word embeddings from scratch. Instead, we can build our models on the information
    packed in these embeddings, focusing on the task at hand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that using pretrained embeddings isn’t always the right
    choice. For example, if you work on niche-based text data such as medical or legal
    data, sectors that apply a lot of domain-specific terminology may be underrepresented.
    When we use a pretrained embedding blindly for these use cases, they may lead
    to suboptimal performance. In these types of scenarios, you can either train your
    own embedding, which comes with an increased computational cost, or use a more
    balanced approach and fine-tune pretrained embeddings on your data. Let’s see
    how we can apply pretrained embeddings in our workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Text classification using pretrained embedding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To follow this experiment, you will need to use the second notebook in this
    chapter’s GitHub repository called `modelling with pretrained embeddings`. We
    will continue with the same dataset. This time, we will focus on using our best
    model with the GloVe pretrained embedding. We will use our best model (LSTM) from
    our initial round of experiments. Let’s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by importing the necessary libraries for this experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we import our libraries, we will download our pretrained embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands to download the pretrained embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will download the GloVe 6B embedding files from the Stanford NLP website
    into our Colab notebooks, using the `wget` command. Then, we unzip the compressed
    files. We can see that this file contains different pretrained embeddings, as
    shown in *Figure 11**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.6 – The directory displaying the GloVe 6B embeddings files](img/B18118_11_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – The directory displaying the GloVe 6B embeddings files
  prefs: []
  type: TYPE_NORMAL
- en: The GloVe 6B embedding is made up of a 6 billion-token word embedding, trained
    by researchers at Stanford University and made publicly available to us all. For
    computational reasons, we will use the 50-dimensional vectors. You may wish to
    try out higher dimensions for richer representations, especially when working
    on tasks that require you to capture more complex semantic relationships, but
    also be mindful of the compute required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we load the AG News dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We load our dataset and split it into training and testing sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we tokenize and sequence our training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We prepare our data for modeling, just like our last experiment. We set the
    vocabulary size to 2,000 words and use `OOV` for out-of-vocabulary words. Then,
    we tokenize and pad the data to ensure consistency in the length of it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we process our test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We process our testing data in a similar fashion to our training data. However,
    it is important to note that we do not apply `fit_on_texts` to our test set, ensuring
    that the tokenizer remains the same as the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we set the embedding parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We define the size of our vocabulary and we set the dimensionality of our embeddings
    to `50`. We use this because we are working with 50d pretrained embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the pretrained word embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We access the `glove.6B.50d.txt` file and read it line by line. Each line contains
    a word and its corresponding embeddings. We cross-match words in the GloVe file
    with those in our own vocabulary, constructed with the Keras tokenizer. If there
    is a match, we take the corresponding word index from our own vocabulary and update
    our initially zero-initialized embedding matrix at that index with the GloVe embeddings.
    Conversely, words that do not match will remain as zero vectors in the matrix.
    We will use this embedding matrix to initialize the weight of our embedding layer
    shortly. We use the file path to the 50d embeddings, as shown in *Figure 11**.6*.
    You can do this by right-clicking on the specified file and copying the file path.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build, compile, and train our LSTM model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE139]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: While building our model, the key difference from our previous experiment is
    the embedding layer initialization. Here, we leverage our pretrained embedding
    matrix, and to ensure the weights remain unchanged, we set the trainable parameter
    to `false`. Everything else in our model’s architecture remains the same. Then,
    we compile and fit our model for 10 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we evaluate our model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We reached an accuracy of 89% on our test set, although we did not outperform
    our best model when we didn’t apply pretrained embeddings. Perhaps you may want
    to try out larger embedding dimensions from `glove6B` or other word embeddings
    to improve our result here. That would be a good exercise and is well encouraged.
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to move on to another exciting topic – text generation with
    LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: Using LSTMs to generate text
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have explored LSTMs in text classification. Now, we will look at how to generate
    text that you would see in a novel, blog post, or children’s storybook, ensuring
    that is coherent and consistent with what we expect from these types of texts.
    LSTMs prove useful here, due to their ability to capture and remember intricate
    patterns for long sequences. When we train an LSTM on a large volume of text data,
    we allow it to learn the linguistic structure, style, and nuances. It can apply
    this to generate new sentences in line with the style and approach of the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s imagine we are playing a word prediction game with our friends. The goal
    is to coin a story in which each friend comes up with a word to continue the story.
    To begin, we have a set of words, which we will call the seed, to set the tone
    for our story. From the seed sentence, each friend contributes a subsequent word
    until we have a complete story. We can also apply this idea to LSTMs – we feed
    a seed sentence into our model, and then we ask it to predict the next word, just
    like our word prediction game. This time, however, the game is played only by
    the LSTM, and we just need to specify the number of words it will play for. The
    result of each round of the game will serve as input to the LSTM for the next
    round, until we achieve our specified number of words.
  prefs: []
  type: TYPE_NORMAL
- en: 'The question that comes to mind is, how does an LSTM know what word to predict
    next? This is where the concept of **windowing** comes into play. Let’s say we
    have a sample sentence such as “*I once had a dog called Jack.*” When we apply
    windowing with a window size of four to this sentence, we will have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: “*I once* *had a*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “*once had* *a dog*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “*had a* *dog called*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: “*a dog* *called Jack*”
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can now split each of these sentences into input-output pairs. For example,
    in the first sentence, we will have “*I once had*” as the input, and the output
    will be “*a*.” We will apply the same approach to all the other sentences, thus
    giving us the following input-output pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: ([“*I*”, “*once*”, “*had*”], “*a*”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([“*once*”, “*had*”, “*a*”], “*dog*”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([“*had*”, “*a*”, “*dog*”], “*called*”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ([“*a*”, “*dog*”, “*called*”], “*Jack*”)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By using windowing, our LSTM focuses on the most recent set of words, which
    often holds the most relevant information to predict the next word in our text.
    Also, when we work with smaller, fixed-length sequences, it streamlines our training
    process and also optimizes our memory usage. Let’s see how we can apply this idea
    in our next case study.
  prefs: []
  type: TYPE_NORMAL
- en: Story generation using LSTMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this case study, imagine you are a new NLP engineer working for a London-based
    start-up called Readrly. Your job is to build an AI storyteller for the company.
    You have been provided with a training dataset called `stories.txt`, which contains
    30 sample stories. Your job is to train an LSTM to generate exciting children’s
    stories. Let’s return to our notebook and see how to make this happen:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we did previously, we will begin by importing all the libraries required
    for this task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we load the `stories.txt` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We read our story file and convert all the contents to lowercase to ensure our
    data is in a consistent form, avoiding duplicate tokens generated from capitalized
    and non-capitalized versions of the same word.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This step helps us to reduce the vocabulary size by removing semantic nuances
    created by capitalization. However, this step should be carried out judiciously,
    as it may have a negative effect – for example, when we use the word “*march*.”
    If we lowercase the word, it will denote some form of walking, while with a capital
    M, it refers to the month of the year.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tokenize the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We calculate the total number of unique words in our vocabulary. We add one
    to account for out-of-vocabulary words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we convert the text to sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this step, we develop a dataset made up of *n*-gram sequences, where each
    entry in the “input sequence” is a sequence of words (that is, word numbers) that
    appear in the text. For every sequence of *n* words, *n*-1 words will be our input
    features, and the *n*-th word is the label that our model tries to predict.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: An n-gram is a sequence of n words from a given text.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let’s say we have a sample sentence such as “*The dog played with
    the cat.*” Using a 2-gram (bigram) model on a sentence would split it into the
    following bigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: (“*The*”, “*dog*”), (“*dog*”, “*played*”), (“*played*”, “*with*”), (“*with*”,
    “*the*”), (“*the*”, “*cat*”)
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, if we use a 3-gram (trigram) model, it would split the sentence
    into trigrams:'
  prefs: []
  type: TYPE_NORMAL
- en: (“*The*”, “*dog*”, “*played*”), (“*dog*”, “*played*”, “*with*”), (“*played*”,
    “*with*”, “*the*”), (“*with*”, “*the*”, “*cat*”)
  prefs: []
  type: TYPE_NORMAL
- en: '*N*-gram models are common in NLP for text prediction, spelling correction,
    language modeling, and feature extraction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we pad the sequences:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We use padding to ensure our input data is of a consistent format. We set `padding`
    to `pre` to ensure that our LSTM captures the most recent words at the end of
    each sequence; these words are relevant to predict the next word in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now split the sequences into features and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, we one-hot-encode our labels to represent them as vectors. Then, we build
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We build a text generation model using a bidirectional LSTM, as it has the ability
    to capture both past and future data points. Our embedding layer transforms each
    word’s numerical representation into a dense vector, with a dimension of 200 each.
    The next layer is the bidirectional LSTM layer with 200 units, after which we
    feed the output data into the dense layers. Then, we compile and fit the model
    for 300 epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function to make the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE210]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We construct the `story_generator` function. We start with the seed text, which
    we use to prompt the model to generate more text for our children’s stories. The
    seed text is transformed into its tokenized form, after which it is padded at
    the beginning to match the expected length of the input sequence, `max_sequence_len-1`.
    To predict the next word’s token, we use the `predict` method and apply `np.argmax`
    to select the most likely next word. The predicted token is then mapped back to
    its corresponding word, which is appended to the existing seed text. The process
    is repeated until we achieve the desired number of words (`next_words`), and the
    function returns the fully generated text (`seed_text +` `next_words`)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s generate the text:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the seed text, which is the `input_text` variable here. Our choice
    of next words we want the model to generate is `50`, and we pass in our trained
    model and also `max_sequence_len`. When we run the code, it returns the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: The sample of generated text indeed resembles something you might find in a
    children’s storybook. It continues the initial prompt coherently and creatively.
    While this example showcases the power of LSTMs as text generators, in this era,
    we leverage **large language models** (**LLMs**) for such applications. These
    models are trained on massive datasets, and they have a more sophisticated understanding
    of language; hence, they can generate more compelling stories when we prompt or
    fine-tune them properly.
  prefs: []
  type: TYPE_NORMAL
- en: We are now at the end of this chapter on NLP. You should now have the requisite
    foundational ideas to build your own NLP projects with TensorFlow. All you have
    learned here will also help you effectively navigate the NLP section of the TensorFlow
    Developer certificate exam. You have come a long way, and you should give yourself
    credit. We have one more chapter to go. Before we proceed to the chapter on time
    series, let's summarize what we learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we embarked on a voyage through the world of RNNs. We began
    by looking at the anatomy of RNNs and their variants, and we explored the task
    of classifying news articles using different model architectures. We took a step
    further by applying pretrained word embeddings to our best-performing model in
    our quest to improve it. Here, we learned how to apply pretrained word embeddings
    in our workflow. For our final challenge, we took on the task of building a text
    generator to generate children’s stories.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will examine time series, explore its unique characteristics,
    and uncover various methods of building forecasting models. We will tackle a time-series
    problem, where we will master how to prepare, train, and evaluate time-series
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s test what we learned in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Load the IMDB movies review dataset from TensorFlow and preprocess it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a CNN movie classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build an LSTM movie classifier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the GloVe 6B embedding to apply a pretrained word embedding to LSTM architecture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluate all the models, and save your best-performing one.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more, you can check out the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: Cho, K., et al. (2014). *Learning Phrase Representations using RNN Encoder–Decoder
    for Statistical Machine Translation*. arXiv preprint arXiv:1406.1078.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter, S., & Schmidhuber, J. (1997). *Long short-term memory*. Neural computation,
    9(8), 1735–1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). *Efficient Estimation
    of Word Representations in Vector Space*. arXiv preprint arXiv:1301.3781.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*The Unreasonable Effectiveness of Recurrent Neural* *Networks*: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Part 4 – Time Series with TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part, you will learn to build time series forecasting applications with
    TensorFlow. You will understand how to perform preprocess and build models for
    time series data. In this part, you will also learn to generate forecast for time
    series.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B18118_12.xhtml#_idTextAnchor291), *Introduction to Time Series,
    Sequences, and Predictions*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B18118_13.xhtml#_idTextAnchor318), *Time Series, Sequence and
    Prediction with TensorFlow*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
