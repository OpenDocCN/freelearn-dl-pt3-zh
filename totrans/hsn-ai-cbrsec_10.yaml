- en: Fraud Prevention with Cloud AI Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The objective of many security attacks and data breaches that corporations suffer
    from is the violation of sensitive information, such as customers' credit card
    details. Such attacks are often conducted in stealth mode, and so it is difficult
    to detect such threats using traditional methods. In addition, the amount of data
    to be monitored often assumes dimensions that cannot be effectively analyzed with
    just traditional **extract, transform, and load** (**ETL**) procedures that are
    executed on relational databases, which is why it is important to adopt **artificial
    intelligence** (**AI**) solutions that are scalable. By doing this, companies
    can take advantage of cloud architectures in order to manage big data and leverage
    predictive analytics methodology.
  prefs: []
  type: TYPE_NORMAL
- en: Credit card fraud represents an important test for the application of AI solutions
    in the field of cybersecurity since it requires the development of predictive
    analytics models that exploit big data analytics through the use of cloud computing
    platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to leverage **machine learning** (**ML**) algorithms for fraud detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How bagging and boosting techniques can improve an algorithm's effectiveness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to analyze data with IBM Watson and Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to resort to statistical metrics for results evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's introduce the role that's played by algorithms in credit card fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing fraud detection algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, we have witnessed an increase in fraudulent activities in the
    financial sector, and particularly in the area of ​​credit card frauds. This is
    due to the fact that it is rather easy for cybercriminals to set up credit card
    fraud, and it has, therefore, become important for financial institutions and
    organizations to be able to promptly identify fraud attempts.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the activity of fraud detection and prevention in the context of
    credit card fraud is complicated by the fact that this type of fraud assumes global
    characteristics; that is, it involves different geographical areas as well as
    a variety of financial institutions and organizations.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is essential to be able to share the information sources that
    are available within different organizations around the world.
  prefs: []
  type: TYPE_NORMAL
- en: These sources of information are heterogeneous and characterized by explosive
    growth in data generations, which need to be analyzed in real time.
  prefs: []
  type: TYPE_NORMAL
- en: This resembles a typical big data analytics scenario, which requires analysis
    tools and appropriate software and hardware platforms, such as those offered by
    cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: The complexity of the scenario is aggravated by the fact that we are more likely
    than ever to find money laundering and illegal activities, such as international
    terrorism financing, to be associated with credit card fraud.
  prefs: []
  type: TYPE_NORMAL
- en: Illicit activities that are conducted by cybercriminals, therefore, takes on
    a transnational dimension that involves different sectors of organized crime.
  prefs: []
  type: TYPE_NORMAL
- en: All organizations, both in public and private sectors, are called upon to cooperate
    and counter these illicit activities on the basis of regulatory laws such as anti-money
    laundering legislation.
  prefs: []
  type: TYPE_NORMAL
- en: The growing interest of cybercriminals toward credit card fraud is due to distorted
    economic incentives; the expected payout of credit card fraud is considerably
    higher than alternative illegal activities, combined with the fact that the risk
    of being caught by the police is much lower than other forms of traditional crimes.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, if individual financial fraud involves amounts of money and values
    ​​that do not exceed certain thresholds, financial institutions themselves are
    discouraged from pursuing illegal activities because investigation activities
    can prove to be uneconomical (just think, for example, of fraud that's  carried
    out through fake e-commerce websites located in different countries and geographic
    areas, which entail the need for investigative activities involving different
    legal jurisdictions, with an increase in costs and implementation times of law
    enforcement).
  prefs: []
  type: TYPE_NORMAL
- en: Financial losses due to credit card fraud are not the only problem that financial
    institutions must face; there are also reputational damages that are caused by
    the loss of credibility and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, credit card fraud can also be a threat to customers; one of the
    most disturbing aspects of credit card fraud is related to the growing phenomenon
    of identity theft, which can be easily achieved by creating counterfeit documents
    or through the appropriation of digital copies of identity documents (found, for
    example, through data breaches, phishing emails, and other sources).
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with credit card fraud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'However, in light of the preceding discussion, financial institutions have
    introduced fraud prevention measures over time: in fact, financial institutions
    have introduced security measures based on two-factor authentication, which integrates
    traditional authentication procedures by sending an OTP code via SMS to the customer''s
    mobile phone number to prevent abuse in the use of payment instruments.'
  prefs: []
  type: TYPE_NORMAL
- en: However, the fact remains that such measures are not sufficient and the monetary
    losses that financial institutions suffer as a result of credit card frauds are
    still in the order of billions of dollars; therefore, the most effective prevention
    activities to reduce these losses are procedures based on fraud detection and
    prevention.
  prefs: []
  type: TYPE_NORMAL
- en: The field of analysis associated with credit card fraud detection and prevention
    is rather complex and will offer us the opportunity to see, in action, different
    analysis approaches that make use of the techniques of predictive analytics, ML,
    and big data analytics.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look the advantages of using cloud computing platforms
    (using the tools provided by the IBM Watson platform) in light of the fact that
    fraud detection and prevention requires the integration of different activity
    analysis, as well as the integration of heterogeneous data sources.
  prefs: []
  type: TYPE_NORMAL
- en: This will lead us to the adoption of a detection approach that leverages predictive
    analytics, including innovative approaches such as cognitive computing.
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning for fraud detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The introduction of algorithmic procedures for fraud detection in the credit
    card sector represents an important test bench in the field of predictive analytics
    (as we will see shortly). Among the first examples of scientific research that
    were conducted in this field, we must mention *Adaptive Machine Learning for Credit
    Card Fraud Detection* by Andrea Dal Pozzolo available at [https://dalpozz.github.io/static/pdf/Dalpozzolo2015PhD.pdf](https://dalpozz.github.io/static/pdf/Dalpozzolo2015PhD.pdf)),
    one of the most thorough pieces of scientific research, which widely exposed how
    to effectively leverage ML algorithms in credit card fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice and design of appropriate algorithms for credit card fraud detection
    are characterized by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Data concerning fraud transactions is not commonly available as financial institutions
    are reluctant to disseminate such information for fear of reputational damage,
    as well as confidentiality compliance requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From a technical point of view, the data on fraud usually represents non-stationary
    distributions, that is to say, they undergo changes over time; this is also due
    to the change in customers' spending behaviors.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transaction distributions are heavily unbalanced as fraud usually represents
    a small percentage of overall transactions; therefore, the distributions show
    a high skewness toward genuine transactions. In fact, we are usually only able
    to measure fraud that has actually been detected, while it is much more difficult
    to estimate the number of fraud instances that haven't been detected at all (false
    negatives). Furthermore, fraud is usually recorded long after it actually occurred.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These intrinsic characteristics of misrepresentations concerning fraud transactions
    result in challenges in the selection and design of detection and prevention algorithms,
    such as:'
  prefs: []
  type: TYPE_NORMAL
- en: The use of sampling strategies in data analysis; in the presence of unbalanced
    distributions the choice of an undersampling/oversampling strategy can be more
    useful.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration of feedback generated by human operators in identifying fraud alerts.
    This aspect is particularly important for improving the learning process of algorithms
    in the presence of non-stationary data, which evolves over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All this translates into the development of a fraud detection and prevention
    system, able to integrate big data analytics, ML algorithms, and human operator's
    feedback. Therefore, it is clear that the use of cloud computing architectures
    is the obligatory implementation of choice.
  prefs: []
  type: TYPE_NORMAL
- en: Fraud detection and prevention systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are various possible credit card fraud scenarios, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theft of credit cards**: This is the most frequent case in practice; criminals
    steal or spend as much money as possible in a short time span. This activity is
    noisy and can be identified by means of anomalous or unusual pattern detection
    that''s carried out with respect to the spending habits of the legitimate credit
    card holder.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Credit card abuse**: Unlike the previous case, fraudsters don''t need to
    physically hold the credit card, but it is sufficient that they know the relevant
    information associated with the card (identification codes, PIN, personal identifier
    number, card number, device code, and so on). This is represented by one of the
    most insidious fraud scenarios as it is conducted in stealth mode (it isn''t noisy,
    compared to the previous scenario) and the legitimate owner of the card is often
    unaware of the ongoing fraud taking place behind his/her back.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identity theft**: In this case, the credit card is issued on the basis of
    false personal information, or by exploiting the personal information of unsuspecting
    third parties, who find themselves charged for service costs and withdrawals and
    payments that have been made in their name.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should bear in mind that fraud scenarios evolve over time in relation to
    process and product innovations concerning financial services and technologies
    that are adopted by financial institutions.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, fraudsters adapt their behavior based on the technical measures that
    are adopted by credit card issuers to prevent and combat fraud.
  prefs: []
  type: TYPE_NORMAL
- en: 'To correctly implement a **fraud detection and prevention system** (**FDPS**),
    it is necessary to distinguish between the two activities related to the management
    of credit card fraud:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fraud detection**: This constitutes the set of procedures aimed at correctly
    and reliably identifying cases of fraud; it is put in place after the fraud occurs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fraud prevention**: This constitutes the set of procedures aimed at effectively
    preventing the realization of the fraud; it is put in place before the fraud occurs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The two activities are characterized by the different types of procedures that
    are implemented, as well as by the timing with which they are introduced. These
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: In the case of fraud prevention, analysis procedures can exploit rule-based
    alarm systems that are processed by experts in the field (and, as such, require
    constant fine-tuning by human operators), or leverage advanced analysis techniques
    based on data mining, machine learning, neural networks, and more, through which
    it is possible to automatically discover the presence of patterns within the data
    distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of fraud detection, the analysis procedures are aimed at correctly
    classifying fraud based on the available data, thereby distinguishing it from
    genuine transactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An important aspect of the implementation of an FDPS is not only the reliability
    of the results that it allows us to achieve but also its cost-effectiveness. It
    wouldn't make sense to adopt an FDPS if the implementation costs proved to be
    greater than the losses suffered as a result of fraud!
  prefs: []
  type: TYPE_NORMAL
- en: There is an obvious trade-off between the two considered activities; in the
    event that an attempt at fraud cannot be prevented, then it must be detected as
    quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, the two activities share the need to minimize the number of
    false positives (that is, the number of transactions that are treated as fraudulent
    when, in reality, they are legitimate) and avoid the possible denial of service
    caused to the customer in consequence of automated reactions resulting from false
    positives (such as the automatic blocking of credit cards, despite the transactions
    being legitimate).
  prefs: []
  type: TYPE_NORMAL
- en: Compounding the management of false positives is the poor scalability of the
    checks carried out by human operators; if the use of controls carried out by human
    operators is often decisive in the correct identification of real fraud, systematically
    recurring human control of all transactions is, indeed, overkill.
  prefs: []
  type: TYPE_NORMAL
- en: This is why it has become crucial to correctly implement automated detection
    and prevention procedures to support the analysis carried out by the operators.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will see how to take the difficulties involved in managing
    large data, which are often unbalanced and subject to continuous changes due to
    customers' changing buying habits, into account, in terms of the algorithms that
    are available.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will examine the possible strategies that we can
    adopt in the implementation of automated predictive models, analyzing the differences
    existing between expert- and data-driven strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Expert-driven predictive models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The expert-driven approach consists of implementing predictive models based
    on rules that have been established by experts in the sector (not by chance that
    the expert-driven approach is also defined as a rule-based approach).
  prefs: []
  type: TYPE_NORMAL
- en: The rules follow logical conditions of the `if...then..else` form, and are aimed
    at representing the different fraud scenarios and the related countermeasures
    to be adopted automatically following the checks carried out on the transaction
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, a possible rule that identifies  all credit card transactions as
    fraudulent if they exceed a certain amount of money, related to purchases made
    with a certain daily frequency (also compared with the historical series resembling
    a customer''s buying habits), could be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the case of subsequent transactions that are executed in places that are
    geographically very distant from one another, it might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the first case, we will look at an example of a scoring rule, while in the
    second case, we will talk about a blocking rule.
  prefs: []
  type: TYPE_NORMAL
- en: The scoring rules are aimed to estimate the probability of fraud associated
    with a transaction based on rules of common experience, and also by classifying
    the events upon exceeding a specific threshold that's been assigned to the score.
  prefs: []
  type: TYPE_NORMAL
- en: The blocking rules are more restrictive as they do not limit themselves to estimating
    the probabilities of fraud. Instead they are aimed at denying the authorization
    of the transaction before it is completed; therefore, blocking rules must be based
    on more stringent logical conditions (as in our example, in which a transaction,
    issued in less than half an hour from the previous one, is denied if the distance
    between the places of execution is greater than 1,000 km. It is reasonable to
    presume that the same customer cannot physically move to places that are so distant
    from each other in such a short period of time).
  prefs: []
  type: TYPE_NORMAL
- en: 'The advantages associated with rule-based predictive models are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Ease of alerts implementation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ease of alerts understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greater alerts explicability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Equally obvious are the disadvantages of expert-driven predictive models:'
  prefs: []
  type: TYPE_NORMAL
- en: They express subjective judgments and may differ according to the experts who
    implement them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are able to handle only a few significant variables and their mutual correlations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They are based on past experiences and are not able to automatically identify
    new fraud patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A constant, manual fine-tuning of the rules needs to be carried out manually
    by the experts in order to take into account the evolution of the fraud strategies
    that are adopted by fraudsters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These disadvantages, therefore, favor the adoption of data-driven predictive
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Data-driven predictive models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data-driven predictive models exploit automated learning algorithms in an attempt
    to adapt their prediction based on data-driven learning approaches, constantly
    updating detection and prevention procedures, and based on dynamically identified
    behavior patterns.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithms that are used in data-driven predictive models are derived from
    distinct fields of quantitative analysis, starting from statistics, ending in
    data mining and ML, and having an objective of learning about hidden or latent
    patterns within the data.
  prefs: []
  type: TYPE_NORMAL
- en: The privileged role of ML algorithms in the implementation of data-driven predictive
    models is immediately evident; ML makes it possible to identify predictive models
    based on the training that's been performed on the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the use of ML in the field of fraud detection has several advantages:'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to analyze multidimensional datasets (characterized by a high number
    of features, representative of the possible explanatory variables of fraud)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to correlate the various identified features between them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to dynamically update models, adapting them to changes in strategies
    adopted by fraudsters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML adopts the data-driven approach, which makes use of large amounts of data
    (big data) in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In light of this, data-driven predictive models usually prove to be more robust
    and scalable than rule-based models.
  prefs: []
  type: TYPE_NORMAL
- en: However, unlike rule-based models, data-driven predictive models often behave
    like black boxes, meaning that the alerts they generate are difficult to interpret
    and justify (for example, in the face of requests for clarification that have
    been issued by customers whose transactions were denied based on automated decisions
    made by the algorithms).
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, the very nature of the data can lead to difficulties in the
    correct implementation of the algorithms; in the case of credit cards, transaction
    distributions present important irregularities, such as being unbalanced, non-stationary,
    and skewed. Therefore, it is necessary to carefully choose machine learning algorithms
    that are capable of adequately dealing with these irregularities.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of non-stationary data in particular (that is, data that changes
    its characteristics over time in relation to changes in customers' buying behaviors),
    the algorithms must carefully update their own learning parameters, weigh the
    most recent data, or neglect outdated samples.
  prefs: []
  type: TYPE_NORMAL
- en: An undoubted advantage of data-driven predictive models consists of the ability
    to integrate the operators' feedback within the predictions, thus improving the
    accuracy of the procedures.
  prefs: []
  type: TYPE_NORMAL
- en: The operator's feedback is, in fact, characterized by greater reliability in
    the correct classification of fraud cases, consequently reducing the number of
    false negatives (that is, frauds that may go undetected), and can be automatically
    integrated within data-driven predictive models.
  prefs: []
  type: TYPE_NORMAL
- en: Instead, rule-based models require manual revisions, to take account of operators'
    feedback.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to combine the advantages deriving from both expert-driven and data-driven
    predictive models constitutes the strength of the FDPS, as we will see shortly.
  prefs: []
  type: TYPE_NORMAL
- en: FDPS – the best of both worlds
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Expert-driven and data-driven predictive models can, therefore, be combined in
    an FDPS in order to exploit the benefits of both approaches to improve the accuracy
    of the forecasts by reducing both false negatives and false positives.
  prefs: []
  type: TYPE_NORMAL
- en: The rules-based models usually reduce the number of false negatives, though
    this is at the cost of an increase in false positives; in combination with data-driven
    models, it is possible to improve forecasts by reducing false positives.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, as we have seen, data-driven models allow operators' feedback to
    be integrated with other big data sources, thus contributing to dynamically updating
    the FDPS.
  prefs: []
  type: TYPE_NORMAL
- en: The FDPS automated maintenance and fine-tuning activities require the implementation
    of machine learning algorithms that can autonomously learn new forecasting patterns
    start from huge amounts of data.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw earlier, the statistical distributions related to credit card transactions
    are characterized by non-stationary data (which changes its characteristics in
    relation to changes in spending habits), which also tend to be skewed toward the
    bigger class of data that's representative of legitimate transactions rather than
    toward the smaller class representing fraud.
  prefs: []
  type: TYPE_NORMAL
- en: This is due to the fact that the number of fraud cases is minimal with respect
    to the total number of overall transactions (furthermore, the detection of fraud
    transactions often takes longer, so the class of fraud transactions is systematically
    smaller).
  prefs: []
  type: TYPE_NORMAL
- en: Not all ML algorithms can adequately manage data that simultaneously has the
    characteristic of being non-stationary and unbalanced. Due to this, it is necessary
    to adequately select algorithms to obtain reliable and precise predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Learning from unbalanced and non-stationary data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 1](752acbc8-bc15-44cc-ae83-d023265eeb84.xhtml), *Introduction to
    AI for Cybersecurity Professionals*, we saw how machine learning algorithms are
    divided into supervised and unsupervised learning; this subdivision is also valid
    in regards to credit card fraud detection, although attention must be paid to
    the different assumptions that inspire the two categories of algorithms. This
    is because they have important consequences on the reliability and accuracy of
    the predictions.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of supervised learning algorithms, it is assumed that a dataset
    of already categorized samples (labeled samples) is available; that is, each sample
    was previously associated with one of the two possible categories (legitimate
    or fraud).
  prefs: []
  type: TYPE_NORMAL
- en: The supervised algorithms are, therefore, trained on the basis of this information,
    and the predictions they make are conditioned by the previous categorization that
    was carried out on the training samples, which can lead to an increase in false
    negatives.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised algorithms, on the other hand, do not benefit from any previous
    information on the possible categorization of the sample data (unlabeled samples)
    and must, therefore, independently infer the possible classes of membership to
    be attributed to the data in order to generate false positives more easily.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with unbalanced datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of credit card transactions, we said that the distribution of data
    is both unbalanced and non-stationary.
  prefs: []
  type: TYPE_NORMAL
- en: A solution to the unbalanced data distribution problem consists of rebalancing
    the classes before proceeding with training the algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Among the strategies that are commonly used to rebalance the sample classes
    includes undersampling and oversampling the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In essence, undersampling consists of removing some observations that belongs
    to a certain class at random, in order to reduce its relative consistency.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of unbalanced distributions, such as those relating to transactions
    with credit cards, if we exclude random samples from the main class (which is
    representative of legitimate transactions), we can reasonably expect that the
    distribution of data will not change substantially due to the removal of data
    (which can be reliably considered redundant).
  prefs: []
  type: TYPE_NORMAL
- en: However, we can always incur the risk of eliminating data that contains relevant
    information. Therefore determining the correct sampling level is not always immediate
    as it depends on the specific characteristics of the dataset, and therefore requires
    the use of adaptive strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Another data sampling strategy consists of oversampling, that is to say, to
    increase the size of the smaller classes by generating synthetic samples within
    them.
  prefs: []
  type: TYPE_NORMAL
- en: The disadvantages associated with oversampling techniques consist of the risk
    of introducing overfitting, and of increasing the training time of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with non-stationary datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to manage the non-stationary characteristic of the distribution, it
    may be useful to overweigh the feedback that was obtained by human operators,
    which contributes to improving the classification of supervised samples.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in the presence of non-stationary data, it may be useful to use an
    ensemble of classifiers (ensemble learning), whose training is carried out on
    different samples, to improve the overall prediction accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: By integrating different classifiers, it is possible to combine the knowledge
    that was obtained on the basis of the new observations with the knowledge that
    was previously acquired, weighing each classifier on the basis of its classification
    capability, and excluding those classifiers that are no longer capable of representing
    changes in data distribution over time.
  prefs: []
  type: TYPE_NORMAL
- en: Predictive analytics for credit card fraud detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To adequately address the problem of fraud detection, it is necessary to develop
    predictive analytics models, that is, mathematical models that can identify trends
    within the data, using a data-driven approach.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike descriptive analytics (whose paradigm is constituted by **business intelligence**
    (**BI**)), which limits itself to classifying the past data on the basis of measures
    deriving from the application of descriptive statistics (such as sums, averages,
    variances, and so on), precisely describe the characteristics of the data being
    analyzed; instead, by looking at the present and past situation, predictive analytics
    tries to project itself in order to predict future events with a certain degree
    of probability. It does this by extrapolating hidden patterns within the analyzed
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Being data-driven, predictive analytics makes use of data mining and ML techniques
    to make its predictions, and is based on the analysis of large amounts of available
    data (big data analytics).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following sections, we will discover how to develop predictive analytics
    models for the analysis of credit card fraud. We will learn to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Take advantage of big data analytics to integrate information from different
    sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combine different classifiers (ensemble learning) to improve the performance
    of our predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use bagging and boosting algorithms to develop predictive models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use sampling techniques to rebalance datasets, thereby improving prediction
    accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start by discovering the advantages of leveraging big data analytics in
    developing predictive models in order to manage credit card fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: Embracing big data analytics in fraud detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The traditional ETL solutions that are commonly adopted by organizations, which
    make use of data architectures based on relational databases and data warehouses,
    are undoubtedly adequate to perform reports according to descriptive analytics
    BI reporting) but not to manage large amounts of data following a data-driven
    approach, which is typical of predictive analytics.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, it is necessary to adopt data architectures that allow the achievement
    of processing scalability through the use of functional programming paradigms
    (such as MapReduce, NoSQL primitives, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to exploit the techniques of big data analytics and combine them
    with ML and data mining algorithms in order to automate fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: Embracing the paradigm of big data analytics helps organizations make the most
    of their information assets, which come disseminated from different (often heterogeneous)
    data sources. This allows for the implementation of advanced forms of contextual
    awareness, which can be used to adapt detection procedures to context changes
    in real time.
  prefs: []
  type: TYPE_NORMAL
- en: It is well-known that illegal activities are often linked to each other, and
    being able to construct an overall picture of the ongoing fraudulent activities
    presupposes constantly monitoring the different sources of available information.
  prefs: []
  type: TYPE_NORMAL
- en: The real-time monitoring and analysis of data are facilitated by the adoption
    of cloud computing platforms, which also make it possible to aggregate the various
    data sources.
  prefs: []
  type: TYPE_NORMAL
- en: Just think, for example, of the integration of data and information that's produced
    within the organization with publicly available data on websites, social media,
    and other platforms. By integrating these different sources of information, it
    is possible to reconstruct the context of the financial transactions to be monitored
    (for example, via social media, you may discover that the credit card holder is
    currently in a geographical location far from the one in which a credit card transaction
    is in progress).
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, the integration of different data sources allows you to feature
    augment the datasets; that is, the introduction of new variables starting from
    those existing within the datasets, which can describe the behavior of legitimate
    card holders and compare it with fraudsters' behavior.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we can add new variables to the existing ones, which contain recalculated
    values ​​such as the average expenditure level in the last time period, the number
    of purchases made on a daily basis, and in which shops (including e-commerce websites)
    the purchases usually take place.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, it is possible to keep customer profiles constantly updated, and
    we can promptly detect possible anomalies in behavior and consolidated spending
    habits.
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Moving on from data to algorithms, earlier, we mentioned how, in the presence
    of non-stationary data, it may be useful to introduce an ensemble of classifiers,
    rather than simply using individual classifiers to improve overall prediction
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the purpose of ensemble learning is to combine different classification
    algorithms in order to obtain a classifier that allows you to get better predictions
    than those that can be obtained by using individual classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: To understand why the ensemble classifier behaves better than individual classifiers,
    we need to imagine that we have a certain number of binary classifiers, all of
    the same type, characterized by the ability to make correct predictions in 75%
    of cases and erroneous forecasts in the remaining 25% of cases.
  prefs: []
  type: TYPE_NORMAL
- en: By using combinatorics analysis and binomial probability distribution (since
    we are considering binary classifiers), it is possible to demonstrate that, by
    using the ensemble classifier rather than individual classifiers, the probability
    of obtaining correct predictions improves (while the probability of errors decreases).
  prefs: []
  type: TYPE_NORMAL
- en: If we had, for example, 11 binary classifiers taken together (ensemble learning),
    the error rate would be reduced to 3.4% (compared to the 25% error rate of individual
    classifiers).
  prefs: []
  type: TYPE_NORMAL
- en: For a formal demonstration, refer to *Python Machine Learning – Second Edition*, by
    Sebastian Raschka, Packt Publishing.
  prefs: []
  type: TYPE_NORMAL
- en: There are several methods that you can use to combine classifiers; one of these
    is the use of majority voting (also known as the **majority voting principle**).
  prefs: []
  type: TYPE_NORMAL
- en: The term majority voting principle refers to the fact that, among the predictions
    made by individual classifiers, we select the one that shows the highest frequency.
  prefs: []
  type: TYPE_NORMAL
- en: In formal terms, this translates to calculating one of the statistical measures
    of position, known as **mode**, that is, the class that has achieved the highest
    frequency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imagine we have *n* classifiers, *C**[i]**(x)*, and have to determine the prediction,
    *y*, most voted, that is, the prediction that has been confirmed by most of the
    individual classifiers. We can write the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c20fa09-a8ba-4e68-b888-4c3e41ead895.png)'
  prefs: []
  type: TYPE_IMG
- en: Obviously, we can choose individual classifiers among the different types of
    algorithms that are available (such as decision trees, random forest, **support
    vector machines** (**SVMs**), and others).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the same time, there are several ways to create an ensemble classifier,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging (bootstrap aggregating)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the bagging method, it is possible to reduce the variance of individual
    estimators by selecting different training sets and applying the bootstrap resampling
    technique to them.
  prefs: []
  type: TYPE_NORMAL
- en: Through boosting, we can create an ensemble estimator that reduces the bias
    of the individual classifiers. Finally, with stacking, the different predictions
    that have been obtained by heterogeneous estimators are combined.
  prefs: []
  type: TYPE_NORMAL
- en: We will analyze the different methods of creating ensemble estimators in the
    following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging (bootstrap aggregating)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term **bootstrap** refers to the operation of sampling with a replacement that's
    been applied to a dataset. The bagging method, therefore, associates an individual
    estimator with each bootstrap; the ensemble estimator is implemented by applying
    the majority voting method to individual classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: The number of bootstraps to be taken into consideration can be predetermined
    or adjusted using a validation dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The bagging method is particularly useful in the case where sampling with replacement
    helps to rebalance the original dataset, thus reducing total variance.
  prefs: []
  type: TYPE_NORMAL
- en: Boosting algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The boosting method, on the other hand, uses weighed samples that have been
    extracted from the data, whose weights are readjusted iteratively based on the
    classification errors that have been reported by the individual classifiers to
    reduce their bias.
  prefs: []
  type: TYPE_NORMAL
- en: Greater importance (weight) is given to the most difficult classification observations.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best-known boosting algorithms is **Adaptive Boosting** (**AdaBoost**),
    in which a first classifier is trained on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: The weight associated with the samples that are incorrectly classified by the
    first classifier is then incremented, a second classifier is trained on the dataset
    containing the updated weights, and so on. The iterative process ends when the
    predetermined number of estimators is reached, or when an optimal predictor is
    found.
  prefs: []
  type: TYPE_NORMAL
- en: Among the main disadvantages of AdaBoost is the fact that the algorithm cannot
    be executed in parallel due to its sequential learning strategy.
  prefs: []
  type: TYPE_NORMAL
- en: Stacking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The stacking method owes its name to the fact that the ensemble estimator is
    constructed by superimposing two layers, in which the first consists of single
    estimators, whose predictions are forwarded to the underlying layer, in which
    another estimator has the task of classifying the predictions that are received.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the bagging and boosting methods, stacking can use different types of
    basic estimators, whose predictions can, in turn, be classified by a different
    type of algorithm than the previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some examples of ensemble estimators.
  prefs: []
  type: TYPE_NORMAL
- en: Bagging example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following example, we will use the Python scikit-learn library to instantiate
    an object of the `BaggingClassifier` class, which is passed as a parameter and
    basic classifier of the `DecisionTreeClassifier` type; the number of basic estimators
    of the  `DecisionTreeClassifier` type to be instantiated is set with the `n_estimators`
    parameter.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to invoke on the **bagging** instance of the `BaggingClassifier`
    type and the `fit()` and `predict()` methods, which are usually invoked on the
    common classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we already know, the bagging method uses sampling replacement. Due to this,
    we can set the maximum number of samples to associate with each basic estimator
    (using the `max_samples` parameter and activate the bootstrap mechanism by setting
    the homonymous `bootstrap` parameter to `True`, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Boosting with AdaBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As an example of the boosting method, we will instantiate an object of the `AdaBoostClassifier` type of
    the `scikit-learn` library, which provides us with the implementation of the AdaBoost
    algorithm; as a base estimator, we will also use an instance of the `DecisionTreeClassifier`
    class in this example and set the number of base estimators with the `n_estimators`
    parameter*:*
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Another widely used boosting algorithm is the **gradient boosting** algorithm.
    To understand the characteristics of the gradient boosting algorithm, we must
    first introduce the concept of the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the gradient
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In mathematical terms, a gradient represents the partial derivative that's calculated
    on a given point in the n-dimensional space; it also represents the tangent line
    (slope) of the point that's being considered.
  prefs: []
  type: TYPE_NORMAL
- en: The gradient is used in machine learning as a cost function to be minimized
    in order to reduce the prediction errors that are produced by the algorithms.
    This consists of minimizing the difference between the value estimated by the
    algorithm and the observed value.
  prefs: []
  type: TYPE_NORMAL
- en: The minimization method that's used is known as gradient descent, which is a
    method of optimizing the combination of weights to be assigned to the input data
    in order to obtain the minimum difference between the values estimated and the
    values observed.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the gradient descent method calculates the partial derivatives of
    the individual weights, updating the weights themselves on the basis of these
    partial derivatives until it reaches a stationary value of the partial derivatives
    corresponding to the minimum value sought.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient descent formula, along with its graphical representation, is shown
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32760f0f-58f1-4fb4-86bf-21e2fd8a6aa5.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Image credit: Wikipedia, at https://commons.wikimedia.org/wiki/File:Gradient_descent.jpg
    )'
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that the minimum value returned by the gradient descent method
    can correspond to a global minimum (that is, not further minimizable), but it
    is more likely to corresponds with a local minimum; the problem is that the gradient
    descent method is unable to establish whether a local minimum has been reached
    because the optimization process stops when it reaches a stationary value.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient descent optimization method is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c065f93-a617-4f63-a071-f426d6b38a27.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Image credit: Wikipedia, at https://commons.wikimedia.org/wiki/File:Gradient_descent_method.png
    )'
  prefs: []
  type: TYPE_NORMAL
- en: Now let's look at the features of the gradient boosting algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar to the AdaBoost algorithm, gradient boosting also iteratively corrects
    the estimators on the basis of the values ​​returned by them; in the case of gradient
    boosting, the adjustment takes place on the basis of the residual error generated
    by the previous estimators, rather than on the weights to be assigned (as in the
    case of AdaBoost).
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will show an example that uses the `GradientBoostingClassifier` class
    of the `scikit-learn` library.
  prefs: []
  type: TYPE_NORMAL
- en: The default estimators are represented by decision trees, whose characteristics
    are specified in the parameters (such as `max_depth`, which establishes the growth
    of decision trees).
  prefs: []
  type: TYPE_NORMAL
- en: Also, note the `learning_rate` parameter, which must be considered together
    with the `warm_start` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: The value assigned to the `learning_rate` parameter determines the contribution
    that each estimator provides to the ensemble classifier; if the assigned value
    is `low`, a greater number of estimators will be needed (to be set with the `n_estimators`
    parameter) to proceed with the fitting of the ensemble on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'The decision on the optimal value to be assigned to the `learning_rate` and
    `n_estimators` parameters must take into account the problem related to overfitting
    (that is, the possible generalization errors deriving from the excessive fitting
    of the model on training data). One way to overcome these problems is to set the
    `warm_start=True` parameter, which determines the early stopping in the training
    phase, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: eXtreme Gradient Boosting (XGBoost)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An algorithm that's similar to gradient boosting is the XGBoost algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: It represents an extension of gradient boosting that proves to be more suitable
    in managing large amounts of data since it is more scalable.
  prefs: []
  type: TYPE_NORMAL
- en: XGBoost also uses the gradient descent method to minimize the residual error
    of the estimators, and is particularly suitable for parallel computing (a feature
    that makes it more suitable for cloud computing).
  prefs: []
  type: TYPE_NORMAL
- en: We will see the XGBoost algorithm in action shortly when we use IBM Watson to
    implement credit card fraud detection on the IBM Cloud platform.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling methods for unbalanced datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A final aspect to consider before moving on to the operational phase of fraud
    detection relates to the management of unbalanced data.
  prefs: []
  type: TYPE_NORMAL
- en: We have already said that one of the characteristics of credit card transactions
    is to show unbalanced distributions toward genuine transactions.
  prefs: []
  type: TYPE_NORMAL
- en: To manage this asymmetry in the data, we can use different sampling methods
    that intend to rebalance the transaction dataset, thereby allowing the classifier
    to perform better.
  prefs: []
  type: TYPE_NORMAL
- en: The two most adopted sampling modes are undersampling and oversampling. Through
    undersampling, some random samples are removed from the most numerous class (in
    our case, the class of legitimate transactions); with oversampling, synthetic
    samples are added to the class with the lowest occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: Oversampling with SMOTE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among the oversampling methods, we have the **Synthetic Minority Over-sampling
    Technique** (**SMOTE**); this allows for the generation of synthetic samples by
    interpolating the values that are present within the class subjected to oversampling.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, synthetic samples are generated based on the clusters that are identified
    around the observations present in the class, therefore calculating the **k-Nearest
    Neighbors** (**k-NNs**).
  prefs: []
  type: TYPE_NORMAL
- en: Based on the number of synthetic samples that are needed to rebalance the class,
    a number of k-NN clusters are randomly chosen, around which synthetic examples
    are generated by interpolating the values that fall within the selected clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Sampling examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following examples are taken from the official Python library imbalanced-learn
    documentation, which implements undersampling and oversampling algorithms, among
    others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of the undersampling technique by using the `RandomUnderSampler`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is an example of the oversampling technique using the SMOTE class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Getting to know IBM Watson Cloud solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The time has come to get to know one of the most interesting cloud-based solutions
    available on the market, and will allow us to look at a concrete example of credit
    card fraud detection in action: we are talking about the IBM Watson Cloud solution,
    which introduces, among the other things, the innovative concept of cognitive
    computing.'
  prefs: []
  type: TYPE_NORMAL
- en: Through cognitive computing, it is possible to emulate the typically human ability
    of pattern recognition, which allows adequate contextual awareness to be obtained
    for decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: 'IBM Watson can be successfully used in various real scenarios; here are few:'
  prefs: []
  type: TYPE_NORMAL
- en: Augmented reality
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Crime prevention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer support
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facial recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud prevention
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Healthcare and medical diagnosis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IoT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Language translation and **natural language processing** (**NLP**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Malware detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before going into detail about the IBM Watson Cloud platform, let's see the
    advantages associated with cloud computing and cognitive computing.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing advantages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the spread of higher bandwidth networks, combined with the availability
    of low-cost computers and storage, the architectural model of cloud computing
    has rapidly taken hold thanks to the availability of virtualization solutions,
    both on the software and hardware side.
  prefs: []
  type: TYPE_NORMAL
- en: The central element that characterizes cloud computing is the scalability of
    the architecture, which has determined its commercial success.
  prefs: []
  type: TYPE_NORMAL
- en: Organizations that have adopted cloud computing solutions have succeeded in
    optimizing investments in the IT sector, thereby improving their profit margins;
    instead of being forced to dimension their technological infrastructure based
    on the worst scenario (that is, the one that takes into account the peaks of workload,
    even if only temporary), the organizations that have embraced cloud solutions
    have benefited from an on-demand model, thereby reducing fixed costs and turning
    them into variable costs.
  prefs: []
  type: TYPE_NORMAL
- en: This improvement in the quality of technological investments has allowed organizations
    to focus on the management and analysis of data constituting corporate information
    assets.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, cloud computing allows for the storage and management of large amounts
    of data efficiently, guaranteeing high performance, high availability, and low
    latency; to offer these guarantees of access and performance, the data is stored
    and replicated on servers that are distributed in various geographical areas.
    Furthermore, by partitioning the data, it is possible to obtain the advantages
    connected to the scalability of the architecture.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, scalability is related to the ability to manage increasing
    workloads by adding resources to the architecture—increasing costs in a linear
    manner, proportional to the number of resources being added.
  prefs: []
  type: TYPE_NORMAL
- en: Achieving data scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the main problems of traditional architectures based on relational databases
    and the data warehouse is that these solutions do not scale well compared to the
    explosive growth of data. Such architectures need to be adequately sized, even
    in the design phase.
  prefs: []
  type: TYPE_NORMAL
- en: With the spread of big data analytics, it was, therefore, necessary to move
    on to other paradigms for data storage, known as **distributed storage systems**,
    which allow for the precise prevention of bottlenecks in the management and storage
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing makes extensive use of such distributed storage systems to enable
    the analysis of large amounts of data (big data analytics), even in streaming
    mode.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed storage systems consist of non-relational databases and are defined
    as NoSQL databases, which store data in key-value pairs. This allows for the management
    of data in a distributed mode on multiple servers by following functional programming
    paradigms such as MapReduce. This, in turn, allows for the execution of data processing
    in parallel, takes full advantage of the distributed computing capabilities offered
    by the Cloud.
  prefs: []
  type: TYPE_NORMAL
- en: The use of NoSQL databases also allows data to be managed in a flexible manner,
    without the need to reorganize its overall structure as the analysis changes.
  prefs: []
  type: TYPE_NORMAL
- en: However, traditional solutions based on relational databases require reconfiguration
    of almost the entire structure of the archives, which makes data unavailable for
    long periods of time. This is no longer acceptable in a context that's characterized
    by the need to verify predictive model accuracy in real time that's based on which
    business decisions to take; this aspect is also of particular relevance for decision-making
    in the area of cybersecurity.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud delivery models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The scalability of the architecture, combined with the ability to manage resources
    in on-demand mode, allows providers to offer different cloud delivery models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infrastructure as a Service** (**IaaS**): The provider deploys an IT infrastructure,
    such as storage capabilities and networking equipment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Platform as a Service** (**PaaS**): The provider deploys middleware, a database,
    and more'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Software as a Service** (**SaaS**): The provider deploys complete applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The IBM Cloud platform offers a delivery model that includes IaaS and PaaS,
    as well as a series of cloud services that can be integrated into applications
    that are developed by organizations, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual recognition**: This enables apps to locate information such as objects,
    faces, and text contained within images and videos; the services that are offered
    by the platform include checking the availability of pre-trained models, as well
    as the opportunity to train using corporate datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Natural language understanding**: This service can extract information about
    sentiment based on the analysis of a text; it is particularly useful if you want
    to extract information from social media (to understand, for example, whether
    the credit card holder is actually on vacation in a foreign state when a transaction
    is made with their credit card). The service can identify information regarding
    people, places, organizations, concepts, and categories, and is adaptable on the
    basis of specific application domains of interest to the company via Watson Knowledge
    Studio.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The IBM Cloud platform also offers a series of advanced tools for application
    development:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Watson Studio**: This allows the management of projects and offers tools
    for collaboration between team members. With Watson Studio, it is possible to
    add data sources, create Jupyter Notebooks, train models, and use many other features
    that facilitate data analysis, such as data cleansing functions. We will have
    the opportunity to deepen our knowledge of Watson Studio soon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge Studio**: This allows the development of customized models on the
    specific needs of the company; once developed, the models can be used by Watson
    services, in addition to, or in place of, the predefined models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge Catalog**: This allows the management and sharing of company data;
    the tool also makes it possible to perform data cleaning and wrangling operations,
    thereby profiling data access permissions through security policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Among the major advantages offered by the IBM Cloud platform, there is the undoubted
    possibility of implementing advanced solutions that exploit cognitive computing.
    Let's look at what this is.
  prefs: []
  type: TYPE_NORMAL
- en: Empowering cognitive computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The spread of AI has been accompanied since the beginning by often excessive
    and unjustified concerns; many authors and commentators have foreseen apocalyptic
    scenarios in which machines (in the not too distant future) take precedence over
    humans. The cause of such disasters would have to be found precisely in the rise
    of AI.
  prefs: []
  type: TYPE_NORMAL
- en: The reality is that, despite the amazing successes achieved by computers, they
    still continue to be idiot savants.
  prefs: []
  type: TYPE_NORMAL
- en: There is no doubt that the computational capacities reached by computers exceed
    those of human beings by several orders of magnitude; the victory achieved by
    IBM Watson in the match of the century, which saw the computer beating the then
    world chess champion, Garry Kasparov, seemed to have decreed the final overcoming
    of human cognitive faculties using AI.
  prefs: []
  type: TYPE_NORMAL
- en: However, despite their computational limitations, humans are still unbeaten
    in relation to a whole range of skills, such as the ability to adapt, interact,
    make judgments, and more.
  prefs: []
  type: TYPE_NORMAL
- en: We human beings can recognize, for example, a person (or an object) at a glance,
    without the need to be trained with large amounts of sample data; just one photograph
    (or an identikit) is enough to recognize the depicted person amid a crowd of people.
    Computers are far from reaching such levels of expertise.
  prefs: []
  type: TYPE_NORMAL
- en: It is not a question, then, of replacing humans with machines; on the contrary,
    the most likely scenario before us is one in which humans and machines work together
    ever more closely, integrating their mutual skills in an increasingly pervasive
    way.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the meaning of cognitive computing: integrating human abilities with
    the computational abilities of computers, combining forces to face the growing
    complexity that characterizes contemporary society.'
  prefs: []
  type: TYPE_NORMAL
- en: In this symbiotic relationship, machines make their enormous computational capabilities
    and inexhaustible memory available to human beings, which allows them to amplify
    their capacity for judgment, intuition, empathy, and creativity.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a sense, through cognitive computing, machines allow us not only to amplify
    our five natural senses but to add a sixth *artificial* sense: contextual awareness.'
  prefs: []
  type: TYPE_NORMAL
- en: We have said several times that one of the major difficulties that's encountered,
    especially in the field of cybersecurity, is that of being able to reconstruct
    a precise overall picture, starting from the multiple, dispersed, and fragmented
    information at hand.
  prefs: []
  type: TYPE_NORMAL
- en: Human abilities are at a loss in the face of the overabundance of data and information
    that we constantly receive from various data sources; big data analytics is beyond
    the capabilities of human analysis, precisely because of the countless dimensions
    (consisting of the many different features, as well as the amount of data) that
    characterize big data.
  prefs: []
  type: TYPE_NORMAL
- en: However, big data allows us to define the semantic context within which we can
    carry out our analysis; it is as if they increased our perceptive capacity, adding
    an indefinite number of artificial sensors.
  prefs: []
  type: TYPE_NORMAL
- en: Only the computational capacity of machines can filter the numerous pieces of
    information we receive in a constant and incessant way from artificial sensors
    to human judgment skills and human intuition to give us an overall meaning and
    allows us to make sense of such information.
  prefs: []
  type: TYPE_NORMAL
- en: Importing sample data and running Jupyter Notebook in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's learn how to use the IBM Watson platform. The first thing we need
    to do is create an account, if we don't have one already; just connect to the
    IBM Cloud platform home link provided here at[https://dataplatform.cloud.ibm.com/](https://dataplatform.cloud.ibm.com/)[.
    You will see the following screen:](https://dataplatform.cloud.ibm.com/)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77d67c6e-61f9-49cd-9926-9775187cba76.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson home page
  prefs: []
  type: TYPE_NORMAL
- en: 'To proceed with the registration, select Try it for Free (register) as shown
    in the preceding screenshot. We will be automatically redirected to the registration
    form, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/28f7d868-1a85-4d61-8a4b-7b29ec83d3b1.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson Registration page
  prefs: []
  type: TYPE_NORMAL
- en: 'Once registration is complete, we can log in again from the home page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09dd24fc-1b5b-47b3-b12c-ea0dd3726b4e.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson login form
  prefs: []
  type: TYPE_NORMAL
- en: 'After logging in, we can create a new project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/24b3e857-f5a1-4d71-9413-438a4e9798f3.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson start by creating a project screen
  prefs: []
  type: TYPE_NORMAL
- en: 'We can select the type of project we want to create:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01f0b6f4-2388-4ede-b814-ad4e20cd59bf.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson project selection
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, we will choose Data Science, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c721daa-a337-4814-a130-cb849720a158.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson Data Science project
  prefs: []
  type: TYPE_NORMAL
- en: 'We assign the name  `CREDIT CARD FRAUD DETECTION` to the project (or another
    name of our choice):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6c102afe-f26f-42b8-9452-d570f9eb5ace.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson new project screen
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now add a dataset to our project by selecting Add to project | Data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ee7b3139-5238-4f54-836a-9529550d11cb.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson-Add Data
  prefs: []
  type: TYPE_NORMAL
- en: To add the dataset, just click on Find and Add Data and go to the Files tab.
    From there, you can click on and add data files from your computer.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset that we will use is the credit card dataset, available for download
    in `.csv` format at[ ](https://www.openml.org/data/get_csv/1673544/phpKo8OWT)[https://www.openml.org/data/get_csv/1673544/phpKo8OWT](https://www.openml.org/data/get_csv/1673544/phpKo8OWT)[.](https://www.openml.org/data/get_csv/1673544/phpKo8OWT)
  prefs: []
  type: TYPE_NORMAL
- en: The credit card dataset has been released under the public domain ([https://creativecommons.org/publicdomain/mark/1.0/](https://creativecommons.org/publicdomain/mark/1.0/))
    license ([https://www.openml.org/d/1597](https://www.openml.org/d/1597)) and is
    credited to Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson, and Gianluca
    Bontempi for their paper *Calibrating Probability with Undersampling for Unbalanced
    Classification*, in Symposium on **Computational Intelligence and Data Mining**
    (**CIDM**), IEEE, 2015.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset contains 31 numerical input variables, such as time (representing
    the time that had elapsed between each transaction), the transaction amount, and
    the class feature.
  prefs: []
  type: TYPE_NORMAL
- en: The class feature is a binary variable that takes only the values 1 and 0 (indicating
    a fraudulent or legitimate transaction, respectively).
  prefs: []
  type: TYPE_NORMAL
- en: The main characteristic of the dataset is that it is highly unbalanced, with
    frauds accounting for just 0.172% of all transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Having added the dataset, we can add a Jupyter notebook to the project by selecting
    Add to project | Notebook, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/896b3a18-70ee-408f-925d-e6c7f9dc3e72.png)'
  prefs: []
  type: TYPE_IMG
- en: IBM Watson-Add Notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a Jupyter Notebook, perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on create a notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the tab
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter a name for the notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Optionally, enter a description for the notebook
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter the notebook URL: [https://github.com/IBM/xgboost-smote-detect-fraud/blob/master/notebook/Fraud_Detection.ipynb](https://github.com/IBM/xgboost-smote-detect-fraud/blob/master/notebook/Fraud_Detection.ipynb)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose the Runtime
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Create
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You have successfully completed configured your project and
    are ready to see the credit card fraud detection model with IBM Watson Studio
    on the IBM Cloud platform in action.
  prefs: []
  type: TYPE_NORMAL
- en: Credit card fraud detection with IBM Watson Studio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s see the fraud detection predictive model that we loaded into the IBM
    Watson Studio Jupyter notebook (the complete code, released by IBM with the Apache
    2.0 license, is available at the link: [https://github.com/IBM/xgboost-smote-detect-fraud/blob/master/notebook/Fraud_Detection.ipynb](https://github.com/IBM/xgboost-smote-detect-fraud/blob/master/notebook/Fraud_Detection.ipynb)
    ) in action.'
  prefs: []
  type: TYPE_NORMAL
- en: The first operation to perform is to convert the credit card dataset, loaded
    in `.csv` format, into a `pandas` DataFrame;. This operation can be performed
    as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Select the cell below Read the Data and convert it to the DataFrame section
    in the notebook and perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Use Find and Add Data and its Files tab. You should see the file names that
    we uploaded earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select Insert to Code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on Insert Pandas DataFrame.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the dataset has been converted into a `pandas` DataFrame, we can rename
    it by replacing the name that was automatically assigned by Watson Studio with
    a name of our choice, as shown in the following snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can proceed to subdivide the dataset into train and test
    data using the `train_test_split` method; this is done by utilizing the usual
    split rate (30% for the test and the remaining 70% for training), as shown in
    the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Remember that the dataset contains 31 numerical input variables, in which the
    `Time` feature denotes the seconds elapsed between each transaction and the first
    transaction in the dataset, and the `Amount` feature, which represents the transaction
    amount.
  prefs: []
  type: TYPE_NORMAL
- en: The `Class` feature is the response variable and it takes a value of `1` in
    cases of fraud and `0` otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: For confidentiality reasons, the meaning of most variables (indicated with `V1`,
    `V2`, …, `V28`) is not revealed and the features have been transformed by means
    of principal components.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we can introduce our first ensemble classifier in order to test
    the quality of its classification on the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting with RandomForestClassifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The choice falls on one of the most used among the ensemble algorithms, that
    is, the random forest algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: This type of algorithm is used by the `RandomForestClassifier` class of `scikit-learn`
    to create a set of decision trees from a subset extracted at random from a training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm represents an example of a learning ensemble that uses the bagging
    technique and is, therefore, particularly suitable for reducing the overfitting
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at an example of `RandomForestClassifier` and its accuracy score:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of the model is rather high (99.9414%), demonstrating the effectiveness
    of the ensemble learning.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see if we can improve the predictions obtained by using another classifier
    ensemble, this time taking advantage of the boosting technique.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting with GradientBoostingClassifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now we will use `GradientBoostingClassifier`, which is based on AlgaBoost.
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm used by the ensemble classifier adopts the boosting technique;
    it also uses gradient descent to minimize the cost function (represented by the
    residual error returned by the individual base classifiers, also constituted by
    decision trees).
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we can see the gradient-boosting ensemble classifier
    in action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy of the model is still high, but it hasn't improved any further
    than `RandomForestClassifier`; we have, in fact, reached just 99.89% accuracy
    in the predictions, but we can do better.
  prefs: []
  type: TYPE_NORMAL
- en: Predicting with XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now try to further improve our predictions by using **XGBoost**, which
    represents an improved version of the gradient boosting algorithm since it was
    designed to optimize performance (using parallel computing), thus reducing overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the `XGBClassifier` class of the `xgboost` library, which implements
    the eXtreme Gradient Boosting Classifier, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The accuracy has improved even further; we have reached a percentage equal to
    99.9472% (higher, albeit slightly, than the accuracy of `RandomForestClassifier`,
    which is equal to 99.9414%). This isn't bad, but now we must carefully evaluate
    the quality of our predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the quality of our predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To correctly evaluate the quality of the predictions that were obtained by our
    classifiers, we cannot be satisfied with just `accuracy_score`, but must also
    use other measures, such as the **F1 score** and the **ROC curve**, which we previously
    encountered in [Chapter 5](a6eab48a-f031-44c9-ae4a-0cfd5db2e05e.xhtml), *Network
    Anomalies Detection with AI*, dealing with the topic related to anomaly detection.
  prefs: []
  type: TYPE_NORMAL
- en: F1 value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the convenience, let''s briefly go over the metrics that were previously
    introduced and their definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Sensitivity or True Positive Rate (TPR) = True Positive / (True Positive +
    False Negative);*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, sensitivity is also known as the recall rate:'
  prefs: []
  type: TYPE_NORMAL
- en: '*False Positive Rate (FPR) = False Positive / (False Positive + True Negative);*'
  prefs: []
  type: TYPE_NORMAL
- en: '*Precision = True Positive / (True Positive + False Positive)*'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the basis of these metrics, it is possible to estimate the F1 score, which
    represents the harmonic average between precision and sensitivity:'
  prefs: []
  type: TYPE_NORMAL
- en: '*F1 = 2 * Precision * Sensitivity / (Precision + Sensitivity)*'
  prefs: []
  type: TYPE_NORMAL
- en: The F1 score can be used to evaluate the results that were obtained from the
    predictions; the best estimates are obtained with F1 values close to 1, while
    the worst estimates correspond to F1 values close to 0.
  prefs: []
  type: TYPE_NORMAL
- en: ROC curve
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Often, between false positives and false negatives, there is a trade-off; reducing
    the number of false negatives leads to an increase in false positives and to detect
    the existence of this trade-off, a particular curve is used, known as the ROC
    curve. This is as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/39ea1875-2a82-4ba1-8969-149be037b7e7.png)'
  prefs: []
  type: TYPE_IMG
- en: '(Image Credit: Wikipedia, at https://commons.wikimedia.org/wiki/File:ROC_curve.svg
    )'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ROC curve is calculated using `roc_curve()` of `scikit-learn`, which takes
    the target values and the corresponding probabilities as parameters as shown in
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We should notice the existing link between the True Positive Rate (`TPR` or
    sensitivity), the False Positive Rate (`FPR`), and the ROC curve (instead, the
    `OPC` parameter represents a control coefficient, known as an **operating characteristic**,
    which identifies the possible classification thresholds on the curve). We can,
    therefore, represent the sensitivity by plotting the `TPR` value with respect
    to the value of the `OPC` control coefficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see how sensitivity (`TPR`) decreases as the value of `OPC` increases;
    in the same way, we can draw the ROC curve by comparing `TPR` with `FPR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: AUC (Area Under the ROC curve)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The ROC curve allows us to evaluate the performance of a classifier by plotting
    `TPR` against `FPR` (where each point of the curve corresponds to a different
    classification threshold).
  prefs: []
  type: TYPE_NORMAL
- en: We can also compare different classifiers to find out which one is more accurate,
    using the area under the ROC curve.
  prefs: []
  type: TYPE_NORMAL
- en: To understand the logic of this comparison, we must consider that the optimal
    classifier within the ROC space is identified by the coordinates of points *x*
    = 0 and *y* = 1 (which correspond to the limit case of no false negatives and
    no false positives).
  prefs: []
  type: TYPE_NORMAL
- en: To compare different classifiers, we can calculate the value of the **Area Under
    the ROC Curve** (**AUC**) associated with each classifier; the classifier that
    obtains the highest AUC value is the most accurate.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also take into account the AUC values of two peculiar classifiers:'
  prefs: []
  type: TYPE_NORMAL
- en: AUC of the best classifier is 1 x 1 = 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUC of the worst classifier is 0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, AUC also represents a measure for unbalanced datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate `AUC` using `scikit-learn`, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Based on what we have said, we can now proceed to a more accurate evaluation
    of the predictions that were obtained by our classifiers, and compare them with
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing ensemble classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now proceed to calculate the main accuracy measurements for each classifier
    by comparing them.
  prefs: []
  type: TYPE_NORMAL
- en: The RandomForestClassifier report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classification report for the `RandomForestClassifier` metrics is shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The GradientBoostingClassifier report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classification report for the `GradientBoostingClassifier` metrics is shown
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The XGBClassifier report
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The classification report for the `XGBClassifier` metrics is shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: By comparing the AUC and F1 score values, which are calculated using the individual
    classifiers, `XGBClassifier` remains the most accurate classifier and `GradientBoostingClassifier`
    is the least accurate of the three.
  prefs: []
  type: TYPE_NORMAL
- en: Improving predictions accuracy with SMOTE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We conclude our considerations by showing you how the use of a rebalancing technique
    based on oversampling contributes to improving the accuracy of predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the implementation of the SMOTE oversampling algorithm offered
    by the imbalanced-learn library, increasing the fraud samples from 102 to 500
    and reusing `RandomForestClassifier` on resampled data, as shown in the following
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: We can see an increase in both the F1 score and the AUC due to the application
    of a synthetic oversampling technique.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have learned how to develop a predictive model for credit card fraud detection,
    exploiting the IBM Cloud platform with IBM Watson Studio.
  prefs: []
  type: TYPE_NORMAL
- en: By leveraging the IBM Cloud platform, we have also learned how to address the
    issues related to the presence of unbalanced and non-stationary data within the
    dataset concerning credit card transactions and made full use of ensemble learning
    and data sampling techniques.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will delve deep into **generative adversarial networks**
    (**GANs**).
  prefs: []
  type: TYPE_NORMAL
