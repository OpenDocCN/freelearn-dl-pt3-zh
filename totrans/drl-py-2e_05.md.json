["```\nimport gym\nimport pandas as pd \n```", "```\nenv = gym.make('FrozenLake-v0') \n```", "```\ndef random_policy():\n    return env.action_space.sample() \n```", "```\nV = {}\nfor s in range(env.observation_space.n):\n    V[s]=0.0 \n```", "```\nalpha = 0.85\ngamma = 0.90 \n```", "```\nnum_episodes = 50000\nnum_timesteps = 1000 \n```", "```\nfor i in range(num_episodes): \n```", "```\n s = env.reset() \n```", "```\n for t in range(num_timesteps): \n```", "```\n a = random_policy() \n```", "```\n s_, r, done, _ = env.step(a) \n```", "```\n V[s] += alpha * (r + gamma * V[s_]-V[s]) \n```", "```\n s = s_ \n```", "```\n if done:\n            break \n```", "```\ndf = pd.DataFrame(list(V.items()), columns=['state', 'value']) \n```", "```\ndf \n```", "```\nimport gym\nimport random \n```", "```\nenv = gym.make('FrozenLake-v0') \n```", "```\nQ = {}\nfor s in range(env.observation_space.n):\n    for a in range(env.action_space.n):\n        Q[(s,a)] = 0.0 \n```", "```\ndef epsilon_greedy(state, epsilon):\n    if random.uniform(0,1) < epsilon:\n        return env.action_space.sample()\n    else:\n        return max(list(range(env.action_space.n)), key = lambda x: Q[(state,x)]) \n```", "```\nalpha = 0.85\ngamma = 0.90\nepsilon = 0.8 \n```", "```\nnum_episodes = 50000\nnum_timesteps = 1000 \n```", "```\nfor i in range(num_episodes): \n```", "```\n s = env.reset() \n```", "```\n a = epsilon_greedy(s,epsilon) \n```", "```\n for t in range(num_timesteps): \n```", "```\n s_, r, done, _ = env.step(a) \n```", "```\n a_ = epsilon_greedy(s_,epsilon) \n```", "```\n Q[(s,a)] += alpha * (r + gamma * Q[(s_,a_)]-Q[(s,a)]) \n```", "```\n s = s_\n        a = a_ \n```", "```\n if done:\n            break \n```", "```\nimport gym\nimport numpy as np\nimport random \n```", "```\nenv = gym.make('FrozenLake-v0') \n```", "```\nQ = {}\nfor s in range(env.observation_space.n):\n    for a in range(env.action_space.n):\n        Q[(s,a)] = 0.0 \n```", "```\ndef epsilon_greedy(state, epsilon):\n    if random.uniform(0,1) < epsilon:\n        return env.action_space.sample()\n    else:\n        return max(list(range(env.action_space.n)), key = lambda x: Q[(state,x)]) \n```", "```\nalpha = 0.85\ngamma = 0.90\nepsilon = 0.8 \n```", "```\nnum_episodes = 50000\nnum_timesteps = 1000 \n```", "```\nfor i in range(num_episodes): \n```", "```\n s = env.reset() \n```", "```\n for t in range(num_timesteps): \n```", "```\n a = epsilon_greedy(s,epsilon) \n```", "```\n s_, r, done, _ = env.step(a) \n```", "```\n a_ = np.argmax([Q[(s_, a)] for a in range(env.action_space.n)]) \n```", "```\n Q[(s,a)] += alpha * (r + gamma * Q[(s_,a_)]-Q[(s,a)]) \n```", "```\n s = s_ \n```", "```\n if done:\n            break \n```"]