<html><head></head><body>
  <div id="_idContainer005">
    <h1 id="_idParaDest-5" class="mainHeading">Preface</h1>
    <p class="normal">TensorFlow is at the center of developing <strong class="keyWord">Machine Learning </strong>(<strong class="keyWord">ML</strong>) solutions. It is an ecosystem that can support all of the different stages in the life cycle of an ML project, from the early prototyping up until the productionization of the model. TensorFlow provides various reusable building blocks, allowing you to build not just the simplest but also the most complex deep neural networks.</p>
    <h1 id="_idParaDest-6" class="heading-1">Who this book is for</h1>
    <p class="normal">This book is aimed at novice - to intermediate-level users of TensorFlow. The reader may be from academia doing cutting-edge research on ML or an industry practioner using ML in their job. You will get the most benefit from this book if you have some basic familiarity with TensorFlow (or a similar framework like Pytorch) already. This will help you to grasp the concepts and use cases discussed in the book quicker.</p>
    <h1 id="_idParaDest-7" class="heading-1">What this book covers</h1>
    <p class="normal"><em class="chapterRef">Chapter 1</em>, <em class="italic">Introduction to Natural Language Processing</em>, explains what natural language processing is and the kinds of tasks it may entail. We then discuss how an NLP task is solved using traditional methods. This paves the way to discuss how deep learning is used in NLP and what the benefits are. Finally, we discuss the installation and usage of the technical tools in this book.</p>
    <p class="normal"><em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding TensorFlow 2</em>, provides you with a sound guide to writing programs and running them in TensorFlow 2. This chapter will first offer an in-depth explanation of how TensorFlow executes a program. This will help you to understand the TensorFlow execution workflow and feel comfortable with TensorFlow terminology. Next, we will discuss various building blocks in TensorFlow and useful operations that are available. We will finally discuss how all this knowledge of TensorFlow can be used to implement a simple neural network to classify images of handwritten digits.</p>
    <p class="normal"><em class="chapterRef">Chapter 3</em>, <em class="italic">Word2vec – Learning Word Embeddings</em>, introduces Word2vec—a method to learn numerical representations of words that reflect the semantics of the words. But before diving straight into Word2vec techniques, we first discuss some classical approaches used to represent words, such as one-hot-encoded representations, and the <strong class="keyWord">Term Frequency-Inverse Document Frequency</strong> (<strong class="keyWord">TF-IDF</strong>) frequency method. Following this, we will move on to a modern tool forlearning word vectors known as Word2vec, which uses a neural network to learn word representations. We will discuss two popular Word2vec variants: skip-gram and the <strong class="keyWord">Continuous Bag-of-Words</strong> (<strong class="keyWord">CBOW</strong>) model. Finally, we will visualize the word representations learned using a dimensionality reduction technique to map the vectors to a more interpretable two-dimensional surface.</p>
    <p class="normal"><em class="chapterRef">Chapter 4</em>, <em class="italic">Advanced Word Vector Algorithms</em>, starts with a more recent word embedding learning technique known as GloVe, which incorporates both global and local statisticsin text data to find word vectors. Next, we will learn about one of the modern, more sophisticated techniques for generating dynamic word representations based on the context of a word, known as ELMo.</p>
    <p class="normal"><em class="chapterRef">Chapter 5</em>, <em class="italic">Sentence Classification with Convolutional Neural Networks</em>, introduces you to <strong class="keyWord">Convolutional Neural Networks</strong> (<strong class="keyWord">CNNs</strong>). CNNs are a powerful family of deep models that can leverage the spatial structure of an input to learn from data. In other words, a CNN can process images in their two-dimensional form, whereas a multilayer perceptron needs the image to be unwrapped to a one-dimensional vector. We will first discuss various operations that are undergone in CNNs, such as the convolution and pooling operations, in detail. Then, we will see an example where we will learn to classify images of clothes with a CNN. Then, we will transition into an application of CNNs in NLP. More precisely, we will be investigating how to apply a CNN to classify sentences, where the task is to classify if a sentence is about a person, location, object, and so on.</p>
    <p class="normal"><em class="chapterRef">Chapter 6</em>, <em class="italic">Recurrent Neural Networks</em>, focuses on introducing <strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>) and using RNNs for language generation. RNNs are different from feed-forward neural networks (for example, CNNs) as RNNs have memory. The memory is stored as a continuously updated system state. We will start with a representation of a feed-forward neural network and modify that representation to learn from sequences of data instead of individual data points. This process will transform the feed-forward network to an RNN. This will be followed by a technical description of the exact equations used for computations within the RNN. Next, we will discuss the optimization process of RNNs that is used to update the RNN’s weights. Thereafter we will iterate through different types of RNNs such as one-to-one RNNs and one-to-many RNNs. We will then discuss a popular application of RNNs, which is to identify named entities in text (for example, Person name, Organization, and so on). Here, we’ll be using a basic RNN model to learn.Next, we will enhance our model further by incorporating embeddings at different scales (for example, token embeddings and character embeddings). The token embeddings are generated through an embedding layer, where the character embeddings are generated using a CNN. We will then analyze the new model’s performance on the named entity recognition task.</p>
    <p class="normal"><em class="chapterRef">Chapter 7</em>, <em class="italic">Understanding</em> <em class="italic">Long Short-Term Memory Networks</em>, discusses <strong class="keyWord">Long Short-Term Memory networks</strong> ( <strong class="keyWord">LSTMs</strong>) by initially providing an intuitive explanation of how these models work and progressively diving into the technical details required to implement them on your own. Standard RNNs suffer from the crucial limitation of the inability to persist long-term memory. However, advanced RNN models (for example, LSTMs and <strong class="keyWord">Gated Recurrent Units</strong> (<strong class="keyWord">GRUs</strong>)) have been proposed, which can remember sequences for a large number of time steps. We will also examine how exactly LSTMs alleviate the problem of persisting long-term memory (this is known as the vanishing gradient problem). We will then discuss several modifications that can be used to improve LSTM models further, such as predicting several time steps ahead at once and reading sequences both forward and backward. Finally, we will discuss several variants of LSTM models such as GRUs and LSTMs with peephole connections.</p>
    <p class="normal"><em class="chapterRef">Chapter 8</em>, <em class="italic">Applications of LSTM – Generating Text</em>, explains how to implement the LSTMs, GRUs, and LSTMs with peephole connections discussed in <em class="chapterRef">Chapter 7</em>, <em class="italic">Understanding</em> <em class="italic">Long Short-Term Memory Networks</em>. Furthermore, we will compare the performance of these extensions both qualitatively and quantitatively. We will also discuss how to implement some of the extensions examined in <em class="chapterRef">Chapter 7</em>, <em class="italic">Understanding</em> <em class="italic">Long Short-Term Memory Networks</em>, such as predicting several time steps ahead (known as beam search) and using word vectors as inputs instead of one-hot-encoded representations.</p>
    <p class="normal"><em class="chapterRef">Chapter 9</em>, <em class="italic">Sequence-to-Sequence Learning – Neural Machine Translation</em>, discusses machine translation, which has gained a lot of attention both due to the necessity of automating translation and the inherent difficulty of the task. We start the chapter with a brief historical flashback explaining how machine translation was implemented in the early days. This discussion ends with an introduction to <strong class="keyWord">Neural Machine Translation</strong> (<strong class="keyWord">NMT</strong>) systems. We will see how well current NMT systems are doing compared to old systems (such as statistical machine translation systems), which will motivate us to learn about NMT systems. Afterward, we will discuss the concepts underpinning the design of NMT systems and continue with the technical details. Then, we will discuss the evaluation metric we use to evaluate our system. Following this, we will investigate how we can implement an English-to-German translator from scratch. Next, we will learn about ways to improve NMT systems. We will look at one of those extensions in detail, called the attention mechanism. The attention mechanism has become essential in sequence-to-sequence learning problems.Finally, we will compare the performance improvement obtained with the attention mechanism and analyze the reasons behind the performance gain. This chapter concludes with a section on how the same concept of NMT systems can be extended to implement chatbots. Chatbots are systems that can communicate with humans and are used to fulfill various customer requests.</p>
    <p class="normal"><em class="chapterRef">Chapter 10</em>, <em class="italic">Transformers</em>, discusses Transformers, the latest breakthrough in the domain of NLP which have outperformed many other previous state-of-the-art models. In this chapter, we will use the Hugging Face Transformers library to use pre-trained models for downstream tasks with ease. In this chapter, we will learn about the Transformer architecture in depth. This discussion will lead into a popular Transformer model called BERT, which we will use to solve a problem of question answering. We will discuss specific components found in BERT to effectively use it for the application. Next, we will train the model on a popular question-answer dataset known as SQUAD. Finally, we will evaluate the model on a test dataset and use the trained model to generate answers for unseen questions.</p>
    <p class="normal"><em class="chapterRef">Chapter 11</em>, <em class="italic">Image Captioning with Transformers</em>, looks at another exciting application, where Transformers are used to generate captions (that is, descriptions) for images. This application is interesting because it shows us how to combine two different types of models as well as how to learn with multimodal data (for example, images and text). Here, we will use a pre-trained Vision Transformer model that generates a rich hidden representation for a given image. This representation, along with caption tokens, is fed to a text-based Transformer model. The text-based Transformer predicts the next caption token, given previous caption tokens. Once the model is trained, we will evaluate the captions generated by our model, both qualitatively and quantitatively. We will also discuss some of the popular metrics used to measure the quality of sequences such as image captions.</p>
    <p class="normal"><em class="chapterRef">Appendix </em><em class="italic">A:</em> <em class="italic">Mathematical Foundations and Advanced TensorFlow</em>, introduces various mathematical data structures (for example, matrices) and operations (for example, a matrix inverse). We will also discuss several important concepts in probability. Finally, we will walk you through a guide aimed at teaching you to use TensorBoard to visualize word embeddings. TensorBoard is a handy visualization tool that is shipped with TensorFlow. This can be used to visualize and monitor various variables in your TensorFlow client.</p>
    <h1 id="_idParaDest-8" class="heading-1">To get the most out of this book</h1>
    <p class="normal">To get the most out of this book you need a basic understanding of TensorFlow or a similar framework such as PyTorch. Familiarity obtained through basic TensorFlow tutorials that are freely available in the web should suffice to get started on this book.</p>
    <p class="normal"> A basic knowledge of mathematics, including an understanding of n-dimensional tensors, matrix multiplication, and so on, will also prove invaluable throughout this book. Finally, you need an enthusiasm for learning about cutting edge machine learning that is setting the stage for modern NLP solutions.</p>
    <p class="normal">Download the example code files</p>
    <p class="normal">The code bundle for the book is hosted on GitHub at <span class="url">https://github.com/thushv89/packt_nlp_tensorflow_2</span>. We also have other code bundles from our rich catalog of books and videos available at <span class="url">https://github.com/PacktPublishing/</span>. Check them out!</p>
    <h2 id="_idParaDest-9" class="heading-2">Download the color images</h2>
    <p class="normal">We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <span class="url">https://static.packt-cdn.com/downloads/9781838641351_ColorImages.pdf</span>.</p>
    <h2 id="_idParaDest-10" class="heading-2">Conventions used</h2>
    <p class="normal">There are a number of text conventions used throughout this book.</p>
    <p class="normal"><code class="inlineCode">CodeInText</code>: Indicates code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. For example: “After running the <code class="inlineCode">pip install</code> command, you should have Jupyter Notebook available in the Conda environment.”</p>
    <p class="normal">A block of code is set as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">layer</span>(<span class="hljs-params">x, W, b</span>):
    <span class="hljs-comment"># Building the graph</span>
    h = tf.nn.sigmoid(tf.matmul(x,W) + b) <span class="hljs-comment"># Operation to perform</span>
    <span class="hljs-keyword">return</span> h
</code></pre>
    <p class="normal">Any command-line input or output is written as follows:</p>
    <pre class="programlisting con"><code class="hljs-con">&lt;tf.Variable 'ref:0' shape=(3, 2) dtype=float32, numpy=
array([[-1., -9.],
       [ 3., 10.],
       [ 5., 11.]], dtype=float32)&gt;
</code></pre>
    <p class="normal"><strong class="keyWord">Bold</strong>: Indicates a new term or an important word. Words that you see on the screen (such as in menus or dialog boxes) also appear in the text like this , for example: “The feature that builds this computational graph automatically in TensorFlow is known as <strong class="keyWord">AutoGraph</strong>.”</p>
    <div class="note">
      <p class="normal">Warnings or important notes appear like this.</p>
    </div>
    <div class="packt_tip">
      <p class="normal">Tips and tricks appear like this.</p>
    </div>
    <h1 id="_idParaDest-11" class="heading-1">Get in touch</h1>
    <p class="normal">Feedback from our readers is always welcome.</p>
    <p class="normal"><strong class="keyWord">General feedback</strong>: Email <code class="inlineCode">feedback@packtpub.com</code>, and mention the book’s title in the subject of your message. If you have questions about any aspect of this book, please email us at <code class="inlineCode">questions@packtpub.com</code>.</p>
    <p class="normal"><strong class="keyWord">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book we would be grateful if you would report this to us. Please visit <span class="url">http://www.packtpub.com/submit-errata</span>, select your book, click on the Errata Submission Form link, and enter the details.</p>
    <p class="normal"><strong class="keyWord">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <code class="inlineCode">copyright@packtpub.com</code> with a link to the material.</p>
    <p class="normal"><strong class="keyWord">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <span class="url">http://authors.packtpub.com</span>.</p>
  </div>
  <div id="_idContainer006">
    <h1 id="_idParaDest-12" class="heading-1">Share your thoughts</h1>
    <p class="normal">Once you’ve read <em class="italic">Natural Language Processing with TensorFlow, Second Edition</em>, we’d love to hear your thoughts! Please <a href="https://packt.link/r/1838641351"><span class="url">click here to go straight to the Amazon review page</span></a> for this book and share your feedback.</p>
    <p class="normal">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p>
  </div>
</body></html>