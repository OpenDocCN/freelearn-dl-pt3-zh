- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Chapter 3
  prefs: []
  type: TYPE_NORMAL
- en: What's a stochastic policy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's a policy defined in terms of a probability distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How can a return be defined in terms of the return at the next time step?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/a8f2ea82-7dd4-4a28-9c02-a139773735e5.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: Why is the Bellman equation so important?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because it provides a general formula to compute the value of a state using
    the current reward and the value of the subsequent state.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Which are the limiting factors of DP algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to a complexity explosion with the number of states, they have to be limited.
    The other constraint is that the dynamics of the system have to be fully known.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's policy evaluation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is an iterative method to compute the value function for a given policy using
    the Bellman equations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How does policy iteration and value iteration differs?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Policy iteration alternate between policy evaluation and policy improvement,
    value iteration instead, combine the two in a single update using the max function.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 4
  prefs: []
  type: TYPE_NORMAL
- en: What's the main property of the MC method used in RL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The estimation of the value function as the average return from a state.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are MC methods offline?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because they update the state value only when the complete trajectory is available.
    Thus they have to wait until the end of the episode.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the two main ideas of TD learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They combine the ideas of sampling and bootstrapping
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the differences between MC and TD?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MC learn from the full trajectory, whereas TD learn at every step acquiring
    knowledge also from an incomplete trajectory.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is exploration important in TD learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the TD update is done only on the action-state visited, so if some of
    them has not been discovered, in the absence of an exploration strategy they will
    never be visited. Thus, some good policy may not be discovered.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why Q-learning is off-policy?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the Q-learning update is done independently of the behavior policy.
    It uses the greedy policy of the max operation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 5
  prefs: []
  type: TYPE_NORMAL
- en: What arise of the deadly triad problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When off-policy learning are combined with function approximation and boostrapping.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How DQN overcome the instabilities?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a replay buffer and a separate online and target network.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the moving target problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's a problem that arises when the target values aren't fixed and they change
    as the network is optimized.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How the moving target problem is mitigated in DQN?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing a target network that is updated less frequently than the online
    network.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the optimization procedure used in DQN?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A mean square error loss function is optimized through stochastic gradient descent,
    an iterative method that performs gradient descent on a batch.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the definition of a state-action advantage value function?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/f4a26ee4-c9ad-415b-91a1-0edc7a8c48f8.png)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_IMG
- en: Chapter 6
  prefs: []
  type: TYPE_NORMAL
- en: How PG algorithms maximize the objective function?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They do it by taking a step in the opposite direction of the objective function's
    derivative. The step is proportional to the return.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the main intuition behind PG algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encourage good actions and dissuade the agent from the bad ones.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why introducing a baseline in REINFORCE it remains unbiased?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because in expectation ![](img/d61043ed-4794-4c60-8ea9-3bda3f910e9b.png)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To which broader class of algorithms belong to REINFORCE?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is a Monte Carlo method as it relies on full trajectories like MC methods
    do.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How the critic in AC methods differs from a value function used as a baseline
    in REINFORCE?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides the learned function is the same, the critic uses the approximated value
    function for bootstrap the action-state value instead in REINFORCE (but also in
    AC) it is used as a baseline to reduce the variance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If you had to develop an algorithm for an agent that has to learn to move, would
    you prefer REINFORCE or AC?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should first try an actor-critic algorithm as the agent has to learn a continuous
    task.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Could you use an n-step Actor-Critic algorithm as a REINFORCE algorithm?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, you could as far as ![](img/07bc1b22-035f-494f-b830-2a334312721a.png) is
    greater than the maximum possible number of steps in the environment.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 7
  prefs: []
  type: TYPE_NORMAL
- en: How can a policy neural network control a continuous agent?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way to do it is to predict the mean and the standard deviation that describe
    a Gaussian distribution. The standard deviation could either be conditioned on
    a state (the input of the neural network) or be a standalone parameter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the KL divergence?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a measure of proximity of two probability distributions.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the main idea behind TRPO?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To optimize the new objective function in a region near the old probability
    distribution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How is the KL divergence used in TRPO?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is used as a hard constraint to limit the digression between an old and a
    new policy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the main benefit of PPO?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It uses only a first-order optimization that increase the simplicity of the
    algorithm and has a better sample efficiency and performance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How does PPO achieve good sample efficiency?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It run minibatch updates several times exploiting better the data.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 8
  prefs: []
  type: TYPE_NORMAL
- en: Which is the primary limitation of Q-learning algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ther action space has to be discrete and small in order to compute the global
    maximum.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why are stochastic gradient algorithms sample inefficient?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because the are on-policy and need new data every time the policy changes.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How does deterministic policy gradient overcome the maximization problem?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DPG model the policy as a deterministic function predicting only a deterministic
    action and the deterministic policy gradient theorem gives a way to compute the
    gradient used to update the policy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How does DPG guarantee enough exploration?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By adding noise into the deterministic policy or by learning a different behavior
    policy.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What DDPG stands for? And what is its main contribution?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DDPG stands for Deep Deterministic Policy Gradient and is an algorithm that
    adapts the deterministic policy gradient to work with deep neural networks. They
    use new strategies to stabilize and speed up learning.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Which problems does TD3 propose to minimize?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overestimation bias common in Q-learning and high variance estimates.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What new mechanisms does TD3 employ?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To reduce the overestimation bias, they use a Clipped Double Q-learning while
    they address the variance problem with a delayed policy update and a smoothing
    regularization technique.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 9
  prefs: []
  type: TYPE_NORMAL
- en: Would you use a model-based or a model-free algorithm if you had only 10 games
    to train your agent to play checkers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I would use a model-based algorithm. The model of checkers is known and plan
    on is a feasible task.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the disadvantages of model-based algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, they require more computational power and achieve lower asymptotical
    performance with respect to model-free algorithms.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: If a model of the environment is unknown, how can it be learned?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a dataset is collected through interactions with the real environment,
    the dynamics model can be learned in a usual supervised way.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why data-aggregation methods are used?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because usually the first interactions with the environment are done with a
    naive policy that doesn't explore all of it. Further interactions with a more
    defined policy are required to affine the model of the environment.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How does ME-TRPO stabilize training?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ME-TRPO employs two main features: an ensemble of models and early stopping
    techniques.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why an ensemble of models improve policy learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because predictions that are done by an ensemble of models take into account
    any uncertainty of the single model.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 10
  prefs: []
  type: TYPE_NORMAL
- en: Is imitation learning considered a reinforcement learning technique?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No, because the underlying frameworks are different. The objective of IL isn't
    to maximize the reward as in RL.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Would you use imitation learning to build a unbitable agent in Go?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probably not, because it requires an expert from which to learn. And if the
    agent has to be the best player in the world means that there's no worthy expert.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the full name of DAgger?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dataset aggregations
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the main strength of DAgger?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It overcomes the problem of distribution mismatch by employing the expert to
    teach actively the learner to recover from errors.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Where would you use IRL instead of IL?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In problems where the reward function is easier to learn and where there's the
    necessity to learn a policy better than that of the expert.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 11
  prefs: []
  type: TYPE_NORMAL
- en: What are two alternative algorithms to reinforcement learning for solving sequential
    decision problems?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: evolution strategies and genetic algorithms
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the processes that give birth to new individuals in evolutionary algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The mutation that mutates the gene of a parent and crossover that combines genetic
    information from two parents.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the source of inspiration of evolutionary algorithms like genetic algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolutionary algorithms are principally inspired by biological evolution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How does CMA-ES evolve evolution strategies?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CMA-ES samples new candidate from a multivariate normal distribution with the
    covariance matrix that is adapted to the population.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's one advantage and one disadvantage of evolution strategies?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One advantage is that they are derivative-free methods while a disadvantage
    is that of being sample inefficient.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's the trick used in the "Evolution Strategies as Scalable Alternative to
    Reinforcement Learning" paper to reduce the variance?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: They propose to use mirroring noise and generate an additional mutation with
    a perturbation with the opposite sign.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 12
  prefs: []
  type: TYPE_NORMAL
- en: What's the exploration-exploitation dilemma?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is a decision problem of whether it's better to explore in order to make better
    decisions in the future or exploit the best current option.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What are two exploration strategies that we already used in previous RL algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/618091c9-6f25-492e-a561-fd2e229c3a57.png)-greedy and a strategy that
    introduces some additional noise into the policy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's UCB?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upper Confidence Bound is an optimistic exploration algorithm that estimates
    an upper confidence bound for each value and selects the action that maximizes
    (12.3)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is Montezuma's Revenge or Multi-armed bandit problem more difficult to solve?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Montezuma's Revenge is much more difficult than the multi-armed bandit problem
    just for the fact that the latter is stateless while the former has an astronomical
    number of possible states. Montezuma's Revenge has also more complexity intrinsic
    in the game.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How ESBAS tackle the problem of online RL algorithm selection?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By employing a meta-algorithm that learns which algorithm among a fixed portfolio
    performs better in a given circumstance.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Chapter 13
  prefs: []
  type: TYPE_NORMAL
- en: How would you rank DQN, A2C, and ES based on their sample efficiency?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DQN is the most sample-efficiency followed by A2C and ES.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What would their rank be if rated on the training time and 100 CPUs are available?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ES probably would be the faster to train, then A2C and DQN.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Would you start debugging an RL algorithm on CartPole or  MontezumaRevenge?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CartPole. You should start the debug of an algorithm with an easy task.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Why is it better to use multiple seeds when comparing multiple deep RL algorithms?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results from a single trial can be highly volatile due to the stochasticity
    of the neural network and environment. By averaging multiple random seeds the
    results would approximate the average case.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Does the intrinsic reward help the exploration of an environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yes, this's because the intrinsic reward is a sort of exploration bonus that
    would increase the curiosity of the agent to visit novel states.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What's transfer learning?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the task of efficiently transfer knowledge between two environments.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
