<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer325">
<h1 class="chapterNumber">8</h1>
<h1 class="chapterTitle" id="_idParaDest-233">Autoencoders</h1>
<p class="normal">Autoencoders are neural networks that learn by unsupervised learning, also sometimes called semi-supervised learning, since<a id="_idIndexMarker843"/> the input is treated as the target too. In this chapter, you will learn about and implement different variants of autoencoders and eventually learn how to stack autoencoders. We will also see how autoencoders can be used to create MNIST digits, and finally, also cover the steps involved in building a long short-term memory autoencoder to generate sentence vectors. This chapter includes the following topics:</p>
<ul>
<li class="bulletList">Vanilla autoencoders</li>
<li class="bulletList">Sparse autoencoders</li>
<li class="bulletList">Denoising autoencoders</li>
<li class="bulletList">Convolutional autoencoders</li>
<li class="bulletList">Stacked autoencoders</li>
<li class="bulletList">Generating sentences using LSTM autoencoders</li>
<li class="bulletList">Variational autoencoders for generating images</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp8"><span class="url">https://packt.link/dltfchp8</span></a></p>
</div>
<p class="normal">Let’s begin!</p>
<h1 class="heading-1" id="_idParaDest-234">Introduction to autoencoders</h1>
<p class="normal">Autoencoders are a class of neural networks that attempt to <a id="_idIndexMarker844"/>recreate input as their target using backpropagation. An autoencoder consists of two parts: an encoder and a decoder. The encoder will read the input and compress it to a compact representation, and the decoder will read the compact representation and recreate the input from it. In other words, the autoencoder tries to learn the identity function by minimizing the reconstruction error. </p>
<p class="normal">They have an inherent capability to learn a compact representation of data. They are at the center of deep belief networks and find applications in image reconstruction, clustering, machine translation, and much more.</p>
<p class="normal">You might think that implementing an identity function using deep neural networks is boring; however, the way in which this is done makes it interesting. The number of hidden units in the autoencoder is typically fewer than the number of input (and output) units. This forces the encoder to<a id="_idIndexMarker845"/> learn a compressed representation of the input, which the decoder reconstructs. If there is a structure in the input data in the form of correlations between input features, then the autoencoder will discover some of these correlations, and end up learning a low-dimensional representation of the data similar to that <a id="_idIndexMarker846"/>learned using <strong class="keyWord">principal component analysis</strong> (<strong class="keyWord">PCA</strong>).</p>
<p class="normal">While PCA uses linear transformations, autoencoders on the other hand use non-linear transformations.</p>
<p class="normal">Once the autoencoder is trained, we would typically just discard the decoder component and use the encoder component to generate compact representations of the input. Alternatively, we could use the encoder as a feature detector that generates a compact, semantically rich representation of our input and build a classifier by attaching a softmax classifier to the hidden layer.</p>
<p class="normal">The encoder and decoder components of an autoencoder can be implemented using either dense, convolutional, or recurrent networks, depending on the kind of data that is being modeled. For example, dense networks might be a good choice for autoencoders used to<a id="_idIndexMarker847"/> build <strong class="keyWord">collaborative filtering</strong> (<strong class="keyWord">CF</strong>) models, where we learn a compressed model of user preferences based on actual sparse user ratings. Similarly, convolutional neural networks may be appropriate for the use case described in the article <em class="italic">iSee: Using Deep Learning to Remove Eyeglasses from Faces</em>, by M. Runfeldt. Recurrent networks, on the other hand, are a good choice for autoencoders working on sequential or text data, such as Deep Patient (<em class="italic">Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records</em>, Miotto et al.) and skip-thought vectors.</p>
<p class="normal">We can think of autoencoders as consisting of two cascaded networks. The first network is an encoder; it takes the input <em class="italic">x</em>, and encodes it using a transformation <em class="italic">h</em> to an encoded signal <em class="italic">y</em>, that is:</p>
<p class="center"><em class="italic">y= h</em>(<em class="italic">x</em>)</p>
<p class="normal">The second network uses the encoded signal <em class="italic">y</em> as its input and performs another transformation <em class="italic">f</em> to get a reconstructed signal <em class="italic">r</em>, that is:</p>
<p class="center"><em class="italic">r= f</em>(<em class="italic">y</em>)<em class="italic"> = f</em>(<em class="italic">h</em>(<em class="italic">x</em>))</p>
<p class="normal">We define error, <em class="italic">e</em>, as the difference between the original input <em class="italic">x</em> and the reconstructed signal <em class="italic">r, e= x- r</em>. The network then learns by reducing the loss function (for example, <strong class="keyWord">mean squared error</strong> (<strong class="keyWord">MSE</strong>)), and the error is<a id="_idIndexMarker848"/> propagated backward to the hidden layers as<a id="_idIndexMarker849"/> in the case of <strong class="keyWord">multilayer perceptrons</strong> (<strong class="keyWord">MLPs</strong>).</p>
<p class="normal">Depending upon the actual dimensions of the encoded layer with respect to the input, the loss function, and<a id="_idIndexMarker850"/> constraints, there are various types of autoencoders: variational autoencoders, sparse autoencoders, denoising autoencoders, and convolution autoencoders.</p>
<p class="normal">Autoencoders can also be stacked by successively stacking encoders that compress their input to smaller and smaller representations, then stacking decoders in the opposite sequence. Stacked autoencoders have greater expressive power and the successive layers of representations capture a hierarchical grouping of the input, similar to the convolution and pooling operations in convolutional neural networks.</p>
<p class="normal">Stacked autoencoders used to be trained layer by layer. For example, in the network in <em class="italic">Figure 8.1</em>, we would first train layer <strong class="keyWord">X</strong> to reconstruct layer <strong class="keyWord">X’</strong> using the hidden layer <strong class="keyWord">H1 </strong>(ignoring <strong class="keyWord">H2</strong>). We would then train layer <strong class="keyWord">H1</strong> to reconstruct layer <strong class="keyWord">H1’</strong> using the hidden layer <strong class="keyWord">H2</strong>. Finally, we would stack all the layers together in the configuration shown and fine-tune it to reconstruct <strong class="keyWord">X’</strong> from <strong class="keyWord">X</strong>. With better activation and regularization functions nowadays, however, it is quite common to train these networks in totality:</p>
<figure class="mediaobject"><img alt="Diagram, shape  Description automatically generated" height="455" src="../Images/B18331_08_01.png" width="380"/></figure>
<p class="packt_figref">Figure 8.1: Visualization of stacked autoencoders</p>
<p class="normal">In this chapter, we will learn <a id="_idIndexMarker851"/>about these variations in autoencoders and implement them using TensorFlow.</p>
<h1 class="heading-1" id="_idParaDest-235">Vanilla autoencoders</h1>
<p class="normal">The vanilla autoencoder, as proposed by Hinton in his 2006 paper <em class="italic">Reducing the Dimensionality of Data with Neural Networks</em>, consists of one<a id="_idIndexMarker852"/> hidden layer only. The number of neurons in the hidden layer is fewer than the number of neurons in the input (or output) layer.</p>
<p class="normal">This results in producing a bottleneck effect in the flow of information in the network. The hidden layer (<em class="italic">y</em>) between the encoder input<a id="_idIndexMarker853"/> and decoder output is also called the “bottleneck layer.” Learning in the autoencoder consists of developing a compact representation of the input signal at the hidden layer so that the output layer can faithfully reproduce the original input.</p>
<p class="normal">In <em class="italic">Figure 8.2</em>, you can see the architecture of a vanilla autoencoder:</p>
<figure class="mediaobject"><img alt="Chart, waterfall chart  Description automatically generated" height="465" src="../Images/B18331_08_02.png" width="735"/></figure>
<p class="packt_figref">Figure 8.2: Architecture of the vanilla autoencoder</p>
<p class="normal">Let’s try to build a vanilla autoencoder. While in the paper Hinton used it for dimension reduction, in the code to follow, we will use autoencoders for image reconstruction. We will train the autoencoder on the <a id="_idIndexMarker854"/>MNIST database and will use it to reconstruct the test images. In the code, we will use the TensorFlow Keras <code class="inlineCode">Layers</code> class to build our own encoder and decoder layers, so firstly let’s learn a little about the <code class="inlineCode">Layers</code> class.</p>
<h2 class="heading-2" id="_idParaDest-236">TensorFlow Keras layers ‒ defining custom layers</h2>
<p class="normal">TensorFlow provides an easy way to <a id="_idIndexMarker855"/>define your own custom layer both from scratch or as a composition of existing layers. The TensorFlow Keras <code class="inlineCode">layers</code> package defines a <code class="inlineCode">Layers</code> object. We can make our own layer by simply making it a subclass of the <code class="inlineCode">Layers</code> class. It is necessary to define the dimensions of the output while <a id="_idIndexMarker856"/>defining the layer. Though input dimensions are optional, if you do not define them, it will infer them automatically from the data. To build our own layer we will need to implement three methods:</p>
<ul>
<li class="bulletList"><code class="inlineCode">__init__()</code>: Here, you define all input-independent initializations.</li>
<li class="bulletList"><code class="inlineCode">build()</code>: Here, we define the shapes of input tensors and can perform rest initializations if required. In our example, since we are not explicitly defining input shapes, we need not define the <code class="inlineCode">build()</code> method.</li>
<li class="bulletList"><code class="inlineCode">call()</code>: This is where the forward computation is performed.</li>
</ul>
<p class="normal">Using the <code class="inlineCode">tensorflow.keras.layers.Layer</code> class, we now define the encoder and decoder layers. First let’s start with <a id="_idIndexMarker857"/>the encoder layer. We import <code class="inlineCode">tensorflow.keras</code> as <code class="inlineCode">K</code>, and create an <code class="inlineCode">Encoder</code> class. The <code class="inlineCode">Encoder</code> takes in the input and generates the hidden or the bottleneck layer as the output:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span>(K.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_dim</span>):
        <span class="hljs-built_in">super</span>(Encoder, self).__init__()
        self.hidden_layer = K.layers.Dense(units=hidden_dim, activation=tf.nn.relu)
<span class="hljs-keyword">    def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, input_features</span>):
        activation = self.hidden_layer(input_features)
        <span class="hljs-keyword">return</span> activation
</code></pre>
<p class="normal">Next, we define the <code class="inlineCode">Decoder</code> class; this class takes in the output from the <code class="inlineCode">Encoder</code> and then passes it through a fully connected <a id="_idIndexMarker858"/>neural network. The aim is to be able to reconstruct the input to the <code class="inlineCode">Encoder</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(K.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_dim, original_dim</span>):
        <span class="hljs-built_in">super</span>(Decoder, self).__init__()
        self.output_layer = K.layers.Dense(units=original_dim, activation=tf.nn.relu)
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, encoded</span>):
        activation = self.output_layer(encoded)
        <span class="hljs-keyword">return</span> activation
</code></pre>
<p class="normal">Now that we have both the encoder and decoder defined we use the <code class="inlineCode">tensorflow.keras.Model</code> object to build the autoencoder model. You can see in the following code that in the <code class="inlineCode">__init__()</code> function we instantiate the encoder and decoder objects, and in the <code class="inlineCode">call()</code> method we define the signal flow. Also notice the member list <code class="inlineCode">self.loss</code> initialized in the <code class="inlineCode">_init__()</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Autoencoder</span>(K.Model):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_dim, original_dim</span>):
        <span class="hljs-built_in">super</span>(Autoencoder, self).__init__()
        self.loss = []
        self.encoder = Encoder(hidden_dim=hidden_dim)
        self.decoder = Decoder(hidden_dim=hidden_dim, original_dim=original_dim)
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, input_features</span>):
        encoded = self.encoder(input_features)
        reconstructed = self.decoder(encoded)
        <span class="hljs-keyword">return</span> reconstructed
</code></pre>
<p class="normal">In the next section, we will<a id="_idIndexMarker859"/> use the autoencoder that we defined here<a id="_idIndexMarker860"/> to reconstruct handwritten digits.</p>
<h2 class="heading-2" id="_idParaDest-237">Reconstructing handwritten digits using an autoencoder</h2>
<p class="normal">Now that we have our model autoencoder with<a id="_idIndexMarker861"/> its layer encoder and decoder ready, let us try to reconstruct handwritten digits. The complete code is <a id="_idIndexMarker862"/>available in the GitHub repo of the chapter in the notebook <code class="inlineCode">VanillaAutoencoder.ipynb</code>. The code will require the NumPy, TensorFlow, and Matplotlib modules:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow.keras <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
</code></pre>
<p class="normal">Before starting with the actual implementation, let’s also define some hyperparameters. If you play around with them, you will notice that even though the architecture of your model remains the same, there is a significant change in model performance. Hyperparameter tuning (refer to <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, for more details) is one of the important steps in deep learning. For reproducibility, we set the seeds for random calculation:</p>
<pre class="programlisting code"><code class="hljs-code">np.random.seed(<span class="hljs-number">11</span>)
tf.random.set_seed(<span class="hljs-number">11</span>)
batch_size = <span class="hljs-number">256</span>
max_epochs = <span class="hljs-number">50</span>
learning_rate = <span class="hljs-number">1e-3</span>
momentum = <span class="hljs-number">8e-1</span>
hidden_dim = <span class="hljs-number">128</span>
original_dim = <span class="hljs-number">784</span>
</code></pre>
<p class="normal">For training data, we are using the MNIST dataset available in the TensorFlow datasets. We normalize the data so that pixel values lie between [0,1]; this is achieved by simply dividing each pixel element by 255.</p>
<p class="normal">We reshape the tensors from 2D to 1D. We employ the <code class="inlineCode">from_tensor_slices</code> function to generate a batched dataset with the training dataset sliced along its first dimension (slices of tensors). Also note that we are not using one-hot encoded labels; this is because we are not using labels to train the network since autoencoders learn via unsupervised learning:</p>
<pre class="programlisting code"><code class="hljs-code">(x_train, _), (x_test, _) = K.datasets.mnist.load_data()
x_train = x_train / <span class="hljs-number">255.</span>
x_test = x_test / <span class="hljs-number">255.</span>
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)
x_train = np.reshape(x_train, (x_train.shape[<span class="hljs-number">0</span>], <span class="hljs-number">784</span>))
x_test = np.reshape(x_test, (x_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">784</span>))
training_dataset = tf.data.Dataset.from_tensor_slices(x_train).batch(batch_size)
</code></pre>
<p class="normal">Now we instantiate our autoencoder <a id="_idIndexMarker863"/>model object and define the loss and optimizers to be used for training. Observe the<a id="_idIndexMarker864"/> formulation of the loss function carefully; it is simply the difference between the original image and the reconstructed image. You may find that the term <em class="italic">reconstruction loss</em> is also used to describe it in many books and papers:</p>
<pre class="programlisting code"><code class="hljs-code">autoencoder = Autoencoder(hidden_dim=hidden_dim, original_dim=original_dim)
opt = tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">1e-2</span>)
<span class="hljs-keyword">def</span> <span class="hljs-title">loss</span>(<span class="hljs-params">preds, real</span>):
    <span class="hljs-keyword">return</span> tf.reduce_mean(tf.square(tf.subtract(preds, real)))
</code></pre>
<p class="normal">Instead of using the auto-training loop, for our custom autoencoder model, we will define a custom training. We use <code class="inlineCode">tf.GradientTape</code> to record the gradients as they are calculated and implicitly apply the gradients to all the trainable variables of our model:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span>(<span class="hljs-params">loss, model, opt, original</span>):
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        preds = model(original)
        reconstruction_error = loss(preds, original)
        gradients = tape.gradient(reconstruction_error, model.trainable_variables)
        gradient_variables = <span class="hljs-built_in">zip</span>(gradients, model.trainable_variables)
    opt.apply_gradients(gradient_variables)
    <span class="hljs-keyword">return</span> reconstruction_error
</code></pre>
<p class="normal">The preceding <code class="inlineCode">train()</code> function will be invoked in a training loop, with the dataset fed to the model in batches:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">train_loop</span>(<span class="hljs-params">model, opt, loss, dataset, epochs=</span><span class="hljs-number">20</span>):
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):
        epoch_loss = <span class="hljs-number">0</span>
        <span class="hljs-keyword">for</span> step, batch_features <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataset):
            loss_values = train(loss, model, opt, batch_features)
            epoch_loss += loss_values
        model.loss.append(epoch_loss)
        <span class="hljs-built_in">print</span>(<span class="hljs-string">'Epoch {}/{}. Loss: {}'</span>.<span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>, epochs, epoch_loss.numpy()))
</code></pre>
<p class="normal">Let’s now train our<a id="_idIndexMarker865"/> autoencoder:</p>
<pre class="programlisting code"><code class="hljs-code">train_loop(autoencoder, opt, loss, training_dataset, epochs=max_epochs)
</code></pre>
<p class="normal">And plot our training graph:</p>
<pre class="programlisting code"><code class="hljs-code">plt.plot(<span class="hljs-built_in">range</span>(max_epochs), autoencoder.loss)
plt.xlabel(<span class="hljs-string">'Epochs'</span>)
plt.ylabel(<span class="hljs-string">'Loss'</span>)
plt.show()
</code></pre>
<p class="normal">The training graph is shown as<a id="_idIndexMarker866"/> follows. We can see that loss/cost is decreasing as the network learns and after 50 epochs it is almost constant about a line. This means that further increasing the number of epochs will not be useful. If we want to improve our training further, we should change the hyperparameters like learning rate and <code class="inlineCode">batch_size</code>:</p>
<figure class="mediaobject"><img alt="Chart  Description automatically generated" height="372" src="../Images/B18331_08_03.png" width="551"/></figure>
<p class="packt_figref">Figure 8.3: Loss plot of the vanilla autoencoder</p>
<p class="normal">In <em class="italic">Figure 8.4</em>, you can see the<a id="_idIndexMarker867"/> original (top) and reconstructed (bottom) images; they <a id="_idIndexMarker868"/>are slightly blurred, but accurate:</p>
<pre class="programlisting code"><code class="hljs-code">number = <span class="hljs-number">10</span>  <span class="hljs-comment"># how many digits we will display</span>
plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">4</span>))
<span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(number):
    <span class="hljs-comment"># display original</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, number, index + <span class="hljs-number">1</span>)
    plt.imshow(x_test[index].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>), cmap=<span class="hljs-string">'</span><span class="hljs-string">gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
    <span class="hljs-comment"># display reconstruction</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, number, index + <span class="hljs-number">1</span> + number)
    plt.imshow(autoencoder(x_test)[index].numpy().reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>), cmap=<span class="hljs-string">'gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
plt.show()
</code></pre>
<figure class="mediaobject"><img alt="Text  Description automatically generated with medium confidence" height="100" src="../Images/B18331_08_04.png" width="489"/></figure>
<p class="packt_figref">Figure 8.4: Original and reconstructed images using vanilla autoencoder</p>
<p class="normal">It is interesting to note that in the preceding code we reduced the dimensions of the input from 784 to 128 and our network could still reconstruct the original image. This should give you an idea of the<a id="_idIndexMarker869"/> power of the autoencoder for dimensionality reduction. One advantage of autoencoders over PCA<a id="_idIndexMarker870"/> for dimensionality reduction is that while PCA can only represent linear transformations, we can use non-linear activation functions in autoencoders, thus introducing non-linearities in our encodings:</p>
<figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" height="414" src="../Images/B18331_08_05.png" width="826"/></figure>
<p class="packt_figref">Figure 8.5: LHS image: The two-dimensional code for 500 digits of each class produced by taking the first two principal components of all 60,000 training samples. RHS image: The two-dimensional code found by a 784-500-2 autoencoder</p>
<p class="normal"><em class="italic">Figure 8.5</em> compares the result of a PCA with that of stacked autoencoders with architecture consisting of 784-500-2 (here the numbers represent the size of the encoder layers in each autoencoder; the autoencoders had a symmetric decoder).</p>
<p class="normal">You can see that the colored dots <a id="_idIndexMarker871"/>on the right are<a id="_idIndexMarker872"/> nicely separated, thus stacked autoencoders give much better results compared to PCA. Now that you are familiar with vanilla autoencoders, let us see different variants of autoencoders and their implementation details.</p>
<h1 class="heading-1" id="_idParaDest-238">Sparse autoencoder</h1>
<p class="normal">The autoencoder we covered in the previous section works more like an identity network; it simply reconstructs the input. The<a id="_idIndexMarker873"/> emphasis is on reconstructing the image at the pixel level, and the only constraint is the number of units in the bottleneck layer. While it is interesting, pixel-level reconstruction is primarily a compression mechanism and does not necessarily ensure that the network will learn abstract features from the dataset. We can ensure that a network learns abstract features from the dataset by adding further constraints.</p>
<p class="normal">In sparse autoencoders, a sparse penalty term is added to the reconstruction error. This tries to ensure that fewer units in the bottleneck layer will fire at any given time. We can include the sparse penalty within the encoder layer itself. </p>
<p class="normal">In the following code, you can see that the dense layer of <code class="inlineCode">Encoder</code> now has an additional parameter, <code class="inlineCode">activity_regularizer</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">SparseEncoder</span>(K.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_dim</span>):
        <span class="hljs-comment"># encoder initializer</span>
        <span class="hljs-built_in">super</span>(SparseEncoder, self).__init__()
        self.hidden_layer = K.layers.Dense(units=hidden_dim, activation=tf.nn.relu, activity_regularizer=regularizers.l1(<span class="hljs-number">10e-5</span>))
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, input_features</span>):
        <span class="hljs-comment"># forward function</span>
        activation = self.hidden_layer(input_features)
        <span class="hljs-keyword">return</span> activation
</code></pre>
<p class="normal">The activity regularizer tries to reduce the layer output (refer to <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>). It will reduce both the weights and bias of the fully connected layer to ensure that the output is as small as it can be. TensorFlow supports three types of <code class="inlineCode">activity_regularizer</code>:</p>
<ul>
<li class="bulletList"><code class="inlineCode">l1</code>: Here the activity is computed as the sum of absolute values</li>
<li class="bulletList"><code class="inlineCode">l2</code>: The activity here is calculated as the sum of the squared values</li>
<li class="bulletList"><code class="inlineCode">l1_l2</code>: This includes both L1 and L2 terms</li>
</ul>
<p class="normal">Keeping the rest of the code the same, and just changing the encoder, you can get the sparse autoencoder from the vanilla autoencoder. The complete code for the sparse autoencoder is in the Jupyter <a id="_idIndexMarker874"/>notebook <code class="inlineCode">SparseAutoencoder.ipynb</code>.</p>
<p class="normal">Alternatively, you can explicitly add a regularization term for sparsity in the loss function. To do so you will need to implement the regularization for the sparsity term as a function. If <em class="italic">m</em> is the total number of input patterns, then we can define a quantity <img alt="" height="46" src="../Images/B18331_08_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/> (you can check the mathematical details in Andrew Ng’s lecture here: <a href="https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf"><span class="url">https://web.stanford.edu/class/cs294a/sparseAutoencoder_2011new.pdf</span></a>), which measures the net activity (how many times on average it fires) for each hidden layer unit. The basic idea is to put a constraint <img alt="" height="46" src="../Images/B18331_08_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/>, such that it is equal to the sparsity parameter <img alt="" height="46" src="../Images/B18331_08_003.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/>. This results in adding a regularization term for sparsity in the loss function so that now the loss function becomes:</p>
<p class="center"><em class="italic">loss = Mean squared error + Regularization for sparsity parameter</em></p>
<p class="normal">This regularization term will penalize the network if <img alt="" height="46" src="../Images/B18331_08_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/> deviates from <img alt="" height="46" src="../Images/B18331_08_003.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/>. One standard way to do this is to<a id="_idIndexMarker875"/> use <strong class="keyWord">Kullback-Leiber</strong> (<strong class="keyWord">KL</strong>) divergence (you can learn more about KL divergence from this interesting lecture: <a href="https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf"><span class="url">https://www.stat.cmu.edu/~cshalizi/754/2006/notes/lecture-28.pdf</span></a>) between <img alt="" height="46" src="../Images/B18331_08_003.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> and <img alt="" height="46" src="../Images/B18331_08_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/>.</p>
<p class="normal">Let’s explore the KL divergence, <em class="italic">D</em><sub class="subscript">KL</sub>, a little more. It is a non-symmetric measure of the difference between the two distributions, in our case, <img alt="" height="46" src="../Images/B18331_08_003.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> and <img alt="" height="46" src="../Images/B18331_08_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/>. When <img alt="" height="46" src="../Images/B18331_08_003.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> and <img alt="" height="46" src="../Images/B18331_08_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/> are equal then the difference is zero; otherwise, it increases monotonically as <img alt="" height="46" src="../Images/B18331_08_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/> diverges from <img alt="" height="46" src="../Images/B18331_08_003.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/>. Mathematically, it is expressed as:</p>
<p class="center"><img alt="" height="104" src="../Images/B18331_08_014.png" style="height: 2.60em !important; vertical-align: 0.05em !important;" width="708"/></p>
<p class="normal">We add this to the loss to implicitly include the sparse term. We will need to fix a constant value for the sparsity term <img alt="" height="46" src="../Images/B18331_08_003.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> and compute <img alt="" height="46" src="../Images/B18331_08_001.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="100"/> using the encoder output.</p>
<p class="normal">The compact representation of the inputs is stored in weights. Let us visualize the weights learned by the network. The following are the weights of the encoder layer for the standard and sparse autoencoders respectively.</p>
<p class="normal">We can see that in the standard<a id="_idIndexMarker876"/> autoencoder (a) many hidden units have very large weights (brighter), suggesting that they are overworked, while all the hidden units of the sparse autoencoder (b) learn the input representation almost equally, and we see a more even color distribution:</p>
<figure class="mediaobject"><img alt="A picture containing appliance, grate  Description automatically generated" height="406" src="../Images/B18331_08_06.png" width="799"/></figure>
<p class="packt_figref">Figure 8.6: Encoder weight matrix for (a) standard autoencoder and (b) sparse autoencoder</p>
<p class="normal">Now that we have learned about sparse autoencoders, we next move to a case where autoencoders can learn to remove noise from the image.</p>
<h1 class="heading-1" id="_idParaDest-239">Denoising autoencoders</h1>
<p class="normal">The two autoencoders that we have covered in the previous sections are examples of undercomplete autoencoders, because the<a id="_idIndexMarker877"/> hidden layer in them has lower dimensionality compared to the input (output) layer. Denoising autoencoders belong to the class of overcomplete autoencoders because they work better when the dimensions of the hidden layer are more than the input layer.</p>
<p class="normal">A denoising autoencoder learns from a corrupted (noisy) input; it feeds its encoder network the noisy input, and then the reconstructed image from the decoder is compared with the original input. The idea is that this will help the network learn how to denoise an input. It will no longer just make pixel-wise comparisons, but in order to denoise, it will learn the information of neighboring pixels as well.</p>
<p class="normal">A denoising autoencoder has two main differences from other autoencoders: first, <code class="inlineCode">n_hidden</code>, the number of hidden units in the bottleneck layer is greater than the number of units in the input layer, <code class="inlineCode">m</code>, that is, <code class="inlineCode">n_hidden</code> &gt; <code class="inlineCode">m</code>. Second, the input to the encoder is corrupted input. </p>
<p class="normal">To do this, we add a noise term in both the test and training images:</p>
<pre class="programlisting code"><code class="hljs-code">noise = np.random.normal(loc=<span class="hljs-number">0.5</span>, scale=<span class="hljs-number">0.5</span>, size=x_train.shape)
x_train_noisy = x_train + noise
noise = np.random.normal(loc=<span class="hljs-number">0.5</span>, scale=<span class="hljs-number">0.5</span>, size=x_test.shape)
x_test_noisy = x_test + noise
x_train_noisy = np.clip(x_train_noisy, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>)
x_test_noisy = np.clip(x_test_noisy, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>)
</code></pre>
<p class="normal">Let us see the denoising autoencoder in action next.</p>
<h2 class="heading-2" id="_idParaDest-240">Clearing images using a denoising autoencoder</h2>
<p class="normal">Let us use the denoising autoencoder to<a id="_idIndexMarker878"/> clear the handwritten MNIST digits:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">We start by importing the required modules:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow.keras <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
</code></pre>
</li>
<li class="numberedList">Next, we define the hyperparameters for our model:
        <pre class="programlisting code"><code class="hljs-code">np.random.seed(<span class="hljs-number">11</span>)
tf.random.set_seed(<span class="hljs-number">11</span>)
batch_size = <span class="hljs-number">256</span>
max_epochs = <span class="hljs-number">50</span>
learning_rate = <span class="hljs-number">1e-3</span>
momentum = <span class="hljs-number">8e-1</span>
hidden_dim = <span class="hljs-number">128</span>
original_dim = <span class="hljs-number">784</span>
</code></pre>
</li>
<li class="numberedList">We read in the MNIST dataset, normalize it, and introduce noise to it:
        <pre class="programlisting code"><code class="hljs-code">(x_train, _), (x_test, _) = K.datasets.mnist.load_data()
x_train = x_train / <span class="hljs-number">255.</span>
x_test = x_test / <span class="hljs-number">255.</span>
x_train = x_train.astype(np.float32)
x_test = x_test.astype(np.float32)
x_train = np.reshape(x_train, (x_train.shape[<span class="hljs-number">0</span>], <span class="hljs-number">784</span>))
x_test = np.reshape(x_test, (x_test.shape[<span class="hljs-number">0</span>], <span class="hljs-number">784</span>))
<span class="hljs-comment"># Generate corrupted MNIST images by adding noise with normal dist</span>
<span class="hljs-comment"># centered at 0.5 and std=0.5</span>
noise = np.random.normal(loc=<span class="hljs-number">0.5</span>, scale=<span class="hljs-number">0.5</span>, size=x_train.shape)
x_train_noisy = x_train + noise
noise = np.random.normal(loc=<span class="hljs-number">0.5</span>, scale=<span class="hljs-number">0.5</span>, size=x_test.shape)
x_test_noisy = x_test + noise
</code></pre>
</li>
<li class="numberedList">We use the same encoder, decoder, and autoencoder<a id="_idIndexMarker879"/> classes as defined in the <em class="italic">Vanilla autoencoders </em>section:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Encoder</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span>(K.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_dim</span>):
        <span class="hljs-built_in">super</span>(Encoder, self).__init__()
        self.hidden_layer = K.layers.Dense(units=hidden_dim, activation=tf.nn.relu)
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, input_features</span>):
        activation = self.hidden_layer(input_features)
        <span class="hljs-keyword">return</span> activation
<span class="hljs-comment"># Decoder</span>
<span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(K.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_dim, original_dim</span>):
        <span class="hljs-built_in">super</span>(Decoder, self).__init__()
        self.output_layer = K.layers.Dense(units=original_dim, activation=tf.nn.relu)
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, encoded</span>):
        activation = self.output_layer(encoded)
        <span class="hljs-keyword">return</span> activation
<span class="hljs-keyword">class</span> <span class="hljs-title">Autoencoder</span>(K.Model):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, hidden_dim, original_dim</span>):
        <span class="hljs-built_in">super</span>(Autoencoder, self).__init__()
        self.loss = []
        self.encoder = Encoder(hidden_dim=hidden_dim)
        self.decoder = Decoder(hidden_dim=hidden_dim, original_dim=original_dim)
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, input_features</span>):
        encoded = self.encoder(input_features)
        reconstructed = self.decoder(encoded)
        <span class="hljs-keyword">return</span> reconstructed
</code></pre>
</li>
<li class="numberedList">Next, we create the model <a id="_idIndexMarker880"/>and define the loss and optimizers to be used. Notice that this time, instead of writing the custom training loop, we are using the easier Keras inbuilt <code class="inlineCode">compile()</code> and <code class="inlineCode">fit()</code> methods:
        <pre class="programlisting code"><code class="hljs-code">model = Autoencoder(hidden_dim=hidden_dim, original_dim=original_dim)
model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'mse'</span>, optimizer=<span class="hljs-string">'adam'</span>)
loss = model.fit(x_train_noisy,
            x_train,
            validation_data=(x_test_noisy, x_test),
            epochs=max_epochs,
            batch_size=batch_size)
</code></pre>
</li>
<li class="numberedList">Now let’s plot the training loss:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(<span class="hljs-built_in">range</span>(max_epochs), loss.history[<span class="hljs-string">'loss'</span>])
plt.xlabel(<span class="hljs-string">'Epochs'</span>)
plt.ylabel(<span class="hljs-string">'Loss'</span>)
plt.show()
</code></pre>
</li>
</ol>
<p class="normal"><em class="italic">Figure 8.7</em> shows the loss over epochs:</p>
<figure class="mediaobject"><img alt="Chart, histogram  Description automatically generated" height="423" src="../Images/B18331_08_07.png" width="671"/></figure>
<p class="packt_figref">Figure 8.7: Loss plot of a denoising autoencoder</p>
<p class="normal">And finally, let’s see our model<a id="_idIndexMarker881"/> in action: </p>
<pre class="programlisting code"><code class="hljs-code">number = <span class="hljs-number">10</span>  <span class="hljs-comment"># how many digits we will display</span>
plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">4</span>))
<span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(number):
    <span class="hljs-comment"># display original</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, number, index + <span class="hljs-number">1</span>)
    plt.imshow(x_test_noisy[index].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>), cmap=<span class="hljs-string">'gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
    <span class="hljs-comment"># display reconstruction</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, number, index + <span class="hljs-number">1</span> + number)
    plt.imshow(model(x_test_noisy)[index].numpy().reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>), cmap=<span class="hljs-string">'gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
plt.show()
</code></pre>
<p class="normal">The top row shows the input noisy image, and the bottom row shows cleaned images produced from our trained denoising autoencoder:</p>
<figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" height="128" src="../Images/B18331_08_08.png" width="627"/></figure>
<p class="packt_figref">Figure 8.8: The noisy input images and corresponding denoised reconstructed images</p>
<p class="normal">An impressive reconstruction of images <a id="_idIndexMarker882"/>from noisy images, I’m sure you’ll agree. You can access the code in the notebook <code class="inlineCode">DenoisingAutoencoder.ipynb</code> if you want to play around with it.</p>
<h1 class="heading-1" id="_idParaDest-241">Stacked autoencoder</h1>
<p class="normal">Until now, we have restricted ourselves to autoencoders with only one hidden layer. We can build deep autoencoders by stacking many layers of both encoders and decoders; such an autoencoder is called a stacked <a id="_idIndexMarker883"/>autoencoder. The features extracted by one encoder are passed on to the next encoder as input. The stacked autoencoder can be trained as a whole network with the aim of minimizing the reconstruction error. Alternatively, each individual encoder/decoder network can first be pretrained using the unsupervised method you learned earlier, and then the complete network can be fine-tuned. When the deep autoencoder network is<a id="_idIndexMarker884"/> a convolutional network, we call it a <strong class="keyWord">convolutional autoencoder</strong>. Let us implement a convolutional autoencoder in TensorFlow next.</p>
<h2 class="heading-2" id="_idParaDest-242">Convolutional autoencoder for removing noise from images</h2>
<p class="normal">In the previous section, we <a id="_idIndexMarker885"/>reconstructed handwritten digits from noisy input images. We used a fully connected network as the encoder and decoder for the work. However, we know that for images, a convolutional network can give better results, so in this section, we will use a convolution network for both the encoder and decoder. To get better results we will use multiple convolution layers in both the encoder and decoder networks; that is, we will make stacks of convolutional layers (along with max pooling or upsampling layers). We will also be training the entire autoencoder as a single entity:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">We import all the required modules and the specific layers from <code class="inlineCode">tensorflow.keras.layers</code>:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow.keras <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense, Conv2D, MaxPooling2D, UpSampling2D
</code></pre>
</li>
<li class="numberedList">We specify our hyperparameters. If you look carefully, the list is slightly different compared to<a id="_idIndexMarker886"/> earlier autoencoder implementations; instead of learning rate and momentum, this time we are concerned with filters of the convolutional layer:
        <pre class="programlisting code"><code class="hljs-code">np.random.seed(<span class="hljs-number">11</span>)
tf.random.set_seed(<span class="hljs-number">11</span>)
batch_size = <span class="hljs-number">128</span>
max_epochs = <span class="hljs-number">50</span>
filters = [<span class="hljs-number">32</span>,<span class="hljs-number">32</span>,<span class="hljs-number">16</span>]
</code></pre>
</li>
<li class="numberedList">In the next step, we read in the data and preprocess it. Again, you may observe a slight variation from the previous code, especially in the way we are adding noise and then limiting the range between [0-1]. We are doing so because in this case, instead of the mean squared error loss, we will be using binary cross-entropy loss and the final output of the decoder will pass through sigmoid activation, restricting it between [0-1]:
        <pre class="programlisting code"><code class="hljs-code">(x_train, _), (x_test, _) = K.datasets.mnist.load_data()
x_train = x_train / <span class="hljs-number">255.</span>
x_test = x_test / <span class="hljs-number">255.</span>
x_train = np.reshape(x_train, (<span class="hljs-built_in">len</span>(x_train),<span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))
x_test = np.reshape(x_test, (<span class="hljs-built_in">len</span>(x_test), <span class="hljs-number">28</span>, <span class="hljs-number">28</span>, <span class="hljs-number">1</span>))
noise = <span class="hljs-number">0.5</span>
x_train_noisy = x_train + noise * np.random.normal(loc=<span class="hljs-number">0.0</span>, scale=<span class="hljs-number">1.0</span>, size=x_train.shape)
x_test_noisy = x_test + noise * np.random.normal(loc=<span class="hljs-number">0.0</span>, scale=<span class="hljs-number">1.0</span>, size=x_test.shape)
x_train_noisy = np.clip(x_train_noisy, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
x_test_noisy = np.clip(x_test_noisy, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>)
x_train_noisy = x_train_noisy.astype(<span class="hljs-string">'float32'</span>)
x_test_noisy = x_test_noisy.astype(<span class="hljs-string">'float32'</span>)
<span class="hljs-comment">#print(x_test_noisy[1].dtype)</span>
</code></pre>
</li>
<li class="numberedList">Let us now define our encoder. The encoder consists of three convolutional layers, each followed by a max pooling layer. Since we are using the MNIST dataset the shape of the<a id="_idIndexMarker887"/> input image is 28 × 28 (single channel) and the output image is of size 4 × 4 (and since the last convolutional layer has 16 filters, the image has 16 channels):
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Encoder</span>(K.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, filters</span>):
        <span class="hljs-built_in">super</span>(Encoder, self).__init__()
        self.conv1 = Conv2D(filters=filters[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>)
        self.conv2 = Conv2D(filters=filters[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>)
        self.conv3 = Conv2D(filters=filters[<span class="hljs-number">2</span>], kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>)
        self.pool = MaxPooling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), padding=<span class="hljs-string">'</span><span class="hljs-string">same'</span>)
           
    
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, input_features</span>):
        x = self.conv1(input_features)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.conv3(x)
        x = self.pool(x)
        <span class="hljs-keyword">return</span> x
</code></pre>
</li>
<li class="numberedList">Next comes the decoder. It is the exact opposite of the encoder in design, and instead of max pooling, we are using upsampling to increase the size back. Notice the commented <code class="inlineCode">print</code> statements; you can use them to understand how the shape gets modified after each step. (Alternatively, you can also use the <code class="inlineCode">model.summary</code> function to get the complete model summary.) Also notice that both the encoder and decoder are still classes based on the TensorFlow Keras <code class="inlineCode">Layers</code> class, but now they have multiple layers inside them. So now you know how to build a complex custom layer:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Decoder</span>(K.layers.Layer):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, filters</span>):
        <span class="hljs-built_in">super</span>(Decoder, self).__init__()
        self.conv1 = Conv2D(filters=filters[<span class="hljs-number">2</span>], kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>)
        self.conv2 = Conv2D(filters=filters[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>)
        self.conv3 = Conv2D(filters=filters[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'valid'</span>)
        self.conv4 = Conv2D(<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>, padding=<span class="hljs-string">'same'</span>)
        self.upsample = UpSampling2D((<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, encoded</span>):
        x = self.conv1(encoded)
        <span class="hljs-comment">#print("dx1", x.shape)</span>
        x = self.upsample(x)
        <span class="hljs-comment">#print("dx2", x.shape)</span>
        x = self.conv2(x)
        x = self.upsample(x)
        x = self.conv3(x)
        x = self.upsample(x)
        <span class="hljs-keyword">return</span> self.conv4(x)
</code></pre>
</li>
<li class="numberedList">We combine the encoder and decoder to make an autoencoder model. This remains exactly the same <a id="_idIndexMarker888"/>as before:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">Autoencoder</span>(K.Model):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self, filters</span>):
        <span class="hljs-built_in">super</span>(Autoencoder, self).__init__()
        self.encoder = Encoder(filters)
        self.decoder = Decoder(filters)
    <span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, input_features</span>):
        <span class="hljs-comment">#print(input_features.shape)</span>
        encoded = self.encoder(input_features)
        <span class="hljs-comment">#print(encoded.shape)</span>
        reconstructed = self.decoder(encoded)
        <span class="hljs-comment">#print(reconstructed.shape)</span>
        <span class="hljs-keyword">return</span> reconstructed
</code></pre>
</li>
<li class="numberedList">Now we instantiate our model, then specify the binary cross-entropy as the loss function and Adam as the optimizer in the <code class="inlineCode">compile()</code> method. Then, fit the model to the training dataset:
        <pre class="programlisting code"><code class="hljs-code">model = Autoencoder(filters)
model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'binary_crossentropy'</span>, optimizer=<span class="hljs-string">'adam'</span>)
loss = model.fit(x_train_noisy,
            x_train,
            validation_data=(x_test_noisy, x_test),
            epochs=max_epochs,
            batch_size=batch_size)
</code></pre>
</li>
<li class="numberedList">Plot the loss curve:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(<span class="hljs-built_in">range</span>(max_epochs), loss.history[<span class="hljs-string">'loss'</span>])
plt.xlabel(<span class="hljs-string">'Epochs'</span>)
plt.ylabel(<span class="hljs-string">'Loss'</span>)
plt.show()
</code></pre>
<p class="normal">You can see the loss curve as the<a id="_idIndexMarker889"/> model is trained; in 50 epochs the loss was reduced to 0.0988:</p>
<figure class="mediaobject"><img alt="Chart  Description automatically generated" height="388" src="../Images/B18331_08_09.png" width="582"/></figure>
<p class="packt_figref">Figure 8.9: Loss plot for the convolutional autoencoder</p>
</li>
</ol>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="9">And finally, you can see the wonderful reconstructed images from the noisy input images:
        <pre class="programlisting code"><code class="hljs-code">number = <span class="hljs-number">10</span>  <span class="hljs-comment"># how many digits we will display</span>
plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">4</span>))
<span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(number):
    <span class="hljs-comment"># display original</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, number, index + <span class="hljs-number">1</span>)
    plt.imshow(x_test_noisy[index].reshape(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>), cmap=<span class="hljs-string">'gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
    <span class="hljs-comment"># display reconstruction</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, number, index + <span class="hljs-number">1</span> + number)
    plt.imshow(tf.reshape(model(x_test_noisy)[index], (<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)), cmap=<span class="hljs-string">'gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
plt.show()
</code></pre>
</li>
</ol>
<figure class="mediaobject"><img alt="A picture containing text  Description automatically generated" height="127" src="../Images/B18331_08_10.png" width="623"/></figure>
<p class="packt_figref">Figure 8.10: The inputted noisy images and reconstructed denoised images</p>
<p class="normal">You can see that the images are much<a id="_idIndexMarker890"/> clearer and sharper relative to the previous autoencoders we have covered in this chapter. The magic lies in the stacking of convolutional layers. The code for this section is available in the Jupyter notebook <code class="inlineCode">ConvolutionAutoencoder.ipynb</code>.</p>
<h2 class="heading-2" id="_idParaDest-243">A TensorFlow Keras autoencoder example ‒ sentence vectors</h2>
<p class="normal">In this example, we will build and<a id="_idIndexMarker891"/> train an LSTM-based autoencoder to generate sentence vectors for documents in the Reuters-21578 corpus (<a href="https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection"><span class="url">https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection</span></a>). We have already seen in <em class="chapterRef">Chapter 4</em>, <em class="italic">Word Embeddings</em>, how to represent a word using word embeddings to create vectors that represent the word’s meaning in the context of other words it appears with. Here, we will see how to build similar vectors for sentences. Sentences are sequences of words, so a sentence vector represents the meaning of a sentence.</p>
<p class="normal">The easiest way to build a sentence vector is to just add up the word vectors and divide them by the number of words. However, this treats the sentence as a bag of words, and does not take the order of words into account. Thus, the sentences <em class="italic">The dog bit the man</em> and <em class="italic">The man bit the dog</em> would be treated as identical in this scenario. LSTMs are designed to work with sequence input and do take the order of words into consideration, thus providing<a id="_idIndexMarker892"/> a better and more natural representation of the sentence.</p>
<p class="normal">First, we import the necessary libraries:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
<span class="hljs-keyword">from</span> tensorflow.keras.callbacks <span class="hljs-keyword">import</span> ModelCheckpoint
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Input
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> RepeatVector
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> LSTM
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Bidirectional
<span class="hljs-keyword">from</span> tensorflow.keras.models <span class="hljs-keyword">import</span> Model
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing <span class="hljs-keyword">import</span> sequence
<span class="hljs-keyword">from</span> scipy.stats <span class="hljs-keyword">import</span> describe
<span class="hljs-keyword">import</span> collections
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> nltk
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> os
<span class="hljs-keyword">from</span> time <span class="hljs-keyword">import</span> gmtime, strftime
<span class="hljs-keyword">from</span> tensorflow.keras.callbacks <span class="hljs-keyword">import</span> TensorBoard
<span class="hljs-keyword">import</span> re
<span class="hljs-comment"># Needed to run only once</span>
nltk.download(<span class="hljs-string">'punkt'</span>)
nltk.download(<span class="hljs-string">'reuters'</span>)
<span class="hljs-keyword">from</span> nltk.corpus <span class="hljs-keyword">import</span> reuters
</code></pre>
<p class="normal">In case you are using Google’s Colab to run the code, you will also need to unzip the Reuters corpus by adding the following to the code:</p>
<pre class="programlisting code"><code class="hljs-code">%%capture
!unzip /root/nltk_data/corpora/reuters.<span class="hljs-built_in">zip</span> -d /root/nltk_data/corpora
</code></pre>
<p class="normal">Next, we will be using the GloVe embeddings, so let us download them as well:</p>
<pre class="programlisting code"><code class="hljs-code">!wget http://nlp.stanford.edu/data/glove<span class="hljs-number">.6</span>B.<span class="hljs-built_in">zip</span>
!unzip glove*.<span class="hljs-built_in">zip</span>
</code></pre>
<p class="normal">Now that all our tools are in our workspace, we will first convert each block of text (documents) into a list of sentences, one sentence per line. Also, each word in the sentence is normalized as it is added. The normalization involves removing all numbers and replacing them with the <a id="_idIndexMarker893"/>number <code class="inlineCode">9</code>, then converting the word to lowercase. Simultaneously we also calculate the word frequencies in the same code. The result is the word frequency table, <code class="inlineCode">word_freqs</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">is_number</span>(<span class="hljs-params">n</span>):
    temp = re.sub(<span class="hljs-string">"[.,-/]"</span>, <span class="hljs-string">""</span>,n)
    <span class="hljs-keyword">return</span> temp.isdigit()
<span class="hljs-comment"># parsing sentences and building vocabulary</span>
word_freqs = collections.Counter()
documents = reuters.fileids()
<span class="hljs-comment">#ftext = open("text.tsv", "r")</span>
sents = []
sent_lens = []
num_read = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(documents)):
    <span class="hljs-comment"># periodic heartbeat report</span>
    <span class="hljs-keyword">if</span> num_read % <span class="hljs-number">100</span> == <span class="hljs-number">0</span>:
        <span class="hljs-built_in">print</span>(<span class="hljs-string">"building features from {:d} docs"</span>.<span class="hljs-built_in">format</span>(num_read))
    <span class="hljs-comment"># skip docs without specified topic</span>
    title_body = reuters.raw(documents[i]).lower()
    <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(title_body) == <span class="hljs-number">0</span>:
        <span class="hljs-keyword">continue</span>
    num_read += <span class="hljs-number">1</span>
    <span class="hljs-comment"># convert to list of word indexes</span>
    title_body = re.sub(<span class="hljs-string">"\n"</span>, <span class="hljs-string">""</span>, title_body)
    <span class="hljs-keyword">for</span> sent <span class="hljs-keyword">in</span> nltk.sent_tokenize(title_body):
        <span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> nltk.word_tokenize(sent):
            <span class="hljs-keyword">if</span> is_number(word):
                word = <span class="hljs-string">"9"</span>
            word = word.lower()
            word_freqs[word] += <span class="hljs-number">1</span>
        sents.append(sent)
        sent_lens.append(<span class="hljs-built_in">len</span>(sent))
</code></pre>
<p class="normal">Let us use the preceding generated arrays to get some information about the corpus that will help us figure out good values for the constants for our LSTM network:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(<span class="hljs-string">"Total number of sentences are: {:d} "</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(sents)))
<span class="hljs-built_in">print</span> (<span class="hljs-string">"Sentence distribution min {:d}, max {:d} , mean {:3f}, median {:3f}"</span>.<span class="hljs-built_in">format</span>(np.<span class="hljs-built_in">min</span>(sent_lens), np.<span class="hljs-built_in">max</span>(sent_lens), np.mean(sent_lens), np.median(sent_lens)))
<span class="hljs-built_in">print</span>(<span class="hljs-string">"Vocab size (full) {:d}"</span>.<span class="hljs-built_in">format</span>(<span class="hljs-built_in">len</span>(word_freqs)))
</code></pre>
<p class="normal">This gives us the following information <a id="_idIndexMarker894"/>about the corpus:</p>
<pre class="programlisting con"><code class="hljs-con">Total number of sentences are: 50470 
Sentence distribution min 1, max 3688 , mean 167.072657, median 155.000000
Vocab size (full) 33748
</code></pre>
<p class="normal">Based on this information, we set the following constants for our LSTM model. We choose our <code class="inlineCode">VOCAB_SIZE</code> as <code class="inlineCode">5000</code>; that is, our vocabulary covers the most frequent 5,000 words, which covers over 93% of the words used in the corpus. The remaining words are treated as <strong class="keyWord">out of vocabulary</strong> (<strong class="keyWord">OOV</strong>) and replaced <a id="_idIndexMarker895"/>with the token <code class="inlineCode">UNK</code>. At prediction time, any word that the model hasn’t seen will also be assigned the token <code class="inlineCode">UNK</code>. <code class="inlineCode">SEQUENCE_LEN</code> is set to approximately half the median length of sentences in the training set. Sentences that are shorter than <code class="inlineCode">SEQUENCE_LEN</code> will be padded by a special <code class="inlineCode">PAD</code> character, and those that are longer will be truncated to fit the limit:</p>
<pre class="programlisting code"><code class="hljs-code">VOCAB_SIZE = <span class="hljs-number">5000</span>
SEQUENCE_LEN = <span class="hljs-number">50</span>
</code></pre>
<p class="normal">Since the input to our LSTM will be numeric, we need to build lookup tables that go back and forth between words and word IDs. Since we limit our vocabulary size to 5,000 and we have to add the two pseudo-words <code class="inlineCode">PAD</code> and <code class="inlineCode">UNK</code>, our lookup table contains entries for the most frequently occurring 4,998 words plus <code class="inlineCode">PAD</code> and <code class="inlineCode">UNK</code>:</p>
<pre class="programlisting code"><code class="hljs-code">word2id = {}
word2id[<span class="hljs-string">"PAD"</span>] = <span class="hljs-number">0</span>
word2id[<span class="hljs-string">"UNK"</span>] = <span class="hljs-number">1</span>
<span class="hljs-keyword">for</span> v, (k, _) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(word_freqs.most_common(VOCAB_SIZE - <span class="hljs-number">2</span>)):
    word2id[k] = v + <span class="hljs-number">2</span>
id2word = {v:k <span class="hljs-keyword">for</span> k, v <span class="hljs-keyword">in</span> word2id.items()}
</code></pre>
<p class="normal">The input to our network is a sequence of words, where each word is represented by a vector. Simplistically, we could just use one-hot encoding for each word, but that makes the input data very large. So, we encode each word using its 50-dimensional GloVe embeddings.</p>
<p class="normal">The embedding is generated into<a id="_idIndexMarker896"/> a matrix of shape (<code class="inlineCode">VOCAB_SIZE</code> and <code class="inlineCode">EMBED_SIZE</code>) where each row represents the GloVe embedding for a word in our vocabulary. The <code class="inlineCode">PAD</code> and <code class="inlineCode">UNK</code> rows (<code class="inlineCode">0</code> and <code class="inlineCode">1</code> respectively) are populated with zeros and random uniform values respectively:</p>
<pre class="programlisting code"><code class="hljs-code">EMBED_SIZE = <span class="hljs-number">50</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">lookup_word2id</span>(<span class="hljs-params">word</span>):
    <span class="hljs-keyword">try</span>:
        <span class="hljs-keyword">return</span> word2id[word]
    <span class="hljs-keyword">except</span> KeyError:
        <span class="hljs-keyword">return</span> word2id[<span class="hljs-string">"</span><span class="hljs-string">UNK"</span>]
<span class="hljs-keyword">def</span> <span class="hljs-title">load_glove_vectors</span>(<span class="hljs-params">glove_file, word2id, embed_size</span>):
    embedding = np.zeros((<span class="hljs-built_in">len</span>(word2id), embed_size))
    fglove = <span class="hljs-built_in">open</span>(glove_file, <span class="hljs-string">"rb"</span>)
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> fglove:
        cols = line.strip().split()
        word = cols[<span class="hljs-number">0</span>].decode(<span class="hljs-string">'utf-8'</span>)
        <span class="hljs-keyword">if</span> embed_size == <span class="hljs-number">0</span>:
            embed_size = <span class="hljs-built_in">len</span>(cols) - <span class="hljs-number">1</span>
        <span class="hljs-keyword">if</span> word in word2id:
            vec = np.array([<span class="hljs-built_in">float</span>(v) <span class="hljs-keyword">for</span> v <span class="hljs-keyword">in</span> cols[<span class="hljs-number">1</span>:]])
        embedding[lookup_word2id(word)] = vec
    embedding[word2id[<span class="hljs-string">"PAD"</span>]] = np.zeros((embed_size))
    embedding[word2id[<span class="hljs-string">"</span><span class="hljs-string">UNK"</span>]] = np.random.uniform(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, embed_size)
    <span class="hljs-keyword">return</span> embedding
</code></pre>
<p class="normal">Next, we use these functions to generate embeddings:</p>
<pre class="programlisting code"><code class="hljs-code">sent_wids = [[lookup_word2id(w) <span class="hljs-keyword">for</span> w <span class="hljs-keyword">in</span> s.split()] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> sents]
sent_wids = sequence.pad_sequences(sent_wids, SEQUENCE_LEN)
<span class="hljs-comment"># load glove vectors into weight matrix</span>
embeddings = load_glove_vectors(<span class="hljs-string">"glove.6B.{:d}d.txt"</span>.<span class="hljs-built_in">format</span>(EMBED_SIZE), word2id, EMBED_SIZE)
</code></pre>
<p class="normal">Our autoencoder model takes a sequence of GloVe word vectors and learns to produce another sequence that is similar to the input sequence. The encoder LSTM compresses the sequence into a fixed-size context vector, which the decoder LSTM uses to reconstruct the original sequence. </p>
<p class="normal">A schematic of the network is shown here:</p>
<figure class="mediaobject"><img alt="A screenshot of a cell phone  Description automatically generated" height="468" src="../Images/B18331_08_11.png" width="476"/></figure>
<p class="packt_figref">Figure 8.11: Visualization of the LSTM network</p>
<p class="normal">Because the input is quite large, we will use a generator to produce each batch of input. Our generator produces batches of tensors of shape (<code class="inlineCode">BATCH_SIZE</code>, <code class="inlineCode">SEQUENCE_LEN</code>, <code class="inlineCode">EMBED_SIZE</code>). Here <code class="inlineCode">BATCH_SIZE</code> is <code class="inlineCode">64</code>, and since we are using 50-dimensional GloVe vectors, <code class="inlineCode">EMBED_SIZE</code> is <code class="inlineCode">50</code>. We <a id="_idIndexMarker897"/>shuffle the sentences at the beginning of each epoch and return batches of 64 sentences. Each sentence is represented as a vector of GloVe word vectors. If a word in the vocabulary does not have a corresponding GloVe embedding, it is represented by a zero vector. We construct two instances of the generator, one for training data and one for test data, consisting of 70% and 30% of the original dataset respectively:</p>
<pre class="programlisting code"><code class="hljs-code">BATCH_SIZE = <span class="hljs-number">64</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">sentence_generator</span>(<span class="hljs-params">X, embeddings, batch_size</span>):
    <span class="hljs-keyword">while</span> <span class="hljs-literal">True</span>:
        <span class="hljs-comment"># loop once per epoch</span>
        num_recs = X.shape[<span class="hljs-number">0</span>]
        indices = np.random.permutation(np.arange(num_recs))
        num_batches = num_recs // batch_size
        <span class="hljs-keyword">for</span> bid <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_batches):
            sids = indices[bid * batch_size : (bid + <span class="hljs-number">1</span>) * batch_size]
            Xbatch = embeddings[X[sids, :]]
            <span class="hljs-keyword">yield</span> Xbatch, Xbatch
train_size = <span class="hljs-number">0.7</span>
Xtrain, Xtest = train_test_split(sent_wids, train_size=train_size)
train_gen = sentence_generator(Xtrain, embeddings, BATCH_SIZE)
test_gen = sentence_generator(Xtest, embeddings, BATCH_SIZE)
</code></pre>
<p class="normal">Now we are ready to define the autoencoder. As we have shown in the diagram, it is composed of an encoder LSTM and a decoder LSTM. The encoder LSTM reads a tensor of shape (<code class="inlineCode">BATCH_SIZE</code>, <code class="inlineCode">SEQUENCE_LEN</code>, <code class="inlineCode">EMBED_SIZE</code>) representing a batch of sentences. Each sentence is represented as a padded fixed-length sequence of words of size <code class="inlineCode">SEQUENCE_LEN</code>. Each word is represented as a 300-dimensional GloVe vector. The output dimension of the encoder LSTM is a hyperparameter, <code class="inlineCode">LATENT_SIZE</code>, which is the size of the sentence vector that will come from the encoder part of the trained autoencoder later. The vector space of dimensionality <code class="inlineCode">LATENT_SIZE</code> represents the latent space that encodes the meaning of the <a id="_idIndexMarker898"/>sentence. The output of the LSTM is a vector of size (<code class="inlineCode">LATENT_SIZE</code>) for each sentence, so for the batch, the shape of the output tensor is (<code class="inlineCode">BATCH_SIZE</code>, <code class="inlineCode">LATENT_SIZE</code>). This is now fed to a <code class="inlineCode">RepeatVector</code> layer, which replicates this across the entire sequence; that is, the output tensor from this layer has the shape (<code class="inlineCode">BATCH_SIZE</code>, <code class="inlineCode">SEQUENCE_LEN</code>, <code class="inlineCode">LATENT_SIZE</code>). This tensor is now fed into the decoder LSTM, whose output dimension is the <code class="inlineCode">EMBED_SIZE</code>, so the output tensor has shape (<code class="inlineCode">BATCH_SIZE</code>, <code class="inlineCode">SEQUENCE_LEN</code>, <code class="inlineCode">EMBED_SIZE</code>), that is, the same shape as the input tensor.</p>
<p class="normal">We compile this model with the Adam optimizer and the MSE loss function. The reason we use MSE is that we want to reconstruct a sentence that has a similar meaning, that is, something that is close to the original sentence in the embedded space of dimension <code class="inlineCode">LATENT_SIZE</code>:</p>
<pre class="programlisting code"><code class="hljs-code">LATENT_SIZE = <span class="hljs-number">512</span>
EMBED_SIZE = <span class="hljs-number">50</span>
BATCH_SIZE = <span class="hljs-number">64</span>
NUM_EPOCHS = <span class="hljs-number">20</span>
inputs = Input(shape=(SEQUENCE_LEN, EMBED_SIZE), name=<span class="hljs-string">"input"</span>)
encoded = Bidirectional(LSTM(LATENT_SIZE), merge_mode=<span class="hljs-string">"sum"</span>, name=<span class="hljs-string">"encoder_lstm"</span>)(inputs)
decoded = RepeatVector(SEQUENCE_LEN, name=<span class="hljs-string">"repeater"</span>)(encoded)
decoded = Bidirectional(LSTM(EMBED_SIZE, return_sequences=<span class="hljs-literal">True</span>), merge_mode=<span class="hljs-string">"sum"</span>, name=<span class="hljs-string">"decoder_lstm"</span>)(decoded)
autoencoder = Model(inputs, decoded)
</code></pre>
<p class="normal">We define the loss function as mean squared error and choose the Adam optimizer:</p>
<pre class="programlisting code"><code class="hljs-code">autoencoder.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">"adam"</span>, loss=<span class="hljs-string">"mse"</span>)
</code></pre>
<p class="normal">We train the autoencoder for 20 epochs using the following code. 20 epochs was chosen because the MSE loss converges within this time:</p>
<pre class="programlisting code"><code class="hljs-code">num_train_steps = <span class="hljs-built_in">len</span>(Xtrain) // BATCH_SIZE
num_test_steps = <span class="hljs-built_in">len</span>(Xtest) // BATCH_SIZE
steps_per_epoch=num_train_steps,
epochs=NUM_EPOCHS,
validation_data=test_gen,
validation_steps=num_test_steps,
history = autoencoder.fit_generator(train_gen,
                                steps_per_epoch=num_train_steps,
                                epochs=NUM_EPOCHS,
                                validation_data=test_gen,
                                validation_steps=num_test_steps)
</code></pre>
<p class="normal">The results of the training <a id="_idIndexMarker899"/>are shown as follows. The plot below shows the loss plot for both training and validation data; we can see that as our model learns, the losses decrease as expected:</p>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="401" src="../Images/B18331_08_12.png" width="599"/></figure>
<p class="packt_figref">Figure 8.12: Loss plot of the LSTM autoencoder</p>
<p class="normal">Since we are feeding in a matrix of embeddings, the output will also be a matrix of word embeddings. Since the embedding space is continuous and our vocabulary is discrete, not every output embedding will correspond to a word. The best we can do is to find a word that is closest to the output embedding in order to reconstruct the original text. This is a bit cumbersome, so we will evaluate our autoencoder in a different way.</p>
<p class="normal">Since the objective of the<a id="_idIndexMarker900"/> autoencoder is to produce a good latent representation, we compare the latent vectors produced from the encoder using the original input versus the output of the autoencoder.</p>
<p class="normal">First, we extract the encoder component into its own network:</p>
<pre class="programlisting code"><code class="hljs-code">encoder = Model(autoencoder.<span class="hljs-built_in">input</span>, autoencoder.get_layer(<span class="hljs-string">"encoder_lstm"</span>).output)
</code></pre>
<p class="normal">Then we run the autoencoder on the test set to return the predicted embeddings. We then send both the input embedding and the predicted embedding through the encoder to produce sentence vectors from each and compare the two vectors using <em class="italic">cosine</em> similarity. Cosine similarities close to “one” indicate high similarity and those close to “zero” indicate low similarity. </p>
<p class="normal">The following code runs against a random subset of 500 test sentences and produces some sample values of cosine similarities, between the sentence vectors generated from the source embedding and the corresponding target embedding produced by the autoencoder:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">compute_cosine_similarity</span>(<span class="hljs-params">x, y</span>):
    <span class="hljs-keyword">return</span> np.dot(x, y) / (np.linalg.norm(x, <span class="hljs-number">2</span>) * np.linalg.norm(y, <span class="hljs-number">2</span>))
k = <span class="hljs-number">500</span>
cosims = np.zeros((k))
i= <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> bid <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_test_steps):
    xtest, ytest = next(test_gen)
    ytest_ = autoencoder.predict(xtest)
    Xvec = encoder.predict(xtest)
    Yvec = encoder.predict(ytest_)
    <span class="hljs-keyword">for</span> rid <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(Xvec.shape[<span class="hljs-number">0</span>]):
        <span class="hljs-keyword">if</span> i &gt;= k:
            <span class="hljs-keyword">break</span>
        cosims[i] = compute_cosine_similarity(Xvec[rid], Yvec[rid])
        <span class="hljs-keyword">if</span> i &lt;= <span class="hljs-number">10</span>:
            <span class="hljs-built_in">print</span>(cosims[i])
        i += <span class="hljs-number">1</span>
    <span class="hljs-keyword">if</span> i &gt;= k:
        <span class="hljs-keyword">break</span>
</code></pre>
<p class="normal">The first 10 values of cosine similarities are shown as follows. As we can see, the vectors seem to be quite similar:</p>
<pre class="programlisting con"><code class="hljs-con">0.9765363335609436
0.9862152338027954
0.9831727743148804
0.977733314037323
0.9851642847061157
0.9849132895469666
0.9831638932228088
0.9843543767929077
0.9825796484947205
0.9877195954322815
0.9820773601531982
</code></pre>
<p class="normal"><em class="italic">Figure 8.13</em> shows a histogram of the distribution of values of cosine similarities for the sentence vectors from the first 500 sentences. </p>
<p class="normal">As previously mentioned, it confirms that the sentence vectors generated<a id="_idIndexMarker901"/> from the input and output of the autoencoder are very similar, showing that the resulting sentence vector is a good representation of the sentence:</p>
<figure class="mediaobject"><img alt="Chart, histogram  Description automatically generated" height="469" src="../Images/B18331_08_13.png" width="684"/></figure>
<p class="packt_figref">Figure 8.13: Cosine similarity distribution</p>
<p class="normal">Till now we have focused on autoencoders that can reconstruct data; in the next section, we will go through<a id="_idIndexMarker902"/> a slightly different variant of the autoencoder – the variational autoencoder, which is used to generate data.</p>
<h1 class="heading-1" id="_idParaDest-244">Variational autoencoders</h1>
<p class="normal">Like DBNs (<em class="chapterRef">Chapter 7</em>, <em class="italic">Unsupervised Learning</em>) and GANs (see <em class="chapterRef">Chapter 9</em>, <em class="italic">Generative Models</em>, for more details), variational autoencoders are also generative models. <strong class="keyWord">Variational autoencoders</strong> (<strong class="keyWord">VAEs</strong>) are a mix of the best neural networks and Bayesian inference. They are one of the most interesting <a id="_idIndexMarker903"/>neural networks and have emerged as one of the most popular approaches to unsupervised learning. They are autoencoders with a twist. Along with the conventional encoder and decoder network of autoencoders, they have additional stochastic layers. The stochastic layer, after the encoder network, samples the data using a Gaussian distribution, and the one after the decoder network samples the data using Bernoulli’s distribution. Like GANs, VAEs can be used to generate images and figures based on the distribution they have been trained on. </p>
<p class="normal">VAEs allow one to set complex priors in the latent space and thus learn powerful latent representations. <em class="italic">Figure 8.14</em> describes a VAE:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="338" src="../Images/B18331_08_14.png" width="531"/> </figure>
<p class="packt_figref">Figure 8.14: Architecture of a variational autoencoder</p>
<p class="normal">The encoder network <img alt="" height="54" src="../Images/B18331_08_017.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="129"/> approximates the true but intractable posterior distribution <img alt="" height="50" src="../Images/B18331_08_018.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="108"/>, where <em class="italic">x</em> is the input to the VAE and <em class="italic">z</em> is the latent representation. The decoder network <img alt="" height="50" src="../Images/B18331_08_019.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="125"/> takes the <em class="italic">d</em>-dimensional latent variables (also called latent space) as its input and generates<a id="_idIndexMarker904"/> new images following the same distribution as <em class="italic">P</em>(<em class="italic">x</em>). As you can see from the preceding diagram, the latent representation <em class="italic">z</em> is sampled from <img alt="" height="54" src="../Images/B18331_08_020.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="304"/>, and the <a id="_idIndexMarker905"/>output of the decoder network samples <img alt="" height="46" src="../Images/B18331_08_021.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="54"/> from <img alt="" height="54" src="../Images/B18331_08_022.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="304"/>. Here <em class="italic">N</em> represents a normal distribution with mean <img alt="" height="46" src="../Images/B18331_08_023.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="25"/> and variance <img alt="" height="42" src="../Images/B18331_01_017.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/>.</p>
<p class="normal">Now that we have the basic architecture of VAEs, the question arises of how they can be trained, since the maximum likelihood of the training data and posterior density are intractable. The network is trained by maximizing the lower bound of the log data likelihood. Thus, the loss term consists of two components: generation loss, which is obtained from the decoder network <a id="_idIndexMarker906"/>through sampling, and the Kullback–Leibler divergence term, also called the latent loss.</p>
<p class="normal">Generation loss ensures that the image generated by the decoder and the image used to train the network are similar, and latent loss ensures that the posterior distribution <img alt="" height="50" src="../Images/B18331_08_025.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="108"/> is close to the prior <img alt="" height="50" src="../Images/B18331_08_026.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="92"/>. Since the encoder uses Gaussian distribution for sampling, the latent loss measures how closely the latent variables match this distribution.</p>
<p class="normal">Once the VAE is trained, we can use only the decoder network to generate new images. Let us try coding a VAE. This time we are using the Fashion-MNIST dataset; the dataset contains Zalando’s (<a href="https://github.com/zalandoresearch/fashion-mnist"><span class="url">https://github.com/zalandoresearch/fashion-mnist</span></a>) article images. The test-train split is exactly the same as for MNIST, that is, 60,000 train images and 10,000 test images. The size of each image is also 28 × 28, so we can easily replace the code running on the MNIST dataset with the Fashion-MNIST dataset. </p>
<p class="normal">The code in this section has been adapted from <a href="https://github.com/dragen1860/TensorFlow-2.x-Tutorials"><span class="url">https://github.com/dragen1860/TensorFlow-2.x-Tutorials</span></a>. As the first step we, as usual, import all the necessary libraries:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">from</span> matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt
</code></pre>
<p class="normal">Let us fix the seeds for a random number, so that the results are reproducible. We can also add an <code class="inlineCode">assert</code> statement to ensure that our code runs on TensorFlow 2.0 or above:</p>
<pre class="programlisting code"><code class="hljs-code">np.random.seed(<span class="hljs-number">333</span>)
tf.random.set_seed(<span class="hljs-number">333</span>)
<span class="hljs-keyword">assert</span> tf.__version__.startswith(<span class="hljs-string">'2.'</span>), <span class="hljs-string">"TensorFlow Version Below 2.0"</span>
</code></pre>
<p class="normal">Before going ahead with making the VAE, let us also explore the Fashion-MNIST dataset a little. The dataset is available in the TensorFlow Keras API:</p>
<pre class="programlisting code"><code class="hljs-code">(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
x_train, x_test = x_train.astype(np.float32)/<span class="hljs-number">255.</span>, x_test.astype(np.float32)/<span class="hljs-number">255.</span>
<span class="hljs-built_in">print</span>(x_train.shape, y_train.shape)
<span class="hljs-built_in">print</span>(x_test.shape, y_test.shape)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">--------------------------------------------------
(60000, 28, 28) (60000,)
(10000, 28, 28) (10000,)
</code></pre>
<p class="normal">We see some sample images:</p>
<pre class="programlisting code"><code class="hljs-code">number = <span class="hljs-number">10</span>  <span class="hljs-comment"># how many digits we will display</span>
plt.figure(figsize=(<span class="hljs-number">20</span>, <span class="hljs-number">4</span>))
<span class="hljs-keyword">for</span> index <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(number):
    <span class="hljs-comment"># display original</span>
    ax = plt.subplot(<span class="hljs-number">2</span>, number, index + <span class="hljs-number">1</span>)
    plt.imshow(x_train[index], cmap=<span class="hljs-string">'gray'</span>)
    ax.get_xaxis().set_visible(<span class="hljs-literal">False</span>)
    ax.get_yaxis().set_visible(<span class="hljs-literal">False</span>)
plt.show()
</code></pre>
<figure class="mediaobject"><img alt="" height="73" src="../Images/B18331_08_15.png" width="721"/></figure>
<p class="packt_figref">Figure 8.15: Sample images from the Fashion-MNIST dataset</p>
<p class="normal">Before we start, let us declare <a id="_idIndexMarker907"/>some hyperparameters like learning rate, dimensions of the hidden layer and the latent space, batch size, epochs, and so on:</p>
<pre class="programlisting code"><code class="hljs-code">image_size = x_train.shape[<span class="hljs-number">1</span>]*x_train.shape[<span class="hljs-number">2</span>]
hidden_dim = <span class="hljs-number">512</span>
latent_dim = <span class="hljs-number">10</span>
num_epochs = <span class="hljs-number">80</span>
batch_size = <span class="hljs-number">100</span>
learning_rate = <span class="hljs-number">0.001</span>
</code></pre>
<p class="normal">We use the TensorFlow Keras Model API to build a VAE model. The <code class="inlineCode">__init__()</code> function defines all the layers that we will be using:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span> <span class="hljs-title">VAE</span>(tf.keras.Model):
    <span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span>(<span class="hljs-params">self,dim,**kwargs</span>):
        h_dim = dim[<span class="hljs-number">0</span>]
        z_dim = dim[<span class="hljs-number">1</span>]
        <span class="hljs-built_in">super</span>(VAE, self).__init__(**kwargs)
        self.fc1 = tf.keras.layers.Dense(h_dim)
        self.fc2 = tf.keras.layers.Dense(z_dim)
        self.fc3 = tf.keras.layers.Dense(z_dim)
        self.fc4 = tf.keras.layers.Dense(h_dim)
        self.fc5 = tf.keras.layers.Dense(image_size)
</code></pre>
<p class="normal">We define the functions to give<a id="_idIndexMarker908"/> us the encoder output and decoder output and reparametrize. The implementation of the encoder and decoder functions are straightforward; however, we need to delve a little deeper into the <code class="inlineCode">reparametrize</code> function. As you know, VAEs sample from a random node <em class="italic">z</em>, which is approximated by <img alt="" height="50" src="../Images/B18331_08_027.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="117"/> of the true posterior. Now, to get parameters we need to use backpropagation. However, backpropagation cannot work on random nodes. Using reparameterization, we can use a new parameter, <code class="inlineCode">eps</code>, which allows us to reparametrize <code class="inlineCode">z</code> in a way that will allow backpropagation through the deterministic random node (<a href="https://arxiv.org/pdf/1312.6114v10.pdf"><span class="url">https://arxiv.org/pdf/1312.6114v10.pdf</span></a>):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">encode</span>(<span class="hljs-params">self, x</span>):
    h = tf.nn.relu(self.fc1(x))
    <span class="hljs-keyword">return</span> self.fc2(h), self.fc3(h)
<span class="hljs-keyword">def</span> <span class="hljs-title">reparameterize</span>(<span class="hljs-params">self, mu, log_var</span>):
    std = tf.exp(log_var * <span class="hljs-number">0.5</span>)
    eps = tf.random.normal(std.shape)
    <span class="hljs-keyword">return</span> mu + eps * std
<span class="hljs-keyword">def</span> <span class="hljs-title">decode_logits</span>(<span class="hljs-params">self, z</span>):
    h = tf.nn.relu(self.fc4(z))
    <span class="hljs-keyword">return</span> self.fc5(h)
<span class="hljs-keyword">def</span> <span class="hljs-title">decode</span>(<span class="hljs-params">self, z</span>):
    <span class="hljs-keyword">return</span> tf.nn.sigmoid(self.decode_logits(z))
</code></pre>
<p class="normal">Lastly, we define the <code class="inlineCode">call()</code> function, which will control how signals move through different layers of the VAE:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">call</span>(<span class="hljs-params">self, inputs, training=</span><span class="hljs-literal">None</span><span class="hljs-params">, mask=</span><span class="hljs-literal">None</span>):
    mu, log_var = self.encode(inputs)
    z = self.reparameterize(mu, log_var)
    x_reconstructed_logits = self.decode_logits(z)
    <span class="hljs-keyword">return</span> x_reconstructed_logits, mu, log_var
</code></pre>
<p class="normal">Now, we create the VAE model <a id="_idIndexMarker909"/>and declare the optimizer for it. You can see the summary of the model:</p>
<pre class="programlisting code"><code class="hljs-code">model = VAE([hidden_dim, latent_dim])
model.build(input_shape=(<span class="hljs-number">4</span>, image_size))
model.summary()
optimizer = tf.keras.optimizers.Adam(learning_rate)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Model: "vae"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               multiple                  401920    
                                                                 
 dense_1 (Dense)             multiple                  5130      
                                                                 
 dense_2 (Dense)             multiple                  5130      
                                                                 
 dense_3 (Dense)             multiple                  5632      
                                                                 
 dense_4 (Dense)             multiple                  402192    
                                                                 
=================================================================
Total params: 820,004
Trainable params: 820,004
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p class="normal">Now, we train the model. We define our loss function, which is the sum of the reconstruction loss and KL divergence loss:</p>
<pre class="programlisting code"><code class="hljs-code">dataset = tf.data.Dataset.from_tensor_slices(x_train)
dataset = dataset.shuffle(batch_size * <span class="hljs-number">5</span>).batch(batch_size)
num_batches = x_train.shape[<span class="hljs-number">0</span>] // batch_size
<span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_epochs):
    <span class="hljs-keyword">for</span> step, x <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataset):
        x = tf.reshape(x, [-<span class="hljs-number">1</span>, image_size])
        <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
            <span class="hljs-comment"># Forward pass</span>
            x_reconstruction_logits, mu, log_var = model(x)
            <span class="hljs-comment"># Compute reconstruction loss and kl divergence</span>
            <span class="hljs-comment"># Scaled by 'image_size' for each individual pixel.</span>
            reconstruction_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=x, logits=x_reconstruction_logits)
            reconstruction_loss = tf.reduce_sum(reconstruction_loss) / batch_size
            
            kl_div = - <span class="hljs-number">0.5</span> * tf.reduce_sum(<span class="hljs-number">1.</span> + log_var - tf.square(mu) - tf.exp(log_var), axis=-<span class="hljs-number">1</span>)
            kl_div = tf.reduce_mean(kl_div)
            <span class="hljs-comment"># Backprop and optimize</span>
            loss = tf.reduce_mean(reconstruction_loss) + kl_div
        gradients = tape.gradient(loss, model.trainable_variables)
        <span class="hljs-keyword">for</span> g <span class="hljs-keyword">in</span> gradients:
            tf.clip_by_norm(g, <span class="hljs-number">15</span>)
        optimizer.apply_gradients(<span class="hljs-built_in">zip</span>(gradients, model.trainable_variables))
        <span class="hljs-keyword">if</span> (step + <span class="hljs-number">1</span>) % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:
            <span class="hljs-built_in">print</span>(<span class="hljs-string">"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}"</span>
            .<span class="hljs-built_in">format</span>(epoch + <span class="hljs-number">1</span>, num_epochs, step + <span class="hljs-number">1</span>, num_batches, <span class="hljs-built_in">float</span>(reconstruction_loss), <span class="hljs-built_in">float</span>(kl_div)))
</code></pre>
<p class="normal">Once the model is trained it should be able to generate images similar to the original Fashion-MNIST images. To<a id="_idIndexMarker910"/> do so we need to use only the decoder network, and we will pass to it a randomly generated <em class="italic">z</em> input:</p>
<pre class="programlisting code"><code class="hljs-code">z = tf.random.normal((batch_size, latent_dim))
out = model.decode(z)  <span class="hljs-comment"># decode with sigmoid</span>
out = tf.reshape(out, [-<span class="hljs-number">1</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>]).numpy() * <span class="hljs-number">255</span>
out = out.astype(np.uint8)
</code></pre>
<p class="normal"><em class="italic">Figure 8.16</em> shows the results after 80 epochs:</p>
<figure class="mediaobject"><img alt="" height="76" src="../Images/B18331_08_16.png" width="753"/></figure>
<p class="packt_figref">Figure 8.16: Results after 80 epochs</p>
<p class="normal">The generated images<a id="_idIndexMarker911"/> resemble the input space. The generated images are similar to the original Fashion-MNIST images as desired.</p>
<h1 class="heading-1" id="_idParaDest-245">Summary</h1>
<p class="normal">In this chapter, we’ve had an extensive look at a new generation of deep learning models: autoencoders. We started with the vanilla autoencoder, and then moved on to its variants: sparse autoencoders, denoising autoencoders, stacked autoencoders, and convolutional autoencoders. We used the autoencoders to reconstruct images, and we also demonstrated how they can be used to clean noise from an image. Finally, the chapter demonstrated how autoencoders can be used to generate sentence vectors and images. The autoencoders learned through unsupervised learning.</p>
<p class="normal">In the next chapter, we will delve deeper into generative adversarial networks, another interesting deep learning model that learns via an unsupervised learning paradigm.</p>
<h1 class="heading-1" id="_idParaDest-246">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1985). <em class="italic">Learning Internal Representations by Error Propagation</em>. No. ICS-8506. University of California, San Diego. La Jolla Institute for Cognitive Science: <a href="http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf"><span class="url">http://www.cs.toronto.edu/~fritz/absps/pdp8.pdf</span></a></li>
<li class="numberedList">Hinton, G. E. and Salakhutdinov, R. R. (2016). <em class="italic">Reducing the dimensionality of data with neural networks</em>. science 313.5786: 504–507: <a href="https://www.cs.toronto.edu/~hinton/science.pdf"><span class="url">https://www.cs.toronto.edu/~hinton/science.pdf</span></a> </li>
<li class="numberedList">Masci, J. et al. (2011). <em class="italic">Stacked convolutional auto-encoders for hierarchical feature extraction</em>. Artificial Neural Networks and Machine Learning–ICANN 2011: 52–59: <a href="https://www.semanticscholar.org/paper/Reducing-the-dimensionality-of-data-with-neural-Hinton-Salakhutdinov/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e"><span class="url">https://www.semanticscholar.org/paper/Reducing-the-dimensionality-of-data-with-neural-Hinton-Salakhutdinov/46eb79e5eec8a4e2b2f5652b66441e8a4c921c3e</span></a></li>
<li class="numberedList">Japkowicz, N., Myers, C., and Gluck, M. (1995). <em class="italic">A novelty detection approach to classification</em>. IJCAI. Vol: <a href="https://www.ijcai.org/Proceedings/95-1/Papers/068.pdf"><span class="url">https://www.ijcai.org/Proceedings/95-1/Papers/068.pdf</span></a></li>
<li class="numberedList">Sedhain, S. (2015).<em class="italic"> AutoRec: Autoencoders Meet Collaborative Filtering</em>. Proceedings of the 24th International Conference on World Wide Web, ACM.</li>
<li class="numberedList">Cheng, H. (2016).<em class="italic"> Wide &amp; Deep Learning for Recommender Systems</em>. Proceedings of the 1st Workshop on Deep Learning for Recommender Systems, ACM.</li>
<li class="numberedList">Runfeldt, M.<em class="italic"> Using Deep Learning to Remove Eyeglasses from Faces</em>.</li>
<li class="numberedList">Miotto, R. (2016).<em class="italic"> Deep Patient: An Unsupervised Representation to Predict the Future of Patients from the Electronic Health Records</em>. Scientific Reports.</li>
<li class="numberedList">Kiros, R. (2015).<em class="italic"> Skip-Thought Vectors</em>, Advances in Neural Information Processing Systems.</li>
<li class="numberedList">Kullback-Leibler divergence: <a href="http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf"><span class="url">http://hanj.cs.illinois.edu/cs412/bk3/KL-divergence.pdf</span></a></li>
<li class="numberedList">Denoising autoencoders: <a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.xhtml"><span class="url">https://cs.stanford.edu/people/karpathy/convnetjs/demo/autoencoder.xhtml</span></a></li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="163" src="../Images/QR_Code18312172242788196872.png" width="177"/></p>
</div>
</div>
</body></html>