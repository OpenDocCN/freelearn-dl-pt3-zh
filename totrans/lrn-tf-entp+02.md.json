["```\n!pip list | grep tensorflow\n```", "```\n    SELECT * FROM `project1-190517.myworkdataset.iris` LIMIT 1000\n    ```", "```\nmy_train_dataset = tf.data.TFRecordDataset('gs://<BUCKET_NAME>/<FILE_NAME>*.tfrecord')\n```", "```\nmy_train_dataset = my_train_dataset.repeat()\n```", "```\nmy_train_dataset = my_train_dataset.batch()\n```", "```\n…\n```", "```\nmodel.fit(my_train_dataset, …)\n```", "```\n    project_id = '<PROJECT-XXXXX>'\n    ```", "```\n    %%bigquery --project $project_id mydataframe\n    SELECT * from `bigquery-public-data.covid19_jhu_csse.summary` limit 5\n    ```", "```\n    type(mydataframe)\n    ```", "```\n    mydataframe\n    ```", "```\nme integration\n```", "```\nfrom google.cloud import bigquery\n```", "```\nproject_id ='project-xxxxx'\n```", "```\nclient = bigquery.Client(project=project_id)\n```", "```\nsample_count = 1000\n```", "```\nrow_count = client.query('''\n```", "```\n  SELECT \n```", "```\n    COUNT(*) as total\n```", "```\n  FROM `bigquery-public-data.covid19_jhu_csse.summary`''').to_dataframe().total[0]\n```", "```\ndf = client.query('''\n```", "```\n  SELECT\n```", "```\n    *\n```", "```\n  FROM\n```", "```\n    `bigquery-public-data.covid19_jhu_csse.summary`\n```", "```\n  WHERE RAND() < %d/%d\n```", "```\n''' % (sample_count, row_count)).to_dataframe()\n```", "```\nprint('Full dataset has %d rows' % row_count)\n```", "```\n    import tensorflow as tf\n    from tensorflow_io.bigquery import BigQueryClient\n    PROJECT_ID = 'project-xxxxx' # This is from what you created in your Google Cloud Account. \n    DATASET_GCP_PROJECT_ID = 'bigquery-public-data'\n    DATASET_ID = 'covid19_jhu_csse'\n    TABLE_ID = 'summary'\n    ```", "```\n    batch_size = 2048\n    client = BigQueryClient()\n    ```", "```\n    read_session = client.read_session(\n        'projects/' + PROJECT_ID,\n        DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,\n        ['province_state',\n           'country_region',\n           'confirmed',\n           'deaths',\n           'date',\n           'recovered'\n           ],\n        [tf.string,\n           tf.string,\n           tf.int64,\n           tf.int64,\n           tf.int32,\n           tf.int64],\n          requested_streams=10\n    )\n    ```", "```\n    dataset = read_session.parallel_read_rows(sloppy=True).batch(batch_size)\n    ```", "```\n    type(dataset)\n    ```", "```\n    itr = tf.compat.v1.data.make_one_shot_iterator(\n        dataset\n    )\n     next(itr)\n    ```", "```\n    project_id = 'project1-190517'\n    ```", "```\n    %%bigquery --project $project_id mydataframe\n    SELECT * from `bigquery-public-data.covid19_jhu_csse.summary`\n    ```", "```\n    import pandas as pd\n    mydataframe.to_csv('my_new_data.csv')\n    ```", "```\n    dataset_id = 'my_new_dataset'\n    ```", "```\n    !bq --location=US mk --dataset $dataset_id\n    ```", "```\n    'my_new_data.csv' will suffice. Otherwise, a full path is required. Also, {dataset_id}.my_new_data_table indicates that we want to write the CSV file into this particular dataset and the table name.\n    ```", "```\n    import tensorflow as tf\n    import pickle as pkl\n    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.imdb.load_data(\n        path='imdb.npz',\n        num_words=None,\n        skip_top=0,\n        maxlen=None,\n        seed=113,\n        start_char=1,\n        oov_char=2,\n        index_from=3\n    )\n    with open('/home/jupyter/x_train.pkl','wb') as f:\n        pkl.dump(x_train, f)\n    ```", "```\n    bucket_name = 'ai-platform-bucket'\n    ```", "```\n    !gsutil mb gs://{bucket_name}/\n    ```", "```\n    !gsutil cp /home/jupyter/x_train.pkl gs://{bucket_name}/\n    ```", "```\n    !gsutil cp gs://{bucket_name}/x_train.pkl /home/jupyter/x_train_readback.pkl\n    ```"]