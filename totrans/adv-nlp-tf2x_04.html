<html><head></head><body>
  <div id="_idContainer104">
    <h1 class="chapterNumber">4</h1>
    <h1 id="_idParaDest-65" class="chapterTitle">Transfer Learning with BERT</h1>
    <p class="normal">Deep learning models really shine with large amounts of training data. Having enough labeled data is a constant challenge in the field, especially in NLP. A successful approach that has yielded great results in the last couple of years is that of transfer learning. A model is trained in an unsupervised or semi-supervised way on a large corpus and then fine-tuned for a specific application. Such models have shown excellent results. In this chapter, we will build on the IMDb movie review sentiment analysis and use transfer learning to build models using <strong class="keyword">GloVe</strong> (<strong class="keyword">Global Vectors for Word Representation</strong>) pre-trained embeddings and <strong class="keyword">BERT</strong> (<strong class="keyword">Bi-Directional Encoder Representations from Transformers</strong>) contextual models. In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bullet">Overview of transfer learning and use in NLP</li>
      <li class="bullet">Loading pre-trained GloVe embeddings in a model</li>
      <li class="bullet">Building a sentiment analysis model using pre-trained GloVe embeddings and fine-tuning</li>
      <li class="bullet">Overview of contextual embeddings using Attention – BERT</li>
      <li class="bullet">Loading pre-trained BERT models using the Hugging Face library</li>
      <li class="bullet">Using pre-trained and custom BERT-based fine-tuned models for sentiment analysis</li>
    </ul>
    <p class="normal">Transfer learning is a core concept that has made rapid advances in NLP possible. We will discuss transfer learning first.</p>
    <h1 id="_idParaDest-66" class="title">Transfer learning overview</h1>
    <p class="normal">Traditionally, a machine learning <a id="_idIndexMarker206"/>model is trained for performance on a specific task. It is only expected to work for that task and is not likely to have high performance beyond that task. Let's take the example of the problem of classifying the sentiment of IMDb movie reviews <em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>. The model that was trained for this particular task was optimized for performance on this task alone. A separate set of labeled data specific to a different task is required if we wish to train another model. Building another model might not be effective if there isn't enough labeled data for that task.</p>
    <p class="normal">Transfer learning is the concept of learning a fundamental representation of the data that can be adapted to different tasks. In the case of transfer learning, a more abundantly available dataset may be used to distill knowledge and in building a new ML model for a specific<a id="_idIndexMarker207"/> task. Through the use of this knowledge, this new ML model can have decent performance even when there is not enough labeled data available for a traditional ML approach to return good results. For this scheme to be effective, there are a few important considerations:</p>
    <ul>
      <li class="bullet">The knowledge<a id="_idIndexMarker208"/> distillation step, called <strong class="keyword">pre-training</strong>, should have<a id="_idIndexMarker209"/> an abundant amount of data available relatively cheaply</li>
      <li class="bullet">Adaptation, often called fine-tuning, should be done with data that shares similarities with the data used for pre-training</li>
    </ul>
    <p class="normal">The figure below illustrates this concept:</p>
    <figure class="mediaobject"><img src="image/B16252_04_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.1: Comparing traditional machine learning with transfer learning</p>
    <p class="normal">This technique has been very effective in computer vision. ImageNet is often used as the dataset for pre-training. Specific models are then fine-tuned for a variety of tasks such as image classification, object detection, image segmentation, and pose detection, among others.</p>
    <h2 id="_idParaDest-67" class="title">Types of transfer learning</h2>
    <p class="normal">The concepts of <strong class="keyword">domains</strong> and <strong class="keyword">tasks</strong> underpin the concept of transfer learning. A domain represents a <a id="_idIndexMarker210"/>specific area of knowledge or data. News articles, social media posts, medical records, Wikipedia entries, and court judgments could be considered examples of different domains. A task is<a id="_idIndexMarker211"/> a specific objective or action within a domain. Sentiment<a id="_idIndexMarker212"/> analysis and stance detection of tweets are specific tasks in the social media posts domain. Detection of cancer and fractures could be different tasks in the domain of medical records. Different types of transfer learning have different combinations of source and target domains and tasks. Three main types of transfer learning, namely domain adaptation, multi-task learning, and sequential learning, are described below.</p>
    <h3 id="_idParaDest-68" class="title">Domain adaptation</h3>
    <p class="normal">In this setting, the domains of<a id="_idIndexMarker213"/> source and target tasks are usually the same. However, the differences are related to the distribution of training and testing data. This case of transfer learning is related to a fundamental assumption in any machine learning task – the assumption that training and testing data are <em class="italic">i.i.d</em>. The first <em class="italic">i</em> stands for <em class="italic">independent</em>, which implies that each sample is <a id="_idIndexMarker214"/>independent of the others. In practice, this assumption can be violated when there are feedback loops, like in recommendation systems. The second section is <em class="italic">i.d.</em>, which stands for <em class="italic">identically distributed</em> and implies that the distribution of labels and other characteristics between training and test samples is the same.</p>
    <p class="normal">Suppose the domain was animal photos, and the task was identifying cats in the photos. This task can be modeled as a binary classification problem. The identically distributed assumption implies that the distribution of cats in the photos between training and test samples is similar. This also implies that characteristics of photos, such as resolutions, lighting conditions, and orientations, are very similar. In practice, this assumption is also frequently violated.</p>
    <p class="normal">There is a case about a very early perceptron model built to identify tanks in the woods. The model was performing quite well on the training set. When the test set was expanded, it was discovered that all the pictures of tanks in woods were taken on sunny days, whereas the pictures of woods without tanks were taken on a cloudy day. </p>
    <p class="normal">In this case, the network learned to differentiate sunny and cloudy conditions more than the presence or absence of tanks. During testing, the pictures supplied were from a different distribution, but the same domain, which led to the model failing.</p>
    <p class="normal">Dealing with similar situations is called domain adaptation. There are many techniques for domain adaptation, one of which is data augmentation. In computer vision, images in the training set can be cropped, warped, or rotated, and varying amounts of exposure or contrast or saturation can be applied to them. These transformations would increase the training data and could mitigate the gap between training and potential testing data. Similar techniques are used in speech and audio by adding random noises, including street sounds or background chatter, to an audio sample. Domain adaptation techniques are well known in traditional machine learning with several resources already available on it.</p>
    <p class="normal">However, what <a id="_idIndexMarker215"/>makes transfer learning exciting is using data from a different source domain or task for pre-training results in improvements in model performance on a different task or domain. There are two types of transfer learning in this area. The first one is multi-task learning, and the second one is sequential learning.</p>
    <h3 id="_idParaDest-69" class="title">Multi-task learning</h3>
    <p class="normal">In multi-task learning, data<a id="_idIndexMarker216"/> from different but related tasks are passed through a set of common layers. Then, there may be task-specific layers <a id="_idIndexMarker217"/>on the top that learn about a particular task objective. <em class="italic">Figure 4.2</em> shows the multi-task learning setting:</p>
    <figure class="mediaobject"><img src="image/B16252_04_02.png" alt="A picture containing building, drawing  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.2: Multi-task transfer learning</p>
    <p class="normal">The output of these task-specific layers would be evaluated on different loss functions. All the training examples for all the tasks are passed through all the layers of the model. The task-specific layers are not expected to do well for all the tasks. The expectation is that the common layers learn some of the underlying structure that is shared by the different tasks. This information about structure provides useful signals and improves the performance of all the models. The data for each task has many features. However, these features may be used to construct representations that can be useful in other related tasks.</p>
    <p class="normal">Intuitively, people learn some elementary skills before mastering more complex skills. Learning to write requires first becoming skilled in holding a pen or pencil. Writing, drawing, and painting can be considered different tasks that share a standard "layer" of holding a pen or pencil. The same concept applies while learning a new language where the structure and grammar of one language may help with learning a related language. Learning Latin-based languages like French, Italian, and Spanish becomes more comfortable if one of the other Latin languages is known, as these languages share word roots.</p>
    <p class="normal">Multi-task learning<a id="_idIndexMarker218"/> increases the amount of data available for training by pooling data from different tasks together. Further, it forces the network to generalize better by trying to learn representations that are common<a id="_idIndexMarker219"/> across tasks in shared layers.</p>
    <p class="normal">Multi-task learning is a crucial reason behind the recent success of models such as GPT-2 and BERT. It is the most common technique used for pre-training models that are then used for specific tasks.</p>
    <h3 id="_idParaDest-70" class="title">Sequential learning</h3>
    <p class="normal">Sequential <a id="_idIndexMarker220"/>learning is the most common form of <a id="_idIndexMarker221"/>transfer learning. It is named so because it involves two simple steps executed in sequence. The first step is pre-training and the second step is fine-tuning. These steps are shown in <em class="italic">Figure 4.3</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_04_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.3: Sequential learning</p>
    <p class="normal">The first step is to pre-train a model. The most successful pre-trained models use some form of multi-task learning objectives, as depicted on the left side of the figure. A portion of the model used for pre-training is then used for different tasks shown on the right in the figure. This reusable part of the pre-trained model depends on the specific architecture and may have a different set of layers. The reusable partition shown in <em class="italic">Figure 4.3</em> is just illustrative. In the <a id="_idIndexMarker222"/>second step, the pre-trained model is loaded and added as the starting layer of a task-specific model. The weights learned by the pre-trained model can be frozen during the training of the task-specific model, or those weights can be updated or fine-tuned. When the weights are frozen, then this pattern of<a id="_idIndexMarker223"/> using the pre-trained model is called <em class="italic">feature extraction</em>.</p>
    <p class="normal">Generally, fine-tuning <a id="_idIndexMarker224"/>gives better performance than a feature extraction approach. However, there are some pros and cons to both approaches. In fine-tuning, not all weights get updated as the task-specific training data may be much smaller in size. If the pre-trained model is an embedding for words, then other embeddings can become stale. If the task is such that it has a small vocabulary or has many out-of-vocabulary words, then this can hurt the performance of the model. Generally, if the source and target tasks are similar, then fine-tuning would produce better results.</p>
    <p class="normal">An example of such a pre-trained model is Word2vec, which we saw in <em class="chapterRef">Chapter 1</em>, <em class="italic">Essentials of NLP</em>. There is another <a id="_idIndexMarker225"/>model of generating word-level embeddings called <strong class="keyword">GloVe</strong> or <strong class="keyword">Global Vectors for Word Representation</strong>, introduced in 2014 by researchers from Stanford. Let's take a practical tour of transfer learning by re-building the IMDb movie sentiment analysis using GloVe embeddings in the next section. After that, we shall take a tour of BERT and apply BERT in the same sequential learning setting.</p>
    <h1 id="_idParaDest-71" class="title">IMDb sentiment analysis with GloVe embeddings</h1>
    <p class="normal">In <em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>, a BiLSTM model was built to<a id="_idIndexMarker226"/> predict the sentiment of IMDb movie reviews. That model learned embeddings of the words from scratch. This model had an accuracy of 83.55% on the test set, while the SOTA result was<a id="_idIndexMarker227"/> closer to 97.4%. If pre-trained embeddings are used, we expect an increase in model accuracy. Let's try this out and see the impact of transfer learning on this model. But first, let's understand the GloVe embedding model.</p>
    <h2 id="_idParaDest-72" class="title">GloVe embeddings</h2>
    <p class="normal">In <em class="chapterRef">Chapter 1</em>, <em class="italic">Essentials of NLP</em>, we discussed the Word2Vec algorithm, which is based on skip-grams with negative sampling. The GloVe model came out in 2014, a year after the Word2Vec paper <a id="_idIndexMarker228"/>came out. The GloVe and Word2Vec models are similar as the embeddings generated for a word are determined by the words that occur around it. However, these context words occur with different frequencies. Some of these context words appear more frequently in the text compared to other words. Due to this difference in frequencies of occurrence, training data for some words may be more common than other words.</p>
    <p class="normal">Beyond this part, Word2Vec does not use these statistics of co-occurrence in any way. GloVe takes these frequencies into account and posits that the co-occurrences provide vital information. The <em class="italic">Global</em> part of the name refers to the fact that the model considers these co-occurrences over the entire corpus. Rather than focus on the probabilities of co-occurrence, GloVe focuses on the ratios of co-occurrence considering probe words.</p>
    <p class="normal">In the paper, the authors take the example of the words <em class="italic">ice</em> and <em class="italic">steam</em> to illustrate the concept. Let's say that <em class="italic">solid</em> is another word that is going to be used to probe the relationship between ice and steam. A probability of occurrence of solid given steam is <em class="italic">p</em><sub class="" style="font-style: italic;">solid|steam</sub>. Intuitively, we expect this probability to be small. Conversely, the probability of occurrence of solid with ice is represented by <em class="italic">p</em><sub class="" style="font-style: italic;">solid|ice</sub> and is expected to be large. If <img src="image/B16252_04_001.png" alt="" style="max-height: 40px;"/> is computed, we expect this value to be significant. If the same ratio is computed with the probe word being gas, the opposite behavior would be expected. In cases where both are equally probable, either due to the probe word being unrelated, or equally probable to occur with the two words, then the ratio should be closer to 1. An example of a probe word close to both ice and steam is <em class="italic">water</em>. An example of a word unrelated to ice or steam is <em class="italic">fashion</em>. GloVe ensures that this relationship is factored into the embeddings generated for the words. It also has optimizations for rare co-occurrences, numerical stability issues computation, and others.</p>
    <p class="normal">Now let us see how to use these pre-trained embeddings for predicting sentiment. The first step is to load the data. The code here is identical to the code used in <em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>; it's provided here for the sake of completeness.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">All the code for this exercise is in the file <code class="Code-In-Text--PACKT-">imdb-transfer-learning.ipynb</code> located in the <code class="Code-In-Text--PACKT-">chapter4-Xfer-learning-BERT</code> directory in GitHub.</p>
    </div>
    <h2 id="_idParaDest-73" class="title">Loading IMDb training data</h2>
    <p class="normal">TensorFlow <a id="_idIndexMarker229"/>Datasets or the <code class="Code-In-Text--PACKT-">tfds</code> package will be used to load the data:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
imdb_train, ds_info = tfds.load(name=<span class="hljs-string">"imdb_reviews"</span>,
                      split=<span class="hljs-string">"train"</span>, 
                      with_info=<span class="hljs-literal">True</span>, as_supervised=<span class="hljs-literal">True</span>)
imdb_test = tfds.load(name=<span class="hljs-string">"imdb_reviews"</span>, split=<span class="hljs-string">"test"</span>, 
                      as_supervised=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Note that the additional 50,000 reviews that are unlabeled are ignored for the purpose of this exercise. After the training and test sets are loaded as shown above, the content of the reviews needs to be tokenized and encoded:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Use the default tokenizer settings</span>
tokenizer = tfds.features.text.Tokenizer()
vocabulary_set = <span class="hljs-built_in">set</span>()
MAX_TOKENS = <span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> example, label <span class="hljs-keyword">in</span> imdb_train:
  some_tokens = tokenizer.tokenize(example.numpy())
  <span class="hljs-keyword">if</span> MAX_TOKENS &lt; <span class="hljs-built_in">len</span>(some_tokens):
            MAX_TOKENS = <span class="hljs-built_in">len</span>(some_tokens)
  vocabulary_set.update(some_tokens)
</code></pre>
    <p class="normal">The code shown above tokenizes the review text and constructs a vocabulary. This vocabulary is used to construct a tokenizer:</p>
    <pre class="programlisting code"><code class="hljs-code">imdb_encoder = tfds.features.text.TokenTextEncoder(vocabulary_set,
                                                   <span class="code-highlight"><strong class="hljs-slc">lowercase=</strong><strong class="hljs-literal-slc">True</strong></span>,
                                                   tokenizer=tokenizer)
vocab_size = imdb_encoder.vocab_size
print(vocab_size, MAX_TOKENS)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">93931 2525
</code></pre>
    <p class="normal">Note that text was converted to lowercase before encoding. Converting to lowercase helps reduce the vocabulary size and may benefit the lookup of corresponding GloVe vectors later on. Note that capitalization may contain important information, which may help in tasks such as NER, which we covered in previous chapters. Also note that all languages do not distinguish between capital and small letters. Hence, this particular transformation should be applied after due consideration.</p>
    <p class="normal">Now that the<a id="_idIndexMarker230"/> tokenizer is ready, the data needs to be tokenized, and sequences padded to a maximum length. Since we are interested in comparing performance with the model trained in <em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>, we can use the same setting of sampling a maximum of 150 words of the review. The following convenience methods help in performing this task:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># transformation functions to be used with the dataset</span>
<span class="hljs-keyword">from</span> tensorflow.keras.preprocessing <span class="hljs-keyword">import</span> sequence
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">encode_pad_transform</span><span class="hljs-functio">(</span><span class="hljs-params">sample</span><span class="hljs-functio">):</span>
    encoded = imdb_encoder.encode(sample.numpy())
    pad = sequence.pad_sequences([encoded], padding=<span class="hljs-string">'post'</span>, 
                                 maxlen=<span class="hljs-number">150</span>)
    <span class="hljs-keyword">return</span> np.array(pad[<span class="hljs-number">0</span>], dtype=np.int64)  
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">encode_tf_fn</span><span class="hljs-functio">(</span><span class="hljs-params">sample, label</span><span class="hljs-functio">):</span>
    encoded = tf.py_function(encode_pad_transform, 
                                       inp=[sample], 
                                       Tout=(tf.int64))
    encoded.set_shape([<span class="hljs-literal">None</span>])
    label.set_shape([])
    <span class="hljs-keyword">return</span> encoded, label 
</code></pre>
    <p class="normal">Finally, the data is encoded using the convenience functions above like so:</p>
    <pre class="programlisting code"><code class="hljs-code">encoded_train = imdb_train.<span class="hljs-built_in">map</span>(encode_tf_fn,
                      num_parallel_calls=tf.data.experimental.AUTOTUNE)
encoded_test = imdb_test.<span class="hljs-built_in">map</span>(encode_tf_fn,
                      num_parallel_calls=tf.data.experimental.AUTOTUNE)
</code></pre>
    <p class="normal">At this point, all the training and test data is ready for training.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Note that in limiting the size of the reviews, only the first 150 tokens will be counted for a long review. Typically, the first few sentences of the review have the context or description, and the latter part of the review has the conclusion. By limiting to the first part of the review, valuable information could be lost. The reader is encouraged to try a different padding scheme where tokens from the first part of the review are dropped instead of the second part and observe the difference in the accuracy.</p>
    </div>
    <p class="normal">The next step is the <a id="_idIndexMarker231"/>foremost step in transfer learning – loading the pre-trained GloVe embeddings and using these as the weights of the embedding layer.</p>
    <h2 id="_idParaDest-74" class="title">Loading pre-trained GloVe embeddings</h2>
    <p class="normal">First, the pre-trained<a id="_idIndexMarker232"/> embeddings need to be downloaded and unzipped:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Download the GloVe embeddings</span>
!wget http://nlp.stanford.edu/data/glove.<span class="hljs-number">6</span>B.<span class="hljs-built_in">zip</span>
!unzip glove.<span class="hljs-number">6</span>B.<span class="hljs-built_in">zip</span>
Archive:  glove.<span class="hljs-number">6</span>B.<span class="hljs-built_in">zip</span>
  inflating: glove.<span class="hljs-number">6</span>B.<span class="hljs-number">50</span>d.txt        
  inflating: glove.<span class="hljs-number">6</span>B.<span class="hljs-number">100</span>d.txt       
  inflating: glove.<span class="hljs-number">6</span>B.<span class="hljs-number">200</span>d.txt       
  inflating: glove.<span class="hljs-number">6</span>B.<span class="hljs-number">300</span>d.txt   
</code></pre>
    <p class="normal">Note that this is a huge download of over 800 MB, so this step may take some time to execute. Upon unzipping, there will be four different files, as shown in the output above. Each file has a vocabulary of 400,000 words. The main difference is the dimensions of embeddings generated.</p>
    <p class="normal">In the previous chapter, an embedding dimension of 64 was used for the model. The nearest GloVe dimension is 50, so let's use that. The file format is quite simple. Each line of the text has multiple values separated by spaces. The first item of each row is the word, and the rest of the items are the values of the vector for each dimension. So, in the 50-dimensional file, each row will have 51 columns. These vectors need to be loaded up in memory:</p>
    <pre class="programlisting code"><code class="hljs-code">dict_w2v = {}
<span class="hljs-keyword">with</span> <span class="hljs-built_in">open</span>(<span class="hljs-string">'glove.6B.50d.txt'</span>, <span class="hljs-string">"r"</span>) <span class="hljs-keyword">as</span> file:
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> file:
        tokens = line.split()
        word = tokens[<span class="hljs-number">0</span>]
        vector = np.array(tokens[<span class="hljs-number">1</span>:], dtype=np.float32)
        <span class="hljs-keyword">if</span> vector.shape[<span class="hljs-number">0</span>] == <span class="hljs-number">50</span>:
            dict_w2v[word] = vector
        <span class="hljs-keyword">else</span>:
            print(<span class="hljs-string">"There was an issue with "</span> + word)
<span class="hljs-comment"># let's check the vocabulary size</span>
print(<span class="hljs-string">"Dictionary Size: "</span>, <span class="hljs-built_in">len</span>(dict_w2v))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Dictionary Size:  400000
</code></pre>
    <p class="normal">If the code processed the<a id="_idIndexMarker233"/> file correctly, you shouldn't see any errors and you should see a dictionary size of 400,000 words. Once these vectors are loaded, an embedding matrix needs to be created.</p>
    <h2 id="_idParaDest-75" class="title">Creating a pre-trained embedding matrix using GloVe</h2>
    <p class="normal">So far, we have a <a id="_idIndexMarker234"/>dataset, its vocabulary, and a dictionary of GloVe words and their corresponding vectors. However, there is no <a id="_idIndexMarker235"/>correlation between these two vocabularies. The way to connect them is through the creation of an embedding matrix. First, let's initialize an embedding matrix of zeros:</p>
    <pre class="programlisting code"><code class="hljs-code">embedding_dim = <span class="hljs-number">50</span>
embedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim))
</code></pre>
    <p class="normal">Note that this is a crucial step. When a pre-trained word list is used, finding a vector for each word in the training/test is not guaranteed. Recall the discussion on transfer learning earlier, where the source and target domains are different. One way this difference manifests itself is through having a mismatch in tokens between the training data and the pre-trained model. As we go through the next steps, this will become more apparent.</p>
    <p class="normal">After this embedding matrix of zeros is initialized, it needs to be populated. For each word in the vocabulary of reviews, the corresponding vector is retrieved from the GloVe dictionary. </p>
    <p class="normal">The ID<a id="_idIndexMarker236"/> of the word is<a id="_idIndexMarker237"/> retrieved using the encoder, and then the embedding matrix entry corresponding to that entry is set to the retrieved vector:</p>
    <pre class="programlisting code"><code class="hljs-code">unk_cnt = <span class="hljs-number">0</span>
unk_set = <span class="hljs-built_in">set</span>()
<span class="hljs-keyword">for</span> word <span class="hljs-keyword">in</span> imdb_encoder.tokens:
    embedding_vector = dict_w2v.get(word)
    <span class="hljs-keyword">if</span> embedding_vector <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:
        tkn_id = imdb_encoder.encode(word)[<span class="hljs-number">0</span>]
        embedding_matrix[tkn_id] = embedding_vector
    <span class="hljs-keyword">else</span>:
        unk_cnt += <span class="hljs-number">1</span>
        unk_set.add(word)
<span class="hljs-comment"># Print how many weren't found</span>
print(<span class="hljs-string">"Total unknown words: "</span>, unk_cnt)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Total unknown words:  14553
</code></pre>
    <p class="normal">During the data loading step, we saw that the total number of tokens was 93,931. Out of these, 14,553 words could not be found, which is approximately 15% of the tokens. For these words, the embedding matrix will have zeros. This is the first step in transfer learning. Now that the setup is completed, we will need to use TensorFlow to use these pre-trained embeddings. There will be two different models that will be tried – the first will be based on feature extraction and the second one on fine-tuning.</p>
    <h2 id="_idParaDest-76" class="title">Feature extraction model</h2>
    <p class="normal">As discussed earlier, the feature extraction model freezes the pre-trained weights and does not update them. An<a id="_idIndexMarker238"/> important issue with this approach in the current setup is that there are a large number of tokens, over 14,000, that have zero embedding vectors. These words could not be matched to an entry in the GloVe word list.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">To minimize the chances of not finding matches between the pre-trained vocabulary and task-specific vocabulary, ensure that similar tokenization schemes are used. GloVe uses a word-based tokenization scheme like the one provided by the Stanford tokenizer. As seen in <em class="chapterRef">Chapter 1</em>, <em class="italic">Essentials of NLP</em>, this works better than a whitespace tokenizer, which is used for the training data above. We see 15% unmatched tokens due to different tokenizers. As an exercise, the reader can implement the Stanford tokenizer and see the reduction in unknown tokens.</p>
      <p class="Tip--PACKT-">Newer methods like BERT use parts of subword tokenizers. Subword tokenization schemes can break up words into parts, which minimizes this chance of mismatch in tokens. Some examples of subword tokenization schemes are <strong class="keyword">Byte Pair Encoding</strong> (<strong class="keyword">BPE</strong>) or WordPiece <a id="_idIndexMarker239"/>tokenization. The BERT section of this chapter explains subword tokenization schemes in more detail.</p>
    </div>
    <p class="normal">If pre-trained vectors were not used, then the vectors for all the words would start with nearly zero and get trained through gradient descent. In this case, the vectors are already trained, so we <a id="_idIndexMarker240"/>expect the training to go along much faster. For a baseline, one epoch of training of the BiLSTM model while training embeddings takes between 65 seconds and 100 seconds, with most values around 63 seconds on an Ubuntu machine with an i5 processor and an Nvidia RTX-2070 GPU.</p>
    <p class="normal">Now, let's build the model and plug in the embedding matrix generated above into the model. Some basic parameters need to be set up:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Length of the vocabulary in chars</span>
vocab_size = imdb_encoder.vocab_size <span class="hljs-comment"># len(chars)</span>
<span class="hljs-comment"># Number of RNN units</span>
rnn_units = <span class="hljs-number">64</span>
<span class="hljs-comment"># batch size</span>
BATCH_SIZE=<span class="hljs-number">100</span>
</code></pre>
    <p class="normal">A convenience function being set up will enable fast switching. This method enables building models with the same architecture but different hyperparameters:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Embedding, LSTM, \
                                    Bidirectional, Dense
            
<span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">build_model_bilstm</span><span class="hljs-functio">(</span><span class="hljs-params">vocab_size, embedding_dim, </span>
<span class="hljs-params">                       rnn_units, batch_size, </span><span class="code-highlight"><strong class="hljs-params-slc">train_emb=</strong><strong class="hljs-literal-slc">False</strong></span><span class="hljs-functio">):</span>
  model = tf.keras.Sequential([
    Embedding(vocab_size, embedding_dim, mask_zero=<span class="hljs-literal">True</span>,
              <span class="code-highlight"><strong class="hljs-slc">weights=[embedding_matrix], trainable=train_emb</strong></span>),
   Bidirectional(LSTM(rnn_units, return_sequences=<span class="hljs-literal">True</span>, 
                                      dropout=<span class="hljs-number">0.5</span>)),
   Bidirectional(LSTM(rnn_units, dropout=<span class="hljs-number">0.25</span>)),
   Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>)
  ])
  <span class="hljs-keyword">return</span> model
</code></pre>
    <p class="normal">The model is identical to what was used in the previous chapter with the exception of the highlighted code pieces above. First, a flag can now be passed to this method that specifies whether the embeddings should be trained further or frozen. This parameter is set to false as it's the default value. The second change is in the definition of the <code class="Code-In-Text--PACKT-">Embedding</code> layer. A new<a id="_idIndexMarker241"/> parameter, <code class="Code-In-Text--PACKT-">weights</code>, loads the embedding matrix as the weights for the layer. Just after this parameter, a Boolean parameter called <code class="Code-In-Text--PACKT-">trainable</code> is passed that determines whether the weights of this layer should be updated during training time. A feature extraction-based model can now be created like so:</p>
    <pre class="programlisting code"><code class="hljs-code">model_fe = build_model_bilstm(
  vocab_size = vocab_size,
  embedding_dim=embedding_dim,
  rnn_units=rnn_units,
  batch_size=BATCH_SIZE)
model_fe.summary()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Model: "sequential_5"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
embedding_5 (Embedding)      (None, None, 50)          4696550   
_________________________________________________________________
bidirectional_6 (Bidirection (None, None, 128)         58880     
_________________________________________________________________
bidirectional_7 (Bidirection (None, 128)               98816     
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 129       
=================================================================
Total params: 4,854,375
Trainable params: 157,825
Non-trainable params: 4,696,550
_________________________________________________________________
</code></pre>
    <p class="normal">This model has about 4.8 million trainable parameters. It should be noted that this model is considerably smaller than the previous BiLSTM model, which had over 12 million parameters. A simpler or smaller model will train faster and possibly be less likely to overfit as the model capacity is lower.</p>
    <p class="normal">This model needs to be <a id="_idIndexMarker242"/>compiled with the loss function, optimizer, and metrics for observation progress of the model. Binary cross-entropy is the right loss function for this problem of binary classification. The Adam optimizer is a decent choice in most cases.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">Adaptive Moment Estimation or Adam Optimizer</strong></p>
      <p class="Information-Box--PACKT-">The simplest optimization <a id="_idIndexMarker243"/>algorithm used in backpropagation for the training of deep neural networks is mini-batch <strong class="keyword">Stochastic Gradient Descent</strong> (<strong class="keyword">SGD</strong>). Any error in the prediction is propagated back and weights, called parameters, of the various units are adjusted according to the error. Adam is a method that eliminates some of the issues of SGD such as getting trapped in sub-optimal local optima, and having the same learning rate for each parameter. Adam computes adaptive learning rates for each parameter and adjusts them based on not only the error but also previous adjustments. Consequently, Adam converges much faster than other optimization methods and is recommended as the default choice.</p>
    </div>
    <p class="normal">The metrics that will be observed are the same as before, accuracy, precision, and recall:</p>
    <pre class="programlisting code"><code class="hljs-code">model_fe.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'binary_crossentropy'</span>, 
             optimizer=<span class="hljs-string">'adam'</span>, 
             metrics=[<span class="hljs-string">'accuracy'</span>, <span class="hljs-string">'Precision'</span>, <span class="hljs-string">'Recall'</span>])
</code></pre>
    <p class="normal">After setting up batches for preloading, the model is ready for training. Similar to previously, the model will be trained for 10 epochs:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Prefetch for performance</span>
encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(<span class="hljs-number">100</span>)
model_fe.fit(encoded_train_batched, epochs=<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Epoch 1/10
250/250 [==============================] - 28s 113ms/step - loss: 0.5896 - accuracy: 0.6841 - Precision: 0.6831 - Recall: 0.6870
Epoch 2/10
250/250 [==============================] - 17s 70ms/step - loss: 0.5160 - accuracy: 0.7448 - Precision: 0.7496 - Recall: 0.7354
<span class="hljs-co -meta">...</span>
Epoch 9/10
250/250 [==============================] - 17s 70ms/step - loss: 0.4108 - accuracy: 0.8121 - Precision: 0.8126 - Recall: 0.8112
Epoch 10/10
250/250 [==============================] - 17s 70ms/step - loss: 0.4061 - accuracy: 0.8136 - Precision: 0.8147 - Recall: 0.8118
</code></pre>
    <p class="normal">A few things can be seen immediately. The model trained significantly faster. Each epoch took approximately 17 seconds with a maximum of 28 seconds for the first epoch. Secondly, the model has not overfit. The final accuracy is just over 81% on the training set. In the previous setup, the accuracy on the training set was 99.56%.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">It should also be noted that the accuracy was still increasing at the end of the tenth epoch, with lots of room to go. This indicates that training this model for longer would probably increase accuracy further. Quickly changing the number of epochs to 20 and training the model yields an accuracy of just over 85% on the testing set, with precision at 80% and recall at 92.8%.</p>
    </div>
    <p class="normal">For now, let's understand the <a id="_idIndexMarker244"/>utility of this model. To make an assessment of the quality of this model, performance on the test set should be evaluated:</p>
    <pre class="programlisting code"><code class="hljs-code">model_fe.evaluate(encoded_test.batch(BATCH_SIZE))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">250/Unknown - 21s 85ms/step - loss: 0.3999 - accuracy: 0.8282 - Precision: 0.7845 - Recall: 0.9050
</code></pre>
    <p class="normal">Compared to the previous model's accuracy of 83.6% on the test set, this model produces an accuracy of 82.82%. This performance is quite impressive because this model is just 40% of the size of the previous model and represents a 70% reduction in training time for a less than 1% drop in accuracy. This model has a slightly better recall for slightly worse accuracy. This result should not be entirely unexpected. There are over 14,000 word vectors that are zeros in this model! To fix this issue, and also to try the fine-tuning sequential transfer learning approach, let's build a fine-tuning-based model.</p>
    <h2 id="_idParaDest-77" class="title">Fine-tuning model</h2>
    <p class="normal">Creating the fine-tuning model is<a id="_idIndexMarker245"/> trivial when using the convenience function. All that is needed is to pass the <code class="Code-In-Text--PACKT-">train_emb</code> parameter as true:</p>
    <pre class="programlisting code"><code class="hljs-code">model_ft = build_model_bilstm(
  vocab_size=vocab_size,
  embedding_dim=embedding_dim,
  rnn_units=rnn_units,
  batch_size=BATCH_SIZE,
  train_emb=<span class="hljs-literal">True</span>)
model_ft.summary()
</code></pre>
    <p class="normal">This model is identical to the feature extraction model in size. However, since the embeddings will be fine-tuned, training is expected to take a little longer. There are several thousand zero embeddings, which can now be updated. The resulting accuracy is expected to be much better than the previous model. The model is compiled with the same loss function, optimizer, and metrics, and trained for 10 epochs:</p>
    <pre class="programlisting code"><code class="hljs-code">model_ft.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'binary_crossentropy'</span>, 
             optimizer=<span class="hljs-string">'adam'</span>, 
             metrics=[<span class="hljs-string">'accuracy'</span>, <span class="hljs-string">'Precision'</span>, <span class="hljs-string">'Recall'</span>])
model_ft.fit(encoded_train_batched, epochs=<span class="hljs-number">10</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Epoch 1/10
250/250 [==============================] - 35s 139ms/step - loss: 0.5432 - accuracy: 0.7140 - Precision: 0.7153 - Recall: 0.7111
Epoch 2/10
250/250 [==============================] - 24s 96ms/step - loss: 0.3942 - accuracy: 0.8234 - Precision: 0.8274 - Recall: 0.8171
<span class="hljs-co -meta">...</span>
Epoch 9/10
250/250 [==============================] - 24s 97ms/step - loss: 0.1303 - accuracy: 0.9521 - Precision: 0.9530 - Recall: 0.9511
Epoch 10/10
250/250 [==============================] - 24s 96ms/step - loss: 0.1132 - accuracy: 0.9580 - Precision: 0.9583 - Recall: 0.9576
</code></pre>
    <p class="normal">This accuracy is very impressive but needs to be checked against the test set:</p>
    <pre class="programlisting code"><code class="hljs-code">model_ft.evaluate(encoded_test.batch(BATCH_SIZE))
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">250/Unknown - 22s 87ms/step - loss: 0.4624 - accuracy: 0.8710 - Precision: 0.8789 - Recall: 0.8605
</code></pre>
    <p class="normal">That is the best result we have obtained so far at an accuracy of 87.1%. Data about state-of-the-art results on datasets are maintained by the <a href="http://paperswithcode.com"><span class="url">paperswithcode.com</span></a> website. Research papers that have reproducible code are featured on the leaderboards for datasets. This result would be about seventeenth on the SOTA result on the <a href="http://paperswithcode.com"><span class="url">paperswithcode.com</span></a> website at the time of writing!</p>
    <p class="normal">It can also be seen that the<a id="_idIndexMarker246"/> network is overfitting a little bit. A <code class="Code-In-Text--PACKT-">Dropout</code> layer can be added between the <code class="Code-In-Text--PACKT-">Embedding</code> layer and the first <code class="Code-In-Text--PACKT-">LSTM</code> layer to help reduce this overfitting. It should also be noted that this network is still much faster than training embeddings from scratch. Most epochs took 24 seconds for training. Overall, this model is smaller in size, takes much less time to train, and has much higher accuracy! This is why transfer learning is so important in machine learning in general and NLP more specifically.</p>
    <p class="normal">So far, we have seen the use of context-free word embeddings. The major challenge with this approach is that a word could have multiple meanings depending on the context. The word <em class="italic">bank</em> could refer to a place for storing money and valuables and also the side of a river. A more recent innovation in this area is BERT, published in May 2019. The next step in improving the accuracy of movie review sentiment analysis is to use a pre-trained BERT model. The next section explains the BERT model, its vital innovations, and the impact of using this model for the task at hand. Please note that the BERT model is enormous! If you do not have adequate local computing resources, using Google Colab with a GPU accelerator would be an excellent choice for the next section.</p>
    <h1 id="_idParaDest-78" class="title">BERT-based transfer learning</h1>
    <p class="normal">Embeddings like GloVe <a id="_idIndexMarker247"/>are context-free embeddings. Lack of context can be limiting in NLP contexts. As discussed before, the word bank can mean different things depending on the context. <strong class="keyword">Bi-directional Encoder Representations</strong> <strong class="keyword">from Transformers</strong>, or <strong class="keyword">BERT</strong>, came out of Google Research in May 2019 and demonstrated significant improvements on baselines. The BERT model builds on several innovations that came before it. The BERT paper also introduces several innovations of ERT works.</p>
    <p class="normal">Two foundational<a id="_idIndexMarker248"/> advancements that enabled BERT are the <strong class="keyword">encoder-decoder network</strong> architecture and<a id="_idIndexMarker249"/> the <strong class="keyword">Attention mechanism</strong>. The Attention mechanism further evolved to produce the <strong class="keyword">Transformer architecture</strong>. The Transformer <a id="_idIndexMarker250"/>architecture is the fundamental building block of BERT. These concepts are covered next and detailed further in later chapters. After these two <a id="_idIndexMarker251"/>sections, we will discuss specific innovations and structures of the BERT model.</p>
    <h2 id="_idParaDest-79" class="title">Encoder-decoder networks</h2>
    <p class="normal">We have seen the <a id="_idIndexMarker252"/>use of LSTMs and BiLSTMs on sentences modeled as sequences of words. These sequences can be of varying lengths as sentences are composed of a different number of words. Recall that in <em class="chapterRef">Chapter 2</em>, <em class="italic">Understanding Sentiment in Natural Language with BiLSTMs</em>, we discussed the core concept of an LSTM being a unit unrolled in time. For each input token, the LSTM unit generated an output. Consequently, the number of outputs produced by the LSTM depends on the number of input tokens. All of these input tokens are combined through a <code class="Code-In-Text--PACKT-">TimeDistributed()</code> layer for use by later <code class="Code-In-Text--PACKT-">Dense()</code> layers in the network. The main issue is that the input and output sequence lengths are linked. This model cannot handle variable-length sequences effectively. Translation-type tasks where the input and the output may have different lengths, consequently, won't do well with this architecture.</p>
    <p class="normal">The solution to these challenges was posed in a paper titled <em class="italic">Sequence to Sequence Learning with Neural Networks</em> written by Ilya <a id="_idIndexMarker253"/>Sutskever et al. in 2014. This model is also referred to as the <strong class="keyword">seq2seq</strong> model. </p>
    <p class="normal">The basic idea is shown in the figure below:</p>
    <figure class="mediaobject"><img src="image/B16252_04_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.4: Encoder-decoder network</p>
    <p class="normal">The model is divided into two parts – an encoder and a decoder. A special token that denotes the end of the input sequence is appended to the input sequence. Note that now the input sequence can have any length as this end of sentence token, (<strong class="keyword">EOS</strong>) in the figure above, denotes the end. In the figure above, the input sequence is denoted by tokens (<strong class="keyword">I</strong><sub class="" style="font-weight: bold;">1</sub>, <strong class="keyword">I</strong><sub class="" style="font-weight: bold;">2</sub>, <strong class="keyword">I</strong><sub class="" style="font-weight: bold;">3</sub>,…). Each input token, after vectorization, is passed to an LSTM model. The output is only collected from the last (<strong class="keyword">EOS</strong>) token. The vector generated by the encoder LSTM network for the (<strong class="keyword">EOS</strong>) token is a representation of the entire input sequence. It can be thought of as a summary of the entire input. A variable-length sequence has not been transformed into a fixed-length or dimensional vector.</p>
    <p class="normal">This vector <a id="_idIndexMarker254"/>becomes the input to the decoder layer. The model is auto-regressive in the sense that the output generated by the previous step of the decoder is fed into the next step as input. Output generation continues until the special (<strong class="keyword">EOS</strong>) token is generated. This scheme allows the model to determine the length of the output sequence. It breaks apart the dependency between the length of the input and output sequences. Conceptually, this is a straightforward model to understand. However, this is a potent model. Many tasks can be cast as a sequence-to-sequence problem.</p>
    <p class="normal">Some examples include translating a sentence from one language to another, summarizing an article where the input sequence is the text of the article and the output sequence is the summary, or question-answering where the question is the input sequence and the output is the answer. Speech recognition is a sequence-to-sequence problem with input sequences of 10 ms samples of voice, and the output is text. At the time of its release, it garnered much attention because it had a massive impact on the quality of Google Translate. In nine months of work using this model, the team behind the seq2seq model was able to provide much higher performance than that after over 10 years of improvements in Google Translate.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">The Great A.I. Awakening</strong></p>
      <p class="Information-Box--PACKT-">The New York Times published a fantastic article with the above title in 2016 that documents the journey of deep learning and especially the authors of the seq2seq paper and its dramatic effect on the quality of Google Translate. This article is highly recommended to see how transformational this architecture was for NLP. This article is available at <a href="https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html"><span class="url">https://www.nytimes.com/2016/12/14/magazine/the-great-ai-awakening.html</span></a>.</p>
    </div>
    <p class="normal">With these techniques at hand, the next innovation was the use of the Attention mechanism, which allows the modeling of dependencies <a id="_idIndexMarker255"/>between tokens irrespective of their distance. The Attention model became the cornerstone of the <strong class="keyword">Transformer model</strong>, described in the next section.</p>
    <h2 id="_idParaDest-80" class="title">Attention model </h2>
    <p class="normal">In the encoder-decoder <a id="_idIndexMarker256"/>model, the encoder part of the network creates a fixed dimensional representation of the input sequence. As the input sequence length grows, more and more of the input is compressed into this vector. The encodings or hidden states generated by processing the input tokens are not available to the decoder layer. The encoder states are hidden from the decoder. The Attention mechanism allows the decoder part of the network to see the encoder hidden states. These hidden states are depicted in <em class="italic">Figure 4.4</em> as the output of each of the input tokens, (I<sub class="Subscript--PACKT-">1</sub>, I<sub class="Subscript--PACKT-">2</sub>, I<sub class="Subscript--PACKT-">3</sub>,…), but shown only as feeding in to the next input token.</p>
    <p class="normal">In the Attention mechanism, these input token encodings will also be made available to the decoder layer. This is <a id="_idIndexMarker257"/>called <strong class="keyword">General Attention</strong>, and it refers to the ability of output tokens to directly have a dependence on the encodings or hidden states of input tokens. The main innovation here is the decoder operates on a sequence of vectors generating by encoding the input rather than one fixed vector generated at the end of the input. The Attention mechanism allows the decoder to focus its attention on a subset of the encoded input vectors while decoding, hence the name.</p>
    <p class="normal">There is another form of<a id="_idIndexMarker258"/> attention, called <strong class="keyword">self-attention</strong>. Self-attention enables connections between different encodings of input tokens in different positions. As depicted in the model in <em class="italic">Figure 4.4</em>, an input token only sees the encoding of the previous token. Self-attention will allow it to look at the encodings of previous tokens. Both forms are an improvement to the encoder-decoder architecture.</p>
    <p class="normal">While there are many Attention architectures, a prevalent form is called <strong class="keyword">Bahdanau Attention</strong>. It is named after the<a id="_idIndexMarker259"/> first author of the paper, published in 2016, where this Attention mechanism was proposed. Building on the encoder-decoder network, this form enables each output state to look at the encoded inputs and learn some weights for each of these inputs. Consequently, each output could focus on different input tokens. An illustration of this model is shown in <em class="italic">Figure 4.5</em>, which is a modified version of <em class="italic">Figure 4.4</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_04_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.5: Bahdanau Attention architecture</p>
    <p class="normal">Two specific changes have been made in the Attention mechanism when compared to the encoder-decoder architecture. The first change is in the encoder. The encoder layer here uses BiLSTMs. The use of BiLSTMs allows each word to learn from the words preceding and succeeding them both. In the standard encoder-decoder architecture, LSTMs were used, which meant each input word could only learn from the words before it.</p>
    <p class="normal">The second <a id="_idIndexMarker260"/>change is related to how the decoder uses the output of the encoders. In the previous architecture, only the output of the last token, the end-of-sentence token, used the summary of the entire input sequence. In the Bahdanau Attention architecture, the hidden state output of each input token is multiplied by an <em class="italic">alignment weight</em> that represents the degree of match between the input token at a specific position with the output token in question. A context vector is computed by multiplying each input hidden state output with the corresponding alignment weight and concatenating all the results. This context vector is fed to the output token along with the previous output token.</p>
    <p class="normal"><em class="italic">Figure 4.5</em> shows this computation, for only the second output token. This alignment model with the weights for each output token can help point to the most helpful input tokens in generating that<a id="_idIndexMarker261"/> output token. Note that some of the details have been simplified for brevity and can be found in the paper. We will implement Attention from scratch in later chapters.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">Attention is not an explanation</strong></p>
      <p class="Information-Box--PACKT-">It can be tempting to interpret the alignment scores or attention weights as an explanation of the model predicting a particular output token. A paper with the title of this information box was published that tests this hypothesis that Attention is an explanation. The conclusion from the research is that Attention should not be interpreted as an explanation. Different attention weights on the same set of inputs may result in the same outputs.</p>
    </div>
    <p class="normal">The next advancement to the Attention model came in the form of the Transformer architecture in 2017. The Transformer model is the key to the BERT architecture, so let's understand that next.</p>
    <h2 id="_idParaDest-81" class="title">Transformer model</h2>
    <p class="normal">Vaswani et al. published a ground-breaking paper in 2017 titled <em class="italic">Attention Is All You Need</em>. This paper laid the foundation of the Transformer model, which has been behind most of the recent advanced <a id="_idIndexMarker262"/>models such as ELMo, GPT, GPT-2, and BERT. The transformer model is built on the Attention model by taking the critical innovation from it – enabling the decoder to see all of the input hidden states while getting rid of the recurrence in it, which makes the model slow to train due to the sequential nature of processing the input sequences.</p>
    <p class="normal">The Transformer model has an encoder and a decoder part. This encoder-decoder structure enables it to perform best on machine translation-type tasks. However, not all tasks need full encoder and decoder layers. BERT only uses the encoder part, while generative models like GPT-2 use the decoder part. In this section, only the encoder part of the architecture is covered. The next chapter deals with the generation of text and the best models that use the Transformer decoder. Hence, the decoder will be covered in that chapter.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><strong class="scree Text">What is a Language Model?</strong></p>
      <p class="Information-Box--PACKT-">A <strong class="keyword">Language Model</strong> (<strong class="keyword">LM</strong>) task is traditionally defined as predicting the next word in a sequence of words. LMs <a id="_idIndexMarker263"/>are particularly useful for text generation, but less for classification. GPT-2 is an example of a model that fits this definition of an LM. Such a model only has context from the words or tokens that have occurred on its left (reverse for a right-to-left language). This is a trade-off that is appropriate in the generation of text. However, in other tasks such as question-answering or translation, the full sentence should be available. In such a case, using a bi-directional model that can use the context from both sides is useful. BERT is such a model. It loses the auto-regression property in favor of gaining context from both sides of a word of the token.</p>
    </div>
    <p class="normal">An encoder block of the Transformer has sub-layers parts – the multi-head self-attention sub-layer and a feed-forward sub-layer. The self-attention sub-layer looks at all the words of the<a id="_idIndexMarker264"/> input sequence and generates an encoding for these words in the context of each other. The feed-forward sublayer is composed of two layers using linear transformations and a ReLU activation in between. Each encoder block is composed of these two sub-layers, while the entire encoder is composed of six such blocks, as shown in <em class="italic">Figure 4.6</em>:</p>
    <figure class="mediaobject"><img src="image/B16252_04_06.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.6: Transformer encoder architecture</p>
    <p class="normal">A residual connection around the multi-head attention block and the feed-forward block is made in each encoder block. While <a id="_idIndexMarker265"/>adding the output of the sublayer with the input it received, layer normalization is performed. The main<a id="_idIndexMarker266"/> innovation here is the <strong class="keyword">Multi-Head Attention</strong> block. There are eight identical attention blocks whose outputs are concatenated to produce the multi-head attention output. Each attention block takes in the encoding and defines three new vectors called the query, key, and value vectors. Each of these vectors is defined as 64-dimensional, though this size is a hyperparameter that can be tuned. The query, key, and value vectors are learned through training.</p>
    <p class="normal">To understand how this works, let's assume that the input has three tokens. Each token has a corresponding embedding. Each of these tokens is initialized with its query, key, and value vectors. A weight vector is also initialized, which, when multiplied with the embedding of the input token, produces the key for that token. After the query vector is computed for a token, it is<a id="_idIndexMarker267"/> multiplied by the key vectors of all the input tokens. Note that the encoder has access to all the inputs, on both sides of each token. As a result, a score has now been computed by taking the query vector of the word in question and the value vector of all the tokens in the input sequence. All of these scores are passed through a softmax. The result can be interpreted as providing a sense of which tokens of the input are important to this particular input token.</p>
    <p class="normal">In a way, the input token in question is attentive to the other tokens with a high softmax score. This score is expected to be high when the input token attends to itself but can be high for other tokens as well. Next, this softmax score is multiplied by the value vector of each token. All these value vectors of the different input tokens are then summed up. Value vectors of tokens with higher softmax scores will have a higher contribution to the output value vector of the input token in question. This completes the calculation of the output for a given token in the Attention layer.</p>
    <p class="normal">Multi-head self-attention creates multiple copies of the query, key, and value vectors along with the weights matrix used to compute the query from the embedding of the input token. The paper proposed eight heads, though this could be experimented with. An additional weight matrix is used to combine the multiple outputs of each of the heads and concatenate them together into one output value vector.</p>
    <p class="normal">This output value vector is fed to the feed-forward layer, and the output of the feed-forward layer goes to the next encoder block or becomes the output of the model at the final encoder block.</p>
    <p class="normal">While the core BERT model is essentially the core Transformer encoder model, there are a few specific enhancements it introduced that are covered next. Note that using the BERT model is much easier as all of these details are abstracted. Knowing these details may, however, help in understanding BERT inputs and outputs. The code to use BERT for the IMDb sentiment analysis follows the next section.</p>
    <h2 id="_idParaDest-82" class="title">The bidirectional encoder representations from transformers (BERT) model</h2>
    <p class="normal">The emergence of the Transformer architecture was a seminal moment in the NLP world. This architecture has driven a lot of innovation through several derivative architectures. BERT is one<a id="_idIndexMarker268"/> such model. It was released in 2018. The BERT model only uses the encoder part of the Transformer architecture. The layout of the encoder is identical to the one described earlier with twelve encoder blocks and twelve attention heads. The size of the hidden layers is 768. These sets of parameters are referred to as <em class="italic">BERT Base</em>. These hyperparameters result in a total model size of 110 million parameters. A larger model was also published with 24 encoder blocks, 16 attention heads, and a hidden unit size of 1,024. Since the paper came out, a number of different variants of BERT like ALBERT, DistilBERT, RoBERTa, CamemBERT, and so on have also emerged. Each of these models has tried to improve the BERT performance in terms of accuracy or in terms of training/inference time.</p>
    <p class="normal">The way BERT is pre-trained is unique. It uses the multi-task transfer learning principle explained above to pre-train on two different objectives. The first objective is the <strong class="keyword">Masked Language Model</strong> (<strong class="keyword">MLM</strong>) task. In this<a id="_idIndexMarker269"/> task, some of the input tokens are masked randomly. The model has to predict the right token given the tokens on both sides of the masked token. Specifically, a token in the input sequence is replaced with a special <code class="Code-In-Text--PACKT-">[MASK]</code> token 80% of the time. In 10% of the cases, the selected token is replaced with another random token from the vocabulary. In the last 10% of the cases, the token is kept unchanged. Further, this happens for 15% of the overall tokens in a batch. The consequence of this scheme is that the model cannot rely on certain tokens being present and is forced to learn a contextual representation based on the distribution of the tokens before and after any given token. Without this masking, the bidirectional nature of the model means each word would be able to indirectly <em class="italic">see</em> itself from either direction. This would make the task of predicting the target token really easy.</p>
    <p class="normal">The second objective the model is <a id="_idIndexMarker270"/>pre-trained on is <strong class="keyword">Next Sentence Prediction</strong> (<strong class="keyword">NSP</strong>). The intuition here is that there are many NLP tasks that deal with pairs of sentences. For example, a question-answering problem can model the question as the first sentence, and the passage to be used to answer the question becomes the second sentence. The output from the model may be a span identifier that identifies the start and end token indices in the passage provided as the answer to the question. In the case of sentence similarity or paraphrasing, both sentence pairs can be passed in to get a similarity score. The NSP model is trained by passing in sentence pairs with a binary label that indicates whether the second sentence follows the first sentence. 50% of the training examples are passed as actual next sentences from the corpus with the label <strong class="keyword">IsNext</strong>, while in the other 50% a random sentence is passed with the output label <strong class="keyword">NotNext</strong>.</p>
    <p class="normal">BERT also addresses a problem we saw in the GloVe example above – out-of-vocabulary tokens. About 15% of the tokens were not in the vocabulary. To address this problem, BERT uses the <strong class="keyword">WordPiece</strong> tokenization<a id="_idIndexMarker271"/> scheme with a vocabulary size of 30,000 tokens. Note that this is much smaller than the GloVe vocabulary size. WordPiece belongs to a class of tokenization <a id="_idIndexMarker272"/>schemes called <strong class="keyword">subword</strong> tokenization. Other members of this class are <strong class="keyword">Byte Pair Encoding</strong> (<strong class="keyword">BPE</strong>), SentencePiece, and<a id="_idIndexMarker273"/> the Unigram language model. Inspiration for the WordPiece model came from the Google Translate team working with Japanese and Korean <a id="_idIndexMarker274"/>texts. If you recall the discussion on tokenization in the first chapter, we showed that the Japanese language does not use spaces for delimiting words. Hence, it is hard to tokenize it into words. Methods developed for creating vocabularies for such languages are quite useful for applying to languages like English and keeping the dictionary size down to a reasonable size.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Consider the German translation of the phrase <em class="italic">Life Insurance Company</em>. This would translate to <em class="italic">Lebensversicherungsgesellschaft</em>. Similarly, <em class="italic">Gross Domestic Product</em> would translate to <em class="italic">Bruttoinlandsprodukt</em>. If words are taken as such, the size of the vocabulary would be very large. A subword approach could represent these words more efficiently.</p>
    </div>
    <p class="normal">A smaller dictionary reduces training time and memory requirements. If a smaller dictionary does not come at the cost of out-of-vocabulary tokens, then it is quite useful. To help understand the concept of subword tokenization, consider an extreme example where the tokenization breaks apart the work into individual characters and numbers. The size of this vocabulary would be 37 – with 26 alphabets, 10 numbers, and space. An example of a subword tokenization scheme is to introduce two new tokens, <em class="italic">-ing</em> and <em class="italic">-tion</em>. Every word that ends with these two tokens can be broken into two subwords – the part before the suffix and one of the two suffixes. This can be done through knowledge of the language grammar and constructs, using techniques such as stemming and lemmatization. The WordPiece tokenization approach used in BERT is based on BPE. In BPE, the first step is defining a target vocabulary size.</p>
    <p class="normal">Next, the entire text is converted to a vocabulary of just the individual character tokens and mapped to the frequency of occurrence. Now multiple passes are made on this to combine pairs of tokens so as to maximize the frequency of the bigram created. For each subword created, a special token is added to denote the end of the word so that detokenization can be performed. Further, if the subword is not the start of the word, a <code class="Code-In-Text--PACKT-">##</code> tag is added to help in reconstructing the original words. This process is continued until the desired vocabulary is hit, or the base condition of a minimum frequency of 1 is hit for tokens. BPE maximizes the frequency, and WordPiece builds on top of this to include another objective. </p>
    <p class="normal">The objective for WordPiece includes increasing mutual information by considering the frequencies of the tokens being merged along with the frequency of the merged bigram. This introduces a minor adjustment to the model. RoBERTa from Facebook experimented with using a BPE model and did not see a material difference in performance. The GPT-2 generative model is based on the BPE model.</p>
    <p class="normal">To take an example from the IMDb dataset, here is an example sentence:</p>
    <pre class="programlisting gen"><code class="hljs">This was an absolutely terrible movie. Don't be <code style="font-weight: bold;" class="codeHighlighted">lured</code> in by Christopher Walken or Michael Ironside.
</code></pre>
    <p class="normal">After tokenization with BERT, it would look like this:</p>
    <pre class="programlisting gen"><code class="hljs">[CLS] This was an absolutely terrible movie . Don' t be <code style="font-weight: bold;" class="codeHighlighted">lure ##d</code> in by Christopher Walk ##en or Michael Iron ##side . [SEP]
</code></pre>
    <p class="normal">Where <code class="Code-In-Text--PACKT-">[CLS]</code> and <code class="Code-In-Text--PACKT-">[SEP]</code> are special tokens, which will be introduced shortly. Note how the word <em class="italic">lured</em> was broken up as a consequence. Now that we understand the underlying construct of the BERT model, let's try to use it for transfer learning on the IMDb sentiment classification <a id="_idIndexMarker275"/>problem. The first step is preparing the data.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">All the code for the BERT implementation can be found in the <code class="Code-In-Text--PACKT-">imdb-transfer-learning.ipynb</code> notebook in this chapter's GitHub folder, in the section <em class="italic">BERT-based transfer learning</em>. Please run the code in the section titled <em class="italic">Loading IMDb training data</em> to ensure the data is loaded prior to proceeding.</p>
    </div>
    <h3 id="_idParaDest-83" class="title">Tokenization and normalization with BERT</h3>
    <p class="normal">After reading the <a id="_idIndexMarker276"/>description of the BERT model, you may be bracing yourself for a difficult implementation in code. Have no fear. Our friends at Hugging Face have provided pre-trained models <a id="_idIndexMarker277"/>as well as abstractions that make working with advanced models like BERT a breeze. The general flow for getting BERT to work will be:</p>
    <ol>
      <li class="numbered">Load a pre-trained model</li>
      <li class="numbered">Instantiate a tokenizer and tokenize the data</li>
      <li class="numbered">Set up a model and compile it</li>
      <li class="numbered">Fit the model on the data</li>
    </ol>
    <p class="normal">These steps won't take more than a few lines of code each. So let's get started. The first step is to install the Hugging Face libraries:</p>
    <pre class="programlisting code"><code class="hljs-code">!pip install transformers==3.0.2
</code></pre>
    <p class="normal">The tokenizer is the first step – it needs to be imported before it can be used:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer
bert_name = <span class="hljs-string">'bert-base-cased'</span>
tokenizer = BertTokenizer.from_pretrained(bert_name, 
                                          add_special_tokens=<span class="hljs-literal">True</span>, 
                                          do_lower_case=<span class="hljs-literal">False</span>,
                                          max_length=<span class="hljs-number">150</span>,
                                          pad_to_max_length=<span class="hljs-literal">True</span>) 
</code></pre>
    <p class="normal">That is all there is to<a id="_idIndexMarker278"/> load a pre-trained tokenizer! A few things to note in the code above. First, there are a number of models published by Hugging Face that are available for download. A<a id="_idIndexMarker279"/> full list of the models and their names can be found at <a href="https://huggingface.co/transformers/pretrained_models.html"><span class="url">https://huggingface.co/transformers/pretrained_models.html</span></a>. Some key BERT models that are available are:</p>
    <table id="table001-2">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Model Name</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Description</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="normal"><code class="Code-In-Text--PACKT-">bert-base-uncased</code> / <code class="Code-In-Text--PACKT-">bert-base-cased</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Variants of the base BERT model with 12 encoder layers, hidden size of 768 units, and 12 attention heads for a total of ~110 million parameters. The only difference is whether the inputs were cased or all lowercase.</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="normal"><code class="Code-In-Text--PACKT-">bert-large-uncased</code> / <code class="Code-In-Text--PACKT-">bert-large-cased</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">This model has 24 encoder layers, 1,024 hidden units, and 16 attention heads for a total of ~340 million parameters. Similar split by cased and lowercase models.</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="normal"><code class="Code-In-Text--PACKT-">bert-base-multilingual-cased</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Parameters here are the same as <code class="Code-In-Text--PACKT-">bert-base-cased</code> above, trained on 104 languages with the largest Wikipedia entries. However, it is not recommended to use the uncased version for international languages, while that model is available. </p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="normal"><code class="Code-In-Text--PACKT-">bert-base-cased-finetuned-mrpc</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">This model has been fine-tuned on the Microsoft Research Paraphrase Corpus task for paraphrase identification in the news domain. </p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="normal"><code class="Code-In-Text--PACKT-">bert-base-japanese</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Same size as the base model but trained on Japanese text. Note that both the MeCab and WordPiece tokenizers are used.</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="normal"><code class="Code-In-Text--PACKT-">bert-base-chinese</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Same size as the base model but trained on cased-simplified Chinese and traditional Chinese.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Any of the values on the left can be used in the <code class="Code-In-Text--PACKT-">bert_name</code> variable above to load the appropriate tokenizer. The second line in the code above downloads the configuration and the vocabulary file from the cloud and instantiates a tokenizer. This loader takes a number of parameters. Since a cased English model is being used, we don't want the tokenizer to convert words to lowercase as specified by the <code class="Code-In-Text--PACKT-">do_lower_case</code> parameter. Note that the default <a id="_idIndexMarker280"/>value of this parameter is <code class="Code-In-Text--PACKT-">True</code>. The input sentences will be tokenized to a maximum of 150 tokens, as we saw in the GloVe model as well. <code class="Code-In-Text--PACKT-">pad_to_max_length</code> further indicates that the tokenizer<a id="_idIndexMarker281"/> should also pad the sequences it generates.</p>
    <p class="normal">The first argument, <code class="Code-In-Text--PACKT-">add_special_tokens</code>, deserves some explanation. In the example so far, we have taken a sequence and a maximum length. If the sequence is shorter than this maximum length, then the sequence is padded with a special padding token. However, BERT has a special way to encode its sequence due to the next sentence prediction task pre-training. It needs a way to provide two sequences as the input. In the case of classification, like the IMDb sentiment prediction, the second sequence is just left empty. There are three sequences that need to <a id="_idIndexMarker282"/>be provided to the BERT model:</p>
    <ul>
      <li class="bullet"><code class="Code-In-Text--PACKT-">input_ids</code>: This corresponds to the tokens in the inputs converted into IDs. This is what we have been doing thus far in other examples. In the IMDb example, we only have one sequence. However, if the problem required passing in two sequences, then a special token, <code class="Code-In-Text--PACKT-">[SEP]</code>, would be added in between the sequences. <code class="Code-In-Text--PACKT-">[SEP]</code> is an example of a special token that has been added by the tokenizer. Another special token, <code class="Code-In-Text--PACKT-">[CLS]</code>, is appended to the start of the inputs. <code class="Code-In-Text--PACKT-">[CLS]</code> stands for classifier token. The embedding for this token can be viewed as the summary of the inputs in the case of a classification problem, and additional layers on top of the BERT model would use this token. It is also possible to use the sum of the embeddings of all the inputs as an alternative.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">token_type_ids</code>: If the input contains two sequences, for a question-answering problem, for example, then these IDs tell the model indicates which <code class="Code-In-Text--PACKT-">input_ids</code> correspond to which sequence. In some texts, this is referred to as the segment identifiers. The first sequence would be the first segment, and the second sequence would be the second segment.</li>
      <li class="bullet"><code class="Code-In-Text--PACKT-">attention_mask</code>: Given that the sequences are padded, this mask tells the model where the actual tokens end so that the attention calculation does not use the padding tokens.</li>
    </ul>
    <p class="normal">Given that BERT can take two sequences as input, understanding the padding is essential as it can be confusing how padding works in the context of the maximum sequence length when a pair of sequences is provided. The maximum sequence length refers to the combined length of the pair. There are three different ways to do truncation if the combined length <a id="_idIndexMarker283"/>exceeds the maximum length. The first two could be to reduce the lengths from either the first or the second sequence. The third way is to truncate from the lengthiest sequence, a token at a time so that the lengths of the pair are only off by one at maximum. In the <a id="_idIndexMarker284"/>constructor, this behavior can be configured by passing the <code class="Code-In-Text--PACKT-">truncation_strategy</code> parameter with the values <code class="Code-In-Text--PACKT-">only_first</code>, <code class="Code-In-Text--PACKT-">only_second</code>, or <code class="Code-In-Text--PACKT-">longest_first</code>.</p>
    <p class="normal"><em class="italic">Figure 4.7</em> shows how an input sequence is converted into the three input sequences listed above:</p>
    <figure class="mediaobject"><img src="image/B16252_04_07.png" alt="A close up of a keyboard  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.7: Mapping inputs to BERT sequences</p>
    <p class="normal">If the input sequence was <em class="italic">Don't be lured</em>, then the figure above shows how it is tokenized with the WordPiece tokenizer as well as the addition of special tokens. The example above sets a maximum sequence length of nine tokens. Only one sequence is provided, hence the token type IDs or segment IDs all have the same value. The attention mask is set to 1, where the corresponding entry in the tokens is an actual token. The following code is used to generate these encodings:</p>
    <pre class="programlisting code"><code class="hljs-code">tokenizer.encode_plus(<span class="hljs-string">" Don't be lured"</span>, add_special_tokens=<span class="hljs-literal">True</span>, 
                      max_length=<span class="hljs-number">9</span>,
                      pad_to_max_length=<span class="hljs-literal">True</span>, 
                      return_attention_mask=<span class="hljs-literal">True</span>, 
                      return_token_type_ids=<span class="hljs-literal">True</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">{'input_ids': [101, 1790, 112, 189, 1129, 19615, 1181, 102, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 0]}
</code></pre>
    <p class="normal">Even though we won't be using a pair of sequences in this chapter, it is useful to be aware of how the encodings look when a pair is passed. If two strings are passed to the tokenizer, then they <a id="_idIndexMarker285"/>are treated as a pair. This is shown in the code below:</p>
    <pre class="programlisting code"><code class="hljs-code">tokenizer.encode_plus(<span class="hljs-string">" Don't be"</span>,<span class="hljs-string">" lured"</span>, add_special_tokens=<span class="hljs-literal">True</span>, 
                      max_length=<span class="hljs-number">10</span>,
                      pad_to_max_length=<span class="hljs-literal">True</span>, 
                      return_attention_mask=<span class="hljs-literal">True</span>, 
                      return_token_type_ids=<span class="hljs-literal">True</span>)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">{'input_ids': [101, 1790, 112, 189, 1129, <span class="code-highlight"><strong class="hljs-con-slc">102</strong></span>, 19615, 1181, <span class="code-highlight"><strong class="hljs-con-slc">102</strong></span>, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, <span class="code-highlight"><strong class="hljs-con-slc">1, 1, 1</strong></span>, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0]}
</code></pre>
    <p class="normal">The input IDs have two separators to distinguish between the two sequences. The token type IDs help distinguish<a id="_idIndexMarker286"/> which tokens correspond to which sequence. Note that the token type ID for the padding token is set to 0. In the network, it is never used as all the values are multiplied by the attention mask.</p>
    <p class="normal">To perform encoding of the inputs for all the IMDb reviews, a helper function is defined, as shown below:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">bert_encoder</span><span class="hljs-functio">(</span><span class="hljs-params">review</span><span class="hljs-functio">):</span>
    txt = review.numpy().decode(<span class="hljs-string">'utf-8'</span>)
    encoded = tokenizer.encode_plus(txt, add_special_tokens=<span class="hljs-literal">True</span>, 
                                    max_length=<span class="hljs-number">150</span>, 
                                    pad_to_max_length=<span class="hljs-literal">True</span>, 
                                    return_attention_mask=<span class="hljs-literal">True</span>, 
                                    return_token_type_ids=<span class="hljs-literal">True</span>)
    <span class="hljs-keyword">return</span> encoded[<span class="hljs-string">'input_ids'</span>], encoded[<span class="hljs-string">'token_type_ids'</span>], \
           encoded[<span class="hljs-string">'attention_mask'</span>]
</code></pre>
    <p class="normal">The method is pretty straightforward. It takes the input tensor and uses UTF-8 decoding. Using the tokenizer, this input is converted into the three sequences.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">This would be a great opportunity to implement a different padding algorithm. For example, implement an algorithm that takes the last 150 tokens instead of the first 150 and compare the performance of the two methods.</p>
    </div>
    <p class="normal">Now, this needs to be applied to every review in the training data:</p>
    <pre class="programlisting code"><code class="hljs-code">bert_train = [bert_encoder(r) <span class="hljs-keyword">for</span> r, l <span class="hljs-keyword">in</span> imdb_train]
bert_lbl = [l <span class="hljs-keyword">for</span> r, l <span class="hljs-keyword">in</span> imdb_train]
bert_train = np.array(bert_train)
bert_lbl = tf.keras.utils.to_categorical(bert_lbl, num_classes=<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">Labels of the<a id="_idIndexMarker287"/> reviews are also converted into categorical values. Using the <code class="Code-In-Text--PACKT-">sklearn</code> package, the training data is split into<a id="_idIndexMarker288"/> training and validation sets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># create training and validation splits</span>
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
x_train, x_val, y_train, y_val = train_test_split(bert_train, 
                                         bert_lbl,
                                         test_size=<span class="hljs-number">0.2</span>, 
                                         random_state=<span class="hljs-number">42</span>)
print(x_train.shape, y_train.shape)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">(20000, 3, 150) (20000, 2)
</code></pre>
    <p class="normal">A little more data processing is required to wrangle the inputs into three input dictionaries in <code class="Code-In-Text--PACKT-">tf.DataSet</code> for easy use in training:</p>
    <pre class="programlisting code"><code class="hljs-code">tr_reviews, tr_segments, tr_masks = np.split(x_train, <span class="hljs-number">3</span>, axis=<span class="hljs-number">1</span>)
val_reviews, val_segments, val_masks = np.split(x_val, <span class="hljs-number">3</span>, axis=<span class="hljs-number">1</span>)
tr_reviews = tr_reviews.squeeze()
tr_segments = tr_segments.squeeze()
tr_masks = tr_masks.squeeze()
val_reviews = val_reviews.squeeze()
val_segments = val_segments.squeeze()
val_masks = val_masks.squeeze()
</code></pre>
    <p class="normal">These training and validation sequences are converted into a dataset like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-functio"> </span><span class="hljs-title">example_to_features</span><span class="hljs-functio">(</span><span class="hljs-params">input_ids,attention_masks,token_type_ids,y</span><span class="hljs-functio">):</span>
  <span class="hljs-keyword">return</span> {<span class="hljs-string">"input_ids"</span>: input_ids,
          <span class="hljs-string">"attention_mask"</span>: attention_masks,
          <span class="hljs-string">"token_type_ids"</span>: token_type_ids},y
train_ds = tf.data.Dataset.from_tensor_slices((tr_reviews, 
tr_masks, tr_segments, y_train)).\
            <span class="hljs-built_in">map</span>(example_to_features).shuffle(<span class="hljs-number">100</span>).batch(<span class="hljs-number">16</span>)
valid_ds = tf.data.Dataset.from_tensor_slices((val_reviews, 
val_masks, val_segments, y_val)).\
            <span class="hljs-built_in">map</span>(example_to_features).shuffle(<span class="hljs-number">100</span>).batch(<span class="hljs-number">16</span>)
</code></pre>
    <p class="normal">A batch size<a id="_idIndexMarker289"/> of 16 has been used here. The memory of the GPU is the limiting factor here. Google Colab can support a batch length of 32. An 8 GB RAM GPU can support a batch size of 16. Now, we are ready to train a model using BERT for classification. We will see two approaches. The first approach will use a pre-built classification model on top of BERT. This is<a id="_idIndexMarker290"/> shown in the next section. The second approach will use the base BERT model and adds custom layers on top to accomplish the same task. This technique will be demonstrated in the section after.</p>
    <h3 id="_idParaDest-84" class="title">Pre-built BERT classification model</h3>
    <p class="normal">Hugging Face libraries<a id="_idIndexMarker291"/> make it really easy to use a pre-built BERT model for classification by providing a class to do so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFBertForSequenceClassification
bert_model = TFBertForSequenceClassification.from_pretrained(bert_name)
</code></pre>
    <p class="normal">That was quite easy, wasn't it? Note that the instantiation of the model will require a download of the model from the cloud. However, these models are cached on the local machine if the code is being run from a local or dedicated machine. In the Google Colab environment, this download will be run every time a Colab instance is initialized. To use this model, we only need to provide an optimizer and a loss function and compile the model:</p>
    <pre class="programlisting code"><code class="hljs-code">optimizer = tf.keras.optimizers.Aadam(learning_rate=<span class="hljs-number">2e-5</span>)
loss = tf.keras.losses.BinaryCrossentropy(from_logits=<span class="hljs-literal">True</span>)
bert_model.<span class="hljs-built_in">compile</span>(optimizer=optimizer, loss=loss, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">This model is actually quite simple in<a id="_idIndexMarker292"/> layout as its summary shows:</p>
    <pre class="programlisting code"><code class="hljs-code">bert_model.summary()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Model: "tf_bert_for_sequence_classification_7"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  108310272 
_________________________________________________________________
dropout_303 (Dropout)        multiple                  0         
_________________________________________________________________
classifier (Dense)           multiple                  1538      
=================================================================
Total params: 108,311,810
Trainable params: 108,311,810
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">So, the model has the entire BERT model, a dropout layer, and a classifier layer on top. This is as simple as it gets.</p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">The BERT paper suggests some settings for fine-tuning. They suggest a batch size of 16 or 32, run for 2 to 4 epochs. Further, they suggest using one of the following learning rates for Adam: 5e-5, 3e-5, or 2e-5. Once this model is up and running in your environment, please feel free to train with different settings to see the impact on accuracy.</p>
    </div>
    <p class="normal">In the previous section, we batched the data into sets of 16. Here, the Adam optimizer is configured to use a learning rate of 2e-5. Let's train this model for 3 epochs. Note that training is going to be quite slow:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">"Fine-tuning BERT on IMDB"</span>)
bert_history = bert_model.fit(train_ds, epochs=<span class="hljs-number">3</span>, 
                              validation_data=valid_ds)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Fine-tuning BERT on IMDB
Train for 1250 steps, validate for 313 steps
Epoch 1/3
1250/1250 [==============================] - 480s 384ms/step - loss: 0.3567 - accuracy: 0.8320 - val_loss: 0.2654 - val_accuracy: 0.8813
Epoch 2/3
1250/1250 [==============================] - 469s 375ms/step - loss: 0.2009 - accuracy: 0.9188 - val_loss: 0.3571 - val_accuracy: 0.8576
Epoch 3/3
1250/1250 [==============================] - 470s 376ms/step - loss: 0.1056 - accuracy: 0.9613 - val_loss: 0.3387 - val_accuracy: 0.8883
</code></pre>
    <p class="normal">The validation accuracy is<a id="_idIndexMarker293"/> quite impressive for the little work we have done here if it holds on the test set. That needs to be checked next. Using the convenience methods from the previous section, the test data will be tokenized and encoded in the right format:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># prep data for testing</span>
bert_test = [bert_encoder(r) <span class="hljs-keyword">for</span> r,l <span class="hljs-keyword">in</span> imdb_test]
bert_tst_lbl = [l <span class="hljs-keyword">for</span> r, l <span class="hljs-keyword">in</span> imdb_test]
bert_test2 = np.array(bert_test)
bert_tst_lbl2 = tf.keras.utils.to_categorical (bert_tst_lbl,                                                num_classes=<span class="hljs-number">2</span>)
ts_reviews, ts_segments, ts_masks = np.split(bert_test2, <span class="hljs-number">3</span>, axis=<span class="hljs-number">1</span>)
ts_reviews = ts_reviews.squeeze()
ts_segments = ts_segments.squeeze()
ts_masks = ts_masks.squeeze()
test_ds = tf.data.Dataset.from_tensor_slices((ts_reviews, 
                    ts_masks, ts_segments, bert_tst_lbl2)).\
            <span class="hljs-built_in">map</span>(example_to_features).shuffle(<span class="hljs-number">100</span>).batch(<span class="hljs-number">16</span>)
</code></pre>
    <p class="normal">Evaluating the performance of this model on the test dataset, we get the following:</p>
    <pre class="programlisting code"><code class="hljs-code">bert_model.evaluate(test_ds)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1563/1563 [==============================] - 202s 129ms/step - loss: 0.3647 - accuracy: 0.8799
[0.3646871318983454, 0.8799]
</code></pre>
    <p class="normal">The model accuracy is almost 88%! This is higher than the best GloVe model shown previously, and it took much less code to<a id="_idIndexMarker294"/> implement.</p>
    <p class="normal">In the next section, let's try to build custom layers on top of the BERT model to take transfer learning to the next level.</p>
    <h3 id="_idParaDest-85" class="title">Custom model with BERT</h3>
    <p class="normal">The BERT model outputs <a id="_idIndexMarker295"/>contextual embeddings for all of the input tokens. The embedding corresponding to the <code class="Code-In-Text--PACKT-">[CLS]</code> token is generally used for classification tasks, and it represents the entire document. The pre-built model from Hugging Face returns the embeddings for the entire sequence as well as this <em class="italic">pooled output</em>, which represents the entire document as the output of the model. This pooled output vector can be used in future layers to help with the classification task. This is the approach we will take in building a customer model.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">The code for this section is under the heading <em class="italic">Customer Model With BERT</em> in the same notebook as above.</p>
    </div>
    <p class="normal">The starting point for this exploration is the base <code class="Code-In-Text--PACKT-">TFBertModel</code>. It can be imported and instantiated like so:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TFBertModel
bert_name = <span class="hljs-string">'bert-base-cased'</span>
bert = TFBertModel.from_pretrained(bert_name) 
bert.summary()
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Model: "tf_bert_model"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
bert (TFBertMainLayer)       multiple                  108310272 
=================================================================
Total params: 108,310,272
Trainable params: 108,310,272
Non-trainable params: 0
_________________________________________________________________
</code></pre>
    <p class="normal">Since we are using the same pre-trained model, the cased BERT-Base model, we can reuse the tokenized and prepared data from the section above. If you haven't already, take a moment to ensure the code in the <em class="italic">Tokenization and normalization with BERT</em> section has been run to prepare the data.</p>
    <p class="normal">Now, the custom<a id="_idIndexMarker296"/> model needs to be defined. The first layer of this model is the BERT layer. This layer will take three inputs, namely the input tokens, attention masks, and token type IDs:</p>
    <pre class="programlisting code"><code class="hljs-code">max_seq_len = <span class="hljs-number">150</span>
inp_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=<span class="hljs-string">"input_ids"</span>)
att_mask = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=<span class="hljs-string">"attention_mask"</span>)
seg_ids = tf.keras.layers.Input((max_seq_len,), dtype=tf.int64, name=<span class="hljs-string">"token_type_ids"</span>)
</code></pre>
    <p class="normal">These names need to match the dictionary defined in the training and testing dataset. This can be checked by printing the specification of the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">train_ds.element_spec
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">({'input_ids': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None),
  'attention_mask': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None),
  'token_type_ids': TensorSpec(shape=(None, 150), dtype=tf.int64, name=None)},
 TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))
</code></pre>
    <p class="normal">The BERT model expects these inputs in a dictionary. It can also accept the inputs as named arguments, but this approach is clearer and makes it easy to trace the inputs. Once the inputs are mapped, the output of the BERT model can be computed:</p>
    <pre class="programlisting code"><code class="hljs-code">inp_dict = {<span class="hljs-string">"input_ids"</span>: inp_ids,
            <span class="hljs-string">"attention_mask"</span>: att_mask,
            <span class="hljs-string">"token_type_ids"</span>: seg_ids}
outputs = bert(inp_dict)
<span class="hljs-comment"># let's see the output structure</span>
outputs
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">(&lt;tf.Tensor 'tf_bert_model_3/Identity:0' shape=(None, 150, 768) dtype=float32&gt;,
 &lt;tf.Tensor 'tf_bert_model_3/Identity_1:0' shape=(None, 768) dtype=float32&gt;)
</code></pre>
    <p class="normal">The first output has embeddings for each of the input tokens including the special tokens <code class="Code-In-Text--PACKT-">[CLS]</code> and <code class="Code-In-Text--PACKT-">[SEP]</code>. The<a id="_idIndexMarker297"/> second output corresponds to the output of the <code class="Code-In-Text--PACKT-">[CLS]</code> token. This output will be used further in the model:</p>
    <pre class="programlisting code"><code class="hljs-code">x = tf.keras.layers.Dropout(<span class="hljs-number">0.2</span>)(outputs[<span class="hljs-number">1</span>])
x = tf.keras.layers.Dense(<span class="hljs-number">200</span>, activation=<span class="hljs-string">'relu'</span>)(x)
x = tf.keras.layers.Dropout(<span class="hljs-number">0.2</span>)(x)
x = tf.keras.layers.Dense(<span class="hljs-number">2</span>, activation=<span class="hljs-string">'sigmoid'</span>)(x)
custom_model = tf.keras.models.Model(inputs=inp_dict, outputs=x)
</code></pre>
    <p class="normal">The model above is only illustrative, to demonstrate the technique. We add a dense layer and a couple of dropout layers before an output layer. Now, the customer model is ready for training. The model needs to be compiled with an optimizer, loss function, and metrics to watch for:</p>
    <pre class="programlisting code"><code class="hljs-code">optimizer = tf.keras.optimizers.Adam(learning_rate=<span class="hljs-number">2e-5</span>)
loss = tf.keras.losses.BinaryCrossentropy(from_logits=<span class="hljs-literal">True</span>)
custom_model.<span class="hljs-built_in">compile</span>(optimizer=optimizer, loss=loss, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">Here is what this model looks like:</p>
    <pre class="programlisting code"><code class="hljs-code">custom_model.summary()
</code></pre>
    <figure class="mediaobject"><img src="image/B16252_04_08.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="normal">This custom model has 154,202 additional trainable parameters in addition to the BERT parameters. The model is<a id="_idIndexMarker298"/> ready to be trained. We will use the same settings from the previous BERT section and train the model for 3 epochs:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">"Custom Model: Fine-tuning BERT on IMDB"</span>)
custom_history = custom_model.fit(train_ds, epochs=<span class="hljs-number">3</span>, 
                                  validation_data=valid_ds)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">Custom Model: Fine-tuning BERT on IMDB
Train for 1250 steps, validate for 313 steps
Epoch 1/3
1250/1250 [==============================] - 477s 381ms/step - loss: 0.5912 - accuracy: 0.8069 - val_loss: 0.6009 - val_accuracy: 0.8020
Epoch 2/3
1250/1250 [==============================] - 469s 375ms/step - loss: 0.5696 - accuracy: 0.8570 - val_loss: 0.5643 - val_accuracy: 0.8646
Epoch 3/3
1250/1250 [==============================] - 470s 376ms/step - loss: 0.5559 - accuracy: 0.8883 - val_loss: 0.5647 - val_accuracy: 0.8669
</code></pre>
    <p class="normal">Evaluating on the test set gives an accuracy of 86.29%. Note that the test data encoding steps used in the pretrained BERT model section are used here as well:</p>
    <pre class="programlisting code"><code class="hljs-code">custom_model.evaluate(test_ds)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1563/1563 [==============================] - 201s 128ms/step - loss: 0.5667 - accuracy: 0.8629
</code></pre>
    <p class="normal">Fine-tuning of BERT is run for a small number of epochs with a small value for Adam's learning rate. If a lot of fine-tuning is done, then there is a risk of BERT forgetting its pretrained parameters. This can be a limitation while building custom models on top as a few epochs may not be sufficient to train the layers that have been added. In this case, the BERT model layer <a id="_idIndexMarker299"/>can be frozen, and training can be continued further. Freezing the BERT layer is fairly easy, though it needs the re-compilation of the model:</p>
    <pre class="programlisting code"><code class="hljs-code">bert.trainable = <span class="hljs-literal">False                </span>  <span class="hljs-comment"># don't train BERT any more</span>
optimizer = tf.keras.optimizers.Adam()  <span class="hljs-comment"># standard learning rate</span>
custom_model.<span class="hljs-built_in">compile</span>(optimizer=optimizer, loss=loss, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">We can check the model summary to verify that the number of trainable parameters has changed to reflect the BERT layer being frozen:</p>
    <pre class="programlisting code"><code class="hljs-code">custom_model.summary()
</code></pre>
    <figure class="mediaobject"><img src="image/B16252_04_09.png" alt="A screenshot of a cell phone  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 4.8: Model summary</p>
    <p class="normal">We can see that all the BERT parameters are now set to non-trainable. Since the model was being recompiled, we also took the opportunity to change the learning rate. </p>
    <div class="packt_tip">
      <p class="Tip--PACKT-">Changing the sequence length and learning rate during training are advanced techniques in TensorFlow. The BERT model also used 128 as the sequence length for initial epochs, which was changed to 512 later in training. It is also common to see a learning rate increase for the first few epochs and then decrease as training proceeds.</p>
    </div>
    <p class="normal">Now, training can be <a id="_idIndexMarker300"/>continued for a number of epochs like so:</p>
    <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">"Custom Model: Keep training custom model on IMDB"</span>)
custom_history = custom_model.fit(train_ds, epochs=<span class="hljs-number">10</span>, 
                                  validation_data=valid_ds)
</code></pre>
    <p class="normal">The training output has not been shown for brevity. Checking the model on the test set yields 86.96% accuracy:</p>
    <pre class="programlisting code"><code class="hljs-code">custom_model.evaluate(test_ds)
</code></pre>
    <pre class="programlisting con"><code class="hljs-con">1563/1563 [==============================] - 195s 125ms/step - loss: 0.5657 - accuracy: 0.8696
</code></pre>
    <p class="normal">If you are contemplating whether the accuracy of this custom model is lower than the pre-trained model, then it is a fair question to ponder over. A bigger network is not always better, and overtraining can lead to a reduction in model performance due to overfitting. Something to<a id="_idIndexMarker301"/> try in the custom model is to use the output encodings of all the input tokens and pass them through an LSTM layer or concatenate them together to pass through dense layers and then make the prediction.</p>
    <p class="normal">Having done the tour of the encoder side of the Transformer architecture, we are ready to look into the decoder side of the architecture, which is used for text generation. That will be the focus of the next chapter. Before we go there, let's review everything we covered in this chapter.</p>
    <h1 id="_idParaDest-86" class="title">Summary</h1>
    <p class="normal">Transfer learning has made a lot of progress possible in the world of NLP, where data is readily available, but labeled data is a challenge. We covered different types of transfer learning first. Then, we took pre-trained GloVe embeddings and applied them to the IMDb sentiment analysis problem, seeing comparable accuracy with a much smaller model that takes much less time to train.</p>
    <p class="normal">Next, we learned about seminal moments in the evolution of NLP models, starting from encoder-decoder architectures, attention, and Transformer models, before understanding the BERT model. Using the Hugging Face library, we used a pre-trained BERT model and a custom model built on top of BERT for the purpose of sentiment classification of IMDb reviews.</p>
    <p class="normal">BERT only uses the encoder part of the Transformer model. The decoder side of the stack is used in text generation. The next two chapters will focus on completing the understanding of the Transformer model. The next chapter will use the decoder side of the stack to perform text generation and sentence completion. The chapter after that will use the full encoder-decoder network architecture for text summarization.</p>
    <p class="normal">Thus far, we have trained embeddings for tokens in the models. A considerable amount of lift can be achieved by using pre-trained embeddings. The next chapter will focus on the concept of transfer learning and the use of pre-trained embeddings like BERT.</p>
  </div>
</body></html>