<html><head></head><body>
  <div id="_idContainer610">
    <h1 class="chapterNumber">4</h1>
    <h1 id="_idParaDest-107" class="chapterTitle">Monte Carlo Methods</h1>
    <p class="normal">In the previous chapter, we learned how to compute the optimal policy using two interesting dynamic programming methods called value and policy iteration. Dynamic programming is a model-based method and it requires the model dynamics of the environment to compute the value and Q functions in order to find the optimal policy. </p>
    <p class="normal">But let's suppose we don't have the model dynamics of the environment. In that case, how do we compute the value and Q functions? Here is where we use model-free methods. Model-free methods do not require the model dynamics of the environment to compute the value and Q functions in order to find the optimal policy. One such popular model-free method is the <strong class="keyword">Monte Carlo</strong> (<strong class="keyword">MC</strong>) method. </p>
    <p class="normal">We will begin the chapter by understanding what the MC method is, then we will look into two important types of tasks in reinforcement learning called prediction and control tasks. Later, we will learn how the Monte Carlo method is used in reinforcement learning and how it is beneficial compared to the dynamic programming method we learned about in the previous chapter. Moving forward, we will understand what the MC prediction method is and the different types of MC prediction methods. We will also learn how to train an agent to play blackjack with the MC prediction method. </p>
    <p class="normal">Going ahead, we will learn about the Monte Carlo control method and different types of Monte Carlo control methods. Following this, we will learn how to train an agent to play blackjack with the MC control method.</p>
    <p class="normal">To summarize, in this chapter, we will learn about the following topics:</p>
    <ul>
      <li class="bullet">Understanding the Monte Carlo method</li>
      <li class="bullet">Prediction and control tasks</li>
      <li class="bullet">The Monte Carlo prediction method</li>
      <li class="bullet">Playing blackjack with the MC prediction method</li>
      <li class="bullet">The Monte Carlo control method</li>
      <li class="bullet">Playing blackjack with the MC control method</li>
    </ul>
    <h1 id="_idParaDest-108" class="title">Understanding the Monte Carlo method</h1>
    <p class="normal">Before understanding how the Monte Carlo method is useful in reinforcement learning, first, let's understand <a id="_idIndexMarker339"/>what the Monte Carlo method is and how it works. The Monte Carlo method is a statistical technique used to find an approximate solution through sampling. </p>
    <p class="normal">For instance, the Monte Carlo method approximates the expectation of a random variable by sampling, and when the sample size is greater, the approximation will be better. Let's suppose we have a random variable <em class="italic">X</em> and say we need to compute the expected value of <em class="italic">X</em>; that is <em class="italic">E(X)</em>, then we can compute it by taking the sum of the values of <em class="italic">X</em> multiplied by their respective probabilities as follows:</p>
    <p class="packt_figref"><img src="../Images/B15558_04_001.png" alt="" style="height: 3.5em;"/></p>
    <p class="normal">But instead of computing the expectation like this, can we approximate it with the Monte Carlo method? Yes! We can estimate the expected value of <em class="italic">X</em> by just sampling the values of <em class="italic">X </em>for some <em class="italic">N</em> times and compute the average value of <em class="italic">X</em> as the expected value of <em class="italic">X</em> as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_002.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">When <em class="italic">N</em> is larger our approximation will be better. Thus, with the Monte Carlo method, we can approximate the solution through sampling and our approximation will be better when the sample size is large.</p>
    <p class="normal">In the upcoming sections, we will learn how exactly the Monte Carlo method is used in reinforcement learning.</p>
    <h1 id="_idParaDest-109" class="title">Prediction and control tasks</h1>
    <p class="normal">In reinforcement <a id="_idIndexMarker340"/>learning, we perform two important tasks, and they are:</p>
    <ul>
      <li class="bullet">The prediction task</li>
      <li class="bullet">The control task </li>
    </ul>
    <h2 id="_idParaDest-110" class="title">Prediction task </h2>
    <p class="normal">In the <a id="_idIndexMarker341"/>prediction task, a policy <img src="../Images/B15558_03_172.png" alt="" style="height: 0.84em;"/> is given as an input and we try to predict the value function or Q function using the given policy. But what is the use of doing this? Our goal is to evaluate the given policy. That is, we need to determine whether the given policy is good or bad. How can we determine that? If the agent obtains a good return using the given policy then we can say that our policy is good. Thus, to evaluate the given policy, we need to understand what the return the agent would obtain if it uses the given policy. To obtain the return, we predict the value function or Q function using the given policy.</p>
    <p class="normal">That is, we learned that the value function or value of a state denotes the expected return an agent would obtain starting from that state following some policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/>. Thus, by predicting the value function using the given policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/>, we can understand what the expected return the agent would obtain in each state if it uses the given policy <img src="../Images/B15558_03_050.png" alt="" style="height: 0.84em;"/>. If the return is good then we can say that the given policy is good.</p>
    <p class="normal">Similarly, we learned that the Q function or Q value denotes the expected return the agent would obtain starting from the state <em class="italic">s</em> and an action <em class="italic">a</em> following the policy <img src="../Images/B15558_03_082.png" alt="" style="height: 0.84em;"/>. Thus, predicting the Q function using the given policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/>, we can understand what the expected return the agent would obtain in each state-action pair if it uses the given policy. If the return is good then we can say that the given policy is good.</p>
    <p class="normal">Thus, we can evaluate the given policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/> by computing the value and Q functions.</p>
    <p class="normal">Note that, in the prediction task, we don't make any change to the given input policy. We keep <a id="_idIndexMarker342"/>the given policy as fixed and predict the value function or Q function using the given policy and obtain the expected return. Based on the expected return, we evaluate the given policy.</p>
    <h2 id="_idParaDest-111" class="title">Control task</h2>
    <p class="normal">Unlike the <a id="_idIndexMarker343"/>prediction task, in the control task, we will not be given any policy as an input. In the control task, our goal is to find the optimal policy. So, we will start off by initializing a random policy and we try to find the optimal policy iteratively. That is, we try to find an optimal policy that gives the maximum return.</p>
    <p class="normal">Thus, in a nutshell, in the prediction task, we evaluate the given input policy by predicting the value function or Q function, which helps us to understand the expected return an agent would get if it uses the given policy, while in the control task our goal is to find the optimal policy and we will not be given any policy as input; so we will start off by initializing a random policy and we try to find the optimal policy iteratively.</p>
    <p class="normal">Now that we have understood what prediction and control tasks are, in the next section, we will learn how to use the Monte Carlo method for performing the prediction and control tasks.</p>
    <h1 id="_idParaDest-112" class="title">Monte Carlo prediction</h1>
    <p class="normal">In this section, we will learn how to use the Monte Carlo method to perform the prediction <a id="_idIndexMarker344"/>task. We have learned that in the prediction task, we will be given a policy and we predict the value function or Q function using the given policy to evaluate it. First, we will learn how to predict the value function using the given policy with the Monte Carlo method. Later, we will look into predicting the Q function using the given policy. Alright, let's get started with the section.</p>
    <p class="normal">Why do we need the Monte Carlo method for predicting the value function of the given policy? Why can't we predict the value function using the dynamic programming methods we learned about in the previous chapter? We learned that in order to compute the value function using the dynamic programming method, we need to know the model dynamics (transition probability), and when we don't know the model dynamics, we use the model-free methods.</p>
    <p class="normal">The Monte Carlo method is a model-free method, meaning that it doesn't require the model dynamics to compute the value function.</p>
    <p class="normal">First, let's recap the definition of the value function. The value function or the value of the state <em class="italic">s</em> can be defined as the expected return the agent would obtain starting from the state <em class="italic">s</em> and following the policy <img src="../Images/B15558_04_010.png" alt="" style="height: 0.84em;"/>. It can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_011.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">Okay, how can we estimate the value of the state (value function) using the Monte Carlo method? At the beginning of the chapter, we learned that the Monte Carlo method approximates the expected value of a random variable by sampling, and when the sample size is greater, the approximation will be better. Can we leverage this concept of the Monte Carlo method to predict the value of a state? Yes!</p>
    <p class="normal">In order <a id="_idIndexMarker345"/>to approximate the value of the state using the Monte Carlo method, we sample episodes (trajectories) following the given policy <img src="../Images/B15558_04_012.png" alt="" style="height: 0.84em;"/> for some <em class="italic">N</em> times and then we compute the value of the state as the average return of a state across the sampled episodes, and it can be expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_013.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">From the preceding equation, we can understand that the value of a state <em class="italic">s</em> can be approximated by computing the average return of the state <em class="italic">s</em> across some <em class="italic">N</em> episodes. Our approximation will be better when <em class="italic">N</em> is higher.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">In a nutshell, in the Monte Carlo prediction method, we approximate the value of a state by taking the average return of a state across <em class="italic">N</em> episodes instead of taking the expected return. </p>
    </div>
    <p class="normal">Okay, let's get a better understanding of how the Monte Carlo method estimates the value of a state (value function) with an example. Let's take our favorite grid world environment we covered in <em class="chapterRef">Chapter 1</em>, <em class="italic">Fundamentals of Reinforcement Learning</em>, as shown in <em class="italic">Figure 4.1</em>. Our goal is to reach the state <strong class="keyword">I</strong> from the state <strong class="keyword">A</strong> without visiting the shaded states, and the agent receives +1 reward when it visits the unshaded states and -1 reward when it visits the shaded states:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.1: Grid world environment</p>
    <p class="normal">Let's say <a id="_idIndexMarker346"/>we have a stochastic policy <img src="../Images/B15558_03_050.png" alt="" style="height: 0.84em;"/>. Let's suppose, in state <strong class="keyword">A,</strong> our stochastic policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> selects action <em class="italic">down</em> 80% of time and action <em class="italic">right</em> 20% of the time, and it selects action <em class="italic">right</em> in states <strong class="keyword">D</strong> and <strong class="keyword">E</strong> and action <em class="italic">down</em> in states <strong class="keyword">B</strong> and <strong class="keyword">F</strong> 100% of the time.</p>
    <p class="normal">First, we generate an episode <img src="../Images/B15558_04_016.png" alt="" style="height: 0.84em;"/> using our given stochastic policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> as <em class="italic">Figure 4.2</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.2: Episode <img src="../Images/B15558_04_018.png" alt="" style="height: 0.84em;"/></p>
    <p class="normal">For a better understanding, let's focus only on state <strong class="keyword">A</strong>. Let's now compute the return of state <strong class="keyword">A</strong>. The return of a state is the sum of the rewards of the trajectory starting from that state. Thus, the return of state <strong class="keyword">A</strong> is computed as <em class="italic">R</em><sub class="Subscript--PACKT-">1</sub>(<em class="italic">A</em>) = 1+1+1+1 = 4 where the subscript 1 in <em class="italic">R</em><sub class="Subscript--PACKT-">1</sub> indicates the return from episode 1.</p>
    <p class="normal">Say we <a id="_idIndexMarker347"/>generate another episode <img src="../Images/B15558_04_019.png" alt="" style="height: 0.84em;"/> using the same given stochastic policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/> as <em class="italic">Figure 4.3</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.3: Episode <img src="../Images/B15558_04_019.png" alt="" style="height: 0.84em;"/></p>
    <p class="normal">Let's now compute the return of state <strong class="keyword">A</strong>. The return of state <strong class="keyword">A</strong> is <em class="italic">R</em><sub class="Subscript--PACKT-">2</sub>(<em class="italic">A</em>) = -1+1+1+1 = 2.</p>
    <p class="normal">Say we generate another episode <img src="../Images/B15558_04_022.png" alt="" style="height: 0.84em;"/> using the same given stochastic policy <img src="../Images/B15558_03_082.png" alt="" style="height: 0.84em;"/> as <em class="italic">Figure 4.4</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.4: Episode <img src="../Images/B15558_04_024.png" alt="" style="height: 0.84em;"/></p>
    <p class="normal">Let's now <a id="_idIndexMarker348"/>compute the return of state <strong class="keyword">A</strong>. The return of state <strong class="keyword">A</strong> is <em class="italic">R</em><sub class="Subscript--PACKT-">3</sub>(<em class="italic">A</em>) = 1+1+1+1 = 4.</p>
    <p class="normal">Thus, we generated three episodes and computed the return of state <strong class="keyword">A</strong> in all three episodes. Now, how can we compute the value of the state <strong class="keyword">A</strong>? We learned that in the Monte Carlo method, the value of a state can be approximated by computing the average return of the state across some <em class="italic">N</em> episodes (trajectories):</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_013.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">We need to compute the value of state <strong class="keyword">A</strong>, so we can compute it by just taking the average return of the state <strong class="keyword">A </strong>across the <em class="italic">N</em> episodes as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_026.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">We generated three episodes, thus:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_027.png" alt="" style="height: 3.42em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_04_028.png" alt="" style="height: 2.22em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_04_029.png" alt="" style="height: 2.22em;"/></figure>
    <p class="normal">Thus, the value of state <strong class="keyword">A</strong> is 3.3. Similarly, we can compute the value of all other states by just <a id="_idIndexMarker349"/>taking the average return of the state across the three episodes.</p>
    <p class="normal">For easier understanding, in the preceding example, we only generated three episodes. In order to find a better and more accurate estimate of the value of the state, we should generate many episodes (not just three) and compute the average return of the state as the value of the state.</p>
    <p class="normal">Thus, in the Monte Carlo prediction method, to predict the value of a state (value function) using the given input policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/>, we generate some <em class="italic">N</em> episodes using the given policy and then we compute the value of a state as the average return of the state across these <em class="italic">N</em> episodes.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Note that while computing the return of the state, we can also include the discount factor and compute the discounted return, but for simplicity let's not include the discount factor.</p>
    </div>
    <p class="normal">Now, that we have a basic understanding of how the Monte Carlo prediction method predicts <a id="_idIndexMarker350"/>the value function of the given policy, let's look into more detail by understanding the algorithm of the Monte Carlo prediction method in the next section.</p>
    <h2 id="_idParaDest-113" class="title">MC prediction algorithm</h2>
    <p class="normal">The Monte <a id="_idIndexMarker351"/>Carlo prediction algorithm is given as follows:</p>
    <ol>
      <li class="numbered">Let total_return(<em class="italic">s</em>) be the sum of return of a state across several episodes and <em class="italic">N</em>(<em class="italic">s</em>) be the counter, that is, the number of times a state is visited across several episodes. Initialize total_return(<em class="italic">s</em>) and <em class="italic">N</em>(<em class="italic">s</em>) as zero for all the states. The policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> is given as input.</li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using the policy <img src="../Images/B15558_04_032.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:
<ol>
      <li class="bullet-l2" value="1">Compute the return of state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
      <li class="bullet-l2">Update the total return of state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as total_returns(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>)</li>
      <li class="bullet-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
    </ol></li>
        </ol>
      </li>
      <li class="numbered">Compute the value of a state by just taking the average, that is:
<figure class="mediaobject"><img src="../Images/B15558_04_033.png" alt="" style="height: 2.51em;"/></figure></li>
    </ol>
    
    <p class="normal">The preceding algorithm implies that the value of the state is just the average return of the state across several episodes.</p>
    <p class="normal">To get a better understanding of how exactly the preceding algorithm works, let's take a simple example and compute the value of each state manually. Say we need to compute the value of three states <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub>, <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>, and <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub>. We know that we obtain a reward when we transition from one state to another. Thus, the reward for the final state will be 0 as we don't make any transitions from the final state. Hence, the value of the final state <em class="italic">s</em><sub class="Subscript--PACKT-">2</sub> will be zero. Now, we need to find the value of two states <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">The upcoming sections are explained with manual calculations, for a better understanding, follow along with a pen and paper.</p>
    </div>
    <p class="normal"><strong class="keyword">Step 1</strong>:</p>
    <p class="normal">Initialize the total_returns(<em class="italic">s</em>) and <em class="italic">N</em>(<em class="italic">s</em>) for all the states to zero as <em class="italic">Table 4.1</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_05.png" alt=""/></figure>
    <p class="packt_figref">Table 4.1: Initial values </p>
    <p class="normal">Say we are <a id="_idIndexMarker352"/>given a stochastic policy <img src="../Images/B15558_03_082.png" alt="" style="height: 0.84em;"/>; in state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> our stochastic policy selects the action 0 for 50% of the time and action 1 for 50% of the time, and it selects action 1 in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> for 100% of the time.</p>
    <p class="normal"><strong class="keyword">Step 2: Iteration 1</strong>:</p>
    <p class="normal">Generate an episode using the given input policy <img src="../Images/B15558_03_038.png" alt="" style="height: 0.84em;"/>, as <em class="italic">Figure 4.5</em> shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.5: Generating an episode using the given policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/></p>
    <p class="normal">Store all rewards obtained in the episode in the list called rewards. Thus, rewards = [1, 1].</p>
    <p class="normal">First, we compute the return of the state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> (sum of rewards from <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_037.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Update the total return of the state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> in our table as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_038.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">Update the number of times the state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> is visited in our table as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_039.png" alt="" style="height: 2.13em;"/></figure>
    <p class="normal">Now, let's compute the return of the state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> (sum of rewards from <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_040.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Update <a id="_idIndexMarker353"/>the total return of the state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> in our table as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_041.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">Update the number of times the state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> is visited in our table as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_042.png" alt="" style="height: 2.13em;"/></figure>
    <p class="normal">Our updated table, after iteration 1, is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_07.png" alt=""/> </figure>
    <p class="packt_figref">Table 4.2: Updated table after the first iteration</p>
    <p class="normal"><strong class="keyword">Iteration 2:</strong></p>
    <p class="normal">Say we generate another episode using the same given policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> as <em class="italic">Figure 4.6</em> shows: </p>
    <figure class="mediaobject"><img src="../Images/B15558_04_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.6: Generating an episode using the given policy <img src="../Images/B15558_03_172.png" alt="" style="height: 0.84em;"/></p>
    <p class="normal">Store <a id="_idIndexMarker354"/>all rewards obtained in the episode in the list called rewards. Thus, rewards = [3, 1].</p>
    <p class="normal">First, we compute the return of the state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> (sum of rewards from <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_045.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Update the total return of the state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> in our table as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_046.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">Update the number of times the state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> is visited in our table as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_047.png" alt="" style="height: 2.13em;"/></figure>
    <p class="normal">Now, let's compute the return of the state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> (sum of rewards from <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub>):</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_040.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Update the return of the state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> in our table as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_049.png" alt="" style="height: 2.04em;"/></figure>
    <p class="normal">Update <a id="_idIndexMarker355"/>the number of times the state is visited:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_050.png" alt="" style="height: 2.13em;"/></figure>
    <p class="normal">Our updated table after the second iteration is as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_09.png" alt=""/></figure>
    <p class="packt_figref">Table 4.3: Updated table after the second iteration</p>
    <p class="normal">Since we are computing manually, for simplicity, let's stop at two iterations; that is, we just generate only two episodes. </p>
    <p class="normal"><strong class="keyword">Step 3:</strong></p>
    <p class="normal">Now, we can compute the value of the state as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_051.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">Thus:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_052.png" alt="" style="height: 2.51em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_04_053.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">Thus, we <a id="_idIndexMarker356"/>computed the value of the state by just taking the average return across multiple episodes. Note that in the preceding example, for our manual calculation, we just generated two episodes, but for a better estimation of the value of the state, we generate several episodes and then we compute the average return across those episodes (not just 2).</p>
    <h2 id="_idParaDest-114" class="title">Types of MC prediction</h2>
    <p class="normal">We just <a id="_idIndexMarker357"/>learned how the Monte Carlo prediction algorithm works. We can categorize the Monte Carlo prediction algorithm into two types:</p>
    <ul>
      <li class="bullet">First-visit Monte Carlo</li>
      <li class="bullet">Every-visit Monte Carlo</li>
    </ul>
    <h3 id="_idParaDest-115" class="title">First-visit Monte Carlo</h3>
    <p class="normal">We <a id="_idIndexMarker358"/>learned that in the MC prediction method, we estimate the value of the state by just taking the average return of the state across multiple episodes. We know that in each episode a state can be visited multiple times. In the first-visit Monte Carlo method, if the same state is visited again in the same episode, we don't compute the return for that state again. For example, consider a case where an agent is playing snakes and ladders. If the agent lands on a snake, then there is a good chance that the agent will return to a state that it had visited earlier. So, when the agent revisits the same state, we don't compute the return for that state for the second time. </p>
    <p class="normal">The following shows the algorithm of first-visit MC; as the point in bold says, we compute the return for the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> only if it is occurring for the first time in the episode:</p>
    <ol>
      <li class="numbered" value="1">Let total_return(<em class="italic">s</em>) be the sum of return of a state across several episodes and <em class="italic">N</em>(<em class="italic">s</em>) be the counter, that is, the number of times a state is visited across several episodes. Initialize total_return(<em class="italic">s</em>) and <em class="italic">N</em>(<em class="italic">s</em>) as zero for all the states. The policy <img src="../Images/B15558_04_054.png" alt="" style="height: 0.84em;"/> is given as input</li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using the policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:<p class="bullet-para"><strong class="keyword">If </strong><strong class="keyword"><a id="_idIndexMarker359"/></strong><strong class="keyword">the state s</strong><sub class="" style="font-weight: bold;">t</sub><strong class="keyword"> is occurring for the first time in the episode:</strong></p>
<ol>
      <li class="bullet-l2" value="1">Compute the return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
      <li class="bullet-l2">Update the total return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>)</li>
      <li class="bullet-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
    </ol></li>

    </ol>
        
      </li>
      <li class="numbered">Compute the value of a state by just taking the average, that is:
<figure class="mediaobject"><img src="../Images/B15558_04_056.png" alt="" style="height: 2.51em;"/></figure></li>
    </ol>
    <h3 id="_idParaDest-116" class="title">Every-visit Monte Carlo</h3>
    <p class="normal">As <a id="_idIndexMarker360"/>you might have guessed, every-visit Monte Carlo is just the opposite of first-visit Monte Carlo. Here, we compute the return every time a state is visited in the episode. The algorithm of every-visit Monte Carlo is the same as the one we saw earlier at the beginning of this section and it is as follows:</p>
    <ol>
      <li class="numbered" value="1">Let total_return(<em class="italic">s</em>) be the sum of the return of a state across several episodes and <em class="italic">N</em>(<em class="italic">s</em>) be the counter, that is, the number of times a state is visited across several episodes. Initialize total_return(<em class="italic">s</em>) and <em class="italic">N</em>(<em class="italic">s</em>) as zero for all the states. The policy <img src="../Images/B15558_04_012.png" alt="" style="height: 0.84em;"/> is given as input</li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using the policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:<ol>
      <li class="bullet-l2" value="1">Compute the return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
      <li class="bullet-l2">Update the total return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub> as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>)</li>
      <li class="bullet-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
    </ol></li>

        </ol>
      </li>
      <li class="numbered">Compute the value of a state by just taking the average, that is:<figure class="mediaobject"><img src="../Images/B15558_04_056.png" alt="" style="height: 2.51em;"/></figure></li>
    </ol>
    <p class="normal">Remember <a id="_idIndexMarker361"/>that the only difference between the first-visit MC and every-visit MC methods is that in the first-visit MC method, we compute the return for a state only for its first time of occurrence in the episode but in the every-visit MC method, the return of the state is computed every time the state is visited in an episode. We can choose between first-visit MC and every-visit MC based on the problem that we are trying to solve. </p>
    <p class="normal">Now that we have understood how the Monte Carlo prediction method predicts the value function of the given policy, in the next section, we will learn how to implement the Monte Carlo prediction method.</p>
    <h2 id="_idParaDest-117" class="title">Implementing the Monte Carlo prediction method</h2>
    <p class="normal">If you <a id="_idIndexMarker362"/>love playing card games then this section is definitely going to be interesting for you. In this section, we will learn how to play blackjack with the Monte Carlo prediction method. Before diving in, let's understand how the blackjack game works and its rules. </p>
    <h3 id="_idParaDest-118" class="title">Understanding the blackjack game</h3>
    <p class="normal">Blackjack, also known as <strong class="keyword">21</strong>, is one of the most popular card games. The game consists of a player <a id="_idIndexMarker363"/>and a dealer. The goal of the player is to have the value of the sum of all their cards be 21 or a larger value than the sum of the dealer's cards while not exceeding 21. If one of these criteria is met then the player wins the game; else the dealer wins the game. Let's understand this in more detail. </p>
    <p class="normal">The values of the cards <strong class="keyword">Jack (J)</strong>, <strong class="keyword">King (K)</strong>, and <strong class="keyword">Queen (Q)</strong> will be considered as 10. The value of the <strong class="keyword">Ace (A)</strong> can be 1 or 11, depending on the player's choice. That is, the player can decide whether the value of an <strong class="keyword">Ace</strong> should be 1 or 11 during the game. The value of the rest of the cards (<strong class="keyword">2</strong> <strong class="keyword">to</strong> <strong class="keyword">10</strong>) is just their face value. For instance, the value of the card <strong class="keyword">2</strong> will be 2, the value of the card <strong class="keyword">3</strong> will be 3, and so on. </p>
    <p class="normal">We learned that the game consists of a player and a dealer. There can be many players at a time but only one dealer. All the players compete with the dealer and not with other players. Let's consider a case where there is only one player and a dealer. Let's understand blackjack by playing the game along with different cases. Let's suppose we are the player and we are competing with the dealer.</p>
    <p class="normal"><strong class="keyword">Case 1: When the player wins the game</strong></p>
    <p class="normal">Initially, a player is given two cards. Both of these cards are face up, that is, both of the player's cards are visible to the dealer. Similarly, the dealer is also given two cards. But one of the dealer's cards is face up, and the other is face down. That is, the dealer shows only one of their cards. </p>
    <p class="normal">As we can see in <em class="italic">Figure 4.7</em>, the player has two cards (both face up) and the dealer also has two cards (only one face up):</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.7: The player has 20, and the dealer has 2 with one card face down</p>
    <p class="normal">Now, the player <a id="_idIndexMarker364"/>performs either of the two actions, which are <strong class="keyword">Hit</strong> and <strong class="keyword">Stand</strong>. If we (the player) perform the action <strong class="keyword">hit,</strong> then we get one more card. If we perform <strong class="keyword">stand,</strong> then it implies that we don't need any more cards and tells the dealer to show all their cards. Whoever has a sum of cards value equal to 21 or a larger value than the other player but not exceeding 21 wins the game.</p>
    <p class="normal">We learned that the value of <strong class="keyword">J</strong>, <strong class="keyword">K</strong>, and <strong class="keyword">Q</strong> is 10. As shown in <em class="italic">Figure 4.7</em>, we have cards <strong class="keyword">J</strong> and <strong class="keyword">K</strong>, which sums to 20 (10+10). Thus, the total value our cards is already a large number and it didn't exceed 21. So we <strong class="keyword">stand</strong>, and this action tells the dealer to show their cards. As we can observe in <em class="italic">Figure 4.8</em>, the dealer has now shown all their cards and the total value of the dealer's cards is 12 and the total value of our (the player's) cards is 20, which is larger and also didn't exceed 21, so we win the game.</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.8: The player wins!</p>
    <p class="normal"><strong class="keyword">Case 2: When the player loses the game</strong></p>
    <p class="normal"><em class="italic">Figure 4.9</em> shows we <a id="_idIndexMarker365"/>have two cards and the dealer also has two cards and only one of the dealer's card is visible to us:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.9: The player has 13, and the dealer has 7 with one card face down</p>
    <p class="normal">Now, we have to decide whether we should (perform the action) hit or stand. <em class="italic">Figure 4.9</em> shows we have two cards, <strong class="keyword">K</strong> and <strong class="keyword">3</strong>, which sums to 13 (10+3). Let's be a little optimistic and hope that the total value of the dealer's cards will not be greater than ours. So we <strong class="keyword">stand,</strong> and this action tells the dealer to show their cards. As we can observe in <em class="italic">Figure 4.10</em>, the sum of the dealer's card is 17, but ours is only 13, so we lose the game. That is, the dealer <a id="_idIndexMarker366"/>has got a larger value than us, and it did not exceed 21, so the dealer wins the game, and we lose: </p>
    <figure class="mediaobject"><img src="../Images/B15558_04_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.10: The dealer wins!</p>
    <p class="normal"><strong class="keyword">Case 3: When the player goes bust</strong></p>
    <p class="normal"><em class="italic">Figure 4.11</em> shows we have two cards and the dealer also has two cards but only one of the dealer's cards is visible to us:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.11: The player has 8, and the dealer has 10 with one card face down</p>
    <p class="normal">Now, we have <a id="_idIndexMarker367"/>to decide whether we should (perform the action) hit or stand. We learned that the goal of the game is to have a sum of cards value of 21, or a larger value than the dealer while not exceeding 21. Right now, the total value of our cards is just 3+5 = 8. Thus, we (perform the action) <strong class="keyword">hit</strong> so that we can make our sum value larger. After we <strong class="keyword">hit,</strong> we receive a new card as shown in <em class="italic">Figure 4.12</em>: </p>
    <figure class="mediaobject"><img src="../Images/B15558_04_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.12: The player has 18, and the dealer has 10 with one card face down</p>
    <p class="normal">As we can <a id="_idIndexMarker368"/>observe, we got a new card. Now, the total value of our cards is 3+5+10 = 18. Again, we need to decide whether we should (perform the action) hit or stand. Let's be a little greedy and (perform the action) <strong class="keyword">hit</strong> so that we can make our sum value a little larger. As shown in <em class="italic">Figure 4.13</em>, we <strong class="keyword">hit</strong> and received one more card but now the total value of our cards becomes 3+5+10+10 = 28, which exceeds 21, and this is called a <strong class="keyword">bust</strong> and we lose the game:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_16.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.13: The player goes bust!</p>
    <p class="normal"><strong class="keyword">Case 4: Useable Ace</strong></p>
    <p class="normal">We learned <a id="_idIndexMarker369"/>that the value of the <strong class="keyword">Ace</strong> can be either 1 or 11, and the player can decide the value of the <strong class="keyword">ace</strong> during the game. Let's learn how this works. As <em class="italic">Figure 4.14</em> shows, we have been given two cards and the dealer also has two cards and only one of the dealer's cards is face up:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_17.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.14: The player has 10, and the dealer has 5 with one card face down</p>
    <p class="normal">As we can see, the total value of our cards is 5+5 = 10. Thus, we <strong class="keyword">hit</strong> so that we <a id="_idIndexMarker370"/>can make our sum value larger. As <em class="italic">Figure 4.15</em> shows, after performing the hit action we received a new card, which is an <strong class="keyword">Ace</strong>. Now, we can decide the value of the <strong class="keyword">Ace </strong>to be either 1 or 11. If we consider the value of <strong class="keyword">Ace</strong> to be 1, then the total value of our cards will be 5+5+1 = 11. But if we consider the value of the <strong class="keyword">Ace</strong> to be 11, then the total value of our cards will be 5+5+11 = 21. In this case, we consider the value of our <strong class="keyword">Ace</strong> to be 11 so that our sum value becomes 21.</p>
    <p class="normal">Thus, we set the value of the <strong class="keyword">Ace</strong> to be 11 and win the game, and in this case, the <strong class="keyword">Ace</strong> is called the usable <strong class="keyword">Ace</strong> since it helped us to win the game:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_18.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.15: The player uses the <strong class="keyword">Ace</strong> as 11 and wins the game</p>
    <p class="normal"><strong class="keyword">Case 5: Non-usable Ace</strong></p>
    <p class="normal"><em class="italic">Figure 4.16</em> shows we <a id="_idIndexMarker371"/>have two cards and the dealer has two cards with one face up:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_19.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.16: The player has 13, and the dealer has 10 with one card face down</p>
    <p class="normal">As we <a id="_idIndexMarker372"/>can observe, the total value of our cards is 13 (10+3). We (perform the action) <strong class="keyword">hit</strong> so that we can make our sum value a little larger:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_20.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.17: The player has to use the <strong class="keyword">Ace</strong> as a 1 else they go bust</p>
    <p class="normal">As <em class="italic">Figure 4.17</em> shows, we <strong class="keyword">hit</strong> and received a new card, which is an <strong class="keyword">Ace</strong>. Now we can decide <a id="_idIndexMarker373"/>the value of <strong class="keyword">Ace</strong> to be 1 or 11. If we choose 11, then our sum value becomes 10+3+11 = 23. As we can observe, when we set our ace to 11, then our sum value exceeds 21, and we lose the game. Thus, instead of choosing <strong class="keyword">Ace</strong> = 11, we set the <strong class="keyword">Ace</strong> value to be 1; so our sum value becomes 10+3+1 = 14.</p>
    <p class="normal">Again, we need to decide whether we should (perform the action) hit or stand. Let's say we stand hoping that the dealer sum value will be lower than ours. As <em class="italic">Figure 4.18</em> shows, after performing the stand action, both of the dealer's cards are shown, and the sum of the dealer's card is 20, but ours is just 14, and so we lose the game, and in this case, the <strong class="keyword">Ace</strong> is called a <strong class="keyword">non-usable</strong> <strong class="keyword">Ace</strong> since it did not help us to win the game. </p>
    <figure class="mediaobject"><img src="../Images/B15558_04_21.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.18: The player has 14, and the dealer has 20 and wins</p>
    <p class="normal"><strong class="keyword">Case 6: When the game is a draw</strong></p>
    <p class="normal">If both <a id="_idIndexMarker374"/>the player and the dealer's sum of cards value is the same, say 20, then the game is called a draw. </p>
    <p class="normal">Now that we have understood how to play blackjack, let's implement the Monte Carlo prediction method in the blackjack game. But before going ahead, first, let's learn how the blackjack environment is designed in Gym.</p>
    <h3 id="_idParaDest-119" class="title">The blackjack environment in the Gym library</h3>
    <p class="normal">Import <a id="_idIndexMarker375"/>the Gym library:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
</code></pre>
    <p class="normal">The environment id of blackjack is <code class="Code-In-Text--PACKT-">Blackjack-v0</code>. So, we can create the blackjack game using the <code class="Code-In-Text--PACKT-">make</code> function as shown as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'Blackjack-v0'</span>)
</code></pre>
    <p class="normal">Now, let's look at the state of the blackjack environment; we can just reset our environment and look at the initial state:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.reset())
</code></pre>
    <p class="normal">Note that every time we run the preceding code, we might get a different result, as the initial state is randomly initialized. The preceding code will print something like this:</p>
    <pre class="programlisting code"><code class="hljs-code">(<span class="hljs-number">15</span>, <span class="hljs-number">9</span>, <span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">As we can observe, our state is represented as a tuple, but what does this mean? We learned that in the blackjack game, we will be given two cards and we also get to see one of the dealer's cards. Thus, <code class="Code-In-Text--PACKT-">15</code> implies that the value of the sum of our cards, <code class="Code-In-Text--PACKT-">9</code> implies the face value of one of the dealer's cards, <code class="Code-In-Text--PACKT-">True</code> implies that we have a usable ace, and it will be <code class="Code-In-Text--PACKT-">False</code> if we don't have a usable ace.</p>
    <p class="normal">Thus, in <a id="_idIndexMarker376"/>the blackjack environment the state is represented as a tuple consisting of three values:</p>
    <ol>
      <li class="numbered" value="1">The value of the sum of our cards</li>
      <li class="numbered">The face value of one of the dealer's card</li>
      <li class="numbered">Boolean value—<code class="Code-In-Text--PACKT-">True</code> if we have a useable ace and <code class="Code-In-Text--PACKT-">False</code> if we don't have a useable ace</li>
    </ol>
    <p class="normal">Let's look at the action space of our blackjack environment:</p>
    <pre class="programlisting code"><code class="hljs-code">print(env.action_space)
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code">Discrete(<span class="hljs-number">2</span>)
</code></pre>
    <p class="normal">As we can observe, it implies that we have two actions in our action space, which are 0 and 1:</p>
    <ul>
      <li class="bullet">The action <strong class="keyword">stand</strong> is represented by 0</li>
      <li class="bullet">The action <strong class="keyword">hit</strong> is represented by 1</li>
    </ul>
    <p class="normal">Okay, what about the reward? The reward will be assigned as follows:</p>
    <ul>
      <li class="bullet"><strong class="keyword">+1.0</strong> reward if we win the game</li>
      <li class="bullet"><strong class="keyword">-1.0</strong> reward if we lose the game</li>
      <li class="bullet"><strong class="keyword">0</strong> reward if the game is a draw</li>
    </ul>
    <p class="normal">Now that we have understood how the blackjack environment is designed in Gym, let's start implementing the MC prediction method in the blackjack game. First, we will look at every-visit MC and then we will learn how to implement first-visit MC prediction. </p>
    <h3 id="_idParaDest-120" class="title">Every-visit MC prediction with the blackjack game</h3>
    <p class="normal">To <a id="_idIndexMarker377"/>understand this section clearly, you should recap the every-visit Monte Carlo method we learned earlier. Let's now understand how to implement every-visit MC prediction <a id="_idIndexMarker378"/>with the blackjack game step by step:</p>
    <p class="normal">Import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict
</code></pre>
    <p class="normal">Create a blackjack environment:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'Blackjack-v0'</span>)
</code></pre>
    <h4 class="title">Defining a policy </h4>
    <p class="normal">We learned <a id="_idIndexMarker379"/>that in the prediction method, we will be given an input policy and we predict the value function of the given input policy. So, now, we first define a policy function that acts as an input policy. That is, we define the input policy whose value function will be predicted in the upcoming steps. </p>
    <p class="normal">As shown in the following code, our policy function takes the state as an input and if the <strong class="keyword">state[0]</strong>, the sum of our cards, value, is greater than 19, then it will return action <strong class="keyword">0</strong> (stand), else it will return action <strong class="keyword">1</strong> (hit):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">policy</span><span class="hljs-function">(</span><span class="hljs-params">state</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> <span class="hljs-number">0</span> <span class="hljs-keyword">if</span> state[<span class="hljs-number">0</span>] &gt; <span class="hljs-number">19</span> <span class="hljs-keyword">else</span> <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">We defined an optimal policy: it makes more sense to perform an action 0 (stand) when our sum value is already greater than 19. That is, when the sum value is greater than 19 we don't have to perform a 1 (hit) action and receive a new card, which may cause us to lose the game or bust.</p>
    <p class="normal">For example, let's generate an initial state by resetting the environment as shown as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">state = env.reset()
print(state)
</code></pre>
    <p class="normal">Suppose the preceding code prints the following: </p>
    <pre class="programlisting code"><code class="hljs-code">(<span class="hljs-number">20</span>, <span class="hljs-number">5</span>, <span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">As we can notice, <code class="Code-In-Text--PACKT-">state[0] = 20</code>; that is, the value of the sum of our cards is 20, so in this case, our policy will return the action 0 (stand) as the following shows:</p>
    <pre class="programlisting code"><code class="hljs-code">print(policy(state))
</code></pre>
    <p class="normal">The preceding code will print:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-number">0</span>
</code></pre>
    <p class="normal">Now that <a id="_idIndexMarker380"/>we have defined the policy, in the next sections, we will predict the value function (state values) of this policy.</p>
    <h4 class="title">Generating an episode</h4>
    <p class="normal">Next, we <a id="_idIndexMarker381"/>generate an episode using the given policy, so we define a function called <code class="Code-In-Text--PACKT-">generate_episode</code>, which takes the policy as an input and generates the episode using the given policy.</p>
    <p class="normal">First, let's set the number of time steps:</p>
    <pre class="programlisting code"><code class="hljs-code">num_timesteps = <span class="hljs-number">100</span>
</code></pre>
    <p class="normal">For a clear understanding, let's look into the function line by line:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">generate_episode</span><span class="hljs-function">(</span><span class="hljs-params">policy</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Let's define a list called <code class="Code-In-Text--PACKT-">episode</code> for storing the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    episode = []
</code></pre>
    <p class="normal">Initialize the state by resetting the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = env.reset()
</code></pre>
    <p class="normal">Then for each time step:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
</code></pre>
    <p class="normal">Select the action according to the given policy:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = policy(state)
</code></pre>
    <p class="normal">Perform the action and store the next state information:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state, reward, done, info = env.step(action)
</code></pre>
    <p class="normal">Store the state, action, and reward into our episode list:</p>
    <pre class="programlisting code"><code class="hljs-code">        episode.append((state, action, reward))
</code></pre>
    <p class="normal">If the next state is a final state then break the loop, else update the next state to the current state:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> done:
            <span class="hljs-keyword">break</span>
            
        state = next_state
    <span class="hljs-keyword">return</span> episode
</code></pre>
    <p class="normal">Let's take <a id="_idIndexMarker382"/>a look at what the output of our <code class="Code-In-Text--PACKT-">generate_episode</code> function looks like. Note that we generate an episode using the policy we defined earlier:</p>
    <pre class="programlisting code"><code class="hljs-code">print(generate_episode(policy))
</code></pre>
    <p class="normal">The preceding code will print something like the following:</p>
    <pre class="programlisting code"><code class="hljs-code">[((<span class="hljs-number">10</span>, <span class="hljs-number">2</span>, <span class="hljs-literal">False</span>), <span class="hljs-number">1</span>, <span class="hljs-number">0</span>), ((<span class="hljs-number">20</span>, <span class="hljs-number">2</span>, <span class="hljs-literal">False</span>), <span class="hljs-number">0</span>, <span class="hljs-number">1.0</span>)]
</code></pre>
    <p class="normal">As we can observe our output is in the form of <strong class="keyword">[(state, action, reward)]</strong>. As shown previously, we have two states in our episode. We performed action 1 (hit) in the state <code class="Code-In-Text--PACKT-">(10, 2, False)</code> and received a 0 reward, and we performed action 0 (stand) in the state <code class="Code-In-Text--PACKT-">(20, 2, False)</code> and received a reward of 1.0.</p>
    <p class="normal">Now that we have learned how to generate an episode using the given policy, next, we will look at how to compute the value of the state (value function) using the every-visit MC method.</p>
    <h4 class="title">Computing the value function</h4>
    <p class="normal">We learned <a id="_idIndexMarker383"/>that in order to predict the value function, we generate several episodes using the given policy and compute the value of the state as an average return across several episodes. Let's see how to implement that. </p>
    <p class="normal">First, we define the <code class="Code-In-Text--PACKT-">total_return</code> and <code class="Code-In-Text--PACKT-">N</code> as a dictionary for storing the total return and the number of times the state is visited across episodes respectively: </p>
    <pre class="programlisting code"><code class="hljs-code">total_return = defaultdict(float)
N = defaultdict(int)
</code></pre>
    <p class="normal">Set the number of iterations, that is, the number of episodes, we want to generate:</p>
    <pre class="programlisting code"><code class="hljs-code">num_iterations = <span class="hljs-number">500000</span>
</code></pre>
    <p class="normal">Then, for every iteration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
</code></pre>
    <p class="normal">Generate the episode using the given policy; that is, generate an episode using the policy function we defined earlier:</p>
    <pre class="programlisting code"><code class="hljs-code">    episode = generate_episode(policy)
</code></pre>
    <p class="normal">Store all <a id="_idIndexMarker384"/>the states, actions, and rewards obtained from the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    states, actions, rewards = zip(*episode)
</code></pre>
    <p class="normal">Then, for each step in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t, state <span class="hljs-keyword">in</span> enumerate(states):
</code></pre>
    <p class="normal">Compute the return <code class="Code-In-Text--PACKT-">R</code> of the state as the sum of rewards, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:]):</p>
    <pre class="programlisting code"><code class="hljs-code">        R = (sum(rewards[t:]))
</code></pre>
    <p class="normal">Update the <code class="Code-In-Text--PACKT-">total_return</code> of the state as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>):</p>
    <pre class="programlisting code"><code class="hljs-code">        total_return[state] =  total_return[state] + R
</code></pre>
    <p class="normal">Update the number of times the state is visited in the episode as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>) + 1:</p>
    <pre class="programlisting code"><code class="hljs-code">        N[state] =  N[state] + <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">After computing the <code class="Code-In-Text--PACKT-">total_return</code> and <code class="Code-In-Text--PACKT-">N</code> we can just convert them into a pandas data frame for a better understanding. Note that this is just to give a clear understanding of the algorithm; we don't necessarily have to convert to the pandas data frame, we can also implement this efficiently just by using the dictionary.</p>
    <p class="normal">Convert the <code class="Code-In-Text--PACKT-">total_returns</code> dictionary into a data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">total_return = pd.DataFrame(total_return.items(),columns=[<span class="hljs-string">'state'</span>, <span class="hljs-string">'total_return'</span>])
</code></pre>
    <p class="normal">Convert the counter <code class="Code-In-Text--PACKT-">N</code> dictionary into a data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">N = pd.DataFrame(N.items(),columns=[<span class="hljs-string">'state'</span>, <span class="hljs-string">'N'</span>])
</code></pre>
    <p class="normal">Merge the two data frames on states:</p>
    <pre class="programlisting code"><code class="hljs-code">df = pd.merge(total_return, N, on=<span class="hljs-string">"state"</span>)
</code></pre>
    <p class="normal">Look at the first few rows of the data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">df.head(<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker385"/>code will display the following. As we can observe, we have the total return and the number of times the state is visited:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_22.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.19: The total return and the number of times a state has been visited</p>
    <p class="normal">Next, we can compute the value of the state as the average return:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_056.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">Thus, we can write:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-string">'value'</span>] = df[<span class="hljs-string">'total_return'</span>]/df[<span class="hljs-string">'N'</span>]
</code></pre>
    <p class="normal">Let's look at the first few rows of the data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">df.head(<span class="hljs-number">10</span>)
</code></pre>
    <p class="normal">The preceding <a id="_idIndexMarker386"/>code will display something like this:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_23.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.20: The value is calculated as the average of the return of each state</p>
    <p class="normal">As we can observe, we now have the value of the state, which is just the average of a return of the state across several episodes. Thus, we have successfully predicted the value function of the given policy using the every-visit MC method. </p>
    <p class="normal">Okay, let's check the value of some states and understand how accurately our value function is estimated according to the given policy. Recall that when we started off, to generate episodes, we used the optimal policy, which selects the action 0 (stand) when the sum value is greater than 19 and the action 1 (hit) when the sum value is lower than 19. </p>
    <p class="normal">Let's evaluate the value of the state <code class="Code-In-Text--PACKT-">(21,9,False)</code>, as we can observe, the value of the sum of our cards is already 21 and so this is a good state and should have a high value. Let's see what our estimated value of the state is:</p>
    <pre class="programlisting code"><code class="hljs-code">df[df[<span class="hljs-string">'state'</span>]==(<span class="hljs-number">21</span>,<span class="hljs-number">9</span>,<span class="hljs-literal">False</span>)][<span class="hljs-string">'value'</span>].values
</code></pre>
    <p class="normal">The preceding will print something like this:</p>
    <pre class="programlisting code"><code class="hljs-code">array([<span class="hljs-number">1.0</span>])
</code></pre>
    <p class="normal">As we can <a id="_idIndexMarker387"/>observe, the value of the state is high.</p>
    <p class="normal">Now, let's check the value of the state <code class="Code-In-Text--PACKT-">(5,8,False)</code>. As we can notice, the value of the sum of our cards is just 5 and even the one dealer's single card has a high value, 8; in this case, the value of the state should be lower. Let's see what our estimated value of the state is:</p>
    <pre class="programlisting code"><code class="hljs-code">df[df[<span class="hljs-string">'state'</span>]==(<span class="hljs-number">5</span>,<span class="hljs-number">8</span>,<span class="hljs-literal">False</span>)][<span class="hljs-string">'value'</span>].values
</code></pre>
    <p class="normal">The preceding code will print something like this:</p>
    <pre class="programlisting code"><code class="hljs-code">array([<span class="hljs-number">-1.0</span>])
</code></pre>
    <p class="normal">As we can notice, the value of the state is lower.</p>
    <p class="normal">Thus, we learned how to predict the value function of the given policy using the every-visit MC prediction method. In the next section, we will look at how to compute the value of the state using the first-visit MC method. </p>
    <h3 id="_idParaDest-121" class="title">First-visit MC prediction with the blackjack game</h3>
    <p class="normal">Predicting <a id="_idIndexMarker388"/>the value function using the first-visit MC method is exactly the same as how <a id="_idIndexMarker389"/>we predicted the value function using the every-visit MC method, except that here we compute the return of a state only for its first time of occurrence in the episode. The code for first-visit MC is the same as what we have seen in every-visit MC except here, we compute the return only for its first time of occurrence as shown in the following highlighted code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
    
    episode = generate_episode(env,policy)
    states, actions, rewards = zip(*episode)
    <span class="hljs-keyword">for</span> t, state <span class="hljs-keyword">in</span> enumerate(states):
 <span class="code-highlight"><strong class="hljs-slc">       </strong><strong class="hljs-keyword-slc">if</strong><strong class="hljs-slc"> state </strong><strong class="hljs-keyword-slc">not</strong><strong class="hljs-slc"> </strong><strong class="hljs-keyword-slc">in</strong><strong class="hljs-slc"> states[</strong><strong class="hljs-number-slc">0</strong><strong class="hljs-slc">:t]:</strong></span>
            R = (sum(rewards[t:]))
            total_return[state] = total_return[state] + R
            N[state] = N[state] + <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">You can <a id="_idIndexMarker390"/>obtain the complete code from the GitHub repo of the book and you will get <a id="_idIndexMarker391"/>results similar to what we saw in the every-visit MC section.</p>
    <p class="normal">Thus, we learned how to predict the value function of the given policy using the first-visit and every-visit MC methods. </p>
    <h2 id="_idParaDest-122" class="title">Incremental mean updates</h2>
    <p class="normal">In both <a id="_idIndexMarker392"/>first-visit MC and every-visit MC, we estimate the value of a state as an average (arithmetic mean) return of the <a id="_idIndexMarker393"/>state across several episodes as shown as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_056.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">Instead of using the arithmetic mean to approximate the value of the state, we can also use the incremental mean, and it is expressed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_062.png" alt="" style="height: 1.11em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_04_063.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">But why do we need incremental mean? Consider our environment as non-stationary. In that case, we don't have to take the return of the state from all the episodes and compute <a id="_idIndexMarker394"/>the average. As the environment is non-stationary we can ignore returns from earlier episodes and use only <a id="_idIndexMarker395"/>the returns from the latest episodes for computing the average. Thus, we can compute the value of the state using the incremental mean as shown as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_064.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_04_065.png" alt="" style="height: 1.11em;"/> and <em class="italic">R</em><sub class="" style="font-style: italic;">t</sub> is the return of the state <em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>.</p>
    <h2 id="_idParaDest-123" class="title">MC prediction (Q function)</h2>
    <p class="normal">So far, we have learned how to predict the value function of the given policy using the Monte Carlo method. In this section, we will see how to predict the Q function of the given policy <a id="_idIndexMarker396"/>using the Monte Carlo method. </p>
    <p class="normal">Predicting the Q function of the given policy using the MC method is exactly the same as how we predicted the value function in the previous section except that here we use the return of the state-action pair, whereas in the case of the value function we used the return of the state. That is, just like we approximated the value of a state (value function) by computing the average return of the state across several episodes, we can also approximate the value of a state-action pair (Q function) by computing the average return of the state-action pair across several episodes. </p>
    <p class="normal">Thus, we generate several episodes using the given policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/>, then, we calculate the total_return(<em class="italic">s</em>, <em class="italic">a</em>), the sum of the return of the state-action pair across several episodes, and also we calculate <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>), the number of times the state-action pair is visited across several episodes. Then we compute the Q function or Q value as the average return of the state-action pair as shown as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_067.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">For instance, let consider a small example. Say we have two states <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> and we have two possible actions 0 and 1. Now, we compute total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>). Let's say our table after computation looks like <em class="italic">Table 4.4</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_24.png" alt=""/></figure>
    <p class="packt_figref">Table 4.4: The result of two actions in two states</p>
    <p class="normal">Once we <a id="_idIndexMarker397"/>have this, we can compute the Q value by just taking the average, that is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_067.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">Thus, we can compute the Q value for all state-action pairs as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_069.png" alt="" style="height: 1.11em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_04_070.png" alt="" style="height: 1.11em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_04_071.png" alt="" style="height: 1.11em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_04_072.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">The algorithm for predicting the Q function using the Monte Carlo method is as follows. As we can see, it is exactly the same as how we predicted the value function using the return of the state except that here we predict the Q function using the return of a state-action pair:</p>
    <ol>
      <li class="numbered" value="1">Let total_return(<em class="italic">s</em>, <em class="italic">a</em>) be the sum of the return of a state-action pair across several episodes and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) be the number of times a state-action pair is visited across several episodes. Initialize total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) for all state-action pairs to zero. The policy <img src="../Images/B15558_03_185.png" alt="" style="height: 0.84em;"/> is given as input</li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:<ol>
      <li class="bullet-l2" value="1">Compute return for the state-action pair, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
      <li class="bullet-l2">Update the total return of the state-action pair, total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>)</li>
      <li class="bullet-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
    </ol></li>
			
        </ol>
      </li>
      <li class="numbered"> Compute <a id="_idIndexMarker398"/>the Q function (Q value) by just taking the average, that is:
<figure class="mediaobject"><img src="../Images/B15558_04_067.png" alt="" style="height: 2.51em;"/></figure></li>
    </ol>
    <p class="normal">Recall that in the MC prediction of the value function, we learned two types of MC—first-visit MC and every-visit MC. In first-visit MC, we compute the return of the state only for the first time the state is visited in the episode and in every-visit MC we compute the return of the state every time the state is visited in the episode. </p>
    <p class="normal">Similarly, in the MC prediction of the Q function, we have two types of MC—first-visit MC and every-visit MC. In first-visit MC, we compute the return of the state-action pair only for the first time the state-action pair is visited in the episode and in every-visit MC we compute the return of the state-action pair every time the state-action pair is visited in the episode. </p>
    <p class="normal">As mentioned in the previous section, instead of using the arithmetic mean, we can also use the incremental mean. We learned that the value of a state can be computed using the incremental mean as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_064.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Similarly, we <a id="_idIndexMarker399"/>can also compute the Q value using the incremental mean as shown as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_077.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Now that we have learned how to perform the prediction task using the Monte Carlo method, in the next section, we will learn how to perform the control task using the Monte Carlo method.</p>
    <h1 id="_idParaDest-124" class="title">Monte Carlo control</h1>
    <p class="normal">In the control task, our goal is to find the optimal policy. Unlike the prediction task, here, we will <a id="_idIndexMarker400"/>not be given any policy as an input. So, we will begin by initializing a random policy, and then we try to find the optimal policy iteratively. That is, we try to find an optimal policy that gives the maximum return. In this section, we will learn how to perform the control task to find the optimal policy using the Monte Carlo method.</p>
    <p class="normal">Okay, we learned that in the control task our goal is to find the optimal policy. First, how can we compute a policy? We learned that the policy can be extracted from the Q function. That is, if we have a Q function, then we can extract policy by selecting an action in each state that has the maximum Q value as the following shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_078.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">So, to compute a policy, we need to compute the Q function. But how can we compute the Q function? We can compute the Q function similarly to what we learned in the MC prediction method. That is, in the MC prediction method, we learned that when given a policy, we can generate several episodes using that policy and compute the Q function (Q value) as the average return of the state-action pair across several episodes.</p>
    <p class="normal">We can perform the same step here to compute the Q function. But in the control method, we are not given any policy as input. So, we will initialize a random policy, and then we compute the Q function using the random policy. That is, just like we learned in the prediction method, we generate several episodes using our random policy. Then we compute the Q function (Q value) as the average return of a state-action pair across several episodes as the following shows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_067.png" alt="" style="height: 2.51em;"/></figure>
    <p class="normal">Let's suppose <a id="_idIndexMarker401"/>after computing the Q function as the average return of the state-action pair, our Q function looks like <em class="italic">Table 4.5</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_25.png" alt=""/></figure>
    <p class="packt_figref">Table 4.5: The Q table </p>
    <p class="normal">From the preceding Q function, we can extract a new policy by selecting an action in each state that has the maximum Q value. That is, <img src="../Images/B15558_04_078.png" alt="" style="height: 1.49em;"/>. Thus, our new policy selects action 0 in state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and action 1 in state <em class="italic">s</em><sub class="Subscript--PACKT-">1</sub> as it has the maximum Q value.</p>
    <p class="normal">However, this new policy will not be an optimal policy because this new policy is extracted from the Q function, which is computed using the random policy. That is, we initialized a random policy and generated several episodes using the random policy, then we computed the Q function by taking the average return of the state-action pair across several episodes. Thus, we are using the random policy to compute the Q function and so the new policy extracted from the Q function will not be an optimal policy. </p>
    <p class="normal">But now that we have extracted a new policy from the Q function, we can use this new policy to generate episodes in the next iteration and compute the new Q function. Then, from this new Q function, we extract a new policy. We repeat these steps iteratively until we find the optimal policy. This is explained clearly in the following steps:</p>
    <p class="normal"><strong class="keyword">Iteration 1</strong>—Let <img src="../Images/B15558_04_081.png" alt="" style="height: 0.84em;"/> be the random policy. We use this random policy to generate an episode, and then we compute the Q function <img src="../Images/B15558_04_082.png" alt="" style="height: 1.11em;"/> by taking the average return of the state-action pair. Then, from this Q function <img src="../Images/B15558_04_083.png" alt="" style="height: 1.11em;"/>, we extract a new policy <img src="../Images/B15558_04_084.png" alt="" style="height: 0.84em;"/>. This new policy <img src="../Images/B15558_04_085.png" alt="" style="height: 0.84em;"/> will not be an optimal policy since it is extracted from the Q function, which is computed using the random policy.</p>
    <p class="normal"><strong class="keyword">Iteration 2—</strong>So, we use <a id="_idIndexMarker402"/>the new policy <img src="../Images/B15558_04_086.png" alt="" style="height: 0.84em;"/> derived from the previous iteration to generate an episode and compute the new Q function <img src="../Images/B15558_04_087.png" alt="" style="height: 1.11em;"/> as average return of a state-action pair. Then, from this Q function <img src="../Images/B15558_04_088.png" alt="" style="height: 1.11em;"/>, we extract a new policy <img src="../Images/B15558_03_158.png" alt="" style="height: 0.84em;"/>. If the policy <img src="../Images/B15558_03_158.png" alt="" style="height: 0.84em;"/> is optimal we stop, else we go to iteration 3.</p>
    <p class="normal"><strong class="keyword">Iteration 3—</strong>Now, we use the new policy <img src="../Images/B15558_03_159.png" alt="" style="height: 0.84em;"/> derived from the previous iteration to generate an episode and compute the new Q function <img src="../Images/B15558_04_092.png" alt="" style="height: 1.11em;"/>. Then, from this Q function <img src="../Images/B15558_04_092.png" alt="" style="height: 1.11em;"/>, we extract a new policy <img src="../Images/B15558_04_094.png" alt="" style="height: 0.84em;"/>. If <img src="../Images/B15558_04_094.png" alt="" style="height: 0.84em;"/> is optimal we stop, else we go to the next iteration.</p>
    <p class="normal">We repeat this process for several iterations until we find the optimal policy <img src="../Images/B15558_04_096.png" alt="" style="height: 1.11em;"/> as shown in <em class="italic">Figure 4.21</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_097.png" alt="" style="height: 1.29em;"/></figure>
    <p class="packt_figref">Figure 4.21: The path to finding the optimal policy</p>
    <p class="normal">This step is called policy evaluation and improvement and is similar to the policy iteration method we covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">The Bellman Equation and Dynamic Programming</em>. Policy evaluation implies that at each step we evaluate the policy. Policy improvement implies that <a id="_idIndexMarker403"/>at each step we are improving the policy by taking the maximum Q value. Note that here, we select the policy in a greedy manner meaning that we are selecting policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/> by just taking the maximum Q value, and so we can call our policy a greedy policy.</p>
    <p class="normal">Now that we have a basic understanding of how the MC control method works, in the next section, we will look into the algorithm of the MC control method and learn about it in more detail. </p>
    <h2 id="_idParaDest-125" class="title">MC control algorithm</h2>
    <p class="normal">The following <a id="_idIndexMarker404"/>steps show the Monte Carlo control algorithm. As we can observe, unlike the MC prediction method, here, we will not be given any policy. So, we start off by initializing the random policy and use the random policy to generate an episode in the first iteration. Then, we will compute the Q function (Q value) as the average return of the state-action pair.</p>
    <p class="normal">Once we have the Q function, we extract a new policy by selecting an action in each state that has the maximum Q value. In the next iteration, we use the extracted new policy to generate an episode and compute the new Q function (Q value) as the average return of the state-action pair. We repeat these steps for many iterations to find the optimal policy. </p>
    <p class="normal">One more thing, we need to observe that just as we learned in the first-visit MC prediction method, here, we compute the return of the state-action pair only for the first time a state-action pair is visited in the episode. </p>
    <p class="normal">For a better understanding, we can compare the MC control algorithm with the MC prediction of the Q function. One difference we can observe is that, here, we compute the Q function in each iteration. But if you notice, in the MC prediction of the Q function, we compute the Q function after all the iterations. The reason for computing the Q function in every iteration here is that we need the Q function to extract the new policy so <a id="_idIndexMarker405"/>that we can use the extracted new policy in the next iteration to generate an episode:</p>
    <ol>
      <li class="numbered" value="1">Let total_return(<em class="italic">s</em>, <em class="italic">a</em>) be the sum of the return of a state-action pair across several episodes and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) be the number of times a state-action pair is visited across several episodes. Initialize total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) for all state-action pairs to zero and initialize a random policy <img src="../Images/B15558_04_099.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using policy <img src="../Images/B15558_04_099.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode: 
<p class="bullet-para">If (<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) is occurring for the first time in the episode: </p>
<ol>
      <li class="bullet-l2" value="1">Compute the return of a state-action pair, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
      <li class="bullet-l2">Update the total return of the state-action pair as, total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>)</li>
      <li class="bullet-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
      <li class="bullet-l2">Compute the Q value by just taking the average, that is, <figure class="mediaobject"><img src="../Images/B15558_04_101.png" alt="" style="height: 2.51em;"/></figure></li>
    </ol>
</li>
<li class="numbered-l2">Compute the new updated policy <img src="../Images/B15558_04_032.png" alt="" style="height: 0.84em;"/> using the Q function:<figure class="mediaobject"><img src="../Images/B15558_04_078.png" alt="" style="height: 1.49em;"/></figure></li>
        </ol>
      </li>
    </ol>
    <p class="normal">From the preceding algorithm, we can observe that we generate an episode using the policy <img src="../Images/B15558_04_054.png" alt="" style="height: 0.84em;"/>. Then for each step in the episode, we compute the return of state-action pair and compute the Q function <em class="italic">Q</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) as an average return, then from this Q function, we extract a new policy <img src="../Images/B15558_03_038.png" alt="" style="height: 0.84em;"/>. We repeat this step iteratively to find the optimal policy <img src="../Images/B15558_03_172.png" alt="" style="height: 0.84em;"/>. Thus, we learned <a id="_idIndexMarker406"/>how to perform the control task using the Monte Carlo method.</p>
    <p class="normal">We can classify the control methods into two types:</p>
    <ul>
      <li class="bullet">On-policy control</li>
      <li class="bullet">Off-policy control</li>
    </ul>
    <p class="normal"><strong class="keyword">On-policy control—</strong>In the <a id="_idIndexMarker407"/>on-policy control method, the agent behaves using one policy and also tries to improve the same policy. That is, in the on-policy method, we generate episodes using one policy and also improve the same policy iteratively to find the optimal policy. For instance, the MC control method, which we just learned above, can be called on-policy MC control as we are generating episodes using a policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/>, and we also try to improve the same policy <img src="../Images/B15558_03_185.png" alt="" style="height: 0.84em;"/> on every iteration to compute the optimal policy.</p>
    <p class="normal"><strong class="keyword">Off-policy control—</strong>In the <a id="_idIndexMarker408"/>off-policy control method, the agent behaves using one policy <em class="italic">b</em> and tries to improve a different policy <em class="italic"><img src="../Images/B15558_04_099.png" alt="" style="height: 0.84em;"/></em>. That is, in the off-policy method, we generate episodes using one policy and we try to improve the different policy iteratively to find the optimal policy.</p>
    <p class="normal">We will learn how exactly the preceding two control methods work in detail in the upcoming sections.</p>
    <h2 id="_idParaDest-126" class="title">On-policy Monte Carlo control</h2>
    <p class="normal">There <a id="_idIndexMarker409"/>are two types of on-policy Monte Carlo control <a id="_idIndexMarker410"/>methods:</p>
    <ul>
      <li class="bullet">Monte Carlo exploring starts</li>
      <li class="bullet">Monte Carlo with the epsilon-greedy policy</li>
    </ul>
    <h3 id="_idParaDest-127" class="title">Monte Carlo exploring starts</h3>
    <p class="normal">We have already learned how the Monte Carlo control method works. One thing we may want <a id="_idIndexMarker411"/>to take into account is exploration. There can be several actions in a state: some actions will be optimal, while others won't. To understand whether an action is optimal or not, the agent has to explore by performing that action. If the agent never explores a particular action in a state, then it will never know whether it is a good action or not. So, how can we solve this? That is, how can we ensure enough exploration? Here is where Monte Carlo exploring starts helps us. </p>
    <p class="normal">In the MC exploring starts method, we set all state-action pairs to a non-zero probability for being the initial state-action pair. So before generating an episode, first, we choose the initial state-action pair randomly and then we generate the episode starting from this initial state-action pair following the policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/>. Then, in every iteration, our policy will be updated as a greedy policy (selecting the max Q value; see the next section on <em class="italic">Monte Carlo with the epsilon-greedy policy</em> for more details).</p>
    <p class="normal">The following steps show the algorithm of MC control exploring starts. It is essentially the same as what we learned earlier for the MC control algorithm section, except that here, we select an initial state-action pair and generate episodes starting from this initial state-action pair as shown in the bold point:</p>
    <ol>
      <li class="numbered" value="1">Let total_return(<em class="italic">s</em>, <em class="italic">a</em>) be the sum of the return of a state-action pair across several episodes and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) be the number of times a state-action pair is visited across several episodes. Initialize total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) for all state-action pairs to zero and initialize a random policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2" value="1"><strong class="keyword">Select the initial state s</strong><strong class="keyword">0</strong><strong class="keyword"> and initial action a</strong><strong class="keyword">0</strong><strong class="keyword"> randomly such that all state-action pairs have a probability greater than 0</strong></li>
          <li class="numbered-l2">Generate an episode from the selected initial state <em class="italic">s</em><sub class="Subscript--PACKT-">0</sub> and action <em class="italic">a</em><sub class="Subscript--PACKT-">0</sub> using policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all the rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:<p class="bullet-para">If (<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) is occurring for the first time in the episode: </p><ol>
      <li class="bullet-l2" value="1">Compute the return of a state-action pair, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
      <li class="bullet-l2">Update <a id="_idIndexMarker412"/>the total return of the state-action pair as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>)</li>
      <li class="bullet-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
      <li class="bullet-l2">Compute the Q value by just taking the average, that is, <figure class="mediaobject"><img src="../Images/B15558_04_101.png" alt="" style="height: 2.51em;"/></figure></li>
    </ol></li>
<li class="numbered-l2">Compute the updated policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/> using the Q function: <figure class="mediaobject"><img src="../Images/B15558_04_078.png" alt="" style="height: 1.49em;"/></figure></li>
        </ol> 
      </li>
    </ol>
    <p class="normal">One of the <a id="_idIndexMarker413"/>major drawbacks of the exploring starts method is that it is not applicable to every environment. That is, we can't just randomly choose any state-action pair as an initial state-action pair because in some environments there can be only one state-action pair that can act as an initial state-action pair. So we can't randomly select the state-action pair as the initial state-action pair.</p>
    <p class="normal">For example, suppose we are training an agent to play a car racing game; we can't start the episode in a random position as the initial state and a random action as the initial action because we have a fixed single starting state and action as the initial state and action.</p>
    <p class="normal">Thus, to overcome the problem in exploring starts, in the next section, we will learn about the <a id="_idIndexMarker414"/>Monte Carlo control method with a new type of policy called the epsilon-greedy policy.</p>
    <h3 id="_idParaDest-128" class="title">Monte Carlo with the epsilon-greedy policy</h3>
    <p class="normal">Before going <a id="_idIndexMarker415"/>ahead, first, let us understand what the epsilon-greedy policy is as it is ubiquitous in reinforcement learning.</p>
    <p class="normal">First, let's learn what a greedy policy is. A greedy policy is one that selects the best action available at the moment. For instance, let's say we are in some state <strong class="keyword">A</strong> and we have four possible actions in the state. Let the actions be <em class="italic">up, down, left, </em>and<em class="italic"> right</em>. But let's suppose our agent has explored only two actions, <em class="italic">up</em> and <em class="italic">right</em>, in the state <strong class="keyword">A</strong>; the Q value of actions <em class="italic">up</em> and <em class="italic">right</em> in the state <strong class="keyword">A</strong> are shown in <em class="italic">Table 4.6</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_26.png" alt=""/></figure>
    <p class="packt_figref">Table 4.6: The agent has only explored two actions in state A</p>
    <p class="normal">We learned that the greedy policy selects the best action available at the moment. So the greedy policy checks the Q table and selects the action that has the maximum Q value in state <strong class="keyword">A</strong>. As we can see, the action <em class="italic">up</em> has the maximum Q value. So our greedy policy selects the action <em class="italic">up</em> in state <strong class="keyword">A</strong>. </p>
    <p class="normal">But one problem with the greedy policy is that it never explores the other possible actions; instead, it always picks the best action available at the moment. In the preceding example, the greedy policy always selects the action <em class="italic">up</em>. But there could be other actions in state <strong class="keyword">A</strong> that might be more optimal than the action <em class="italic">up</em> that the agent has not explored yet. That is, we still have two more actions—<em class="italic">down</em> and <em class="italic">left</em>—in state <strong class="keyword">A</strong> that the agent has not explored yet, and they might be more optimal than the action <em class="italic">up</em>. </p>
    <p class="normal">So, now the question is whether the agent should explore all the other actions in the state and select the best action as the one that has the maximum Q value or exploit the best action <a id="_idIndexMarker416"/>out of already-explored actions. This is called an <strong class="keyword">exploration-exploitation dilemma.</strong></p>
    <p class="normal">Say there are many routes from our work to home and we have explored only two routes so far. Thus, to reach home, we can select the route that takes us home most quickly out of the two routes we have explored. However, there are still many other routes that we have not explored yet that might be even better than our current optimal route. The question is whether we should explore new routes (exploration) or whether we should always use our current optimal route (exploitation).</p>
    <p class="normal">To avoid this dilemma, we introduce a new policy called the epsilon-greedy policy. Here, all actions are tried with a non-zero probability (epsilon). With a probability epsilon, we explore different actions randomly and with a probability 1-epsilon, we choose an action that has the maximum Q value. That is, with a probability epsilon, we select a random action (exploration) and with a probability 1-epsilon we select the best action (exploitation).</p>
    <p class="normal">In the <a id="_idIndexMarker417"/>epsilon-greedy policy, if we set the value of epsilon to 0, then it becomes a greedy policy (only exploitation), and when we set the value of epsilon to 1, then we will always end up doing only the exploration. So, the value of epsilon has to be chosen optimally between 0 and 1. </p>
    <p class="normal">Say we set epsilon = 0.5; then we will generate a random number from the uniform distribution and if the random number is less than epsilon (0.5), then we select a random action (exploration), but if the random number is greater than or equal to epsilon then we select the best action, that is, the action that has the maximum Q value (exploitation).</p>
    <p class="normal">So, in this way, we explore actions that we haven't seen before with the probability epsilon and select the best actions out of the explored actions with the probability 1-epsilon. As <em class="italic">Figure 4.22 </em>shows, if the random number we generated from the uniform distribution is less than epsilon, then we choose a random action. If the random number is greater than or equal to epsilon, then we choose the best action: </p>
    <figure class="mediaobject"><img src="../Images/B15558_04_27.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.22: Epsilon-greedy policy </p>
    <p class="normal">The following snippet shows the Python code for the epsilon-greedy policy:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">epsilon_greedy_policy</span><span class="hljs-function">(</span><span class="hljs-params">state, epsilon</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">if</span> random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>) &lt; epsilon:
         <span class="hljs-keyword">return</span> env.action_space.sample()
    <span class="hljs-keyword">else</span>:
         <span class="hljs-keyword">return</span> max(list(range(env.action_space.n)), key = <span class="hljs-keyword">lambda</span> x: q[(state,x)])
</code></pre>
    <p class="normal">Now that <a id="_idIndexMarker418"/>we have understood what an epsilon-greedy policy is, and how it is used to solve the exploration-exploitation dilemma, in the next section, we will look at how to use the epsilon-greedy policy in the Monte Carlo control method.</p>
    <h4 class="title">The MC control algorithm with the epsilon-greedy policy</h4>
    <p class="normal">The algorithm <a id="_idIndexMarker419"/>of Monte Carlo control with the epsilon-greedy policy is essentially the same as the MC control algorithm we learned earlier except that here we select actions based on the epsilon-greedy policy to avoid the exploration-exploitation dilemma. The following steps show the algorithm of Monte Carlo with the epsilon-greedy policy:</p>
    <ol>
      <li class="numbered" value="1">Let total_return(<em class="italic">s</em>, <em class="italic">a</em>) be the sum of the return of a state-action pair across several episodes and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) be the number of times a state-action pair is visited across several episodes. Initialize total_return(<em class="italic">s</em>, <em class="italic">a</em>) and <em class="italic">N</em>(<em class="italic">s</em>, <em class="italic">a</em>) for all state-action pairs to zero and initialize a random policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/></li>
      <li class="numbered">For <em class="italic">M</em> number of iterations:<ol>
          <li class="numbered-l2">Generate an episode using policy <img src="../Images/B15558_04_117.png" alt="" style="height: 0.84em;"/></li>
          <li class="numbered-l2">Store all rewards obtained in the episode in the list called rewards</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode:<p class="bullet-para">If (<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) is occurring for the first time in the episode:</p>
<ol>
      <li class="bullet-l2" value="1">Compute the return of a state-action pair, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:])</li>
      <li class="bullet-l2">Update the total return of the state-action pair as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>)</li>
      <li class="bullet-l2">Update the counter as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1</li>
      <li class="bullet-l2">Compute the Q value by just taking the average, that is, <figure class="mediaobject"><img src="../Images/B15558_04_101.png" alt="" style="height: 2.51em;"/></figure></li>
    </ol>
</li>
<li class="numbered-l2">Compute the updated policy <img src="../Images/B15558_04_119.png" alt="" style="height: 0.84em;"/> using the Q function. Let <img src="../Images/B15558_04_120.png" alt="" style="height: 1.49em;"/>. The policy <img src="../Images/B15558_04_054.png" alt="" style="height: 0.84em;"/> selects the best action <img src="../Images/B15558_04_122.png" alt="" style="height: 1.11em;"/> with probability <img src="../Images/B15558_04_123.png" alt="" style="height: 1.11em;"/> and random action with probability <img src="../Images/B15558_04_124.png" alt="" style="height: 0.93em;"/></li>
</ol>  
      </li>
    </ol>
    <p class="normal">As we can <a id="_idIndexMarker420"/>observe, in every iteration, we generate the episode using the policy <img src="../Images/B15558_04_125.png" alt="" style="height: 0.84em;"/> and also we try to improve the same policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> in every iteration to compute the optimal policy.</p>
    <h3 id="_idParaDest-129" class="title">Implementing on-policy MC control</h3>
    <p class="normal">Now, let's <a id="_idIndexMarker421"/>learn how to implement the MC control method with the epsilon-greedy policy to play the blackjack game; that is, we will see how can we use the MC control method to find the optimal policy in the blackjack game.</p>
    <p class="normal">First, let's import the necessary libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> gym
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> random
<span class="hljs-keyword">from</span> collections <span class="hljs-keyword">import</span> defaultdict
</code></pre>
    <p class="normal">Create a blackjack environment:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">'Blackjack-v0'</span>)
</code></pre>
    <p class="normal">Initialize the dictionary for storing the Q values:</p>
    <pre class="programlisting code"><code class="hljs-code">Q = defaultdict(float)
</code></pre>
    <p class="normal">Initialize <a id="_idIndexMarker422"/>the dictionary for storing the total return of the state-action pair:</p>
    <pre class="programlisting code"><code class="hljs-code">total_return = defaultdict(float)
</code></pre>
    <p class="normal">Initialize the dictionary for storing the count of the number of times a state-action pair is visited:</p>
    <pre class="programlisting code"><code class="hljs-code">N = defaultdict(int)
</code></pre>
    <h4 class="title">Define the epsilon-greedy policy</h4>
    <p class="normal">We learned <a id="_idIndexMarker423"/>that we select actions based on the epsilon-greedy policy, so we define a function called <code class="Code-In-Text--PACKT-">epsilon_greedy_policy</code>, which takes the state and Q value as an input and returns the action to be performed in the given state:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">epsilon_greedy_policy</span><span class="hljs-function">(</span><span class="hljs-params">state,Q</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Set the epsilon value to 0.5:</p>
    <pre class="programlisting code"><code class="hljs-code">    epsilon = <span class="hljs-number">0.5</span>
</code></pre>
    <p class="normal">Sample a random value from the uniform distribution; if the sampled value is less than epsilon then we select a random action, else we select the best action that has the maximum Q value as shown:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> random.uniform(<span class="hljs-number">0</span>,<span class="hljs-number">1</span>) &lt; epsilon:
        <span class="hljs-keyword">return</span> env.action_space.sample()
    <span class="hljs-keyword">else</span>:
        <span class="hljs-keyword">return</span> max(list(range(env.action_space.n)), key = <span class="hljs-keyword">lambda</span> x: Q[(state,x)]) 
</code></pre>
    <h4 class="title">Generating an episode</h4>
    <p class="normal">Now, let's <a id="_idIndexMarker424"/>generate an episode using the epsilon-greedy policy. We define a function called <code class="Code-In-Text--PACKT-">generate_episode</code>, which takes the Q value as an input and returns the episode. </p>
    <p class="normal">First, let's set the number of time steps:</p>
    <pre class="programlisting code"><code class="hljs-code">num_timesteps = <span class="hljs-number">100</span>
</code></pre>
    <p class="normal">Now, let's define the function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">generate_episode</span><span class="hljs-function">(</span><span class="hljs-params">Q</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Initialize a list for storing the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    episode = []
</code></pre>
    <p class="normal">Initialize the state using the <code class="Code-In-Text--PACKT-">reset</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = env.reset()
</code></pre>
    <p class="normal">Then for each time step:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
</code></pre>
    <p class="normal">Select the action according to the epsilon-greedy policy:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = epsilon_greedy_policy(state,Q)
</code></pre>
    <p class="normal">Perform the selected action and store the next state information:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state, reward, done, info = env.step(action)
</code></pre>
    <p class="normal">Store the state, action, and reward in the episode list:</p>
    <pre class="programlisting code"><code class="hljs-code">        episode.append((state, action, reward))
</code></pre>
    <p class="normal">If the next state is the final state then break the loop, else update the next state to the current state:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> done:
            <span class="hljs-keyword">break</span>
            
        state = next_state
    <span class="hljs-keyword">return</span> episode
</code></pre>
    <h4 class="title">Computing the optimal policy</h4>
    <p class="normal">Now, let's <a id="_idIndexMarker425"/>learn how to compute the optimal policy. First, let's set the number of iterations, that is, the number of episodes, we want to generate:</p>
    <pre class="programlisting code"><code class="hljs-code">num_iterations = <span class="hljs-number">500000</span>
</code></pre>
    <p class="normal">For each iteration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_iterations):
</code></pre>
    <p class="normal">We learned that in the on-policy control method, we will not be given any policy as an input. So, we initialize a random policy in the first iteration and improve the policy iteratively by computing the Q value. Since we extract the policy from the Q function, we don't have to explicitly define the policy. As the Q value improves, the policy also improves implicitly. That is, in the first iteration, we generate the episode by extracting the policy (epsilon-greedy) from the initialized Q function. Over a series of iterations, we will find the optimal Q function, and hence we also find the optimal policy. </p>
    <p class="normal">So, here we pass our initialized Q function to generate an episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    episode = generate_episode(Q)
</code></pre>
    <p class="normal">Get all the state-action pairs in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    all_state_action_pairs = [(s, a) <span class="hljs-keyword">for</span> (s,a,r) <span class="hljs-keyword">in</span> episode]
</code></pre>
    <p class="normal">Store all the rewards obtained in the episode in the rewards list:</p>
    <pre class="programlisting code"><code class="hljs-code">    rewards = [r <span class="hljs-keyword">for</span> (s,a,r) <span class="hljs-keyword">in</span> episode]
</code></pre>
    <p class="normal">For each step in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t, (state, action,_) <span class="hljs-keyword">in</span> enumerate(episode):
</code></pre>
    <p class="normal">If the state-action pair is occurring for the first time in the episode:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> (state, action) <span class="hljs-keyword">in</span> all_state_action_pairs[<span class="hljs-number">0</span>:t]:
          
</code></pre>
    <p class="normal">Compute the return <code class="Code-In-Text--PACKT-">R</code> of the state-action pair as the sum of rewards, <em class="italic">R</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = sum(rewards[t:]):</p>
    <pre class="programlisting code"><code class="hljs-code">            R = sum(rewards[t:])
</code></pre>
    <p class="normal">Update the total return of the state-action pair as total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = total_return(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + R(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>):</p>
    <pre class="programlisting code"><code class="hljs-code">            total_return[(state,action)] = total_return[(state,action)] + R
</code></pre>
    <p class="normal">Update the number of times the state-action pair is visited as <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">N</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + 1:</p>
    <pre class="programlisting code"><code class="hljs-code">            N[(state, action)] += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">Compute the Q value by just taking the average, that is,</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_127.png" alt="" style="height: 2.51em;"/></figure>
    <pre class="programlisting code"><code class="hljs-code">            Q[(state,action)] = total_return[(state, action)] / N[(state, action)]
</code></pre>
    <p class="normal">Thus on <a id="_idIndexMarker426"/>every iteration, the Q value improves and so does the policy.</p>
    <p class="normal">After all the iterations, we can have a look at the Q value of each state-action pair in the pandas data frame for more clarity. First, let's convert the Q value dictionary into a pandas data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">df = pd.DataFrame(Q.items(),columns=[<span class="hljs-string">'state_action pair'</span>,<span class="hljs-string">'value'</span>])
</code></pre>
    <p class="normal">Let's look at the first few rows of the data frame:</p>
    <pre class="programlisting code"><code class="hljs-code">df.head(<span class="hljs-number">11</span>)
</code></pre>
    <figure class="mediaobject"><img src="../Images/B15558_04_28.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.23: The Q values of the state-action pairs</p>
    <p class="normal">As we <a id="_idIndexMarker427"/>can observe, we have the Q values for all the state-action pairs. Now we can extract the policy by selecting the action that has the maximum Q value in each state. For instance, say we are in the state <code class="Code-In-Text--PACKT-">(21,8, True)</code>. Now, should we perform action 0 (stand) or action 1 (hit)? It makes more sense to perform action 0 (stand) here, since the value of the sum of our cards is already 21, and if we perform action 1 (hit) our game will lead to a bust. </p>
    <p class="normal">Note that due to stochasticity, you might get different results than those shown here. </p>
    <p class="normal">Let's look at the Q values of all the actions in this state, <code class="Code-In-Text--PACKT-">(21,8, True)</code>:</p>
    <pre class="programlisting code"><code class="hljs-code">df[<span class="hljs-number">124</span>:<span class="hljs-number">126</span>]
</code></pre>
    <p class="normal">The preceding code will print the following:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_29.png" alt=""/></figure>
    <p class="packt_figref">Figure 4.24: The Q values of the state (21,8, True)</p>
    <p class="normal">As we can <a id="_idIndexMarker428"/>observe, we have a maximum Q value for action 0 (stand) compared to action 1 (hit). So, we perform action 0 in the state <code class="Code-In-Text--PACKT-">(21,8, True)</code>. Similarly, in this way, we can extract the policy by selecting the action in each state that has the maximum Q value. </p>
    <p class="normal">In the next section, we will learn about an off-policy control method that uses two different policies. </p>
    <h2 id="_idParaDest-130" class="title">Off-policy Monte Carlo control</h2>
    <p class="normal">Off-policy <a id="_idIndexMarker429"/>Monte Carlo is another <a id="_idIndexMarker430"/><a id="_idIndexMarker431"/>interesting Monte Carlo control method. In the off-policy method, we use two policies called the behavior policy <a id="_idIndexMarker432"/>and the target policy. As the name suggests, we behave (generate episodes) using <a id="_idIndexMarker433"/>the behavior policy and we try to improve the other policy called the target policy.</p>
    <p class="normal">In the on-policy method, we generate an episode using the policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> and we improve the same policy <img src="../Images/B15558_04_129.png" alt="" style="height: 0.84em;"/> iteratively to find the optimal policy. But in the off-policy method, we generate an episode using a policy called the behavior policy <em class="italic">b</em> and we try to iteratively improve a different policy called the target policy <img src="../Images/B15558_04_130.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">That is, in the on-policy method, we learned that the agent generates an episode using the policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/>. Then for each step in the episode, we compute the return of the state-action pair and compute the Q function <em class="italic">Q</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) as an average return, then from this Q function, we extract a new policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/>. We repeat this step iteratively to find the optimal policy <img src="../Images/B15558_04_032.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">But in the off-policy method, the agent generates an episode using a policy called the behavior policy <em class="italic">b</em>. Then for each step in the episode, we compute the return of the state-action pair and compute the Q function <em class="italic">Q</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) as an average return, then from this Q function, we extract a new policy called the target policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/>. We repeat this step iteratively to find the optimal target policy <img src="../Images/B15558_04_032.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">The behavior policy will usually be set to the epsilon-greedy policy and thus the agent explores the environment with the epsilon-greedy policy and generates an episode. Unlike the behavior policy, the target policy is set to be the greedy policy and so the target policy will always select the best action in each state.</p>
    <p class="normal">Let's now understand how the off-policy Monte Carlo method works exactly. First, we will initialize the Q function with random values. Then we generate an episode using the behavior policy, which is the epsilon-greedy policy. That is, from the Q function we select the <a id="_idIndexMarker434"/>best action (the action that has the max Q value) with probability 1-epsilon and we select the random action <a id="_idIndexMarker435"/>with probability epsilon. Then for each step in the episode, we compute the return of the state-action pair and compute the Q function <em class="italic">Q</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) as an average return. Instead of using the arithmetic mean to compute the Q function, we can use the incremental mean. We can compute the Q function using the incremental mean as shown as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_077.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">After computing the Q function, we extract the target policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/> by selecting an action in each state that has the maximum Q value as shown as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_138.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">The algorithm is given as follows:</p>
    <ol>
      <li class="numbered" value="1">Initialize the Q function <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) with random values, set the behavior policy <em class="italic">b</em> to be epsilon-greedy, and also set the target policy <img src="../Images/B15558_03_055.png" alt="" style="height: 0.84em;"/> to be greedy policy.</li>
      <li class="numbered">For <em class="italic">M</em> number of episodes:<ol>
          <li class="numbered-l2">Generate an episode using the behavior policy <em class="italic">b</em></li>
          <li class="numbered-l2">Initialize return <em class="italic">R</em> to 0</li>
          <li class="numbered-l2">For each step <em class="italic">t</em> in the episode, <em class="italic">t</em> = <em class="italic">T</em>-1, <em class="italic">T</em>-2,…, 0:<ol>
      <li class="bullet-l2" value="1">Compute the return as <em class="italic">R</em> = <em class="italic">R</em>+<em class="italic">r</em><sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">+1</sub></li>
      <li class="bullet-l2">Compute the Q value as <img src="../Images/B15558_04_077.png" alt="" style="height: 1.29em;"/></li>
      <li class="bullet-l2">Compute the target policy <img src="../Images/B15558_04_138.png" alt="" style="height: 1.58em;"/></li>
    </ol></li>
        </ol>
      </li>
      <li class="numbered">Return the target policy <img src="../Images/B15558_04_142.png" alt="" style="height: 0.84em;"/></li>
    </ol>
    <p class="normal">As we can <a id="_idIndexMarker436"/>observe from the preceding algorithm, first we set the Q values of all the state-action pairs to random values and then we generate an episode <a id="_idIndexMarker437"/>using the behavior policy. Then on each step of the episode, we compute the updated Q function (Q values) using the incremental mean and then we extract the target policy from the updated Q function. As we can notice, on every iteration, the Q function is constantly improving and since we are extracting the target policy from the Q function, our target policy will also be improving on every iteration.</p>
    <p class="normal">Also, note that since it is an off-policy method, the episode is generated using the behavior policy and we try to improve the target policy.</p>
    <p class="normal">But wait! There is a small issue here. Since we are finding the target policy <img src="../Images/B15558_04_142.png" alt="" style="height: 0.84em;"/> from the Q function, which is computed based on the episodes generated by a different policy called the behavior policy, our target policy will be inaccurate. This is because the distribution of the behavior policy and the target policy will be different. So, to correct this, we introduce a new technique called <strong class="keyword">importance sampling</strong>. This is a technique for estimating the values of one distribution when given samples from another.</p>
    <p class="normal">Let us say we want to compute the expectation of a function <em class="italic">f</em>(<em class="italic">x</em>) where the value of <em class="italic">x</em> is sampled from the distribution <em class="italic">p</em>(<em class="italic">x</em>) that is, <img src="../Images/B15558_04_143.png" alt="" style="height: 1.11em;"/>; then we can write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_144.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">With the importance sampling method, we estimate the expectation using a different distribution <em class="italic">q</em>(<em class="italic">x</em>); that is, instead of sampling <em class="italic">x</em> from <em class="italic">p</em>(<em class="italic">x</em>) we use a different distribution <em class="italic">q</em>(<em class="italic">x</em>) as shown as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_145.png" alt="" style="height: 5.73em;"/></figure>
    <p class="normal">The ratio <img src="../Images/B15558_04_146.png" alt="" style="height: 2.51em;"/> is called the importance sampling ratio or importance correction.</p>
    <p class="normal">Okay, how does <a id="_idIndexMarker438"/>importance sampling help us? We learned that with importance sampling, we can estimate the value of <a id="_idIndexMarker439"/>one distribution by sampling from another using the importance sampling ratio. In off-policy control, we can estimate the target policy with the samples (episodes) from the behavior policy using the importance sampling ratio.</p>
    <p class="normal">Importance sampling has two types:</p>
    <ul>
      <li class="bullet">Ordinary importance sampling</li>
      <li class="bullet">Weighted importance sampling</li>
    </ul>
    <p class="normal">In ordinary importance sampling, the importance sampling ratio will be the ratio of the target policy to the behavior policy <img src="../Images/B15558_04_147.png" alt="" style="height: 2.51em;"/> and in weighted importance sampling, the importance sampling ratio will be the weighted ratio of the target policy to the behavior policy <img src="../Images/B15558_04_148.png" alt="" style="height: 2.51em;"/>.</p>
    <p class="normal">Let's now understand how we use weighted importance sampling in the off-policy Monte Carlo method. Let <em class="italic">W</em> be the weight and <em class="italic">C</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) denote the cumulative sum of weights across all the episodes. We learned that we compute the Q function (Q values) using the incremental mean as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_077.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Now, we slightly modify our Q function computation with the weighted importance sampling as shown as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_04_150.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">The algorithm <a id="_idIndexMarker440"/>of the off-policy Monte Carlo method is shown next. First, we generate an episode using the behavior <a id="_idIndexMarker441"/>policy and then we initialize return <em class="italic">R</em> to 0 and the weight <em class="italic">W</em> to 1. Then on every step of the episode, we compute the return and update the cumulative weight as <em class="italic">C</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">C</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + <em class="italic">W</em>. After updating the cumulative weights, we update the Q value as <img src="../Images/B15558_04_150.png" alt="" style="height: 2.4em;"/>.</p>
    <p class="normal">From the Q value, we extract the target policy as <img src="../Images/B15558_04_138.png" alt="" style="height: 1.58em;"/>. When the action <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub> given by the behavior policy and the target policy is not the same then we break the loop and generate the next episode; else we update the weight as <img src="../Images/B15558_04_153.png" alt="" style="height: 2.4em;"/>.</p>
    <p class="normal">The complete algorithm of the off-policy Monte Carlo method is explained in the following steps:</p>
    <ol>
      <li class="numbered" value="1">Initialize the Q function <em class="italic">Q</em>(<em class="italic">s</em>, <em class="italic">a</em>) with random values, set the behavior policy <em class="italic">b</em> to be epsilon-greedy, and target policy <img src="../Images/B15558_03_084.png" alt="" style="height: 0.84em;"/> to be greedy policy and initialize the cumulative weights as <em class="italic">C</em>(<em class="italic">s</em>, <em class="italic">a</em>) = 0</li>
      <li class="numbered">For <em class="italic">M</em> number of episodes:<ol>
          <li class="numbered-l2">Generate an episode using the behavior policy <em class="italic">b</em></li>
          <li class="numbered-l2">Initialize return <em class="italic">R</em> to 0 and weight <em class="italic">W</em> to 1</li>
          <li class="numbered-l2">For <a id="_idIndexMarker442"/>each step <em class="italic">t</em> in the episode, <em class="italic">t</em> = <em class="italic">T</em>-1, <em class="italic">T</em>-2,…, 0:<ol>
      <li class="bullet-l2" value="1">Compute <a id="_idIndexMarker443"/>the return as <em class="italic">R</em> = <em class="italic">R</em>+<em class="italic">r</em><sub class="" style="font-style: italic;">t</sub><sub class="Subscript--PACKT-">+1</sub></li>
      <li class="bullet-l2">Update the cumulative weights <em class="italic">C</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) = <em class="italic">C</em>(<em class="italic">s</em><sub class="" style="font-style: italic;">t</sub>, <em class="italic">a</em><sub class="" style="font-style: italic;">t</sub>) + <em class="italic">W</em></li>
      <li class="bullet-l2">Update the Q value as <img src="../Images/B15558_04_150.png" alt="" style="height: 2.4em;"/></li>
      <li class="bullet-l2">Compute the target policy <img src="../Images/B15558_04_138.png" alt="" style="height: 1.58em;"/></li>
      <li class="bullet-l2">If <img src="../Images/B15558_04_157.png" alt="" style="height: 1.11em;"/> then break</li>
      <li class="bullet-l2">Update the weight as <img src="../Images/B15558_04_153.png" alt="" style="height: 2.4em;"/></li>
    </ol></li>
        </ol>
      </li>
      <li class="numbered">Return the target policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/></li>
    </ol>
    <h1 id="_idParaDest-131" class="title">Is the MC method applicable to all tasks?</h1>
    <p class="normal">We learned that Monte Carlo is a model-free method, and so it doesn't require the model dynamics <a id="_idIndexMarker444"/>of the environment to compute the value and Q function in order to find the optimal policy. The Monte Carlo method computes the value function and Q function by just taking the average return of the state and the average return of the state-action pair, respectively.</p>
    <p class="normal">But one issue with the Monte Carlo method is that it is applicable only to episodic tasks. We learned that in the Monte Carlo method, we compute the value of the state by taking the average return of the state and the return is the sum of rewards of the episode. But when there is no episode, that is, if our task is a continuous task (non-episodic task), then we cannot apply the Monte Carlo method.</p>
    <p class="normal">Okay, how do we compute the value of the state where we have a continuous task and also where <a id="_idIndexMarker445"/>we don't know the model dynamics of the environment? Here is where we use another interesting model-free method called temporal difference learning. In the next chapter, we will learn exactly how temporal difference learning works.</p>
    <h1 id="_idParaDest-132" class="title">Summary</h1>
    <p class="normal">We started the chapter by understanding what the Monte Carlo method is. We learned that in the Monte Carlo method, we approximate the expectation of a random variable by sampling, and when the sample size is greater, the approximation will be better. Then we learned about the prediction and control tasks. In the prediction task, we evaluate the given policy by predicting the value function or Q function, which helps us to understand the expected return an agent would get if it uses the given policy. In the control task, our goal is to find the optimal policy, and we will not be given any policy as input, so we start by initializing a random policy and we try to find the optimal policy iteratively.</p>
    <p class="normal">Moving forward, we learned how to use the Monte Carlo method to perform the prediction task. We learned that the value of a state and the value of a state-action pair can be computed by just taking the average return of the state and an average return of state-action pair across several episodes, respectively. </p>
    <p class="normal">We also learned about the first-visit MC and every-visit MC methods. In first-visit MC, we compute the return only for the first time the state is visited in the episode, and in every-visit MC, we compute the return every time the state is visited in the episode. </p>
    <p class="normal">Following this, we explored how to perform a control task using the Monte Carlo method. We learned about two different types of control methods—on-policy and off-policy control.</p>
    <p class="normal">In the on-policy method, we generate episodes using one policy and also improve the same policy iteratively to find the optimal policy. We first learned about the Monte Carlo control exploring starts method where we set all the state-action pairs to a non-zero probability to ensure exploration. Later, we learned about Monte Carlo control with an epsilon-greedy policy where we select a random action (exploration) with probability epsilon, and with probability 1-epsilon we select the best action (exploitation).</p>
    <p class="normal">At the end of the chapter, we discussed the off-policy Monte Carlo control method where we use two different policies called the behavior policy, for generating the episode, and the target policy, for finding the optimal policy.</p>
    <h1 id="_idParaDest-133" class="title">Questions</h1>
    <p class="normal">Let's assess our knowledge of the Monte Carlo methods by answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">What is the Monte Carlo method?</li>
      <li class="numbered">Why is the Monte Carlo method preferred over dynamic programming?</li>
      <li class="numbered">How do prediction tasks differ from control tasks?</li>
      <li class="numbered">How does the MC prediction method predict the value function?</li>
      <li class="numbered">What is the difference between first-visit MC and every-visit MC?</li>
      <li class="numbered">Why do we use incremental mean updates?</li>
      <li class="numbered">How does on-policy control differ from off-policy control?</li>
      <li class="numbered">What is the epsilon-greedy policy?</li>
    </ol>
  </div>
</body></html>