<html><head></head><body>
		<div id="_idContainer035">
			<h1 id="_idParaDest-92"><em class="italic"><a id="_idTextAnchor099"/>Chapter 3</em>: Harnessing the Power of Pre-Trained Networks with Transfer Learning</h1>
			<p>Despite the undeniable power deep neural networks bring to computer vision, they are very complex to tune, train, and make performant. This difficulty comes from three main sources: </p>
			<ul>
				<li>Deep neural networks start to pay off when we have sufficient data, but more often than not, this is not the case. Furthermore, data is expensive and, sometimes, impossible to expand.</li>
				<li>Deep neural networks contain a wide range of parameters that need tuning and can affect the overall performance of the model.</li>
				<li>Deep learning is very resource-intensive in terms of time, hardware, and effort.</li>
			</ul>
			<p>Do not be dismayed! With <strong class="bold">transfer learning</strong>, we can save ourselves loads of time and effort by leveraging the rich amount of knowledge present in seminal architectures that have been pre-trained on gargantuan datasets, such as ImageNet. And the best part? Besides being such a powerful and useful tool, transfer learning is also easy to apply. We'll learn how to do this in this chapter. </p>
			<p>In this chapter, we are going to cover the following recipes:</p>
			<ul>
				<li>Implementing a feature extractor using a pre-trained network</li>
				<li><a id="_idTextAnchor100"/><a id="_idTextAnchor101"/>Training a simple classifier on extracted features</li>
				<li>Spot-checking extractors and classifiers</li>
				<li>Using incremental learning to train a classifier</li>
				<li><a id="_idTextAnchor102"/><a id="_idTextAnchor103"/>Fine-tuning a network using the Keras API</li>
				<li>Fine-tuning a network using TFHub</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor104"/>Technical requirements</h1>
			<p>It's highly encouraged that you have access to a GPU since transfer learning tends to be quite computationally heavy. In the <em class="italic">Getting ready</em> section of each recipe, you'll receive specific instructions – if they're needed – on how to install the dependencies for that recipe. You can find all the code for this chapter here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/39wR6DT">https://bit.ly/39wR6DT</a>.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor105"/><a id="_idTextAnchor106"/>Implementing a feature extractor using a pre-trained network</h1>
			<p>One of the <a id="_idIndexMarker186"/>easiest ways to seize <a id="_idIndexMarker187"/>the power of transfer learning is to use pre-trained models as feature extractors. This way, we can combine both deep learning and machine learning, something that we normally cannot do, because traditional machine learning algorithms don't work with raw images. In this recipe, we'll implement a reusable <strong class="source-inline">FeatureExtractor</strong> class to produce a dataset of vectors from a set of input images, and then save it in the blazingly fast HDF5 format. </p>
			<p>Are you ready? Let's get started!</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor107"/>Getting ready</h2>
			<p>You'll need to install <strong class="source-inline">Pillow</strong> and <strong class="source-inline">tqdm</strong> (which we'll use to display a nice progress bar). Fortunately, this is very easy with <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">$&gt; pip install Pillow tqdm</p>
			<p>We'll be using th<a id="_idTextAnchor108"/><a id="_idTextAnchor109"/>e <strong class="source-inline">Stanford Cars</strong> dataset, which you can download here: <a href="http://imagenet.stanford.edu/internal/car196/car_ims.tgz">http://imagenet.stanford.edu/internal/car196/car_ims.tgz</a>. Decompress the data to a location of your preference. In this recipe, we assume the data is inside the <strong class="source-inline">~/.keras/datasets</strong> directory, under the name <strong class="source-inline">car_ims</strong>.</p>
			<p>Here are some sample images from the dataset:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B14768_03_001.jpg" alt="Figure 3.1 – Sample images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1 – Sample images</p>
			<p>We'll store the extracted features in HDF5 format, a binary, hierarchical protocol designed to <a id="_idIndexMarker188"/>store very large<a id="_idIndexMarker189"/> numerical datasets on disk, while keeping ease of access and computation on a row-wise level. You can read more about HDF5 here: <a href="https://portal.hdfgroup.org/display/HDF5/HDF5">https://portal.hdfgroup.org/display/HDF5/HDF5</a>.</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor110"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li>Import all the necessary packages:<p class="source-code">import glob</p><p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">import h5py</p><p class="source-code">import numpy as np</p><p class="source-code">import sklearn.utils as skutils</p><p class="source-code">from sklearn.preprocessing import LabelEncoder</p><p class="source-code">from tensorflow.keras.applications import imagenet_utils</p><p class="source-code">from tensorflow.keras.applications.vgg16 import VGG16</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p><p class="source-code">from tqdm import tqdm</p></li>
				<li>Define the <strong class="source-inline">FeatureExtractor</strong> class and its constructor:<p class="source-code">class FeatureExtractor(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 model,</p><p class="source-code">                 input_size,</p><p class="source-code">                 label_encoder,</p><p class="source-code">                 num_instances,</p><p class="source-code">                 feature_size,</p><p class="source-code">                 output_path,</p><p class="source-code">                 features_key='features',</p><p class="source-code">                 buffer_size=1000):</p></li>
				<li>We need <a id="_idIndexMarker190"/>to make<a id="_idIndexMarker191"/> sure the output path can be written:<p class="source-code">        if os.path.exists(output_path):</p><p class="source-code">            error_msg = (f'{output_path} already </p><p class="source-code">                           exists. '</p><p class="source-code">                         f'Please delete it and try </p><p class="source-code">                          again.')</p><p class="source-code">            raise FileExistsError(error_msg)</p></li>
				<li>Now, let's store the input parameter as object members:<p class="source-code">        self.model = model</p><p class="source-code">        self.input_size = input_size</p><p class="source-code">        self.le = label_encoder</p><p class="source-code">        self.feature_size = feature_size</p><p class="source-code">        self.buffer_size = buffer_size</p><p class="source-code">        self.buffer = {'features': [], 'labels': []}</p><p class="source-code">        self.current_index = 0</p></li>
				<li><strong class="source-inline">self.buffer</strong> will contain a buffer of both instances and labels, while <strong class="source-inline">self.current_index</strong> will point to the next free location within the datasets <a id="_idIndexMarker192"/>in the inner <a id="_idIndexMarker193"/>HDF5 database. We'll create this now:<p class="source-code">        self.db = h5py.File(output_path, 'w')</p><p class="source-code">        self.features = self.db.create_dataset(features_      key,</p><p class="source-code">                                       (num_instances,</p><p class="source-code">                                         feature_size),</p><p class="source-code">                                         dtype='float')</p><p class="source-code">       </p><p class="source-code">self.labels = self.db.create_dataset('labels',</p><p class="source-code">                                  (num_instances,),</p><p class="source-code">                                  dtype='int')</p></li>
				<li>Define a method that will extract features and labels from a list of image paths and store them in the <strong class="source-inline">HDF5</strong> database:<p class="source-code">    def extract_features(self,</p><p class="source-code">                       image_paths,</p><p class="source-code">                       labels,</p><p class="source-code">                       batch_size=64,</p><p class="source-code">                       shuffle=True):</p><p class="source-code">        if shuffle:</p><p class="source-code">            image_paths, labels = </p><p class="source-code">            skutils.shuffle(image_paths,</p><p class="source-code">                           labels)</p><p class="source-code">        encoded_labels = self.le.fit_transform(labels)</p><p class="source-code">        self._store_class_labels(self.le.classes_)</p></li>
				<li>After<a id="_idIndexMarker194"/> shuffling the image <a id="_idIndexMarker195"/>paths and their labels, as well as encoding and storing the latter, we'll iterate over batches of images, passing them through the pre-trained network. Once we've done this, we'll save the resulting features into the HDF5 database (the helper methods we've used here will be defined shortly):<p class="source-code">        for i in tqdm(range(0, len(image_paths), </p><p class="source-code">                            batch_size)):</p><p class="source-code">            batch_paths = image_paths[i: i + </p><p class="source-code">                                      batch_size]</p><p class="source-code">            batch_labels = encoded_labels[i:i + </p><p class="source-code">                                         batch_size]</p><p class="source-code">            batch_images = []</p><p class="source-code">            for image_path in batch_paths:</p><p class="source-code">                image = load_img(image_path,</p><p class="source-code">                                 </p><p class="source-code">                        target_size=self.input_size)</p><p class="source-code">                image = img_to_array(image)</p><p class="source-code">                image = np.expand_dims(image, axis=0)</p><p class="source-code">                image = </p><p class="source-code">               imagenet_utils.preprocess_input(image)</p><p class="source-code">                batch_images.append(image)</p><p class="source-code">            batch_images = np.vstack(batch_images)</p><p class="source-code">            feats = self.model.predict(batch_images,</p><p class="source-code">                                batch_size=batch_size)</p><p class="source-code">            new_shape = (feats.shape[0], </p><p class="source-code">                        self.feature_size)</p><p class="source-code">            feats = feats.reshape(new_shape)</p><p class="source-code">            self._add(feats, batch_labels)</p><p class="source-code">        self._close()</p></li>
				<li>Define a <a id="_idIndexMarker196"/>private <a id="_idIndexMarker197"/>method that will add features and labels to the corresponding datasets:<p class="source-code">    def _add(self, rows, labels):</p><p class="source-code">        self.buffer['features'].extend(rows)</p><p class="source-code">        self.buffer['labels'].extend(labels)</p><p class="source-code">        if len(self.buffer['features']) &gt;= </p><p class="source-code">                               self.buffer_size:</p><p class="source-code">            self._flush()</p></li>
				<li>Define a private method that will flush the buffers to disk:<p class="source-code">    def _flush(self):</p><p class="source-code">        next_index = (self.current_index +</p><p class="source-code">                      len(self.buffer['features']))</p><p class="source-code">        buffer_slice = slice(self.current_index, </p><p class="source-code">                            next_index)</p><p class="source-code">        self.features[buffer_slice] = </p><p class="source-code">                       self.buffer['features']</p><p class="source-code">        self.labels[buffer_slice] = self.buffer['labels']</p><p class="source-code">        self.current_index = next_index</p><p class="source-code">        self.buffer = {'features': [], 'labels': []}</p></li>
				<li>Define a <a id="_idIndexMarker198"/>private <a id="_idIndexMarker199"/>method that will store the class labels in the HDF5 database:<p class="source-code">    def _store_class_labels(self, class_labels):</p><p class="source-code">        data_type = h5py.special_dtype(vlen=str)</p><p class="source-code">        shape = (len(class_labels),)</p><p class="source-code">        label_ds = self.db.create_dataset('label_names',</p><p class="source-code">                      shape,</p><p class="source-code">                    dtype=data_type)</p><p class="source-code">        label_ds[:] = class_labels</p></li>
				<li>Define a private method that will close the HDF5 dataset:<p class="source-code">    def _close(self):</p><p class="source-code">        if len(self.buffer['features']) &gt; 0:</p><p class="source-code">            self._flush()</p><p class="source-code">        self.db.close()</p></li>
				<li>Load the <a id="_idIndexMarker200"/>paths to<a id="_idIndexMarker201"/> the images in the dataset:<p class="source-code">files_pattern = (pathlib.Path.home() / '.keras' / </p><p class="source-code">                'datasets' /'car_ims' / '*.jpg')</p><p class="source-code">files_pattern = str(files_pattern)</p><p class="source-code">input_paths = [*glob.glob(files_pattern)]</p></li>
				<li>Create the output directory. We'll create a dataset of rotated car images so that a potential classifier can learn how to correctly revert the photos back to their original orientation, by correctly predicting the rotation angle:<p class="source-code">output_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">              'datasets' /</p><p class="source-code">               'car_ims_rotated')</p><p class="source-code">if not os.path.exists(str(output_path)):</p><p class="source-code">    os.mkdir(str(output_path))</p></li>
				<li>Create a copy of the dataset with random rotations performed on the images:<p class="source-code">labels = []</p><p class="source-code">output_paths = []</p><p class="source-code">for index in tqdm(range(len(input_paths))):</p><p class="source-code">    image_path = input_paths[index]</p><p class="source-code">    image = load_img(image_path)</p><p class="source-code">    rotation_angle = np.random.choice([0, 90, 180, 270])</p><p class="source-code">    rotated_image = image.rotate(rotation_angle)</p><p class="source-code">    rotated_image_path = str(output_path / </p><p class="source-code">                          f'{index}.jpg')</p><p class="source-code">    rotated_image.save(rotated_image_path, 'JPEG')</p><p class="source-code">    output_paths.append(rotated_image_path)</p><p class="source-code">    labels.append(rotation_angle)</p><p class="source-code">    image.close()</p><p class="source-code">    rotated_image.close()</p></li>
				<li>Instantiate <a id="_idIndexMarker202"/> <strong class="source-inline">FeatureExtractor</strong> while<a id="_idIndexMarker203"/> using a pre-trained <strong class="source-inline">VGG16</strong> network to extract features from the images in the dataset:<p class="source-code">features_path = str(output_path / 'features.hdf5')</p><p class="source-code">model = VGG16(weights='imagenet', include_top=False)</p><p class="source-code">fe = FeatureExtractor(model=model,</p><p class="source-code">                      input_size=(224, 224, 3),</p><p class="source-code">                      label_encoder=LabelEncoder(),</p><p class="source-code">                      num_instances=len(input_paths),</p><p class="source-code">                      feature_size=512 * 7 * 7,</p><p class="source-code">                      output_path=features_path)</p></li>
				<li>Extract the features and labels:<p class="source-code">fe.extract_features(image_paths=output_paths, </p><p class="source-code">                    labels=labels)</p></li>
			</ol>
			<p>After<a id="_idIndexMarker204"/> several minutes, there<a id="_idIndexMarker205"/> should be a file named <strong class="source-inline">features.hdf5</strong> in <strong class="source-inline">~/.keras/datasets/car_ims_rotated</strong>.</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor111"/>How it works…</h2>
			<p>In this recipe, we implemented a reusable component in order to use pre-trained networks on ImageNet, such as <strong class="bold">VGG16</strong> and <strong class="bold">ResNet</strong>, as feature extractors. This is a great way to <a id="_idIndexMarker206"/>harness<a id="_idIndexMarker207"/> the knowledge encoded in these models, since it allows us to utilize the resulting high-quality vectors to train traditional machine learning models such as <strong class="source-inline">Logistic Regression</strong> and <strong class="source-inline">Support Vector Machines</strong>. </p>
			<p>Because image datasets tend to be too big to fit in memory, we resorted to the high-performance, user-friendly HDF5 format, which is perfect for storing large numeric data on disk, while also keeping the ease of access that's typical of <strong class="source-inline">NumPy</strong>. This means we can interact with HDF5 datasets <em class="italic">as if they were</em> regular <strong class="source-inline">NumPy</strong> arrays, making them compatible with the whole <strong class="source-inline">SciPy</strong> ecosystem. </p>
			<p>The result of <strong class="source-inline">FeatureExtractor</strong> is a hierarchical HDF5 file (think of it as a folder in a filesystem) containing three datasets: <strong class="source-inline">features</strong>, which contains the feature vectors, <strong class="source-inline">labels</strong>, which stores the encoded labels, and <strong class="source-inline">label_names</strong>, which holds the human-readable labels prior to encoding.</p>
			<p>Finally, we used <strong class="source-inline">FeatureExtractor</strong> to create a binary representation of a dataset of car images rotated 0º, 90º, 180º, or 270º. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We'll use the modified version of the <strong class="source-inline">Stanford Cars</strong> dataset we just worked on in future recipes in this chapter.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor112"/>See also</h2>
			<p>For more information on the <strong class="source-inline">Stanford Cars</strong> dataset, you can visit the official page here: <a href="https://ai.stanford.edu/~jkrause/cars/car_dataset.html">https://ai.stanford.edu/~jkrause/cars/car_dataset.html</a>. To learn more about HDF5, head to the official HDF Group website: <a href="https://www.hdfgroup.org/">https://www.hdfgroup.org/</a>.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor113"/>Training a simple classifier on extracted features</h1>
			<p>Machine<a id="_idIndexMarker208"/> learning algorithms are not properly<a id="_idIndexMarker209"/> equipped to work with tensors, which forbid them from learning directly from images. However, by using pre-trained networks as feature extractors, we close this gap, enabling <a id="_idIndexMarker210"/>us to access the power of widely popular, battle-tested <a id="_idIndexMarker211"/>algorithms <a id="_idIndexMarker212"/>such as <strong class="bold">Logistic Regression</strong>, <strong class="bold">Decision Trees,</strong> and <strong class="bold">Support Vector Machines</strong>. </p>
			<p>In this recipe, we'll use the features we generated in the previous recipe (in HDF5 format) to train an image orientation detector to correct the degrees of rotation of a picture, to restore its original state.</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor114"/>Getting ready</h2>
			<p>As we mentioned in the introduction to this reipce, we'll use the <strong class="source-inline">features.hdf5</strong> dataset we generated in the previous recipe, which contains encoded information about rotated images from the <strong class="source-inline">Stanford Cars</strong> dataset. We assume the dataset is in the following location: <strong class="source-inline">~/.keras/datasets/car_ims_rotated/features.hdf5</strong>. </p>
			<p>Here are some rotated samples:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B14768_03_002.jpg" alt="Figure 3.2 – Example of a car rotated 180º (left), and another rotated 90º (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – Example of a car rotated 180º (left), and another rotated 90º (right)</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-101"><a id="_idTextAnchor115"/>How to do it…</h2>
			<p>Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import the required packages:<p class="source-code">import pathlib</p><p class="source-code">import h5py</p><p class="source-code">from sklearn.linear_model import LogisticRegressionCV</p><p class="source-code">from sklearn.metrics import classification_report</p></li>
				<li>Load<a id="_idIndexMarker213"/> the<a id="_idIndexMarker214"/> dataset in HDF5 format:<p class="source-code">dataset_path = str(pathlib.Path.home()/'.keras'/'datasets'/'car_ims_rotated'/'features.hdf5')</p><p class="source-code">db = h5py.File(dataset_path, 'r')</p></li>
				<li>Because the dataset is too big, we'll only work with 50% of the data. The following block splits both the features and labels in half:<p class="source-code">SUBSET_INDEX = int(db['labels'].shape[0] * 0.5)</p><p class="source-code">features = db['features'][:SUBSET_INDEX]</p><p class="source-code">labels = db['labels'][:SUBSET_INDEX]</p></li>
				<li>Take the first 80% of the data to train the model, and the remaining 20% to evaluate it later on:<p class="source-code">TRAIN_PROPORTION = 0.8</p><p class="source-code">SPLIT_INDEX = int(len(labels) * TRAIN_PROPORTION)</p><p class="source-code">X_train, y_train = (features[:SPLIT_INDEX],</p><p class="source-code">                    labels[:SPLIT_INDEX])</p><p class="source-code">X_test, y_test = (features[SPLIT_INDEX:],</p><p class="source-code">                  labels[SPLIT_INDEX:])</p></li>
				<li>Train a cross-validated <strong class="bold">Logistic Regression</strong> model on the training set. <strong class="source-inline">LogisticRegressionCV</strong> will find the best <strong class="source-inline">C</strong> parameter using cross-validation:<p class="source-code">model = LogisticRegressionCV(n_jobs=-1)</p><p class="source-code">model.fit(X_train, y_train)</p><p>Notice that <strong class="source-inline">n_jobs=-1</strong> means we'll use all available cores to find the best model <a id="_idIndexMarker215"/>in parallel. You can adjust this value <a id="_idIndexMarker216"/>based on the capacity of your hardware.</p></li>
				<li>Evaluate the model on the test set. We'll compute a classification report to get a fine-grained view of the model's performance:<p class="source-code">predictions = model.predict(X_test)</p><p class="source-code">report = classification_report(y_test, predictions,</p><p class="source-code">                       target_names=db['label_names'])</p><p class="source-code">print(report)</p><p>This prints the following report:</p><p class="source-code">              precision    recall  f1-score   support</p><p class="source-code">           0       1.00      1.00      1.00       404</p><p class="source-code">          90       0.98      0.99      0.99       373</p><p class="source-code">         180       0.99      1.00      1.00       409</p><p class="source-code">         270       1.00      0.98      0.99       433</p><p class="source-code">    accuracy                           0.99      1619</p><p class="source-code">   macro avg       0.99      0.99      0.99      1619</p><p class="source-code">weighted avg       0.99      0.99      0.99      1619</p><p>The model does a good job of discriminating between the four classes, achieving an overall accuracy of 99% on the test set!</p></li>
				<li>Finally, close the HDF5 file to free up any resources:<p class="source-code">db.close()                    </p></li>
			</ol>
			<p>We'll understand how this all works in the next section.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor116"/>How it works…</h2>
			<p>We just<a id="_idIndexMarker217"/> trained a very simple <strong class="bold">Logistic Regression</strong> model <a id="_idIndexMarker218"/>to detect the degree of rotation in an image. To achieve this, we leveraged the rich and expressive features we extracted using a pre-trained <strong class="bold">VGG16</strong> network on ImageNet (for a deeper explanation, refer to the first recipe of this chapter).</p>
			<p>Because this data is too<a id="_idIndexMarker219"/> big, and <strong class="bold">scikit-learn</strong>'s machine learning algorithms work with the full data in one go (more specifically, most of them cannot work in batches), we only used 50% of the features and labels, due to memory constraints.</p>
			<p>After a couple of minutes, we obtained an incredible performance of 99% on the test set. Moreover, by analyzing the classification report, we can see that the model is very confident in its predictions, achieving an F1 score of at least 0.99 in all four cases.</p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor117"/>See also</h2>
			<p>For more information on how to extract features from pre-trained networks, refer to the <em class="italic">Implementing a feature extractor using a pre-trained network</em> recipe in this chapter.</p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor118"/>Spot-checking extractors and classifiers </h1>
			<p>Often, when<a id="_idIndexMarker220"/> we are tackling a new project, we are victims of the<a id="_idIndexMarker221"/> Paradox of Choice: we don't know where or how to start due to the presence of so many options to choose from. Which feature extractor is the best? What's the most performant model we can train? How should we pre-process our data?</p>
			<p>In this recipe, we will implement a framework that will automatically spot-check feature extractors and classifiers. The goal is not to get the best possible model right away, but to narrow down our options so that we can focus on the most promising ones at a later stage.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor119"/>Getting ready</h2>
			<p>First, we must install <strong class="source-inline">Pillow</strong> and<a id="_idTextAnchor120"/><a id="_idTextAnchor121"/> <strong class="source-inline">tqdm</strong>:</p>
			<p class="source-code">$&gt; pip install Pillow tqdm</p>
			<p>We'll use a dataset called <strong class="source-inline">17 Category Flower Dataset</strong>, available here: <a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/17">http://www.robots.ox.ac.uk/~vgg/data/flowers/17</a>. However, a curated version, organized into subfolders per class, can be downloaded here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip</a>. Unzip it in a location of your preference. In this recipe, we assume the data is inside the <strong class="source-inline">~/.keras/datasets</strong> directory, under the name <strong class="source-inline">flowers17</strong>. </p>
			<p>Finally, we'll reuse the <strong class="source-inline">FeatureExtractor()</strong> class we defined in the <em class="italic">Implementing a feature extractor using a pre-trained network</em> recipe, at the start of this chapter. Refer to it if you want to learn more about it.</p>
			<p>The following are some example images from the dataset for this recipe, <strong class="source-inline">17 Category Flower Dataset</strong>:</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B14768_03_003.jpg" alt="Figure 3.3 – Example images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – Example images</p>
			<p>With the preparation out of the way, let's get to it!</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor122"/>How to do it…</h2>
			<p>The following steps will allow us to spot-check several combinations of feature extractors and machine learning algorithms. Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import<a id="_idIndexMarker222"/> the<a id="_idIndexMarker223"/> necessary packages:<p class="source-code">import json</p><p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import h5py</p><p class="source-code">from sklearn.ensemble import *</p><p class="source-code">from sklearn.linear_model import *</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">from sklearn.neighbors import KNeighborsClassifier</p><p class="source-code">from sklearn.preprocessing import LabelEncoder</p><p class="source-code">from sklearn.svm import LinearSVC</p><p class="source-code">from sklearn.tree import *</p><p class="source-code">from tensorflow.keras.applications import *</p><p class="source-code">from tqdm import tqdm</p><p class="source-code">from ch3.recipe1.feature_extractor import FeatureExtractor</p></li>
				<li>Define the input size of all the feature extractors:<p class="source-code">INPUT_SIZE = (224, 224, 3)</p></li>
				<li>Define a function that will obtain a list of tuples of pre-trained networks, along with the<a id="_idIndexMarker224"/> dimensionality of the vectors they<a id="_idIndexMarker225"/> output:<p class="source-code">def get_pretrained_networks():</p><p class="source-code">    return [</p><p class="source-code">        (VGG16(input_shape=INPUT_SIZE,</p><p class="source-code">               weights='imagenet',</p><p class="source-code">               include_top=False),</p><p class="source-code">         7 * 7 * 512),</p><p class="source-code">        (VGG19(input_shape=INPUT_SIZE,</p><p class="source-code">               weights='imagenet',</p><p class="source-code">               include_top=False),</p><p class="source-code">         7 * 7 * 512),</p><p class="source-code">        (Xception(input_shape=INPUT_SIZE,</p><p class="source-code">                  weights='imagenet',</p><p class="source-code">                  include_top=False),</p><p class="source-code">         7 * 7 * 2048),</p><p class="source-code">        (ResNet152V2(input_shape=INPUT_SIZE,</p><p class="source-code">                     weights='imagenet',</p><p class="source-code">                     include_top=False),</p><p class="source-code">         7 * 7 * 2048),</p><p class="source-code">        (InceptionResNetV2(input_shape=INPUT_SIZE,</p><p class="source-code">                           weights='imagenet',</p><p class="source-code">                           include_top=False),</p><p class="source-code">         5 * 5 * 1536)</p><p class="source-code">    ]</p></li>
				<li>Define a <a id="_idIndexMarker226"/>function that returns a <strong class="source-inline">dict</strong> of machine <a id="_idIndexMarker227"/>learning models to spot-check:<p class="source-code">def get_classifiers():</p><p class="source-code">    models = {}</p><p class="source-code">    models['LogisticRegression'] = </p><p class="source-code">                            LogisticRegression()</p><p class="source-code">    models['SGDClf'] = SGDClassifier()</p><p class="source-code">    models['PAClf'] = PassiveAggressiveClassifier()</p><p class="source-code">    models['DecisionTreeClf'] = </p><p class="source-code">                         DecisionTreeClassifier()</p><p class="source-code">    models['ExtraTreeClf'] = ExtraTreeClassifier()</p><p class="source-code">    n_trees = 100</p><p class="source-code">    models[f'AdaBoostClf-{n_trees}'] = \</p><p class="source-code">        AdaBoostClassifier(n_estimators=n_trees)</p><p class="source-code">    models[f'BaggingClf-{n_trees}'] = \</p><p class="source-code">        BaggingClassifier(n_estimators=n_trees)</p><p class="source-code">    models[f'RandomForestClf-{n_trees}'] = \</p><p class="source-code">        RandomForestClassifier(n_estimators=n_trees)</p><p class="source-code">    models[f'ExtraTreesClf-{n_trees}'] = \</p><p class="source-code">        ExtraTreesClassifier(n_estimators=n_trees)</p><p class="source-code">    models[f'GradientBoostingClf-{n_trees}'] = \</p><p class="source-code">        GradientBoostingClassifier(n_estimators=n_trees)</p><p class="source-code">    number_of_neighbors = range(3, 25)</p><p class="source-code">    for n in number_of_neighbors:</p><p class="source-code">        models[f'KNeighborsClf-{n}'] = \</p><p class="source-code">            KNeighborsClassifier(n_neighbors=n)</p><p class="source-code">    reg = [1e-3, 1e-2, 1, 10]</p><p class="source-code">    for r in reg:</p><p class="source-code">        models[f'LinearSVC-{r}'] = LinearSVC(C=r)</p><p class="source-code">        models[f'RidgeClf-{r}'] = </p><p class="source-code">              RidgeClassifier(alpha=r)</p><p class="source-code">    print(f'Defined {len(models)} models.')</p><p class="source-code">    return models</p></li>
				<li>Define the <a id="_idIndexMarker228"/>path to the dataset, as well as a list of<a id="_idIndexMarker229"/> all image paths:<p class="source-code">dataset_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">               'datasets' 'flowers17')</p><p class="source-code">files_pattern = (dataset_path / 'images' / '*' / '*.jpg')</p><p class="source-code">images_path = [*glob(str(files_pattern))]</p></li>
				<li>Load the labels into memory:<p class="source-code">labels = []</p><p class="source-code">for index in tqdm(range(len(images_path))):</p><p class="source-code">    image_path = images_path[index]</p><p class="source-code">    label = image_path.split(os.path.sep)[-2]</p><p class="source-code">    labels.append(label)</p></li>
				<li>Define some variables in order to keep track of the spot-checking process. <strong class="source-inline">final_report</strong> will contain the accuracy of each classifier, trained on the features produced by different pre-trained networks. <strong class="source-inline">best_model</strong>, <strong class="source-inline">best_accuracy</strong>, and <strong class="source-inline">best_features</strong> will contain the name of the best model, its accuracy, and the name of the pre-trained network that produced the features, respectively:<p class="source-code">final_report = {}</p><p class="source-code">best_model = None</p><p class="source-code">best_accuracy = -1</p><p class="source-code">best_features = None</p></li>
				<li>Iterate over<a id="_idIndexMarker230"/> each pre-trained network, using it to<a id="_idIndexMarker231"/> extract features from the images in the dataset:<p class="source-code">for model, feature_size in get_pretrained_networks():</p><p class="source-code">    output_path = dataset_path / f'{model.name}_features.hdf5'</p><p class="source-code">    output_path = str(output_path)</p><p class="source-code">    fe = FeatureExtractor(model=model,</p><p class="source-code">                          input_size=INPUT_SIZE,</p><p class="source-code">                          label_encoder=LabelEncoder(),</p><p class="source-code">                          num_instances=len(images_path),</p><p class="source-code">                          feature_size=feature_size,</p><p class="source-code">                          output_path=output_path)</p><p class="source-code">    fe.extract_features(image_paths=images_path,</p><p class="source-code">                        labels=labels)</p></li>
				<li>Take 80% of the data to train, and 20% to test:<p class="source-code">    db = h5py.File(output_path, 'r')</p><p class="source-code">    TRAIN_PROPORTION = 0.8</p><p class="source-code">    SPLIT_INDEX = int(len(labels) * TRAIN_PROPORTION)</p><p class="source-code">    X_train, y_train = (db['features'][:SPLIT_INDEX],</p><p class="source-code">                        db['labels'][:SPLIT_INDEX])</p><p class="source-code">    X_test, y_test = (db['features'][SPLIT_INDEX:],</p><p class="source-code">                      db['labels'][SPLIT_INDEX:])</p><p class="source-code">    classifiers_report = {</p><p class="source-code">        'extractor': model.name</p><p class="source-code">    }</p><p class="source-code">    print(f'Spot-checking with features from </p><p class="source-code">          {model.name}')</p></li>
				<li>Using the<a id="_idIndexMarker232"/> extracted features in the current<a id="_idIndexMarker233"/> iteration, go over all the machine learning models, training them on the training set and evaluating them on the test set:<p class="source-code">    for clf_name, clf in get_classifiers().items():</p><p class="source-code">        try:</p><p class="source-code">            clf.fit(X_train, y_train)</p><p class="source-code">        except Exception as e:</p><p class="source-code">            print(f'\t{clf_name}: {e}')</p><p class="source-code">            continue</p><p class="source-code">        predictions = clf.predict(X_test)</p><p class="source-code">        accuracy = accuracy_score(y_test, predictions)</p><p class="source-code">        print(f'\t{clf_name}: {accuracy}')</p><p class="source-code">        classifiers_report[clf_name] = accuracy</p></li>
				<li>Check if we have a new best model. If that's the case, update the proper variables:<p class="source-code">        if accuracy &gt; best_accuracy:</p><p class="source-code">            best_accuracy = accuracy</p><p class="source-code">            best_model = clf_name</p><p class="source-code">            best_features = model.name</p></li>
				<li>Store the <a id="_idIndexMarker234"/>results of this iteration in <strong class="source-inline">final_report</strong> and<a id="_idIndexMarker235"/> free the resources of the HDF5 file:<p class="source-code">    final_report[output_path] = classifiers_report</p><p class="source-code">    db.close()</p></li>
				<li>Update <strong class="source-inline">final_report</strong> with the information of the best model. Finally, write it to disk:<p class="source-code">final_report['best_model'] = best_model</p><p class="source-code">final_report['best_accuracy'] = best_accuracy</p><p class="source-code">final_report['best_features'] = best_features</p><p class="source-code">with open('final_report.json', 'w') as f:</p><p class="source-code">    json.dump(final_report, f)</p></li>
			</ol>
			<p>Examining the <strong class="source-inline">final_report.json</strong> file, we can see that the best model is a <strong class="source-inline">PAClf</strong> (<strong class="source-inline">PassiveAggressiveClassifier</strong>), which achieved an accuracy of 0.934 (93.4%) on the test set and was trained on the features we extracted from a <strong class="bold">VGG19</strong> network. You can check the full output here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/final_report.json">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/final_report.json</a>. Let's head over to the next section to study the project we completed in this recipe in more detail.</p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor123"/>How it works…</h2>
			<p>In this recipe, we developed a framework that automatically enabled us to spot-check 40 different machine learning algorithms by using the features produced by five different<a id="_idIndexMarker236"/> pre-trained networks, resulting in 200 experiments. Leveraging<a id="_idIndexMarker237"/> the results of this approach, we found that the best model combination for this particular problem was a <strong class="source-inline">PassiveAggressiveClassifier</strong> trained on vectors produced by a <strong class="bold">VGG19</strong> network. </p>
			<p>Notice that we did not focus on achieving maximal performance, but rather on making an educated decision, based on hard evidence, on where to spend our time and resources if we were to optimize a classifier on this dataset. Now, we know that fine-tuning a <strong class="bold">Passive Aggressive Classifier</strong> will, most likely, pay off. How long would it have taken <a id="_idIndexMarker238"/>us to arrive at this conclusion? Hours or maybe days. </p>
			<p>The power of letting the computer do the heavy lifting is that we don't have to guess and, at the same time, are free to spend our time on other tasks. It's great, isn't it?</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor124"/>Using incremental learning to train a classifier</h1>
			<p>One of the <a id="_idIndexMarker239"/>problems of traditional machine<a id="_idIndexMarker240"/> learning <a id="_idIndexMarker241"/>libraries, such as <strong class="bold">scikit-learn</strong>, is that they seldom offer the possibility to train models on high volumes of data, which, coincidentally, is the best type of data for deep neural networks. What good is having large amounts of data if we can't use it?</p>
			<p>Fortunately, there is a way to circumvent this limitation, and it's called <strong class="bold">incremental learning</strong>. In this <a id="_idIndexMarker242"/>recipe, we'll use a powerful library, <strong class="source-inline">creme</strong>, to train a classifier on a dataset too big to fit in memory. </p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor125"/>Getting ready</h2>
			<p>In this recipe, we'll leverage <strong class="source-inline">creme</strong>, an experimental library specifically designed to train machine learning models on huge datasets that are too big to fit in memory. To install <strong class="source-inline">creme</strong>, execute the following command:</p>
			<p class="source-code">$&gt; pip install creme==0.5.1</p>
			<p>We'll use the <strong class="source-inline">features.hdf5</strong> dataset we generated in the <em class="italic">Implementing a feature extractor using a pre-trained network</em> recipe in this chapter, which contains encoded information about rotated images from the <strong class="source-inline">Stanford Cars</strong> dataset. We assume the dataset is in the following location: <strong class="source-inline">~/.keras/datasets/car_ims_rotated/features.hdf5</strong>. </p>
			<p>The following <a id="_idIndexMarker243"/>are some sample images from <a id="_idIndexMarker244"/>this dataset:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B14768_03_004.jpg" alt="Figure 3.4 – Example of a car rotated 90º (left), and another rotated 0º (right)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4 – Example of a car rotated 90º (left), and another rotated 0º (right)</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor126"/>How to do it…</h2>
			<p>The following steps will guide us through how to incrementally train a classifier on big data:</p>
			<ol>
				<li value="1">Import all the necessary packages:<p class="source-code">import pathlib</p><p class="source-code">import h5py</p><p class="source-code">from creme import stream</p><p class="source-code">from creme.linear_model import LogisticRegression</p><p class="source-code">from creme.metrics import Accuracy</p><p class="source-code">from creme.multiclass import OneVsRestClassifier</p><p class="source-code">from creme.preprocessing import StandardScaler</p></li>
				<li>Define<a id="_idIndexMarker245"/> a function that will save a <a id="_idIndexMarker246"/>dataset as a CSV file:<p class="source-code">def write_dataset(output_path, feats, labels, </p><p class="source-code">                  batch_size):</p><p class="source-code">    feature_size = feats.shape[1]</p><p class="source-code">    csv_columns = ['class'] + [f'feature_{i}'</p><p class="source-code">                               for i in range(feature_        size)]</p></li>
				<li>We'll have one column for the class of each feature, and as many columns of elements in each feature vector. Next, let's write the contents of the CSV file in batches, starting with the header:<p class="source-code">    dataset_size = labels.shape[0]</p><p class="source-code">    with open(output_path, 'w') as f:</p><p class="source-code">        f.write(f'{“,”.join(csv_columns)}\n')</p></li>
				<li>Extract the batch in this iteration:<p class="source-code">        for batch_number, index in \</p><p class="source-code">                enumerate(range(0, dataset_size, </p><p class="source-code">                          batch_size)):</p><p class="source-code">            print(f'Processing batch {batch_number + </p><p class="source-code">                                      1} of '</p><p class="source-code">                  f'{int(dataset_size / </p><p class="source-code">                  float(batch_size))}')</p><p class="source-code">            batch_feats = feats[index: index + </p><p class="source-code">                                 batch_size]</p><p class="source-code">            batch_labels = labels[index: index + </p><p class="source-code">                                  batch_size]</p></li>
				<li>Now, write all the rows in the batch:<p class="source-code">            for label, vector in \</p><p class="source-code">                    zip(batch_labels, batch_feats):</p><p class="source-code">                vector = ','.join([str(v) for v in </p><p class="source-code">                                   vector])</p><p class="source-code">                f.write(f'{label},{vector}\n')</p></li>
				<li>Load <a id="_idIndexMarker247"/>the<a id="_idIndexMarker248"/> dataset in HDF5 format:<p class="source-code">dataset_path = str(pathlib.Path.home()/'.keras'/'datasets'/'car_ims_rotated'/'features.hdf5')</p><p class="source-code">db = h5py.File(dataset_path, 'r')</p></li>
				<li>Define the split index to separate the data into training (80%) and test (20%) chunks:<p class="source-code">TRAIN_PROPORTION = 0.8</p><p class="source-code">SPLIT_INDEX = int(db['labels'].shape[0] * </p><p class="source-code">                  TRAIN_PROPORTION) </p></li>
				<li>Write the training and test subsets to disk as CSV files:<p class="source-code">BATCH_SIZE = 256</p><p class="source-code">write_dataset('train.csv',</p><p class="source-code">              db['features'][:SPLIT_INDEX],</p><p class="source-code">              db['labels'][:SPLIT_INDEX],</p><p class="source-code">              BATCH_SIZE)</p><p class="source-code">write_dataset('test.csv',</p><p class="source-code">              db['features'][SPLIT_INDEX:],</p><p class="source-code">              db['labels'][SPLIT_INDEX:],</p><p class="source-code">              BATCH_SIZE)</p></li>
				<li><strong class="source-inline">creme</strong> requires us to specify the type of each column in the CSV file as a <strong class="source-inline">dict</strong>. instance The following<a id="_idIndexMarker249"/> block specifies<a id="_idIndexMarker250"/> that <strong class="source-inline">class</strong> should be encoded as <strong class="source-inline">int</strong>, while the remaining columns, corresponding to the features, should be of the <strong class="source-inline">float</strong> type:<p class="source-code">FEATURE_SIZE = db['features'].shape[1]</p><p class="source-code">types = {f'feature_{i}': float for i in range(FEATURE_SIZE)}</p><p class="source-code">types['class'] = int</p></li>
				<li>In the following code, we are defining a <strong class="source-inline">creme</strong> pipeline, where each input will be standardized prior to being passed to the classifier. Because this is a multi-class problem, we need to wrap <strong class="source-inline">LogisticRegression</strong> with <strong class="source-inline">OneVsRestClassifier</strong>:<p class="source-code">model = StandardScaler()</p><p class="source-code">model |= OneVsRestClassifier(LogisticRegression())</p></li>
				<li>Define <strong class="source-inline">Accuracy</strong> as the target metric and create an iterator over the <strong class="source-inline">train.csv</strong> dataset:<p class="source-code">metric = Accuracy()</p><p class="source-code">dataset = stream.iter_csv('train.csv',</p><p class="source-code">                          target_name='class',</p><p class="source-code">                          converters=types)</p></li>
				<li>Train the <a id="_idIndexMarker251"/>classifier, one example at <a id="_idIndexMarker252"/>a time. Print the running accuracy every 100 examples:<p class="source-code">print('Training started...')</p><p class="source-code">for i, (X, y) in enumerate(dataset):</p><p class="source-code">    predictions = model.predict_one(X)</p><p class="source-code">    model = model.fit_one(X, y)</p><p class="source-code">    metric = metric.update(y, predictions)</p><p class="source-code">    if i % 100 == 0:</p><p class="source-code">        print(f'Update {i} - {metric}')</p><p class="source-code">print(f'Final - {metric}')</p></li>
				<li>Create an iterator over the <strong class="source-inline">test.csv</strong> file:<p class="source-code">metric = Accuracy()</p><p class="source-code">test_dataset = stream.iter_csv('test.csv',</p><p class="source-code">                               target_name='class',</p><p class="source-code">                               converters=types)</p></li>
				<li>Evaluate the model on the test set once more, one sample at a time:<p class="source-code">print('Testing model...')</p><p class="source-code">for i, (X, y) in enumerate(test_dataset):</p><p class="source-code">    predictions = model.predict_one(X)</p><p class="source-code">    metric = metric.update(y, predictions)</p><p class="source-code">    if i % 1000 == 0:</p><p class="source-code">        print(f'(TEST) Update {i} - {metric}')</p><p class="source-code">print(f'(TEST) Final - {metric}')</p></li>
			</ol>
			<p>After several minutes, we should have a model with around 99% accuracy on the test set. We'll look at this in more detail in the next section.</p>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor127"/>How it works…</h2>
			<p>Often, even though we have massive amounts of data at our disposal, we are unable to use it all due to hardware or software limitations (in the <em class="italic">Training a simple classifier on extracted features</em> recipe, we had to use only 50%, because we couldn't keep it all in memory). However, with incremental learning (also known as online learning), we can train traditional<a id="_idIndexMarker253"/> machine learning models in <a id="_idIndexMarker254"/>batches, similar to what we can do with neural networks. </p>
			<p>In this recipe, in order to seize the totality of the feature vector from our <strong class="source-inline">Stanford Cars</strong> dataset, we had to write both the training and test sets into CSV files. Next, we trained <strong class="source-inline">LogisticRegression</strong> and wrapped it inside <strong class="source-inline">OneVsRestClassifier</strong>, which learned to detect the degrees of rotation in the feature vectors of the images. Finally, we achieved a very satisfying 99% accuracy on the test set. </p>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor128"/>Fine-tuning a network using the Keras API</h1>
			<p>Perhaps <a id="_idIndexMarker255"/>one of the greatest advantages of transfer <a id="_idIndexMarker256"/>learning is its ability to seize the tailwind produced by the knowledge encoded in pre-trained networks. By simply swapping the shallower layers in one of these networks, we can obtain remarkable performance on new, unrelated datasets, even if our data is small. Why? Because the information in the bottom layers is virtually universal: It encodes basic forms and shapes that apply to almost any computer vision problem. </p>
			<p>In this recipe, we'll fine-tune a pre-trained <strong class="bold">VGG16</strong> network on a tiny dataset, achieving an otherwise unlikely high accuracy score.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor129"/>Getting ready</h2>
			<p>We will need <strong class="source-inline">Pillow</strong> for this recipe. We can install it as follows:</p>
			<p class="source-code">$&gt; pip install Pillow</p>
			<p>We'll be using a dataset known as <strong class="source-inline">17 Category Flower Dataset</strong>, which is available here: <a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/17">http://www.robots.ox.ac.uk/~vgg/data/flowers/17</a>. A version of it that's been organized into subfolders per class can be found here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip</a>. Download and decompress it in a location of your choosing. From now on, we'll assume the data is in <strong class="source-inline">~/.keras/datasets/flowers17</strong>.</p>
			<p>The following are some sample images from this dataset:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B14768_03_005.jpg" alt="Figure 3.5 – Example images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5 – Example images</p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor130"/>How to do it…</h2>
			<p>Fine-tuning is easy! Follow these steps to complete this recipe:</p>
			<ol>
				<li value="1">Import<a id="_idIndexMarker257"/> the<a id="_idIndexMarker258"/> necessary dependencies:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Model</p><p class="source-code">from tensorflow.keras.applications import VGG16</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.optimizers import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Set the random seed:<p class="source-code">SEED = 999</p></li>
				<li>Define a function that will build a new network from a pre-trained model, where the top fully connected layers will be brand new and adapted to the problem at hand:<p class="source-code">def build_network(base_model, classes):</p><p class="source-code">    x = Flatten()(base_model.output)</p><p class="source-code">    x = Dense(units=256)(x)</p><p class="source-code">    x = ReLU()(x)</p><p class="source-code">    x = BatchNormalization(axis=-1)(x)</p><p class="source-code">    x = Dropout(rate=0.5)(x)</p><p class="source-code">    x = Dense(units=classes)(x)</p><p class="source-code">    output = Softmax()(x)</p><p class="source-code">    return output</p></li>
				<li>Define a<a id="_idIndexMarker259"/> function that will load the images<a id="_idIndexMarker260"/> and labels in the dataset as <strong class="source-inline">NumPy</strong> arrays:<p class="source-code">def load_images_and_labels(image_paths,</p><p class="source-code">                           target_size=(256, 256)):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, </p><p class="source-code">                         target_size=target_size)</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        label = image_path.split(os.path.sep)[-2]</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p></li>
				<li>Load the image paths and extract the set of classes from them:<p class="source-code">dataset_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">               'datasets' /'flowers17')</p><p class="source-code">files_pattern = (dataset_path / 'images' / '*' / '*.jpg')</p><p class="source-code">image_paths = [*glob(str(files_pattern))]</p><p class="source-code">CLASSES = {p.split(os.path.sep)[-2] for p in </p><p class="source-code">           image_paths}</p></li>
				<li>Load the <a id="_idIndexMarker261"/>images and normalize them, one-hot <a id="_idIndexMarker262"/>encode the labels with <strong class="source-inline">LabelBinarizer()</strong>, and split the data into subsets for training (80%) and testing (20%):<p class="source-code">X, y = load_images_and_labels(image_paths)</p><p class="source-code">X = X.astype('float') / 255.0</p><p class="source-code">y = LabelBinarizer().fit_transform(y)</p><p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y,</p><p class="source-code">                                     test_size=0.2,</p><p class="source-code">                                     </p><p class="source-code">                                  random_state=SEED)</p></li>
				<li>Instantiate a pre-trained <strong class="source-inline">VGG16</strong>, without the top layers. Specify an input shape of 256x256x3:<p class="source-code">base_model = VGG16(weights='imagenet',</p><p class="source-code">                   include_top=False,</p><p class="source-code">                   input_tensor=Input(shape=(256, 256, </p><p class="source-code">                                              3)))</p><p>Freeze all the layers in the base model. We are doing this because we don't want to re-train them, but use their existing knowledge:</p><p class="source-code">for layer in base_model.layers:</p><p class="source-code">    layer.trainable = False</p></li>
				<li>Build the full network with a new set of layers on top using <strong class="source-inline">build_network()</strong> (defined in <em class="italic">Step 3</em>):<p class="source-code">model = build_network(base_model, len(CLASSES))</p><p class="source-code">model = Model(base_model.input, model)</p></li>
				<li>Define <a id="_idIndexMarker263"/>the batch size and a set of augmentations <a id="_idIndexMarker264"/>to be applied through <strong class="source-inline">ImageDataGenerator()</strong>:<p class="source-code">BATCH_SIZE = 64</p><p class="source-code">augmenter = ImageDataGenerator(rotation_range=30,</p><p class="source-code">                               horizontal_flip=True,</p><p class="source-code">                               width_shift_range=0.1,</p><p class="source-code">                               height_shift_range=0.1,</p><p class="source-code">                               shear_range=0.2,</p><p class="source-code">                               zoom_range=0.2,</p><p class="source-code">                               fill_mode='nearest')</p><p class="source-code">train_generator = augmenter.flow(X_train, y_train, </p><p class="source-code">                                 BATCH_SIZE)</p></li>
				<li>Warm up the network. This means we'll only train the new layers (the rest are frozen) for 20<a id="_idIndexMarker265"/> epochs, using <strong class="bold">RMSProp</strong> with a learning rate of 0.001. Finally, we'll evaluate the network on the test set:<p class="source-code">WARMING_EPOCHS = 20</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer=RMSprop(lr=1e-3),</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">model.fit(train_generator,</p><p class="source-code">          steps_per_epoch=len(X_train) // BATCH_SIZE,</p><p class="source-code">          validation_data=(X_test, y_test),</p><p class="source-code">          epochs=WARMING_EPOCHS)</p><p class="source-code">result = model.evaluate(X_test, y_test)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p></li>
				<li>Now that the <a id="_idIndexMarker266"/>network has been warmed up, we'll <a id="_idIndexMarker267"/>fine-tune the final layers of the base model, specifically from the 16th onward (remember, zero-indexing), along with the fully connected layers, for 50<a id="_idIndexMarker268"/> epochs, using <strong class="bold">SGD</strong> with a learning rate of 0.001:<p class="source-code">for layer in base_model.layers[15:]:</p><p class="source-code">    layer.trainable = True</p><p class="source-code">EPOCHS = 50</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer=SGD(lr=1e-3),</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">model.fit(train_generator,</p><p class="source-code">          steps_per_epoch=len(X_train) // BATCH_SIZE,</p><p class="source-code">          validation_data=(X_test, y_test),</p><p class="source-code">          epochs=EPOCHS)</p><p class="source-code">result = model.evaluate(X_test, y_test)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p></li>
			</ol>
			<p>After warming up, the network achieved 81.6% accuracy on the test set. Then, when we fine-tuned it, after 50 epochs, the accuracy rose to 94.5% on the test set. We'll see how this all works in the next section.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor131"/>How it works…</h2>
			<p>We successfully harnessed the knowledge of a pre-trained <strong class="bold">VGG16</strong> on the massive ImageNet database. By replacing the top layers, which are fully connected and are in charge of the actual classification (the rest act as feature extractors), with our own set of deep layers suited to our problem, we managed to obtain a more than decent 94.5% accuracy on the test set. </p>
			<p>This result is a <a id="_idIndexMarker269"/>demonstration of the power of transfer<a id="_idIndexMarker270"/> learning, especially considering we only have 81 images per class in the dataset (81x17=1,377 in total), an insufficient amount for training a good performing deep learning model from scratch.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Although not always required, when fine-tuning networks, it is a good idea to first <em class="italic">warm up</em> the <em class="italic">head</em> (the fully connected layers at the top) to give them time to get accustomed to the features coming from the pre-trained networks. </p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor132"/>See also</h2>
			<p>You can read more about Keras pre-trained models here: https://www.tensorflow.org/api_docs/python/tf/keras/applications.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor133"/>Fine-tuning a network using TFHub</h1>
			<p>One of the <a id="_idIndexMarker271"/>easiest ways to fine-tune a<a id="_idIndexMarker272"/> network is to rely on the wealth of pre-trai<a id="_idTextAnchor134"/><a id="_idTextAnchor135"/>ned models that live in <strong class="bold">TensorFlow Hub</strong> (<strong class="bold">TFHub</strong>). In this recipe, we'll <a id="_idIndexMarker273"/>fine-tune a <strong class="bold">ResNetV1152</strong> feature extractor to classify flowers from a very small dataset.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor136"/>Getting ready</h2>
			<p>We will need <strong class="source-inline">tensorflow-hub</strong> and <strong class="source-inline">Pillow</strong> for this recipe. Both can be installed easily, like this:</p>
			<p class="source-code">$&gt; pip install tensorflow-hub Pillow</p>
			<p>We'll use a dataset known as <strong class="source-inline">17 Category Flower Dataset</strong>, which can be accessed at <a href="http://www.robots.ox.ac.uk/~vgg/data/flowers/17">http://www.robots.ox.ac.uk/~vgg/data/flowers/17</a>. I encourage you to get a re-organized copy of the data here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch3/recipe3/flowers17.zip</a>. Download and decompress it in a location of your choosing. From now on, we'll assume the data is in <strong class="source-inline">~/.keras/datasets/flowers17</strong>.</p>
			<p>The<a id="_idIndexMarker274"/> following are some sample images <a id="_idIndexMarker275"/>from this dataset:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B14768_03_006.jpg" alt="Figure 3.6 – Example images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6 – Example images</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor137"/>How to do it…</h2>
			<p>Follow these steps to successfully complete this recipe:</p>
			<ol>
				<li value="1">Import the required packages:<p class="source-code">import os</p><p class="source-code">import pathlib</p><p class="source-code">from glob import glob</p><p class="source-code">import numpy as np</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">from sklearn.preprocessing import LabelBinarizer</p><p class="source-code">from tensorflow.keras import Sequential</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.optimizers import RMSprop</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p><p class="source-code">from tensorflow_hub import KerasLayer</p></li>
				<li>Set<a id="_idIndexMarker276"/> the<a id="_idIndexMarker277"/> random seed:<p class="source-code">SEED = 999</p></li>
				<li>Define a function that will build a new network from a pre-trained model, where the top fully connected layer will be brand new and adapted to the number of categories in our data:<p class="source-code">def build_network(base_model, classes):</p><p class="source-code">    return Sequential([</p><p class="source-code">        base_model,</p><p class="source-code">        Dense(classes),</p><p class="source-code">        Softmax()</p><p class="source-code">    ])</p></li>
				<li>Define a function that will load the images and labels in the dataset as <strong class="source-inline">NumPy</strong> arrays:<p class="source-code">def load_images_and_labels(image_paths,</p><p class="source-code">                           target_size=(256, 256)):</p><p class="source-code">    images = []</p><p class="source-code">    labels = []</p><p class="source-code">    for image_path in image_paths:</p><p class="source-code">        image = load_img(image_path, </p><p class="source-code">                         target_size=target_size)</p><p class="source-code">        image = img_to_array(image)</p><p class="source-code">        label = image_path.split(os.path.sep)[-2]</p><p class="source-code">        images.append(image)</p><p class="source-code">        labels.append(label)</p><p class="source-code">    return np.array(images), np.array(labels)</p></li>
				<li>Load the<a id="_idIndexMarker278"/> image paths and<a id="_idIndexMarker279"/> extract the set of classes from them:<p class="source-code">dataset_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">                 'datasets' /'flowers17')</p><p class="source-code">files_pattern = (dataset_path / 'images' / '*' / '*.jpg')</p><p class="source-code">image_paths = [*glob(str(files_pattern))]</p><p class="source-code">CLASSES = {p.split(os.path.sep)[-2] for p in image_paths}</p></li>
				<li>Load the images and normalize them, one-hot encode the labels with <strong class="source-inline">LabelBinarizer()</strong>, and split the data into subsets for training (80%) and testing (20%):<p class="source-code">X, y = load_images_and_labels(image_paths)</p><p class="source-code">X = X.astype('float') / 255.0</p><p class="source-code">y = LabelBinarizer().fit_transform(y)</p><p class="source-code">(X_train, X_test,</p><p class="source-code"> y_train, y_test) = train_test_split(X, y,</p><p class="source-code">                                     test_size=0.2,</p><p class="source-code">                                   random_state=SEED)</p></li>
				<li>Instantiate <a id="_idIndexMarker280"/>a pre-trained <strong class="bold">ResNetV152</strong>, which <a id="_idIndexMarker281"/>we'll use as a feature extractor. We are passing the model's <strong class="bold">TFHub</strong> URL to the <strong class="source-inline">KerasLayer()</strong> class, indicating an input shape of 256x256x3:<p class="source-code">model_url = ('https://tfhub.dev/google/imagenet/'</p><p class="source-code">             'resnet_v1_152/feature_vector/4')</p><p class="source-code">base_model = KerasLayer(model_url, input_shape=(256, </p><p class="source-code">                                              256, 3))</p><p>Make the base model untrainable:</p><p class="source-code">base_model.trainable = False</p></li>
				<li>Build the full network while using the base model as a starting point:<p class="source-code">model = build_network(base_model, len(CLASSES))</p></li>
				<li>Define the batch size and a set of augmentations to be applied through <strong class="source-inline">ImageDataGenerator()</strong>:<p class="source-code">BATCH_SIZE = 32</p><p class="source-code">augmenter = ImageDataGenerator(rotation_range=30,</p><p class="source-code">                               horizontal_flip=True,</p><p class="source-code">                               width_shift_range=0.1,</p><p class="source-code">                               height_shift_range=0.1,</p><p class="source-code">                               shear_range=0.2,</p><p class="source-code">                               zoom_range=0.2,</p><p class="source-code">                               fill_mode='nearest')</p><p class="source-code">train_generator = augmenter.flow(X_train, y_train, </p><p class="source-code">                                 BATCH_SIZE)</p></li>
				<li>Train the <a id="_idIndexMarker282"/>full model for<a id="_idIndexMarker283"/> 20 epochs and evaluate its performance on the test set:<p class="source-code">EPOCHS = 20</p><p class="source-code">model.compile(loss='categorical_crossentropy',</p><p class="source-code">              optimizer=RMSprop(lr=1e-3),</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">model.fit(train_generator,</p><p class="source-code">          steps_per_epoch=len(X_train) // BATCH_SIZE,</p><p class="source-code">          validation_data=(X_test, y_test),</p><p class="source-code">          epochs=EPOCHS)</p><p class="source-code">result = model.evaluate(X_test, y_test)</p><p class="source-code">print(f'Test accuracy: {result[1]}')</p></li>
			</ol>
			<p>In a matter of minutes, we obtained a model with an accuracy of around 95.22% on the test set. Awesome, don't you think? Now, let's dive deeper.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor138"/>How it works…</h2>
			<p>We leveraged the knowledge encoded in the pre-trained <strong class="bold">ResNetV1152</strong> we used as a starting point, a gargantuan network that we could hardly train on our own, let alone on a such a small dataset as <strong class="source-inline">17 Category Flower Dataset</strong>. </p>
			<p>With just a<a id="_idIndexMarker284"/> quick top layer swap, we <a id="_idIndexMarker285"/>managed to obtain an impressive 95.22% accuracy on the test set, which is not a small feat, all constraints considered. </p>
			<p>Unlike the <em class="italic">Fine-tuning a network using the Keras API</em> recipe, we didn't warm up the model's head this time. Again, this is not a hard rule, but yet another tool in our toolbox that we should try on a per-project basis.</p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor139"/>See also</h2>
			<p>You can read more about the pre-trained model we used in this recipe here: <a href="https://tfhub.dev/google/imagenet/resnet_v1_152/feature_vector/4">https://tfhub.dev/google/imagenet/resnet_v1_152/feature_vector/4</a>.</p>
		</div>
	</body></html>