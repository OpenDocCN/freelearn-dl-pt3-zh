["```\n    import tensorflow as tf\n    ```", "```\n    var1 = tf.Variable(2706, tf.int32)\n    var2 = tf.Variable(2386, tf.int32)\n    ```", "```\n    var_sum = var1 + var2\n    var_sum.numpy()\n    ```", "```\n    5092\n    ```", "```\n    scalar1 = tf.Variable(95, tf.int32)\n    vector1 = tf.Variable([2706, 2799, 5102], \\\n                          tf.int32)\n    ```", "```\n    vector_scalar_sum = scalar1 + vector1\n    vector_scalar_sum.numpy()\n    ```", "```\n    array([2801, 2894, 5197])\n    ```", "```\n    matrix1 = tf.Variable([[2706, 2799, 5102], \\\n                           [2386, 4089, 5932]], tf.int32)\n    matrix2 = tf.Variable([[5901, 1208, 645], \\\n                           [6235, 1098, 948]], tf.int32)\n    matrix3 = tf.Variable([[3908, 2339, 5520], \\\n                           [4544, 1978, 4729]], tf.int32)\n    ```", "```\n    matrix_sum = matrix1 + matrix2 + matrix3\n    matrix_sum.numpy()\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    array1 = tf.Variable([*range(24)])\n    array1.shape.as_list()\n    ```", "```\n    [24]\n    ```", "```\n    reshape1 = tf.reshape(array1, shape=[12, 2])\n    reshape1.shape.as_list()\n    ```", "```\n    [12, 2]\n    ```", "```\n    reshape2 = tf.reshape(array1, shape=[3, 4, 2])\n    reshape2.shape.as_list()\n    ```", "```\n    [3, 4, 2]\n    ```", "```\n    tf.rank(reshape2).numpy()\n    ```", "```\n    3\n    ```", "```\n    transpose1 = tf.transpose(reshape1)\n    transpose1.shape.as_list()\n    ```", "```\n    [2, 12]\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    input1 = tf.Variable([[-0.013, 0.024, 0.06, 0.022], \\\n                          [0.001, -0.047, 0.039, 0.016], \\\n                          [0.018, 0.030, -0.021, -0.028]], \\\n                         tf.float32)\n    ```", "```\n    weights = tf.Variable([[19995.95], [24995.50], \\\n                           [36745.50], [29995.95]], \\\n                          tf.float32)\n    ```", "```\n    bias = tf.Variable([[-2500.0],[-2500.0],[-2500.0]], \\\n                       tf.float32)\n    ```", "```\n    output = tf.matmul(input1,weights) + bias\n    output\n    ```", "```\n    output = tf.keras.activations.relu(output)\n    output \n    ```", "```\n    import pandas as pd\n    ```", "```\n    df = pd.read_csv('Bias_correction_ucl.csv')\n    ```", "```\n    df.drop('Date', inplace=True, axis=1)\n    ```", "```\n    ax = df['Present_Tmax'].hist(color='gray')\n    ax.set_xlabel(\"Normalized Temperature\")\n    ax.set_ylabel(\"Frequency\")\n    ```", "```\n    from sklearn.preprocessing import MinMaxScaler\n    scaler = MinMaxScaler()\n    df2 = scaler.fit_transform(df)\n    df2 = pd.DataFrame(df2, columns=df.columns)\n    ```", "```\n    ax = df2['Present_Tmax'].hist(color='gray')\n    ax.set_xlabel(\"Normalized Temperature\")\n    ax.set_ylabel(\"Frequency\")\n    ```", "```\n    from tensorflow.keras.preprocessing.image \\\n        import ImageDataGenerator\n    ```", "```\n    train_datagen = ImageDataGenerator(rescale = 1./255,\\\n                                       shear_range = 0.2,\\\n                                       rotation_range= 180,\\\n                                       zoom_range = 0.2,\\\n                                       horizontal_flip = True)\n    ```", "```\n    training_set = train_datagen.flow_from_directory\\\n                   ('image_data',\\\n                    target_size = (64, 64),\\\n                    batch_size = 25,\\\n                    class_mode = 'binary')\n    ```", "```\n    import matplotlib.pyplot as plt\n    def show_batch(image_batch, label_batch):\\\n        lookup = {v: k for k, v in \n            training_set.class_indices.items()}\n        label_batch = [lookup[label] for label in \\\n                      label_batch]\n        plt.figure(figsize=(10,10))\n        for n in range(25):\n            ax = plt.subplot(5,5,n+1)\n            plt.imshow(image_batch[n])\n            plt.title(label_batch[n].title())\n            plt.axis('off')\n    ```", "```\n    image_batch, label_batch = next(training_set)\n    show_batch(image_batch, label_batch)\n    ```", "```\n    import tensorflow as tf\n    import os\n    ```", "```\n    def load_audio(file_path, sample_rate=44100):\n        # Load audio at 44.1kHz sample-rate\n        audio = tf.io.read_file(file_path)\n        audio, sample_rate = tf.audio.decode_wav\\\n                             (audio,\\\n                              desired_channels=-1,\\\n                              desired_samples=sample_rate)\n        return tf.transpose(audio)\n    ```", "```\n    prefix = \" ../Datasets/data_speech_commands_v0.02\"\\\n            \"/zero/\"\n    paths = [os.path.join(prefix, path) for path in \\\n             os.listdir(prefix)]\n    ```", "```\n    def prep_ds(ds, shuffle_buffer_size=1024, \\\n                batch_size=16):\n        # Randomly shuffle (file_path, label) dataset\n        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n        # Load and decode audio from file paths\n        ds = ds.map(load_audio)\n        # Take the absolute value\n        ds = ds.map(tf.abs)\n        # Apply log1p function\n        ds = ds.map(tf.math.log1p)\n        # Repeat dataset forever\n        ds = ds.repeat()\n        # Prepare batches\n        ds = ds.batch(batch_size)\n        # Prefetch\n        ds = ds.prefetch(buffer_size=batch_size)\n        return ds\n    ```", "```\n    ds = tf.data.Dataset.from_tensor_slices(paths)\n    train_ds = prep_ds(ds)\n    ```", "```\n    for x in train_ds.take(1):\\\n         print(x)\n    ```", "```\n    import matplotlib.pyplot as plt\n    plt.plot(x[0,:,:].numpy().T, color = 'gray')\n    plt.xlabel('Sample')\n    plt.ylabel('Value'))\n    ```", "```\n    import tensorflow as tf\n    tf.random.set_seed(42)\n    ```", "```\n    logdir = 'logs/'\n    writer = tf.summary.create_file_writer(logdir)\n    ```", "```\n    @tf.function\n    def my_func(x, y):\n        r1 = tf.matmul(x, y)\n        r2 = r1 + tf.ones_like(r1)\n        r3 = tf.keras.activations.sigmoid(r2)\n        return r3\n    ```", "```\n    x = tf.random.uniform((5, 5, 5))\n    y = tf.random.uniform((5, 5, 5))\n    ```", "```\n    tf.summary.trace_on(graph=True, profiler=True)\n    ```", "```\n    z = my_func(x, y)\n    with writer.as_default():\n        tf.summary.trace_export(name=\"my_func_trace\",\\\n                                step=0,\\\n                                profiler_outdir=logdir)\n    ```", "```\n    tensorboard --logdir=./logs\n    ```", "```\n    import tensorflow as tf\n    import tensorflow_hub as hub\n    print('TF version: ', tf.__version__)\n    print('HUB version: ', hub.__version__)\n    ```", "```\n    module_handle =\"https://tfhub.dev/google\"\\\n                   \"/universal-sentence-encoder/4\"\n    ```", "```\n    hub_layer = hub.KerasLayer(module_handle, input_shape=[],\\ \n                               dtype=tf.string)\n    ```", "```\n    text = ['The TensorFlow Workshop']\n    ```", "```\n    hub_layer(text)\n    ```", "```\n    import tensorflow as tf\n    import pandas as pd\n    ```", "```\n    df = pd.read_csv('superconductivity.csv')\n    ```", "```\n    df.dropna(inplace=True)\n    ```", "```\n    target = df['critical_temp']\n    features = df.drop('critical_temp', axis=1)\n    ```", "```\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    feature_array = scaler.fit_transform(features)\n    features = pd.DataFrame(feature_array, columns=features.columns)\n    ```", "```\n    model = tf.keras.Sequential()\n    ```", "```\n    model.add(tf.keras.layers.InputLayer\\\n             (input_shape=features.shape[1],), \\\n              name='Input_layer'))\n    model.add(tf.keras.layers.Dense(64, activation='relu', \\\n                                    name='Dense_layer_1'))\n    model.add(tf.keras.layers.Dense(32, name='Dense_layer_2'))\n    model.add(tf.keras.layers.Dense(16, name='Dense_layer_3'))\n    model.add(tf.keras.layers.Dense(8, name='Dense_layer_4'))\n    model.add(tf.keras.layers.Dense(1, name='Output_layer'))\n    ```", "```\n    model.compile(tf.optimizers.RMSprop(0.001), loss='mse')\n    ```", "```\n    tensorboard_callback = tf.keras.callbacks\\\n                             .TensorBoard(log_dir=\"./logs\")\n    ```", "```\n    model.fit(x=features.to_numpy(), y=target.to_numpy(), \\\n              epochs=100, callbacks=[tensorboard_callback], \\\n              batch_size=32, validation_split=0.2)\n    ```", "```\n    loss = model.evaluate(features.to_numpy(), target.to_numpy())\n    print('loss:', loss)\n    ```", "```\n    loss: 165.735601268987\n    ```", "```\n    tensorboard –-logdir=logs/\n    ```", "```\n    import tensorflow as tf\n    import pandas as pd\n    ```", "```\n    df = pd.read_csv('superconductivity.csv')\n    ```", "```\n    df.dropna(inplace=True)\n    ```", "```\n    target = df['critical_temp'].apply(lambda x: 1 if x>77.36 else 0)\n    features = df.drop('critical_temp', axis=1)\n    ```", "```\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    feature_array = scaler.fit_transform(features)\n    features = pd.DataFrame(feature_array, columns=features.columns)\n    ```", "```\n    model = tf.keras.Sequential()\n    ```", "```\n    model.add(tf.keras.layers.InputLayer\\\n             (input_shape=features.shape[1], \\\n              name='Input_layer'))\n    model.add(tf.keras.layers.Dense(32, name='Hidden_layer_1'))\n    model.add(tf.keras.layers.Dense(16, name='Hidden_layer_2'))\n    model.add(tf.keras.layers.Dense(8, name='Hidden_layer_3'))\n    model.add(tf.keras.layers.Dense(1, name='Output_layer', \\\n                                    activation='sigmoid'))\n    ```", "```\n    model.compile(tf.optimizers.RMSprop(0.0001), \\\n                  loss= 'binary_crossentropy', metrics=['accuracy'])\n    ```", "```\n    tensorboard_callback = tf.keras.callbacks.TensorBoard\\\n                           (log_dir=\"./logs\")\n    ```", "```\n    model.fit(x=features.to_numpy(), y=target.to_numpy(),\\\n              epochs=50, callbacks=[tensorboard_callback],\\\n              validation_split=0.2)\n    ```", "```\n    loss, accuracy = model.evaluate(features.to_numpy(), \\\n                                    target.to_numpy())\n    print(f'loss: {loss}, accuracy: {accuracy}')\n    ```", "```\n    loss: 0.21984571637242145, accuracy: 0.8893383145332336\n    ```", "```\n    tensorboard –-logdir=logs/\n    ```", "```\n    import pandas as pd\n    ```", "```\n    file_url = 'https://raw.githubusercontent.com/PacktWorkshops'\\\n              '/The-TensorFlow-Workshop/master/Chapter05'\\\n              '/dataset/letter-recognition.data'\n    ```", "```\n    data = pd.read_csv(file_url, header=None)\n    data.head()\n    ```", "```\n    target = data.pop(0)\n    ```", "```\n    X_train = data[:15000]\n    y_train = target[:15000]\n    ```", "```\n    X_test = data[15000:]\n    y_test = target[15000:]\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    tf.random.set_seed(8)\n    ```", "```\n    model = tf.keras.Sequential()\n    ```", "```\n    from tensorflow.keras.layers import Dense\n    ```", "```\n    fc1 = Dense(512, input_shape=(16,), activation='relu')\n    ```", "```\n    fc2 = Dense(512, activation='relu')\n    ```", "```\n    fc3 = Dense(128, activation='relu')\n    ```", "```\n    fc4 = Dense(128, activation='relu')\n    ```", "```\n    fc5 = Dense(26, activation='softmax')\n    ```", "```\n    model.add(fc1)\n    model.add(fc2)\n    model.add(fc3)\n    model.add(fc4)\n    model.add(fc5)\n    ```", "```\n    model.summary()\n    ```", "```\n    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n    ```", "```\n    optimizer = tf.keras.optimizers.Adam(0.001)\n    ```", "```\n    model.compile(optimizer=optimizer, loss=loss, \\\n                  metrics=['accuracy'])\n    ```", "```\n    model.fit(X_train, y_train, epochs=5)\n    ```", "```\n    model.evaluate(X_test, y_test)\n    ```", "```\n    preds_proba = model.predict(X_test)\n    ```", "```\n    preds = preds_proba.argmax(axis=1)\n    ```", "```\n    from tensorflow.math import confusion_matrix\n    ```", "```\n    confusion_matrix(y_test, preds)\n    ```", "```\n    import pandas as pd\n    ```", "```\n    feature_url = 'https://raw.githubusercontent.com'\\\n                  '/PacktWorkshops'/The-TensorFlow-Workshop'\\\n                  '/master/Chapter05'/dataset/IMDB-F-features.csv'\n    ```", "```\n    feature = pd.read_csv(feature_url)\n    feature.head()\n    ```", "```\n    target_url = 'https://raw.githubusercontent.com'\\\n                 '/PacktWorkshops/The-TensorFlow-Workshop'\\\n                 '/master/Chapter05'/dataset/IMDB-F-targets.csv'\n    ```", "```\n    target = pd.read_csv(target_url)\n    target.head()\n    ```", "```\n    X_train = feature[:15000]\n    y_train = target[:15000]\n    ```", "```\n    X_test = feature[15000:]\n    y_test = target[15000:]\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    tf.random.set_seed(8)\n    ```", "```\n    model = tf.keras.Sequential()\n    ```", "```\n    from tensorflow.keras.layers import Dense\n    ```", "```\n    fc1 = Dense(512, input_shape=(1001,), activation='relu')\n    ```", "```\n    fc2 = Dense(512, activation='relu')\n    ```", "```\n    fc3 = Dense(128, activation='relu')\n    ```", "```\n    fc4 = Dense(128, activation='relu')\n    ```", "```\n    fc5 = Dense(28, activation='sigmoid')\n    ```", "```\n    model.add(fc1)\n    model.add(fc2)\n    model.add(fc3)\n    model.add(fc4)\n    model.add(fc5)\n    ```", "```\n    model.summary()\n    ```", "```\n    loss = tf.keras.losses.BinaryCrossentropy()\n    ```", "```\n    optimizer = tf.keras.optimizers.Adam(0.001)\n    ```", "```\n    model.compile(optimizer=optimizer, loss=loss, \\\n                  metrics=['accuracy'])\n    ```", "```\n    model.fit(X_train, y_train, epochs=20)\n    ```", "```\n    model.evaluate(X_test, y_test)\n    ```", "```\n    import pandas as pd\n    ```", "```\n    usecols = ['AAGE','ADTIND','ADTOCC','SEOTR','WKSWORK', 'PTOTVAL']\n    ```", "```\n    train_url = 'https://raw.githubusercontent.com/PacktWorkshops'\\\n                '/The-TensorFlow-Workshop/master/Chapter06'\\\n                '/dataset/census-income-train.csv'\n    ```", "```\n    train_data = pd.read_csv(train_url, usecols=usecols)\n    train_data.head()\n    ```", "```\n    train_target = train_data.pop('PTOTVAL')\n    ```", "```\n    test_url = 'https://github.com/PacktWorkshops'\\\n               '/The-TensorFlow-Workshop/blob/master/Chapter06'\\\n               '/dataset/census-income-test.csv?raw=true'\n    ```", "```\n    test_data = pd.read_csv(test_url, usecols=usecols)\n    test_data.head()\n    ```", "```\n    test_target = test_data.pop('PTOTVAL')\n    ```", "```\n    import tensorflow as tf\n    from tensorflow.keras.layers import Dense\n    ```", "```\n    tf.random.set_seed(8)\n    ```", "```\n    model = tf.keras.Sequential()\n    ```", "```\n    from tensorflow.keras.layers import Dense\n    ```", "```\n    fc1 = Dense(1048, input_shape=(5,), activation='relu')\n    ```", "```\n    fc2 = Dense(512, activation='relu')\n    fc3 = Dense(128, activation='relu')\n    fc4 = Dense(64, activation='relu')\n    ```", "```\n    fc5 = Dense(3, activation='softmax')\n    ```", "```\n    fc5 = Dense(1)\n    ```", "```\n    model.add(fc1)\n    model.add(fc2)\n    model.add(fc3)\n    model.add(fc4)\n    model.add(fc5)\n    ```", "```\n    model.summary()\n    ```", "```\n    optimizer = tf.keras.optimizers.Adam(0.05)\n    ```", "```\n    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n    ```", "```\n    model.fit(train_data, train_target, epochs=5, \\\n              validation_split=0.2)\n    ```", "```\n    reg_fc1 = Dense(1048, input_shape=(5,), activation='relu', \\\n                    kernel_regularizer=tf.keras.regularizers\\\n                                         .l1_l2(l1=0.001, l2=0.001))\n    reg_fc2 = Dense(512, activation='relu', \\\n                    kernel_regularizer=tf.keras.regularizers\\\n                                         .l1_l2(l1=0.001, l2=0.001))\n    reg_fc3 = Dense(128, activation='relu', \\\n                    kernel_regularizer=tf.keras.regularizers\\\n                                         .l1_l2(l1=0.001, l2=0.001))\n    reg_fc4 = Dense(64, activation='relu', \\\n                    kernel_regularizer=tf.keras.regularizers\\\n                                         .l1_l2(l1=0.001, l2=0.001))\n    reg_fc5 = Dense(1, activation='relu')\n    ```", "```\n    model2 = tf.keras.Sequential()\n    model2.add(reg_fc1)\n    model2.add(reg_fc2)\n    model2.add(reg_fc3)\n    model2.add(reg_fc4)\n    model2.add(reg_fc5)\n    ```", "```\n    model2.summary()\n    ```", "```\n    optimizer = tf.keras.optimizers.Adam(0.1)\n    model2.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n    ```", "```\n    model2.fit(train_data, train_target, epochs=5, \\\n               validation_split=0.2)\n    ```", "```\n    import pandas as pd\n    ```", "```\n    usecols = ['AAGE','ADTIND','ADTOCC','SEOTR','WKSWORK', 'PTOTVAL']\n    ```", "```\n    train_url = 'https://raw.githubusercontent.com/PacktWorkshops'\\\n                '/The-TensorFlow-Workshop/master/Chapter06'\\\n                '/dataset/census-income-train.csv'\n    ```", "```\n    train_data = pd.read_csv(train_url, usecols=usecols)\n    train_data.head()\n    ```", "```\n    train_target = train_data.pop('PTOTVAL')\n    ```", "```\n    test_url = 'https://github.com/PacktWorkshops'\\\n               '/The-TensorFlow-Workshop/blob/master/Chapter06'\\\n               '/dataset/census-income-test.csv?raw=true'\n    ```", "```\n    test_data = pd.read_csv(test_url, usecols=usecols)\n    test_data.head()\n    ```", "```\n    test_target = test_data.pop('PTOTVAL')\n    ```", "```\n    import tensorflow as tf\n    from tensorflow.keras.layers import Dense\n    ```", "```\n    tf.random.set_seed(8)\n    ```", "```\n    def model_builder(hp):\n    model = tf.keras.Sequential()\n    hp_l2 = hp.Choice('l2', values = [0.1, 0.01, 0.001])\n    hp_units = hp.Int('units', min_value=128, max_value=512, step=64)\n    reg_fc1 = Dense(hp_units, input_shape=(5,), activation='relu', \\\n                    kernel_regularizer=tf.keras.regularizers\\\n                                         .l2(l=hp_l2))\n    reg_fc2 = Dense(512, activation='relu', \\\n                    kernel_regularizer=tf.keras.regularizers\\\n                                         .l2(l=hp_l2))\n    reg_fc3 = Dense(128, activation='relu', \\\n                    kernel_regularizer=tf.keras.regularizers\\\n                                         .l2(l=hp_l2))\n    reg_fc4 = Dense(128, activation='relu', \\\n                    kernel_regularizer=tf.keras.regularizers\\\n                                         .l2(l=hp_l2))\n    reg_fc5 = Dense(1)\n    model.add(reg_fc1)\n    model.add(reg_fc2)\n    model.add(reg_fc3)\n    model.add(reg_fc4)\n    model.add(reg_fc5)\n    hp_learning_rate = hp.Choice('learning_rate', \\\n                                 values = [0.01, 0.001])\n    optimizer = tf.keras.optimizers.Adam(hp_learning_rate)\n    model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n    return model\n    ```", "```\n    !pip install keras-tuner\n    import kerastuner as kt\n    ```", "```\n    tuner = kt.BayesianOptimization(model_builder, \\\n                                    objective = 'val_mse', \\\n                                    max_trials = 10)\n    ```", "```\n    tuner.search(train_data, train_target, \\\n                 validation_data=(test_data, test_target))\n    ```", "```\n    best_hps = tuner.get_best_hyperparameters()[0]\n    ```", "```\n    best_units = best_hps.get('units')\n    best_units\n    ```", "```\n    128\n    ```", "```\n    best_lr = best_hps.get('learning_rate')\n    best_lr\n    ```", "```\n    0.001\n    ```", "```\n    best_l2 = best_hps.get('l2')\n    best_l2\n    ```", "```\n    0.001\n    ```", "```\n    model = tuner.hypermodel.build(best_hps)\n    model.fit(X_train, y_train, epochs=5, \\\n              validation_data=(X_test, y_test))\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import tensorflow as tf\n    import tensorflow_datasets as tfds\n    from tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, \\\n        Dropout, Activation, Rescaling\n    from tensorflow.keras.models import Model\n    from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n    ```", "```\n    (c100_train_dataset, c100_test_dataset), \\\n    dataset_info = tfds.load('cifar100',\\\n                             split = ['train', 'test'],\\\n                             data_dir = 'content/Cifar100/',\\\n                             shuffle_files = True,\\\n                             as_supervised = True,\\\n                             with_info = True)\n    assert isinstance(c100_train_dataset, tf.data.Dataset)\n    image_shape = dataset_info.features[\"image\"].shape\n    print(f'Shape of Images in the Dataset: \\t{image_shape}')\n    num_classes = dataset_info.features[\"label\"].num_classes\n    print(f'Number of Classes in the Dataset: \\t{num_classes}')\n    names_of_classes = dataset_info.features[\"label\"].names\n    print(f'Names of Classes in the Dataset: \\t{names_of_classes}\\n')\n    print(f'Total examples in Train Dataset: \\\n          \\t{len(c100_train_dataset)}')\n    print(f'Total examples in Test Dataset: \\\n          \\t{len(c100_test_dataset)}')\n    ```", "```\n    normalization_layer = Rescaling(1./255)\n    c100_train_dataset = c100_train_dataset.map\\\n                         (lambda x, y: (normalization_layer(x), y), \\\n                          num_parallel_calls = \\\n                          tf.data.experimental.AUTOTUNE)\n    c100_train_dataset = c100_train_dataset.cache()\n    c100_train_dataset = c100_train_dataset.shuffle\\\n                         (len(c100_train_dataset))\n    c100_train_dataset = c100_train_dataset.batch(32)\n    c100_train_dataset = c100_train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    c100_test_dataset = c100_test_dataset.map\\\n                        (lambda x, y: (normalization_layer(x), y), \\\n                         num_parallel_calls = \\\n                         tf.data.experimental.AUTOTUNE)\n    c100_test_dataset = c100_test_dataset.cache()\n    c100_test_dataset = c100_test_dataset.batch(128)\n    c100_test_dataset = \\\n    c100_test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    ```", "```\n    input_layer = Input(shape=image_shape)\n    x = Conv2D(filters = 32, kernel_size = \\\n               (3, 3), strides=2)(input_layer)\n    x = Activation('relu')(x)\n    x = Conv2D(filters = 64, kernel_size = (3, 3), strides=2)(x)\n    x = Activation('relu')(x)\n    x = Conv2D(filters = 128, kernel_size = (3, 3), strides=2)(x)\n    x = Activation('relu')(x)\n    x = Flatten()(x)\n    x = Dropout(rate = 0.5)(x)\n    x = Dense(units = 1024)(x)\n    x = Activation('relu')(x)\n    x = Dropout(rate = 0.2)(x)\n    x = Dense(units = num_classes)(x)\n    output = Activation('softmax')(x)\n    c100_classification_model = Model(input_layer, output)\n    ```", "```\n    c100_classification_model.compile(\\\n        optimizer='adam', \\\n        loss='sparse_categorical_crossentropy', \\\n        metrics = ['accuracy'], loss_weights = None, \\\n        weighted_metrics = None, run_eagerly = None, \\\n        steps_per_execution = None\n    )\n    history = c100_classification_model.fit\\\n              (c100_train_dataset, \\\n               validation_data=c100_test_dataset, \\\n               epochs=15)\n    ```", "```\n    def plot_trend_by_epoch(tr_values, val_values, title):\n        epoch_number = range(len(tr_values))\n        plt.plot(epoch_number, tr_values, 'r')\n        plt.plot(epoch_number, val_values, 'b')\n        plt.title(title)\n        plt.xlabel('epochs')\n        plt.legend(['Training '+title, 'Validation '+title])\n        plt.figure()\n    hist_dict = history.history\n    tr_loss, val_loss = hist_dict['loss'], \\\n                        hist_dict['val_loss']\n    plot_trend_by_epoch(tr_loss, val_loss, \"Loss\")\n    tr_accuracy, val_accuracy = hist_dict['accuracy'], \\\n                                hist_dict['val_accuracy']\n    plot_trend_by_epoch(tr_accuracy, val_accuracy, \"Accuracy\")\n    ```", "```\n    test_labels = []\n    test_images = []\n    for image, label in tfds.as_numpy(c100_test_dataset.unbatch()):\n        test_images.append(image)\n        test_labels.append(label)\n    test_labels = np.array(test_labels)\n    predictions = c100_classification_model.predict\\\n                  (c100_test_dataset).argmax(axis=1)\n    incorrect_predictions = np.where(predictions != test_labels)[0]\n    index = np.random.choice(incorrect_predictions)\n    plt.imshow(test_images[index])\n    print(f'True label: {names_of_classes[test_labels[index]]}')\n    print(f'Predicted label: {names_of_classes[predictions[index]]}')\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    file_url = 'https://github.com/PacktWorkshops/'\\\n              'The-TensorFlow-Workshop/blob/master'\\\n              '/Chapter08/dataset/fruits360.zip'\n    ```", "```\n    zip_dir = tf.keras.utils.get_file('fruits360.zip', \\\n                                      origin=file_url, extract=True)\n    ```", "```\n    import pathlib\n    ```", "```\n    path = pathlib.Path(zip_dir).parent / 'fruits360_filtered'\n    ```", "```\n    train_dir = path / 'Training'\n    validation_dir = path / 'Test'\n    ```", "```\n    total_train = 11398\n    total_val = 4752\n    ```", "```\n    from tensorflow.keras.preprocessing.image\n        import ImageDataGenerator\n    ```", "```\n    train_img_gen = ImageDataGenerator(rescale=1./255, \\\n                                       rotation_range=40, \\\n                                       width_shift_range=0.1, \\\n                                       height_shift_range=0.1, \\\n                                       shear_range=0.2, \\\n                                       zoom_range=0.2, \\\n                                       horizontal_flip=True, \\\n                                       fill_mode='nearest'))\n    ```", "```\n    val_img_gen = ImageDataGenerator(rescale=1./255)\n    ```", "```\n    Batch_size = 32\n    img_height = 224\n    img_width = 224\n    channel = 3\n    ```", "```\n    train_data_gen = train_image_generator.flow_from_directory\\\n                     (batch_size=batch_size, directory=train_dir, \\\n                      target_size=(img_height, img_width))\n    ```", "```\n    val_data_gen = validation_image_generator.flow_from_directory\\\n                   (batch_size=batch_size, directory=validation_dir,\\\n                    target_size=(img_height, img_width))\n    ```", "```\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras import layers\n    ```", "```\n    np.random.seed(8)\n    tf.random.set_seed(8)\n    ```", "```\n    from tensorflow.keras.applications\n    import NASNetMobile\n    ```", "```\n    base_model = NASNetMobile(input_shape=(img_height, img_width, \\\n                                           channel), \\\n                              weights='imagenet', include_top=False)\n    ```", "```\n    base_model.summary()\n    ```", "```\n    model = tf.keras.Sequential([base_model,\\\n                                 layers.Flatten(),\\\n                                 layers.Dense(500, \\\n                                              activation='relu'), \\\n                                 layers.Dense(120, \\\n                                              activation='softmax')])\n    ```", "```\n    optimizer = tf.keras.optimizers.Adam(0.001)\n    ```", "```\n    model.compile(loss='categorical_crossentropy', \\\n                  optimizer=optimizer, metrics=['accuracy'])\n    ```", "```\n    model.fit(train_data_gen,\n              steps_per_epoch=len(features_train) // batch_size,\\\n              epochs=5,\\\n              validation_data=val_data_gen,\\\n              validation_steps=len(features_test) // batch_size\\\n    )\n    ```", "```\n    import tensorflow as tf\n    ```", "```\n    file_url = 'https://storage.googleapis.com'\\\n               '/mledu-datasets/cats_and_dogs_filtered.zip'\n    ```", "```\n    zip_dir = tf.keras.utils.get_file('cats_and_dogs.zip', \\\n                                      origin=file_url, extract=True)\n    ```", "```\n    import pathlib\n    ```", "```\n    path = pathlib.Path(zip_dir).parent / 'cats_and_dogs_filtered'\n    ```", "```\n    train_dir = path / 'train'\n    validation_dir = path / 'validation'\n    ```", "```\n    total_train = 2000\n    total_val = 1000\n    ```", "```\n    from tensorflow.keras.preprocessing.image \n    import ImageDataGenerator\n    ```", "```\n    train_image_generator = ImageDataGenerator(rescale=1./255)\n    validation_image_generator = ImageDataGenerator(rescale=1./255)\n    ```", "```\n    batch_size = 32\n    img_height = 224\n    img_width = 224\n    ```", "```\n    train_data_gen = train_image_generator.flow_from_directory\\\n                     (batch_size=batch_size, directory=train_dir, \\\n                      shuffle=True, target_size=(img_height, \\\n                                                 img_width), \\\n                      class_mode='binary')\n    ```", "```\n    val_data_gen = validation_image_generator.flow_from_directory\\\n                   (batch_size=batch_size, \\\n                    directory=validation_dir, \\\n                    target_size=(img_height, img_width), \\\n                    class_mode='binary')\n    ```", "```\n    import numpy as np\n    import tensorflow as tf\n    from tensorflow.keras import layers\n    ```", "```\n    np.random.seed(8)\n    tf.random.set_seed(8)\n    ```", "```\n    import tensorflow_hub as hub\n    ```", "```\n    MODULE_HANDLE = 'https://tfhub.dev/google/efficientnet/b0'\\\n                    '/feature-vector/1'\n    module = hub.load(MODULE_HANDLE)\n    ```", "```\n    model = tf.keras.Sequential\\\n            ([hub.KerasLayer(MODULE_HANDLE,\\\n                             input_shape=(224, 224, 3)),\n              layers.Dense(500, activation='relu'),\n              layers.Dense(1, activation='sigmoid')])\n    ```", "```\n    model.compile(loss='binary_crossentropy', \\\n                  optimizer=tf.keras.optimizers.Adam(0.001), \\\n                  metrics=['accuracy'])\n    ```", "```\n    model.fit(train_data_gen, \\\n              steps_per_epoch = total_train // batch_size, \\\n              epochs=5, \\\n              validation_data=val_data_gen, \\\n              validation_steps=total_val // batch_size)\n    ```", "```\n    import numpy as np\n    import pandas as pd\n    import datetime\n    from sklearn.preprocessing import MinMaxScaler\n    ```", "```\n    data = pd.read_csv(\"household_power_consumption.csv\")\n    ```", "```\n    data['Date'] = pd.to_datetime(data['Date'], format=\"%d/%m/%Y\")\n    data['Datetime'] = data['Date'].dt.strftime('%Y-%m-%d') + ' ' \\\n                       +  data['Time']\n    data['Datetime'] = pd.to_datetime(data['Datetime'])\n    ```", "```\n    data = data.sort_values(['Datetime'])\n    ```", "```\n    num_cols = ['Global_active_power', 'Global_reactive_power', \\\n                'Voltage', 'Global_intensity', 'Sub_metering_1', \\\n                'Sub_metering_2', 'Sub_metering_3']\n    ```", "```\n    for col in num_cols:\n        data[col] = pd.to_numeric(data[col], errors='coerce')\n    ```", "```\n    data.head()\n    ```", "```\n    data.tail()\n    ```", "```\n    for col in num_cols:\n        data[col].fillna(data[col].mean(), inplace=True)\n    ```", "```\n    df = data.drop(['Date', 'Time', 'Global_reactive_power', 'Datetime'], \\\n                   axis = 1)\n    ```", "```\n    scaler = MinMaxScaler()\n    scaled_data = scaler.fit_transform(df)\n    scaled_data \n    ```", "```\n    X = []\n    y = []\n    ```", "```\n    for i in range(60, scaled_data.shape[0]):\n        X.append(scaled_data [i-60:i])\n        y.append(scaled_data [i, 0])\n    ```", "```\n    X, y = np.array(X), np.array(y)\n    ```", "```\n    X_train = X[:217440]\n    y_train = y[:217440]\n    X_test = X[217440:]\n    y_test = y[217440:]\n    ```", "```\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.layers import Dense, LSTM, Dropout\n    ```", "```\n    regressor = Sequential()\n    regressor.add(LSTM(units= 20, activation = 'relu',\\\n                       return_sequences = True,\\\n                       input_shape = (X_train.shape[1], X_train.shape[2])))\n    regressor.add(Dropout(0.5))\n    regressor.add(LSTM(units= 40, \\\n                       activation = 'relu', \\\n                       return_sequences = True))\n    regressor.add(Dropout(0.5))\n    regressor.add(LSTM(units= 80, \\\n                       activation = 'relu'))\n    regressor.add(Dropout(0.5))\n    regressor.add(Dense(units = 1))  \n    ```", "```\n    regressor.summary()\n    ```", "```\n    regressor.compile(optimizer='adam', loss = 'mean_squared_error')\n    ```", "```\n    regressor.fit(X_train, y_train, epochs=2, batch_size=32)\n    ```", "```\n    y_pred = regressor.predict(X_test)\n    ```", "```\n    plt.figure(figsize=(14,5))\n    plt.plot(y_test[-60:], color = 'black', \\\n             label = \"Real Power Consumption\")\n    plt.plot(y_pred[-60:], color = 'gray', \\\n             label = 'Predicted Power Consumption')\n    plt.title('Power Consumption Prediction')\n    plt.xlabel('time')\n    plt.ylabel('Power Consumption')\n    plt.legend()\n    plt.show()\n    ```", "```\n    import numpy as np\n    import pandas as pd\n    ```", "```\n    data = pd.read_csv(\"https://raw.githubusercontent.com\"\\\n                       \"/PacktWorkshops/The-TensorFlow-Workshop\"\\\n                       \"/master/Chapter09/Datasets/tweets.csv\")\n    ```", "```\n    data.head()\n    ```", "```\n    data.tail()\n    ```", "```\n    df = data[['text','airline_sentiment']]\n    ```", "```\n    df = df[df['airline_sentiment'] != 'neutral']\n    ```", "```\n    y = df['airline_sentiment'].map({'negative':0, 'positive':1}).values\n    ```", "```\n    X = df['text']\n    ```", "```\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    from tensorflow.keras.preprocessing.sequence \\\n        import pad_sequences\n    ```", "```\n    tokenizer = Tokenizer(num_words=10000)\n    ```", "```\n    tokenizer.fit_on_texts(X)\n    ```", "```\n    tokenizer.word_index\n    ```", "```\n    vocab_size = len(tokenizer.word_index) + 1\n    ```", "```\n    encoded_tweets = tokenizer.texts_to_sequences(X)\n    ```", "```\n    padded_tweets = pad_sequences(encoded_tweets, maxlen=280, padding='post')\n    ```", "```\n    padded_tweets.shape\n    ```", "```\n    (11541, 280)\n    ```", "```\n    indices = np.random.permutation(padded_tweets.shape[0])\n    ```", "```\n    train_idx = indices[:10000]\n    test_idx = indices[10000:]\n    ```", "```\n    X_train = padded_tweets[train_idx,]\n    X_test = padded_tweets[test_idx,]\n    y_train = y[train_idx,]\n    y_test = y[test_idx,]\n    ```", "```\n    from tensorflow.keras import Sequential\n    from tensorflow.keras.layers import Dense, LSTM, Dropout, Embedding\n    ```", "```\n    model = Sequential()\n\n    model.add(Embedding(vocab_size, embedding_vector_length, input_length=280))\n    model.add(LSTM(units= 50, activation = 'relu', return_sequences = True))\n    model.add(Dropout(0.2))\n    model.add(LSTM(100, activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    ```", "```\n    model.summary()\n    ```", "```\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    ```", "```\n    model.fit(X_train, y_train, epochs=2, batch_size=32)\n    ```", "```\n    from google.colab import files\n    uploaded = files.upload()\n    ```", "```\n    !unzip \\*.zip\n    ```", "```\n    directory = \"/content/gdrive/My Drive/Datasets/pneumonia-or-healthy/\"\n    ```", "```\n    import numpy as np\n    import pandas as pd\n    import pathlib\n    import os\n    import matplotlib.pyplot as plt\n    from keras.models import Sequential\n    from keras import optimizers\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    import tensorflow as tf\n    from tensorflow.keras.layers import Input, Conv2D, ReLU, \\\n        BatchNormalization,Add, AveragePooling2D, Flatten, Dense\n    from tensorflow.keras.models import Model  \n    ```", "```\n    path = pathlib.Path(directory)\n    ```", "```\n    train_dir = path / 'training_set'\n    validation_dir = path / 'test_set'\n    ```", "```\n    train_table_dir = train_dir / 'table'\n    train_glass_dir = train_dir /'glass'\n    validation_table_dir = validation_dir / 'table'\n    validation_glass_dir = validation_dir / 'glass'\n    ```", "```\n    num_train_table = len([f for f in os.listdir(train_table_dir)if \\\n                           os.path.isfile(os.path.join\\\n                                          (train_table_dir, f))])\n    num_train_glass = len([f for f in os.listdir(train_glass_dir)if \\\n                           os.path.isfile(os.path.join\\\n                                          (train_glass_dir, f))])\n    num_validation_table = len([f for f in os.listdir\\\n                                (validation_table_dir)if\n    os.path.isfile(os.path.join(validation_table_dir, f))])\n    num_validation_glass = len([f for f in os.listdir\\\n                                (validation_glass_dir)if \\\n                                os.path.isfile\\\n                                (os.path.join\\\n                                (validation_glass_dir, f))])\n    ```", "```\n    plt.bar(['table', 'glass'], \\\n            [num_train_table + num_validation_table, \\\n             num_train_glass + num_validation_glass], \\\n            align='center', \\\n            alpha=0.5)\n    plt.show()\n    ```", "```\n    total_train = len(os.listdir(train_table_dir)) + \\\n                  len(os.listdir(validation_table_dir))\n    total_val = len(os.listdir(train_glass_dir)) + \\\n                len(os.listdir(validation_glass_dir))\n    ```", "```\n    from tensorflow.keras.preprocessing.image \\\n        import ImageDataGenerator\n    ```", "```\n    train_image_generator = ImageDataGenerator(rescale=1./255)\n    validation_image_generator = ImageDataGenerator(rescale=1./255)\n    ```", "```\n    batch_size = 32\n    img_height = 100\n    img_width = 100\n    ```", "```\n    train_data_gen = train_image_generator.flow_from_directory\\\n                     (batch_size=batch_size, directory=train_dir, \\\n                      shuffle=True, \\\n                      target_size=(img_height, img_width), \\\n                      class_mode='binary')\n    ```", "```\n    val_data_gen = validation_image_generator.flow_from_directory\\\n                   (batch_size=batch_size, directory=validation_dir,\\\n                    target_size=(img_height, img_width), \\\n                    class_mode='binary')\n    ```", "```\n    def custom_loss_function(y_true, y_pred):\n        squared_difference = tf.square(float(y_true) - float(y_pred))\n        return tf.reduce_mean(squared_difference, axis=-1)\n    ```", "```\n    def relu_batchnorm_layer(input):\n        return BatchNormalization()(ReLU()(input))\n    ```", "```\n    def residual_block(input, downsample: bool, filters: int, \\\n                       kernel_size: int = 3):\n        int_output = Conv2D(filters=filters, kernel_size=kernel_size, \n                            strides= (1 if not downsample else 2), \n                            padding=\"same\")(input)\n        int_output = relu_batchnorm_layer(int_output)\n        int_output = Conv2D(filters=filters, kernel_size=kernel_size, \n                            padding=\"same\")(int_output)\n        if downsample:\n            int_output2 = Conv2D(filters=filters, kernel_size=1, strides=2,\n                                 padding=\"same\")(input)\n            output = Add()([int_output2, int_output]) \n        else:\n            output = Add()([input, int_output])\n        output = relu_batchnorm_layer(output)\n        return output\n    ```", "```\n    inputs = Input(shape=(100, 100, 3))\n    num_filters = 32\n\n    t = BatchNormalization()(inputs)\n    t = Conv2D(kernel_size=3,\n               strides=1,\n               filters=32,\n               padding=\"same\")(t)\n    t = relu_batchnorm_layer(t)\n\n    num_blocks_list = [1, 3, 5, 6, 1]\n    for i in range(len(num_blocks_list)):\n        num_blocks = num_blocks_list[i]\n        for j in range(num_blocks):\n            t = residual_block(t, downsample=(j==0 and i!=0), filters=num_filters)\n        num_filters *= 2\n\n    t = AveragePooling2D(4)(t)\n    t = Flatten()(t)\n    outputs = Dense(1, activation='sigmoid')(t)\n\n    model = Model(inputs, outputs)\n    ```", "```\n    model.summary()\n    ```", "```\n    model.compile(\n           optimizer='adam',\n           loss=custom_loss_function,\n           metrics=['accuracy']\n    )\n    ```", "```\n    history = model.fit(\n        Train_data_gen,\n        steps_per_epoch=total_train // batch_size,\n        epochs=5,\n        validation_data=val_data_gen,\n        validation_steps=total_val // batch_size\n    )\n    ```", "```\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Training Accuracy vs Validation Accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n    ```", "```\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Training Loss vs Validation Loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n    ```", "```\n    try:\n        from google.colab import drive\n        drive.mount('/content/drive', force_remount=True)\n        COLAB = True\n        print(\"Note: using Google CoLab\")\n        %tensorflow_version 2.x\n    except:\n        print(\"Note: not using Google CoLab\")\n        COLAB = False\n    ```", "```\n    Mounted at /content/drive\n    Note: using Google CoLab\n    ```", "```\n    import tensorflow as tf\n    from tensorflow.keras.models import Sequential, Model, load_model\n    from tensorflow.keras.layers import InputLayer, Reshape, Dropout, Dense \n    from tensorflow.keras.layers import Flatten, BatchNormalization\n    from tensorflow.keras.layers import UpSampling2D, Conv2D\n    from tensorflow.keras.layers import Activation, ZeroPadding2D\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.layers import LeakyReLU\n    import zipfile\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from PIL import Image\n    from tqdm import tqdm\n    import os \n    import time\n    from skimage.io import imread\n    ```", "```\n    def time_string(sec_elapsed):\n        hour = int(sec_elapsed / (60 * 60))\n        minute = int((sec_elapsed % (60 * 60)) / 60)\n        second = sec_elapsed % 60\n        return \"{}:{:>02}:{:>05.2f}\".format(hour, minute, second)\n    ```", "```\n    gen_res = 3 \n    gen_square = 32 * gen_res\n    img_chan = 3\n    img_rows = 5\n    img_cols = 5\n    img_margin = 16\n    seed_vector = 200\n    data_path = 'banana-or-orange/training_set/'\n    epochs = 500\n    num_batch = 32\n    num_buffer = 60000\n    print(f\"Will generate a resolution of {gen_res}.\")\n    print(f\"Will generate {gen_square}px square images.\")\n    print(f\"Will generate {img_chan} image channels.\")\n    print(f\"Will generate {img_rows} preview rows.\")\n    print(f\"Will generate {img_cols} preview columns.\")\n    print(f\"Our preview margin equals {img_margin}.\")\n    print(f\"Our data path is: {data_path}.\")\n    print(f\"Our number of epochs are: {epochs}.\")\n    print(f\"Will generate a batch size of {num_batch}.\")\n    print(f\"Will generate a buffer size of {num_buffer}.\")\n    ```", "```\n    training_binary_path = os.path.join(data_path,\n            f'training_data_{gen_square}_{gen_square}.npy')\n    print(f\"Looking for file: {training_binary_path}\")\n    if not os.path.isfile(training_binary_path):\n        start = time.time()\n        print(\"Loading training images…\")\n        train_data = []\n        images_path = os.path.join(data_path,'banana')\n        for filename in tqdm(os.listdir(images_path)):\n            path = os.path.join(images_path,filename)\n            images = Image.open(path).resize((gen_square,\n                                              gen_square),\\\n                                             Image.ANTIALIAS)\n            train_data.append(np.asarray(images))\n        train_data = np.reshape(train_data,(-1,gen_square,\n                  gen_square,img_chan))\n        train_data = train_data.astype(np.float32)\n        train_data = train_data / 127–5 - 1.\n        print(\"Saving training image binary...\")\n        np.save(training_binary_path,train_data)\n        elapsed = time.time()-start\n        print (f'Image preprocess time: {time_string(elapsed)}')\n    else:\n        print(\"Loading training data...\")\n        train_data = np.load(training_binary_path)\n    ```", "```\n    train_dataset = tf.data.Dataset.from_tensor_slices(train_data) \\\n                           .shuffle(num_buffer).batch(num_batch)\n    ```", "```\n    def create_dc_generator(seed_size, channels):\n        model = Sequential()\n        model.add(Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n        model.add(Reshape((4,4,256)))\n        model.add(UpSampling2D())\n        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        model.add(UpSampling2D())\n        model.add(Conv2D(256,kernel_size=3,padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n\n        # Output resolution, additional upsampling\n        model.add(UpSampling2D())\n        model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        if gen_res>1:\n            model.add(UpSampling2D(size=(gen_res,gen_res)))\n            model.add(Conv2D(128,kernel_size=3,padding=\"same\"))\n            model.add(BatchNormalization(momentum=0.8))\n            model.add(Activation(\"relu\"))\n        # Final CNN layer\n        model.add(Conv2D(channels,kernel_size=3,padding=\"same\"))\n        model.add(Activation(\"tanh\"))\n        return model\n    ```", "```\n    def create_dc_discriminator(image_shape):\n        model = Sequential()\n        model.add(Conv2D(32, kernel_size=3, strides=2, \\\n                         input_shape=image_shape, \n                         padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n        model.add(Dense(1, activation='sigmoid'))\n        return model\n    ```", "```\n    def create_generator(seed_size, channels):\n        model = Sequential()\n        model.add(Dense(96*96*3,activation=\"tanh\",input_dim=seed_size))\n        model.add(Reshape((96,96,3)))\n        return model\n    ```", "```\n    def create_discriminator(img_size):\n        model = Sequential()\n        model.add(InputLayer(input_shape=img_size))\n        model.add(Dense(1024, activation=\"tanh\"))\n        model.add(Flatten())\n        model.add(Dense(1, activation='sigmoid'))\n        return model\n    ```", "```\n    def save_images(generator, cnt, noise, prefix=None):\n        img_array = np.full(( \n            img_margin + (img_rows * (gen_square+img_margin)), \n            img_margin + (img_cols * (gen_square+img_margin)), 3), \n            255, dtype=np.uint8)\n\n        gen_imgs = generator.predict(noise)\n        gen_imgs = 0.5 * gen_imgs + 0.5\n        img_count = 0\n        for row in range(img_rows):\n            for col in range(img_cols):\n                r = row * (gen_square+16) + img_margin\n                c = col * (gen_square+16) + img_margin\n                img_array[r:r+gen_square,c:c+gen_square] \\\n                    = gen_imgs[img_count] * 255\n                img_count += 1\n\n        output_path = os.path.join(data_path,'output')\n        if not os.path.exists(output_path):\n            os.makedirs(output_path)\n\n        filename = os.path.join(output_path,f\"train{prefix}-{cnt}.png\")\n        im = Image.fromarray(img_array)\n        im.save(filename)\n    ```", "```\n    dc_generator = create_dc_generator(seed_vector, img_chan)\n    noise = tf.random.normal([1, seed_vector])\n    gen_img = dc_generator(noise, training=False)\n    plt.imshow(gen_img[0, :, :, 0])\n    ```", "```\n    generator = create_generator(seed_vector, img_chan)\n    gen_van_img = generator(noise, training=False)\n    plt.imshow(gen_van_img[0, :, :, 0])\n    ```", "```\n    img_shape = (gen_square,gen_square,img_chan)\n    discriminator = create_discriminator(img_shape)\n    decision = discriminator(gen_img)\n    print (decision)\n    ```", "```\n    tf.Tensor([[0.4994658]], shape=(1,1), dtype=float32)\n    ```", "```\n    discriminator = create_discriminator(img_shape)\n    decision = discriminator(gen_img)\n    print(decision)\n    ```", "```\n    tf.Tensor([[0.5055983]], shape=(1,1), dtype=float32)\n    ```", "```\n    cross_entropy = tf.keras.losses.BinaryCrossentropy()\n    def discrim_loss(real_output, fake_output):\n        real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n        fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n        total_loss = real_loss + fake_loss\n        return total_loss\n    def gen_loss(fake_output):\n        return cross_entropy(tf.ones_like(fake_output), fake_output)\n    ```", "```\n    gen_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    disc_optimizer = tf.keras.optimizers.Adam(1.5e-4,0.5)\n    ```", "```\n    @tf.function\n    def train_step(generator, discriminator, images):\n        seed = tf.random.normal([num_batch, seed_vector])\n        with tf.GradientTape() as gen_tape, \\\n             tf.GradientTape() as disc_tape:\n             gen_imgs = generator(seed, training=True)\n            real_output = discriminator(images, training=True)\n            fake_output = discriminator(gen_imgs, training=True)\n            g_loss = gen_loss(fake_output)\n            d_loss = discrim_loss(real_output, fake_output)\n\n            gradients_of_generator = gen_tape.gradient(\\\n                g_loss, generator.trainable_variables)\n            gradients_of_discriminator = disc_tape.gradient(\\\n                d_loss, discriminator.trainable_variables)\n            gen_optimizer.apply_gradients(zip(\n                gradients_of_generator, generator.trainable_variables))\n            disc_optimizer.apply_gradients(zip(\n                gradients_of_discriminator, \n                discriminator.trainable_variables))\n        return g_loss,d_loss\n    ```", "```\n    def train(generator, discriminator, dataset, epochs, prefix=None):\n        fixed_seed = np.random.normal(0, 1, (img_rows * img_cols, \n                                             seed_vector))\n        start = time.time()\n        for epoch in range(epochs):\n             epoch_start = time.time()\n            g_loss_list = []\n            d_loss_list = []\n            for image_batch in dataset:\n                t = train_step(image_batch)\n                g_loss_list.append(t[0])\n                d_loss_list.append(t[1])\n            generator_loss = sum(g_loss_list) / len(g_loss_list)\n            discriminator_loss = sum(d_loss_list) / len(d_loss_list)\n            epoch_elapsed = time.time() - epoch_start\n            if (epoch + 1) % 100 == 0:\n                print (f'Epoch {epoch+1}, gen loss={generator_loss},\n            disc loss={discriminator_loss},'\\\n                       f' {time_string(epoch_elapsed)}')\n            save_images(epoch,fixed_seed)\n        elapsed = time.time()-start\n        print (f'Training time: {time_string(elapsed)}')\n    ```", "```\n    train(dc_generator, dc_discriminator, train_dataset, \\\n          epochs, prefix='-dc-gan')\n    ```", "```\n    train(generator, discriminator, train_dataset, epochs, \\\n          prefix='-vanilla')\n    ```", "```\n    a = imread('banana-or-orange/training_set/output'\\\n               '/train-dc-gan-99.png')\n    plt.imshow(a)\n    ```", "```\n    a = imread('/ banana-or-orange/training_set'\\\n               '/output/train-dc-gan-499.png')\n    plt.imshow(a)\n    ```", "```\n    a = imread('banana-or-orange/training_set'\\\n               '/output/train-vanilla-99.png')\n    plt.imshow(a)\n    ```", "```\n    a = imread('/ banana-or-orange/training_set'\\\n               '/output/train-vanilla-499.png')\n    plt.imshow(a)\n    ```"]