<html><head></head><body>
		<div id="_idContainer048">
			<h1 id="_idParaDest-34"><em class="italic"><a id="_idTextAnchor039"/>Chapter 2</em>: Variational Autoencoder</h1>
			<p>In the previous chapter, we looked at how a computer sees an image as pixels, and we devised a probabilistic model for pixel distribution for image generation. However, this is not the most efficient way to generate an image. Instead of scanning an image pixel by pixel, we first look at the image and try to understand what is inside. For example, a girl is sitting, wearing a hat, and smiling. Then we use that information to draw a portrait. This is how autoencoders work.</p>
			<p>In this chapter, we will first learn how to use an autoencoder to encode pixels into latent variables that we can sample from to generate images. Then we will learn how to tweak it to create a more powerful model known as a <strong class="bold">variational autoencoder</strong> (<strong class="bold">VAE</strong>). Finally, we will train our VAE to generate faces and perform face editing. The following topics will be covered in this chapter:</p>
			<ul>
				<li>Learning latent variables with autoencoders</li>
				<li>Variational autoencoders</li>
				<li>Generating faces with VAEs</li>
				<li>Controlling face attributes</li>
			</ul>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor040"/>Technical requirements</h1>
			<p>The Jupyter notebooks and codes can be found at <a href="https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter02">https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter02</a>.</p>
			<p>The notebooks used in this chapter are as follows: </p>
			<ul>
				<li><strong class="source-inline">ch2_autoencoder.ipynb</strong></li>
				<li><strong class="source-inline">ch2_vae_mnist.ipynb</strong></li>
				<li><strong class="source-inline">ch2_vae_faces.ipynb</strong></li>
			</ul>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor041"/>Learning latent variables with autoencoders</h1>
			<p>Autoencoders <a id="_idIndexMarker058"/>were first introduced in the 1980s, and one of the inventors is Geoffrey Hinton, who is one of the godfathers of modern <a id="_idIndexMarker059"/>deep learning. The hypothesis is that there are many redundancies in high-dimensional input space that can be compressed into some low-dimensional variables. There are traditional machine learning <a id="_idIndexMarker060"/>techniques such as <strong class="bold">Principal Component Analysis</strong> (<strong class="bold">PCA</strong>) for dimension reduction. </p>
			<p>However, in image generation, we will also want to restore the low dimension space into high dimension space. Although the way to do it is quite different, you can think of it like image compression where a raw image is compressed into a file format such as JPEG, which is small and easy to store and transfer. Then the computer can restore the JPEG into pixels that we can see and manipulate. In other words, the raw pixels are compressed into low-dimensional JPEG format and restored to high-dimensional raw pixels for display.</p>
			<p>Autoencoders are an <em class="italic">unsupervised machine learning</em> technique where no labels are needed to train the model. However, some call this <em class="italic">self-supervised</em> machine learning (<em class="italic">auto</em> means <em class="italic">self</em> in Latin) because we do need to use labels, and these labels are not annotated labels but the images themselves.</p>
			<p>The basic building <a id="_idIndexMarker061"/>blocks of autoencoders are <a id="_idIndexMarker062"/>an <strong class="bold">encoder</strong> and a <strong class="bold">decoder</strong>. The encoder <a id="_idIndexMarker063"/>is responsible for reducing high-dimensional input into some low-dimensional latent (hidden) variables. Although it is not clear from the name, the decoder <a id="_idIndexMarker064"/>is the block that converts latent variables back into high dimensional space. The encoder-decoder architecture is also used in other machine learning tasks, such as <strong class="bold">semantic segmentation</strong>, where the <a id="_idIndexMarker065"/>neural network first learns about the image representation, then produces pixel-level labels. The following diagram shows the general <a id="_idIndexMarker066"/>architecture of an autoencoder:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B14538_02_01.jpg" alt="Figure 2.1 – General autoencoder architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – General autoencoder architecture</p>
			<p>In the preceding image, the <strong class="bold">input</strong> and <strong class="bold">output</strong> are images of the same dimension, and <strong class="bold">z</strong> is the low dimensional latent vector. The <strong class="bold">encoder</strong> compresses input into <strong class="bold">z</strong>, and the <strong class="bold">decoder</strong> reverses the process to generate the output image.</p>
			<p>Having examined the <a id="_idIndexMarker067"/>overall architecture, let's look into how the encoder works.</p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor042"/>Encoder</h2>
			<p>The encoder is <a id="_idIndexMarker068"/>made up of multiple neural network layers, and it is <a id="_idIndexMarker069"/>best illustrated by using fully connected (dense) layers. We will now jump straight into building an encoder for the <strong class="source-inline">MNIST</strong> dataset, which has a dimension of 28x28x1. We need to set the dimension of latent variables, which is a 1D vector. We will stick to the convention and name the latent variables as <strong class="source-inline">z</strong>, as seen in the following code. </p>
			<p>The code can be found in <strong class="source-inline">ch2_autoencoder.ipynb</strong>:</p>
			<p class="source-code">def Encoder(z_dim):</p>
			<p class="source-code">    inputs  = layers.Input(shape=[28,28,1])</p>
			<p class="source-code">    x = inputs    </p>
			<p class="source-code">    x = Flatten()(x)</p>
			<p class="source-code">    x = Dense(128, activation='relu')(x)</p>
			<p class="source-code">    x = Dense(64, activation='relu')(x)</p>
			<p class="source-code">    x = Dense(32, activation='relu')(x)    </p>
			<p class="source-code">    z = Dense(z_dim, activation='relu')(x)</p>
			<p class="source-code">    </p>
			<p class="source-code">    return Model(inputs=inputs, outputs=z, name='encoder')</p>
			<p>The size of the latent <a id="_idIndexMarker070"/>variable should be smaller than the <a id="_idIndexMarker071"/>input dimension. It is a hyperparameter, and we will first try with 10, which will give us a compression rate of <em class="italic">28*28/10 = 78.4</em>. </p>
			<p>We will then use three fully connected layers with a decreasing number of neurons (<strong class="source-inline">128</strong>, <strong class="source-inline">64</strong>, <strong class="source-inline">32</strong>, and finally <strong class="source-inline">10</strong>, which is our <strong class="source-inline">z</strong> dimension). We can see in the following model summary that the feature sizes got squeezed from <strong class="source-inline">784</strong> gradually down to <strong class="source-inline">10</strong> in the network's output: </p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B14538_02_02.jpg" alt="Figure 2.2 – Model summary of our encoder&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Model summary of our encoder</p>
			<p>This network <a id="_idIndexMarker072"/>topology forces the model to learn what is important <a id="_idIndexMarker073"/>and discard less important features from layer to layer, to finally come down to the 10 most important features. If you come to think of it, this looks very similar to the <strong class="bold">CNN</strong> classification, where the feature map size reduces gradually as it traverses to the top layers. <strong class="bold">Feature map</strong> refers to the first two dimensions (height, width) of the tensor. </p>
			<p>As CNNs are more efficient and better suited for image inputs, we will build the encoder using <strong class="bold">convolutional layers</strong>. Old CNNs, such as <strong class="bold">VGG</strong>, used max pooling for feature map downsampling, but newer networks tend to achieve that by using stride of 2 in the convolutional layers. The following diagram illustrates the sliding of the convolutional kernel with a stride of 2 to produce a feature map that is half the size of the input feature map:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B14538_02_03.jpg" alt="Figure 2.3 – From left to right, the figure illustrates a convolutional operation working with an input stride of 2 "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – From left to right, the figure illustrates a convolutional operation working with an input stride of 2 </p>
			<p class="figure-caption">(Source: Vincent Dumoulin, Francesco Visin, “A guide to convolution arithmetic for deep learning” https://www.arxiv-vanity.com/papers/1603.07285/)</p>
			<p>In this <a id="_idIndexMarker074"/>example, we will <a id="_idIndexMarker075"/>use four convolutional layers with <strong class="source-inline">8</strong> filters and include an input stride of <strong class="source-inline">2</strong> for downsampling, as follows:</p>
			<p class="source-code">def Encoder(z_dim):</p>
			<p class="source-code">    inputs  = layers.Input(shape=[28,28,1])</p>
			<p class="source-code">    x = inputs    </p>
			<p class="source-code">    x = Conv2D(filters=8,  kernel_size=(3,3), strides=2,  	  	               padding='same', activation='relu')(x)</p>
			<p class="source-code">    x = Conv2D(filters=8,  kernel_size=(3,3), strides=1, 	 	               padding='same', activation='relu')(x)</p>
			<p class="source-code">    x = Conv2D(filters=8,  kernel_size=(3,3), strides=2,                	               padding='same', activation='relu')(x)</p>
			<p class="source-code">    x = Conv2D(filters=8,  kernel_size=(3,3), strides=1, 	 	               padding='same', activation='relu')(x)</p>
			<p class="source-code">    x = Flatten()(x)</p>
			<p class="source-code">    out = Dense(z_dim, activation='relu')(x)</p>
			<p class="source-code">    return Model(inputs=inputs, outputs=out, name='encoder')</p>
			<p>In a typical CNN architectures, the number of filters increases while the feature map size decreases. However, our <a id="_idIndexMarker076"/>objective is to reduce the <a id="_idIndexMarker077"/>dimension, hence I have kept the filter size as constant. This is sufficient for simple data such as MNIST, and it is fine to change the filter sizes as we move toward the latent variables. Lastly, we flatten the output of the last convolutional layer and feed it to a dense layer to output our latent variables.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor043"/>Decoder</h2>
			<p>If a decoder were a human, they would probably feel ill-treated. This is <a id="_idIndexMarker078"/>because the decoder does half of the work but only the encoder gets a place in the name. It should <a id="_idIndexMarker079"/>have been called auto-encoder-decoder!</p>
			<p>The job of the decoder is essentially the reverse of the encoder, which is to convert low-dimensional latent variables into high-dimensional output to look like the input image. There is no need for layers in the decoder to look like the encoder in reverse order. You could use completely different layers, for instance, dense layers only in the encoder and convolutional layers only in the decoder. Anyway, we will still use convolutional layers in our decoder to upsample feature maps from 7x7 to 28x28. The following code snippet shows the construction of the decoder:</p>
			<p class="source-code">def Decoder(z_dim):</p>
			<p class="source-code">    inputs  = layers.Input(shape=[z_dim])</p>
			<p class="source-code">    x = inputs    </p>
			<p class="source-code">    x = Dense(7*7*64, activation='relu')(x)</p>
			<p class="source-code">    x = Reshape((7,7,64))(x)</p>
			<p class="source-code">    x = Conv2D(filters=64, kernel_size=(3,3), strides=1,  	 	               padding='same', activation='relu')(x)</p>
			<p class="source-code">    x = UpSampling2D((2,2))(x)</p>
			<p class="source-code">    x = Conv2D(filters=32, kernel_size=(3,3), strides=1, 	 	               padding='same', activation='relu')(x)</p>
			<p class="source-code">    x = UpSampling2D((2,2))(x)    </p>
			<p class="source-code">    x = Conv2D(filters=32, kernel_size=(3,3), strides=2, 	 	               padding='same', activation='relu')(x)</p>
			<p class="source-code">    out = Conv2(filters=1, kernel_size=(3,3), strides=1,  	 	                padding='same', activation='sigmoid')(x)</p>
			<p class="source-code">    return Model(inputs=inputs, outputs=out, name='decoder') </p>
			<p>The first layer is a dense layer that takes in the latent variables and produces a tensor with a size of [7 x 7 x the <a id="_idIndexMarker080"/>number of filters] of our first <a id="_idIndexMarker081"/>convolutional layer. Unlike the encoder, the objective of the decoder is not to reduce dimensionality, thus we could and should use more filters to give it more generative capacity.</p>
			<p><strong class="source-inline">UpSampling2D</strong> interpolates the pixels to increase the resolution. It is an affine transformation (linear multiplications and additions), therefore it could <strong class="bold">backpropagate</strong>, but it uses fixed weights and is therefore is not trainable. Another popular upsampling method is to use the <strong class="bold">transpose convolutional layer</strong>, which is trainable, but it can create checkerboard-like artifacts in <a id="_idIndexMarker082"/>the generated image. You can read more at <a href="https://distill.pub/2016/deconv-checkerboard/">https://distill.pub/2016/deconv-checkerboard/</a>. </p>
			<p>The checkerboard artifacts are more obvious for low-dimension images or when you zoom into an image. The effect can be reduced by using an even-numbered convolutional kernel size, for example, 4 rather than the more popular size of 3. Therefore, recent image generative models tend to not use transpose convolution. We will be using <strong class="source-inline">UpSampling2D</strong> throughout the rest of the book. The following table shows the model summary of the decoder:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B14538_02_04.jpg" alt="Figure 2.4 – Model summary of the decoder&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Model summary of the decoder</p>
			<p class="callout-heading">Tips</p>
			<p class="callout">When designing a CNN, it is <a id="_idIndexMarker083"/>important to know how to work out the convolutional layer's output <a id="_idIndexMarker084"/>tensor shape. If <strong class="source-inline">padding='same'</strong> is used, the output feature map will have the same size (height and width) as the input feature map. If <strong class="source-inline">padding='valid'</strong> is used instead, then the output size may be slightly smaller depending on the filter kernel dimension. When input <strong class="source-inline">stride = 2</strong> is used together with the same padding, the feature map size is halved. Lastly, the channel number of the output tensor is the same as the convolutional filter number. For example, if the input tensor has a shape of (28,28,1) and goes through <strong class="source-inline">conv2d(filters=32, strides=2, padding='same')</strong>, we know the output will have a shape of (14,14, 32).</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor044"/>Building an autoencoder</h2>
			<p>Now we are ready to put the <a id="_idIndexMarker085"/>encoder and decoder together to create an autoencoder. First, we instantiate the encoder and decoder separately. We then feed the encoder's output into the decoder's input, and we instantiate a <strong class="source-inline">Model</strong> using the encoder's input and the decoder's output as follows:</p>
			<p class="source-code">z_dim = 10</p>
			<p class="source-code">encoder = Encoder(z_dim)</p>
			<p class="source-code">decoder = Decoder(z_dim) </p>
			<p class="source-code">model_input = encoder.input</p>
			<p class="source-code">model_output = decoder(encoder.output)</p>
			<p class="source-code">autoencoder = Model(model_input, model_output)</p>
			<p>A deep neural network can look complex and scary to build. However, we could break it down into smaller blocks or modules, then put them together later. The whole task becomes more manageable! For training, we will use L2 loss, this is implemented using <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>) to compare <a id="_idIndexMarker086"/>each of the pixels between the output and expected result. In this example, I have added in some callback functions that will be called after training every epoch as follows:</p>
			<ul>
				<li><strong class="source-inline">ModelCheckpoint(monitor='val_loss')</strong> to save the model if the validation loss is lower than in earlier epochs.</li>
				<li><strong class="source-inline">EarlyStopping(monitor='val_loss', patience = 10)</strong> to stop the training earlier if the validation loss has not improved for 10 epochs.</li>
			</ul>
			<p>The image generated is as follows:</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B14538_02_05.jpg" alt="Figure 2.5 – The first row is the input image and the second row is generated by the autoencoder&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – The first row is the input image and the second row is generated by the autoencoder</p>
			<p>As you can see, the first row is the input image and the second row is generated by our autoencoder. We can see that the generated images are a bit blurry; that is probably because we have <a id="_idIndexMarker087"/>compressed it too much and some data information is lost during the process.</p>
			<p>To confirm our suspicion, we increase the latent variable dimension from 10 to 100 and generate the output and the result is as follows:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B14538_02_06.jpg" alt="Figure 2.6 – Image generated by autoencoder with z_dim = 100&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – Image generated by autoencoder with z_dim = 100</p>
			<p>As you can see, the generated images now look a lot sharper!</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor045"/>Generating images from latent variables</h2>
			<p>So, how <a id="_idIndexMarker088"/>do we use an autoencoder? It is not very <a id="_idIndexMarker089"/>useful to have an AI model to convert an image into a blurrier version of itself. One of the first applications of autoencoders is image denoising, where we add some noise into the input image and train the model to produce a clean image. However, we are more interested in using it to generate images. So, let's see how we can do it. </p>
			<p>Now that we have a trained autoencoder, we can ignore the encoder and use only the decoder to sample from the latent variables to generate images (See? The decoder deserves more recognition because it will still need to keep working after completing the training). The first challenge we face is working out how we sample from the latent variables. As we did not use any activation in the last layer before the latent variables, the latent space is unbounded and can be any real floating numbers, and there are hundreds of them! </p>
			<p>To illustrate how this should work, we will train another autoencoder using <strong class="source-inline">z_dim=2</strong> so we can explore the latent space in two dimensions. The following graph shows the plot of the latent space:</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B14538_02_07.jpg" alt="Figure 2.7 – Plot of latent space. A color version is available in the Jupyter notebook&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – Plot of latent space. A color version is available in the Jupyter notebook</p>
			<p>The <a id="_idIndexMarker090"/>plot was generated by passing 1,000 <a id="_idIndexMarker091"/>samples into the trained encoder and plotting the two latent variables on the scatter plot. The color bar on the right indicates the intensity of the digit labels. We can observe the following from the plots:</p>
			<ul>
				<li>The latent variables sit roughly between <strong class="bold">–5</strong> and <strong class="bold">+4</strong>. We won't know the exact range unless we create this plot and look at it. This can change when you train the model again, and quite often the samples can scatter more widely beyond +-10.</li>
				<li>The classes are not distributed uniformly. You can see clusters in the top left and on the right that are well separated from other classes (refer to the color version in the Jupyter notebook). However, the classes at the center of the plot tend to be more densely packed and overlap with each other.</li>
			</ul>
			<p>You <a id="_idIndexMarker092"/>might be able to see the non-uniformity <a id="_idIndexMarker093"/>better in the following images, which were generated by sweeping the latent variables from <strong class="bold">–5</strong> to <strong class="bold">+5</strong> with a 1.0 interval:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B14538_02_08.jpg" alt="Figure 2.8 - Images generated by sweeping the two latent variables&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 - Images generated by sweeping the two latent variables</p>
			<p>We can see that digits 0 and 1 are well represented in the sample distribution and they are nicely drawn too. It is not the case for digits in the center, which are blurry, and some digits are even missing from the samples. The latter shows the shortcoming where there is very little variation in generated images for those classes.</p>
			<p>It's not all bad. If you look closer, you can see how the digit 1 morphs into 7, then to 9 and <a id="_idIndexMarker094"/>4, and that is interesting! It looks like the <a id="_idIndexMarker095"/>autoencoder has learned some relationship between the latent variables. It might be that the digits with round appearances are mapped into the latent space toward the top-right corner, while digits that look more like a stick sit on the left-hand side. That is good news!</p>
			<p class="callout-heading">Fun</p>
			<p class="callout">There is a widget in the notebook that allows you to slide the latent variable bars to generate images interactively. Have fun!</p>
			<p>In the coming section, we will see how we can use a VAE to solve the distribution issue in the latent space.</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor046"/>Variational autoencoders</h1>
			<p>In an autoencoder, the decoder <a id="_idIndexMarker096"/>samples directly from latent variables. <strong class="bold">Variational autoencoders</strong> (<strong class="bold">VAEs</strong>), which were invented in 2014, differ in that the sampling is taken from a distribution parameterized by the latent variables. To be clear, let's say we have an autoencoder with two latent variables, and we draw samples randomly and get two samples of 0.4 and 1.2. We then send them to the decoder to generate an image. </p>
			<p>In a VAE, these samples don't go to the decoder directly. Instead, they are used as a mean and variance of a <strong class="bold">Gaussian distribution</strong>, and we draw samples from this distribution to be sent to the decoder for image generation. As this is one of the most important distributions in machine learning, so let's go over some basics of Gaussian distributions before creating a VAE.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor047"/>Gaussian distribution</h2>
			<p>A Gaussian <a id="_idIndexMarker097"/>distribution is characterized by two parameters – <strong class="bold">mean</strong> and <strong class="bold">variance</strong>. I think we <a id="_idIndexMarker098"/>are all familiar with the different <a id="_idIndexMarker099"/>bell curves shown in the following graph. The bigger the standard deviation (the square root of the variance), the larger the spread:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B14538_02_09.jpg" alt="Figure 2.9 – Gaussian distribution probability density function with different standard deviations&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – Gaussian distribution probability density function with different standard deviations</p>
			<p>We can use the <img src="image/Formula_02_001.png" alt=""/> notation to describe a univariate Gaussian distribution, where <em class="italic">µ</em> is the mean and <img src="image/Formula_02_002.png" alt=""/> is the standard deviation.</p>
			<p>The mean tells us where the peak is: it is the value that has the highest probability density, in other words, the most frequent value. If we are to draw samples of pixel location (x,y) of an image, and each x and y have different Gaussian distributions, then we have a <em class="italic">multivariate</em> Gaussian distribution. In this case, it is a <em class="italic">bivariate</em> distribution. </p>
			<p>The mathematical equations of a multivariate Gaussian distribution can look quite intimidating, so I'm not going to put them in here. The only thing we need to know is that we now incorporate <a id="_idIndexMarker100"/>standard deviations into the covariance matrix. The diagonal elements in the covariance matrix are simply the standard deviations of individual Gaussian distributions. The other elements measure the covariance between two Gaussian distributions, that is, the correlation between them.</p>
			<p>The following graph shows us the bivariate Gaussian samples without correlation:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B14538_02_10.jpg" alt="Figure 2.10 – Samples from a bivariate Gaussian distribution with no correlation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – Samples from a bivariate Gaussian distribution with no correlation</p>
			<p>We can see that when the standard deviation of one dimension increases from 1 to 4, the spread increases only in that dimension (the <em class="italic">y</em> axis) without affecting the others. Here, we say the two <a id="_idIndexMarker101"/>Gaussian distributions are <strong class="bold">identically and independently distributed</strong> (abbreviated as <strong class="bold">iid</strong>).</p>
			<p>Now, in the second example, the plot on the left shows that the covariance is non-zero and positive, which means when the density increases in one dimension, the other dimension will follow suit and they are correlated. The plot on the right shows a negative correlation: </p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B14538_02_11.jpg" alt="Figure 2.11 – Samples from a bivariate Gaussian distribution with correlation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – Samples from a bivariate Gaussian distribution with correlation</p>
			<p>Here is some good <a id="_idIndexMarker102"/>news for you: Gaussian distributions in VAEs are assumed to be iid and therefore do not require covariance matrix to describe the correlation between the variables. As a result, we need just <em class="italic">n</em>-pairs of mean and variance to describe our multivariate Gaussian distribution. What we hope to achieve is to create a nicely distributed latent space where latent variables' distributions for different data classes are as follows:</p>
			<ul>
				<li>Evenly spread so we have a better variation to sample from</li>
				<li>Overlap slightly with each other to create a continuous transition</li>
			</ul>
			<p>This can be illustrated with the following plot:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B14538_02_12.jpg" alt="Figure 2.12 – Four samples drawn from a multivariate Gaussian distribution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12 – Four samples drawn from a multivariate Gaussian distribution</p>
			<p>Next, we will learn how to incorporate Gaussian distribution sampling into a VAE.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor048"/>Sampling latent variables</h2>
			<p>When <a id="_idIndexMarker103"/>we train an autoencoder, the encoded latent variables go straight to the decoder. With a VAE, there is an additional sampling step between the encoder and the decoder. The encoder produces the mean and variance of Gaussian distributions as latent variables, and we draw samples from them to send to the decoder. The problem is, sampling is not back-propagatable and therefore is not trainable.</p>
			<p class="callout-heading">Backpropagation</p>
			<p class="callout">For those who are not familiar with the fundamentals of deep learning, a neural network is trained using <strong class="bold">backpropagation</strong>. One of the steps is to calculate the gradients of the loss with respect to the network weights. Therefore, all operations must be differentiable for backpropagation to work.</p>
			<p>To solve this, we can employ a simple <em class="italic">reparameterization trick</em> where we cast the Gaussian random variable <em class="italic">N</em> (mean, variance) into <em class="italic">mean + sigma * N(0, 1)</em>. In other words, we first sample from a standard Gaussian distribution of N(0,1), then multiply it with sigma then add mean to it. As you can see in the following diagram, the sampling becomes an affine transformation (which is composed of only add and multiplication operations) and the error could backpropagate from the output back to the encoder:</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B14538_02_13.jpg" alt="Figure 2.13 – Gaussian sampling in a VAE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – Gaussian sampling in a VAE</p>
			<p>The <a id="_idIndexMarker104"/>sampling from a standard Gaussian distribution <strong class="bold">N(0,1)</strong> can be seen as input to the VAE, and we do not need to backpropagate back to inputs. However, we will put the <strong class="bold">N(0,1)</strong> sampling inside our model. Now that we understand how sampling works, we can now go and build our VAE model.</p>
			<p>Let's now implement the sampling as a custom layer, as shown in the following snippet: </p>
			<p class="source-code">class GaussianSampling(Layer):        </p>
			<p class="source-code">    def call(self, inputs):</p>
			<p class="source-code">        means, logvar = inputs</p>
			<p class="source-code">        epsilon = tf.random.normal(shape=tf.shape(means), 	 	                                   mean=0., stddev=1.)</p>
			<p class="source-code">        samples = means + tf.exp(0.5*logvar)*epsilon</p>
			<p class="source-code">        return samples</p>
			<p>Note that we use log variance in the encoder space rather than variance for numerical stability. By definition, variance is a positive number, but unless we use an activation function such as <strong class="source-inline">relu</strong> to constrain it, the variance of latent variables can become negative. Furthermore, the <a id="_idIndexMarker105"/>variance can vary greatly, say from 0.01 to 100, which can make it difficult to train. However, the natural log of those values is -4.6 and +4.6, which is a smaller range. Nevertheless, we will need to convert the log variance into the standard deviation when doing sampling, hence the <strong class="source-inline">tf.exp(0.5*logvar)</strong> code.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">There are a few ways to construct models in TensorFlow. One is to use the <strong class="source-inline">Sequential</strong> class to add layers sequentially. The input of the last layer goes into the next layer; therefore, you don't need to specify the input for the layer. While this is convenient, you can't use this on models that have branches. The next is to use the <strong class="bold">Functional API</strong>, where you start from the input, chain the layers, and specify the input to each layer. This is flexible and is what we used to build our autoencoder. However, <strong class="source-inline">tf.random.normal()</strong> will fail in eager execution mode, which is the default mode of TensorFlow 2 for creating a dynamic graph. This is because the function needs to know the batch size to generate the random numbers, but it is unknown as we create the layers. Thus, we would get an error in our Jupyter notebook when trying to draw a sample by passing in a size of <strong class="source-inline">(None, 2)</strong>. As a result, we will switch our model creation to using <strong class="bold">subclassing</strong>, which we have used to create custom layers. By the time <strong class="source-inline">call()</strong> is run, we will already know the batch size and hence complete the information of the shape.</p>
			<p>Now we reconstruct our encoder using the <strong class="bold">subclassing</strong> method. The layers can be created in either <strong class="source-inline">__init__()</strong> or in<strong class="source-inline"> __built__()</strong> if we need to use the input shape to construct the layers. Within the subclass, we use the <strong class="source-inline">Sequential</strong> class to create a block of convolutional layers conveniently as we don't need to read any intermediate tensors:</p>
			<p class="source-code">class Encoder(Layer):</p>
			<p class="source-code">    def __init__(self, z_dim, name='encoder'):</p>
			<p class="source-code">        super(Encoder, self).__init__(name=name)        </p>
			<p class="source-code">        self.features_extract = Sequential([</p>
			<p class="source-code">            Conv2D(filters=8,  kernel_size=(3,3), strides=2,  	                   padding='same', activation='relu'),</p>
			<p class="source-code">            Conv2D(filters=8,  kernel_size=(3,3), strides=1, 	                   padding='same', activation='relu'),</p>
			<p class="source-code">            Conv2D(filters=8,  kernel_size=(3,3), strides=2,  	                   padding='same', activation='relu'),</p>
			<p class="source-code">            Conv2D(filters=8,  kernel_size=(3,3), strides=1, 	                   padding='same', activation='relu'),</p>
			<p class="source-code">            Flatten()])</p>
			<p class="source-code">        self.dense_mean = Dense(z_dim, name='mean')</p>
			<p class="source-code">        self.dense_logvar = Dense(z_dim, name='logvar')</p>
			<p class="source-code">        self.sampler = GaussianSampling()</p>
			<p>We then use <a id="_idIndexMarker106"/>two dense layers to predict the mean and log variance of <strong class="source-inline">z</strong> from the extracted features. Latent variables are sampled and return as output together with the mean and log variance for the loss calculation. The decoder is identical to the autoencoder except that we now re-write it using subclassing:</p>
			<p class="source-code">    def call(self, inputs):</p>
			<p class="source-code">        x = self.features_extract(inputs)</p>
			<p class="source-code">        mean = self.dense_mean(x)</p>
			<p class="source-code">        logvar = self.dense_logvar(x)</p>
			<p class="source-code">        z = self.sampler([mean, logvar])</p>
			<p class="source-code">        return z, mean, logvar</p>
			<p>Now the <a id="_idIndexMarker107"/>encoder block is completed. The decoder block design is unchanged from the autoencoder, so what is left to be done is to define a new loss function.</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor049"/>Loss function</h2>
			<p>We can now <a id="_idIndexMarker108"/>sample from a multivariate Gaussian distribution, but there is still no guarantee that the Gaussian blobs won't be far apart from each other and widely spread. The way VAEs do this is by putting in some regularization to encourage the Gaussian distribution to look like N(0,1). In other words, we want them to have a mean close to 0 to keep them close together, and variance close to 1 for a better variation to sample <a id="_idIndexMarker109"/>from. This is done by using <strong class="bold">Kullback-Leibler divergence (KLD)</strong>.</p>
			<p>KLD is a measurement of how different one probability distribution is to another. For two distributions, <em class="italic">P</em> and <em class="italic">Q</em>, the KLD of <em class="italic">P</em> with respect to <em class="italic">Q</em> is the cross-entropy of <em class="italic">P</em> and <em class="italic">Q</em> minus the entropy of <em class="italic">P</em>. In information theory, entropy is a measure of information or the uncertainty of a random variable:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/Formula_02_003.jpg" alt=""/>
				</div>
			</div>
			<p>Without going into the mathematical details, KLD is proportional to cross-entropy, hence minimizing cross-entropy will also minimize KLD. When KLD is zero, then the two distributions are identical. It suffices to say that there is a closed-form solution for KLD when the distribution to compare with is a standard Gaussian. This can be calculated directly from the following means and variances:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/Formula_02_004.jpg" alt=""/>
				</div>
			</div>
			<p>We create custom loss function that takes in labels and network output to calculate the KL loss. I have used <strong class="source-inline">tf.reduce_mean()</strong> instead of <strong class="source-inline">tf.reduce_sum()</strong> to normalize it to the number of latent space dimensions. This doesn't really matter as the KL loss is multiplied by a hyperparameter, which we will discuss shortly:</p>
			<p class="source-code">def vae_kl_loss(y_true, y_pred):</p>
			<p class="source-code">    kl_loss =  - 0.5 * tf.reduce_mean(vae.logvar - tf.exp(vae.logvar) - tf.square(vae.mean) - + 1)</p>
			<p class="source-code">    return kl_loss    </p>
			<p>The <a id="_idIndexMarker110"/>other loss function is what we have used in the autoencoder to compare the generated images with the label images. This is also <a id="_idIndexMarker111"/>called the <strong class="bold">reconstruction loss</strong>, which measures the difference in reconstructed images with the target image, hence the name. This <a id="_idIndexMarker112"/>can be either <strong class="bold">binary cross-entropy</strong> (<strong class="bold">BCE</strong>) or <strong class="bold">mean squared error</strong> (<strong class="bold">MSE</strong>). MSE tends <a id="_idIndexMarker113"/>to generate sharper images as it penalizes more severely for pixels that deviate from the label (by squaring the error):</p>
			<p class="source-code">def vae_rc_loss(y_true, y_pred):</p>
			<p class="source-code">    rc_loss = tf.keras.losses.MSE(y_true, y_pred)</p>
			<p class="source-code">    return rc_loss</p>
			<p>Finally, we add the two losses together:</p>
			<p class="source-code">def vae_loss(y_true, y_pred):</p>
			<p class="source-code">    kl_loss = vae_kl_loss(y_true, y_pred)</p>
			<p class="source-code">    rc_loss = vae_rc_loss(y_true, y_pred)</p>
			<p class="source-code">    kl_weight_factor = 1e-2</p>
			<p class="source-code">    return kl_weight_factor*kl_loss + rc_loss</p>
			<p>Now, let's talk about <strong class="source-inline">kl_weight_factor</strong>, which is an important hyperparameter that is often neglected in VAE examples or tutorials. As we can see, the total loss is made up of the KL loss and the reconstruction loss. The background of MNIST digits is black, and therefore the reconstruction loss is relatively low even though the network hasn't learned much and only outputs all zeroes. </p>
			<p>Comparatively, the distribution of latent variables is all over the place at the beginning, and therefore the gain in reducing the KLD outweighs that of reducing the reconstruction loss. This encourages the network to ignore the reconstruction loss and optimize only for the KLD loss. As a result, the latent variables will have a perfect standard Gaussian distribution of N(0,1) but the generated images will look nothing like the training images, and that is a disaster for a generative model!</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The encoder is discriminative in that it tries to spot differences in the images. We can think of each latent variable as a feature. If we use two latent variables for MNIST digits, they could mean <em class="italic">round</em> or <em class="italic">straight</em>. When a decoder sees a digit, it predicts the likelihood of whether they are round or straight by using the means and variances. If a neural network is forced to make the KLD loss 0, the distribution of the latent variables will be identical – the center at 0 with a variance of 1. In other words, it is equally likely to be round and straight. Hence, the encoder loses its discriminative capacity. When this happens, you will see the decoder produces the same image every time and they look like the average pixel values. </p>
			<p>Before we <a id="_idIndexMarker114"/>move on to the next part, I suggest you go to <strong class="source-inline">ch2_vae_mnist.ipynb</strong> and try a different <strong class="source-inline">kl_weight_factor</strong> with <strong class="source-inline">VAE(z_dim=2)</strong> to look at the latent variable distribution after training. You can also try to increase <strong class="source-inline">kl_weight_factor</strong> to see how it stops the VAE from learning to generate, and then look at the generated images and distributions again.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor050"/>Generating faces with VAEs</h1>
			<p>Now that you <a id="_idIndexMarker115"/>have learned the theory of VAEs and have built one for MNIST, it is <a id="_idIndexMarker116"/>time to grow up, ditch the toy, and generate some serious stuff. We will use VAE to generate some faces. Let's get started! The code is in <strong class="source-inline">ch2_vae_faces.ipynb</strong>. There are a few face datasets available for training:</p>
			<ul>
				<li>Celeb A (<a href="http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html">http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html</a>). This is a popular dataset in <a id="_idIndexMarker117"/>academia as it contains annotations of face attributes, but unfortunately it is not available for commercial use.</li>
				<li><strong class="bold">Flickr-Faces-HQ Dataset</strong> (<strong class="bold">FFHQ</strong>) (<a href="https://github.com/NVlabs/ffhq-dataset">https://github.com/NVlabs/ffhq-dataset</a>). This dataset is freely <a id="_idIndexMarker118"/>available for commercial use and contains high-resolution images.</li>
			</ul>
			<p>In this exercise, we will <a id="_idIndexMarker119"/>only assume the dataset <a id="_idIndexMarker120"/>contains RGB images; feel free to use any dataset that suits your needs.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor051"/>Network architecture</h2>
			<p>We <a id="_idIndexMarker121"/>reuse the <strong class="bold">MNIST VAE</strong> and training pipeline with some modifications given that the dataset is now different from MNIST. Feel free to reduce the layers, parameters, image size, epoch number, and batch size to suit your computing power. The modifications are as follows:</p>
			<ul>
				<li>Increase the latent space dimension to 200. </li>
				<li>The input shape is changed from (28,28,1) to (112,112,3) as we now have 3 color channels instead of grayscale. Why 112? Early CNNs such as VGG use the input size of 224x224 and set the standard for image classification CNNs. We don't want to use too-high resolutions now as we have not mastered the skills needed to generate high-resolution images. Therefore, I picked 224/2 = 112, but you could use any even values.</li>
				<li>Add image resizing in the pre-processing pipeline. We add more downsampling layers. In MNIST, the encoder downsamples twice, from 28 to 14 to 7. As we have a higher resolution to start with, we need to downsample four times in total.</li>
				<li>As the dataset is more complex, we increase the number of filters to increase the network capacity. Therefore, the convolutional layers in encoders are as follows. It is similar for the decoder but in the reverse direction. Instead of downsampling, the convolutional <a id="_idIndexMarker122"/>layers upsample the feature maps by striding:<p>a) <strong class="source-inline">Conv2D(filters = 32, kernel_size=(3,3), strides = 2)</strong></p><p>b) <strong class="source-inline">Conv2D(filters = 32, kernel_size=(3,3), strides = 2)</strong></p><p>c) <strong class="source-inline">Conv2D(filters = 64, kernel_size=(3,3), strides = 2)</strong></p><p>d) <strong class="source-inline">Conv2D(filters = 64, kernel_size=(3,3), strides = 2)</strong></p><p class="callout-heading">Tips</p><p class="callout">Although we use the overall loss, that is, the KLD loss and the reconstruction loss in network training, we should only use the reconstruction loss as a metric to monitor when to save the model and early termination of the training. The KLD loss acts as regularization, but we are more interested in the reconstructed image's quality.</p></li>
			</ul>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor052"/>Facial reconstruction</h2>
			<p>Let's look at <a id="_idIndexMarker123"/>the following reconstructed images:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B14538_02_14.jpg" alt=" Figure 2.14 – Reconstructed images with a VAE&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 2.14 – Reconstructed images with a VAE</p>
			<p>They do look good despite not being a perfect reconstruction. The VAE has managed to learn some features from the input image and use that to paint a new face. It looks like the VAE is better at reconstructing female faces. This is not surprising as we have seen the <em class="italic">mean face</em> in <a href="B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with Image Generation Using TensorFlow</em>, which is of female appearance due to the higher proportion of females in the dataset. That is why mature men were given a younger, more feminine complexion. </p>
			<p>The image background is also interesting. As the image backgrounds are so diverse, it was not possible for the encoder to encode every fine detail into low dimensions, so we can see the VAE encodes the background colors and the decoder creates a blurry backdrop based on those colors.</p>
			<p>One fun thing to share with you, when the KL weight factor is too high and the VAE doesn't learn, then the <em class="italic">mean face</em> will come back to haunt you again. This is as if the VAE's <a id="_idIndexMarker124"/>encoder was blinded and told the decoder <em class="italic">“Hey, I can't see anything, just draw me a person”</em>, and then the decoder draws a portrait of what it thinks an average person looks like.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor053"/>Generating new faces</h2>
			<p>To <a id="_idIndexMarker125"/>generate a new image, we create random numbers from the standard Gaussian distribution and feed it to the decoder, as shown in the following code snippet:</p>
			<p class="source-code">z_samples = np.random.normal(loc=0, scale=1, size=(image_num, </p>
			<p class="source-code">                                                        z_dim))</p>
			<p class="source-code">images = vae.decoder(z_samples.astype(np.float32))</p>
			<p>And most of the generated faces look horrible! </p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B14538_02_15.jpg" alt=" Figure 2.15 – Faces generated with standard normal sampling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 2.15 – Faces generated with standard normal sampling</p>
			<p>We can improve the image fidelity by using a <strong class="bold">sampling trick</strong>.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor054"/>Sampling trick</h2>
			<p>We have just seen that the <a id="_idIndexMarker126"/>trained VAE could reconstruct the faces rather well. My suspicion <a id="_idIndexMarker127"/>was that there was something not quite right in samples generated by random sampling. To debug this problem, I fed in a few thousand images into the VAE decoder to collect the latent space means and variance. Then I plotted the average mean of each latent space variable, and the following is what I got:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B14538_02_16.jpg" alt="Figure 2.16 – Average mean of the latent variable&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.16 – Average mean of the latent variable</p>
			<p>In theory, they should center at 0 and have a variance of 1, but they may not due to suboptimal KLD weight and stochasticity in the network training. Because of this, the randomly generated samples do not always match the distribution expected by the decoder. This is the trick I use to generate samples. Using steps similar to the preceding ones, I have collected the average standard deviation of latent variables (one scalar value), which I use for generating normally distributed samples (200 dimensions). Then I added the average mean (200 dimensions) to it.</p>
			<p>Ta-da! Now <a id="_idIndexMarker128"/>they look a lot better and <a id="_idIndexMarker129"/>sharper!</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B14538_02_17.jpg" alt="Figure 2.17 – Faces generated with the sampling trick&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.17 – Faces generated with the sampling trick</p>
			<p>Instead of generating random faces, in the next section we will learn how to perform face editing.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor055"/>Controlling face attributes</h1>
			<p>Everything we <a id="_idIndexMarker130"/>have done in this chapter serves only one purpose: to prepare us for <strong class="bold">face editing</strong>! This is the climax of this chapter! </p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor056"/>Latent space arithmetic</h2>
			<p>We have talked <a id="_idIndexMarker131"/>about the latent space several times now but haven't given it a proper definition. Essentially, it means every possible value of the latent variables. In our VAE, it is a vector of 200 dimensions, or simply 200 variables. As much as we hope each variable has a distinctive semantic meaning to us, such as <em class="italic">z[0]</em> is for eyes, <em class="italic">z[1]</em> dictates the eye color, and so on, things are never that straightforward. We will simply have to assume the information is encoded in all the latent vectors and we can use vector arithmetic to explore the space.</p>
			<p>Before diving into high-dimensional space, let's try to understand it using a two-dimensional example. Imagine you are now at point <em class="italic">(0,0)</em> on a map and your home is at <em class="italic">(x,y)</em>. Therefore, the direction toward your home is <em class="italic">(x – 0 ,y - 0)</em> divided by the L2 norm of <em class="italic">(x,y)</em>, or let's denote the direction as <em class="italic">(x_dot, y_dot)</em>. Therefore, whenever you move <em class="italic">(x_dot, y_dot)</em>, you are moving toward your house; and when you move <em class="italic">(-2*x_dot, -2*y_dot)</em>, you are moving further away from home with twice as many steps.</p>
			<p>Now, if we know the <a id="_idIndexMarker132"/>direction vector of the <strong class="source-inline">smiling</strong> attributes, we could add that to the latent variables to make the face smile:</p>
			<p class="source-code">new_z_samples = z_samples +  smiling_magnitude*smiling_vector</p>
			<p><strong class="source-inline">smiling_magnitude</strong> is a scalar value that we set, so the next step is to work out the way to obtain <strong class="source-inline">smiling_vector</strong>.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor057"/>Finding attribute vectors</h2>
			<p>Some datasets, such as Celeb A, come with annotations of facial attributes for each image. The labels are <a id="_idIndexMarker133"/>binary, meaning they indicate whether a certain attribute exists or not in the image. We will use the labels and the encoded latent variables to find our direction vectors! The idea is simple:</p>
			<ol>
				<li>Use the test dataset or a few thousand samples from the training dataset and use the VAE decoder to generate the latent vectors.</li>
				<li>Separate the latent vectors into two groups: with (positive) or without (negative) the one attribute we are interested in.</li>
				<li>Calculate the average of the positive vectors and negative vectors separately.</li>
				<li>Obtain the attribute direction vector by subtracting the average negative vector from the average positive vector.</li>
			</ol>
			<p>The pre-processing function is modified to return the label of the attribute we are interested in. We then use a <strong class="source-inline">lambda</strong> function to map to the data pipeline:</p>
			<p class="source-code">def preprocess_attrib(sample, attribute):</p>
			<p class="source-code">    image = sample['image']</p>
			<p class="source-code">    image = tf.image.resize(image, [112,112])</p>
			<p class="source-code">    image = tf.cast(image, tf.float32)/255.</p>
			<p class="source-code">    return image, sample['attributes'][attribute]</p>
			<p class="source-code">ds = ds.map(lambda x: preprocess_attrib(x, attribute))</p>
			<p>Not to be confused with the Keras Lambda layer that wraps arbitrary TensorFlow functions into a Keras layer, the <strong class="source-inline">lambda</strong> in the code is a generic Python expression. The <strong class="source-inline">lambda</strong> function is used as a small function but without the overhead code to define the function. The <strong class="source-inline">lambda</strong> function in the preceding code is equivalent to the following function:</p>
			<p class="source-code">def preprocess(x):  </p>
			<p class="source-code">    return preprocess_attrib(x, attribute))</p>
			<p>When <a id="_idIndexMarker134"/>chaining <strong class="source-inline">map</strong> to the dataset, the dataset object will read each image sequentially and call the <strong class="source-inline">lambda</strong> function equivalent to <strong class="source-inline">preprocess(image)</strong>.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor058"/>Face editing</h2>
			<p>With the attribute <a id="_idIndexMarker135"/>vectors extracted, we can now do the magic:</p>
			<ol>
				<li value="1">First, we take an image from the dataset, which is the leftmost face from the following screenshot.</li>
				<li>We encode the face into latent variables, then decode it to generate a new face, which we place in the middle of the row. </li>
				<li>Then we add the attribute vector increasingly toward the right.</li>
				<li>Similarly, we minus the attribute vector while going toward the left of the row. </li>
			</ol>
			<p>The following screenshot shows the generated images by interpolating the latent vector for male, chubby, moustache, smiling, and glasses:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B14538_02_18.jpg" alt="Figure 2.18 – Changing facial features by exploring latent space&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.18 – Changing facial features by exploring latent space</p>
			<p>The transitions <a id="_idIndexMarker136"/>were rather smooth. You should have noticed that these attributes are not exclusive to each other. For example, as we increase the moustache-ness of a female, the complexion and hair become more manlike, and the VAE even puts a tie on the person. This is totally reasonable, and in fact what we wanted. This shows that some latent variable distributions overlap.</p>
			<p>Similarly, some latent variables do not overlap if we set the male vector to be the most negative. It will push the latent states to a place where traversing the moustache vector will not have an effect on growing a moustache on the face.</p>
			<p>Next, we can try to change several face attributes together. The mathematics are similar; we now only need to add up all the attribute vectors. In the following screenshot, the image on the left was generated randomly and is used as a baseline. On the right is a new image after some latent space arithmetic, as shown in the bars preceding <a id="_idIndexMarker137"/>the images:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B14538_02_19.jpg" alt="Figure 2.19 – Latent space exploration widget&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.19 – Latent space exploration widget</p>
			<p>The widget is available in the Jupyter notebook. Feel free to use it to explore the latent space and generate new faces!</p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor059"/>Summary</h1>
			<p>We started this chapter by learning how to use an encoder to compress high-dimensional data into low-dimensional latent variables, then use a decoder to reconstruct the data from the latent variables. We learned that the autoencoder's limitation is not being able to guarantee a continuous and uniform latent space, which makes it difficult to sample from. Then we incorporated Gaussian sampling to build a VAE to generate MNIST digits.</p>
			<p>Finally, we built a bigger VAE to train on the face dataset and had fun creating and manipulating faces. We learned the importance of the sampling distribution in the latent space, latent space arithmetic, and KLD, which lay the foundation for <a href="B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a>, <em class="italic">Generative Adversarial Network</em>. </p>
			<p>Although GANs are more powerful than VAEs in generating photorealistic images, the earlier GANs were difficult to train. Therefore, we will learn about the fundamentals of GANs. By the end of the next chapter, you will have learned the fundamentals of all three main families of deep generative algorithms, which will prepare you for more advanced models in part two of the book.</p>
			<p>Before we move on to GANs, I should stress that (variational) autoencoders are still being used widely. The variational encoding aspect has been incorporated into GANs. Therefore, mastering VAEs will help you master the advanced GAN models that we will cover in later chapters. We will cover the use of autoencoders to generate deep fake videos in <a href="B14538_09_Final_JM_ePub.xhtml#_idTextAnchor175"><em class="italic">Chapter 9</em></a><em class="italic">, Video Synthesis</em>. That chapter doesn't assume prior knowledge of GANs, therefore feel free to jump ahead to have a peek at how to use autoencoders to perform face swapping.</p>
		</div>
	</body></html>