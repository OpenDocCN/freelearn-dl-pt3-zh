<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Learning Stochastic and PG Optimization</h1>
                </header>
            
            <article>
                
<p><span>So far, we've addressed and developed value-based reinforcement learning algorithms. These algorithms learn a value function in order to be able to find a good policy. Despite the fact that they exhibit good performances, their application is constrained by some limits that are embedded in their inner workings. </span><span>In this chapter, we'll introduce a new class of algorithms called policy gradient methods, which are used to overcome the constraints of value-based methods by approaching the RL problem from a different perspective.</span></p>
<p><span>Policy gradient methods select an action based on a learned parametrized policy, instead of relying on a value function. In this chapter, we will also elaborate on the theory and intuition behind these methods, and with this background, develop the most basic version of a policy gradient algorithm, named <strong>REINFORCE</strong>.</span></p>
<p><span>REINFORCE exhibits some deficiencies due to its simplicity, but these can be mitigated with only a small amount of additional effort. Th</span><span>us, we'll present two improved versions of REINFORCE, called <strong>REINFORCE</strong> with baseline and <strong>actor-critic</strong> (<strong>AC</strong>) models. </span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Policy gradient methods</li>
<li>Understanding the REINFORCE algorithm</li>
<li>REINFORCE with a baseline</li>
<li>Learning the AC algorithm</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy gradient methods</h1>
                </header>
            
            <article>
                
<p><span>The algorithms that have been learned and developed so far are value-based, which, at their core, learn a value function, <em>V(s)</em>, or action-value function, <em>Q(s, a)</em>. A value function is a function that defines the total reward that can be accumulated from a given state or state-action pair. An action can then be selected, based on the estimated action (or state) values.</span></p>
<p><span>Therefore, a greedy policy can be defined as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><span> <sub><img class="fm-editor-equation" src="assets/65fc237e-c05d-401a-990e-1965179d5114.png" style="width:12.25em;height:1.50em;"/></sub></span></p>
<p><span>Value-based methods, when combined with deep neural networks,</span><span> can learn very sophisticated policies in order to control agents that operate in high-dimensionality spaces. Despite these great qualities, they suffer when dealing with problems with a large number of actions, or when the action space is continuous.</span></p>
<p><span>In such cases, maximum operation is not feasible. <strong>Policy gradient</strong> (<strong>PG</strong>) algorithms exhibit incredible potential in such contexts, as they can be easily adapted to continuous action spaces.</span></p>
<p>PG methods belong to the broader class of policy-based methods, including evolution strategies, which are studied later in <a href="dab022a7-3243-4e45-9f91-39a82df3a248.xhtml">Chapter 11</a>, <em>Understanding Black-Box Optimization Algorithms</em>. The distinctiveness of PG algorithms is in their use of the gradient of the policy, hence the name <strong>policy gradient</strong>.</p>
<p>A more concise categorization of RL algorithms, with respect to the one reported in <a href="f2414b11-976a-4410-92d8-89ee54745d99.xhtml">Chapter 3</a>, <em>Solving Problems with Dynamic Programming</em>, is shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2032 image-border" src="assets/e4912522-6ce0-48c0-a369-1cf6ecc6b79c.png" style="width:31.58em;height:23.83em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Examples of policy gradient methods are <strong>REINFORCE</strong> and <strong>AC </strong>that will be introduced in the next sections.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The gradient of the policy</h1>
                </header>
            
            <article>
                
<p>The objective of RL is to maximize the expected return (the total reward, discounted or undiscounted) of a trajectory. The objective function, can then be expressed as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/50c66cf3-976b-4e4e-ae01-e5cc9abf869b.png" style="width:19.75em;height:1.42em;"/></p>
<p>Where <em>θ</em> is the parameters of the policy, such as the trainable variables of a deep neural network.</p>
<p>In PG methods, the maximization of the objective function is done through the gradient of the objective function <sub><img class="fm-editor-equation" src="assets/2efadf0f-29b6-46b5-8e32-ec49106fa982.png" style="width:3.17em;height:1.25em;"/></sub>. Using gradient ascent, we can improve <sub><img class="fm-editor-equation" src="assets/eab1062a-1560-4650-ba0f-baf11baf24ee.png" style="width:2.17em;height:1.42em;"/></sub> by moving the parameters toward the direction of the gradient, as the gradient points in the direction in which the function increases.</p>
<div class="packt_infobox"><span>We have to take the same direction of the gradient,</span> because we aim to maximize the objective function (6.1).</div>
<p>Once the maximum is found, the policy, <em>π<sub>θ</sub></em>, will produce trajectories with the highest possible return. On an intuitive level, policy gradient incentivizes good policies by increasing their probability while punishing bad policies by reducing their probabilities. </p>
<p>Using equation (6.1), the gradient of the objective function is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ea2657c5-0fb7-4bd5-a3d2-afc8e12e9819.png" style="width:22.33em;height:1.42em;"/></p>
<p>By relating to the concepts from the previous chapters, in policy gradient methods, policy evaluation is the estimation of the return, <img class="fm-editor-equation" src="assets/7706f255-9576-478c-aaf4-c7ffdf9a32be.png" style="width:0.83em;height:0.92em;"/>. Instead, policy improvement is the optimization step of the parameter <img class="fm-editor-equation" src="assets/22e0eb66-b158-4ef4-a799-6fce86165959.png" style="width:0.50em;height:0.92em;"/>. Thus, policy gradient methods have to <span>symbiotically </span>carry on both phases in order to improve the policy.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Policy gradient theorem</h1>
                </header>
            
            <article>
                
<p>An initial problem is encountered when looking at equation (6.2), because, in its formulation, the gradient of the objective function depends on the distribution of the states of a policy; that is:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9f8a5642-590c-4fca-80ce-db0033892f03.png" style="width:39.92em;height:2.83em;"/></p>
<p>We would use a stochastic approximation of that expectation, but to compute the distribution of the states, <img class="fm-editor-equation" src="assets/52a2958e-64e4-44ac-9fe7-64ed137ebf42.png" style="width:1.67em;height:1.08em;"/>, we still need a complete model of the environment. Thus, this formulation isn't suitable for our purposes.</p>
<p>The policy gradient theorem comes to the rescue here. Its purpose is to provide an analytical formulation to compute the gradient of the objective function, with respect to the parameters of the policy, without involving the derivative of the state distribution. Formally, the policy gradient theorem, enables us to express the gradient of the objective function as:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/9c17f233-6856-4b70-938a-29d6a56f1a4b.png" style="width:53.42em;height:1.83em;"/></p>
<p>The proof of the policy gradient theorem is beyond the scope of this book, and thus, isn't included. However, you can find it in the book by Sutton and Barto (<a href="http://incompleteideas.net/book/the-book-2nd.htmlor">http://incompleteideas.net/book/the-book-2nd.htmlor</a>) or from other online resources.</p>
<p>Now that the derivative of the objective doesn't involve the derivative of the state distribution, the expectation can be estimated by sampling from the policy. Thus, the derivative of the objective can be approximated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b267976e-96b6-4923-8137-cb7c810ccfaf.png" style="width:32.25em;height:3.67em;"/></p>
<p>This can be used to produce a stochastic update with gradient ascent:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/19e5ff29-d46f-4579-90a2-5f09fd3f80bb.png" style="width:17.92em;height:1.42em;"/></p>
<p>Note, that because the goal is to maximize the objective function, gradient ascent is used to move the parameters in the same direction as the gradient (contrary to gradient descent, which performs <img class="fm-editor-equation" src="assets/bc2a2826-6786-41f1-ae1e-21dbfbae61c4.png" style="width:7.83em;height:1.25em;"/>).</p>
<p>The <span><span>idea </span></span>behind equation (6.5) is to increase the probability that good actions will be re-proposed in the future, while reducing the probability of bad actions. The quality of the actions is carried on by the usual scalar value of <img class="fm-editor-equation" src="assets/fcb741e8-5b10-4d2a-a4d3-d84fdca36d90.png" style="width:5.75em;height:1.58em;"/>, which gives the quality of the state-action pair.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing the gradient</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>As long as the policy is differentiable, its </span>gradient can be easily computed, taking advantage of modern automatic differentiation software.</p>
<p class="mce-root">To do that in TensorFlow, we can define the computational graph and call <kbd>tf.gradient(loss_function,variables)</kbd> to calculate the gradient of the loss function (<kbd>loss_function</kbd>) with respect to the <kbd>variables</kbd> trainable parameters. An alternative would be to <span>directly maximize the <kbd>objective</kbd> function using the</span> stochastic gradient descent optimizer, for example, by calling <kbd>tf.train.AdamOptimizer(lr).minimize(-objective_function)</kbd>.</p>
<p class="mce-root">The following snippet is an example of the steps that are required to compute <span>the approximation in formula (6.5)</span>, with a policy of discrete action space of the <kbd>env.action_space.n</kbd> dimension:</p>
<pre>pi = policy(states) # actions probability for each action<br/>onehot_action = tf.one_hot(actions, depth=env.action_space.n) <br/>pi_log = tf.reduce_sum(onehot_action * tf.math.log(pi), axis=1)<br/><br/>pi_loss = -tf.reduce_mean(pi_log * Q_function(states, actions))<br/><br/># calculate the gradients of pi_loss with respect to the variables<br/>gradients = tf.gradient(pi_loss, variables)<br/><br/># or optimize directly pi_loss with Adam (or any other SGD optimizer)<br/># pi_opt = tf.train.AdamOptimizer(lr).minimize(pi_loss) #</pre>
<p class="mce-root"><kbd>tf.one_hot</kbd> produces a one-hot encoding of the <kbd>actions</kbd> actions. T<span><span>hat is, it</span></span> produces a mask with <kbd>1</kbd>, corresponding with the numerical value of the action, <kbd>0</kbd>, in the others.</p>
<p><span><span>Then, in the third line of the code, </span></span>the mask is multiplied by the logarithm of the action probability, in order to obtain the log probability of the <kbd>actions</kbd> actions. The fourth line computes the loss as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a4e87877-72a6-462b-a265-9f2bd0e8d3ea.png" style="width:15.83em;height:3.67em;"/></p>
<p>And f<span>inally, </span><kbd>tf.gradient</kbd> <span>calculates the gradients of</span> <kbd>pi_loss</kbd>, <span>with respect to the</span> <kbd>variables</kbd> <span>parameter,</span> <span>as in formula (6.5).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The policy</h1>
                </header>
            
            <article>
                
<p>In the case that the actions are discrete and limited in number, the most common approach is to create a parameterized policy that produces a numerical value for each action.</p>
<div class="packt_infobox">Note that, differently from the Deep Q-Network algorithm, here, the output values of the policy aren't the <em>Q(s,a)</em> action values.</div>
<p>Then, each output value is converted to a probability. This operation is performed with the softmax function, which is given as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/68fae670-41fd-4d26-8ac0-4bff1fdfcaa5.png" style="width:11.92em;height:3.67em;"/></p>
<p>The softmax values are normalized to have a sum of one, so as to produce a probability distribution where each value corresponds to the probability of selecting a given action. </p>
<p>The next two plots show an example of five action-value predictions before (the plot on the left) and after (the right plot) they are applied to the softmax function. Indeed, from the plot on the right, you can see that, after the softmax is computed, the sum of the new values is one, and that they all have values greater than zero:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1968 image-border" src="assets/e3493f9f-b5bc-4e94-bc9a-cc20b61e8b5d.png" style="width:120.33em;height:74.17em;"/></p>
<p><span>The right plot indicates that actions 0,1,2,3, and 4, will be selected approximately, with probabilities of 0.64, 0.02, 0.09, 0.21, and 0.02,</span> <span>correspondingly.</span></p>
<p class="mce-root"/>
<p>To use a softmax distribution on the action values that are returned by the parameterized policy, we can use the code that is given in the <em>Computing the gradient</em> section, with only one change, which has been highlighted in the following snippet:</p>
<pre>pi = policy(states) # actions probability for each action<br/>onehot_action = tf.one_hot(actions, depth=env.action_space.n) <br/><br/>pi_log = tf.reduce_sum(onehot_action * <strong>tf.nn.log_softmax(pi)</strong>, axis=1) # instead of tf.math.log(pi)<br/><br/>pi_loss = -tf.reduce_mean(pi_log * Q_function(states, actions))<br/>gradients = tf.gradient(pi_loss, variables)</pre>
<p>Here, we used <kbd>tf.nn.log_softmax</kbd>, because it's been designed to be more stable than first calling <kbd>tf.nn.softmax</kbd>, and then <kbd>tf.math.log</kbd>.</p>
<p>An advantage of having actions according to stochastic distribution, is in the intrinsic randomness of the actions selected, which enable a dynamic exploration of the environment. This can seem like a side effect, but it's very important to have a policy that can adapt the level of exploration by itself.</p>
<p>In the case of DQN, we had to use a hand-crafted <img class="fm-editor-equation" src="assets/38d16fa6-2b85-4103-87a9-b626e9a67487.png" style="width:0.67em;height:0.92em;"/> variable to adjust the exploration throughout all the training, using linear <img class="fm-editor-equation" src="assets/8ec787b5-7c40-4a5a-a8b4-cd885d8f2355.png" style="width:0.67em;height:0.92em;"/> decay. Now that the exploration is built into the policy, at most, we have to add a term (the entropy) in the loss function in order to incentivize it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">On-policy PG</h1>
                </header>
            
            <article>
                
<p>A very important aspect of policy gradient algorithms is that they are <em>on-policy</em>. Their on-policy nature comes from the formula (6.4), as it is dependent on the current policy. Thus, unlike off-policy algorithms such as DQN, on-policy methods aren't allowed to reuse old experiences.</p>
<p>This means that all the experience that has been collected with a given policy has to be discarded once the policy changes. As a side effect, policy gradient algorithms are less sample efficient, meaning that they are required to gain more experience to reach the same performance as the off-policy counterpart. Moreover, they <span>usually</span> tend to generalize slightly worse.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the REINFORCE algorithm</h1>
                </header>
            
            <article>
                
<p>The core of policy gradient algorithms has already been covered, but we have another important concept to explain. We are yet to look at how action values are computed.</p>
<p>We already saw with the formula (6.4):</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/6455ffe3-0183-406a-9a13-25258d760860.png" style="width:29.75em;height:1.67em;"/></p>
<p>that we are able to estimate the gradient of the objective function by sampling directly from the experience that is collected following the <em><sub><img class="fm-editor-equation" src="assets/6cf0573f-766a-4776-a94f-18061e74eb65.png" style="width:1.50em;height:1.17em;"/></sub></em> policy.</p>
<p>The only two terms that are involved are the <span>values of </span><sub><img class="fm-editor-equation" src="assets/80fd981c-9df3-4958-aa68-bdfda5ee263a.png" style="width:4.75em;height:1.42em;"/></sub> and the derivative of the logarithm of the policy, which can be obtained through modern deep learning frameworks (such as TensorFlow and PyTorch). While we defined <sub><img class="fm-editor-equation" src="assets/6cf0573f-766a-4776-a94f-18061e74eb65.png" style="width:1.75em;height:1.33em;"/></sub>, we haven't explained how to estimate the action-value function, yet.</p>
<p>The simpler way, introduced for the first time in the REINFORCE algorithm by Williams, is to estimate the return is using <strong>Monte Carlo</strong> (<strong>MC</strong>) returns. For this reason, REINFORCE is considered an MC algorithm. If you remember, MC returns are the return values of sampled trajectories run with a given policy. Thus, we can rewrite equation (6.4), changing the action-value function, <sub><img class="fm-editor-equation" src="assets/62ce1473-0e9d-40ab-8380-0a8582619415.png" style="width:0.92em;height:1.33em;"/></sub>, with the MC return, <sub><img class="fm-editor-equation" src="assets/b1c3bbc1-40ca-430b-9d92-5de9efad9894.png" style="width:0.92em;height:1.00em;"/></sub>: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7105239c-6d19-41c4-91fc-ef132185c2ba.png" style="width:26.92em;height:3.17em;"/></p>
<p>The <img class="fm-editor-equation" src="assets/99c1ce90-6620-46ac-8ab2-f040e9e4f939.png" style="width:1.33em;height:1.17em;"/> return is computed from a complete trajectory, implying that the PG update is available only after<img class="fm-editor-equation" src="assets/674dc43c-0ce5-40ad-9c80-59cbf1c101b8.png" style="width:2.50em;height:1.00em;"/> steps, where <img class="fm-editor-equation" src="assets/0d80fa61-df32-4816-8ec3-6fa90e1e9de2.png" style="width:0.83em;height:0.92em;"/> is the total number of steps in a trajectory. Another consequence is that the MC return is well defined only in episodic problems, where there is an upper bound to the maximum number of steps (the same conclusions that we came up with in the other MC algorithms that we previously learned).</p>
<p>To get more practical, the discounted return at time <img class="fm-editor-equation" src="assets/61b732c8-d3ab-4f9a-84fb-55930cb8428f.png" style="width:0.50em;height:0.92em;"/>, which can also be called the <em>reward to go</em>, as it uses only future rewards, is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e80d67af-90fc-4e63-b4e6-e5e752037fc7.png" style="width:12.50em;height:4.08em;"/></p>
<p>This can be rewritten recursively, as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/14e68d1c-fbe4-4af4-b9e6-372e070041d3.png" style="width:19.25em;height:1.50em;"/></p>
<p>This function can be implemented by proceeding in reverse order, starting from the last reward, as shown here:</p>
<div>
<div>
<div>
<pre><span>def</span><span> </span><span>discounted_rewards</span><span>(</span><span>rews</span><span>, </span><span>gamma</span><span>):<br/></span><span>    rtg </span><span>=</span><span> np.</span><span>zeros_like</span><span>(rews, </span><span>dtype</span><span>=</span><span>np.float32)<br/></span><span>    rtg[</span><span>-</span><span>1</span><span>] </span><span>=</span><span> rews[</span><span>-</span><span>1</span><span>]</span><span><br/></span><span>    for</span><span> i </span><span>in</span><span> </span><span>reversed</span><span>(</span><span>range</span><span>(</span><span>len</span><span>(rews)</span><span>-</span><span>1</span><span>)):<br/></span><span>        rtg[i] </span><span>=</span><span> rews[i] </span><span>+</span><span> gamma</span><span>*</span><span>rtg[i</span><span>+</span><span>1</span><span>]<br/></span><span>    return</span><span> rtg</span></pre></div>
</div>
</div>
<p class="CDPAlignLeft CDPAlign">Here, in the first place, a NumPy array is created, and <span>the value of the last reward is assigned</span> to the <kbd>rtg</kbd> variable. This is done because, at time <img class="fm-editor-equation" src="assets/ae84e3b5-19cc-44c1-85de-178788d74d91.png" style="width:0.92em;height:1.00em;"/>, <img class="fm-editor-equation" src="assets/7b3ac824-c869-4c08-b21e-d523ff5f84e3.png" style="width:11.42em;height:1.50em;"/>. Then, <span><span>the algorithm </span></span>computes <kbd>rtg[i]</kbd> backward, using the subsequent value.</p>
<p>The main cycle of the REINFORCE algorithm involves running a few epochs until it gathers enough experience, and optimizing the policy parameter. To be effective, the algorithm has to complete at least one epoch before performing the update step (it needs at least a full trajectory to compute the reward to go (<img class="fm-editor-equation" src="assets/ce76b287-a263-4070-81d0-e973431e5bbd.png" style="width:1.17em;height:1.08em;"/>)). REINFORCE is summarized in the following pseudocode:</p>
<pre style="padding-left: 60px">Initialize <img class="fm-editor-equation" src="assets/3647c374-4948-4a82-b13e-b160fe9236ad.png" style="width:1.58em;height:1.17em;"/> with random weight<br/><br/><strong>for</strong> episode 1..M <strong>do</strong><br/>    Initialize environment <img class="fm-editor-equation" src="assets/718aa286-6e19-4bb7-91eb-7b5d24df8c04.png" style="width:8.00em;height:1.42em;"/><br/>    Initialize empty buffer<br/><br/>    <em>&gt; Generate a few episodes</em><br/>    <strong>for</strong> step 1..MaxSteps <strong>do</strong><br/>        <em>&gt; Collect experience by acting on the environment</em><br/>        <img class="fm-editor-equation" src="assets/973c8d88-9901-454e-a72c-d86b0b54e59b.png" style="width:5.50em;height:1.58em;"/><br/>        <img class="fm-editor-equation" src="assets/9583afe8-741c-46b8-be18-d6665e1f84d4.png" style="width:8.17em;height:1.50em;"/><br/>        <img class="fm-editor-equation" src="assets/830cd5e4-bc6e-49f3-b4da-28b303fd035a.png" style="width:3.08em;height:1.08em;"/><br/>        <strong>if</strong> <img class="fm-editor-equation" src="assets/83c8b851-d05d-4709-bffc-9473893d8295.png" style="width:5.25em;height:1.00em;"/>:<br/>            <img class="fm-editor-equation" src="assets/109872e4-091e-457e-943a-b19fbf3942e4.png" style="width:8.50em;height:1.50em;"/><br/>       <em>     &gt; Compute the reward to go</em> <br/>            <img class="fm-editor-equation" src="assets/eb37332e-52fb-4885-afc2-7b27d2bf70a2.png" style="width:17.92em;height:1.25em;"/> # for each t<br/>            <em>&gt; Store the episode in the buffer</em><br/>            <img class="fm-editor-equation" src="assets/de93a805-74dc-451b-9c6b-4682d7b8887f.png" style="width:11.67em;height:1.17em;"/> # where <img class="fm-editor-equation" src="assets/856e33ca-e9fd-4b9b-87c5-083bb438a886.png" style="width:0.67em;height:0.75em;"/> is the length of the episode<br/>    <em>&gt; REINFORCE update step using all the experience in <img class="fm-editor-equation" src="assets/56af0138-16a3-45ac-a415-8e84bd262d9e.png" style="width:0.92em;height:0.92em;"/> following formula (6.5)<span class="underline"><br/></span></em>    <img class="fm-editor-equation" src="assets/ffcb111b-79d8-461e-b62e-91f02e8c5e96.png" style="width:18.75em;height:3.00em;"/></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing REINFORCE</h1>
                </header>
            
            <article>
                
<p>It's time to implement REINFORCE. Here, we provide a mere implementation of the algorithm, without the procedures for its debugging and monitoring. The complete implementation is available in the GitHub repository. S<span>o, make sure that you check it out.</span></p>
<p>The code is divided into three main functions, and one class:</p>
<ul>
<li><kbd>REINFORCE(env_name, hidden_sizes, lr, num_epochs, gamma, steps_per_epoch)</kbd>: This is the function that contains the main implementation of the algorithm.</li>
<li><kbd>Buffer</kbd>: This is a class that is used to temporarily store the trajectories.</li>
<li><kbd>mlp(x, hidden_layer, output_size, activation, last_activation)</kbd>: This is used to build a multi-layer perceptron in TensorFlow.</li>
<li><kbd>discounted_rewards(rews, gamma)</kbd>: This computes the discounted reward to go.</li>
</ul>
<p>We'll first look at the main <kbd>REINFORCE</kbd> function, and then implement the supplementary functions and class. </p>
<p>The <kbd>REINFORCE </kbd>function is divided into two main parts. In the first part, the computational graph is created, while in the second, the environment is run and the policy is optimized cyclically until a convergence criterion is met.</p>
<p>The <kbd>REINFORCE</kbd> function takes <span>the name of the</span> <kbd>env_name</kbd> <span>environment</span> as the input, a list with the sizes of the hidden layers—<kbd>hidden_sizes</kbd>, the learning rate—<kbd>lr</kbd>, the number of training epochs—<kbd>num_epochs</kbd>, the discount value—<kbd>gamma</kbd>, and the minimum number of steps per epoch—<kbd>steps_per_epoch</kbd>. Formally, the heading of <kbd>REINFORCE</kbd> is as follows:</p>
<pre><span>def REINFORCE</span><span>(</span><span>env_name</span><span>, </span><span>hidden_sizes</span><span>=</span><span>[</span><span>32</span><span>], </span><span>lr</span><span>=</span><span>5e-3</span><span>, </span><span>num_epochs</span><span>=</span><span>50</span><span>, </span><span>gamma</span><span>=</span><span>0.99</span><span>, </span><span>steps_per_epoch</span><span>=</span><span>100</span><span>):</span></pre>
<p><span>At the beginning of <kbd>REINFORCE(..)</kbd>, the TensorFlow default graph is reset, an environment is created, the placeholder is initialized, and the policy is created. The policy is a fully connected multi-layer perceptron, with an output for each action, and </span><kbd>tanh</kbd><span> activation, on each hidden layer. The outputs of the multi-layer perceptron are the unnormalized values of the actions, called logits. All this is done in the following snippet:</span></p>
<pre><span>def</span><span> </span><span>REINFORCE</span><span>(</span><span>env_name</span><span>, </span><span>hidden_sizes</span><span>=</span><span>[</span><span>32</span><span>], </span><span>lr</span><span>=</span><span>5e-3</span><span>, </span><span>num_epochs</span><span>=</span><span>50</span><span>, </span><span>gamma</span><span>=</span><span>0.99</span><span>, </span><span>steps_per_epoch</span><span>=</span><span>100</span><span>):<br/></span><span><br/>    tf.</span><span>reset_default_graph</span><span>()<br/><br/></span><span>    env </span><span>=</span><span> gym.</span><span>make</span><span>(env_name) <br/></span><span>    obs_dim </span><span>=</span><span> env.observation_space.shape<br/></span><span>    act_dim </span><span>=</span><span> env.action_space.n <br/><br/></span><span>    obs_ph </span><span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>, obs_dim[</span><span>0</span><span>]), </span><span>dtype</span><span>=</span><span>tf.float32, </span><span>name</span><span>=</span><span>'</span><span>obs</span><span>'</span><span>)<br/></span><span>    act_ph </span><span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>,), </span><span>dtype</span><span>=</span><span>tf.int32, </span><span>name</span><span>=</span><span>'</span><span>act</span><span>'</span><span>)<br/></span><span>    ret_ph </span><span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>,), </span><span>dtype</span><span>=</span><span>tf.float32, </span><span>name</span><span>=</span><span>'</span><span>ret</span><span>'</span><span>)<br/></span><span>    <br/>    p_logits </span><span>=</span><span> </span><span>mlp</span><span>(obs_ph, hidden_sizes, act_dim, </span><span>activation</span><span>=</span><span>tf.tanh)</span></pre>
<p>We can then create an operation that will compute the loss function, and one that will optimize the policy. The code is similar to the code that we saw earlier, in the <em>The policy</em> section. The only difference is that now the actions <span>are sampled by </span><kbd>tf.random.multinomial</kbd> , which follows the action distribution that is returned by the policy. This function draws samples from a categorical distribution. In our case, it chooses a single action (depending on the environment, it could be more than one action).</p>
<p>The following snippet is the implementation of the REINFORCE update:</p>
<pre><span> act_multn </span><span>=</span><span> tf.</span><span>squeeze</span><span>(tf.random.</span><span>multinomial</span><span>(p_logits, </span><span>1</span><span>))<br/></span><span> actions_mask </span><span>=</span><span> tf.</span><span>one_hot</span><span>(act_ph, </span><span>depth</span><span>=</span><span>act_dim)<br/></span><span> p_log </span><span>=</span><span> tf.</span><span>reduce_sum</span><span>(actions_mask </span><span>*</span><span> tf.nn.</span><span>log_softmax</span><span>(p_logits), </span><span>axis</span><span>=</span><span>1</span><span>)<br/></span><span> p_loss </span><span>=</span><span> </span><span>-</span><span>tf.</span><span>reduce_mean</span><span>(p_log</span><span>*</span><span>ret_ph)<br/></span><span> p_opt </span><span>=</span><span> tf.train.</span><span>AdamOptimizer</span><span>(lr).</span><span>minimize</span><span>(p_loss)</span></pre>
<p>A mask is created over the actions that are chosen during the interaction with the environment and multiplied by <kbd>log_softmax</kbd> in order to obtain <img class="fm-editor-equation" src="assets/80cb1341-b1db-4b7f-91f8-618d43bad519.png" style="width:5.58em;height:1.50em;"/>. Then, the full loss function is computed. Be careful—there is a minus sign before <kbd>tf.reduce_sum</kbd>. We are interested in the maximization of the objective function. But because the optimizer needs a function to minimize, we have to pass a loss function. The last line optimizes the PG loss function using <kbd>AdamOptimizer</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root">We are now ready to start a session, reset the global variables of the computational graph, and initialize some further variables that we'll use later:</p>
<div>
<pre><span>    sess </span><span>=</span><span> tf.</span><span>Session</span><span>()<br/></span><span>    sess.</span><span>run</span><span>(tf.</span><span>global_variables_initializer</span><span>())<br/></span><span>    step_count </span><span>=</span><span> </span><span>0<br/></span><span>    train_rewards </span><span>=</span><span> []<br/></span><span>    train_ep_len </span><span>=</span><span> []</span></pre></div>
<p>Then, we create the two inner cycles that will interact with the environment to gather experience and optimize the policy, and print a few statistics: </p>
<div>
<pre><span>    for</span><span> ep </span><span>in</span><span> </span><span>range</span><span>(num_epochs):<br/></span><span>        obs </span><span>=</span><span> env.</span><span>reset</span><span>()<br/></span><span>        buffer </span><span>=</span><span> </span><span>Buffer</span><span>(gamma)<br/></span><span>        env_buf </span><span>=</span><span> []<br/></span><span>        ep_rews </span><span>=</span><span> []<br/><br/></span><span>        while</span><span> </span><span>len</span><span>(buffer) </span><span>&lt;</span><span> steps_per_epoch:<br/><br/>            # run the policy <br/></span><span>            act </span><span>=</span><span> sess.</span><span>run</span><span>(act_multn, </span><span>feed_dict</span><span>=</span><span>{obs_ph:[obs]})<br/>            # take a step in the environment<br/></span><span>            obs2, rew, done, _ </span><span>=</span><span> env.</span><span>step</span><span>(np.</span><span>squeeze</span><span>(act))<br/><br/></span><span>            env_buf.</span><span>append</span><span>([obs.</span><span>copy</span><span>(), rew, act])<br/></span><span>            obs </span><span>=</span><span> obs2.</span><span>copy</span><span>()<br/></span><span>            step_count </span><span>+=</span><span> </span><span>1<br/></span><span>            ep_rews.</span><span>append</span><span>(rew)<br/><br/></span><span>            if</span><span> done:<br/>                # add the full trajectory to the environment<br/></span><span>                buffer.</span><span>store</span><span>(np.</span><span>array</span><span>(env_buf))<br/></span><span>                env_buf </span><span>=</span><span> []<br/></span><span>                train_rewards.</span><span>append</span><span>(np.</span><span>sum</span><span>(ep_rews))<br/></span><span>                train_ep_len.</span><span>append</span><span>(</span><span>len</span><span>(ep_rews))<br/></span><span>                obs </span><span>=</span><span> env.</span><span>reset</span><span>()<br/></span><span>                ep_rews </span><span>=</span><span> []<br/>    <br/>        obs_batch, act_batch, ret_batch = buffer.get_batch()<br/>        # Policy optimization<br/></span><span>        sess.</span><span>run</span><span>(p_opt, </span><span>feed_dict</span><span>=</span><span>{obs_ph:obs_batch, act_ph:act_batch, ret_ph:ret_batch})<br/><br/>        # Print some statistics<br/></span><span>        if</span><span> ep </span><span>%</span><span> </span><span>10</span><span> </span><span>==</span><span> </span><span>0</span><span>:<br/></span><span>            print</span><span>(</span><span>'</span><span>Ep:</span><span>%d</span><span> MnRew:</span><span>%.2f</span><span> MxRew:</span><span>%.1f</span><span> EpLen:</span><span>%.1f</span><span> Buffer:</span><span>%d</span><span> -- Step:</span><span>%d</span><span> --</span><span>'</span><span> </span><span>%</span><span> (ep, np.</span><span>mean</span><span>(train_rewards), np.</span><span>max</span><span>(train_rewards), np.</span><span>mean</span><span>(train_ep_len), </span><span>len</span><span>(buffer), step_count))<br/></span><span>            train_rewards </span><span>=</span><span> []<br/></span><span>            train_ep_len </span><span>=</span><span> []<br/></span><span>    env.</span><span>close</span><span>()</span></pre></div>
<p>The two cycles follow the usual flow, with the exception that the interaction with the environment stops whenever the trajectory ends, and the temporary buffer has enough transitions.</p>
<p>We can now implement the <kbd>Buffer</kbd> class that contains the data of the trajectories:</p>
<div>
<pre><span>class</span><span> </span><span>Buffer</span><span>():<br/></span><span>    def</span><span> </span><span>__init__</span><span>(</span><span>self</span><span>, </span><span>gamma</span><span>=</span><span>0.99</span><span>):<br/></span><span>        self</span><span>.gamma </span><span>=</span><span> gamma<br/></span><span>        self</span><span>.obs </span><span>=</span><span> []<br/></span><span>        self</span><span>.act </span><span>=</span><span> []<br/></span><span>        self</span><span>.ret </span><span>=</span><span> []<br/><br/></span><span>    def</span><span> </span><span>store</span><span>(</span><span>self</span><span>, </span><span>temp_traj</span><span>):<br/></span><span>        if</span><span> </span><span>len</span><span>(temp_traj) </span><span>&gt;</span><span> </span><span>0</span><span>:<br/></span><span>            self</span><span>.obs.</span><span>extend</span><span>(temp_traj[</span><span>:</span><span>,</span><span>0</span><span>])<br/></span><span>            ret </span><span>=</span><span> </span><span>discounted_rewards</span><span>(temp_traj[</span><span>:</span><span>,</span><span>1</span><span>], </span><span>self</span><span>.gamma)<br/></span><span>            self</span><span>.ret.</span><span>extend</span><span>(ret)<br/></span><span>            self</span><span>.act.</span><span>extend</span><span>(temp_traj[</span><span>:</span><span>,</span><span>2</span><span>])<br/><br/>    </span>def<span> </span><span>get_batch</span><span>(</span><span>self</span><span>):<br/>        </span>return<span> </span><span>self</span><span>.obs, </span><span>self</span><span>.act, </span><span>self</span><span>.ret<br/>    <br/>    </span><span>def</span><span> </span><span>__len__</span><span>(</span><span>self</span><span>):<br/></span><span>        assert</span><span>(</span><span>len</span><span>(</span><span>self</span><span>.obs) </span><span>==</span><span> </span><span>len</span><span>(</span><span>self</span><span>.act) </span><span>==</span><span> </span><span>len</span><span>(</span><span>self</span><span>.ret))<br/></span><span>        return</span><span> </span><span>len</span><span>(</span><span>self</span><span>.obs)</span></pre></div>
<p>And finally, we can implement the function that creates the neural network with an arbitrary number of hidden layers:</p>
<div>
<pre><span>def</span><span> </span><span>mlp</span><span>(</span><span>x</span><span>, </span><span>hidden_layers</span><span>, </span><span>output_size</span><span>, </span><span>activation</span><span>=</span><span>tf.nn.relu, </span><span>last_activation</span><span>=</span><span>None</span><span>):<br/></span><span>for</span><span> l </span><span>in</span><span> hidden_layers:<br/></span><span>    x </span><span>=</span><span> tf.layers.</span><span>dense</span><span>(x, </span><span>units</span><span>=</span><span>l, </span><span>activation</span><span>=</span><span>activation)<br/></span><span>    return</span><span> tf.layers.</span><span>dense</span><span>(x, </span><span>units</span><span>=</span><span>output_size, </span><span>activation</span><span>=</span><span>last_activation)</span></pre></div>
<p>Here, <kbd>activation</kbd> is the non-linear function that is applied to the hidden layers, and <kbd>last_activation</kbd> is the non-linearity function that is applied to the output layer.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Landing a spacecraft using REINFORCE</h1>
                </header>
            
            <article>
                
<p>The algorithm is complete however, the most interesting part has yet to be explained. In this section, we'll apply REINFORCE to <kbd>LunarLander-v2</kbd>, an episodic Gym environment with the aim of landing a lunar lander.<span> </span></p>
<p>The following is a screenshot of the game in its initial position, and a hypothetical successful final position: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1969 image-border" src="assets/b23124f7-ca9a-40e2-8d2f-294d33d31aab.png" style="width:162.50em;height:50.92em;"/></p>
<p>This is a discrete problem, and the<span> </span>lander<span> </span>has to land at coordinates (0,0), with a<span> </span>penalty<span> if </span>it lands far from that point. The lander has a positive reward when it moves from the top of the screen to the bottom, but when it fires the engine to slow down, it loses 0.3 points on each frame.</p>
<p>Moreover, depending on the conditions of the landing, it receives an additional -100 or +100 points. The game is considered solved with a total of 200 points. Each game is run for a maximum of 1,000 steps.</p>
<p>For that last reason, we'll gather at least 1,000 steps of experience, to be sure that at least one full episode has been completed (this value is set by the <kbd>steps_per_epoch</kbd> hyperparameter).</p>
<p>REINFORCE is run calling the function with the following hyperparameters:</p>
<div>
<pre><span>REINFORCE</span><span>(</span><span>'</span><span>LunarLander-v2</span><span>'</span><span>, </span><span>hidden_sizes</span><span>=</span><span>[64</span><span>], </span><span>lr</span><span>=</span><span>8e-3</span><span>, </span><span>gamma</span><span>=</span><span>0.99</span><span>, </span><span>num_epochs</span><span>=</span><span>1000</span><span>, </span><span>steps_per_epoch</span><span>=</span><span>1000</span><span>)</span></pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the results</h1>
                </header>
            
            <article>
                
<p class="mce-root">Throughout the learning, we monitored many parameters, including <span><kbd>p_loss</kbd> (the loss of the policy), </span><span><kbd>old_p_loss</kbd> (the policy's loss before the optimization phase), the total rewards, and the length of the episodes, in order</span> <span>to get a better understanding of the algorithm, and to properly tune the hyperparameters. We also summarized some histograms. Look at the code in the book's repository to learn more about the TensorBoard summaries!</span></p>
<p>In the following figure, we have plotted the mean of the total rewards of the full trajectories that were obtained during training:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2066 image-border" src="assets/738017d2-2d7e-46aa-b2cd-4159ce1a0cd6.png" style="width:126.42em;height:75.75em;"/></p>
<p>From this plot, we can see that it reaches a mean score of 200, or slightly less, in about 500,000 steps; therefore requiring about 1,000 full trajectories, before it is able to master the game. </p>
<p>When plotting the training performance, remember that it is likely that the algorithm is still exploring. To check whether this is true, monitor the entropy of the actions. If it's higher than 0, it means that the algorithm is uncertain about the actions selected, and it will keep exploring—choosing the other actions, and following their distribution. In this case, after 500,000 steps, the agent is also exploring the environment, as shown in the following plot: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2067 image-border" src="assets/09e63e62-ee86-4032-8ec3-3459730ece14.png" style="width:124.17em;height:75.50em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">REINFORCE with baseline</h1>
                </header>
            
            <article>
                
<p>REINFORCE has the nice property of being unbiased, due to the MC return, which provides the true return of a full trajectory. However, the unbiased estimate is to the detriment of the variance, which increases with the length of the trajectory. Why? This effect is due to the stochasticity of the policy. By executing a full trajectory, you would know its true reward. However, the value that is assigned to each state-action pair may not be correct, since the policy is stochastic, and executing it another time may lead to a new state, and consequently, a different reward. Moreover, you can see that the higher the number of actions in a trajectory, the more stochasticity you will have introduced into the system, therefore, ending up with higher variance.</p>
<p>Luckily, it is possible to introduce a baseline, <img class="fm-editor-equation" src="assets/9a81020c-7604-437d-8bc7-2e35a47209c9.png" style="width:0.50em;height:1.00em;"/>, in the estimation of the return, therefore decreasing the variance, and improving the stability and performance of the algorithm. The algorithms that adopt this strategy is called <strong>REINFORCE</strong> with baseline, and the gradient of its objective function is as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a23a9b32-1a0c-4f25-a33f-89ae7bb58c97.png" style="width:21.67em;height:1.50em;"/></p>
<p>This trick of introducing a baseline is possible, because the gradient estimator still remains unchanged in bias:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d7f552ec-7f1f-4340-b684-0f0ede810167.png" style="width:10.83em;height:1.50em;"/></p>
<p>At the same time, for this equation to be true, the baseline must be a constant with respect to the actions.</p>
<p>Our job now is to find a good <img class="fm-editor-equation" src="assets/94943530-1306-4e84-984e-e6af2b7ca089.png" style="width:0.58em;height:1.25em;"/> baseline. The simplest way is to subtract the average return.</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/48c9de86-fd72-4f33-a17a-e8d21d185829.png" style="width:7.50em;height:3.83em;"/></p>
<p>If you would like to implement this in the REINFORCE code, the only change is in the <kbd>get_batch()</kbd> function of the <kbd>Buffer</kbd> class:</p>
<div>
<pre><span>    def</span><span> </span><span>get_batch</span><span>(</span><span>self</span><span>):<br/></span><span>        b_ret </span><span>=</span><span> </span><span>self</span><span>.ret </span><span>-</span><span> np.</span><span>mean</span><span>(</span><span>self</span><span>.ret)<br/></span><span>        return</span><span> </span><span>self</span><span>.obs, </span><span>self</span><span>.act, b_ret</span></pre></div>
<p><span><span>Although </span></span>this baseline decreases the variance, it's not the best strategy. As the baseline can be conditioned on the state, a better idea is to use an estimate of the value function:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cd704c56-e37e-4f18-860a-fd35b85acf00.png" style="width:25.17em;height:1.58em;"/></p>
<p>Remember that the <img class="fm-editor-equation" src="assets/4f0a54d4-2ee9-43e5-b13f-f4e28cf2f16b.png" style="width:2.08em;height:1.17em;"/> value function is, on average, the return that is obtained following the <img class="fm-editor-equation" src="assets/ff4f4ab5-b993-4e8a-9be3-1b581925803c.png" style="width:1.42em;height:1.08em;"/> policy.</p>
<p>This variation introduces more complexity into the system, as we have to design an approximation of the value function, but it's very common to use, and it considerably increases the performance of the algorithm.</p>
<p>To learn <img class="fm-editor-equation" src="assets/0d4c296b-488f-479e-b008-bfa7f9a5388e.png" style="width:3.25em;height:1.33em;"/>, the best solution is to fit a neural network with MC estimates:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8747c6e4-8e9e-439b-b8db-c0dbb5913b75.png" style="width:14.92em;height:4.08em;"/></p>
<p>In the preceding equation, <img class="fm-editor-equation" src="assets/4f581b80-60d7-46c9-8188-737e83a0c188.png" style="width:1.08em;height:1.00em;"/> is the parameters of the neural network to be learned.</p>
<p>In order to not overrun the notation, from now on, we'll neglect to specify the policy, so that <img class="fm-editor-equation" src="assets/35844138-3049-42c1-b4b6-67d55ada95e5.png" style="width:3.50em;height:1.42em;"/> will become <img class="fm-editor-equation" src="assets/40c76756-1c31-42c1-afbb-0c17efb507e4.png" style="width:3.08em;height:1.50em;"/>.</p>
<p>The neural network is trained on the same trajectories' data that is used for learning <img class="fm-editor-equation" src="assets/76bcd8d3-a24b-41c2-b16b-84cfdbe47bdd.png" style="width:1.58em;height:1.17em;"/>, without requiring additional interaction with the environment. Once computed, the MC estimates, for example, with <kbd>discounted_rewards(rews, gamma)</kbd>, will become the <img class="fm-editor-equation" src="assets/512d6543-23dd-4aac-b864-4ec3642a4a86.png" style="width:0.75em;height:1.33em;"/> target values, and the neural network will be optimized in order to minimize the mean square error (MSE) loss—just as you'd do in a supervised learning task:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cbe98b06-473b-4ca4-823a-dc4dafbcb72c.png" style="width:13.17em;height:2.92em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/aeca18ae-fab6-443e-b0a6-d6237d029998.png" style="width:1.08em;height:1.00em;"/> is the weights of the value function neural network, and each element of the dataset contains the <img class="fm-editor-equation" src="assets/dbeecb7f-e955-4daf-9d4b-06ca14894ff8.png" style="width:1.17em;height:1.17em;"/> state, and the target value <img style="font-size: 1em;width:11.08em;height:3.67em;" class="fm-editor-equation" src="assets/4ee8929c-cacf-4a63-933f-c63dfee82aeb.png"/><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing REINFORCE with baseline</h1>
                </header>
            
            <article>
                
<p>The value function that baseline approximated with a neural network can be implemented by adding a few lines to our previous code:</p>
<ol>
<li>Add the neural network, the operations for computing the MSE loss function, and the optimization procedure to the computational graph:</li>
</ol>
<div>
<pre><span>    ...<br/>    # placeholder that will contain the reward to go values (i.e. the y values)<br/></span>    rtg_ph <span>=</span><span> tf.</span><span>placeholder</span><span>(</span><span>shape</span><span>=</span><span>(</span><span>None</span><span>,), </span><span>dtype</span><span>=</span><span>tf.float32, </span><span>name</span><span>=</span><span>'</span><span>rtg</span><span>'</span><span>)<br/>    <br/>    </span><span># MLP value function<br/></span><span>    s_values </span><span>=</span><span> tf.</span><span>squeeze</span><span>(</span><span>mlp</span><span>(obs_ph, hidden_sizes, </span><span>1</span><span>, </span><span>activation</span><span>=</span><span>tf.tanh))<br/><br/></span><span>    # MSE loss function<br/></span><span>    v_loss </span><span>=</span><span> tf.</span><span>reduce_mean</span><span>((rtg_ph </span><span>-</span><span> s_values)</span><span>**</span><span>2</span><span>)<br/><br/></span><span>    # value function optimization<br/></span><span>    v_opt </span><span>=</span><span> tf.train.</span><span>AdamOptimizer</span><span>(vf_lr).</span><span>minimize</span><span>(v_loss)<br/>    ...</span></pre></div>
<ol start="2">
<li>Run <kbd>s_values</kbd>, and store the <img class="fm-editor-equation" src="assets/0037c50a-75e4-49bb-b6b0-ab2095b92691.png" style="width:3.25em;height:1.33em;"/>predictions, as later we'll need to compute <img class="fm-editor-equation" src="assets/8df6a5dd-8373-4462-961f-805dc3f8eba3.png" style="width:6.75em;height:1.33em;"/>. This operation can be done in the innermost cycle (the differences from the REINFORCE code are shown in bold):</li>
</ol>
<div>
<pre><span>            ...<br/>            # besides act_multn, run also s_values<br/>            act, val </span><span>=</span><span> sess.</span><span>run</span><span>([act_multn, <strong>s_values</strong>], </span><span>feed_dict</span><span>=</span><span>{obs_ph:[obs]})<br/></span><span>            obs2, rew, done, _ </span><span>=</span><span> env.</span><span>step</span><span>(np.</span><span>squeeze</span><span>(act))<br/><br/></span><span>            # add the new transition, included the state value predictions<br/></span><span>            env_buf.</span><span>append</span><span>([obs.</span><span>copy</span><span>(), rew, act, <strong>np.</strong></span><strong><span>squeeze</span></strong><span><strong>(val)</strong>])<br/>            ...<br/></span></pre></div>
<ol start="3">
<li>Retrieve <kbd>rtg_batch</kbd>, which contains the "target" values from the buffer, and optimize the value function<span>:</span></li>
</ol>
<div>
<pre><span>        obs_batch, act_batch, ret_batch, rtg_batch </span><span>=</span><span> buffer.</span><span>get_batch</span><span>() <br/></span><span>        sess.</span><span>run</span><span>([p_opt, v_opt], </span><span>feed_dict</span><span>=</span><span>{obs_ph:obs_batch, act_ph:act_batch, ret_ph:ret_batch, rtg_ph:rtg_batch})</span></pre></div>
<ol start="4">
<li>Compute the reward to go (<img class="fm-editor-equation" src="assets/f0dd0b06-c9c8-43ee-940e-8e21ac00b4ac.png" style="width:1.08em;height:1.08em;"/>), and the target values <img class="fm-editor-equation" src="assets/de87b8d5-2dfe-4c84-b1f7-af49aba9e641.png" style="width:7.00em;height:1.42em;"/>. This change is done in the <kbd>Buffer</kbd> class. We have to create a new empty <kbd>self.rtg</kbd> list in the initialization method of the class, and modify the <kbd>store</kbd> and <kbd>get_batch</kbd> functions, as follows:</li>
</ol>
<div>
<pre><span>    def</span> <span>store</span><span>(</span><span>self</span><span>,</span> <span>temp_traj</span><span>):<br/></span><span>        if</span> <span>len</span><span>(temp_traj)</span> <span>&gt;</span> <span>0:<br/>            </span><span>self</span><span>.obs.</span><span>extend</span><span>(temp_traj[</span><span>:</span><span>,</span><span>0</span><span>])<br/>            </span><span>rtg</span> <span>=</span> <span>discounted_rewards</span><span>(temp_traj[</span><span>:</span><span>,</span><span>1</span><span>],</span> <span>self</span><span>.gamma)<br/>            </span><span># ret = G - V<br/>            </span><span>self</span><span>.ret.</span><span>extend</span><span>(rtg</span> <span>-</span> <span>temp_traj[</span><span>:</span><span>,</span><span>3</span><span>])<br/>            </span><span>self</span><span>.rtg.</span><span>extend</span><span>(rtg)<br/>            </span><span>self</span><span>.act.</span><span>extend</span><span>(temp_traj[</span><span>:</span><span>,</span><span>2</span><span>])<br/><br/>    </span><span>def</span> <span>get_batch</span><span>(</span><span>self</span><span>):<br/>        </span><span>return</span> <span>self</span><span>.obs,</span> <span>self</span><span>.act,</span> <span>self</span><span>.ret,</span> <span>self</span><span>.rtg</span></pre></div>
<p>You can now test the REINFORCE with baseline algorithm on whatever environment you want, and compare the performance with the basic REINFORCE implementation. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Learning the AC algorithm</h1>
                </header>
            
            <article>
                
<p class="mce-root">Simple REINFORCE has the notable property of being unbiased, but it exhibits high variance. Adding a baseline reduces the variance, while keeping it unbiased (asymptotically, the algorithm will converge to a local minimum). A major drawback of REINFORCE with baseline is that it'll converge very slowly, requiring a consistent number of interactions with the environment. </p>
<p>An approach to speed up training is called bootstrapping. This is a technique that we've already seen many times throughout the book. It allows the estimation of the return values from the subsequent state values. The policy gradient algorithms that use this techniques is called actor-critic (AC). In the AC algorithm, the actor is the policy, and the critic is the value function (typically, a state-value function) that "<span>critiques</span>" the behavior of the actor, to help him learn faster. The advantages of AC methods are multiple, but the most important is their ability to learn in non-episodic problems.</p>
<p>It's not possible to solve continuous tasks with REINFORCE, as to compute the reward to go, they need all the rewards until the end of the trajectory (if the trajectories are infinite, there is no end). Relying on the bootstrapping technique, AC methods are also able to learn action values from incomplete trajectories.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a critic to help an actor to learn</h1>
                </header>
            
            <article>
                
<p>The action-value function that uses one-step bootstrapping is defined as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b8a275fb-6b31-4f5d-ac38-f809d016d179.png" style="width:9.00em;height:1.33em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/b0dac0f6-61a1-4938-a6dd-6c3420dcce52.png" style="width:1.17em;height:1.50em;"/> is<span> </span>the notorious next<span> </span>state.</p>
<p>Thus, with an <img class="fm-editor-equation" src="assets/3e0578d1-d7b6-4d9a-9346-71df6842bea9.png" style="width:1.25em;height:0.92em;"/> actor, and a <img class="fm-editor-equation" src="assets/bd9cfb2d-0a7d-4f54-9b50-e2bb986c5bf0.png" style="width:1.42em;height:1.08em;"/> critic using bootstrapping, we obtain a one-step AC step:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0b0dacf6-6010-4b45-b86c-b744ef400c29.png" style="width:22.50em;height:1.33em;"/></p>
<p>This will replace the REINFORCE step with a baseline:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/8de0363f-6c5d-4c22-bd2d-01f4a07d7738.png" style="width:18.08em;height:1.33em;"/></p>
<p>Note the difference between the use of the state-value function in REINFORCE and AC. In the former, it is used only as a baseline, to provide the state value of the current state. In the latter example, the state-value function is used to estimate the value of the next state, so as to only<span> </span>require the current reward to<span> </span>estimate <img class="fm-editor-equation" src="assets/0745c16f-3926-466d-9316-31a1f3bdbbaf.png" style="width:3.25em;height:1.33em;"/>. Thus, we can say that the one-step AC model is a fully online, incremental algorithm. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The n-step AC model</h1>
                </header>
            
            <article>
                
<p><span>In reality, as we already saw in TD learning, a fully online algorithm has low variance but high bias, the opposite of MC learning. However, usually, a middle-ground strategy, between fully online and MC methods, is preferred. To balance this trade-off, an n-step return can replace a one-step return of online algorithms.</span></p>
<p><span>If you remember, we already implemented n-step learning in the DQN algorithm. The only difference is that DQN is an off-policy algorithm, and in theory, n-step can be employed only on on-policy algorithms. Nevertheless, we showed that with a small <img class="fm-editor-equation" src="assets/298ede41-f52b-47b1-b30d-bce9761e87bb.png" style="width:0.92em;height:1.00em;"/>, the performance increased.</span></p>
<p><span>AC algorithms are on-policy, therefore, as far as the performance increase goes, it's possible to use arbitrary large <img class="fm-editor-equation" src="assets/150066f6-e6c8-4530-8594-857271431f41.png" style="width:0.92em;height:1.00em;"/> values. The integration of n-step in AC is pretty straightforward; the one-step return is replaced by <img class="fm-editor-equation" src="assets/6d88c26c-8b10-4908-8a5c-137d77e8c487.png" style="width:3.00em;height:1.25em;"/>, and the value function is taken in the <img class="fm-editor-equation" src="assets/93cfe315-7e80-4559-84f7-64247c471eba.png" style="width:2.75em;height:1.33em;"/> state:<br/></span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f5a7b562-3d72-4b5b-a82b-1b5c491644bd.png" style="width:31.17em;height:1.50em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/b03016a1-1fb3-45ec-93e4-1cbfaa795bd2.png" style="width:21.33em;height:1.58em;"/>. Pay attention here to how, if <img class="fm-editor-equation" src="assets/ded9562c-a399-4da7-b4f9-f63eb41c13d9.png" style="width:1.25em;height:1.25em;"/> is a final state, <img class="fm-editor-equation" src="assets/bdabdf88-e089-4606-a3ed-eae8804cb52e.png" style="width:5.17em;height:1.25em;"/>.</p>
<p>Besides reducing the bias, the n-step return propagates t<span>he subsequent returns</span> faster, making the learning much more efficient.</p>
<p>Interestingly, the <img class="fm-editor-equation" src="assets/ac9730d0-848f-492e-91b9-323c0ab42870.png" style="width:16.17em;height:1.58em;"/>quantity can be seen as an estimate of the advantage function. In fact, the advantage function is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ef6264dd-3aa0-4949-a184-b5d2c7bd4e69.png" style="width:14.08em;height:1.42em;"/> </p>
<p>Due to the fact that <img class="fm-editor-equation" src="assets/68902dba-0710-4680-97f7-ddb38b1e7c5c.png" style="width:9.83em;height:1.42em;"/> is an estimate of <img class="fm-editor-equation" src="assets/8d5a0ace-b449-4f15-8396-d61102b8254c.png" style="width:5.00em;height:1.42em;"/>, we obtain an estimate of the advantage function. U<span>sually, t</span>his function is easier to learn, as it only denotes the preference of one particular action over the others in a particular state. It doesn't have to learn the value of that state.</p>
<p>Regarding the optimization of the weights of the critic, it is optimized <span>using one of the well-known SGD optimization methods, </span>minimizing the MSE loss:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/4d75e122-9f78-42ae-bf15-f5b497f411f0.png" style="width:13.92em;height:3.08em;"/></p>
<p>In the previous equation, the target values are computed as follows: <img class="fm-editor-equation" src="assets/1c87dffc-1161-4c82-9c9b-053e1098d5a2.png" style="width:13.08em;height:1.42em;"/>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The AC implementation</h1>
                </header>
            
            <article>
                
<p>Overall, as we have seen so far, the AC algorithm is very similar to the REINFORCE algorithm, with the state function as a baseline. But, <span>to provide a </span>recap, <span>the algorithm is summarized</span> in the following code:</p>
<pre>Initialize <img class="fm-editor-equation" src="assets/ba44e9fe-6c69-4229-b9db-8b8bd7284157.png" style="width:1.33em;height:1.00em;"/> with random weight<br/>Initialize environment <img class="fm-editor-equation" src="assets/79ba2125-4265-4f12-88d5-d34338119699.png" style="width:8.08em;height:1.25em;"/><br/><strong>for</strong> episode 1..M <strong>do</strong><br/>    Initialize empty buffer<br/><br/>    <span class="underline"><em>&gt; Generate a few episodes</em></span><br/>    <strong>for</strong> step 1..MaxSteps <strong>do</strong><br/>        <em><span class="underline">&gt; Collect experience by acting on the environment</span></em><br/>        <img class="fm-editor-equation" src="assets/517f9a0a-4136-4fd0-af60-af88143b7798.png" style="width:5.42em;height:1.50em;"/><br/>        <img class="fm-editor-equation" src="assets/e8b86671-eb36-4da4-b4aa-8163353c2dde.png" style="width:7.92em;height:1.42em;"/><br/>        <img class="fm-editor-equation" src="assets/43be57d4-e0db-4fd2-994f-e4c39706d810.png" style="width:3.50em;height:1.17em;"/><br/>        <strong>if</strong> <img class="fm-editor-equation" src="assets/715d6a56-4eb9-4f3e-a4ed-0526e1f334b1.png" style="width:6.08em;height:1.08em;"/>:<br/>            <img class="fm-editor-equation" src="assets/3e52aff5-6632-4705-bc02-1fb4fdf5bc99.png" style="width:7.50em;height:1.25em;"/><br/>            <em><span class="underline">&gt; Compute the n-step reward to go</span> </em><br/>            <img class="fm-editor-equation" src="assets/b96e7564-e54f-4087-9087-74c8f915d0c8.png" style="width:13.42em;height:1.42em;"/> # for each t<br/>            <em><span class="underline">&gt; Compute the advantage values</span></em><br/>            <img class="fm-editor-equation" src="assets/c764c1ca-d501-4c08-a730-ef0b9bca12d6.png" style="width:9.50em;height:1.50em;"/> # for each t<br/>            <em><span class="underline">&gt; Store the episode in the buffer</span></em><br/>            <img class="fm-editor-equation" src="assets/8885240d-2081-450e-9e16-dc64f15d4423.png" style="width:18.00em;height:1.50em;"/> # where <img class="fm-editor-equation" src="assets/8c5a7efe-fd5e-4b04-8ff0-ecf6f28b784b.png" style="width:0.75em;height:0.92em;"/> is the lenght of the episode<br/>    <em><span class="underline">&gt; Actor update step using all the experience in <img class="fm-editor-equation" src="assets/c782ce23-3d71-4d58-9cc7-31cb78ce9b55.png" style="width:0.75em;height:0.75em;"/><br/></span></em>    <img class="fm-editor-equation" src="assets/b443a17c-74e0-4294-af15-80c809a8fa1f.png" style="width:18.75em;height:3.08em;"/><br/>    <em><span class="underline">&gt; Critic update using all the experience in <strong>D</strong></span></em><br/>    <img class="fm-editor-equation" src="assets/4b73d620-81b4-46c3-86fb-37b4f5bd5f50.png" style="width:17.33em;height:3.08em;"/></pre>
<p>The only differences with REINFORCE are the calculation of the n-step reward to go, the advantage function calculation, and a few adjustments of the main function. </p>
<p>Let's first look at the new implementation of the discounted reward. Differently to before, the estimated value of the last <kbd>last_sv</kbd> state is now passed in the input and is used to bootstrap, as given in the following implementation: </p>
<div>
<pre><span>def</span><span> </span><span>discounted_rewards</span><span>(</span><span>rews</span><span>, </span><span>last_sv</span><span>, </span><span>gamma</span><span>):<br/></span><span>    rtg </span><span>=</span><span> np.</span><span>zeros_like</span><span>(rews, </span><span>dtype</span><span>=</span><span>np.float32)<br/></span><span>    rtg[</span><span>-</span><span>1</span><span>] </span><span>=</span><span> rews[</span><span>-</span><span>1</span><span>] </span><span>+</span><span> gamma</span><span>*</span><span>last_sv    # Bootstrap with the estimate next state value <br/><br/></span><span>    for</span><span> i </span><span>in</span><span> </span><span>reversed</span><span>(</span><span>range</span><span>(</span><span>len</span><span>(rews)</span><span>-</span><span>1</span><span>)):<br/></span><span>        rtg[i] </span><span>=</span><span> rews[i] </span><span>+</span><span> gamma</span><span>*</span><span>rtg[i</span><span>+</span><span>1</span><span>]<br/></span><span>    return</span><span> rtg<br/></span></pre></div>
<p>The computational graph doesn't change, but in the main cycle, we have to take care of a few small, but very important, changes. </p>
<p>Obviously, the name of the function is changed to <kbd>AC</kbd>, and the learning rate of the <kbd>cr_lr</kbd> critic <span>is added as an argument.</span></p>
<p>The first actual change involves the way in which the environment is reset. If, in REINFORCE, it was preferred to reset the environment on every iteration of the main cycle, in AC, we have to resume the environment from where we left off in the previous iteration, resetting it only when it reaches its final state.</p>
<p>The second change involves the way in which the action-value function is bootstrapped, and how the reward to go is calculated. Remember that <img class="fm-editor-equation" src="assets/6cb365a1-c652-4002-8935-7a268cd532e6.png" style="width:10.17em;height:1.42em;"/> for every state-action pair, except in the case of when <img class="fm-editor-equation" src="assets/3657e493-86d3-42ff-9550-ed4ec30b965f.png" style="width:2.17em;height:1.17em;"/> is a final state. In this case, <img class="fm-editor-equation" src="assets/f60da65c-1493-4b3b-917d-714c659448a8.png" style="width:5.17em;height:1.33em;"/>. Thus, we have to bootstrap with a value of <kbd>0</kbd>, whenever we are in the last state, and bootstrap with <img class="fm-editor-equation" src="assets/057c60da-c9c2-4750-993b-8af4fd807af8.png" style="width:2.25em;height:1.17em;"/> in all the other cases. With these changes, the code is as follows:</p>
<div>
<pre><span>    obs</span> <span>=</span> <span>env.</span><span>reset</span><span>()<br/></span><span>    ep_rews</span> <span>=</span> <span>[]<br/><br/></span><span>    for ep in range(num_epochs):</span><span><br/></span><span>        buffer</span> <span>=</span> <span>Buffer</span><span>(gamma)<br/></span><span>        env_buf</span> <span>=</span> <span>[]<br/><br/></span><span>        for</span> <span>_</span> <span>in</span> <span>range</span><span>(steps_per_env):<br/></span><span>            act, val</span> <span>=</span> <span>sess.</span><span>run</span><span>([act_multn, s_values],</span> <span>feed_dict</span><span>=</span><span>{obs_ph:[obs]})<br/></span><span>            obs2, rew, done, _</span> <span>=</span> <span>env.</span><span>step</span><span>(np.</span><span>squeeze</span><span>(act))<br/><br/></span><span>            env_buf.</span><span>append</span><span>([obs.</span><span>copy</span><span>(), rew, act, np.</span><span>squeeze</span><span>(val)])<br/></span><span>            obs</span> <span>=</span> <span>obs2.</span><span>copy</span><span>()<br/></span><span>            step_count</span> <span>+=</span> <span>1<br/></span><span>            last_test_step</span> <span>+=</span> <span>1<br/></span><span>            ep_rews.</span><span>append</span><span>(rew)<br/><br/></span><span>            </span><span>if</span> <span>done:<br/></span><span><br/></span><span>                buffer.</span><span>store</span><span>(np.</span><span>array</span><span>(env_buf),</span> <span>0</span><span>)<br/></span><span>                env_buf</span> <span>=</span> <span>[]<br/><br/></span><span>                train_rewards.</span><span>append</span><span>(np.</span><span>sum</span><span>(ep_rews))<br/></span><span>                train_ep_len.</span><span>append</span><span>(</span><span>len</span><span>(ep_rews))<br/></span><span>                obs</span> <span>=</span> <span>env.</span><span>reset</span><span>()<br/></span><span>                ep_rews</span> <span>=</span> <span>[]<br/><br/></span><span>        if</span> <span>len</span><span>(env_buf)</span> <span>&gt;</span> <span>0</span><span>:<br/></span><span>            last_sv</span> <span>=</span> <span>sess.</span><span>run</span><span>(s_values,</span> <span>feed_dict</span><span>=</span><span>{obs_ph:[obs]})<br/></span><span>            buffer.</span><span>store</span><span>(np.</span><span>array</span><span>(env_buf), last_sv)<br/><br/></span><span>        obs_batch, act_batch, ret_batch, rtg_batch</span> <span>=</span> <span>buffer.</span><span>get_batch</span><span>()<br/></span><span>        sess.</span><span>run</span><span>([p_opt, v_opt],</span> <span>feed_dict</span><span>=</span><span>{obs_ph:obs_batch, act_ph:act_batch, ret_ph:ret_batch,         rtg_ph:rtg_batch})<br/>        ...</span></pre></div>
<p>The third change is in the <kbd>store</kbd> method of the <kbd>Buffer</kbd> class. In fact, now, we also have to deal with incomplete trajectories. In the previous snippet, we saw that the estimated <img class="fm-editor-equation" src="assets/057c60da-c9c2-4750-993b-8af4fd807af8.png" style="width:2.17em;height:1.17em;"/> state values are passed as the third argument to the <kbd>store</kbd> function. Indeed, we use them to bootstrap and to compute the reward to go. In the new version of <kbd>store</kbd>, we call the variable that is associated with the state values, <kbd>last_sv</kbd>, and pass it as the input to the <kbd>discounted_reward</kbd> function, as follows:</p>
<div>
<pre>    def<span> </span><span>store</span><span>(</span><span>self</span><span>, </span><span>temp_traj</span><span>, </span><span>last_sv</span><span>):<br/></span>        if<span> </span><span>len</span><span>(temp_traj) </span><span>&gt;</span><span> </span><span>0</span><span>:<br/></span>            self<span>.obs.</span><span>extend</span><span>(temp_traj[</span><span>:</span><span>,</span><span>0</span><span>])<br/></span>            rtg <span>=</span><span> </span><span>discounted_rewards</span><span>(temp_traj[</span><span>:</span><span>,</span><span>1</span><span>], last_sv, </span><span>self</span><span>.gamma)<br/></span>            self<span>.ret.</span><span>extend</span><span>(rtg </span><span>-</span><span> temp_traj[</span><span>:</span><span>,</span><span>3</span><span>])<br/></span>            self<span>.rtg.</span><span>extend</span><span>(rtg)<br/></span>            self<span>.act.</span><span>extend</span><span>(temp_traj[</span><span>:</span><span>,</span><span>2</span><span>])</span></pre></div>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Landing a spacecraft using AC </h1>
                </header>
            
            <article>
                
<p>We applied AC to LunarLander-v2, the same environment used for testing REINFORCE. It is an episodic game, and as such, it doesn't fully emphasize the main qualities of the AC algorithm. Nonetheless, it provides a good testbed, and you can freely test it in another environment.</p>
<p>We call the <kbd>AC</kbd> function with the following hyperparameters:</p>
<div>
<pre><span>AC</span><span>(</span><span>'</span><span>LunarLander-v2</span><span>'</span><span>, </span><span>hidden_sizes</span><span>=</span><span>[</span><span>64</span><span>], </span><span>ac_lr</span><span>=4</span><span>e-3</span><span>, </span><span>cr_lr</span><span>=</span><span>1.5e-2</span><span>, </span><span>gamma</span><span>=</span><span>0.99</span><span>, </span><span>steps_per_epoch</span><span>=</span><span>100</span><span>,</span><span> </span><span>num_epochs</span><span>=</span><span>8000</span><span>)</span></pre></div>
<p>The resulting plot that shows the total reward accumulated in the training epochs is as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2068 image-border" src="assets/898b67e6-f59f-42db-8618-b8a038f456a8.png" style="width:127.50em;height:76.00em;"/></p>
<p>You can see that AC is faster than REINFORCE, as shown in the following plot. However, it is less stable, and after about 200,000 steps, the performance declines a little bit, fortunately continuing to increment afterward:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-2069 image-border" src="assets/e162f2ae-1142-428e-8fd9-fe438e87125e.png" style="width:126.83em;height:75.83em;"/></div>
<p class="mce-root">In this configuration, the AC algorithm updates the actor and critic every 100 steps. In theory, you could use a smaller <kbd>steps_per_epochs</kbd> but, usually, it makes the training more unstable. Using a longer epoch can stabilize the training, but the actor learns more slowly. It's all about finding a good trade-off and good learning rates.</p>
<div class="packt_infobox"><span>For all the color references mentioned in the chapter, please refer to the color images bundle at </span><a href="http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf">http://www.packtpub.com/sites/default/files/downloads/9781789131116_ColorImages.pdf</a><span>.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced AC, and tips and tricks</h1>
                </header>
            
            <article>
                
<p>There are several further advancements of AC algorithms, and there are many tips and tricks to keep in mind, while designing such algorithms:</p>
<ul>
<li><strong>Architectural design</strong>: In our implementation, we implemented two distinct neural networks, one for the critic, and one for the actor. It's also possible to design a neural network that shares the main hidden layers, while keeping the heads distinct. This architecture can be more difficult to tune, but overall, it increases the efficiency of the algorithms.</li>
<li><strong>Parallel environments</strong>: A widely adopted technique to decrease the variance is to collect experience from multiple environments in parallel. The <strong>A3C</strong> (<strong>Asynchronous Advantage Actor-Critic</strong>) algorithm updates the global parameters asynchronously. Instead, the synchronous version of it, called <strong>A2C</strong> (<strong>Advantage Actor-Critic</strong>) waits for all of the parallel actors to finish before updating the global parameters. The agent parallelization ensures more independent experience from different parts of the environment.</li>
<li><strong>Batch size</strong>: With respect to other RL algorithms (especially off-policy algorithms), policy gradient and AC methods need large batches. Thus, if after tuning the other hyperparameters, the algorithm doesn't stabilize, consider using a larger batch size.</li>
<li><strong>Learning rate</strong>: Tuning the learning rate in itself is very tricky, so make sure that you use a more advanced SGD optimization method, such as Adam or RMSprop.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned about a new class of reinforcement learning algorithms called policy gradients. They approach the RL problem in a different way, compared to the value function methods that were studied in the previous chapters. </p>
<p>The simpler version of PG methods is called REINFORCE, which was learned, implemented, and tested throughout the course of this chapter. We then proposed adding a baseline in REINFORCE in order to decrease the variance and increase the convergence property of the algorithm. AC algorithms are free from the need for a full trajectory using a critic, and thus, we then solved the same problem using the AC model. </p>
<p>With a solid foundation of the classic policy gradient algorithms, we can now go further. In the next chapter, we'll look at some more complex, state-of-the-art policy gradient algorithms; namely, <strong>Trust Region Policy Optimization</strong> (<strong>TRPO</strong>) and <strong>Proximal Policy Optimization</strong> (<strong>PPO</strong>). These two algorithms are built on top of the material that we have covered in this chapter, but additionally, they propose a new objective function that improves the stability and efficiency of PG algorithms.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>How do PG algorithms maximize the objective function?</li>
<li>What's the main <span>idea </span>behind policy gradient algorithms?</li>
<li>Why does the <span>algorithm remain unbiased when </span>introducing a baseline in REINFORCE?</li>
<li>What broader class of algorithms does <span>REINFORCE </span>belong to?</li>
<li>How does the critic in AC methods <span>differ </span><span>from a value function that is used as a baseline in REINFORCE?</span></li>
<li>If you had to develop an algorithm for an agent that has to learn to move, would you prefer REINFORCE or AC?</li>
<li>Could you use an n-step AC algorithm as a REINFORCE algorithm? </li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>To learn about an asynchronous version of the actor-critic algorithm, read <a href="https://arxiv.org/pdf/1602.01783.pdf">https://arxiv.org/pdf/1602.01783.pdf</a>.</p>


            </article>

            
        </section>
    </body></html>