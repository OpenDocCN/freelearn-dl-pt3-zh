["```\nimport gym\nenv = gym.make('SpaceInvaders-v0')\nenv.reset()\nenv.render()\n```", "```\nimport numpy as np\nimport sys\nimport os\nimport random\nimport tensorflow as tf\n```", "```\nNET = 'bigger' # 'smaller'\n```", "```\nLOSS = 'huber' # 'L2'\n```", "```\ninit = tf.variance_scaling_initializer(scale=2) # tf.contrib.layers.xavier_initializer()\n```", "```\nclass QNetwork():\n def __init__(self, scope=\"QNet\", VALID_ACTIONS=[0, 1, 2, 3]):\n self.scope = scope\n self.VALID_ACTIONS = VALID_ACTIONS\n with tf.variable_scope(scope):\n self._build_model()\n```", "```\ndef _build_model(self):\n  # input placeholders; input is 4 frames of shape 84x84 \n  self.tf_X = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n  # TD\n  self.tf_y = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n  # action\n  self.tf_actions = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n  # normalize input\n  X = tf.to_float(self.tf_X) / 255.0\n  batch_size = tf.shape(self.tf_X)[0]\n```", "```\nif (NET == 'bigger'):\n  # bigger net\n  # 3 conv layers\n  conv1 = tf.contrib.layers.conv2d(X, 32, 8, 4, padding='VALID', activation_fn=tf.nn.relu, weights_initializer=winit)\n  conv2 = tf.contrib.layers.conv2d(conv1, 64, 4, 2, padding='VALID', activation_fn=tf.nn.relu, weights_initializer=winit)\n  conv3 = tf.contrib.layers.conv2d(conv2, 64, 3, 1, padding='VALID', activation_fn=tf.nn.relu, weights_initializer=winit)\n  # fully connected layers\n  flattened = tf.contrib.layers.flatten(conv3)\n  fc1 = tf.contrib.layers.fully_connected(flattened, 512, activation_fn=tf.nn.relu, weights_initializer=winit)\n\n  elif (NET == 'smaller'): \n\n  # smaller net\n  # 2 conv layers\n  conv1 = tf.contrib.layers.conv2d(X, 16, 8, 4, padding='VALID', activation_fn=tf.nn.relu, weights_initializer=winit)\n  conv2 = tf.contrib.layers.conv2d(conv1, 32, 4, 2, padding='VALID',activation_fn=tf.nn.relu, weights_initializer=winit)\n  # fully connected layers\n  flattened = tf.contrib.layers.flatten(conv2)\n  fc1 = tf.contrib.layers.fully_connected(flattened, 256, activation_fn=tf.nn.relu, weights_initializer=winit) \n```", "```\n# Q(s,a)\n  self.predictions = tf.contrib.layers.fully_connected(fc1, len(self.VALID_ACTIONS), activation_fn=None, weights_initializer=winit)\n  action_one_hot = tf.one_hot(self.tf_actions, tf.shape(self.predictions)[1], 1.0, 0.0, name='action_one_hot')\n  self.action_predictions = tf.reduce_sum(self.predictions * action_one_hot, reduction_indices=1, name='act_pred')\n```", "```\nif (LOSS == 'L2'):\n   # L2 loss\n   self.loss = tf.reduce_mean(tf.squared_difference(self.tf_y, self.action_predictions), name='loss')\nelif (LOSS == 'huber'):\n   # Huber loss\n   self.loss = tf.reduce_mean(huber_loss(self.tf_y-self.action_predictions), name='loss')\n```", "```\n# optimizer \n  #self.optimizer = tf.train.RMSPropOptimizer(learning_rate=0.00025, momentum=0.95, epsilon=0.01)\n  self.optimizer = tf.train.AdamOptimizer(learning_rate=2e-5)\n  self.train_op=\n        self.optimizer.minimize(self.loss,global_step=tf.contrib.framework.get_global_step())\n```", "```\ndef predict(self, sess, s):\n   return sess.run(self.predictions, { self.tf_X: s})\n```", "```\ndef update(self, sess, s, a, y):\n   feed_dict = { self.tf_X: s, self.tf_y: y, self.tf_actions: a }\n   _, loss = sess.run([self.train_op, self.loss], feed_dict)\n   return loss\n```", "```\n# huber loss\ndef huber_loss(x):\n condition = tf.abs(x) < 1.0\n output1 = 0.5 * tf.square(x)\n output2 = tf.abs(x) - 0.5\n return tf.where(condition, output1, output2)\n```", "```\nimport numpy as np\nimport sys\nimport tensorflow as tf\n```", "```\n# convert raw Atari RGB image of size 210x160x3 into 84x84 grayscale image\nclass ImageProcess():\n  def __init__(self):\n    with tf.variable_scope(\"state_processor\"):\n     self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n     self.output = tf.image.rgb_to_grayscale(self.input_state)\n     self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n     self.output = tf.image.resize_images(self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n     self.output = tf.squeeze(self.output)\n\n  def process(self, sess, state):\n    return sess.run(self.output, { self.input_state: state })\n```", "```\n# copy params from qnet1 to qnet2\ndef copy_model_parameters(sess, qnet1, qnet2):\n    q1_params = [t for t in tf.trainable_variables() if t.name.startswith(qnet1.scope)]\n    q1_params = sorted(q1_params, key=lambda v: v.name)\n    q2_params = [t for t in tf.trainable_variables() if t.name.startswith(qnet2.scope)]\n    q2_params = sorted(q2_params, key=lambda v: v.name)\n    update_ops = []\n    for q1_v, q2_v in zip(q1_params, q2_params):\n        op = q2_v.assign(q1_v)\n        update_ops.append(op)\n    sess.run(update_ops)\n```", "```\n# epsilon-greedy\ndef epsilon_greedy_policy(qnet, num_actions):\n    def policy_fn(sess, observation, epsilon):\n        if (np.random.rand() < epsilon): \n          # explore: equal probabiities for all actions\n          A = np.ones(num_actions, dtype=float) / float(num_actions)\n        else:\n          # exploit \n          q_values = qnet.predict(sess, np.expand_dims(observation, 0))[0]\n          max_Q_action = np.argmax(q_values)\n          A = np.zeros(num_actions, dtype=float)\n          A[max_Q_action] = 1.0 \n        return A\n    return policy_fn\n```", "```\n# populate replay memory\ndef populate_replay_mem(sess, env, state_processor, replay_memory_init_size, policy, epsilon_start, epsilon_end, epsilon_decay_steps, VALID_ACTIONS, Transition):\n    state = env.reset()\n    state = state_processor.process(sess, state)\n    state = np.stack([state] * 4, axis=2)\n\n    delta_epsilon = (epsilon_start - epsilon_end)/float(epsilon_decay_steps)\n\n    replay_memory = []\n```", "```\nfor i in range(replay_memory_init_size):\n        epsilon = max(epsilon_start - float(i) * delta_epsilon, epsilon_end)\n        action_probs = policy(sess, state, epsilon)\n        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n\n        env.render() \n        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n\n        next_state = state_processor.process(sess, next_state)\n        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n        replay_memory.append(Transition(state, action, reward, next_state, done))\n\n        if done:\n            state = env.reset()\n            state = state_processor.process(sess, state)\n            state = np.stack([state] * 4, axis=2)\n        else:\n            state = next_state\n    return replay_memory\n```", "```\nimport gym\nimport itertools\nimport numpy as np\nimport os\nimport random\nimport sys\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom collections import deque, namedtuple\nfrom model import *\nfrom funcs import *\n```", "```\nGAME = \"BreakoutDeterministic-v4\" # \"BreakoutDeterministic-v0\"\n# Atari Breakout actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) \nVALID_ACTIONS = [0, 1, 2, 3]\n```", "```\n# set parameters for running\ntrain_or_test = 'train' #'test' #'train'\ntrain_from_scratch = True\nstart_iter = 0\nstart_episode = 0\nepsilon_start = 1.0\n```", "```\nenv = gym.envs.make(GAME)\nprint(\"Action space size: {}\".format(env.action_space.n))\nobservation = env.reset()\nprint(\"Observation space shape: {}\".format(observation.shape)\n```", "```\n# experiment dir\nexperiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n\n# create ckpt directory \ncheckpoint_dir = os.path.join(experiment_dir, \"ckpt\")\ncheckpoint_path = os.path.join(checkpoint_dir, \"model\")\n\n```", "```\nif not os.path.exists(checkpoint_dir):\n os.makedirs(checkpoint_dir)\n```", "```\ndef deep_q_learning(sess, env, q_net, target_net, state_processor, num_episodes, train_or_test='train', train_from_scratch=True,start_iter=0, start_episode=0, replay_memory_size=250000, replay_memory_init_size=50000, update_target_net_every=10000, gamma=0.99, epsilon_start=1.0, epsilon_end=[0.1,0.01], epsilon_decay_steps=[1e6,1e6], batch_size=32):\n\n    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n\n    # policy \n    policy = epsilon_greedy_policy(q_net, len(VALID_ACTIONS))\n```", "```\n# populate replay memory\n if (train_or_test == 'train'):\n   print(\"populating replay memory\")\n   replay_memory = populate_replay_mem(sess, env, state_processor, replay_memory_init_size, policy, epsilon_start, epsilon_end[0], epsilon_decay_steps[0], VALID_ACTIONS, Transition)\n```", "```\n# epsilon start\nif (train_or_test == 'train'):\n  delta_epsilon1 = (epsilon_start - epsilon_end[0])/float(epsilon_decay_steps[0]) \n  delta_epsilon2 = (epsilon_end[0] - epsilon_end[1])/float(epsilon_decay_steps[1]) \n  if (train_from_scratch == True):\n    epsilon = epsilon_start\n  else:\n    if (start_iter <= epsilon_decay_steps[0]):\n      epsilon = max(epsilon_start - float(start_iter) * delta_epsilon1, epsilon_end[0])\n    elif (start_iter > epsilon_decay_steps[0] and start_iter < epsilon_decay_steps[0]+epsilon_decay_steps[1]):\n      epsilon = max(epsilon_end[0] - float(start_iter) * delta_epsilon2, epsilon_end[1])\n    else:\n      epsilon = epsilon_end[1] \nelif (train_or_test == 'test'):\n  epsilon = epsilon_end[1]\n```", "```\n# total number of time steps \ntotal_t = start_iter\n```", "```\nfor ep in range(start_episode, num_episodes):\n\n        # save ckpt\n        saver.save(tf.get_default_session(), checkpoint_path)\n\n        # env reset\n        state = env.reset()\n        state = state_processor.process(sess, state)\n        state = np.stack([state] * 4, axis=2)\n\n        loss = 0.0\n        time_steps = 0\n        episode_rewards = 0.0\n\n        ale_lives = 5\n        info_ale_lives = ale_lives\n        steps_in_this_life = 1000000\n        num_no_ops_this_life = 0\n```", "```\nwhile True:\n\n    if (train_or_test == 'train'):\n        #epsilon = max(epsilon - delta_epsilon, epsilon_end) \n        if (total_t <= epsilon_decay_steps[0]):\n            epsilon = max(epsilon - delta_epsilon1, epsilon_end[0]) \n        elif (total_t >= epsilon_decay_steps[0] and total_t <= epsilon_decay_steps[0]+epsilon_decay_steps[1]):\n            epsilon = epsilon_end[0] - (epsilon_end[0]-epsilon_end[1]) / float(epsilon_decay_steps[1]) * float(total_t-epsilon_decay_steps[0]) \n            epsilon = max(epsilon, epsilon_end[1]) \n        else:\n            epsilon = epsilon_end[1]\n```", "```\n # update target net\n if total_t % update_target_net_every == 0:\n    copy_model_parameters(sess, q_net, target_net)\n    print(\"\\n copied params from Q net to target net \")\n```", "```\ntime_to_fire = False\nif (time_steps == 0 or ale_lives != info_ale_lives):\n   # new game or new life \n   steps_in_this_life = 0\n   num_no_ops_this_life = np.random.randint(low=0,high=7)\n   action_probs = [0.0, 1.0, 0.0, 0.0] # fire\n   time_to_fire = True\n   if (ale_lives != info_ale_lives):\n       ale_lives = info_ale_lives\nelse:\n   action_probs = policy(sess, state, epsilon)\n\nsteps_in_this_life += 1 \nif (steps_in_this_life < num_no_ops_this_life and not time_to_fire):\n   # no-op\n   action_probs = [1.0, 0.0, 0.0, 0.0] # no-op\n```", "```\naction = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n\nenv.render()\nnext_state_img, reward, done, info = env.step(VALID_ACTIONS[action]) \n\ninfo_ale_lives = int(info['ale.lives'])\n\n# rewards = -1,0,+1 as done in the paper\n#reward = np.sign(reward)\n\nnext_state_img = state_processor.process(sess, next_state_img)\n\n# state is of size [84,84,4]; next_state_img is of size[84,84]\n#next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\nnext_state = np.zeros((84,84,4),dtype=np.uint8)\nnext_state[:,:,0] = state[:,:,1] \nnext_state[:,:,1] = state[:,:,2]\nnext_state[:,:,2] = state[:,:,3]\nnext_state[:,:,3] = next_state_img \n\nepisode_rewards += reward \ntime_steps += 1\n```", "```\n  if (train_or_test == 'train'):\n\n     # if replay memory is full, pop the first element\n     if len(replay_memory) == replay_memory_size:\n         replay_memory.pop(0)\n\n     # save transition to replay memory\n     # done = True in replay memory for every loss of life \n     if (ale_lives == info_ale_lives):\n         replay_memory.append(Transition(state, action, reward, next_state, done)) \n     else:\n         #print('loss of life ')\n         replay_memory.append(Transition(state, action, reward, next_state, True)) \n\n     # sample a minibatch from replay memory\n     samples = random.sample(replay_memory, batch_size)\n     states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n\n     # calculate q values and targets \n     q_values_next = target_net.predict(sess, next_states_batch)\n     greedy_q = np.amax(q_values_next, axis=1) \n     targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * gamma * greedy_q\n\n     # update net \n     if (total_t % 4 == 0):\n         states_batch = np.array(states_batch)\n         loss = q_net.update(sess, states_batch, action_batch, targets_batch)\n```", "```\nif done:\n    #print(\"done: \", done)\n    break\n\nstate = next_state\ntotal_t += 1\n\n  if (train_or_test == 'train'): \n      print('\\n Episode: ', ep, '| time steps: ', time_steps, '| total episode reward: ', episode_rewards, '| total_t: ', total_t, '| epsilon: ', epsilon, '| replay mem size: ', len(replay_memory))\n  elif (train_or_test == 'test'):\n      print('\\n Episode: ', ep, '| time steps: ', time_steps, '| total episode reward: ', episode_rewards, '| total_t: ', total_t, '| epsilon: ', epsilon)\n\n  if (train_or_test == 'train'):\n      f = open(\"experiments/\" + str(env.spec.id) + \"/performance.txt\", \"a+\")\n      f.write(str(ep) + \" \" + str(time_steps) + \" \" + str(episode_rewards) + \" \" + str(total_t) + \" \" + str(epsilon) + '\\n') \n      f.close()\n```", "```\ntf.reset_default_graph()\n\n# Q and target networks \nq_net = QNetwork(scope=\"q\",VALID_ACTIONS=VALID_ACTIONS)\ntarget_net = QNetwork(scope=\"target_q\", VALID_ACTIONS=VALID_ACTIONS)\n\n# state processor\nstate_processor = ImageProcess()\n\n# tf saver\nsaver = tf.train.Saver()\n```", "```\nwith tf.Session() as sess:\n\n      # load model/ initialize model\n      if ((train_or_test == 'train' and train_from_scratch == False) or train_or_test == 'test'):\n                 latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n                 print(\"loading model ckpt {}...\\n\".format(latest_checkpoint))\n                 saver.restore(sess, latest_checkpoint)\n      elif (train_or_test == 'train' and train_from_scratch == True):\n                 sess.run(tf.global_variables_initializer()) \n\n      # run\n      deep_q_learning(sess, env, q_net=q_net, target_net=target_net, state_processor=state_processor, num_episodes=25000, train_or_test=train_or_test,train_from_scratch=train_from_scratch, start_iter=start_iter, start_episode=start_episode, replay_memory_size=300000, replay_memory_init_size=5000, update_target_net_every=10000, gamma=0.99, epsilon_start=epsilon_start, epsilon_end=[0.1,0.01], epsilon_decay_steps=[1e6,1e6], batch_size=32)\n```"]