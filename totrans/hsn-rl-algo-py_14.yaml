- en: Understanding Black-Box Optimization Algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we looked at reinforcement learning algorithms, ranging
    from value-based to policy-based methods and from model-free to model-based methods. In
    this chapter, we'll provide another solution for solving sequential tasks, that
    is, with a class of black-box algorithms **evolutionary algorithms** (**EA**). EAs
    are driven by evolutionary mechanisms and are sometimes preferred to **reinforcement
    learning** (**RL**) as they don't require backpropagation. They also offer other
    complementary benefits to RL. We'll start this chapter by giving you a brief recap
    of RL algorithms so that you'll better understand how EA fits into these sets
    of problems. Then, you'll learn about the basic building blocks of EA and how
    those algorithms work. We'll also take advantage of this introduction and look
    at one of the most well-known EAs, namely **evolution strategies** (**ES**), in
    more depth.
  prefs: []
  type: TYPE_NORMAL
- en: A recent algorithm that was developed by OpenAI caused a great boost in the
    adoption of ES for solving sequential tasks. They showed how ES algorithms can
    be massively parallelized and scaled linearly on a number of CPUs while achieving
    high performance. After an explanation of evolution strategies, we'll take a deeper
    look at this algorithm and develop it in TensorFlow so that you'll be able to
    apply it to the tasks you care about.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Beyond RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The core of EAs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable evolution strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable ES applied to LunarLander
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RL algorithms are the usual choice when we're faced with sequential decision
    problems. Usually, it's difficult to find other ways to solve these tasks other
    than using RL. Despite the hundreds of different optimization methods that are
    out there, so far, only RL has worked well on problems for sequential decision-making.
    But this doesn't mean it's the only option.
  prefs: []
  type: TYPE_NORMAL
- en: We'll start this chapter by recapping on the inner workings of RL algorithms
    and questioning the usefulness of their components for solving sequential tasks.
    This brief summary will help us introduce a new type of algorithm that offers
    many advantages (as well as some disadvantages) that could be used as a replacement
    for RL.
  prefs: []
  type: TYPE_NORMAL
- en: A brief recap of RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the beginning, a policy is initialized randomly and used to interact with
    the environment for either a given number of steps, or entire trajectories, to
    collect data. On each interaction, the state visited, the action taken, and the
    reward obtained are recorded. This information provides a full description of
    the influence of the agent in the environment. Then, in order to improve the policy,
    the backpropagation algorithm (based on the loss function, in order to move the
    predictions to a better estimate) computes the gradient of each weight of the
    network. These gradients are then applied with a stochastic gradient descent optimizer.
    This process (gathering data from the environment and optimizing the neural network
    with **stochastic gradient descent** (**SGD**)) is repeated until a convergence
    criterion is met.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two important things to note here that will be useful in the following
    discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Temporal credit assignment**: Because RL algorithms optimize the policy on
    each step, allocating the quality of each action and state is required. This is
    done by assigning a value to each state-action pair. Moreover, a discount factor
    is used to minimize the influence of distant actions and to give more weight to
    the last actions. This will help us solve the problem of assigning the credit
    to the actions, but will also introduce inaccuracies in the system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploration**: In order to maintain a degree of exploration in the actions, additional
    noise is injected into the policy of RL algorithms. The way in which the noise
    is injected depends on the algorithm, but usually, the actions are sampled from
    a stochastic distribution. By doing so, if the agent is in the same situation
    twice, it may take different actions that would lead to two different paths. This
    strategy also encourages exploration in deterministic environments. By deviating
    the path each time, the agent may discover different – and potentially better –
    solutions. With this additional noise that asymptotically tends to 0, the agent
    is then able to converge to a better and final deterministic policy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: But are backpropagation, temporal credit assignment, and stochastic actions
    actually a prerequisite for learning and building complex policies?
  prefs: []
  type: TYPE_NORMAL
- en: The alternative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The answer to this question is no.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned in [Chapter 10](e61cf178-d8e8-47bf-ab79-ef546f23e309.xhtml), *Imitation
    Learning with the DAgger Algorithm*, by reducing policy learning to an imitation
    problem using backpropagation and SGD, we can learn about a discriminative model from
    an expert in order to predict which actions to take next. Still, this involves
    backpropagation and requires an expert that may not always be available.
  prefs: []
  type: TYPE_NORMAL
- en: Another general subset of algorithms for global optimization does exist. They
    are called EAs, and they aren't based on backpropagation and don't require any
    of the other two principles, namely temporal credit assignment and noisy actions.
    Furthermore, as we said in the introduction to this chapter, these evolutionary
    algorithms are very general and can be used in a large variety of problems, including
    sequential decision tasks.
  prefs: []
  type: TYPE_NORMAL
- en: EAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you may have guessed, EAs differ in many aspects from RL algorithms and are
    principally inspired by biological evolution. EAs include many similar methods
    such as, genetic algorithms, evolution strategies, and genetic programming, which
    vary in their implementation details and in the nature of their representation.
    However, they are all mainly based on four basic mechanisms – reproductions, mutation,
    crossover, and selection – that are cycled in a guess-and-check process. We'll
    see what this means as we progress through this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Evolutionary algorithms are defined as black-box algorithms. These are algorithms
    that optimize a function, [![](img/7bf9bc1b-7d93-4527-8b6c-db935d6b9f12.png)],
    with respect to [![](img/78d5816f-a32e-4420-83f3-a85a85de01f8.png)] without making
    any assumption about [![](img/0da66c3f-a9e7-4314-9b1c-8029fbd2092e.png)]. Hence, [![](img/469deea3-9131-4342-8c89-058bb702d379.png)] can
    be anything you want. We only care about the output of [![](img/7d4de719-2d7d-4b35-8071-af7c8167991a.png)].
    This has many advantages, as well as some disadvantages. The primary advantage
    is that we don't have to care about the structure of [![](img/434d6333-a4e5-45a8-9b64-e420e9184a95.png)] and
    we are free to use what is best for us and for the problem at hand. On the other
    hand, the main disadvantage is that these optimization methods cannot be explained
    and thus their mechanism cannot be interpreted. In problems where interpretability
    is of great importance, these methods are not appealing.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning has almost always been preferred for solving sequential
    tasks, especially for medium to difficult tasks. However, a recent paper from
    OpenAI highlights that the evolution strategy, which is an evolutionary algorithm,
    can be used as an alternative to RL. This statement is mainly due to the performance
    that's reached asymptotically by the algorithm and its incredible ability to be
    scaled across thousands of CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at how this algorithm is able to scale so well while learning
    good policies on difficult tasks, let's take a more in-depth look at EAs.
  prefs: []
  type: TYPE_NORMAL
- en: The core of EAs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: EAs are inspired by biological evolution and implement techniques and mechanisms
    that simulate biological evolution. This means that EAs go through many trials
    to create a population of new candidate solutions. These solutions are also called
    **individuals** (in RL problems, a candidate solution is a policy) that are better
    than the previous generation, in a similar way to the process within nature wherein
    only the strongest survive and have the possibility to procreate.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the advantages of EAs is that they are derivative-free methods, meaning
    that they don''t use the derivative to find the solution. This allows EAs to work
    very well with all sorts of differentiable and non-differentiable functions, including deep
    neural networks. This combination is schematized in the following diagram. Note
    that each individual is a separate deep neural network, and so we''ll have as
    many neural networks as the number of individuals at any given moment. In the
    following diagram, the population is composed of five individuals:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/185767dc-22b5-4956-a2fe-f07e80085a75.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1\. Optimization of deep neural networks through evolutionary algorithms
  prefs: []
  type: TYPE_NORMAL
- en: 'The specificity of each type of evolutionary algorithm differs from the others,
    but the underlying cycle is common to all the EAs and works as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A population of individuals (also called **candidate solutions** or **phenotypes**)
    is created so that each of them has a set of different properties (called **chromosomes**
    or **genotypes**). The initial population is initialized randomly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Each candidate solution is evaluated independently by a fitness function that
    determines its quality. The fitness function is usually related to the objective
    function and, using the terminology we've used so far, the fitness function could
    be the total reward accumulated by the agent (that is, the candidate solution)
    throughout its life.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, the fitter individuals of the population are selected, and their genome
    is modified in order to produce the new generation. In some cases, the less fit
    candidate solution can be used as a negative example to generate the next generation.
    This whole step varies largely, depending on the algorithm. Some algorithms, such
    as genetic algorithms, breed new individuals through two processes called **crossover**
    and **mutation**, which give birth to new individuals (called **offspring**).
    Others, such as evolution strategies, breed new individuals through mutation only.
    We'll explain crossover and mutation in more depth later in this chapter, but
    generally speaking, crossover is the process that combines genetic information
    from two parents, while mutation only alters some gene values in the offspring.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat the whole process, going through steps 1-3 until a terminal condition
    is met. On each iteration, the population that's created is also called a **generation**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This iterative process, as shown in the following diagram, terminates when
    a given fitness level has been reached or a maximum number of generations have
    been produced. As we can see, the population is created by crossover and mutation,
    but as we habe already explained, these processes may vary, depending on the specific
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f73e8a1d-c237-49b8-9e85-d6a9565eb061.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2\. The main cycle of evolutionary algorithms
  prefs: []
  type: TYPE_NORMAL
- en: 'The main body of a general EA is very simple and can be written in just a few
    lines of code, as shown here. To summarize this code, on each iteration, and until
    a fitted generation has been produced, new candidates are generated and evaluated.
    The candidates are created from the best-fitted individuals of the previous generation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Note that the implementation details of the solver are dependent on the algorithm
    that's used.
  prefs: []
  type: TYPE_NORMAL
- en: The applications of EAs are actually spread across many fields and problems,
    from economy to biology, and from computer program optimization to ant colony
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Since we are mostly interested in the application of evolutionary algorithms
    for solving sequential decision-making tasks, we will explain the two most common
    EAs that are used to solve these kinds of jobs. They are known as **genetic algorithms**
    (**GAs**) and **evolution strategies** (**ESes**). Later, we'll take a step further
    with ES by developing a highly scalable version of it.
  prefs: []
  type: TYPE_NORMAL
- en: Genetic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The idea of GAs is very straightforward—evaluate the current generations, use
    only the top-performing individuals to generate the next candidate solutions,
    and discard the other individuals. This is shown in the preceding diagram. The
    survivors will generate the next population by crossover and mutation. These two
    processes are represented in the following diagram. Crossover is done by selecting
    two solutions among the survivors and combining their parameters. Mutation, on
    the other hand, involves changing a few random parameters on the offspring''s
    genotype:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40983c68-bd09-46a1-8955-2f6bb61edb4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3\. Visual illustration of mutation and crossover
  prefs: []
  type: TYPE_NORMAL
- en: Crossover and mutation can be approached in many different ways. In the simpler
    version, crossover is done by choosing parts from the two parents randomly, and
    mutation is done by mutating the solution that's obtained by adding Gaussian noise
    with a fixed standard deviation. By only keeping the best individuals and injecting
    their genes into the newly born individuals, the solutions will improve over time
    until a condition is met. However, on complex problems, this simple solution is
    prone to be stuck in a local optimum (meaning that the solution is only within
    a small set of candidate solutions). In this case, a more advanced genetic algorithm
    such as **NeroEvolution of Augmenting Topologies** (**NEAT**) is preferred. NEAT
    not only alters the weights of the network but also its structure.
  prefs: []
  type: TYPE_NORMAL
- en: Evolution strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Evolution strategies** (**ESes**) are even easier than GAs as they are primarily
    based on mutation to create a new population.'
  prefs: []
  type: TYPE_NORMAL
- en: Mutation is performed by adding values that have been sampled from a normal
    distribution to the genotype. A very simple version of ES is obtained by just
    selecting the most performant individual across the whole population and sampling
    the next generation from a normal distribution with a fixed standard deviation
    and a mean equal to that of the best-performing individual.
  prefs: []
  type: TYPE_NORMAL
- en: Outside of the sphere of small problems, using this algorithm is not recommended.
    This is because following only a single leader and using a fixed standard deviation
    could prevent potential solutions from exploring a more diverse search space.
    As a consequence, the solution to this method would probably end in a narrow local
    minimum. An immediate and better strategy would be to generate the offspring by
    combining the ![](img/09f6e608-2765-4ccf-96fa-006c8cfe4bd8.png) top performing
    candidate solutions and weighing them by their fitness rank. Ranking the individuals
    according to their fitness values is called fitness ranking. This strategy is
    preferred to using the actual fitness values as it is invariant to the transformation
    of the objective function and it prevents the new generation from moving too much
    toward a possible outlier.
  prefs: []
  type: TYPE_NORMAL
- en: CMA-ES
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Covariance Matrix Adaptation Evolution Strategy**, or **CMA-ES** for short,
    is an evolutionary strategy algorithm. Unlike the simpler version of the evolution
    strategy, it samples the new candidate solution according to a multivariate normal
    distribution. The name CMA comes from the fact that the dependencies between the
    variables are kept in a covariance matrix that has been adapted to increase or
    decrease the search space on the next generation.
  prefs: []
  type: TYPE_NORMAL
- en: Put simply, CMA-ES shrinks the search space by incrementally decreasing the
    covariance matrix in a given direction when it's confident of the space around
    it. Instead, CMA-ES increases the covariance matrix and thus enlarges the possible
    search space when it's less confident.
  prefs: []
  type: TYPE_NORMAL
- en: ES versus RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ESes are an interesting alternative to RL. Nonetheless, the pros and cons must
    be evaluated so that we can pick the correct approach. Let''s briefly look at
    the main advantages of ES:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Derivative-free methods**: There''s no need for backpropagation. Only the
    forward pass is performed for estimating the fitness function (or equivalently,
    the cumulative reward). This opens the door to all the non-differentiable functions,
    for example; hard attention mechanisms. Moreover, by avoiding backpropagation,
    the code gains efficiency and speed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Very general**: The generality of ES is mainly due to its property of being
    a black-box optimization method. Because we don''t care about the agent, the actions
    that it performs, or the states visited, we can abstract these and concentrate
    only on its evaluation. Furthermore, ES allows learning without explicit targets
    and also with extremely sparse feedback. Additionally, ESes are more general in
    the sense that they can optimize a much larger set of functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Highly parallelizable and robust**: As we''ll soon see, ES is much easier
    to parallelize than RL, and the computations can be spread across thousands of
    workers. The robustness of evolution strategies is due to the few hyperparameters
    that are required to make the algorithms work. For example, in comparison to RL,
    there''s no need to specify the length of the trajectories, the lambda value,
    the discount factor, the number of frames to skip, and so on. Also, the ES is
    very attractive for tasks with a very long horizon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, reinforcement learning is preferred for the following key
    aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sample efficiency**: RL algorithms make better use of the information that''s
    acquired from the environment and as a consequence, they require less data and
    fewer steps to learn the tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Excellent performance**: Overall, reinforcement learning algorithms outperform
    performance evolution strategies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalable evolution strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've introduced black-box evolutionary algorithms and evolution strategies
    in particular, we are ready to put what we have just learned into practice. The
    paper called *Evolution Strategies as a Scalable Alternative to Reinforcement
    Learning* by OpenAI made a major contribution to the adoption of evolution strategies
    as an alternative to reinforcement learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The main contribution of this paper is in the approach that scales ES extremely
    well with a number of CPUs. In particular, the new approach uses a novel communication
    strategy across CPUs that involves only scalars, and so it is able to scale across
    thousands of parallel workers.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, ES requires more experience and thus is less efficient than RL. However,
    by spreading the computation across so many workers (thanks to the adoption of
    this new strategy), the task can be solved in less wall clock time. As an example,
    in the paper, the authors solve the 3D Humanoid Walking pattern in just 10 minutes
    with 1,440 CPUs, with a linear speedup in the number of CPU cores. Because usual
    RL algorithms cannot reach this level of scalability, they take hours to solve
    the same task.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at how they are able to scale so well.
  prefs: []
  type: TYPE_NORMAL
- en: The core
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the paper, a version of ES is used that maximizes the average objective
    value, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba38e6a1-fdc3-4592-afec-231c17fd721c.png)'
  prefs: []
  type: TYPE_IMG
- en: It does this by searching over a population, ![](img/539e6611-0c4e-4075-9e74-35cce86c0178.png),
    that's parameterized by ![](img/3a36c710-4434-455b-8b98-03c3151da93e.png) with
    stochastic gradient ascent. ![](img/051c0918-d493-43e6-a1d4-9de7f947e548.png) is
    the objective function (or fitness function) while ![](img/4d706f8e-4f4f-49a4-975c-6f02832c082f.png) is the
    parameters of the actor. In our problems, ![](img/3bc547ab-803d-49b4-ac20-452762b6b5e6.png) is
    simply the stochastic return that's obtained by the agent with ![](img/0f404f12-ce92-410d-9d06-404c260e2f6a.png) in
    the environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The population distribution, ![](img/add68569-90f7-4ffd-bf1c-f2310e74ac9f.png),is
    a multivariate Gaussian with a mean, ![](img/c7584f6b-9665-483c-9396-b5f63d319342.png), and
    fixed standard deviation, ![](img/9585cbc6-5654-4b6b-bb0c-14c85f7ae8c2.png), as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ac4b3a4-7bcc-4497-be9a-739357c40167.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From here, we can define the step update by using the stochastic gradient estimate,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4bc3a83-dad8-43d6-bc80-3e5846b68fd5.png)'
  prefs: []
  type: TYPE_IMG
- en: With this update, we can estimate the stochastic gradient (without performing
    backpropagation) using the results of the episodes from the population. We can
    update the parameters using one of the well-known update methods, such as Adam
    or RMSProp as well.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing ES
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It''s easy to see how ES can be scaled across multiple CPUs: each worker is
    assigned to a separate candidate solution of the population. The evaluation can
    be done in complete autonomy, and as described in the paper, optimization can
    be done in parallel on each worker, with only a few scalars shared between each
    CPU unit.'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the only information that's shared between workers is the scalar
    return, ![](img/42692633-6bcf-401b-bded-d2cb8a1dabb8.png), of an episode and the
    random seed that has been used to sample ![](img/f805a18c-a321-40fd-926a-c72d7f12f46e.png).
    The amount of data can be further shrunk by sending only the return, but in this
    case, the random seed of each worker has to be synchronized with all the others.
    We decided to adopt the first technique, while the paper used the second one.
    In our simple implementation, the difference is negligible and both techniques
    require extremely low bandwidth.
  prefs: []
  type: TYPE_NORMAL
- en: Other tricks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Two more techniques are used to improve the performance of the algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fitness shaping** – **objective ranking**: We discussed this technique previously.
    It''s very simple. Instead of using the raw returns to compute the update, a rank
    transformation is used. The rank is invariant to the transformation of the objective
    function and thus performs better with spread returns. Additionally, it removes
    the noise of the outliers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mirror noise**: This trick reduces the variance and involves the evaluation
    of the network with both noise ![](img/c238b7ca-715c-4fe0-add9-7c75cc76cbd5.png) and ![](img/5fd782b2-9cad-48cc-959d-0ccafd86ee28.png); that
    is, for each individual, we''ll have two mutations: ![](img/8374e454-8f2f-42b4-8157-0055e8b86858.png) and ![](img/9acbcfa7-181f-4d62-82a0-3399abb61d07.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pseudocode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The parallelized evolution strategy that combines all of these features is
    summarized in the following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now, all that remains is to implement this algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Scalable implementation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To simplify the implementation and to make the parallelized version of ES work
    well with a limited number of workers (and CPUs), we will develop a structure
    similar to the one that's shown in the following diagram. The main process creates
    one worker for each CPU core and executes the main cycle. On each iteration, it
    waits until a given number of new candidates are evaluated by the workers. Different
    from the implementation provided in the paper, each worker evaluates more than
    one agent on each iteration. So, if we have four CPUs, four workers will be created.
    Then, if we want a total batch size bigger than the number of workers on each
    iteration of the main process, let's say, 40, each worker will create and evaluate
    10 individuals each time. The return values and seeds are returned to the main
    application, which waits for results from all 40 individuals, before continuing
    with the following lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, these results are propagated in a batch to all the workers, which optimize
    the neural network seperately, following the update provided in the formula (11.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fde043f4-e707-4c42-98ec-7bb6cc7d4855.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4\. Diagram showing the main components involved in the parallel version
    of ES
  prefs: []
  type: TYPE_NORMAL
- en: 'Following what we just described, the code is divided into three main buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: The main process that creates and manages the queues and the workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A function that defines the task of the workers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, there are some functions that perform simple tasks, such as ranking
    the returns and evaluating the agent.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's explain the code of the main process so that you have a broad view of
    the algorithm before going into detail about the workers.
  prefs: []
  type: TYPE_NORMAL
- en: The main function
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This is defined in a function called `ES` that has the following arguments:
    the name of the Gym environment, the size of the neural network''s hidden layers,
    the total number of generations, the number of workers, the Adam learning rate,
    the batch size, and the standard deviation noise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we set an initial seed that is shared among the workers to initialize
    the parameters with the same weights. Moreover, we calculate the number of individuals
    that a worker has to generate and evaluate on each iteration and create two `multiprocessing.Queue`
    queues. These queues are the entry and exit points for the variables that are
    passed to and from the workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, the multiprocessing processes, `multiprocessing.Process`, are instantiated.
    These will run the `worker` function, which is given as the first argument to
    the `Process` constructor in an asynchronous way. All the other variables that
    are passed to the `worker` function are assigned to `args` and are pretty much
    the same as the parameters taken by ES, with the addition of the two queues. The
    processes start running when the `start()` method is called:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the parallel workers have started, we can iterate across the generations
    and wait until all the individuals have been generated and evaluated separately
    in each worker. Remember that the total number of individuals that are created
    on every generation is the number of workers, `num_workers`, multiplied by the
    individuals generated on each worker, `indiv_per_worker`. This architecture is
    unique to our implementation as we have only four CPU cores available, compared
    to the implementation in the paper, which benefits from thousands of CPUs. Generally,
    the population that''s created on every generation is usually between 20 and 1,000:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the previous snippet, `output_queue.get()` gets an element from `output_queue`,
    which is populated by the workers. In our implementation, `output_queue.get()`
    returns two elements. The first element, `p_rews`, is the fitness value (the return
    value) of the agent that's generated using `p_seed`, which is given as the second
    element.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the `for` cycle terminates, we rank the returns and put the batch returns
    and seeds on the `params_queue` queue, which will be read by all the workers to
    optimize the agent. The code for this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, when all the training iterations have been executed, we can terminate
    the workers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the main function. Now, all we need to do is implement the workers.
  prefs: []
  type: TYPE_NORMAL
- en: Workers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The workers' functionalities are defined in the `worker` function, which was
    previously passed as an argument to `mp.Process`. We cannot go through all the
    code because it'd take too much time and space to explain, but we'll explain the
    core components here. As always, the full implementation is available in this
    book's repository on GitHub. So, if you are interested in looking at it in more
    depth, take the time to examine the code on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: In the first few lines of `worker`, the computational graph is created to run
    the policy and optimize it. Specifically, the policy is a multi-layer perceptron
    with `tanh` nonlinearities as the activation function. In this case, Adam is used
    to apply the expected gradient that's computed following the second term of (11.2).
  prefs: []
  type: TYPE_NORMAL
- en: Then, `agent_op(o)` and `evaluation_on_noise(noise)` are defined. The former
    runs the policy (or candidate solution) to obtain the action for a given state
    or observation, `o`, and the latter evaluates the new candidate solution that
    is obtained by adding the perturbation `noise` (that has the same shape as the
    policy) to the current policy's parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Jumping directly to the most interesting part, we create a new session by specifying
    that it can rely on, at most, 4 CPUs and initialize the global variables. Don''t
    worry if you don''t have 4 CPUs available. Setting `allow_soft_placement` to `True`
    tells TensorFlow to use only the supported devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Despite using all 4 CPUs, we allocate only one to each worker. In the definition
    of the computational graph, we set the device on which the computation will be
    performed. For example, to specify that the worker has to use only CPU 0, you
    can put the graph inside a `with` statement, which defines the device to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Going back to our implementation, we can loop forever, or at least until the
    worker has something to do. This condition is checked later, inside the `while`
    cycle.
  prefs: []
  type: TYPE_NORMAL
- en: An important thing to note is that because we perform many calculations on the
    weights of the neural network, it is much easier to deal with flattened weights.
    So, for example, instead of dealing with a list of the form [8,32,32,4], we'll
    perform computations on a one-dimensional array of length 8*32*32*4\. The functions
    that perform the conversion from the former to the latter, and vice versa, are
    defined in TensorFlow (take a look at the full implementation on GitHub if you
    are interested in knowing how this is done).
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, before starting the `while` loop, we retrieve the shape of the flattened agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In the first part of the `while` loop, the candidates are generated and evaluated. The
    candidate solutions are built by adding a normal perturbation to the weights;
    that is, ![](img/35368897-b2dd-4f34-9776-f797818d1a56.png). This is done by choosing
    a new random seed every time, which will uniquely sample the perturbation (or
    noise), ![](img/acc67400-71b8-4b04-8fef-43469154d93a.png), from a normal distribution.
    This is a key part of the algorithm because, later, the other workers will have
    to regenerate the same perturbation from the same seed. After that, the two new
    offspring (there are two because we are using mirror sampling) are evaluated and
    the results are put in the `output_queue` queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the following snippet (which we used previously), is just a way to
    set the NumPy random seed, `seed`, locally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Outside the `with` statement, the seed that's used to generate random values
    will not be `seed` anymore.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second part of the `while` loop involves the acquisition of all the returns
    and seeds, the reconstruction of the perturbations from those seeds, the computation
    of the stochastic gradient estimate following the formula (11.2), and the policy''s
    optimization. The `params_queue` queue is populated by the main process, which
    we saw earlier. It does this by sending the normalized ranks and seeds of the
    population that were generated by the workers in the first phase. The code is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The last few lines in the preceding code compute the gradient estimate; that
    is, they calculate the second term of formula (11.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/abe6a754-4493-48b2-a6a4-789975a1a0c0.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/6fed24d8-5e14-41bd-809e-e74e3ba4eee5.png) is the normalized rank
    of ![](img/5307ef79-2210-4c5d-96a4-4cbc4d08cb52.png) and ![](img/ae2228ad-b5e8-4148-8a27-d20fd14fd478.png) candidates
    their perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: '`apply_g` is the operation that applies the `vars_grads` gradient (11.3) using
    Adam. Note that we pass `-var_grads` as we want to perform gradient ascent and
    not gradient descent.'
  prefs: []
  type: TYPE_NORMAL
- en: That's all for the implementation. Now, we have to apply it to an environment
    and test it to see how it performs.
  prefs: []
  type: TYPE_NORMAL
- en: Applying scalable ES to LunarLander
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How well will the scalable version of evolution strategies perform in the LunarLander
    environment? Let's find out!
  prefs: []
  type: TYPE_NORMAL
- en: As you may recall, we already used LunarLander against A2C and REINFORCE in
    [Chapter 6](6748bde9-7cd9-43a0-a477-b8a867b1e424.xhtml), *Learning Stochastic
    and PG optimization*. This task consists of landing a lander on the moon through
    continuous actions. We decided to use this environment for its medium difficulty
    and to compare the ES results to those that were obtained with A2C.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hyperparameters that performed the best in this environment are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Hyperparameter** | **Variable name** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| Neural network size | `hidden_sizes` | [32, 32] |'
  prefs: []
  type: TYPE_TB
- en: '| Training iterations (or generations) | `number_iter` | 200 |'
  prefs: []
  type: TYPE_TB
- en: '| Worker''s number | `num_workers` | 4 |'
  prefs: []
  type: TYPE_TB
- en: '| Adam learning rate | `lr` | 0.02 |'
  prefs: []
  type: TYPE_TB
- en: '| Individuals per worker | `indiv_per_worker` | 12 |'
  prefs: []
  type: TYPE_TB
- en: '| Standard deviation | `std_noise` | 0.05 |'
  prefs: []
  type: TYPE_TB
- en: The results are shown in the following graph. What immediately catches your
    eye is that the curve is very stable and smooth. Furthermore, notice that it reaches
    an average score of about 200 after 2.5-3 million steps. Comparing the results
    with those obtained with A2C (in Figure 6.7), you can see that the evolution strategy
    took almost 2-3 times more steps than A2C and REINFORCE.
  prefs: []
  type: TYPE_NORMAL
- en: 'As demonstrated in the paper, by using massive parallelization (using at least
    hundreds of CPUs), you should be able to obtain very good policies in just minutes.
    Unfortunately, we don''t have such computational power. However, if you do, you
    may want to try it for yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ee4c3a1-b67b-4d59-918d-73e3d587e9f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 The performance of scalable evolution strategies
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the results are great and show that ES is a viable solution for very
    long horizon problems and tasks with very sparse rewards.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about EAs, a new class of black-box algorithms
    inspired by biological evolution that can be applied to RL tasks. EAs solve these
    problems from a different perspective compared to reinforcement learning. You
    saw that many characteristics that we have to deal with when we design RL algorithms
    are not valid in evolutionary methods. The differences are in both the intrinsic
    optimization method and the underlying assumptions. For example, because EAs are
    black-box algorithms, we can optimize whatever function we want as we are no longer
    constrained to using differentiable functions, like we were with RL. EAs have
    many other advantages, as we saw throughout this chapter, but they also have numerous
    downsides.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we looked at two evolutionary algorithms: genetic algorithms and evolution
    strategies. Genetic algorithms are more complex as they create offspring from
    two parents through crossover and mutation. Evolution strategies select the best-performing
    individuals from a population that has been created only by mutation from the
    previous generation. The simplicity of ES is one of the key elements that enables
    the immense scalability of the algorithm across thousands of parallel workers.
    This scalability has been demonstrated in the paper by OpenAI, showing the ability
    of ES to perform at the levels of RL algorithms in complex environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get hands-on with evolutionary algorithms, we implemented the scalable evolution
    strategy from the paper we cited throughout this chapter. Furthermore, we tested
    it on LunarLander and saw that ES is able to solve the environment with high performance.
    Though the results are great, ES used two to three times more steps than AC and
    REINFORCE to learn the task. This is the main drawback of ESes: they need a lot
    of experience. Despite this, thanks to their capacity to scale linearly to the
    number of workers, with enough computational power, you might be able to solve
    this task in a fraction of the time compared to reinforcement learning algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll go back to reinforcement learning and talk about
    a problem known as the exploration-exploitation dilemma. We'll see what it is
    and why it's crucial in online settings. Then, we'll use a potential solution
    to the problem to develop a meta-algorithm called ESBAS, which chooses the most
    appropriate algorithm for each situation.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are two alternative algorithms to reinforcement learning for solving sequential
    decision-making problems?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the processes that give birth to new individuals in evolutionary algorithms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the source of inspiration for evolutionary algorithms such as genetic
    algorithms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does CMA-ES evolve evolution strategies?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's one advantage and one disadvantage of evolution strategies?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the trick that's used in the *Evolution Strategies as a Scalable Alternative
    to Reinforcement Learning* paper to reduce the variance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To read the original paper of OpenAI that proposed the scalable version of ES,
    that is, the *Evolution Strategies as a Scalable Alternative to Reinforcement
    Learning* paper, go to [https://arxiv.org/pdf/1703.03864.pdf](https://arxiv.org/pdf/1703.03864.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To read the paper that presented NEAT, that is, *Evolving Neural Networks through*
    *Augmenting Topologies*, go to [http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf](http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
