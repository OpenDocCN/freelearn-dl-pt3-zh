["```\np_logits = mlp(obs_ph, hidden_sizes, act_dim, activation=tf.nn.relu, last_activation=None)\n```", "```\np_means = mlp(obs_ph, hidden_sizes, act_dim, activation=tf.tanh, last_activation=None)\nlog_std = tf.get_variable(name='log_std', initializer=np.zeros(act_dim, dtype=np.float32))\n```", "```\np_means = mlp(obs_ph, hidden_sizes, act_dim, activation=tf.tanh, last_activation=tf.tanh)\n```", "```\np_noisy = p_means + tf.random_normal(tf.shape(p_means), 0, 1) * tf.exp(log_std)\n```", "```\nact_smp = tf.clip_by_value(p_noisy, envs.action_space.low, envs.action_space.high)\n```", "```\np_log = gaussian_log_likelihood(act_ph, p_means, log_std)\n\n```", "```\ndef gaussian_log_likelihood(x, mean, log_std):\n    log_p = -0.5 * (np.log(2*np.pi) + (x-mean)**2 / (tf.exp(log_std)**2 + 1e-9) + 2*log_std)\n    return tf.reduce_sum(log_p, axis=-1)\n```", "```\nInitialize  with random weight\nInitialize environment \nfor episode 1..M do\n    Initialize empty buffer\n\n    *> Generate few trajectories*\n    for step 1..TimeHorizon do\n        *> Collect experience by acting on the environment*\n\n        if :\n\n            *> Store the episode in the buffer*\n             # where  is the length of the episode\n\n    Compute the advantage values  and n-step reward to go \n\n    > Estimate the gradient of the objective function\n         (1)\n    > Compute  using conjugate gradient\n         (2)\n    > Compute the step length \n         (3)\n\n    *> Update the policy using all the experience in ![](img/b45dbbe4-23d2-4584-8004-62dee06b5a64.png)* \n    Backtracking line search to find the maximum  value that satisfy the constraint\n\n     (4)\n\n    *> Critic update using all the experience in ![](img/45cd7c04-0a62-48ca-89a3-f1e75effe333.png)*\n\n```", "```\nact_ph = tf.placeholder(shape=(None,act_dim), dtype=tf.float32, name='act')\nobs_ph = tf.placeholder(shape=(None, obs_dim[0]), dtype=tf.float32, name='obs')\nret_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='ret')\nadv_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='adv')\nold_p_log_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='old_p_log')\nold_mu_ph = tf.placeholder(shape=(None, act_dim), dtype=tf.float32, name='old_mu')\nold_log_std_ph = tf.placeholder(shape=(act_dim), dtype=tf.float32, name='old_log_std')\np_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='p_ph')\n# result of the conjugate gradient algorithm\ncg_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='cg')\n\n# Actor neural network\nwith tf.variable_scope('actor_nn'):\n    p_means = mlp(obs_ph, hidden_sizes, act_dim, tf.tanh, last_activation=tf.tanh)\n    log_std = tf.get_variable(name='log_std', initializer=np.ones(act_dim, dtype=np.float32))\n\n# Critic neural network\nwith tf.variable_scope('critic_nn'):\n    s_values = mlp(obs_ph, hidden_sizes, 1, tf.nn.relu, last_activation=None)\n    s_values = tf.squeeze(s_values) \n```", "```\np_noisy = p_means + tf.random_normal(tf.shape(p_means), 0, 1) * tf.exp(log_std)\n\na_sampl = tf.clip_by_value(p_noisy, low_action_space, high_action_space)\n\np_log = gaussian_log_likelihood(act_ph, p_means, log_std)\n```", "```\n# TRPO loss function\nratio_new_old = tf.exp(p_log - old_p_log_ph)\np_loss = - tf.reduce_mean(ratio_new_old * adv_ph)\n\n# MSE loss function\nv_loss = tf.reduce_mean((ret_ph - s_values)**2)\n\n# Critic optimization\nv_opt = tf.train.AdamOptimizer(cr_lr).minimize(v_loss)\n```", "```\ndef variables_in_scope(scope):    \n    return tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n\n# Gather and flatten the actor parameters\np_variables = variables_in_scope('actor_nn')\np_var_flatten = flatten_list(p_variables)\n\n# Gradient of the policy loss with respect to the actor parameters\np_grads = tf.gradients(p_loss, p_variables)\np_grads_flatten = flatten_list(p_grads)\n```", "```\np_old_variables = tf.placeholder(shape=(None,), dtype=tf.float32, name='p_old_variables')\n\n# variable used as index for restoring the actor's parameters\nit_v1 = tf.Variable(0, trainable=False)\nrestore_params = []\n\nfor p_v in p_variables:\n    upd_rsh = tf.reshape(p_old_variables[it_v1 : it_v1+tf.reduce_prod(p_v.shape)], shape=p_v.shape)\n    restore_params.append(p_v.assign(upd_rsh))\n    it_v1 += tf.reduce_prod(p_v.shape)\n\nrestore_params = tf.group(*restore_params)\n\n```", "```\n# gaussian KL divergence of the two policies \ndkl_diverg = gaussian_DKL(old_mu_ph, old_log_std_ph, p_means, log_std)\n\n# Jacobian of the KL divergence (Needed for the Fisher matrix-vector product)\ndkl_diverg_grad = tf.gradients(dkl_diverg, p_variables)\ndkl_matrix_product = tf.reduce_sum(flatten_list(dkl_diverg_grad) * p_ph)\n\n# Fisher vector product\nFx = flatten_list(tf.gradients(dkl_matrix_product, p_variables))\n```", "```\n# NPG update\nbeta_ph = tf.placeholder(shape=(), dtype=tf.float32, name='beta')\nnpg_update = beta_ph * cg_ph\nalpha = tf.Variable(1., trainable=False)\n\n# TRPO update\ntrpo_update = alpha * npg_update\n\n# Apply the updates to the policy\nit_v = tf.Variable(0, trainable=False)\np_opt = []\nfor p_v in p_variables:\n    upd_rsh = tf.reshape(trpo_update[it_v : it_v+tf.reduce_prod(p_v.shape)], shape=p_v.shape)\n    p_opt.append(p_v.assign_sub(upd_rsh))\n    it_v += tf.reduce_prod(p_v.shape)\n\np_opt = tf.group(*p_opt)\n\n```", "```\n    ...    \n    old_p_log, old_p_means, old_log_std = sess.run([p_log, p_means, log_std], feed_dict={obs_ph:obs_batch, act_ph:act_batch, adv_ph:adv_batch, ret_ph:rtg_batch})\n    old_actor_params = sess.run(p_var_flatten)\n    old_p_loss = sess.run([p_loss], feed_dict={obs_ph:obs_batch, act_ph:act_batch, adv_ph:adv_batch, ret_ph:rtg_batch, old_p_log_ph:old_p_log})\n```", "```\n     def H_f(p):\n        return sess.run(Fx, feed_dict={old_mu_ph:old_p_means, old_log_std_ph:old_log_std, p_ph:p, obs_ph:obs_batch, act_ph:act_batch, adv_ph:adv_batch, ret_ph:rtg_batch})\n\n    g_f = sess.run(p_grads_flatten, feed_dict={old_mu_ph:old_p_means,obs_ph:obs_batch, act_ph:act_batch, adv_ph:adv_batch, ret_ph:rtg_batch, old_p_log_ph:old_p_log})\n    conj_grad = conjugate_gradient(H_f, g_f, iters=conj_iters)\n```", "```\n    beta_np = np.sqrt(2*delta / np.sum(conj_grad * H_f(conj_grad)))\n\n    def DKL(alpha_v):\n        sess.run(p_opt, feed_dict={beta_ph:beta_np, alpha:alpha_v, cg_ph:conj_grad, obs_ph:obs_batch, act_ph:act_batch, adv_ph:adv_batch, old_p_log_ph:old_p_log})\n        a_res = sess.run([dkl_diverg, p_loss], feed_dict={old_mu_ph:old_p_means, old_log_std_ph:old_log_std, obs_ph:obs_batch, act_ph:act_batch, adv_ph:adv_batch, ret_ph:rtg_batch, old_p_log_ph:old_p_log})\n        sess.run(restore_params, feed_dict={p_old_variables: old_actor_params})\n        return a_res\n\n    best_alpha = backtracking_line_search(DKL, delta, old_p_loss, p=0.8)\n    sess.run(p_opt, feed_dict={beta_ph:beta_np, alpha:best_alpha, cg_ph:conj_grad, obs_ph:obs_batch, act_ph:act_batch, adv_ph:adv_batch, old_p_log_ph:old_p_log})\n\n    ...\n\n```", "```\ndef GAE(rews, v, v_last, gamma=0.99, lam=0.95):\n    vs = np.append(v, v_last)\n    delta = np.array(rews) + gamma*vs[1:] - vs[:-1]\n    gae_advantage = discounted_rewards(delta, 0, gamma*lam)\n    return gae_advantage\n```", "```\nclass Buffer():\n    def __init__(self, gamma, lam):\n        ...\n\n    def store(self, temp_traj, last_sv):\n        if len(temp_traj) > 0:\n            self.ob.extend(temp_traj[:,0])\n            rtg = discounted_rewards(temp_traj[:,1], last_sv, self.gamma)\n            self.adv.extend(GAE(temp_traj[:,1], temp_traj[:,3], last_sv, self.gamma, self.lam))\n            self.rtg.extend(rtg)\n            self.ac.extend(temp_traj[:,2])\n\n    def get_batch(self):\n        return np.array(self.ob), np.array(self.ac), np.array(self.adv), np.array(self.rtg)\n\n    def __len__(self):\n        ...\n```", "```\ndef clipped_surrogate_obj(new_p, old_p, adv, eps):\n    rt = tf.exp(new_p - old_p) # i.e. pi / old_pi\n    return -tf.reduce_mean(tf.minimum(rt*adv, tf.clip_by_value(rt, 1-eps, 1+eps)*adv))\n```", "```\n# Placeholders\nact_ph = tf.placeholder(shape=(None,act_dim), dtype=tf.float32, name='act')\nobs_ph = tf.placeholder(shape=(None, obs_dim[0]), dtype=tf.float32, name='obs')\nret_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='ret')\nadv_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='adv')\nold_p_log_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='old_p_log')\n\n# Actor\nwith tf.variable_scope('actor_nn'):\n    p_means = mlp(obs_ph, hidden_sizes, act_dim, tf.tanh, last_activation=tf.tanh)\n    log_std = tf.get_variable(name='log_std', initializer=np.ones(act_dim, dtype=np.float32))\n    p_noisy = p_means + tf.random_normal(tf.shape(p_means), 0, 1) * tf.exp(log_std)\n    act_smp = tf.clip_by_value(p_noisy, low_action_space, high_action_space)\n    # Compute the gaussian log likelihood\n    p_log = gaussian_log_likelihood(act_ph, p_means, log_std)\n\n# Critic \nwith tf.variable_scope('critic_nn'):\n    s_values = tf.squeeze(mlp(obs_ph, hidden_sizes, 1, tf.tanh, last_activation=None))\n\n# PPO loss function\np_loss = clipped_surrogate_obj(p_log, old_p_log_ph, adv_ph, eps)\n# MSE loss function\nv_loss = tf.reduce_mean((ret_ph - s_values)**2)\n\n# Optimizers\np_opt = tf.train.AdamOptimizer(ac_lr).minimize(p_loss)\nv_opt = tf.train.AdamOptimizer(cr_lr).minimize(v_loss)\n```", "```\n        ...    \n        obs_batch, act_batch, adv_batch, rtg_batch = buffer.get_batch()     \n        old_p_log = sess.run(p_log, feed_dict={obs_ph:obs_batch, act_ph:act_batch, adv_ph:adv_batch, ret_ph:rtg_batch})\n        old_p_batch = np.array(old_p_log)\nlb = len(buffer)\n        lb = len(buffer)\n        shuffled_batch = np.arange(lb) \n\n        # Policy optimization steps\n        for _ in range(actor_iter):\n            # shuffle the batch on every iteration\n            np.random.shuffle(shuffled_batch)\n\n            for idx in range(0,lb, minibatch_size):\n                minib = shuffled_batch[idx:min(idx+batch_size,lb)]\n                sess.run(p_opt, feed_dict={obs_ph:obs_batch[minib], act_ph:act_batch[minib], adv_ph:adv_batch[minib], old_p_log_ph:old_p_batch[minib]})\n\n        # Value function optimization steps\n        for _ in range(critic_iter):\n            # shuffle the batch on every iteration\n            np.random.shuffle(shuffled_batch)\n\n            for idx in range(0,lb, minibatch_size):\n                minib = shuffled_batch[idx:min(idx+minibatch_size,lb)]\n                sess.run(v_opt, feed_dict={obs_ph:obs_batch[minib], ret_ph:rtg_batch[minib]})\n        ...\n\n```"]