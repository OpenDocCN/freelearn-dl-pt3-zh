<html><head></head><body>
		<div id="_idContainer060">
			<h1 id="_idParaDest-44"><em class="italic"><a id="_idTextAnchor044"/>Chapter 2</em>: Implementing Value-Based, Policy-Based, and Actor-Critic Deep RL Algorithms</h1>
			<p>This chapter provides a practical approach to building value-based, policy-based, and actor-critic algorithm-based <strong class="bold">reinforcement learning</strong> (<strong class="bold">RL</strong>) agents. It includes recipes for implementing value iteration-based learning agents and breaks down the implementation details of several foundational algorithms in RL into simple steps. The policy gradient-based agent and the actor-critic agent make use of the latest major version of <strong class="bold">TensorFlow 2.x</strong> to define the neural network policies.</p>
			<p>The following recipes will be covered in this chapter:</p>
			<ul>
				<li>Building stochastic environments for training RL agents</li>
				<li>Building value-based (RL) agent algorithms</li>
				<li>Implementing temporal difference learning</li>
				<li>Building Monte Carlo prediction and control algorithms for RL</li>
				<li>Implementing the SARSA algorithm and an RL agent</li>
				<li>Building a Q-learning agent</li>
				<li>Implementing policy gradients</li>
				<li>Implementing actor-critic algorithms</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor045"/>Technical requirements</h1>
			<p>The code in this book has been tested extensively on Ubuntu 18.04 and Ubuntu 20.04, and should work with later versions of Ubuntu if Python 3.6+ is available. With Python 3.6 installed, along with the necessary Python packages listed at the beginning of each recipe, the code should run fine on Windows and Mac OS X too. It is advised that you create and use a Python virtual environment named <strong class="source-inline">tf2rl-cookbook</strong> to install the packages and run the code in this book. Installing Miniconda or Anaconda for Python virtual environment management is recommended.</p>
			<p>The complete code for each recipe in each chapter is available here: <a href="https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook">https://github.com/PacktPublishing/Tensorflow-2-Reinforcement-Learning-Cookbook</a>.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/>Building stochastic environments for training RL agents</h1>
			<p>To train RL agents for the real world, we<a id="_idIndexMarker089"/> need learning environments that are stochastic, since real-world problems are stochastic in nature. This<a id="_idIndexMarker090"/> recipe will walk you through the steps for building a <strong class="bold">Maze</strong> learning environment to train RL agents. The <a id="_idIndexMarker091"/>Maze is a simple, stochastic environment where the world is represented as a grid. Each location on the grid can be referred to as a cell. The goal of an agent in this environment is to find its way to the goal state. Consider the maze shown in the following diagram, where the black cells represent walls:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B15074_02_001.jpg" alt="Figure 2.1 – The Maze environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – The Maze environment</p>
			<p>The agent's location is<a id="_idIndexMarker092"/> initialized to be at the top-left cell in the Maze. The agent needs to find its way around the grid to reach the goal<a id="_idIndexMarker093"/> located at the top-right cell in the Maze, collecting a maximum number of coins along the way while avoiding walls. The location of the goal, coins, walls, and the agent's starting location can be modified in the environment's code.</p>
			<p>The four-dimensional discrete actions that are supported in this environment are as follows:</p>
			<ul>
				<li><em class="italic">0</em>: Move up</li>
				<li><em class="italic">1</em>: Move down</li>
				<li><em class="italic">2</em>: Move left</li>
				<li><em class="italic">3</em>: Move right</li>
			</ul>
			<p>The reward is based on the number of coins that are collected by the agent before they reach the goal state. Because the environment is stochastic, the action that's taken by the environment has a slight (0.1) probability of "slipping" wherein the actual action that's executed will be altered<a id="_idIndexMarker094"/> stochastically. The slip action will be the clockwise directional action (LEFT -&gt; UP, UP -&gt; RIGHT, and so on). For example, with <strong class="source-inline">slip_probability=0.2</strong>, there is a 0.2 probability that a RIGHT action may result in DOWN.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Getting ready</h2>
			<p>To complete this recipe, you<a id="_idIndexMarker095"/> will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment and run <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started:</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import numpy as np</p>
			<p>Now, we can begin.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>How to do it…</h2>
			<p>The learning environment is a simulator that provides observations for the RL agent, supports a set of actions that the RL agent can perform by executing the actions, and returns the resultant/new observation as a result of the agent taking the action.</p>
			<p>Follow these steps to implement a stochastic Maze learning environment that represents a simple 2D map with cells representing the location of the agent, their goal, walls, coins, and empty space:</p>
			<ol>
				<li>We'll start by defining the MazeEnv class and a map of the Maze environment:<p class="source-code">class MazeEnv(gym.Env):</p><p class="source-code">    def __init__(self, stochastic=True):</p><p class="source-code">        """Stochastic Maze environment with coins,\</p><p class="source-code">           obstacles/walls and a goal state.  </p><p class="source-code">        """</p><p class="source-code">        self.map = np.asarray(["SWFWG", "OOOOO", "WOOOW",</p><p class="source-code">                               "FOWFW"])</p></li>
				<li>Next, place the obstacles/walls <a id="_idIndexMarker096"/>on the environment map in the<a id="_idIndexMarker097"/> appropriate places:<p class="source-code">        self.dim = (4, 5)</p><p class="source-code">        self.img_map = np.ones(self.dim)</p><p class="source-code">        self.obstacles = [(0, 1), (0, 3), (2, 0), </p><p class="source-code">                          (2, 4), (3, 2), (3, 4)]</p><p class="source-code">        for x in self.obstacles:</p><p class="source-code">            self.img_map[x[0]][x[1]] = 0</p></li>
				<li>Let's define the slip mapping action in clockwise order:<p class="source-code">self.slip_action_map = {</p><p class="source-code">            0: 3,</p><p class="source-code">            1: 2,</p><p class="source-code">            2: 0,</p><p class="source-code">            3: 1,</p><p class="source-code">        }</p></li>
				<li>Now, let's define a lookup table in the form of a dictionary to map indices to cells in the Maze environment:<p class="source-code">self.index_to_coordinate_map = {</p><p class="source-code">            0: (0, 0),</p><p class="source-code">            1: (1, 0),</p><p class="source-code">            2: (3, 0),</p><p class="source-code">            3: (1, 1),</p><p class="source-code">            4: (2, 1),</p><p class="source-code">            5: (3, 1),</p><p class="source-code">            6: (0, 2),</p><p class="source-code">            7: (1, 2),</p><p class="source-code">            8: (2, 2),</p><p class="source-code">            9: (1, 3),</p><p class="source-code">            10: (2, 3),</p><p class="source-code">            11: (3, 3),</p><p class="source-code">            12: (0, 4),</p><p class="source-code">            13: (1, 4),</p><p class="source-code">        }</p></li>
				<li>Next, let's define the<a id="_idIndexMarker098"/> reverse lookup to find a cell, when given an index:<p class="source-code">	self.coordinate_to_index_map = dict((val, key) for \</p><p class="source-code">        key, val in self.index_to_coordinate_map.items())</p><p>With that, we have<a id="_idIndexMarker099"/> finished initializing the environment!</p></li>
				<li>Now, let's define a method that will handle the coins and their statuses in the Maze, where 0 means that the coin wasn't collected by the agent and 1 means that the coin was collected by the agent:<p class="source-code">	def num2coin(self, n: int):</p><p class="source-code">        coinlist = [</p><p class="source-code">            (0, 0, 0),</p><p class="source-code">            (1, 0, 0),</p><p class="source-code">            (0, 1, 0),</p><p class="source-code">            (0, 0, 1),</p><p class="source-code">            (1, 1, 0),</p><p class="source-code">            (1, 0, 1),</p><p class="source-code">            (0, 1, 1),</p><p class="source-code">            (1, 1, 1),</p><p class="source-code">        ]</p><p class="source-code">        return list(coinlist[n])</p></li>
				<li>Now, let's define a<a id="_idIndexMarker100"/> quick method that will do the inverse operation of finding the number status/value of a coin:<p class="source-code">	   def coin2num(self, v: List):</p><p class="source-code">        if sum(v) &lt; 2:</p><p class="source-code">            return np.inner(v, [1, 2, 3])</p><p class="source-code">        else:</p><p class="source-code">            return np.inner(v, [1, 2, 3]) + 1</p></li>
				<li>Next, we will define a <a id="_idIndexMarker101"/>setter function to set the state of the environment. This is useful for algorithms such as value iteration, where each and every state needs to be visited in the environment for it to calculate values:<p class="source-code">def set_state(self, state: int) -&gt; None:</p><p class="source-code">        """Set the current state of the environment. </p><p class="source-code">           Useful for value iteration</p><p class="source-code">        Args:</p><p class="source-code">            state (int): A valid state in the Maze env \</p><p class="source-code">            int: [0, 112]</p><p class="source-code">        """</p><p class="source-code">        self.state = state</p></li>
				<li>Now, it is time to<a id="_idIndexMarker102"/> implement the <strong class="source-inline">step</strong> method. We'll begin by implementing the <strong class="source-inline">step</strong> method and applying the <strong class="source-inline">slip</strong> action<a id="_idIndexMarker103"/> based on <strong class="source-inline">slip_probability</strong>:<p class="source-code">def step(self, action, slip=True):</p><p class="source-code">        """Run one step into the Maze env</p><p class="source-code">        Args:</p><p class="source-code">            state (Any): Current index state of the maze</p><p class="source-code">            action (int): Discrete action for up, down,\</p><p class="source-code">            left, right</p><p class="source-code">            slip (bool, optional): Stochasticity in the \</p><p class="source-code">            env. Defaults to True.</p><p class="source-code">        Raises:</p><p class="source-code">            ValueError: If invalid action is provided as </p><p class="source-code">            input</p><p class="source-code">        Returns:</p><p class="source-code">            Tuple : Next state, reward, done, _</p><p class="source-code">        """</p><p class="source-code">        self.slip = slip</p><p class="source-code">        if self.slip:</p><p class="source-code">            if np.random.rand() &lt; self.slip_probability:</p><p class="source-code">                action = self.slip_action_map[action]</p></li>
				<li>Continuing with our implementation<a id="_idIndexMarker104"/> of the <strong class="source-inline">step</strong> function, we'll update the state of the<a id="_idIndexMarker105"/> maze based on the action that's taken:<p class="source-code">cell = self.index_to_coordinate_map[int(self.state / 8)]</p><p class="source-code">        if action == 0:</p><p class="source-code">            c_next = cell[1]</p><p class="source-code">            r_next = max(0, cell[0] - 1)</p><p class="source-code">        elif action == 1:</p><p class="source-code">            c_next = cell[1]</p><p class="source-code">            r_next = min(self.dim[0] - 1, cell[0] + 1)</p><p class="source-code">        elif action == 2:</p><p class="source-code">            c_next = max(0, cell[1] - 1)</p><p class="source-code">            r_next = cell[0]</p><p class="source-code">        elif action == 3:</p><p class="source-code">            c_next = min(self.dim[1] - 1, cell[1] + 1)</p><p class="source-code">            r_next = cell[0]</p><p class="source-code">        else:</p><p class="source-code">            raise ValueError(f"Invalid action:{action}")</p></li>
				<li>Next, we will determine whether the agent has reached the goal:  <p class="source-code">if (r_next == self.goal_pos[0]) and (</p><p class="source-code">            c_next == self.goal_pos[1]</p><p class="source-code">        ):  # Check if goal reached</p><p class="source-code">            v_coin = self.num2coin(self.state % 8)</p><p class="source-code">            self.state = 8 * self.coordinate_to_index_\</p><p class="source-code">                map[(r_next, c_next)] + self.state % 8</p><p class="source-code">            return (</p><p class="source-code">                self.state,</p><p class="source-code">                float(sum(v_coin)),</p><p class="source-code">                True,</p><p class="source-code">            )</p></li>
				<li>Next, we'll handle cases <a id="_idIndexMarker106"/>when the action results in hitting an obstacle/wall:<p class="source-code"> else:</p><p class="source-code">    if (r_next, c_next) in self.obstacles:  # obstacle </p><p class="source-code">    # tuple list</p><p class="source-code">                return self.state, 0.0, False</p></li>
				<li>The last case you need to <a id="_idIndexMarker107"/>handle is seeing whether the action leads to collecting a coin:<p class="source-code">else:  # Coin locations</p><p class="source-code">                v_coin = self.num2coin(self.state % 8)</p><p class="source-code">                if (r_next, c_next) == (0, 2):</p><p class="source-code">                    v_coin[0] = 1</p><p class="source-code">                elif (r_next, c_next) == (3, 0):</p><p class="source-code">                    v_coin[1] = 1</p><p class="source-code">                elif (r_next, c_next) == (3, 3):</p><p class="source-code">                    v_coin[2] = 1</p><p class="source-code">                self.state = 8 * self.coordinate_to_index_map[(r_next, c_next)] + self.coin2num(v_coin)</p><p class="source-code">                return (</p><p class="source-code">                    self.state,</p><p class="source-code">                    0.0,</p><p class="source-code">                    False,</p><p class="source-code">                )</p></li>
				<li>To visualize<a id="_idIndexMarker108"/> the state of the Gridworld in a human-friendly manner, let's implement a render function that will print out<a id="_idIndexMarker109"/> a text version of the current state of the Maze environment:<p class="source-code">def render(self):</p><p class="source-code">        cell = self.index_to_coordinate_map[int(</p><p class="source-code">                                         self.state / 8)]</p><p class="source-code">        desc = self.map.tolist()</p><p class="source-code">        desc[cell[0]] = (</p><p class="source-code">            desc[cell[0]][: cell[1]]</p><p class="source-code">            + "\x1b[1;34m"</p><p class="source-code">            + desc[cell[0]][cell[1]]</p><p class="source-code">            + "\x1b[0m"</p><p class="source-code">            + desc[cell[0]][cell[1] + 1 :]</p><p class="source-code">        )</p><p class="source-code">        print("\n".join("".join(row) for row in desc))</p></li>
				<li> To test whether the environment is working as expected, let's add a <strong class="source-inline">__main__</strong> function that gets<a id="_idIndexMarker110"/> executed if the environment script is run<a id="_idIndexMarker111"/> directly:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = MazeEnv()</p><p class="source-code">    obs = env.reset()</p><p class="source-code">    env.render()</p><p class="source-code">    done = False</p><p class="source-code">    step_num = 1</p><p class="source-code">    action_list = ["UP", "DOWN", "LEFT", "RIGHT"]</p><p class="source-code">    # Run one episode</p><p class="source-code">    while not done:</p><p class="source-code">        # Sample a random action from the action space</p><p class="source-code">        action = env.action_space.sample()</p><p class="source-code">        next_obs, reward, done = env.step(action)</p><p class="source-code">        print(</p><p class="source-code">            f"step#:{step_num} action:\</p><p class="source-code">            {action_list[action]} reward:{reward} \</p><p class="source-code">             done:{done}"</p><p class="source-code">        )</p><p class="source-code">        step_num += 1</p><p class="source-code">        env.render()</p><p class="source-code">    env.close()</p></li>
				<li>With that, we're all set! The Maze environment is ready and we can quickly test it by running the script (<strong class="source-inline">python envs/maze.py</strong>). An output similar to the following will be displayed:</li>
			</ol>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B15074_02_002.jpg" alt="Figure 2.2 – Textual representation of the Maze environment highlighting and underlining the agent's current state "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – Textual representation of the Maze environment highlighting and underlining the agent's current state</p>
			<p>Let's see how it works.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>How it works…</h2>
			<p>Our <strong class="source-inline">map</strong>, as defined in <em class="italic">step 1</em> in the <em class="italic">How to do it…</em> section, represents the state of the learning environment. The Maze environment defines the observation space, the action space, and the rewarding<a id="_idIndexMarker112"/> mechanism for implementing a <strong class="bold">Markov decision process</strong> (<strong class="bold">MDP</strong>). We sampled a valid action from the action space of the environment and stepped the environment <a id="_idIndexMarker113"/>with the chosen action, which resulted in us getting the new observation, reward, and a done status Boolean (representing<a id="_idIndexMarker114"/> whether the episode has finished) as the response from the Maze environment. The <strong class="source-inline">env.render()</strong> method converts the environment's internal grid representation into a simple text/string grid and prints it for ea<a id="_idTextAnchor050"/><a id="_idTextAnchor051"/><a id="_idTextAnchor052"/><a id="_idTextAnchor053"/><a id="_idTextAnchor054"/><a id="_idTextAnchor055"/><a id="_idTextAnchor056"/><a id="_idTextAnchor057"/><a id="_idTextAnchor058"/><a id="_idTextAnchor059"/><a id="_idTextAnchor060"/>sy visual understanding.</p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor061"/>Building value-based reinforcement learning agent algorithms</h1>
			<p>Value-based reinforcement learning <a id="_idIndexMarker115"/>works by learning the state-value function or the action-value function in a given environment. This recipe will show you how to create and update the value function for the Maze environment to obtain an optimal policy. Learning value functions, especially in model-free RL problems where a model of the environment is not available, can prove to be quite effective, especially for RL problems with low-dimensional state space.</p>
			<p>Upon completing this recipe, you will have an algorithm that can generate the following optimal action sequence based on value functions:</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B15074_02_003.jpg" alt="Figure 2.3 – Optimal action sequence generated by a value-based RL algorithm with state values represented through a jet color map "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Optimal action sequence generated by a value-based RL algorithm with state values represented through a jet color map</p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor062"/>Getting ready</h2>
			<p>To complete this recipe, you<a id="_idIndexMarker116"/> will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment and run <strong class="source-inline">pip install numpy gym</strong>. If the following import statement runs without issues, you are ready to get started:</p>
			<p class="source-code">import numpy as np</p>
			<p>Now, we can begin.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor063"/>How to do it…</h2>
			<p>Let's implement a value function learning algorithm based on value iteration. We will use the Maze environment to implement and analyze the value iteration algorithm. </p>
			<p>Follow these steps to implement this recipe:</p>
			<ol>
				<li value="1">Import the Maze learning environment from <strong class="source-inline">envs.maze</strong>:<p class="source-code">from envs.maze import MazeEnv</p></li>
				<li>Create an instance of <strong class="source-inline">MazeEnv</strong> and print the observation space and action space:<p class="source-code">env = MazeEnv()</p><p class="source-code">print(f"Observation space: {env.observation_space}")</p><p class="source-code">print(f"Action space: {env.action_space}")</p></li>
				<li>Let's define the state<a id="_idIndexMarker117"/> dimension in order to initialize <strong class="source-inline">state-values</strong>, <strong class="source-inline">state-action values</strong>, and our policy:<p class="source-code">state_dim = env.distinct_states</p><p class="source-code">state_values = np.zeros(state_dim)</p><p class="source-code">q_values = np.zeros((state_dim, env.action_space.n))</p><p class="source-code">policy = np.zeros(state_dim)</p></li>
				<li>Now, we are ready to implement a function that can calculate the state/action value when given a state in the environment and an action. We will begin by declaring the <strong class="source-inline">calculate_values</strong> function; we'll complete the implementation in the following steps:<p class="source-code">def calculate_values(state, action):</p><p class="source-code">    """Evaluate Value function for given state and action</p><p class="source-code">    Args:</p><p class="source-code">        state (int): Valid (discrete) state in discrete \</p><p class="source-code">        `env.observation_space`</p><p class="source-code">        action (int): Valid (discrete) action in \</p><p class="source-code">        `env.action_space`</p><p class="source-code">    Returns:</p><p class="source-code">        v_sum: value for given state, action</p><p class="source-code">    """</p></li>
				<li>As the next step, we will generate <strong class="source-inline">slip_action</strong>, which is a stochastic action based on the stochasticity <a id="_idIndexMarker118"/>of the learning environment:<p class="source-code">    slip_action = env.slip_action_map[action]</p></li>
				<li>When calculating the values of a given state-action pair, it is important to be able to set the state of the environment before executing an action to observe the reward/result. The Maze environment provides a convenient <strong class="source-inline">set_state</strong> method for setting the current state of the environment. Let's make use of it and step through the environment with the desired (input) action:<p class="source-code">    env.set_state(state)</p><p class="source-code">    slip_next_state, slip_reward, _ = \</p><p class="source-code">                        env.step(slip_action, slip=False)</p></li>
				<li>We need a list of transitions in the environment to be able to calculate the rewards, as per the Bellman equations. Let's create a <strong class="source-inline">transitions</strong> list and append the newly obtained environment transition information:<p class="source-code">    transitions = []    transitions.append((slip_reward, slip_next_state,</p><p class="source-code">                        env.slip))</p></li>
				<li>Let's obtain another transition using the state and the action, this time without stochasticity. We can do this by not using <strong class="source-inline">slip_action</strong> and setting <strong class="source-inline">slip=False</strong> while stepping through the Maze environment:<p class="source-code">    env.set_state(state)</p><p class="source-code">    next_state, reward, _ = env.step(action, slip=False)</p><p class="source-code">    transitions.append((reward, next_state,</p><p class="source-code">                        1 - env.slip))</p></li>
				<li>There is only one more <a id="_idIndexMarker119"/>step needed to complete the <strong class="source-inline">calculate_values</strong> function, which is to calculate the values:<p class="source-code">    for reward, next_state, pi in transitions:</p><p class="source-code">        v_sum += pi * (reward + discount * \</p><p class="source-code">                       state_values[next_state])</p><p class="source-code">    return v_sum</p></li>
				<li>Now, we can start implementing the state/action value learning. We will begin by defining the <strong class="source-inline">max_iteration</strong> hyperparameters:<p class="source-code"># Define the maximum number of iterations per learning </p><p class="source-code"># step</p><p class="source-code">max_iteration = 1000</p></li>
				<li>Let's implement the <strong class="source-inline">state-value</strong> function learning loop using value iteration:<p class="source-code">for i in range(iters):</p><p class="source-code">    v_s = np.zeros(state_dim)</p><p class="source-code">    for state in range(state_dim):</p><p class="source-code">        if env.index_to_coordinate_map[int(state / 8)]==\</p><p class="source-code">        env.goal_pos:</p><p class="source-code">            continue</p><p class="source-code">        v_max = float("-inf")</p><p class="source-code">        for action in range(env.action_space.n):</p><p class="source-code">            v_sum = calculate_values(state, action)</p><p class="source-code">            v_max = max(v_max, v_sum)</p><p class="source-code">        v_s[state] = v_max</p><p class="source-code">    state_values = np.copy(v_s)</p></li>
				<li>Now that we have the <strong class="source-inline">state-value</strong> function learning loop implemented, let's move on and implement the <strong class="source-inline">action-value</strong> function:<p class="source-code">for state in range(state_dim):</p><p class="source-code">    for action in range(env.action_space.n):</p><p class="source-code">        q_values[state, action] = calculate_values(state,</p><p class="source-code">                                                   action)</p></li>
				<li>With the <strong class="source-inline">action-value</strong> function<a id="_idIndexMarker120"/> computed, we are only one step away from obtaining the optimal policy. Let's go get it!<p class="source-code">for state in range(state_dim):</p><p class="source-code">    policy[state] = np.argmax(q_values[state, :])</p></li>
				<li>We can print the Q values (the <strong class="source-inline">state-action</strong> values) and the policy using the following lines of code:<p class="source-code">print(f"Q-values: {q_values}")</p><p class="source-code">print("Action mapping:[0 - UP; 1 - DOWN; 2 - LEFT; \</p><p class="source-code">       3 - RIGHT")</p><p class="source-code">print(f"optimal_policy: {policy}")</p></li>
				<li>As a final step, let's visualize the value function's learning and policy updates:<p class="source-code">from value_function_utils import viusalize_maze_values</p><p class="source-code">viusalize_maze_values(q_values, env)</p><p>The preceding code will generate the following diagrams, which show the progress of the value function while it's learning and the policy updates:</p></li>
			</ol>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B15074_02_004.jpg" alt="Figure 2.4 – Progression (from left to right and from top to bottom) of the  learned value function and the policy "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Progression (from left to right and from top to bottom) of the learned value function and the policy</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor064"/>How it works…</h2>
			<p>The Maze environment<a id="_idIndexMarker121"/> contains a start cell, a goal cell, and a few cells containing coins, walls, and open spaces. There are 112 distinct states in the Maze environment due to the varying nature of the cells with coins. For illustration purposes, when an agent collects one of the coins, the environment is in a completely different state compared to the state when the agent collects a different coin. This is because the location of the coin also matters.</p>
			<p><strong class="source-inline">q_values</strong> (state-action values) is a big matrix of size 112 x 4, so it will print a long list of values. We will not show these here. The other two print statements in <em class="italic">step 14</em> should produce an output similar to the following:</p>
			<div>
				<div id="_idContainer030" class="IMG---Figure">
					<img src="image/B15074_02_005.jpg" alt="Figure 2.5 – Textual representation of the optimal action sequence "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Textual representation of the optimal action sequence</p>
			<p>Value iteration-based value function learning follows Bellman equations, and the optimal policy is obtained from the Q-value function by simply choosing the action with the highest Q/action-value.</p>
			<p>In <em class="italic">Figure 2.4</em>, the value function is represented using a jet color map, while the policy is represented using the green-arrows. Initially, the values for the states are almost even. As the learning progress, states <a id="_idIndexMarker122"/>with coins get more value than states without coins, and the state that leads to the goal gets a very high value that's only slightly less than the goal state itself. The black cells in the maze represents the walls. The arrows represent the directional action that the policy is prescribing from the given cell in the maze. As the learning converges, as shown in the bottom-right diagram, the policy is optimal, leading the agent to the goal after it's collected every coin.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The color versions of the diagrams in this book are available to download. You can find the link to these diagrams in the <em class="italic">Preface</em> of this book.</p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor065"/>Implementing temporal difference learning</h1>
			<p>This recipe will walk you<a id="_idIndexMarker123"/> through how to implement the <strong class="bold">temporal difference</strong> (<strong class="bold">TD</strong>) learning algorithm. TD algorithms allow us to incrementally learn from incomplete episodes of agent experiences, which means they can be used for problems that require online learning capabilities. TD algorithms are useful in model-free RL settings as they do not depend on a model of the MDP transitions or rewards. To visually understand the learning progression of the TD algorithm, this recipe will also show you how to implement the GridworldV2 learning environment, which looks as follows when rendered:</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B15074_02_006.jpg" alt="Figure 2.6 – The GridworldV2 learning environment 2D rendering with  state values and grid cell coordinates "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – The GridworldV2 learning environment 2D rendering with state values and grid cell coordinates</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor066"/>Getting ready</h2>
			<p>To complete this recipe, you <a id="_idIndexMarker124"/>will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment and run <strong class="source-inline">pip install numpy gym</strong>. If the following import statements run without issues, you are ready to get started:</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">import numpy as np</p>
			<p>Now, we can begin.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor067"/>How to do it…</h2>
			<p>This recipe will contain two components that we will put together at the end. The first component is the GridworldV2<a id="_idIndexMarker125"/> implementation, while the second component is the TD learning algorithm's implementation. Let's get started:</p>
			<ol>
				<li value="1">We will start by implementing GridworldV2 and then by defining the <strong class="source-inline">GridworldV2Eng</strong> class:<p class="source-code">class GridworldV2Env(gym.Env):</p><p class="source-code">    def __init__(self, step_cost=-0.2, max_ep_length=500,</p><p class="source-code">    explore_start=False):</p><p class="source-code">        self.index_to_coordinate_map = {</p><p class="source-code">            "0": [0, 0],</p><p class="source-code">            "1": [0, 1],</p><p class="source-code">            "2": [0, 2],</p><p class="source-code">            "3": [0, 3],</p><p class="source-code">            "4": [1, 0],</p><p class="source-code">            "5": [1, 1],</p><p class="source-code">            "6": [1, 2],</p><p class="source-code">            "7": [1, 3],</p><p class="source-code">            "8": [2, 0],</p><p class="source-code">            "9": [2, 1],</p><p class="source-code">            "10": [2, 2],</p><p class="source-code">            "11": [2, 3],</p><p class="source-code">        }</p><p class="source-code">        self.coordinate_to_index_map = {</p><p class="source-code">            str(val): int(key) for key, val in self.index_to_coordinate_map.items()</p><p class="source-code">        }</p></li>
				<li>In this step, you will continue implementing the <strong class="source-inline">__init__</strong> method and define the necessary values<a id="_idIndexMarker126"/> that define the size of the Gridworld, the goal location, the wall location, and the location of the bomb, among other things:<p class="source-code">self.map = np.zeros((3, 4))</p><p class="source-code">        self.observation_space = gym.spaces.Discrete(1)</p><p class="source-code">        self.distinct_states = [str(i) for i in \</p><p class="source-code">                                 range(12)]</p><p class="source-code">        self.goal_coordinate = [0, 3]</p><p class="source-code">        self.bomb_coordinate = [1, 3]</p><p class="source-code">        self.wall_coordinate = [1, 1]</p><p class="source-code">        self.goal_state = self.coordinate_to_index_map[</p><p class="source-code">                          str(self.goal_coordinate)]  # 3</p><p class="source-code">        self.bomb_state = self.coordinate_to_index_map[</p><p class="source-code">                          str(self.bomb_coordinate)]  # 7</p><p class="source-code">        self.map[self.goal_coordinate[0]]\</p><p class="source-code">                [self.goal_coordinate[1]] = 1</p><p class="source-code">        self.map[self.bomb_coordinate[0]]\</p><p class="source-code">                [self.bomb_coordinate[1]] = -1</p><p class="source-code">        self.map[self.wall_coordinate[0]]\</p><p class="source-code">                [self.wall_coordinate[1]] = 2</p><p class="source-code">        self.exploring_starts = explore_start</p><p class="source-code">        self.state = 8</p><p class="source-code">        self.done = False</p><p class="source-code">        self.max_ep_length = max_ep_length</p><p class="source-code">        self.steps = 0</p><p class="source-code">        self.step_cost = step_cost</p><p class="source-code">        self.action_space = gym.spaces.Discrete(4)</p><p class="source-code">        self.action_map = {"UP": 0, "RIGHT": 1, </p><p class="source-code">                           "DOWN": 2, "LEFT": 3}</p><p class="source-code">        self.possible_actions = \</p><p class="source-code">                           list(self.action_map.values())</p></li>
				<li>Now, we can move <a id="_idIndexMarker127"/>on to the definition of the <strong class="source-inline">reset()</strong> method, which will be called at the start of every episode, including the first one:<p class="source-code">def reset(self):</p><p class="source-code">        self.done = False</p><p class="source-code">        self.steps = 0</p><p class="source-code">        self.map = np.zeros((3, 4))</p><p class="source-code">        self.map[self.goal_coordinate[0]]\</p><p class="source-code">                [self.goal_coordinate[1]] = 1</p><p class="source-code">        self.map[self.bomb_coordinate[0]]\</p><p class="source-code">                 [self.bomb_coordinate[1]] = -1</p><p class="source-code">        self.map[self.wall_coordinate[0]]\</p><p class="source-code">                [self.wall_coordinate[1]] = 2</p><p class="source-code">        if self.exploring_starts:</p><p class="source-code">            self.state = np.random.choice([0, 1, 2, 4, 6,</p><p class="source-code">                                           8, 9, 10, 11])</p><p class="source-code">        else:</p><p class="source-code">            self.state = 8</p><p class="source-code">        return self.state</p></li>
				<li>Let's implement a <strong class="source-inline">get_next_state</strong> method so that we can conveniently obtain the<a id="_idIndexMarker128"/> next state:<p class="source-code">def get_next_state(self, current_position, action):</p><p class="source-code">        next_state = self.index_to_coordinate_map[</p><p class="source-code">                            str(current_position)].copy()</p><p class="source-code">        if action == 0 and next_state[0] != 0 and \</p><p class="source-code">        next_state != [2, 1]:</p><p class="source-code">            # Move up</p><p class="source-code">            next_state[0] -= 1</p><p class="source-code">        elif action == 1 and next_state[1] != 3 and \</p><p class="source-code">        next_state != [1, 0]:</p><p class="source-code">            # Move right</p><p class="source-code">            next_state[1] += 1</p><p class="source-code">        elif action == 2 and next_state[0] != 2 and \</p><p class="source-code">        next_state != [0, 1]:</p><p class="source-code">            # Move down</p><p class="source-code">            next_state[0] += 1</p><p class="source-code">        elif action == 3 and next_state[1] != 0 and \</p><p class="source-code">        next_state != [1, 2]:</p><p class="source-code">            # Move left</p><p class="source-code">            next_state[1] -= 1</p><p class="source-code">        else:</p><p class="source-code">            pass</p><p class="source-code">        return self.coordinate_to_index_map[str(</p><p class="source-code">                                             next_state)]</p></li>
				<li>With that, we are<a id="_idIndexMarker129"/> ready to implement the main <strong class="source-inline">step</strong> method of the <strong class="source-inline">GridworldV2</strong> environment:<p class="source-code">def step(self, action):</p><p class="source-code">        assert action in self.possible_actions, \</p><p class="source-code">        f"Invalid action:{action}"</p><p class="source-code">        current_position = self.state</p><p class="source-code">        next_state = self.get_next_state(</p><p class="source-code">                               current_position, action)</p><p class="source-code">        self.steps += 1</p><p class="source-code">        if next_state == self.goal_state:</p><p class="source-code">            reward = 1</p><p class="source-code">            self.done = True</p><p class="source-code">        elif next_state == self.bomb_state:</p><p class="source-code">            reward = -1</p><p class="source-code">            self.done = True</p><p class="source-code">        else:</p><p class="source-code">            reward = self.step_cost</p><p class="source-code">        if self.steps == self.max_ep_length:</p><p class="source-code">            self.done = True</p><p class="source-code">        self.state = next_state</p><p class="source-code">        return next_state, reward, self.done</p></li>
				<li>Now, we can move on and implement the temporal difference learning algorithm. Let's begin by initializing the state values of the grid using a 2D <strong class="source-inline">numpy</strong> array and then set the <a id="_idIndexMarker130"/>value of the goal location and the bomb state:<p class="source-code">def temporal_difference_learning(env, max_episodes):</p><p class="source-code">    grid_state_values = np.zeros((len(</p><p class="source-code">                               env.distinct_states), 1))</p><p class="source-code">    grid_state_values[env.goal_state] = 1</p><p class="source-code">    grid_state_values[env.bomb_state] = -1</p></li>
				<li>Next, let's define the discount factor, <strong class="source-inline">gamma</strong>, the learning rate, <strong class="source-inline">alpha</strong>, and initialize <strong class="source-inline">done</strong> to <strong class="source-inline">False</strong>:<p class="source-code">    # v: state-value function</p><p class="source-code">    v = grid_state_values</p><p class="source-code">    gamma = 0.99  # Discount factor</p><p class="source-code">    alpha = 0.01  # learning rate</p><p class="source-code">    done = False</p></li>
				<li>We can now define the main outer loop so that it runs <strong class="source-inline">max_episodes</strong> times, resetting the state of the environment to its initial state at the start of every episode:<p class="source-code">for episode in range(max_episodes):</p><p class="source-code">        state = env.reset()</p></li>
				<li>Now, it's time to implement the inner loop with the temporal difference learning update one-liner:<p class="source-code">while not done:</p><p class="source-code">            action = env.action_space.sample()  </p><p class="source-code">              # random policy</p><p class="source-code">            next_state, reward, done = env.step(action)</p><p class="source-code">            # State-value function updates using TD(0)</p><p class="source-code">            v[state] += alpha * (reward + gamma * \</p><p class="source-code">                                v[next_state] - v[state])</p><p class="source-code">            state = next_state</p></li>
				<li>Once the learning has <a id="_idIndexMarker131"/>converged, we want to be able to visualize the state values for each state in the GridwordV2 environment. To do that, we can make use of the <strong class="source-inline">visualize_grid_state_values</strong> function from <strong class="source-inline">value_function_utils</strong>:<p class="source-code">visualize_grid_state_values(grid_state_values.reshape((3, 4)))</p></li>
				<li>We are now ready to run the <strong class="source-inline">temporal_difference_learning</strong> function from our main function:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    max_episodes = 4000</p><p class="source-code">    env = GridworldV2Env(step_cost=-0.1, </p><p class="source-code">                         max_ep_length=30)</p><p class="source-code">    temporal_difference_learning(env, max_episodes)</p></li>
				<li>The preceding code will take a few seconds to run temporal difference learning for <strong class="source-inline">max_episodes</strong>. It will then produce a diagram similar to the following:</li>
			</ol>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B15074_02_007.jpg" alt="Figure 2.7 – Rendering of the GridworldV2 environment, with the grid cell coordinates and state values colored according to the scale shown on the right  "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – Rendering of the GridworldV2 environment, with the grid cell coordinates and state values colored according to the scale shown on the right </p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor068"/>How it works…</h2>
			<p>Based on our environment's implementation, you may have noticed that <strong class="source-inline">goal_state</strong> is located at <strong class="source-inline">(0, 3)</strong> and that <strong class="source-inline">bomb_state</strong> is located at <strong class="source-inline">(1, 3)</strong>. This is based on the coordinates, colors, and values of the grid cells:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B15074_02_008.jpg" alt="Figure 2.8 – Rendering of the GridWorldV2 environment with initial state values "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Rendering of the GridWorldV2 environment with initial state values</p>
			<p>The state is linearized and is represented using a single integer indicating each of the 12 distinct states in the GridWorldV2 environment. The following diagram shows a linearized rendering of the grid states to give you a better understanding of the state encoding:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B15074_02_009.jpg" alt="Figure 2.9 – Linearized representation of the states "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – Linearized representation of the states</p>
			<p>Now that we have seen<a id="_idIndexMarker132"/> how to implement temporal difference learning, let's move on to building Monte Carlo algorithms.</p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor069"/>Building Monte Carlo prediction and control algorithms for RL</h1>
			<p>This recipe <a id="_idIndexMarker133"/>provides the<a id="_idIndexMarker134"/> ingredients for building a <strong class="bold">Monte Carlo</strong> prediction and control algorithm so that you can build your RL agents. Similar to the temporal difference learning algorithm, Monte Carlo learning methods can be used to learn both the state and the action value functions. Monte Carlo <a id="_idIndexMarker135"/>methods have zero bias since they learn<a id="_idIndexMarker136"/> from complete episodes with real experience, without approximate predictions. These methods are suitable for applications that require good<a id="_idIndexMarker137"/> convergence properties. The following diagram illustrates the value that's learned <a id="_idIndexMarker138"/>by the Monte Carlo method for the GridworldV2 environment:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B15074_02_010.jpg" alt="Figure 2.10 – Monte Carlo prediction of state values (left) and state-action values (right) "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – Monte Carlo prediction of state values (left) and state-action values (right)</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor070"/>Getting ready</h2>
			<p>To complete this recipe, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment and run <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statement runs without issues, you are ready to get started:</p>
			<p class="source-code">import numpy as np</p>
			<p>Now, let's begin.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor071"/>How to do it…</h2>
			<p>We will start by implementing the <strong class="source-inline">monte_carlo_prediction</strong> algorithm and visualizing the<a id="_idIndexMarker139"/> learned value function for each state in the <strong class="source-inline">GridworldV2</strong> environment. After that, we will<a id="_idIndexMarker140"/> implement an <strong class="bold">epsilon-greedy policy</strong> and the <strong class="source-inline">monte_carlo_control</strong> algorithm to construct an<a id="_idIndexMarker141"/> agent that will act in an RL environment.</p>
			<p>Follow these steps:</p>
			<ol>
				<li value="1">Let's start with <a id="_idIndexMarker142"/>the import statements and import the <a id="_idIndexMarker143"/>necessary Python modules:<p class="source-code">import numpy as np</p><p class="source-code">from envs.gridworldv2 import GridworldV2Env</p><p class="source-code">from value_function_utils import (</p><p class="source-code">    visualize_grid_action_values,</p><p class="source-code">    visualize_grid_state_values,</p><p class="source-code">)</p></li>
				<li>The next step is to define the <strong class="source-inline">monte_carlo_prediction</strong> function and initialize the necessary objects, as shown here:<p class="source-code">def monte_carlo_prediction(env, max_episodes):</p><p class="source-code">    returns = {state: [] for state in \</p><p class="source-code">               env.distinct_states}</p><p class="source-code">    grid_state_values = np.zeros(len(</p><p class="source-code">                                env.distinct_states))</p><p class="source-code">    grid_state_values[env.goal_state] = 1</p><p class="source-code">    grid_state_values[env.bomb_state] = -1</p><p class="source-code">    gamma = 0.99  # Discount factor</p></li>
				<li>Now, let's implement<a id="_idIndexMarker144"/> the outer loop. Outer loops are<a id="_idIndexMarker145"/> commonplace in all RL agent training <a id="_idIndexMarker146"/>code:<p class="source-code">for episode in range(max_episodes):</p><p class="source-code">        g_t = 0</p><p class="source-code">        state = env.reset()</p><p class="source-code">        done = False</p><p class="source-code">        trajectory = []</p></li>
				<li>Next up is<a id="_idIndexMarker147"/> the inner loop:<p class="source-code">        while not done:</p><p class="source-code">            action = env.action_space.sample()  </p><p class="source-code">                # random policy</p><p class="source-code">            next_state, reward, done = env.step(action)</p><p class="source-code">            trajectory.append((state, reward))</p><p class="source-code">            state = next_state</p></li>
				<li>We now have all the information we need to compute the state values of the states in the grid:<p class="source-code">for idx, (state, reward) in enumerate(trajectory[::-1]):</p><p class="source-code">            g_t = gamma * g_t + reward</p><p class="source-code">            # first visit Monte-Carlo prediction</p><p class="source-code">            if state not in np.array(trajectory[::-1])\</p><p class="source-code">            [:, 0][idx + 1 :]:</p><p class="source-code">                returns[str(state)].append(g_t)</p><p class="source-code">                grid_state_values[state] = np.mean(returns[str(state)])</p><p class="source-code">Let's visualize the learned state value <a id="_idIndexMarker148"/>function using the <strong class="source-inline">visualize_grid_state_values</strong> <a id="_idIndexMarker149"/>helper function from the <strong class="source-inline">value_function_utils</strong> script:</p><p class="source-code">visualize_grid_state_values(grid_state_values.reshape((3, 4)))</p></li>
				<li>Now, it's time to <a id="_idIndexMarker150"/>run our Monte Carlo predictor:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    max_episodes = 4000</p><p class="source-code">    env = GridworldV2Env(step_cost=-0.1, </p><p class="source-code">                         max_ep_length=30)</p><p class="source-code">    print(f"===Monte Carlo Prediction===")</p><p class="source-code">    monte_carlo_prediction(env, max_episodes)</p></li>
				<li>The preceding <a id="_idIndexMarker151"/>code should produce a diagram showing the rendering for the GridworldV2 environment, along with state values:<div id="_idContainer036" class="IMG---Figure"><img src="image/B15074_02_011.jpg" alt="Figure 2.11 – Rendering of GridworldV2 with state values learned using the  Monte Carlo prediction algorithm "/></div><p class="figure-caption">Figure 2.11 – Rendering of GridworldV2 with state values learned using the Monte Carlo prediction algorithm</p></li>
				<li>Let's implement a<a id="_idIndexMarker152"/> function for the epsilon-greedy<a id="_idIndexMarker153"/> policy:<p class="source-code">def epsilon_greedy_policy(action_logits, epsilon=0.2):</p><p class="source-code">    idx = np.argmax(action_logits)</p><p class="source-code">    probs = []</p><p class="source-code">    epsilon_decay_factor = np.sqrt(sum([a ** 2 for a in \</p><p class="source-code">                                        action_logits]))</p><p class="source-code">    if epsilon_decay_factor == 0:</p><p class="source-code">        epsilon_decay_factor = 1.0</p><p class="source-code">    for i, a in enumerate(action_logits):</p><p class="source-code">        if i == idx:</p><p class="source-code">            probs.append(round(1 - epsilon + (</p><p class="source-code">                    epsilon / epsilon_decay_factor), 3))</p><p class="source-code">        else:</p><p class="source-code">            probs.append(round(</p><p class="source-code">                    epsilon / epsilon_decay_factor, 3))</p><p class="source-code">    residual_err = sum(probs) - 1</p><p class="source-code">    residual = residual_err / len(action_logits)</p><p class="source-code">    return np.array(probs) - residual</p></li>
				<li>Now, let's <a id="_idIndexMarker154"/>move on to the <a id="_idIndexMarker155"/>implementation of the <strong class="bold">Monte Carlo Control</strong> algorithm for <a id="_idIndexMarker156"/>reinforcement learning. We will <a id="_idIndexMarker157"/>start by defining the function, along with the initial values for the<a id="_idIndexMarker158"/> state-action values:<p class="source-code">def monte_carlo_control(env, max_episodes):</p><p class="source-code">    grid_state_action_values = np.zeros((12, 4))</p><p class="source-code">    grid_state_action_values[3] = 1</p><p class="source-code">    grid_state_action_values[7] = -1</p></li>
				<li>Let's continue with the implementation of the Monte Carlo Control function by initializing the returns for all the possible state and action pairs:<p class="source-code">    possible_states = ["0", "1", "2", "3", "4", "5", "6", "7", "8", "9", "10", "11"]</p><p class="source-code">    possible_actions = ["0", "1", "2", "3"]</p><p class="source-code">    returns = {}</p><p class="source-code">    for state in possible_states:</p><p class="source-code">        for action in possible_actions:</p><p class="source-code">            returns[state + ", " + action] = []</p></li>
				<li>As the <a id="_idIndexMarker159"/>next step, let's define the outer loop for each episode and then the inner loop for each <a id="_idIndexMarker160"/>step in an episode. By <a id="_idIndexMarker161"/>doing this, we can collect trajectories of experience until the end of an<a id="_idIndexMarker162"/> episode:<p class="source-code">gamma = 0.99</p><p class="source-code">    for episode in range(max_episodes):</p><p class="source-code">        g_t = 0</p><p class="source-code">        state = env.reset()</p><p class="source-code">        trajectory = []</p><p class="source-code">        while True:</p><p class="source-code">            action_values = \</p><p class="source-code">                 grid_state_action_values[state]</p><p class="source-code">            probs = epsilon_greedy_policy(action_values)</p><p class="source-code">            action = np.random.choice(np.arange(4), \</p><p class="source-code">                                p=probs)  # random policy</p><p class="source-code">            next_state, reward, done = env.step(action)</p><p class="source-code">            trajectory.append((state, action, reward))</p><p class="source-code">            state = next_state</p><p class="source-code">            if done:</p><p class="source-code">                break</p></li>
				<li>Now that <a id="_idIndexMarker163"/>we have a full trajectory for an episode in the inner loop, we can implement our <a id="_idIndexMarker164"/>Monte Carlo Control update to update the state<a id="_idIndexMarker165"/>-action values:<p class="source-code">        for step in reversed(trajectory):</p><p class="source-code">            g_t = gamma * g_t + step[2]</p><p class="source-code">            Returns[str(step[0]) + ", " + \</p><p class="source-code">                    str(step[1])].append(g_t)</p><p class="source-code">            grid_state_action_values[step[0]][step[1]]= \</p><p class="source-code">            np.mean(</p><p class="source-code">                Returns[str(step[0]) + ", " + \</p><p class="source-code">                        str(step[1])]</p><p class="source-code">            )</p></li>
				<li>Once the outer loop <a id="_idIndexMarker166"/>completes, we can visualize the state-action values using the <strong class="source-inline">visualize_grid_action_values</strong> helper function from the <strong class="source-inline">value_function_utils</strong> script:<p class="source-code">visualize_grid_action_values(grid_state_action_values</p></li>
				<li>Finally, let's run our <strong class="source-inline">monte_carlo_control</strong> function to learn the <strong class="source-inline">state-action</strong> values in the GridworldV2 environment and display the learned values:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    max_episodes = 4000</p><p class="source-code">    env = GridworldV2Env(step_cost=-0.1, \</p><p class="source-code">                         max_ep_length=30)</p><p class="source-code">    print(f"===Monte Carlo Control===")</p><p class="source-code">    monte_carlo_control(env, max_episodes)</p><p>The preceding code will produce a rendering similar to the following:</p></li>
			</ol>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B15074_02_012.jpg" alt="Figure 2.12 – Rendering of the GridworldV2 environment with four action values per grid state shown using rectangles "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12 – Rendering of the GridworldV2 environment with four action values per grid state shown using rectangles</p>
			<p>That concludes this recipe!</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor072"/>How it works…</h2>
			<p>Monte Carlo methods for<a id="_idIndexMarker167"/> episodic tasks learn directly from experience from full sample returns obtained in an episode. The Monte Carlo <a id="_idIndexMarker168"/>prediction algorithm for <a id="_idIndexMarker169"/>estimating the value function based on first visit averaging is as follows:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B15074_02_013.jpg" alt="Figure 2.13 – Monte Carlo prediction algorithm "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – Monte Carlo prediction algorithm</p>
			<p>Once a series of trajectories <a id="_idIndexMarker170"/>have been collected by the agent, we can use the transition information in the Monte Carlo Control algorithm to<a id="_idIndexMarker171"/> learn the state-action value function. This can be used by an agent so that they <a id="_idIndexMarker172"/>can act in a given RL environment.</p>
			<p>The Monte Carlo Control algorithm is shown in the following diagram:</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B15074_02_014.jpg" alt="Figure 2.14 – Monte-Carlo Control algorithm "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.14 – Monte-Carlo Control algorithm</p>
			<p>The results of the learned <a id="_idIndexMarker173"/>state-action<a id="_idIndexMarker174"/> value function are shown in <em class="italic">Figure 2.12</em>, where each triangle in a grid cell shows the state-action value of taking that directional action in that grid state. The base of the<a id="_idIndexMarker175"/> triangle lies in the direction of <a id="_idIndexMarker176"/>the action. For example, the triangle in the top-left corner of <em class="italic">Figure 2.12</em> that has a value of 0.44 is the state-action value of taking the LEFT action in that grid state.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor073"/>Implementing the SARSA algorithm and an RL agent</h1>
			<p>This recipe will show you<a id="_idIndexMarker177"/> how to implement the <strong class="bold">State-Action-Reward-State-Action</strong> (<strong class="bold">SARSA</strong>) algorithm, as well as how to develop and train an agent using the SARSA algorithm so that it can act in a reinforcement <a id="_idIndexMarker178"/>learning environment. The SARSA algorithm can be applied to model-free control problems and allows us to optimize the value function of an unknown MDP.</p>
			<p>Upon completing this recipe, you will have a working RL agent that, when acting in the GridworldV2 environment, will<a id="_idIndexMarker179"/> generate the following state-action value function using the SARSA algorithm:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B15074_02_015.jpg" alt="Figure 2.15 – Rendering of the GridworldV2 environment – each triangle represents the action value of taking that directional action in that grid state "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.15 – Rendering of the GridworldV2 environment – each triangle represents the action value of taking that directional action in that grid state</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor074"/>Getting ready</h2>
			<p>To complete this recipe, you will need to <a id="_idIndexMarker180"/>activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment and run <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import random</p>
			<p>Now, let's begin.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor075"/>How to do it…</h2>
			<p>Let's implement the SARSA learning update as a function and make use of an epsilon-greedy exploration policy. With these<a id="_idIndexMarker181"/> two pieces combined, we will have a complete agent to act in a given RL environment. In this recipe, we<a id="_idIndexMarker182"/> will train and test the agent in the GridworldV2 environment.</p>
			<p>Let's start our implementation step by step:</p>
			<ol>
				<li value="1">First, let's define a function for implementing the SARSA algorithm and initialize the state-action values with zeros:<p class="source-code">def sarsa(env, max_episodes):</p><p class="source-code">    grid_action_values = np.zeros((len(</p><p class="source-code">               env.distinct_states), env.actio<a id="_idTextAnchor076"/>n_space.n))</p></li>
				<li>We can now update the values for the goal state and the bomb state based on the environment's configuration:<p class="source-code">    grid_action_values[env.goal_state] = 1</p><p class="source-code">    grid_action_values[env.bomb_state] = -1</p></li>
				<li>Let's define the discount factor, <strong class="source-inline">gamma</strong>, and the learning rate hyperparameter, <strong class="source-inline">alpha</strong>. Also, let's create a convenient alias for <strong class="source-inline">grid_action_values</strong> by calling it <strong class="source-inline">q</strong>:<p class="source-code">    gamma = 0.99  # discounting factor</p><p class="source-code">    alpha = 0.01  # learning rate</p><p class="source-code">    # q: state-action-value function</p><p class="source-code">    q = grid_action_values</p></li>
				<li>Let's begin to implement the outer loop:<p class="source-code">for episode in range(max_episodes):</p><p class="source-code">        step_num = 1</p><p class="source-code">        done = False</p><p class="source-code">        state = env.reset()</p><p class="source-code">        action = greedy_policy(q[state], 1)</p></li>
				<li>Now, it's time to implement the<a id="_idIndexMarker183"/> inner loop with the SARSA learning<a id="_idIndexMarker184"/> update step:<p class="source-code">while not done:</p><p class="source-code">            next_state, reward, done = env.step(action)</p><p class="source-code">            step_num += 1</p><p class="source-code">            decayed_epsilon = gamma ** step_num  </p><p class="source-code">            # Doesn't have to be gamma</p><p class="source-code">            next_action = greedy_policy(q[next_state], \</p><p class="source-code">                                        decayed_epsilon)</p><p class="source-code">            q[state][action] += alpha * (</p><p class="source-code">                reward + gamma * q[next_state] \</p><p class="source-code">                    [next_action] - q[state][action]</p><p class="source-code">            )</p><p class="source-code">            state = next_state</p><p class="source-code">            action = next_action</p></li>
				<li>As the final step in the <strong class="source-inline">sarsa</strong> function, let's visualize the state-action value function:<p class="source-code">visualize_grid_action_values(grid_action_values)</p></li>
				<li>Now, we will implement the epsilon-greedy policy that the agent will use:<p class="source-code">def greedy_policy(q_values, epsilon):</p><p class="source-code">    """Epsilon-greedy policy """</p><p class="source-code">    if random.random() &gt;= epsilon:</p><p class="source-code">        return np.argmax(q_values)</p><p class="source-code">    else:</p><p class="source-code">        return random.randint(0, 3)</p></li>
				<li>Finally, we must implement the main function and run the SARSA algorithm:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    max_episodes = 4000</p><p class="source-code">    env = GridworldV2Env(step_cost=-0.1, \</p><p class="source-code">                         max_ep_length=30)</p><p class="source-code">    sarsa(env, max_episodes)</p></li>
			</ol>
			<p>When executed, a <a id="_idIndexMarker185"/>rendering of the <a id="_idIndexMarker186"/>GridworldV2 environment with the state-action values will appear, as shown in the following diagram: </p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B15074_02_016.jpg" alt="Figure 2.16 – Output of the SARSA algorithm in the GridworldV2 environment "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.16 – Output of the SARSA algorithm in the GridworldV2 environment</p>
			<h2 id="_idParaDest-65"><a id="_idTextAnchor077"/>How it works…</h2>
			<p>SARSA is an on-policy <a id="_idIndexMarker187"/>temporal difference learning-based control algorithm. This recipe made uses of the SARSA algorithm to <a id="_idIndexMarker188"/>estimate the optimal state-action values. The SARSA algorithm can be summarized as follows:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B15074_02_017.jpg" alt="Figure 2.17 – SARSA algorithm "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.17 – SARSA algorithm</p>
			<p>As you may be able to tell, this is very <a id="_idIndexMarker189"/>similar to the Q-learning algorithm. The similarities will become clear when we look at the next recipe in<a id="_idIndexMarker190"/> this chapter, <em class="italic">Building a Q-learning agent</em>.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor078"/>Building a Q-learning agent</h1>
			<p>This recipe will show <a id="_idIndexMarker191"/>you how to build a <strong class="bold">Q-learning</strong> agent. Q-learning can be applied to model-free RL problems. It supports off-policy learning and therefore provides a practical solution to problems where available experiences were/are collected using some other policy or by some other agent (even humans).</p>
			<p>Upon completing this recipe, you will have a working RL agent that, when acting in the GridworldV2 environment, will generate the following state-action value function using the SARSA algorithm:</p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B15074_02_018.jpg" alt="Figure 2.18 – State-action values obtained using the Q-learning algorithm "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.18 – State-action values obtained using the Q-learning algorithm</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor079"/>Getting ready</h2>
			<p>To complete this recipe, you <a id="_idIndexMarker192"/>will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment and run <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import random</p>
			<p>Now, let's begin.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor080"/>How to do it…</h2>
			<p>Let's implement the Q-learning algorithm as a function, as well as an epsilon-greedy policy to build our Q-learning agent.</p>
			<p>Let's start our implementation:</p>
			<ol>
				<li value="1">First, let's define a function <a id="_idIndexMarker193"/>for implementing the Q-learning algorithm and initialize the state-action values with zeros:<p class="source-code">def q_learning(env, max_episodes):</p><p class="source-code">    grid_action_values = np.zeros((len(\</p><p class="source-code">        env.distinct_states), env.action_space.n))</p></li>
				<li>We can now update the values for the goal state and the bomb state based on the environment's configuration:<p class="source-code">    grid_action_values[env.goal_state] = 1</p><p class="source-code">    grid_action_values[env.bomb_state] = -1</p></li>
				<li>Let's define the discount factor, <strong class="source-inline">gamma</strong>, and the learning rate hyperparameter, <strong class="source-inline">alpha</strong>. Also, let's create a convenient alias for <strong class="source-inline">grid_action_values</strong> by calling it <strong class="source-inline">q</strong>:<p class="source-code">    gamma = 0.99  # discounting factor</p><p class="source-code">    alpha = 0.01  # learning rate</p><p class="source-code">    # q: state-action-value function</p><p class="source-code">    q = grid_action_values</p></li>
				<li>  Let's begin to implement the outer loop:<p class="source-code">for episode in range(max_episodes):</p><p class="source-code">        step_num = 1</p><p class="source-code">        done = False</p><p class="source-code">        state = env.reset()</p></li>
				<li>As the next step, let's implement the inner loop with the Q-learning update. We will also decay the epsilon used in the<a id="_idIndexMarker194"/> epsilon-greedy policy:<p class="source-code">        while not done:</p><p class="source-code">            decayed_epsilon = 1 * gamma ** step_num  </p><p class="source-code">            # Doesn't have to be gamma</p><p class="source-code">            action = greedy_policy(q[state], \</p><p class="source-code">                     decayed_epsilon)</p><p class="source-code">            next_state, reward, done = env.step(action)</p><p class="source-code">            # Q-Learning update</p><p class="source-code">            grid_action_values[state][action] += alpha *(</p><p class="source-code">                reward + gamma * max(q[next_state]) - \</p><p class="source-code">                q[state][action]</p><p class="source-code">            )</p><p class="source-code">            step_num += 1</p><p class="source-code">            state = next_state</p></li>
				<li>As the final step in the <strong class="source-inline">q_learning</strong> function, let's visualize the state-action value function:<p class="source-code">visualize_grid_action_values(grid_action_values)</p></li>
				<li>Next, we will implement the epsilon-greedy policy that the agent will use:<p class="source-code">def greedy_policy(q_values, epsilon):</p><p class="source-code">    """Epsilon-greedy policy """</p><p class="source-code">    if random.random() &gt;= epsilon:</p><p class="source-code">        return np.argmax(q_values)</p><p class="source-code">    else:</p><p class="source-code">        return random.randint(0, 3)</p></li>
				<li>Finally, we will implement the main<a id="_idIndexMarker195"/> function and run the SARSA algorithm:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    max_episodes = 4000</p><p class="source-code">    env = GridworldV2Env(step_cost=-0.1, </p><p class="source-code">                         max_ep_length=30)</p><p class="source-code">    q_learning(env, max_episodes)</p><p>When executed, a rendering of the GridworldV2 environment with the state-action values will appear, as shown in the following diagram:</p></li>
			</ol>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B15074_02_019.jpg" alt="Figure 2.19 – Rendering of the GridworldV2 environment with the action values obtained using the Q-learning algorithm "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.19 – Rendering of the GridworldV2 environment with the action values obtained using the Q-learning algorithm</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor081"/>How it works…</h2>
			<p>The Q-learning algorithm involves the<a id="_idIndexMarker196"/> Q value update, which can be summarized by the following equation:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/Formula_02_001.jpg" alt=""/>
				</div>
			</div>
			<p>Here, we have the following:</p>
			<ul>
				<li><img src="image/Formula_02_002.png" alt=""/> is the value of the Q function for the current state, s, and action, a.</li>
				<li><img src="image/Formula_02_003.png" alt=""/> is used for choosing the maximum value from the possible next steps.</li>
				<li><img src="image/Formula_02_004.png" alt=""/> is the current position of the agent.</li>
				<li><img src="image/Formula_02_005.png" alt=""/> is the current action.</li>
				<li><img src="image/Formula_02_006.png" alt=""/> is the learning rate.</li>
				<li><img src="image/Formula_02_007.png" alt=""/> is the reward that is received in the current position.</li>
				<li><img src="image/Formula_02_008.png" alt=""/> is the gamma (reward decay, discount factor).</li>
				<li><img src="image/Formula_02_009.png" alt=""/> is the next state.</li>
				<li><img src="image/Formula_02_010.png" alt=""/> is the actions that are available in the next state, <img src="image/Formula_02_011.png" alt=""/>.</li>
			</ul>
			<p>As you may now be able to tell, the difference between Q-learning and SARSA is only in how the action-value/Q-value of the next state and action pair is calculated. In Q-learning, we use <img src="image/Formula_02_012.png" alt=""/>, the maximum value of the Q-function, whereas in the SARSA algorithm, we take the Q-value of the action that was chosen in the next state. This may sound subtle, but because the <a id="_idIndexMarker197"/>Q-learning algorithm infers the value by taking the max over all actions and doesn't just infer based on the current behavior policy, it can directly learn the optimal policy. On the other hand, the SARSA algorithm learns a near-optimal policy based on the behavior policy's exploration parameter (for example, the ε parameter in the ε-greedy policy). The SARSA algorithm has a better convergence property than the Q-learning algorithm, so it is more suited for cases where learning happens online and or on a real-world system, or even if there are real resources (time and/or money) being spent compared to training in a simulation or simulated worlds. Q-learning is more suited for training an "optimal" agent in simulation or when the resources (like time/money) are not too costly.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor082"/>Implementing policy gradients</h1>
			<p><strong class="bold">Policy gradient algorithms</strong> are fundamental to<a id="_idIndexMarker198"/> reinforcement learning and serve as the basis for <a id="_idIndexMarker199"/>several advanced RL algorithms. These algorithms directly optimize for the best policy, which can lead to faster learning compared to value-based algorithms. Policy gradient algorithms are effective for problems/applications with high-dimensional or continuous action spaces. This recipe will show you how to implement policy gradient algorithms using TensorFlow 2.0. Upon <a id="_idIndexMarker200"/>completing this recipe, you will be able to train an RL agent in any compatible OpenAI Gym environment.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor083"/>Getting ready</h2>
			<p>To complete this recipe, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment and run <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">import tensorflow_probability as tfp</p>
			<p class="source-code">from tensorflow import keras</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import gym</p>
			<p>Now, let's begin.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor084"/>How to do it…</h2>
			<p>There are three main parts to this recipe. The first one is applying the policy function, which is going to be represented using a neural network implemented in TensorFlow 2.x. The second part is applying the Agent class' implementation, while the final part will be to apply a trainer function, which is used to train the policy gradient-based agent in a given RL environment.</p>
			<p>Let's start implementing the parts one by one:</p>
			<ol>
				<li value="1">The first step is to define the <strong class="source-inline">PolicyNet</strong> class. We will define the model so that it has three fully connected or <strong class="bold">dense</strong> neural network layers:<p class="source-code">class PolicyNet(keras.Model):</p><p class="source-code">    def __init__(self, action_dim=1):</p><p class="source-code">        super(PolicyNet, self).__init__()</p><p class="source-code">        self.fc1 = layers.Dense(24, activation="relu")</p><p class="source-code">        self.fc2 = layers.Dense(36, activation="relu")</p><p class="source-code">        self.fc3 = layers.Dense(action_dim,</p><p class="source-code">                                activation="softmax")</p></li>
				<li>Next, we will implement the <strong class="source-inline">call</strong> function, which<a id="_idIndexMarker201"/> will be called to process inputs to the model:<p class="source-code">    def call(self, x):</p><p class="source-code">        x = self.fc1(x)</p><p class="source-code">        x = self.fc2(x)</p><p class="source-code">        x = self.fc3(x)</p><p class="source-code">        return x</p></li>
				<li>Let's also define a <strong class="source-inline">process</strong> function that we can call with a batch of observations to be processed by the model:<p class="source-code">def process(self, observations):</p><p class="source-code">        # Process batch observations using `call(x)`</p><p class="source-code">        # behind-the-scenes</p><p class="source-code">        action_probabilities = \</p><p class="source-code">            self.predict_on_batch(observations)</p><p class="source-code">        return action_probabilities</p></li>
				<li>With the policy network defined, we can implement the <strong class="source-inline">Agent</strong> class, which utilizes the policy network, and an optimizer for training the model:<p class="source-code">class Agent(object):</p><p class="source-code">    def __init__(self, action_dim=1):</p><p class="source-code">        """Agent with a neural-network brain powered </p><p class="source-code">           policy</p><p class="source-code">        Args:</p><p class="source-code">            action_dim (int): Action dimension</p><p class="source-code">        """</p><p class="source-code">        self.policy_net = PolicyNet(</p><p class="source-code">                                  action_dim=action_dim)</p><p class="source-code">        self.optimizer = tf.keras.optimizers.Adam(</p><p class="source-code">                                     learning_rate=1e-3)</p><p class="source-code">        self.gamma = 0.99</p></li>
				<li>Now, let's define a policy helper function that<a id="_idIndexMarker202"/> takes an observation as input, has it processed by the policy network, and returns the action as the output:<p class="source-code">    def policy(self, observation):</p><p class="source-code">        observation = observation.reshape(1, -1)</p><p class="source-code">        observation = tf.convert_to_tensor(observation,</p><p class="source-code">                                        dtype=tf.float32)</p><p class="source-code">        action_logits = self.policy_net(observation)</p><p class="source-code">        action = tf.random.categorical(</p><p class="source-code">               tf.math.log(action_logits), num_samples=1)</p><p class="source-code">        return action</p></li>
				<li>Let's define another helper function to get the action from the agent:<p class="source-code">    def get_action(self, observation):</p><p class="source-code">        action = self.policy(observation).numpy()</p><p class="source-code">        return action.squeeze()</p></li>
				<li>Now, it's time to define the learning updates for the policy gradient algorithm. Let's initialize the <strong class="source-inline">learn</strong> function with <a id="_idIndexMarker203"/>an empty list for discounted rewards:<p class="source-code">    def learn(self, states, rewards, actions):</p><p class="source-code">        discounted_reward = 0</p><p class="source-code">        discounted_rewards = []</p><p class="source-code">        rewards.reverse()</p></li>
				<li>This is the right place to calculate the discounted rewards while using the episodic rewards as input:<p class="source-code">        for r in rewards:</p><p class="source-code">            discounted_reward = r + self.gamma * \</p><p class="source-code">                                    discounted_reward</p><p class="source-code">            discounted_rewards.append(discounted_reward)</p><p class="source-code">            discounted_rewards.reverse()</p></li>
				<li>Now, let's implement the crucial step of calculating the policy gradient and update the parameters of the neural network policy using an optimizer:<p class="source-code">        for state, reward, action in zip(states, </p><p class="source-code">        discounted_rewards, actions):</p><p class="source-code">            with tf.GradientTape() as tape:</p><p class="source-code">                action_probabilities = \</p><p class="source-code">                    self.policy_net(np.array([state]),\</p><p class="source-code">                                    training=True)</p><p class="source-code">                loss = self.loss(action_probabilities, \</p><p class="source-code">                                 action, reward)</p><p class="source-code">            grads = tape.gradient(loss, </p><p class="source-code">                     self.policy_net.trainable_variables)</p><p class="source-code">            self.optimizer.apply_gradients(</p><p class="source-code">                zip(grads, </p><p class="source-code">                    self.policy_net.trainable_variables)</p><p class="source-code">            )</p></li>
				<li>Let's implement the loss function that we referred to in the previous step to calculate the policy <a id="_idIndexMarker204"/>parameter updates:<p class="source-code">    def loss(self, action_probabilities, action, reward):</p><p class="source-code">        dist = tfp.distributions.Categorical(</p><p class="source-code">            probs=action_probabilities, dtype=tf.float32</p><p class="source-code">        )</p><p class="source-code">        log_prob = dist.log_prob(action)</p><p class="source-code">        loss = -log_prob * reward</p><p class="source-code">        return loss</p></li>
				<li>With the Agent class fully implemented, we can move on to implementing the agent training function. Let's start with the function's definition:<p class="source-code">def train(agent: Agent, env: gym.Env, episodes: int, render=True):</p><p class="source-code">    """Train `agent` in `env` for `episodes`</p><p class="source-code">    Args:</p><p class="source-code">        agent (Agent): Agent to train</p><p class="source-code">        env (gym.Env): Environment to train the agent</p><p class="source-code">        episodes (int): Number of episodes to train</p><p class="source-code">        render (bool): True=Enable/False=Disable \</p><p class="source-code">                        rendering; Default=True</p><p class="source-code">    """</p></li>
				<li>Now, let's begin with the outer loop implementation of the agent training function:<p class="source-code">for episode in range(episodes):</p><p class="source-code">        done = False</p><p class="source-code">        state = env.reset()</p><p class="source-code">        total_reward = 0</p><p class="source-code">        rewards = []</p><p class="source-code">        states = []</p><p class="source-code">        actions = []</p></li>
				<li>Let's continue to implement the<a id="_idIndexMarker205"/> inner loop to finalize the <strong class="source-inline">train</strong> function:<p class="source-code">        while not done:</p><p class="source-code">            action = agent.get_action(state)</p><p class="source-code">            next_state, reward, done, _ = \</p><p class="source-code">                                   env.step(action)</p><p class="source-code">            rewards.append(reward)</p><p class="source-code">            states.append(state)</p><p class="source-code">            actions.append(action)</p><p class="source-code">            state = next_state</p><p class="source-code">            total_reward += reward</p><p class="source-code">            if render:</p><p class="source-code">                env.render()</p><p class="source-code">            if done:</p><p class="source-code">                agent.learn(states, rewards, actions)</p><p class="source-code">                print("\n")</p><p class="source-code">            print(f"Episode#:{episode} \</p><p class="source-code">            ep_reward:{total_reward}", end="\r")</p></li>
				<li>Finally, we need to implement the main function:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    agent = Agent()</p><p class="source-code">    episodes = 5000</p><p class="source-code">    env = gym.make("MountainCar-v0")</p><p class="source-code">    train(agent, env, episodes)</p><p class="source-code">    env.close()</p><p>The preceding code will launch the<a id="_idIndexMarker206"/> training process for the <a id="_idIndexMarker207"/>agent in the <strong class="bold">MountainCar</strong> environment. This will render the environment (since <strong class="source-inline">render=True</strong>) and display what the agent is doing in the environment with respect to driving the car uphill. Once the agent has been trained for a sufficient number of episodes, you will see the agent driving the car all the way up hill, as shown in the following diagram:</p></li>
			</ol>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B15074_02_020.jpg" alt="Figure 2.20 – Policy gradient agent completing the MountainCar task "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.20 – Policy gradient agent completing the MountainCar task</p>
			<p>That concludes this<a id="_idIndexMarker208"/> recipe!</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor085"/>How it works…</h2>
			<p>We used TensorFlow 2.x's <strong class="bold">Keras API</strong> to define a<a id="_idIndexMarker209"/> multilayer feed-forward neural network model that represents the RL agent's policy. We then defined an Agent class that utilizes the neural network policy to act in the <strong class="source-inline">MountainCar</strong> RL environment. The policy gradient algorithm is shown in the following diagram:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B15074_02_021.jpg" alt="Figure 2.21 – Policy gradient algorithm "/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.21 – Policy gradient algorithm</p>
			<p>As you train the policy gradient-based agent, you will observe that while the agent can learn to drive the car up the mountain, this can take a long time or they may get stuck in local minima. This basic <a id="_idIndexMarker210"/>version of the policy gradient has some limitations. The policy gradient is an on-policy algorithm that can only use experiences/trajectories or episode transitions from the same policy that is being optimized. The basic version of the policy gradient algorithm does not provide a guarantee for monotonic improvements in performance as it can get stuck in local minima.</p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor086"/>Implementing actor-critic RL algorithms</h1>
			<p><strong class="bold">Actor-critic algorithms</strong> allow us to combine value-based and policy-based reinforcement learning – an all-in-one agent. While policy gradient methods directly search and optimize the policy in the <a id="_idIndexMarker211"/>policy space, leading to smoother learning <a id="_idIndexMarker212"/>curves and improvement guarantees, they tend to get stuck at the local maxima (for a long-term reward optimization objective). Value-based methods do not get stuck at local optimum values, but they lack convergence guarantees, and algorithms such as Q-learning tend to have high variance and are not very sample-efficient. Actor-critic methods combine the good qualities of both value-based and policy gradient-based algorithms. Actor-critic methods are also more sample-efficient. This recipe will make it easy for you to implement an actor-critic-based RL agent using TensorFlow 2.x. Upon completing this recipe, you will be able to train the actor-critic agent in any OpenAI Gym-compatible reinforcement learning environment. As an example, we will train the agent in the CartPole-V0 environment.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor087"/>Getting ready</h2>
			<p>To complete this recipe, you will need to activate the <strong class="source-inline">tf2rl-cookbook</strong> Python/conda virtual environment <a id="_idIndexMarker213"/>and run <strong class="source-inline">pip install -r requirements.txt</strong>. If the following import statements run without issues, you are ready to get started:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">import gym</p>
			<p class="source-code">import tensorflow_probability as tfp</p>
			<p>Now, let's begin.</p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor088"/>How to do it…</h2>
			<p>There are three main parts to this recipe. The first is creating the actor-critic model, which is going to be represented using a neural network implemented in TensorFlow 2.x. The second part is creating the Agent class' implementation, while the final part is going to be about creating a trainer function that will train the policy gradient-based agent in a given RL environment.</p>
			<p>Let's start implementing the parts one by one:</p>
			<ol>
				<li value="1">Let's begin with our implementation of the <strong class="source-inline">ActorCritic</strong> class:<p class="source-code">class ActorCritic(tf.keras.Model):</p><p class="source-code">    def __init__(self, action_dim):</p><p class="source-code">        super().__init__()</p><p class="source-code">        self.fc1 = tf.keras.layers.Dense(512, \</p><p class="source-code">                                        activation="relu")</p><p class="source-code">        self.fc2 = tf.keras.layers.Dense(128, \</p><p class="source-code">                                        activation="relu")</p><p class="source-code">        self.critic = tf.keras.layers.Dense(1, \</p><p class="source-code">                                          activation=None)</p><p class="source-code">        self.actor = tf.keras.layers.Dense(action_dim, \</p><p class="source-code">                                         activation=None)</p></li>
				<li>The final thing we need to <a id="_idIndexMarker214"/>do in the <strong class="source-inline">ActorCritic</strong> class is implement the <strong class="source-inline">call</strong> function, which performs a forward pass through the neural network model:<p class="source-code">    def call(self, input_data):</p><p class="source-code">        x = self.fc1(input_data)</p><p class="source-code">        x1 = self.fc2(x)</p><p class="source-code">        actor = self.actor(x1)</p><p class="source-code">        critic = self.critic(x1)</p><p class="source-code">        return critic, actor</p></li>
				<li>With the <strong class="source-inline">ActorCritic</strong> class defined, we can move on and implement the <strong class="source-inline">Agent</strong> class and initialize an <strong class="source-inline">ActorCritic</strong> model, along with an optimizer to update the parameters of the actor-critic model:<p class="source-code">class Agent:</p><p class="source-code">    def __init__(self, action_dim=4, gamma=0.99):</p><p class="source-code">        """Agent with a neural-network brain powered </p><p class="source-code">           policy</p><p class="source-code">        Args:</p><p class="source-code">            action_dim (int): Action dimension</p><p class="source-code">            gamma (float) : Discount factor. Default=0.99</p><p class="source-code">        """</p><p class="source-code">        self.gamma = gamma</p><p class="source-code">        self.opt = tf.keras.optimizers.Adam(</p><p class="source-code">                                      learning_rate=1e-4)</p><p class="source-code">        self.actor_critic = ActorCritic(action_dim)</p></li>
				<li>Next, we must implement <a id="_idIndexMarker215"/>the agent's <strong class="source-inline">get_action</strong> method:<p class="source-code">    def get_action(self, state):</p><p class="source-code">        _, action_probabilities = \</p><p class="source-code">                     self.actor_critic(np.array([state]))</p><p class="source-code">        action_probabilities = tf.nn.softmax(</p><p class="source-code">                                    action_probabilities)</p><p class="source-code">        action_probabilities = \</p><p class="source-code">                             action_probabilities.numpy()</p><p class="source-code">        dist = tfp.distributions.Categorical(</p><p class="source-code">            probs=action_probabilities, dtype=tf.float32</p><p class="source-code">        )</p><p class="source-code">        action = dist.sample()</p><p class="source-code">        return int(action.numpy()[0])</p></li>
				<li>Now, let's implement a function that will calculate the actor loss based on the actor-critic algorithm. This will drive the parameters of the actor-critic network and allow the agent to improve:<p class="source-code">    def actor_loss(self, prob, action, td):</p><p class="source-code">        prob = tf.nn.softmax(prob)</p><p class="source-code">        dist = tfp.distributions.Categorical(probs=prob,</p><p class="source-code">                                       dtype=tf.float32)</p><p class="source-code">        log_prob = dist.log_prob(action)</p><p class="source-code">        loss = -log_prob * td</p><p class="source-code">        return loss</p></li>
				<li>We are now<a id="_idIndexMarker216"/> ready to implement the learning function of the actor-critic agent:<p class="source-code">def learn(self, state, action, reward, next_state, done):</p><p class="source-code">        state = np.array([state])</p><p class="source-code">        next_state = np.array([next_state])</p><p class="source-code">        with tf.GradientTape() as tape:</p><p class="source-code">            value, action_probabilities = \</p><p class="source-code">                self.actor_critic(state, training=True)</p><p class="source-code">            value_next_st, _ = self.actor_critic(</p><p class="source-code">                               next_state, training=True)</p><p class="source-code">            td = reward + self.gamma * value_next_st * \</p><p class="source-code">                  (1 - int(done)) - value</p><p class="source-code">            actor_loss = self.actor_loss(</p><p class="source-code">                        action_probabilities, action, td)</p><p class="source-code">            critic_loss = td ** 2</p><p class="source-code">            total_loss = actor_loss + critic_loss</p><p class="source-code">        grads = tape.gradient(total_loss, </p><p class="source-code">                   self.actor_critic.trainable_variables)</p><p class="source-code">        self.opt.apply_gradients(zip(grads, </p><p class="source-code">                  self.actor_critic.trainable_variables))</p><p class="source-code">        return total_loss</p></li>
				<li>Now, let's define the<a id="_idIndexMarker217"/> training function for training the agent in a given RL environment:<p class="source-code">def train(agent, env, episodes, render=True):</p><p class="source-code">    """Train `agent` in `env` for `episodes`</p><p class="source-code">    Args:</p><p class="source-code">        agent (Agent): Agent to train</p><p class="source-code">        env (gym.Env): Environment to train the agent</p><p class="source-code">        episodes (int): Number of episodes to train</p><p class="source-code">        render (bool): True=Enable/False=Disable \</p><p class="source-code">                        rendering; Default=True</p><p class="source-code">    """</p><p class="source-code">    for episode in range(episodes):</p><p class="source-code">        done = False</p><p class="source-code">        state = env.reset()</p><p class="source-code">        total_reward = 0</p><p class="source-code">        all_loss = []</p><p class="source-code">        while not done:</p><p class="source-code">            action = agent.get_action(state)</p><p class="source-code">            next_state, reward, done, _ = \</p><p class="source-code">                                      env.step(action)</p><p class="source-code">            loss = agent.learn(state, action, reward, </p><p class="source-code">                               next_state, done)</p><p class="source-code">            all_loss.append(loss)</p><p class="source-code">            state = next_state</p><p class="source-code">            total_reward += reward</p><p class="source-code">            if render:</p><p class="source-code">                env.render()</p><p class="source-code">            if done:</p><p class="source-code">                print("\n")</p><p class="source-code">            print(f"Episode#:{episode} </p><p class="source-code">                    ep_reward:{total_reward}", </p><p class="source-code">                    end="\r")</p></li>
				<li>The final step is to implement the main function, which will call the trainer to train the agent for the specified number of episodes:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    env = gym.make("CartPole-v0")</p><p class="source-code">    agent = Agent(env.action_space.n)</p><p class="source-code">    num_episodes = 20000</p><p class="source-code">    train(agent, env, num_episodes)</p><p>Once the agent has been <a id="_idIndexMarker218"/>sufficiently trained, you will see that the agent is able to balance the pole on the cart pretty well, as shown in the following diagram:</p><div id="_idContainer059" class="IMG---Figure"><img src="image/B15074_02_022.jpg" alt="Figure 2.22 – Actor-critic agent solving the CartPole task "/></div></li>
			</ol>
			<p class="figure-caption">Figure 2.22 – Actor-critic agent solving the CartPole task</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor089"/>How it works…</h2>
			<p>In this recipe, we defined a neural <a id="_idIndexMarker219"/>network-based actor-critic model using TensorFlow 2.x's Keras API. In the neural network model, we defined two fully connected or dense neural network layers to extract features from the input. This produced two outputs corresponding to the output for an actor and an output for the critic. The critic's output is a single float value, whereas the actor's output represents the logits for each of the allowed actions in a given RL environment.</p>
		</div>
</body></html>