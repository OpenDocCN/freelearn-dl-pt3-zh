["```\nimport tensorflow as tf \nimport numpy as np\nimport pandas as pd\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\nhousing_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'\npath = tf.keras.utils.get_file(housing_url.split(\"/\")[-1], housing_url)\ncolumns = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE',            'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndata = pd.read_table(path, delim_whitespace=True, \n                     header=None, names=columns)\nnp.random.seed(1)\ntrain = data.sample(frac=0.8).copy()\ny_train = train['MEDV']\ntrain.drop('MEDV', axis=1, inplace=True)\ntest = data.loc[~data.index.isin(train.index)].copy()\ny_test = test['MEDV']\ntest.drop('MEDV', axis=1, inplace=True) \n```", "```\nlearning_rate = 0.05 \ndef make_input_fn(data_df, label_df, num_epochs=10, \n                  shuffle=True, batch_size=256):\n\n    def input_function():\n        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n        if shuffle:\n            ds = ds.shuffle(1000)\n        ds = ds.batch(batch_size).repeat(num_epochs)\n        return ds\n\n    return input_function\ndef define_feature_columns(data_df, categorical_cols, numeric_cols):\n    feature_columns = []\n\n    for feature_name in numeric_cols:              \n        feature_columns.append(tf.feature_column.numeric_column(\n             feature_name, dtype=tf.float32))\n\n    for feature_name in categorical_cols:\n        vocabulary = data_df[feature_name].unique()\n        feature_columns.append(\n                                                                   tf.feature_column.categorical_column_with_vocabulary_list(\n                                                 feature_name, vocabulary))\n    return feature_columns\ncategorical_cols = ['CHAS', 'RAD']\nnumeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nfeature_columns = define_feature_columns(data, categorical_cols, numeric_cols)\ntrain_input_fn = make_input_fn(train, y_train, num_epochs=1400)\ntest_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False) \n```", "```\nlinear_est = tf.estimator.LinearRegressor(feature_columns=feature_columns) \n```", "```\nlinear_est.train(train_input_fn)\nresult = linear_est.evaluate(test_input_fn)\nprint(result) \n```", "```\nINFO:tensorflow:Loss for final step: 25.013594.\n...\nINFO:tensorflow:Finished evaluation at 2020-05-11-15:48:16\nINFO:tensorflow:Saving dict for global step 2800: average_loss = 32.715736, global_step = 2800, label/mean = 22.048513, loss = 32.715736, prediction/mean = 21.27578 \n```", "```\ndef create_interactions(interactions_list, buckets=5):\n    interactions = list()\n    for (a, b) in interactions_list:\n        interactions.append(tf.feature_column.crossed_column([a, b], hash_bucket_size=buckets))\n    return interactions\nderived_feature_columns = create_interactions([['RM', 'LSTAT']])\nlinear_est = tf.estimator.LinearRegressor(feature_columns=feature_columns+derived_feature_columns)\nlinear_est.train(train_input_fn)\nresult = linear_est.evaluate(test_input_fn)\nprint(result) \n```", "```\ndef dicts_to_preds(pred_dicts):\n    return np.array([pred['predictions'] for pred in pred_dicts])\npreds = dicts_to_preds(linear_est.predict(test_input_fn))\nprint(preds) \n```", "```\nimport tensorflow as tf \nimport numpy as np\nimport pandas as pd\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar() \n```", "```\nimport tensorflow.keras as keras \n```", "```\ndef define_feature_columns_layers(data_df, categorical_cols, numeric_cols):\n    feature_columns = []\n    feature_layer_inputs = {}\n\n    for feature_name in numeric_cols:\n        feature_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n        feature_layer_inputs[feature_name] = tf.keras.Input(shape=(1,), name=feature_name)\n\n    for feature_name in categorical_cols:\n        vocabulary = data_df[feature_name].unique()\n        cat = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary)\n        cat_one_hot = tf.feature_column.indicator_column(cat)\n        feature_columns.append(cat_one_hot)\n        feature_layer_inputs[feature_name] = tf.keras.Input(shape=(1,), name=feature_name, dtype=tf.int32)\n\n    return feature_columns, feature_layer_inputs \n```", "```\ndef create_interactions(interactions_list, buckets=5):\n    feature_columns = []\n\n    for (a, b) in interactions_list:\n        crossed_feature = tf.feature_column.crossed_column([a, b], hash_bucket_size=buckets)\n        crossed_feature_one_hot = tf.feature_column.indicator_column(crossed_feature)\n        feature_columns.append(crossed_feature_one_hot)\n\n    return feature_columns \n```", "```\ndef create_linreg(feature_columns, feature_layer_inputs, optimizer):\n    feature_layer = keras.layers.DenseFeatures(feature_columns)\n    feature_layer_outputs = feature_layer(feature_layer_inputs)\n    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n    outputs = keras.layers.Dense(1, kernel_initializer='normal', activation='linear')(norm)\n\n    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n    model.compile(optimizer=optimizer, loss='mean_squared_error')\n    return model \n```", "```\ncategorical_cols = ['CHAS', 'RAD']\nnumeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nfeature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\ninteractions_columns = create_interactions([['RM', 'LSTAT']])\nfeature_columns += interactions_columns\noptimizer = keras.optimizers.Ftrl(learning_rate=0.02)\nmodel = create_linreg(feature_columns, feature_layer_inputs, optimizer) \n```", "```\nimport tempfile\ndef canned_keras(model):\n    model_dir = tempfile.mkdtemp()\n    keras_estimator = tf.keras.estimator.model_to_estimator(\n        keras_model=model, model_dir=model_dir)\n    return keras_estimator\nestimator = canned_keras(model) \n```", "```\ntrain_input_fn = make_input_fn(train, y_train, num_epochs=1400)\ntest_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\nestimator.train(train_input_fn)\nresult = estimator.evaluate(test_input_fn)\nprint(result) \n```", "```\nweights = estimator.get_variable_value('layer_with_weights-1/kernel/.ATTRIBUTES/VARIABLE_VALUE')\nprint(weights) \n```", "```\ndef extract_labels(feature_columns):\n    labels = list()\n    for col in feature_columns:\n        col_config = col.get_config()\n        if 'key' in col_config:\n            labels.append(col_config['key'])\n        elif 'categorical_column' in col_config:\n            if col_config['categorical_column']['class_name']=='VocabularyListCategoricalColumn':\n                key = col_config['categorical_column']['config']['key']\n                for item in col_config['categorical_column']['config']['vocabulary_list']:\n                     labels.append(key+'_val='+str(item))\n            elif col_config['categorical_column']['class_name']=='CrossedColumn':\n                keys = col_config['categorical_column']['config']['keys']\n                for bucket in range(col_config['categorical_column']['config']['hash_bucket_size']):\n                    labels.append('x'.join(keys)+'_bkt_'+str(bucket))\n    return labels \n```", "```\nlabels = extract_labels(feature_columns)\nfor label, weight in zip(labels, weights):\n    print(f\"{label:15s} : {weight[0]:+.2f}\") \n```", "```\n* define_feature_columns_layers\n* make_input_fn\n* create_interactions \n```", "```\nimport tensorflow as tf \nimport tensorflow.keras as keras\nimport numpy as np\nimport pandas as pd\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar() \n```", "```\ndef create_linreg(feature_columns, feature_layer_inputs, optimizer, \n                  loss='mean_squared_error', \n                  metrics=['mean_absolute_error']):\n    feature_layer = keras.layers.DenseFeatures(feature_columns)\n    feature_layer_outputs = feature_layer(feature_layer_inputs)\n    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n    outputs = keras.layers.Dense(1, kernel_initializer='normal', \n                                 activation='linear')(norm)\n\n    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], \n                        outputs=outputs)\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model \n```", "```\ncategorical_cols = ['CHAS', 'RAD']\nnumeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nfeature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\ninteractions_columns = create_interactions([['RM', 'LSTAT']])\nfeature_columns += interactions_columns\noptimizer = keras.optimizers.Ftrl(learning_rate=0.02)\nmodel = create_linreg(feature_columns, feature_layer_inputs, optimizer,\n                      loss='mean_absolute_error', \n                      metrics=['mean_absolute_error',                                'mean_squared_error'])\nestimator = canned_keras(model)\ntrain_input_fn = make_input_fn(train, y_train, num_epochs=1400)\ntest_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\nestimator.train(train_input_fn)\nresult = estimator.evaluate(test_input_fn)\nprint(result) \n```", "```\n{'loss': 3.1208777, 'mean_absolute_error': 3.1208777, 'mean_squared_error': 27.170328, 'global_step': 2800} \n```", "```\nimport tensorflow as tf \nimport tensorflow.keras as keras\nimport numpy as np\nimport pandas as pd\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\ndef create_ridge_linreg(feature_columns, feature_layer_inputs, optimizer, \n                        loss='mean_squared_error', \n                        metrics=['mean_absolute_error'],\n                        l2=0.01):\n\n    regularizer = keras.regularizers.l2(l2)\n    feature_layer = keras.layers.DenseFeatures(feature_columns)\n    feature_layer_outputs = feature_layer(feature_layer_inputs)\n    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n    outputs = keras.layers.Dense(1, \n                                 kernel_initializer='normal', \n                                 kernel_regularizer = regularizer, \n                                 activation='linear')(norm)\n\n    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()],                         outputs=outputs)\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model \n```", "```\ncategorical_cols = ['CHAS', 'RAD']\nnumeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nfeature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\ninteractions_columns = create_interactions([['RM', 'LSTAT']])\nfeature_columns += interactions_columns\noptimizer = keras.optimizers.Ftrl(learning_rate=0.02)\nmodel = create_ridge_linreg(feature_columns, feature_layer_inputs, optimizer,\n                      loss='mean_squared_error', \n                      metrics=['mean_absolute_error',                                'mean_squared_error'],\n                           l2=0.01)\nestimator = canned_keras(model)\ntrain_input_fn = make_input_fn(train, y_train, num_epochs=1400)\ntest_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\nestimator.train(train_input_fn)\nresult = estimator.evaluate(test_input_fn)\nprint(result) \n```", "```\n{'loss': 25.903751, 'mean_absolute_error': 3.27314, 'mean_squared_error': 25.676477, 'global_step': 2800} \n```", "```\ncreate_lasso_linreg.\ndef create_lasso_linreg(feature_columns, feature_layer_inputs, optimizer, \n                        loss='mean_squared_error', metrics=['mean_absolute_error'],\n                        l1=0.001):\n\n    regularizer = keras.regularizers.l1(l1)\n    feature_layer = keras.layers.DenseFeatures(feature_columns)\n    feature_layer_outputs = feature_layer(feature_layer_inputs)\n    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n    outputs = keras.layers.Dense(1, \n                                 kernel_initializer='normal', \n                                 kernel_regularizer = regularizer, \n                                 activation='linear')(norm)\n\n    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model\ncategorical_cols = ['CHAS', 'RAD']\nnumeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nfeature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\ninteractions_columns = create_interactions([['RM', 'LSTAT']])\nfeature_columns += interactions_columns\noptimizer = keras.optimizers.Ftrl(learning_rate=0.02)\nmodel = create_lasso_linreg(feature_columns, feature_layer_inputs, optimizer,\n                      loss='mean_squared_error', \n                      metrics=['mean_absolute_error',                                'mean_squared_error'],\n                           l1=0.001)\nestimator = canned_keras(model)\ntrain_input_fn = make_input_fn(train, y_train, num_epochs=1400)\ntest_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\nestimator.train(train_input_fn)\nresult = estimator.evaluate(test_input_fn)\nprint(result) \n```", "```\n{'loss': 24.616476, 'mean_absolute_error': 3.1985352, 'mean_squared_error': 24.59167, 'global_step': 2800} \n```", "```\ndef create_elasticnet_linreg(feature_columns, feature_layer_inputs, \n                             optimizer,                         \n                             loss='mean_squared_error', \n                             metrics=['mean_absolute_error'],\n                             l1=0.001, l2=0.01):\n\n    regularizer = keras.regularizers.l1_l2(l1=l1, l2=l2)\n    feature_layer = keras.layers.DenseFeatures(feature_columns)\n    feature_layer_outputs = feature_layer(feature_layer_inputs)\n    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n    outputs = keras.layers.Dense(1, \n                                 kernel_initializer='normal', \n                                 kernel_regularizer = regularizer, \n                                 activation='linear')(norm)\n\n    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], \n                        outputs=outputs)\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model \n```", "```\ncategorical_cols = ['CHAS', 'RAD']\nnumeric_cols = ['CRIM', 'ZN', 'INDUS',  'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PTRATIO', 'B', 'LSTAT']\nfeature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\ninteractions_columns = create_interactions([['RM', 'LSTAT']])\nfeature_columns += interactions_columns\noptimizer = keras.optimizers.Ftrl(learning_rate=0.02)\nmodel = create_elasticnet_linreg(feature_columns, feature_layer_inputs,                                  optimizer,\n                                 loss='mean_squared_error', \n                                 metrics=['mean_absolute_error',\n                                          'mean_squared_error'],\n                                 l1=0.001, l2=0.01)\nestimator = canned_keras(model)\ntrain_input_fn = make_input_fn(train, y_train, num_epochs=1400)\ntest_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\nestimator.train(train_input_fn)\nresult = estimator.evaluate(test_input_fn)\nprint(result) \n```", "```\n{'loss': 24.910872, 'mean_absolute_error': 3.208289, 'mean_squared_error': 24.659771, 'global_step': 2800} \n```", "```\nimport tensorflow as tf \nimport tensorflow.keras as keras\nimport numpy as np\nimport pandas as pd\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\nbreast_cancer = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data'\npath = tf.keras.utils.get_file(breast_cancer.split(\"/\")[-1], breast_cancer)\ncolumns = ['sample_code', 'clump_thickness', 'cell_size_uniformity',\n           'cell_shape_uniformity',\n           'marginal_adhesion', 'single_epithelial_cell_size',\n           'bare_nuclei', 'bland_chromatin',\n           'normal_nucleoli', 'mitoses', 'class']\ndata = pd.read_csv(path, header=None, names=columns, na_values=[np.nan, '?'])\ndata = data.fillna(data.median())\nnp.random.seed(1)\ntrain = data.sample(frac=0.8).copy()\ny_train = (train['class']==4).astype(int)\ntrain.drop(['sample_code', 'class'], axis=1, inplace=True)\ntest = data.loc[~data.index.isin(train.index)].copy()\ny_test = (test['class']==4).astype(int)\ntest.drop(['sample_code', 'class'], axis=1, inplace=True) \n```", "```\ndef create_logreg(feature_columns, feature_layer_inputs, optimizer, \n                  loss='binary_crossentropy', metrics=['accuracy'],\n                  l2=0.01):\n\n    regularizer = keras.regularizers.l2(l2)\n    feature_layer = keras.layers.DenseFeatures(feature_columns)\n    feature_layer_outputs = feature_layer(feature_layer_inputs)\n    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n    outputs = keras.layers.Dense(1, \n                                 kernel_initializer='normal', \n                                 kernel_regularizer = regularizer, \n                                 activation='sigmoid')(norm)\n\n    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model \n```", "```\ncategorical_cols = []\nnumeric_cols = ['clump_thickness', 'cell_size_uniformity', 'cell_shape_uniformity',\n                'marginal_adhesion', 'single_epithelial_cell_size', 'bare_  nuclei',\n                'bland_chromatin',\n                'normal_nucleoli', 'mitoses']\nfeature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\noptimizer = keras.optimizers.Ftrl(learning_rate=0.007)\nmodel = create_logreg(feature_columns, feature_layer_inputs, optimizer, l2=0.01)\nestimator = canned_keras(model)\ntrain_input_fn = make_input_fn(train, y_train, num_epochs=300, batch_size=32)\ntest_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\nestimator.train(train_input_fn)\nresult = estimator.evaluate(test_input_fn)\nprint(result) \n```", "```\n{'accuracy': 0.95, 'loss': 0.16382739, 'global_step': 5400} \n```", "```\nimport tensorflow as tf \nimport tensorflow.keras as keras\nimport numpy as np\nimport pandas as pd\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar() \n```", "```\ntry:\n    from tensorflow.python.keras.layers.kernelized import RandomFourierFeatures\nexcept:\n    # from TF 2.2\n    from tensorflow.keras.layers.experimental import RandomFourierFeatures \n```", "```\ndef create_svc(feature_columns, feature_layer_inputs, optimizer, \n               loss='hinge', metrics=['accuracy'],\n               l2=0.01, output_dim=64, scale=None):\n\n    regularizer = keras.regularizers.l2(l2)\n    feature_layer = keras.layers.DenseFeatures(feature_columns)\n    feature_layer_outputs = feature_layer(feature_layer_inputs)\n    norm = keras.layers.BatchNormalization()(feature_layer_outputs)\n    rff = RandomFourierFeatures(output_dim=output_dim, scale=scale, kernel_initializer='gaussian')(norm)\n    outputs = keras.layers.Dense(1, \n                                 kernel_initializer='normal', \n                                 kernel_regularizer = regularizer, \n                                 activation='sigmoid')(rff)\n\n    model = keras.Model(inputs=[v for v in feature_layer_inputs.values()], outputs=outputs)\n    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n    return model \n```", "```\ncategorical_cols = []\nnumeric_cols = ['clump_thickness', 'cell_size_uniformity', 'cell_shape_uniformity',\n                'marginal_adhesion', 'single_epithelial_cell_size', 'bare_nuclei', 'bland_chromatin',\n                'normal_nucleoli', 'mitoses']\nfeature_columns, feature_layer_inputs = define_feature_columns_layers(data, categorical_cols, numeric_cols)\noptimizer = keras.optimizers.Adam(learning_rate=0.00005)\nmodel = create_svc(feature_columns, feature_layer_inputs, optimizer, \n                   loss='hinge', l2=0.001, output_dim=512)\nestimator = canned_keras(model)\ntrain_input_fn = make_input_fn(train, y_train, num_epochs=500, batch_size=512)\ntest_input_fn = make_input_fn(test, y_test, num_epochs=1, shuffle=False)\nestimator.train(train_input_fn)\nresult = estimator.evaluate(test_input_fn)\nprint(result) \n```", "```\n{'accuracy': 0.95 'loss': 0.7390725, 'global_step': 1000} \n```", "```\ncensus_dir = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/'\ntrain_path = tf.keras.utils.get_file('adult.data', census_dir + 'adult.data')\ntest_path = tf.keras.utils.get_file('adult.test', census_dir + 'adult.test')\ncolumns = ['age', 'workclass', 'fnlwgt', 'education', 'education_num',\n           'marital_status', 'occupation', 'relationship', 'race',  \n           'gender', 'capital_gain', 'capital_loss', 'hours_per_week', \n           'native_country', 'income_bracket']\ntrain_data = pd.read_csv(train_path, header=None, names=columns)\ntest_data = pd.read_csv(test_path, header=None, names=columns, skiprows=1) \n```", "```\npredictors = ['age', 'workclass', 'education', 'education_num',\n              'marital_status', 'occupation', 'relationship', 'gender']\ny_train = (train_data.income_bracket==' >50K').astype(int)\ny_test = (test_data.income_bracket==' >50K.').astype(int)\ntrain_data = train_data[predictors]\ntest_data = test_data[predictors] \n```", "```\ntrain_data[['age', 'education_num']] = train_data[['age', 'education_num']].fillna(train_data[['age', 'education_num']].mean())\ntest_data[['age', 'education_num']] = test_data[['age', 'education_num']].fillna(train_data[['age', 'education_num']].mean()) \n```", "```\ndef define_feature_columns(data_df, numeric_cols, categorical_cols, categorical_embeds, dimension=30):\n    numeric_columns = []\n    categorical_columns = []\n    embeddings = []\n\n    for feature_name in numeric_cols:\n        numeric_columns.append(tf.feature_column.numeric_column(feature_name, dtype=tf.float32))\n\n    for feature_name in categorical_cols:\n        vocabulary = data_df[feature_name].unique()\n        categorical_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature_name, vocabulary))\n\n    for feature_name in categorical_embeds:\n        vocabulary = data_df[feature_name].unique()\n        to_categorical = tf.feature_column.categorical_column_with_vocabulary_list(feature_name, \n                                                          vocabulary)\nembeddings.append(tf.feature_column.embedding_column(to_categorical, \n                                                        dimension=dimension))\n\n    return numeric_columns, categorical_columns, embeddings\ndef create_interactions(interactions_list, buckets=10):\n    feature_columns = []\n\n    for (a, b) in interactions_list:\n        crossed_feature = tf.feature_column.crossed_column([a, b],                                             hash_bucket_size=buckets)\n        crossed_feature_one_hot = tf.feature_column.indicator_column(                                                     crossed_feature)\n        feature_columns.append(crossed_feature_one_hot)\n\n    return feature_columns \n```", "```\nnumeric_columns, categorical_columns, embeddings = define_feature_columns(train_data, \n                                                                          numeric_cols=['age', 'education_num'], \n                                                                          categorical_cols=['gender'], \n                                                                          categorical_embeds=['workclass', 'education',\n                                                                                              'marital_status', 'occupation', \n                                                                                              'relationship'], \n                                                                          dimension=32)\ninteractions = create_interactions([['education', 'occupation']], buckets=10) \n```", "```\nestimator = tf.estimator.DNNLinearCombinedClassifier(\n    # wide settings\n    linear_feature_columns=numeric_columns+categorical_columns+interactions,    linear_optimizer=keras.optimizers.Ftrl(learning_rate=0.0002),\n    # deep settings\n    dnn_feature_columns=embeddings,\n    dnn_hidden_units=[1024, 256, 128, 64],\n    dnn_optimizer=keras.optimizers.Adam(learning_rate=0.0001)) \n```", "```\ndef make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=256):\n\n    def input_function():\n        ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df))\n        if shuffle:\n            ds = ds.shuffle(1000)\n        ds = ds.batch(batch_size).repeat(num_epochs)\n        return ds\n\n    return input_function \n```", "```\ntrain_input_fn = make_input_fn(train_data, y_train, \n                               num_epochs=100, batch_size=256)\ntest_input_fn = make_input_fn(test_data, y_test, \n                              num_epochs=1, shuffle=False)\nestimator.train(input_fn=train_input_fn, steps=1500)\nresults = estimator.evaluate(input_fn=test_input_fn)\nprint(results) \n```", "```\n{'accuracy': 0.83391684, 'accuracy_baseline': 0.76377374, 'auc': 0.88012385, 'auc_precision_recall': 0.68032277, 'average_loss': 0.35969484, 'label/mean': 0.23622628, 'loss': 0.35985297, 'precision': 0.70583993, 'prediction/mean': 0.21803579, 'recall': 0.5091004, 'global_step': 1000} \n```", "```\ndef predict_proba(predictor):\n    preds = list()\n    for pred in predictor:\n        preds.append(pred['probabilities'])\n    return np.array(preds)\npredictions = predict_proba(estimator.predict(input_fn=test_input_fn)) \n```"]