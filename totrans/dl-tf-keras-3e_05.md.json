["```\nhidden_dim = 256\npeephole_cell = tf.keras.experimental.PeepholeLSTMCell(hidden_dim)\nrnn_layer = tf.keras.layers.RNN(peephole_cell) \n```", "```\nself.lstm = tf.keras.layers.Bidirectional(\n    tf.keras.layers.LSTM(10, return_sequences=True, \n        input_shape=(5, 10))\n) \n```", "```\nimport os\nimport numpy as np\nimport re\nimport shutil\nimport tensorflow as tf\nDATA_DIR = \"./data\"\nCHECKPOINT_DIR = os.path.join(DATA_DIR, \"checkpoints\") \n```", "```\ndef download_and_read(urls):\n    texts = []\n    for i, url in enumerate(urls):\n        p = tf.keras.utils.get_file(\"ex1-{:d}.txt\".format(i), url,\n            cache_dir=\".\")\n        text = open(p, \"r\").read()\n        # remove byte order mark\n        text = text.replace(\"\\ufeff\", \"\")\n        # remove newlines\n        text = text.replace('\\n', ' ')\n        text = re.sub(r'\\s+', \" \", text)\n        # add it to the list\n        texts.extend(text)\n    return texts\ntexts = download_and_read([\n    \"http://www.gutenberg.org/cache/epub/28885/pg28885.txt\",\n    \"https://www.gutenberg.org/files/12/12-0.txt\"\n]) \n```", "```\n# create the vocabulary\nvocab = sorted(set(texts))\nprint(\"vocab size: {:d}\".format(len(vocab)))\n# create mapping from vocab chars to ints\nchar2idx = {c:i for i, c in enumerate(vocab)}\nidx2char = {i:c for c, i in char2idx.items()} \n```", "```\n# numericize the texts\ntexts_as_ints = np.array([char2idx[c] for c in texts])\ndata = tf.data.Dataset.from_tensor_slices(texts_as_ints)\n# number of characters to show before asking for prediction\n# sequences: [None, 100]\nseq_length = 100\nsequences = data.batch(seq_length + 1, drop_remainder=True)\ndef split_train_labels(sequence):\n    input_seq = sequence[0:-1]\n    output_seq = sequence[1:]\n    return input_seq, output_seq\nsequences = sequences.map(split_train_labels)\n# set up for training\n# batches: [None, 64, 100]\nbatch_size = 64\nsteps_per_epoch = len(texts) // seq_length // batch_size\ndataset = sequences.shuffle(10000).batch(\n    batch_size, drop_remainder=True) \n```", "```\nclass CharGenModel(tf.keras.Model):\n    def __init__(self, vocab_size, num_timesteps,\n            embedding_dim, **kwargs):\n        super(CharGenModel, self).__init__(**kwargs)\n        self.embedding_layer = tf.keras.layers.Embedding(\n            vocab_size,\n            embedding_dim\n        )\n        self.rnn_layer = tf.keras.layers.GRU(\n            num_timesteps,\n            recurrent_initializer=\"glorot_uniform\",\n            recurrent_activation=\"sigmoid\",\n            stateful=True,\n            return_sequences=True)\n        self.dense_layer = tf.keras.layers.Dense(vocab_size)\n    def call(self, x):\n        x = self.embedding_layer(x)\n        x = self.rnn_layer(x)\n        x = self.dense_layer(x)\n        return x\nvocab_size = len(vocab)\nembedding_dim = 256\nmodel = CharGenModel(vocab_size, seq_length, embedding_dim)\nmodel.build(input_shape=(batch_size, seq_length)) \n```", "```\ndef loss(labels, predictions):\n    return tf.losses.sparse_categorical_crossentropy(\n        labels,\n        predictions,\n        from_logits=True\n    )\nmodel.compile(optimizer=tf.optimizers.Adam(), loss=loss) \n```", "```\ndef generate_text(model, prefix_string, char2idx, idx2char,\n        num_chars_to_generate=1000, temperature=1.0):\n    input = [char2idx[s] for s in prefix_string]\n    input = tf.expand_dims(input, 0)\n    text_generated = []\n    model.reset_states()\n    for i in range(num_chars_to_generate):\n        preds = model(input)\n        preds = tf.squeeze(preds, 0) / temperature\n        # predict char returned by model\n        pred_id = tf.random.categorical(\n            preds, num_samples=1)[-1, 0].numpy()\n        text_generated.append(idx2char[pred_id])\n        # pass the prediction as the next input to the model\n        input = tf.expand_dims([pred_id], 0)\n    return prefix_string + \"\".join(text_generated) \n```", "```\nnum_epochs = 50\nfor i in range(num_epochs // 10):\n    model.fit(\n        dataset.repeat(),\n        epochs=10,\n        steps_per_epoch=steps_per_epoch\n        # callbacks=[checkpoint_callback, tensorboard_callback]\n    )\n    checkpoint_file = os.path.join(\n        CHECKPOINT_DIR, \"model_epoch_{:d}\".format(i+1))\n    model.save_weights(checkpoint_file)\n    # create generative model using the trained model so far\n    gen_model = CharGenModel(vocab_size, seq_length, embedding_dim)\n    gen_model.load_weights(checkpoint_file)\n    gen_model.build(input_shape=(1, seq_length))\n    print(\"after epoch: {:d}\".format(i+1)*10)\n    print(generate_text(gen_model, \"Alice \", char2idx, idx2char))\n    print(\"---\") \n```", "```\nAlice nIPJtce otaishein r. henipt il nn tu t hen mlPde hc efa hdtioDDeteeybeaewI teu\"t e9B ce nd ageiw  eai rdoCr ohrSI ey Pmtte:vh ndte taudhor0-gu s5'ria,tr gn inoo luwomg Omke dee sdoohdn ggtdhiAoyaphotd t- kta e c t- taLurtn   hiisd tl'lpei od y' tpacoe dnlhr oG mGhod ut hlhoy .i, sseodli., ekngnhe idlue'aa'  ndti-rla nt d'eiAier adwe ai'otteniAidee hy-ouasq\"plhgs tuutandhptiw  oohe.Rastnint:e,o odwsir\"omGoeuall1*g taetphhitoge ds wr li,raa,  h$jeuorsu  h cidmdg't ku..n,HnbMAsn nsaathaa,' ase woe  ehf re ig\"hTr ddloese eod,aed toe rh k. nalf bte seyr udG n,ug lei hn icuimty\"onw Qee ivtsae zdrye g eut rthrer n sd,Zhqehd' sr caseruhel are fd yse e  kgeiiday odW-ldmkhNw endeM[harlhroa h Wydrygslsh EnilDnt e \"lue \"en wHeslhglidrth\"ylds rln n iiato taue flitl nnyg ittlno re 'el yOkao itswnadoli'.dnd Akib-ehn hftwinh yd ee tosetf tonne.;egren t wf, ota nfsr, t&he desnre e\" oo fnrvnse aid na tesd is ioneetIf Â·itrn tttpakihc s nih'bheY ilenf yoh etdrwdplloU ooaeedo,,dre snno'ofh o epst. lahehrw \n```", "```\nAlice Red Queen. He best I had defores it,' glily do flose time it makes the talking of find a hand mansed in she loweven to the rund not bright prough: the and she a chill be the sand using that whever sullusn--the dear of asker as 'IS now-- Chich the hood.\" \"Oh!\"' '_I'm num about--again was wele after a WAG LoANDE BITTER OF HSE!0 UUL EXMENN 1*.t, this wouldn't teese to Dumark THEVER Project Gutenberg-tmy of himid out flowal woulld: 'Nis song, Eftrin in pully be besoniokinote. \"Com, contimemustion--of could you knowfum to hard, she can't the with talking to alfoeys distrint, for spacemark!' 'You gake to be would prescladleding readieve other togrore what it mughturied ford of it was sen!\" You squs, _It I hap: But it was minute to the Kind she notion and teem what?\" said Alice, make there some that in at the shills distringulf out to the Froge, and very mind to it were it?' the King was set telm, what's the old all reads talking a minuse. \"Where ream put find growned his so,\" _you 'Fust to t \n```", "```\nAlice Vex her,\" he prope of the very managed by this thill deceed. I will ear she a much daid. \"I sha?' Nets: \"Woll, I should shutpelf, and now and then, cried, How them yetains, a tround her about in a shy time, I pashng round the sandle, droug\" shrees went on what he seting that,\" said Alice. \"Was this will resant again. Alice stook of in a faid.' 'It's ale. So they wentle shall kneeltie-and which herfer--the about the heald in pum little each the UKECE P@TTRUST GITE Ever been my hever pertanced to becristrdphariok, and your pringing that why the King as I to the King remark, but very only all Project Grizly: thentiused about doment,' Alice with go ould, are wayings for handsn't replied as mave about to LISTE!' (If the UULE 'TARY-HAVE BUY DIMADEANGNE'G THING NOOT,' be this plam round an any bar here! No, you're alard to be a good aftered of the sam--I canon't?\" said Alice. 'It's one eye of the olleations. Which saw do it just opened hardly deat, we hastowe. 'Of coum, is tried try slowing \n```", "```\n$ python alice_text_generator.py \n```", "```\nimport numpy as np\nimport os\nimport shutil\nimport tensorflow as tf\nfrom sklearn.metrics import accuracy_score, confusion_matrix \n```", "```\ndef download_and_read(url):\n    local_file = url.split('/')[-1]\n    local_file = local_file.replace(\"%20\", \" \")\n    p = tf.keras.utils.get_file(local_file, url,\n        extract=True, cache_dir=\".\")\n    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n    labeled_sentences = []\n    for labeled_filename in os.listdir(local_folder):\n        if labeled_filename.endswith(\"_labelled.txt\"):\n            with open(os.path.join(\n                    local_folder, labeled_filename), \"r\") as f:\n                for line in f:\n                    sentence, label = line.strip().split('\\t')\n                    labeled_sentences.append((sentence, label))\n    return labeled_sentences\nlabeled_sentences = download_and_read(      \n    \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" + \n    \"00331/sentiment%20labelled%20sentences.zip\")\nsentences = [s for (s, l) in labeled_sentences]\nlabels = [int(l) for (s, l) in labeled_sentences] \n```", "```\ntokenizer = tf.keras.preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(sentences)\nvocab_size = len(tokenizer.word_counts)\nprint(\"vocabulary size: {:d}\".format(vocab_size))\nword2idx = tokenizer.word_index\nidx2word = {v:k for (k, v) in word2idx.items()} \n```", "```\nseq_lengths = np.array([len(s.split()) for s in sentences])\nprint([(p, np.percentile(seq_lengths, p)) for p\n    in [75, 80, 90, 95, 99, 100]]) \n```", "```\n[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)] \n```", "```\nmax_seqlen = 64\n# create dataset\nsentences_as_ints = tokenizer.texts_to_sequences(sentences)\nsentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    sentences_as_ints, maxlen=max_seqlen)\nlabels_as_ints = np.array(labels)\ndataset = tf.data.Dataset.from_tensor_slices(\n    (sentences_as_ints, labels_as_ints)) \n```", "```\ndataset = dataset.shuffle(10000)\ntest_size = len(sentences) // 3\nval_size = (len(sentences) - test_size) // 10\ntest_dataset = dataset.take(test_size)\nval_dataset = dataset.skip(test_size).take(val_size)\ntrain_dataset = dataset.skip(test_size + val_size)\nbatch_size = 64\ntrain_dataset = train_dataset.batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)\ntest_dataset = test_dataset.batch(batch_size) \n```", "```\nclass SentimentAnalysisModel(tf.keras.Model):\n    def __init__(self, vocab_size, max_seqlen, **kwargs):\n        super(SentimentAnalysisModel, self).__init__(**kwargs)\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, max_seqlen)\n        self.bilstm = tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(max_seqlen)\n        )\n        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n    def call(self, x):\n        x = self.embedding(x)\n        x = self.bilstm(x)\n        x = self.dense(x)\n        x = self.out(x)\n        return x\nmodel = SentimentAnalysisModel(vocab_size+1, max_seqlen)\nmodel.build(input_shape=(batch_size, max_seqlen))\nmodel.summary()\n# compile\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n)\n# train\ndata_dir = \"./data\"\nlogs_dir = os.path.join(\"./logs\")\nbest_model_file = os.path.join(data_dir, \"best_model.h5\")\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n    save_weights_only=True,\n    save_best_only=True)\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\nnum_epochs = 10\nhistory = model.fit(train_dataset, epochs=num_epochs,\n    validation_data=val_dataset,\n    callbacks=[checkpoint, tensorboard]) \n```", "```\nEpoch 1/10\n29/29 [==============================] - 7s 239ms/step - loss: 0.6918 - accuracy: 0.5148 - val_loss: 0.6940 - val_accuracy: 0.4750\nEpoch 2/10\n29/29 [==============================] - 3s 98ms/step - loss: 0.6382 - accuracy: 0.5928 - val_loss: 0.6311 - val_accuracy: 0.6000\nEpoch 3/10\n29/29 [==============================] - 3s 100ms/step - loss: 0.3661 - accuracy: 0.8250 - val_loss: 0.4894 - val_accuracy: 0.7600\nEpoch 4/10\n29/29 [==============================] - 3s 99ms/step - loss: 0.1567 - accuracy: 0.9564 - val_loss: 0.5469 - val_accuracy: 0.7750\nEpoch 5/10\n29/29 [==============================] - 3s 99ms/step - loss: 0.0768 - accuracy: 0.9875 - val_loss: 0.6197 - val_accuracy: 0.7450\nEpoch 6/10\n29/29 [==============================] - 3s 100ms/step - loss: 0.0387 - accuracy: 0.9937 - val_loss: 0.6529 - val_accuracy: 0.7500\nEpoch 7/10\n29/29 [==============================] - 3s 99ms/step - loss: 0.0215 - accuracy: 0.9989 - val_loss: 0.7597 - val_accuracy: 0.7550\nEpoch 8/10\n29/29 [==============================] - 3s 100ms/step - loss: 0.0196 - accuracy: 0.9987 - val_loss: 0.6745 - val_accuracy: 0.7450\nEpoch 9/10\n29/29 [==============================] - 3s 99ms/step - loss: 0.0136 - accuracy: 0.9962 - val_loss: 0.7770 - val_accuracy: 0.7500\nEpoch 10/10\n29/29 [==============================] - 3s 99ms/step - loss: 0.0062 - accuracy: 0.9988 - val_loss: 0.8344 - val_accuracy: 0.7450 \n```", "```\nbest_model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\nbest_model.build(input_shape=(batch_size, max_seqlen))\nbest_model.load_weights(best_model_file)\nbest_model.compile(\n    loss=\"binary_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\"]\n) \n```", "```\ntest_loss, test_acc = best_model.evaluate(test_dataset)\nprint(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(\n    test_loss, test_acc)) \n```", "```\ntest loss: 0.487, test accuracy: 0.782 \n```", "```\nlabels, predictions = [], []\nidx2word[0] = \"PAD\"\nis_first_batch = True\nfor test_batch in test_dataset:\n   inputs_b, labels_b = test_batch\n   pred_batch = best_model.predict(inputs_b)\n   predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n   labels.extend([l for l in labels_b])\n   if is_first_batch:\n       # print first batch of label, prediction, and sentence\n       for rid in range(inputs_b.shape[0]):\n           words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n           words = [w for w in words if w != \"PAD\"]\n           sentence = \" \".join(words)\n           print(\"{:d}\\t{:d}\\t{:s}\".format(\n               labels[rid], predictions[rid], sentence))\n       is_first_batch = False\nprint(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\nprint(\"confusion matrix\")\nprint(confusion_matrix(labels, predictions)) \n```", "```\nLBL  PRED  SENT\n1     1    one of my favorite purchases ever\n1     1    works great\n1     1    our waiter was very attentive friendly and informative\n0     0    defective crap\n0     1    and it was way to expensive\n0     0    don't waste your money\n0     0    friend's pasta also bad he barely touched it\n1     1    it's a sad movie but very good\n0     0    we recently witnessed her poor quality of management towards other guests as well\n0     1    there is so much good food in vegas that i feel cheated for wasting an eating opportunity by going to rice and company \n```", "```\naccuracy score: 0.782\nconfusion matrix\n[[391  97]\n [121 391]] \n```", "```\n$ python lstm_sentiment_analysis.py \n```", "```\n>>> import nltk\n>>> nltk.download(\"treebank\") \n```", "```\nimport numpy as np\nimport os\nimport shutil\nimport tensorflow as tf \n```", "```\ndef download_and_read(dataset_dir, num_pairs=None):\n    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n        import nltk   \n        if not os.path.exists(dataset_dir):\n            os.makedirs(dataset_dir)\n        fsents = open(sent_filename, \"w\")\n        fposs = open(poss_filename, \"w\")\n        sentences = nltk.corpus.treebank.tagged_sents()\n        for sent in sentences:\n            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n        fsents.close()\n        fposs.close()\n    sents, poss = [], []\n    with open(sent_filename, \"r\") as fsent:\n        for idx, line in enumerate(fsent):\n            sents.append(line.strip())\n            if num_pairs is not None and idx >= num_pairs:\n                break\n    with open(poss_filename, \"r\") as fposs:\n        for idx, line in enumerate(fposs):\n            poss.append(line.strip())\n            if num_pairs is not None and idx >= num_pairs:\n                break\n    return sents, poss\nsents, poss = download_and_read(\"./datasets\")\nassert(len(sents) == len(poss))\nprint(\"# of records: {:d}\".format(len(sents))) \n```", "```\ndef tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n    if vocab_size is None:\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n    else:\n        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n    tokenizer.fit_on_texts(texts)\n    if vocab_size is not None:\n        # additional workaround, see issue 8092\n        # https://github.com/keras-team/keras/issues/8092\n        tokenizer.word_index = {e:i for e, i in\n            tokenizer.word_index.items() if \n            i <= vocab_size+1 }\n    word2idx = tokenizer.word_index\n    idx2word = {v:k for k, v in word2idx.items()}\n    return word2idx, idx2word, tokenizer\nword2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(\n    sents, vocab_size=9000)\nword2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(\n    poss, vocab_size=38, lower=False)\nsource_vocab_size = len(word2idx_s)\ntarget_vocab_size = len(word2idx_t)\nprint(\"vocab sizes (source): {:d}, (target): {:d}\".format(\n    source_vocab_size, target_vocab_size)) \n```", "```\nsequence_lengths = np.array([len(s.split()) for s in sents])\nprint([(p, np.percentile(sequence_lengths, p))\n    for p in [75, 80, 90, 95, 99, 100]])\n[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)] \n```", "```\nmax_seqlen = 271\n# convert sentences to sequence of integers\nsents_as_ints = tokenizer_s.texts_to_sequences(sents)\nsents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n# convert POS tags to sequence of (categorical) integers\nposs_as_ints = tokenizer_t.texts_to_sequences(poss)\nposs_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\nposs_as_catints = []\nfor p in poss_as_ints:\n    poss_as_catints.append(tf.keras.utils.to_categorical(p,\n        num_classes=target_vocab_size+1, dtype=\"int32\"))\nposs_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n    poss_as_catints, maxlen=max_seqlen)\ndataset = tf.data.Dataset.from_tensor_slices(\n    (sents_as_ints, poss_as_catints))\nidx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n# split into training, validation, and test datasets\ndataset = dataset.shuffle(10000)\ntest_size = len(sents) // 3\nval_size = (len(sents) - test_size) // 10\ntest_dataset = dataset.take(test_size)\nval_dataset = dataset.skip(test_size).take(val_size)\ntrain_dataset = dataset.skip(test_size + val_size)\n# create batches\nbatch_size = 128\ntrain_dataset = train_dataset.batch(batch_size)\nval_dataset = val_dataset.batch(batch_size)\ntest_dataset = test_dataset.batch(batch_size) \n```", "```\nclass POSTaggingModel(tf.keras.Model):\n    def __init__(self, source_vocab_size, target_vocab_size,\n            embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n        super(POSTaggingModel, self).__init__(**kwargs)\n        self.embed = tf.keras.layers.Embedding(\n            source_vocab_size, embedding_dim, input_length=max_seqlen)\n        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n        self.rnn = tf.keras.layers.Bidirectional(\n            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n        self.dense = tf.keras.layers.TimeDistributed(\n            tf.keras.layers.Dense(target_vocab_size))\n        self.activation = tf.keras.layers.Activation(\"softmax\")\n    def call(self, x):\n        x = self.embed(x)\n        x = self.dropout(x)\n        x = self.rnn(x)\n        x = self.dense(x)\n        x = self.activation(x)\n        return x\nembedding_dim = 128\nrnn_output_dim = 256\nmodel = POSTaggingModel(source_vocab_size, target_vocab_size,\n    embedding_dim, max_seqlen, rnn_output_dim)\nmodel.build(input_shape=(batch_size, max_seqlen))\nmodel.summary()\nmodel.compile(\n    loss=\"categorical_crossentropy\",\n    optimizer=\"adam\",\n    metrics=[\"accuracy\", masked_accuracy()]) \n the label and the prediction, as a result of which the accuracy numbers are very optimistic. In fact, the validation accuracy reported at the end of the very first epoch is 0.9116. However, the quality of POS tags generated is very poor.\n```", "```\ndef masked_accuracy():\n    def masked_accuracy_fn(ytrue, ypred):\n        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n        mask = tf.keras.backend.cast(\n            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n        matches = tf.keras.backend.cast(\n            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n        numer = tf.keras.backend.sum(matches)\n        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n        accuracy =  numer / denom\n        return accuracy\n    return masked_accuracy_fn \n```", "```\nnum_epochs = 50\nbest_model_file = os.path.join(data_dir, \"best_model.h5\")\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    best_model_file,\n    save_weights_only=True,\n    save_best_only=True)\ntensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\nhistory = model.fit(train_dataset,\n    epochs=num_epochs,\n    validation_data=val_dataset,\n    callbacks=[checkpoint, tensorboard]) \n```", "```\nEpoch 1/50\n19/19 [==============================] - 8s 431ms/step - loss: 1.4363 - accuracy: 0.7511 - masked_accuracy_fn: 0.00\n38 - val_loss: 0.3219 - val_accuracy: 0.9116 - val_masked_accuracy_fn: 0.5833\nEpoch 2/50\n19/19 [==============================] - 6s 291ms/step - loss: 0.3278 - accuracy: 0.9183 - masked_accuracy_fn: 0.17\n12 - val_loss: 0.3289 - val_accuracy: 0.9209 - val_masked_accuracy_fn: 0.1357\nEpoch 3/50\n19/19 [==============================] - 6s 292ms/step - loss: 0.3187 - accuracy: 0.9242 - masked_accuracy_fn: 0.1615 - val_loss: 0.3131 - val_accuracy: 0.9186 - val_masked_accuracy_fn: 0.2236\nEpoch 4/50\n19/19 [==============================] - 6s 293ms/step - loss: 0.3037 - accuracy: 0.9186 - masked_accuracy_fn: 0.1831 - val_loss: 0.2933 - val_accuracy: 0.9129 - val_masked_accuracy_fn: 0.1062\nEpoch 5/50\n19/19 [==============================] - 6s 294ms/step - loss: 0.2739 - accuracy: 0.9182 - masked_accuracy_fn: 0.1054 - val_loss: 0.2608 - val_accuracy: 0.9230 - val_masked_accuracy_fn: 0.1407\n...\nEpoch 45/50\n19/19 [==============================] - 6s 292ms/step - loss: 0.0653 - accuracy: 0.9810 - masked_accuracy_fn: 0.7872 - val_loss: 0.1545 - val_accuracy: 0.9611 - val_masked_accuracy_fn: 0.5407\nEpoch 46/50\n19/19 [==============================] - 6s 291ms/step - loss: 0.0640 - accuracy: 0.9815 - masked_accuracy_fn: 0.7925 - val_loss: 0.1550 - val_accuracy: 0.9616 - val_masked_accuracy_fn: 0.5441\nEpoch 47/50\n19/19 [==============================] - 6s 291ms/step - loss: 0.0619 - accuracy: 0.9818 - masked_accuracy_fn: 0.7971 - val_loss: 0.1497 - val_accuracy: 0.9614 - val_masked_accuracy_fn: 0.5535\nEpoch 48/50\n19/19 [==============================] - 6s 292ms/step - loss: 0.0599 - accuracy: 0.9825 - masked_accuracy_fn: 0.8033 - val_loss: 0.1524 - val_accuracy: 0.9616 - val_masked_accuracy_fn: 0.5579\nEpoch 49/50\n19/19 [==============================] - 6s 293ms/step - loss: 0.0585 - accuracy: 0.9830 - masked_accuracy_fn: 0.8092 - val_loss: 0.1544 - val_accuracy: 0.9617 - val_masked_accuracy_fn: 0.5621\nEpoch 50/50\n19/19 [==============================] - 6s 291ms/step - loss: 0.0575 - accuracy: 0.9833 - masked_accuracy_fn: 0.8140 - val_loss: 0.1569 - val_accuracy: 0.9615 - val_masked_accuracy_fn: 0.5511\n11/11 [==============================] - 2s 170ms/step - loss: 0.1436 - accuracy: 0.9637 - masked_accuracy_fn: 0.5786\ntest loss: 0.144, test accuracy: 0.963, masked test accuracy: 0.578 \n```", "```\nlabeled  : among/IN segments/NNS that/WDT t/NONE 1/VBP continue/NONE 2/TO to/VB operate/RB though/DT the/NN company/POS 's/NN steel/NN division/VBD continued/NONE 3/TO to/VB suffer/IN from/JJ soft/NN demand/IN for/PRP its/JJ tubular/NNS goods/VBG serving/DT the/NN oil/NN industry/CC and/JJ other/NNS\npredicted: among/IN segments/NNS that/WDT t/NONE 1/NONE continue/NONE 2/TO to/VB operate/IN though/DT the/NN company/NN 's/NN steel/NN division/NONE continued/NONE 3/TO to/IN suffer/IN from/IN soft/JJ demand/NN for/IN its/JJ tubular/NNS goods/DT serving/DT the/NNP oil/NN industry/CC and/JJ other/NNS\nlabeled  : as/IN a/DT result/NN ms/NNP ganes/NNP said/VBD 0/NONE t/NONE 2/PRP it/VBZ is/VBN believed/IN that/JJ little/CC or/DT no/NN sugar/IN from/DT the/CD 1989/NN 90/VBZ crop/VBN has/VBN been/NONE shipped/RB 1/RB yet/IN even/DT though/NN the/NN crop/VBZ year/CD is/NNS six/JJ\npredicted: as/IN a/DT result/NN ms/IN ganes/NNP said/VBD 0/NONE t/NONE 2/PRP it/VBZ is/VBN believed/NONE that/DT little/NN or/DT no/NN sugar/IN from/DT the/DT 1989/CD 90/NN crop/VBZ has/VBN been/VBN shipped/VBN 1/RB yet/RB even/IN though/DT the/NN crop/NN year/NN is/JJ\nlabeled  : in/IN the/DT interview/NN at/IN headquarters/NN yesterday/NN afternoon/NN both/DT men/NNS exuded/VBD confidence/NN and/CC seemed/VBD 1/NONE to/TO work/VB well/RB together/RB\npredicted: in/IN the/DT interview/NN at/IN headquarters/NN yesterday/NN afternoon/NN both/DT men/NNS exuded/NNP confidence/NN and/CC seemed/VBD 1/NONE to/TO work/VB well/RB together/RB\nlabeled  : all/DT came/VBD from/IN cray/NNP research/NNP\npredicted: all/NNP came/VBD from/IN cray/NNP research/NNP\nlabeled  : primerica/NNP closed/VBD at/IN 28/CD 25/NONE u/RB down/CD 50/NNS\npredicted: primerica/NNP closed/VBD at/CD 28/CD 25/CD u/CD down/CD \n```", "```\n$ python gru_pos_tagger.py \n```", "```\nimport nltk\nimport numpy as np\nimport re\nimport shutil\nimport tensorflow as tf\nimport os\nimport unicodedata\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction \n```", "```\ndef preprocess_sentence(sent):\n    sent = \"\".join([c for c in unicodedata.normalize(\"NFD\", sent)\n        if unicodedata.category(c) != \"Mn\"])\n    sent = re.sub(r\"([!.?])\", r\" \\1\", sent)\n    sent = re.sub(r\"[^a-zA-Z!.?]+\", r\" \", sent)\n    sent = re.sub(r\"\\s+\", \" \", sent)\n    sent = sent.lower()\n    return sent\ndef download_and_read():\n    en_sents, fr_sents_in, fr_sents_out = [], [], []\n    local_file = os.path.join(\"datasets\", \"fra.txt\")\n    with open(local_file, \"r\") as fin:\n        for i, line in enumerate(fin):\n            en_sent, fr_sent = line.strip().split('\\t')\n            en_sent = [w for w in preprocess_sentence(en_sent).split()]\n            fr_sent = preprocess_sentence(fr_sent)\n            fr_sent_in = [w for w in (\"BOS \" + fr_sent).split()]\n            fr_sent_out = [w for w in (fr_sent + \" EOS\").split()]\n            en_sents.append(en_sent)\n            fr_sents_in.append(fr_sent_in)\n            fr_sents_out.append(fr_sent_out)\n            if i >= num_sent_pairs - 1:\n                break\n    return en_sents, fr_sents_in, fr_sents_out\nsents_en, sents_fr_in, sents_fr_out = download_and_read() \n```", "```\ntokenizer_en = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\", lower=False)\ntokenizer_en.fit_on_texts(sents_en)\ndata_en = tokenizer_en.texts_to_sequences(sents_en)\ndata_en = tf.keras.preprocessing.sequence.pad_sequences(\n    data_en, padding=\"post\")\ntokenizer_fr = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\", lower=False)\ntokenizer_fr.fit_on_texts(sents_fr_in)\ntokenizer_fr.fit_on_texts(sents_fr_out)\ndata_fr_in = tokenizer_fr.texts_to_sequences(sents_fr_in)\ndata_fr_in = tf.keras.preprocessing.sequence.pad_sequences(\n    data_fr_in, padding=\"post\")\ndata_fr_out = tokenizer_fr.texts_to_sequences(sents_fr_out)\ndata_fr_out = tf.keras.preprocessing.sequence.pad_sequences(\n    data_fr_out, padding=\"post\")\nvocab_size_en = len(tokenizer_en.word_index)\nvocab_size_fr = len(tokenizer_fr.word_index)\nword2idx_en = tokenizer_en.word_index\nidx2word_en = {v:k for k, v in word2idx_en.items()}\nword2idx_fr = tokenizer_fr.word_index\nidx2word_fr = {v:k for k, v in word2idx_fr.items()}\nprint(\"vocab size (en): {:d}, vocab size (fr): {:d}\".format(\n    vocab_size_en, vocab_size_fr))\nmaxlen_en = data_en.shape[1]\nmaxlen_fr = data_fr_out.shape[1]\nprint(\"seqlen (en): {:d}, (fr): {:d}\".format(maxlen_en, maxlen_fr)) \n```", "```\nbatch_size = 64\ndataset = tf.data.Dataset.from_tensor_slices(\n    (data_en, data_fr_in, data_fr_out))\ndataset = dataset.shuffle(10000)\ntest_size = NUM_SENT_PAIRS // 4\ntest_dataset = dataset.take(test_size).batch(\n    batch_size, drop_remainder=True)\ntrain_dataset = dataset.skip(test_size).batch(\n    batch_size, drop_remainder=True) \n```", "```\nclass Encoder(tf.keras.Model):\n    def __init__(self, vocab_size, num_timesteps,\n            embedding_dim, encoder_dim, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.encoder_dim = encoder_dim\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim, input_length=num_timesteps)\n        self.rnn = tf.keras.layers.GRU(\n            encoder_dim, return_sequences=False, return_state=True)\n    def call(self, x, state):\n        x = self.embedding(x)\n        x, state = self.rnn(x, initial_state=state)\n        return x, state\n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.encoder_dim))\nclass Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, num_timesteps,\n            decoder_dim, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.decoder_dim = decoder_dim\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim, input_length=num_timesteps)\n        self.rnn = tf.keras.layers.GRU(\n            decoder_dim, return_sequences=True, return_state=True)\n        self.dense = tf.keras.layers.Dense(vocab_size)\n    def call(self, x, state):\n        x = self.embedding(x)\n        x, state = self.rnn(x, state)\n        x = self.dense(x)\n        return x, state\nembedding_dim = 256\nencoder_dim, decoder_dim = 1024, 1024\nencoder = Encoder(vocab_size_en+1, \n    embedding_dim, maxlen_en, encoder_dim)\ndecoder = Decoder(vocab_size_fr+1, \n    embedding_dim, maxlen_fr, decoder_dim) \n```", "```\nfor encoder_in, decoder_in, decoder_out in train_dataset:\n    encoder_state = encoder.init_state(batch_size)\n    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n    decoder_state = encoder_state\n    decoder_pred, decoder_state = decoder(decoder_in, decoder_state)\n    break\nprint(\"encoder input          :\", encoder_in.shape)\nprint(\"encoder output         :\", encoder_out.shape, \"state:\", encoder_state.shape)\nprint(\"decoder output (logits):\", decoder_pred.shape, \"state:\", decoder_state.shape)\nprint(\"decoder output (labels):\", decoder_out.shape) \n```", "```\nencoder input          : (64, 8)\nencoder output         : (64, 1024) state: (64, 1024)\ndecoder output (logits): (64, 16, 7658) state: (64, 1024)\ndecoder output (labels): (64, 16) \n```", "```\ndef loss_fn(ytrue, ypred):\n    scce = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True)\n    mask = tf.math.logical_not(tf.math.equal(ytrue, 0))\n    mask = tf.cast(mask, dtype=tf.int64)\n    loss = scce(ytrue, ypred, sample_weight=mask)\n    return loss \n```", "```\n@tf.function\ndef train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n    with tf.GradientTape() as tape:\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n        decoder_state = encoder_state\n        decoder_pred, decoder_state = decoder(\n            decoder_in, decoder_state)\n        loss = loss_fn(decoder_out, decoder_pred)\n\n    variables = (encoder.trainable_variables + \n        decoder.trainable_variables)\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return loss \n```", "```\ndef predict(encoder, decoder, batch_size,\n        sents_en, data_en, sents_fr_out,\n        word2idx_fr, idx2word_fr):\n    random_id = np.random.choice(len(sents_en))\n    print(\"input    : \",  \" \".join(sents_en[random_id]))\n    print(\"label    : \", \" \".join(sents_fr_out[random_id]))\n    encoder_in = tf.expand_dims(data_en[random_id], axis=0)\n    decoder_out = tf.expand_dims(sents_fr_out[random_id], axis=0)\n    encoder_state = encoder.init_state(1)\n    encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n    decoder_state = encoder_state\n    decoder_in = tf.expand_dims(\n        tf.constant([word2idx_fr[\"BOS\"]]), axis=0)\n    pred_sent_fr = []\n    while True:\n        decoder_pred, decoder_state = decoder(\n            decoder_in, decoder_state)\n        decoder_pred = tf.argmax(decoder_pred, axis=-1)\n        pred_word = idx2word_fr[decoder_pred.numpy()[0][0]]\n        pred_sent_fr.append(pred_word)\n        if pred_word == \"EOS\":\n            break\n        decoder_in = decoder_pred\n\n    print(\"predicted: \", \" \".join(pred_sent_fr))\ndef evaluate_bleu_score(encoder, decoder, test_dataset,\n        word2idx_fr, idx2word_fr):\n    bleu_scores = []\n    smooth_fn = SmoothingFunction()\n    for encoder_in, decoder_in, decoder_out in test_dataset:\n        encoder_state = encoder.init_state(batch_size)\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n        decoder_state = encoder_state\n        decoder_pred, decoder_state = decoder(\n            decoder_in, decoder_state)\n        # compute argmax\n        decoder_out = decoder_out.numpy()\n        decoder_pred = tf.argmax(decoder_pred, axis=-1).numpy()\n        for i in range(decoder_out.shape[0]):\n            ref_sent = [idx2word_fr[j] for j in \n                decoder_out[i].tolist() if j > 0]\n            hyp_sent = [idx2word_fr[j] for j in \n                decoder_pred[i].tolist() if j > 0]\n            # remove trailing EOS\n            ref_sent = ref_sent[0:-1]\n            hyp_sent = hyp_sent[0:-1]\n            bleu_score = sentence_bleu([ref_sent], hyp_sent,\n                smoothing_function=smooth_fn.method1)\n            bleu_scores.append(bleu_score)\n    return np.mean(np.array(bleu_scores)) \n```", "```\noptimizer = tf.keras.optimizers.Adam()\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\ncheckpoint = tf.train.Checkpoint(optimizer=optimizer,\n                                 encoder=encoder,\n                                 decoder=decoder)\nnum_epochs = 250\neval_scores = []\nfor e in range(num_epochs):\n    encoder_state = encoder.init_state(batch_size)\n    for batch, data in enumerate(train_dataset):\n        encoder_in, decoder_in, decoder_out = data\n        # print(encoder_in.shape, decoder_in.shape, decoder_out.shape)\n        loss = train_step(\n            encoder_in, decoder_in, decoder_out, encoder_state)\n\n    print(\"Epoch: {}, Loss: {:.4f}\".format(e + 1, loss.numpy()))\n    if e % 10 == 0:\n        checkpoint.save(file_prefix=checkpoint_prefix)\n\n    predict(encoder, decoder, batch_size, sents_en, data_en,\n        sents_fr_out, word2idx_fr, idx2word_fr)\n    eval_score = evaluate_bleu_score(encoder, decoder, \n        test_dataset, word2idx_fr, idx2word_fr)\n    print(\"Eval Score (BLEU): {:.3e}\".format(eval_score))\n    # eval_scores.append(eval_score)\ncheckpoint.save(file_prefix=checkpoint_prefix) \n```", "```\n$ python seq2seq_wo_attn.py \n```", "```\nclass Encoder(tf.keras.Model):\n     def __init__(self, vocab_size, num_timesteps,\n           embedding_dim, encoder_dim, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.encoder_dim = encoder_dim\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim, input_length=num_timesteps)\n        self.rnn = tf.keras.layers.GRU(\n            encoder_dim, return_sequences=True, return_state=True)\n    def call(self, x, state):\n        x = self.embedding(x)\n        x, state = self.rnn(x, initial_state=state)\n        return x, state\n    def init_state(self, batch_size):\n        return tf.zeros((batch_size, self.encoder_dim)) \n```", "```\nclass BahdanauAttention(tf.keras.layers.Layer):\n    def __init__(self, num_units):\n        super(BahdanauAttention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(num_units)\n        self.W2 = tf.keras.layers.Dense(num_units)\n        self.V = tf.keras.layers.Dense(1)\n    def call(self, query, values):\n        # query is the decoder state at time step j\n        # query.shape: (batch_size, num_units)\n        # values are encoder states at every timestep i\n        # values.shape: (batch_size, num_timesteps, num_units)\n        # add time axis to query: (batch_size, 1, num_units)\n        query_with_time_axis = tf.expand_dims(query, axis=1)\n        # compute score:\n        score = self.V(tf.keras.activations.tanh(\n            self.W1(values) + self.W2(query_with_time_axis)))\n        # compute softmax\n        alignment = tf.nn.softmax(score, axis=1)\n        # compute attended output\n        context = tf.reduce_sum(\n            tf.linalg.matmul(\n                tf.linalg.matrix_transpose(alignment),\n                values\n            ), axis=1\n        )\n        context = tf.expand_dims(context, axis=1)\n        return context, alignment \n```", "```\nclass LuongAttention(tf.keras.layers.Layer):\n    def __init__(self, num_units):\n        super(LuongAttention, self).__init__()\n        self.W = tf.keras.layers.Dense(num_units)\n    def call(self, query, values):\n        # add time axis to query\n        query_with_time_axis = tf.expand_dims(query, axis=1)\n        # compute score\n        score = tf.linalg.matmul(\n            query_with_time_axis, self.W(values), transpose_b=True)\n        # compute softmax\n        alignment = tf.nn.softmax(score, axis=2)\n        # compute attended output\n        context = tf.matmul(alignment, values)\n        return context, alignment \n```", "```\nbatch_size = 64\nnum_timesteps = 100\nnum_units = 1024\nquery = np.random.random(size=(batch_size, num_units))\nvalues = np.random.random(size=(batch_size, num_timesteps, num_units))\n# check out dimensions for Bahdanau attention\nb_attn = BahdanauAttention(num_units)\ncontext, alignments = b_attn(query, values)\nprint(\"Bahdanau: context.shape:\", context.shape, \n    \"alignments.shape:\", alignments.shape)\n# check out dimensions for Luong attention\nl_attn = LuongAttention(num_units)\ncontext, alignments = l_attn(query, values)\nprint(\"Luong: context.shape:\", context.shape, \n    \"alignments.shape:\", alignments.shape) \n```", "```\nBahdanau: context.shape: (64, 1024) alignments.shape: (64, 8, 1)\nLuong: context.shape: (64, 1024) alignments.shape: (64, 8, 1) \n```", "```\nclass Decoder(tf.keras.Model):\n    def __init__(self, vocab_size, embedding_dim, num_timesteps,\n            decoder_dim, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.decoder_dim = decoder_dim\n        self.attention = BahdanauAttention(embedding_dim)\n        # self.attention = LuongAttention(embedding_dim)\n\n        self.embedding = tf.keras.layers.Embedding(\n            vocab_size, embedding_dim, input_length=num_timesteps)\n        self.rnn = tf.keras.layers.GRU(\n            decoder_dim, return_sequences=True, return_state=True)\n        self.Wc = tf.keras.layers.Dense(decoder_dim, activation=\"tanh\")\n        self.Ws = tf.keras.layers.Dense(vocab_size)\n    def call(self, x, state, encoder_out):\n        x = self.embedding(x)\n        context, alignment = self.attention(x, encoder_out)\n        x = tf.expand_dims(\n                tf.concat([\n                    x, tf.squeeze(context, axis=1)\n                ], axis=1),\n            axis=1)\n        x, state = self.rnn(x, state)\n        x = self.Wc(x)\n        x = self.Ws(x)\n        return x, state, alignment \n```", "```\n@tf.function\ndef train_step(encoder_in, decoder_in, decoder_out, encoder_state):\n    with tf.GradientTape() as tape:\n        encoder_out, encoder_state = encoder(encoder_in, encoder_state)\n        decoder_state = encoder_state\n        loss = 0\n        for t in range(decoder_out.shape[1]):\n            decoder_in_t = decoder_in[:, t]\n            decoder_pred_t, decoder_state, _ = decoder(decoder_in_t,\n                decoder_state, encoder_out)\n            loss += loss_fn(decoder_out[:, t], decoder_pred_t)\n    variables = (encoder.trainable_variables +\n        decoder.trainable_variables)\n    gradients = tape.gradient(loss, variables)\n    optimizer.apply_gradients(zip(gradients, variables))\n    return loss / decoder_out.shape[1] \n```", "```\n$ python seq2seq_with_attn.py \n```"]