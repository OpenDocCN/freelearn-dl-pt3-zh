["```\nlogdir = './logs/model'\nwriter = tf.summary.create_file_writer(logdir)\n\ntf.summary.trace_on(profiler=True)\nmodel.predict(train_images)\nwith writer.as_default():\n  tf.summary.trace_export('trace-model', profiler_outdir=logdir)\n```", "```\n$ tensorboard --logdir logs\n```", "```\ntf.saved_model.save(model, export_dir='./saved_model')\n```", "```\nfrom tensorflow.python.tools import freeze_graph\n\noutput_node_names = ['dense/Softmax']\ninput_saved_model_dir = './saved_model_dir'\ninput_binary = False\ninput_saver_def_path = False\nrestore_op_name = None\nfilename_tensor_name = None\nclear_devices = True\ninput_meta_graph = False\ncheckpoint_path = None\ninput_graph_filename = None\nsaved_model_tags = tag_constants.SERVING\n\nfreeze_graph.freeze_graph(input_graph_filename, input_saver_def_path,\n                          input_binary, checkpoint_path, output_node_names,\n                          restore_op_name, filename_tensor_name,\n                          'frozen_model.pb', clear_devices, \"\", \"\", \"\",\n                          input_meta_graph, input_saved_model_dir,\n                          saved_model_tags)\n```", "```\nimport tfcoreml as tf_converter\n\ntf_converter.convert('frozen_model.pb',\n                     'mobilenet.mlmodel',\n                     class_labels=EMOTIONS,\n                     image_input_names=['input_0:0'],\n                     output_feature_names=[output_node_name + ':0'],\n                     red_bias=-1,\n                     green_bias=-1,\n                     blue_bias=-1,\n                     image_scale=1/127.5,\n                     is_bgr=False)\n```", "```\nprivate lazy var model: VNCoreMLModel = try! VNCoreMLModel(for: mobilenet().model)\n\nprivate lazy var classificationRequest: VNCoreMLRequest = {\n    let request = VNCoreMLRequest(model: model, completionHandler: { [weak self] request, error in\n        self?.processClassifications(for: request, error: error)\n    })\n    request.imageCropAndScaleOption = .centerCrop\n    return request\n}()\n```", "```\nprivate let faceDetectionRequest = VNDetectFaceRectanglesRequest()\nprivate let faceDetectionHandler = VNSequenceRequestHandler()\n```", "```\ntry faceDetectionHandler.perform([faceDetectionRequest], on: pixelBuffer, orientation: exifOrientation)\n\nguard let faceObservations = faceDetectionRequest.results as? [VNFaceObservation], faceObservations.isEmpty == false else {\n    return\n}\n```", "```\nlet classificationHandler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, orientation: .right, options: [:])\n\nlet box = faceObservation.boundingBox\nlet region = CGRect(x: box.minY, y: 1 - box.maxX, width: box.height, height:box.width)\nself.classificationRequest.regionOfInterest = region\n\ntry classificationHandler.perform([self.classificationRequest])\n```", "```\n# From a Keras model\nconverter = tf.lite.TFLiteConverter.from_keras_model(model)\n## Or from a SavedModel\nconverter = tf.lite.TFLiteConverter('./saved_model')\n```", "```\ntflite_model = converter.convert()\nopen(\"result.tflite\", \"wb\").write(tflite_model)\n```", "```\ntfliteModel = loadModelFile(activity);\n\n```", "```\nInterpreter.Options tfliteOptions = new Interpreter.Options();\ntflite = new Interpreter(tfliteModel, tfliteOptions);\n\n```", "```\nimgData =\n    ByteBuffer.allocateDirect(\n        DIM_BATCH_SIZE\n            * getImageSizeX()\n            * getImageSizeY()\n            * DIM_PIXEL_SIZE\n            * getNumBytesPerChannel());\n\n```", "```\nfaceDetector = new FaceDetector.Builder(this.getContext())\n        .setMode(FaceDetector.FAST_MODE)\n        .setTrackingEnabled(false)\n        .setLandmarkType(FaceDetector.NO_LANDMARKS)\n        .build();\n```", "```\nBitmap bitmap = textureView.getBitmap(previewSize.getHeight() / 4, previewSize.getWidth() / 4)\n\n```", "```\nFrame frame = new Frame.Builder().setBitmap(bitmap).build();\nfaces = faceDetector.detect(frame);\n```", "```\nBitmap faceBitmap = cropFaceInBitmap(face, bitmap);\nBitmap resized = Bitmap.createScaledBitmap(faceBitmap, \nclassifier.getImageSizeX(), classifier.getImageSizeY(), true)\n```", "```\nimgData.rewind();\nresized.getPixels(intValues, 0, resized.getWidth(), 0, 0, resized.getWidth(), resized.getHeight());\n\nint pixel = 0;\nfor (int i = 0; i < getImageSizeX(); ++i) {\n  for (int j = 0; j < getImageSizeY(); ++j) {\n    final int val = intValues[pixel++];\n    addPixelValue(val);\n  }\n}\n```", "```\nprotected void addPixelValue(int pixelValue) {\n  float mean =  (((pixelValue >> 16) & 0xFF) + ((pixelValue >> 8) & 0xFF) + (pixelValue & 0xFF)) / 3.0f;\n  imgData.putFloat(mean / 127.5f - 1.0f);\n}\n```", "```\nfloat[][] labelProbArray = new float[1][getNumLabels()];\ntflite.run(imgData, labelProbArray);\n```", "```\n$ tensorflowjs_converter ./saved_model --input_format=tf_saved_model my-tfjs --output_format tfjs_graph_model\n```", "```\nimport * as tf from '@tensorflow/tfjs';\nconst model = await tf.loadModel(MOBILENET_MODEL_PATH);\n\n```", "```\nimport * as faceapi from 'face-api.js';\nawait faceapi.loadTinyFaceDetectorModel(DETECTION_MODEL_PATH)\n```", "```\nconst video = document.getElementById('video');\nconst detection = await faceapi.detectSingleFace(video, new faceapi.TinyFaceDetectorOptions())\n\nif (detection) {\n const faceCanvases = await faceapi.extractFaces(video, [detection])\n const values = await predict(faceCanvases[0])\n}\n```", "```\nasync function predict(imgElement) {\n  let img = await tf.browser.fromPixels(imgElement, 3).toFloat();\n\n  const logits = tf.tidy(() => {\n    // tf.fromPixels() returns a Tensor from an image element.\n    img = tf.image.resizeBilinear(img, [IMAGE_SIZE, IMAGE_SIZE]);\n    img = img.mean(2);\n    const offset = tf.scalar(127.5);\n    // Normalize the image from [0, 255] to [-1, 1].\n    const normalized = img.sub(offset).div(offset);\n    const batched = normalized.reshape([1, IMAGE_SIZE, IMAGE_SIZE, 1]);\n\n    return mobilenet.predict(batched);\n  });\n\n  return logits\n}\n```"]