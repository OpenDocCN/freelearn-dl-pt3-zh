<html><head></head><body>
  <div id="_idContainer1837">
    <h1 class="chapterNumber">12</h1>
    <h1 id="_idParaDest-306" class="chapterTitle">Learning DDPG, TD3, and SAC</h1>
    <p class="normal">In the previous chapter, we learned about interesting actor-critic methods, such as <strong class="keyword">Advantage Actor-Critic</strong> (<strong class="keyword">A2C</strong>) and <strong class="keyword">Asynchronous Advantage Actor-Critic</strong> (<strong class="keyword">A3C</strong>). In this chapter, we will learn several state-of-the-art actor-critic methods. We will start off the chapter by understanding one of the popular actor-critic methods called <strong class="keyword">Deep Deterministic Policy Gradient</strong> (<strong class="keyword">DDPG</strong>). DDPG is used only in continuous environments, that is, environments with a continuous action space. We will understand what DDPG is and how it works in detail. We will also learn the DDPG algorithm step by step.</p>
    <p class="normal">Going forward, we will learn about the <strong class="keyword">Twin Delayed Deep Deterministic Policy Gradient </strong>(<strong class="keyword">TD3</strong>). TD3 is an improvement over the DDPG algorithm and includes several interesting features that solve the problems faced in DDPG. We will understand the key features of TD3 in detail and also look into the algorithm of TD3 step by step.</p>
    <p class="normal">Finally, we will learn about another interesting actor-critic algorithm, called <strong class="keyword">Soft Actor-Critic (SAC)</strong>. We will learn what SAC is and how it works using the entropy term in the objective function. We will look into the actor and critic components of SAC in detail and then learn the algorithm of SAC step by step.</p>
    <p class="normal">In this chapter, we will learn the following topics:</p>
    <ul>
      <li class="bullet">Deep deterministic policy gradient (DDPG)</li>
      <li class="bullet">The components of DDPG</li>
      <li class="bullet">The DDPG algorithm</li>
      <li class="bullet">Twin delayed deep deterministic policy gradient (TD3)</li>
      <li class="bullet">The key features of TD3</li>
      <li class="bullet">The TD3 algorithm</li>
      <li class="bullet">Soft actor-critic (SAC)</li>
      <li class="bullet">The components of SAC</li>
      <li class="bullet">The SAC algorithm</li>
    </ul>
    <h1 id="_idParaDest-307" class="title">Deep deterministic policy gradient </h1>
    <p class="normal">DDPG is an off-policy, model-free algorithm, designed <a id="_idIndexMarker1053"/>for environments where the action space is continuous. In the previous chapter, we learned how the actor-critic method works. DDPG is an actor-critic method where the actor estimates the policy using the policy gradient, and the critic evaluates the policy produced by the actor using the Q function.</p>
    <p class="normal">DDPG uses the policy network as an actor and deep Q network as a critic. One important difference between the DPPG and actor-critic algorithms we learned in the previous chapter is that DDPG tries to learn a deterministic policy instead of a stochastic policy. </p>
    <p class="normal">First, we will get an intuitive understanding of how DDPG works and then we will look into the algorithm in detail. </p>
    <h2 id="_idParaDest-308" class="title">An overview of DDPG</h2>
    <p class="normal">DDPG is <a id="_idIndexMarker1054"/>an actor-critic method that takes advantage of both the policy-based method and the value-based method. It uses a deterministic policy <img src="../Images/B15558_12_001.png" alt="" style="height: 0.84em;"/> instead of a stochastic policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">We learned that a deterministic policy tells the agent to perform one particular action in a given state, meaning a deterministic policy maps the state to one particular action:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_003.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Whereas a stochastic policy maps the state to the probability distribution over the action space:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_004.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">In a deterministic policy, whenever the agent visits the state, it always performs the same particular action. But with a stochastic policy, instead of performing the same action every time the agent visits the state, the agent performs a different action each time based on a probability distribution over the action space.</p>
    <p class="normal">Now, we <a id="_idIndexMarker1055"/>will look into an overview of the actor and critic networks in the DDPG algorithm.</p>
    <h3 id="_idParaDest-309" class="title">Actor </h3>
    <p class="normal">The actor <a id="_idIndexMarker1056"/>in DDPG is basically the policy network. The goal of the actor is to learn the mapping between the state and action. That is, the role of the actor is to learn the optimal policy that gives the maximum return. So, the actor uses the policy gradient method to learn the optimal policy.</p>
    <h3 id="_idParaDest-310" class="title">Critic </h3>
    <p class="normal">The critic is <a id="_idIndexMarker1057"/>basically the value network. The goal of the critic is to evaluate the action produced by the actor network. How does the critic network evaluate the action produced by the actor network? Let's suppose we have a Q function; can we evaluate an action using the Q function? Yes! First, let's take a little detour and recap the use of the Q function.</p>
    <p class="normal">We know that the Q function gives the expected return that an agent would obtain starting from state <em class="italic">s</em> and performing an action <em class="italic">a</em> following a particular policy. The expected return produced by the Q function is often called the Q value. Thus, given a state and action, we obtain a Q value:</p>
    <ul>
      <li class="bullet">If the Q value is high, then we can say that the action performed in that state is a good action. That is, if the Q value is high, meaning the expected return is high when we perform an action <em class="italic">a</em> in state <em class="italic">s</em>, we can say that the action <em class="italic">a</em> is a good action. </li>
      <li class="bullet">If the Q value is low, then we can say that the action performed in that state is not a good action. That is, if the Q value is low, meaning the expected return is low when we perform an action <em class="italic">a</em> in state <em class="italic">s</em>, we can say that the action <em class="italic">a</em> is not a good action.</li>
    </ul>
    <p class="normal">Okay, now how can the critic network evaluate an action produced by the actor network based on the Q function (Q value)? Let's suppose the actor network performs a <em class="italic">down</em> action in state <strong class="keyword">A</strong>. So, now, the critic computes the Q value of moving <em class="italic">down</em> in state <strong class="keyword">A</strong>. If the Q value is high, then the critic network gives feedback to the actor network that the action <em class="italic">down</em> is a good action in state <strong class="keyword">A</strong>. If the Q value is low, then the critic network gives feedback to the actor network that the <em class="italic">down</em> action is not a good action in state <strong class="keyword">A,</strong> and so the actor network tries to perform a different action in state <strong class="keyword">A</strong>. </p>
    <p class="normal">Thus, with the Q function, the critic network can evaluate the action performed by the actor network. But wait, how can the critic network learn the Q function? Because only if it knows the Q function can it evaluate the action performed by the actor. So, how does the <a id="_idIndexMarker1058"/>critic network learn the Q function? Here is where we use the <strong class="keyword">deep Q network</strong> (<strong class="keyword">DQN</strong>). We learned that with the DQN, we can use the neural network to approximate the Q function. So, now, we use the DQN as the critic network to compute the Q function. </p>
    <p class="normal">Thus, in a <a id="_idIndexMarker1059"/>nutshell, DDPG is an actor-critic method and so it takes advantage of policy-based and value-based methods. DDPG consists of an actor that is a policy network and uses the policy gradient method to learn the optimal policy and the critic, which is a deep Q network, and it evaluates the action produced by the actor.</p>
    <h2 id="_idParaDest-311" class="title">DDPG components</h2>
    <p class="normal">Now that <a id="_idIndexMarker1060"/>we have a basic understanding of how the DDPG algorithm works, let's go into further detail. We will understand how exactly the actor and critic networks work by looking at them separately.</p>
    <h3 id="_idParaDest-312" class="title">Critic network</h3>
    <p class="normal">We learned <a id="_idIndexMarker1061"/>that the critic network is basically the DQN and it uses the DQN to estimate the Q value. Now, let's learn how the critic network uses the DQN to estimate the Q value in more detail, along with a recap of the DQN.</p>
    <p class="normal">The critic evaluates the action produced by the actor. Thus, the input to the critic will be the state and also the action produced by the actor in that state, and the critic returns the Q value of the given state-action pair, as shown <em class="italic">Figure 12.1</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.1: The critic network</p>
    <p class="normal">To approximate the Q value in the critic, we can use the deep neural network, and if we use the deep neural network to approximate the Q value, then the network is called the DQN. Since we are using the neural network to approximate the Q value in <a id="_idIndexMarker1062"/>the critic, we can represent the Q function with <img src="../Images/B15558_12_005.png" alt="" style="height: 1.11em;"/>, where <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/> is the parameter of the network.</p>
    <p class="normal">Thus, in the critic network, we approximate the Q value using the DQN and the parameter of the critic network is represented by <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/>, as shown in <em class="italic">Figure 12.2</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.2: The critic network</p>
    <p class="normal">As we <a id="_idIndexMarker1063"/>can observe from <em class="italic">Figure 12.2</em>, given state <em class="italic">s</em> and the action <em class="italic">a</em> produced by the actor, the critic network returns the Q value. </p>
    <p class="normal">Now, let's look at how to obtain the action <em class="italic">a</em> produced by the actor. We learned that the actor is basically the policy network and it uses a policy gradient to learn the optimal policy. In DDPG, we learn a deterministic policy instead of a stochastic policy, so we can denote the policy with <img src="../Images/B15558_12_008.png" alt="" style="height: 0.84em;"/> instead of <img src="../Images/B15558_12_009.png" alt="" style="height: 0.84em;"/>. The parameter of the actor network is represented by <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/>. So, we can represent our parameterized policy as <img src="../Images/B15558_12_011.png" alt="" style="height: 1.02em;"/>.</p>
    <p class="normal">Given a state <em class="italic">s</em> as the input, the actor network returns the action <em class="italic">a</em> to be performed in that state:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_012.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Thus, the critic network takes state <em class="italic">s</em> and action <img src="../Images/B15558_12_012.png" alt="" style="height: 1.2em;"/> produced by the actor network in that state as input and returns the Q value, as shown in <em class="italic">Figure 12.3</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.3: The critic network</p>
    <p class="normal">Okay, how can <a id="_idIndexMarker1064"/>we train the critic network (DQN)? We generally train the network by minimizing the loss as the difference between the target value and predicted value. So, we can train the critic network by minimizing the loss as the difference between the target Q value and the Q value predicted by the network. But how can we obtain the target Q value? The target Q value is the optimal Q value and we can obtain the optimal Q value using the Bellman equation.</p>
    <p class="normal">We learned that the optimal Q function (Q value) can be obtained by using the Bellman optimality equation. Thus, the optimal Q function can be obtained using the Bellman optimality equation as follows:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_014.png" alt="" style="height: 1.84em;"/></figure>
    <p class="normal">We know that <img src="../Images/B15558_12_015.png" alt="" style="height: 1.2em;"/> represents the immediate reward <em class="italic">r</em> we obtain while performing an action <em class="italic">a</em> in state <em class="italic">s</em> and moving to the next state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/>, so we can just denote <img src="../Images/B15558_12_015.png" alt="" style="height: 1.2em;"/> with <em class="italic">r</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_018.png" alt="" style="height: 2.4em;"/></figure>
    <p class="normal">In the preceding equation, we can remove the expectation. We will approximate the expectation by sampling <em class="italic">K</em> number of transitions from the replay buffer and taking the average value. We will learn more about this in a while. So, we can express the target Q value as the sum of the immediate reward and discounted maximum Q value of the next state-action pair, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_019.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Thus, we <a id="_idIndexMarker1065"/>can represent the loss function of the critic network as the difference between the target value (optimal Bellman Q value) and the predicted value (the Q value predicted by the critic network):</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_020.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Here, the action <em class="italic">a</em> is the action produced by the actor network, that is,<img src="../Images/B15558_12_021.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Instead of using the loss as simply the difference between the target value and the predicted value, we can use the mean squared error as our loss function. We know that in the DQN, we use the replay buffer and store the transitions as <img src="../Images/B15558_12_022.png" alt="" style="height: 1.2em;"/>. So, we randomly sample a minibatch of <em class="italic">K</em> number of transitions from the replay buffer and train the network by minimizing the mean squared loss between the target value (optimal Bellman Q value) and the predicted value (Q value predicted by the critic network). Thus, our loss function is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_023.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">From the preceding equation, we can observe that both the target and predicted Q functions are parameterized by the same parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/>. This will cause instability in the mean squared error and the network will learn poorly.</p>
    <p class="normal">So, we introduce another neural network to learn the target value, and it is usually referred to as the target critic network. The parameter of the target critic network is represented by <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/>. Our main critic network, which is used to predict Q values, learns the correct parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/> using gradient descent. The target critic network parameter <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/> is updated <a id="_idIndexMarker1066"/>by just copying the parameter of the main critic network <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Thus, the loss function of the critic network can be written as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_029.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Remember that the action <em class="italic">a</em><sub class="" style="font-style: italic;">i</sub> in the preceding equation is the action produced by the actor network, that is, <img src="../Images/B15558_12_030.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">There is a small problem in the target value computation in our loss function due to the presence of the max term, as shown here: </p>
    <figure class="mediaobject"><img src="../Images/B15558_12_06.png" alt="" style="height:7em;"/></figure>
    <p class="normal">The max term means that we compute the Q value of all possible actions <img src="../Images/B15558_12_031.png" alt="" style="height: 1.2em;"/> in state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/> and select the action <img src="../Images/B15558_12_031.png" alt="" style="height: 1.2em;"/> as the one that has the maximum Q value. But when the action space is continuous, we cannot compute the Q value of all possible actions <img src="../Images/B15558_12_031.png" alt="" style="height: 1.2em;"/> in state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/>. So, we need to get rid of the max term in our loss function. How can we do that?</p>
    <p class="normal">Just as we <a id="_idIndexMarker1067"/>use the target network in the critic, we can use a target actor network, and the parameter of the target actor network is denoted by <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/>. Now, instead of selecting the action <img src="../Images/B15558_12_031.png" alt="" style="height: 1.2em;"/> as the one that has the maximum Q value, we can generate an action <img src="../Images/B15558_12_031.png" alt="" style="height: 1.2em;"/> using the target actor network, that is, <img src="../Images/B15558_12_039.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">Thus, as shown in <em class="italic">Figure 12.4</em>, to compute the Q value of the next state-action pair in the target, we feed state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/> and the action <img src="../Images/B15558_12_031.png" alt="" style="height: 1.2em;"/> produced by the target actor network parameterized by <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/> to the target critic network, and it returns the Q value of the next state-action pair:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.4: The target critic network</p>
    <p class="normal">Thus, in our loss function, equation (1), we can remove the max term and instead of <img src="../Images/B15558_12_031.png" alt="" style="height: 1.2em;"/>, we can write <img src="../Images/B15558_12_044.png" alt="" style="height: 1.29em;"/>, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_045.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">To maintain <a id="_idIndexMarker1068"/>a uniform notation, let's represent the loss function with <em class="italic">J</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_046.png" alt="" style="height: 2.96em;"/></figure>
    <p class="normal">To reduce the clutter, we can denote the target value with <em class="italic">y</em> and write:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_047.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <em class="italic">y</em><sub class="" style="font-style: italic;">i</sub> is the target value of the critic, that is, <img src="../Images/B15558_12_048.png" alt="" style="height: 1.76em;"/>, and the action <em class="italic">a</em><sub class="" style="font-style: italic;">i</sub> is the action produced by the main actor network, that is, <img src="../Images/B15558_12_030.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">To minimize the loss, we compute the gradients of the objective function <img src="../Images/B15558_12_050.png" alt="" style="height: 1.11em;"/> and update the main critic network parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/> by performing gradient descent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_052.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Okay, what about the target critic network parameter <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/>? How can we update it? We can update the parameter of the target critic network by just copying the parameter of the main <a id="_idIndexMarker1069"/>critic network parameter <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/> as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_055.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">This is usually called the soft replacement and the value of <img src="../Images/B15558_12_056.png" alt="" style="height: 0.84em;"/> is often set to 0.001.</p>
    <p class="normal">Thus, we learned how the critic network uses the DQN to compute the Q value to evaluate the action produced by the actor network. In the next section, we will learn how the actor network learns the optimal policy.</p>
    <h3 id="_idParaDest-313" class="title">Actor network</h3>
    <p class="normal">We have <a id="_idIndexMarker1070"/>already learned that the actor network is the policy network and it uses the policy gradient to compute the optimal policy. We also learned that we represent the parameter of the actor network with <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/>, and so the parameterized policy is represented with <img src="../Images/B15558_12_011.png" alt="" style="height: 1.02em;"/>.</p>
    <p class="normal">The actor network takes state <em class="italic">s</em> as an input and returns the action <em class="italic">a</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_059.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">One important point that we may want to note down here is that we are using a deterministic policy. Since we are using a deterministic policy, we need to take care of the exploration-exploitation dilemma, because we know that a deterministic policy always selects the same action and doesn't explore new actions, unlike a stochastic policy, which selects different actions based on the probability distribution over the action space. </p>
    <p class="normal">Okay, how can we explore new actions while using a deterministic policy? Note that DDPG is designed for an environment where the action space is continuous. Thus, we are using a deterministic policy in the continuous action space. </p>
    <p class="normal">Unlike the discrete action space, in the continuous action space, we have continuous values. So, to explore new actions, we can just add some noise <img src="../Images/B15558_12_060.png" alt="" style="height: 1.11em;"/> to the action produced by the actor network since the action is a continuous value. We generate this noise using <a id="_idIndexMarker1071"/>a process called the Ornstein-Uhlenbeck <a id="_idIndexMarker1072"/>random process. So, our modified action can be represented as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_061.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">For example, say the action <img src="../Images/B15558_12_062.png" alt="" style="height: 1.29em;"/> produced by the actor network is 13. Suppose the noise <img src="../Images/B15558_12_060.png" alt="" style="height: 1.11em;"/> is 0.1, then our action becomes <em class="italic">a</em> = 13+0.1 = 13.1.</p>
    <p class="normal">We learned that the critic network is represented by <img src="../Images/B15558_12_064.png" alt="" style="height: 1.11em;"/> and it evaluates the action produced by the actor using the Q value. If the Q value is high, then the critic tells the actor that it has produced a good action but when the Q value is low, then the critic tells the actor that it has produced a bad action.</p>
    <p class="normal">But wait! We learned that it is difficult to compute the Q value when the action space is continuous. That is, when the action space is continuous, it is difficult to compute the Q value of all possible actions in the state and take the maximum Q value. That is why we resorted to the policy gradient method. But now, we are computing the Q value with a continuous action space. How will this work?</p>
    <p class="normal">Note that, here in DDPG, we are not computing the Q value of all possible state-action pairs. We simply compute the Q value of state <em class="italic">s</em> and action <em class="italic">a</em> produced by the actor network. </p>
    <p class="normal">The goal of the actor is to make the critic tell that the action it has produced is a good action. That is, the actor wants to get good feedback from the critic network. When does the critic give good feedback to the actor? The critic gives good feedback when the action produced by the actor has a maximum Q value. That is, if the action produced by the actor has a maximum Q value, then the critic tells the actor that it has produced a good action. So, the actor tries to generate an action in such a way that it can maximize the Q value produced by the critic.</p>
    <p class="normal">Thus, the objective function of the actor is to generate an action that maximizes the Q value produced by the critic network. So, we can write the objective function of the actor as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_065.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Where the action <img src="../Images/B15558_12_066.png" alt="" style="height: 1.2em;"/>. Maximizing the above objective function <img src="../Images/B15558_12_067.png" alt="" style="height: 1.11em;"/> implies that we are maximizing the Q value produced by the critic network. Okay, how can we maximize the preceding objective function? We can maximize the objective function by performing gradient ascent and update the actor network parameter as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_068.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Wait. Instead of updating the actor network parameter <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/> just for a single state <img src="../Images/B15558_12_070.png" alt="" style="height: 0.93em;"/>, we sample <img src="../Images/B15558_12_071.png" alt="" style="height: 1.11em;"/> number of states from the replay buffer <img src="../Images/B15558_12_072.png" alt="" style="height: 1.11em;"/> and update the parameter. So, now our objective function becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_073.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where the action <img src="../Images/B15558_12_074.png" alt="" style="height: 1.29em;"/>. Maximizing the preceding objective function implies that the actor tries to generate actions in such a way that it maximizes the Q value over all the sampled states. We can maximize the objective function by performing gradient ascent and update the actor network parameter as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_068.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">To summarize, the objective of the actor is to generate action in such a way that it maximizes the Q value produced by the critic. So, we perform gradient ascent and update the actor network parameter.</p>
    <p class="normal">Okay, what <a id="_idIndexMarker1073"/>about the parameter of the target actor network? How can we update it? We can update the parameter of the target actor network by just copying the parameter of the main actor network parameter <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/> by soft replacement, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_077.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Now that we have understood how actor and critic networks work, let's get a good understanding of what we have learned so far and how DDPG works exactly by putting all the concepts together.</p>
    <h2 id="_idParaDest-314" class="title">Putting it all together</h2>
    <p class="normal">To avoid <a id="_idIndexMarker1074"/>getting lost in notations, first, let's recollect the notations to understand DDPG better. We use four networks, two actor networks and two critic networks:</p>
    <ul>
      <li class="bullet">The main critic network parameter is represented by <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet">The target critic network parameter is represented by <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/></li>
      <li class="bullet">The main actor network parameter is represented by <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet">The target actor network parameter is represented by <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/></li>
    </ul>
    <p class="normal">Note that DDPG is an actor-critic method, and so its parameters will be updated at every step of the episode, unlike the policy gradient method, where we generate complete episodes and then update the parameter. Okay, let's get started and understand how DDPG works.</p>
    <p class="normal">First, we initialize the main critic network parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/> and the main actor network parameter <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/> with random values. We learned that the target network parameter is just a copy of the main network parameter. So, we initialize the target critic network parameter <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/> by just copying the main critic network parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/>. Similarly, we initialize the <a id="_idIndexMarker1075"/>target actor network parameter <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/> by just copying the main actor network parameter <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/>. We also initialize the replay buffer <img src="../Images/B15558_12_072.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, for each step in the episode, first, we select an action, <em class="italic">a</em>, using the actor network:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_059.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">However, instead of using the action <em class="italic">a</em> directly, to ensure exploration, we add some noise <img src="../Images/B15558_12_060.png" alt="" style="height: 1.11em;"/>, and so the action becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_061.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Then, we perform the action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/>, and get the reward <em class="italic">r</em>. We store this transition information in a replay buffer <img src="../Images/B15558_12_072.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Next, we <a id="_idIndexMarker1076"/>randomly sample a minibatch of <em class="italic">K</em> transitions (<em class="italic">s</em>, <em class="italic">a</em>, <em class="italic">r</em>, <em class="italic">s'</em>) from the replay buffer. These <em class="italic">K</em> transitions will be used for updating both our critic and actor network.</p>
    <p class="normal">First, let us compute the loss of the critic network. We learned that the loss function of the critic network is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_047.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <em class="italic">y</em><sub class="" style="font-style: italic;">i</sub> is the target value of the critic, that is, <img src="../Images/B15558_12_095.png" alt="" style="height: 1.76em;"/>, and the action <em class="italic">a</em><sub class="" style="font-style: italic;">i</sub> is the action produced by the actor network, that is, <img src="../Images/B15558_12_074.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">After computing the loss of the critic network, we compute the gradients <img src="../Images/B15558_12_050.png" alt="" style="height: 1.11em;"/> and update the critic network parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/> using gradient descent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_099.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">Now, let us update the actor network. We learned that the objective function of the actor network is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_100.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Note that in the above equation, we are only using the state (<em class="italic">s</em><sub class="" style="font-style: italic;">i</sub>) from the sampled <em class="italic">K</em> transitions (<em class="italic">s</em>, <em class="italic">a</em>, <em class="italic">r</em>, <em class="italic">s'</em>). The action <em class="italic">a</em> is selected by actor network, <img src="../Images/B15558_12_074.png" alt="" style="height: 1.29em;"/>. Now, we need to maximize the preceding objective function. Maximizing the above objective function helps the actor to generate actions in such a way that it maximizes the Q value produced by the critic. We can maximize the objective function by computing the gradients of our objective function <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> and update the actor network parameter <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/> using gradient ascent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_103.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">And then, in the final step, we update the parameter of the target critic network <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/> and the <a id="_idIndexMarker1077"/>parameter of the target actor network <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/> by soft replacement:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_106.png" alt="" style="height: 1.2em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_12_107.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">We repeat these steps for several episodes. Thus, for each step in the episode, we update the parameter of our networks. Since the parameter gets updated at every step, our policy will also be improved at every step in the episode.</p>
    <p class="normal">To have a better understanding of how DDPG works, let's look into the DDPG algorithm in the next section. </p>
    <h2 id="_idParaDest-315" class="title">Algorithm – DDPG</h2>
    <p class="normal">The <a id="_idIndexMarker1078"/>DDPG algorithm is given as follows:</p>
    <ol>
      <li class="numbered">Initialize the main critic network parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/> and the main actor network parameter <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the target critic network parameter <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/> by just copying the main critic network parameter <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the target actor network parameter <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/> by just copying the main actor network parameter <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_12_072.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat steps 6 and 7</li>
      <li class="numbered">Initialize an Ornstein-Uhlenbeck random process <img src="../Images/B15558_12_060.png" alt="" style="height: 1.11em;"/> for an action space exploration</li>
      <li class="numbered">For <a id="_idIndexMarker1079"/>each step in the episode, that is, for <em class="italic">t</em> = 0,…,<em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Select action <em class="italic">a</em> based on the policy <img src="../Images/B15558_12_062.png" alt="" style="height: 1.29em;"/> and exploration noise, that is, <img src="../Images/B15558_12_117.png" alt="" style="height: 1.29em;"/>.</li>
          <li class="numbered-l2">Perform the selected action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_12_016.png" alt="" style="height: 1.2em;"/>, get the reward <em class="italic">r</em>, and store this transition information in the replay buffer <img src="../Images/B15558_12_072.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_12_088.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Compute the target value of the critic, that is, <img src="../Images/B15558_12_095.png" alt="" style="height: 1.76em;"/>.</li>
          <li class="numbered-l2">Compute the loss of the critic network, <img src="../Images/B15558_12_047.png" alt="" style="height: 2.87em;"/>.</li>
          <li class="numbered-l2">Compute the gradient of the loss <img src="../Images/B15558_12_050.png" alt="" style="height: 1.11em;"/> and update the critic network parameter using gradient descent, <img src="../Images/B15558_12_099.png" alt="" style="height: 1.11em;"/>.</li>
          <li class="numbered-l2">Compute the gradient of the actor network <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> and update the actor network parameter by gradient ascent, <img src="../Images/B15558_12_126.png" alt="" style="height: 1.2em;"/>.</li>
          <li class="numbered-l2">Update the target critic and target actor network parameter as <img src="../Images/B15558_12_127.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_107.png" alt="" style="height: 1.2em;"/>.</li>
        </ol>
      </li>
    </ol>
    <h2 id="_idParaDest-316" class="title">Swinging up a pendulum using DDPG</h2>
    <p class="normal">In this <a id="_idIndexMarker1080"/>section, let's implement the DDPG algorithm to train the agent to swing up a pendulum. That is, we will have a pendulum that starts swinging from a random position and the goal of our agent is to swing the pendulum up so it stays upright.</p>
    <p class="normal">First, let's import the required libraries:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> warnings
warnings.filterwarnings(<span class="hljs-string">'ignore'</span>)
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
tf.compat.v1.disable_v2_behavior() 
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> gym
</code></pre>
    <h3 id="_idParaDest-317" class="title">Creating the Gym environment</h3>
    <p class="normal">Let's <a id="_idIndexMarker1081"/>create a pendulum environment using Gym:</p>
    <pre class="programlisting code"><code class="hljs-code">env = gym.make(<span class="hljs-string">"Pendulum-v0"</span>).unwrapped
</code></pre>
    <p class="normal">Get the state shape of the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">state_shape = env.observation_space.shape[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">Get the action shape of the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">action_shape = env.action_space.shape[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">Note that the pendulum is a continuous environment, and thus our action space consists of continuous values. Hence, we get the bounds of our action space:</p>
    <pre class="programlisting code"><code class="hljs-code">action_bound = [env.action_space.low, env.action_space.high]
</code></pre>
    <h3 id="_idParaDest-318" class="title">Defining the variables</h3>
    <p class="normal">Now, let's <a id="_idIndexMarker1082"/>define some of the important variables.</p>
    <p class="normal">Set the discount factor, <img src="../Images/B15558_05_056.png" alt="" style="height: 0.93em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">gamma = <span class="hljs-number">0.9</span> 
</code></pre>
    <p class="normal">Set the value of <img src="../Images/B15558_12_056.png" alt="" style="height: 0.84em;"/>, which is used for soft replacement:</p>
    <pre class="programlisting code"><code class="hljs-code">tau = <span class="hljs-number">0.001</span>
</code></pre>
    <p class="normal">Set the size of our replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code">replay_buffer = <span class="hljs-number">10000</span> 
</code></pre>
    <p class="normal">Set the batch size:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">32</span> 
</code></pre>
    <h3 id="_idParaDest-319" class="title">Defining the DDPG class</h3>
    <p class="normal">Let's <a id="_idIndexMarker1083"/>define the class called <code class="Code-In-Text--PACKT-">DDPG</code>, where we will implement the DDPG algorithm. To aid understanding, let's look into the code line by line. You can also access the complete code from the GitHub repository of the book:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">DDPG</span><span class="hljs-class">(</span><span class="hljs-params">object</span><span class="hljs-class">):</span>
</code></pre>
    <h4 class="title">Defining the init method </h4>
    <p class="normal">First, let's <a id="_idIndexMarker1084"/>define the <code class="Code-In-Text--PACKT-">init</code> method:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, state_shape, action_shape, high_action_value,</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Define the replay buffer for storing the transitions:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.replay_buffer = np.zeros((replay_buffer, state_shape * <span class="hljs-number">2</span> + action_shape + <span class="hljs-number">1</span>), dtype=np.float32)
</code></pre>
    <p class="normal">Initialize <code class="Code-In-Text--PACKT-">num_transitions</code> to <code class="Code-In-Text--PACKT-">0</code>, which means that the number of transitions in our replay buffer is zero:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.num_transitions = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">Start the TensorFlow session:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess = tf.Session()
</code></pre>
    <p class="normal">We learned that in DDPG, instead of selecting the action <em class="italic">a</em> directly, to ensure exploration, we add some noise <img src="../Images/B15558_12_060.png" alt="" style="height: 1.11em;"/> using the Ornstein-Uhlenbeck process. So, we first initialize the noise:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.noise = <span class="hljs-number">3.0</span>
</code></pre>
    <p class="normal">Then, initialize the state shape, action shape, and high action value:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.state_shape, self.action_shape, self.high_action_value = state_shape, action_shape, high_action_value
</code></pre>
    <p class="normal">Define the placeholder for the state:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.state = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, state_shape], <span class="hljs-string">'state'</span>)
</code></pre>
    <p class="normal">Define the placeholder for the next state:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.next_state = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, state_shape], <span class="hljs-string">'next_state'</span>)
</code></pre>
    <p class="normal">Define the placeholder for the reward:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.reward = tf.placeholder(tf.float32, [<span class="hljs-literal">None</span>, <span class="hljs-number">1</span>], <span class="hljs-string">'reward'</span>)
</code></pre>
    <p class="normal">With the actor variable scope:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'Actor'</span>):
</code></pre>
    <p class="normal">Define <a id="_idIndexMarker1085"/>the main actor network, which is parameterized by <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/>. The actor network takes the state as an input and returns the action to be performed in that state:</p>
    <pre class="programlisting code"><code class="hljs-code">            self.actor = self.build_actor_network(self.state, scope=<span class="hljs-string">'main'</span>, trainable=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Define the target actor network that is parameterized by <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/>. The target actor network takes the next state as an input and returns the action to be performed in that state:</p>
    <pre class="programlisting code"><code class="hljs-code">            target_actor = self.build_actor_network(self.next_state, scope=<span class="hljs-string">'target'</span>, trainable=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">With the critic variable scope:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">with</span> tf.variable_scope(<span class="hljs-string">'Critic'</span>):
</code></pre>
    <p class="normal">Define the main critic network, which is parameterized by <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/>. The critic network takes the state and also the action produced by the actor in that state as an input and returns the Q value:</p>
    <pre class="programlisting code"><code class="hljs-code">            critic = self.build_critic_network(self.state, self.actor, scope=<span class="hljs-string">'main'</span>, trainable=<span class="hljs-literal">True</span>)
</code></pre>
    <p class="normal">Define the target critic network, which is parameterized by <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/>. The target critic network takes the next state and also the action produced by the target actor network in that next state as an input and returns the Q value:</p>
    <pre class="programlisting code"><code class="hljs-code">            target_critic = self.build_critic_network(self.next_state, target_actor, scope=<span class="hljs-string">'target'</span>, trainable=<span class="hljs-literal">False</span>)
</code></pre>
    <p class="normal">Get the parameter of the main actor network <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.main_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="hljs-string">'Actor/main'</span>)
</code></pre>
    <p class="normal">Get <a id="_idIndexMarker1086"/>the parameter of the target actor network <img src="../Images/B15558_12_042.png" alt="" style="height: 1.2em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.target_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="hljs-string">'Actor/target'</span>)
</code></pre>
    <p class="normal">Get the parameter of the main critic network <img src="../Images/B15558_12_006.png" alt="" style="height: 1.11em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.main_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="hljs-string">'Critic/main'</span>)
</code></pre>
    <p class="normal">Get the parameter of the target critic network <img src="../Images/B15558_12_025.png" alt="" style="height: 1.2em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.target_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=<span class="hljs-string">'Critic/target'</span>)
</code></pre>
    <p class="normal">Perform the soft replacement, update the parameter of the target actor network as <img src="../Images/B15558_12_107.png" alt="" style="height: 1.2em;"/>, and update the parameter of the target critic network as <img src="../Images/B15558_12_127.png" alt="" style="height: 1.2em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.soft_replacement = [
            [tf.assign(phi_, tau*phi + (<span class="hljs-number">1</span>-tau)*phi_), tf.assign(theta_, tau*theta + (<span class="hljs-number">1</span>-tau)*theta_)]
            <span class="hljs-keyword">for</span> phi, phi_, theta, theta_ <span class="hljs-keyword">in</span> zip(self.main_actor_params, self.target_actor_params, self.main_critic_params, self.target_critic_params)
            ]
</code></pre>
    <p class="normal">Compute the target Q value. We learned that the target Q value can be computed as the sum of reward and the discounted Q value of the next state-action pair, <img src="../Images/B15558_12_095.png" alt="" style="height: 1.76em;"/>:</p>
    <pre class="programlisting code"><code class="hljs-code">        y = self.reward + gamma * target_critic
</code></pre>
    <p class="normal">Now, let's <a id="_idIndexMarker1087"/>compute the loss of the critic network. The loss of the critic network is the mean squared error between the target Q value and the predicted Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_047.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">So, we can define the mean squared error as:</p>
    <pre class="programlisting code"><code class="hljs-code">        MSE = tf.losses.mean_squared_error(labels=y, predictions=critic)
</code></pre>
    <p class="normal">Train the critic network by minimizing the mean squared error using the Adam optimizer:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.train_critic = tf.train.AdamOptimizer(<span class="hljs-number">0.01</span>).minimize(MSE, name=<span class="hljs-string">"adam-ink"</span>, var_list = self.main_critic_params)
</code></pre>
    <p class="normal">We learned that the objective function of the actor is to generate an action that maximizes the Q value produced by the critic network, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_100.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where the action <img src="../Images/B15558_12_074.png" alt="" style="height: 1.29em;"/>, and we can maximize this objective by computing gradients and by performing gradient ascent. However, it is a standard convention to perform minimization rather than maximization. So, we can convert the preceding maximization objective into a minimization objective by just adding a negative sign. Hence, we can define the actor network objective as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_145.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Now, we <a id="_idIndexMarker1088"/>can minimize the actor network objective by computing gradients and by performing gradient descent. Thus, we can write:</p>
    <pre class="programlisting code"><code class="hljs-code">        actor_loss = -tf.reduce_mean(critic) 
</code></pre>
    <p class="normal">Train the actor network by minimizing the loss using the Adam optimizer:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.train_actor = tf.train.AdamOptimizer(<span class="hljs-number">0.001</span>).minimize(actor_loss, var_list=self.main_actor_params)
</code></pre>
    <p class="normal">Initialize all the TensorFlow variables:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess.run(tf.global_variables_initializer())
</code></pre>
    <h4 class="title">Selecting the action</h4>
    <p class="normal">Let's <a id="_idIndexMarker1089"/>define a function called <code class="Code-In-Text--PACKT-">select_action</code> to select the action with the noise to ensure exploration:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">select_action</span><span class="hljs-function">(</span><span class="hljs-params">self, state</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Run the actor network and get the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = self.sess.run(self.actor, {self.state: state[np.newaxis, :]})[<span class="hljs-number">0</span>]
</code></pre>
    <p class="normal">Now, we generate a normal distribution with the mean as the action and the standard deviation as the noise and we randomly select an action from this normal distribution:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = np.random.normal(action, self.noise)
</code></pre>
    <p class="normal">We need <a id="_idIndexMarker1090"/>to make sure that our action should not fall away from the action bound. So, we clip the action so that it lies within the action bound and then we return the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = np.clip(action, action_bound[<span class="hljs-number">0</span>],action_bound[<span class="hljs-number">1</span>])
        
        <span class="hljs-keyword">return</span> action
</code></pre>
    <h4 class="title">Defining the train function </h4>
    <p class="normal">Now, let's <a id="_idIndexMarker1091"/>define the train function:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">train</span><span class="hljs-function">(</span><span class="hljs-params">self</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">Perform the soft replacement:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess.run(self.soft_replacement)
</code></pre>
    <p class="normal">Randomly select indices from the replay buffer with the given batch size:</p>
    <pre class="programlisting code"><code class="hljs-code">        indices = np.random.choice(replay_buffer, size=batch_size)
</code></pre>
    <p class="normal">Select the batch of transitions from the replay buffer with the selected indices:</p>
    <pre class="programlisting code"><code class="hljs-code">        batch_transition = self.replay_buffer[indices, :]
</code></pre>
    <p class="normal">Get the batch of states, actions, rewards, and next states:</p>
    <pre class="programlisting code"><code class="hljs-code">        batch_states = batch_transition[:, :self.state_shape]
        batch_actions = batch_transition[:, self.state_shape: self.state_shape + self.action_shape]
        batch_rewards = batch_transition[:, -self.state_shape - <span class="hljs-number">1</span>: -self.state_shape]
        batch_next_state = batch_transition[:, -self.state_shape:]
</code></pre>
    <p class="normal">Train the actor network:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess.run(self.train_actor, {self.state: batch_states})
</code></pre>
    <p class="normal">Train <a id="_idIndexMarker1092"/>the critic network:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.sess.run(self.train_critic, {self.state: batch_states, self.actor: batch_actions, self.reward: batch_rewards, self.next_state: batch_next_state})
</code></pre>
    <h4 class="title">Storing the transitions </h4>
    <p class="normal">Now, let's <a id="_idIndexMarker1093"/>store the transitions in the replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">store_transition</span><span class="hljs-function">(</span><span class="hljs-params">self, state, actor, reward, next_state</span><span class="hljs-function">):</span>
</code></pre>
    <p class="normal">First, stack the state, action, reward, and next state:</p>
    <pre class="programlisting code"><code class="hljs-code">        trans = np.hstack((state,actor,[reward],next_state))
</code></pre>
    <p class="normal">Get the index:</p>
    <pre class="programlisting code"><code class="hljs-code">        index = self.num_transitions % replay_buffer
</code></pre>
    <p class="normal">Store the transition:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.replay_buffer[index, :] = trans
</code></pre>
    <p class="normal">Update the number of transitions:</p>
    <pre class="programlisting code"><code class="hljs-code">        self.num_transitions += <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">If the number of transitions is greater than the replay buffer, train the network:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> self.num_transitions &gt; replay_buffer:
            self.noise *= <span class="hljs-number">0.99995</span>
            self.train()
</code></pre>
    <h4 class="title">Building the actor network </h4>
    <p class="normal">We define <a id="_idIndexMarker1094"/>a function called <code class="Code-In-Text--PACKT-">build_actor_network</code> to build the actor network. The actor network takes the state and returns the action to be performed in that state:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_actor_network</span><span class="hljs-function">(</span><span class="hljs-params">self, state, scope, trainable</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">with</span> tf.variable_scope(scope):
            layer_1 = tf.layers.dense(state, <span class="hljs-number">30</span>, activation = tf.nn.tanh, name = <span class="hljs-string">'layer_1'</span>, trainable = trainable)
            actor = tf.layers.dense(layer_1, self.action_shape, activation = tf.nn.tanh, name = <span class="hljs-string">'actor'</span>, trainable = trainable) 
            <span class="hljs-keyword">return</span> tf.multiply(actor, self.high_action_value, name = <span class="hljs-string">"scaled_a"</span>)
</code></pre>
    <h4 class="title">Building the critic network</h4>
    <p class="normal">We define <a id="_idIndexMarker1095"/>a function called <code class="Code-In-Text--PACKT-">build_critic_network</code> to build the critic network. The critic network takes the state and the action produced by the actor in that state and returns the Q value:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">build_critic_network</span><span class="hljs-function">(</span><span class="hljs-params">self, state, actor, scope, trainable</span><span class="hljs-function">):</span>
        <span class="hljs-keyword">with</span> tf.variable_scope(scope):
            w1_s = tf.get_variable(<span class="hljs-string">'w1_s'</span>, [self.state_shape, <span class="hljs-number">30</span>], trainable = trainable)
            w1_a = tf.get_variable(<span class="hljs-string">'w1_a'</span>, [self.action_shape, <span class="hljs-number">30</span>], trainable = trainable)
            b1 = tf.get_variable(<span class="hljs-string">'b1'</span>, [<span class="hljs-number">1</span>, <span class="hljs-number">30</span>], trainable = trainable)
            net = tf.nn.tanh( tf.matmul(state, w1_s) + tf.matmul(actor, w1_a) + b1 )
            critic = tf.layers.dense(net, <span class="hljs-number">1</span>, trainable = trainable)
            <span class="hljs-keyword">return</span> critic
</code></pre>
    <h3 id="_idParaDest-320" class="title">Training the network </h3>
    <p class="normal">Now, let's <a id="_idIndexMarker1096"/>start training the network. First, let's create an object for our DDPG class:</p>
    <pre class="programlisting code"><code class="hljs-code">ddpg = DDPG(state_shape, action_shape, action_bound[<span class="hljs-number">1</span>])
</code></pre>
    <p class="normal">Set the number of episodes:</p>
    <pre class="programlisting code"><code class="hljs-code">num_episodes = <span class="hljs-number">300</span>
</code></pre>
    <p class="normal">Set the number of time steps in each episode:</p>
    <pre class="programlisting code"><code class="hljs-code">num_timesteps = <span class="hljs-number">500</span> 
</code></pre>
    <p class="normal">For each episode:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(num_episodes):
</code></pre>
    <p class="normal">Initialize the state by resetting the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">    state = env.reset()
</code></pre>
    <p class="normal">Initialize the return:</p>
    <pre class="programlisting code"><code class="hljs-code">    Return = <span class="hljs-number">0</span>
</code></pre>
    <p class="normal">For every step:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> range(num_timesteps):
</code></pre>
    <p class="normal">Render the environment:</p>
    <pre class="programlisting code"><code class="hljs-code">        env.render()
</code></pre>
    <p class="normal">Select <a id="_idIndexMarker1097"/>the action:</p>
    <pre class="programlisting code"><code class="hljs-code">        action = ddpg.select_action(state)
</code></pre>
    <p class="normal">Perform the selected action:</p>
    <pre class="programlisting code"><code class="hljs-code">        next_state, reward, done, info = env.step(action)
</code></pre>
    <p class="normal">Store the transition in the replay buffer:</p>
    <pre class="programlisting code"><code class="hljs-code">        ddpg.store_transition(state, action, reward, next_state)
</code></pre>
    <p class="normal">Update the return:</p>
    <pre class="programlisting code"><code class="hljs-code">        Return += reward
</code></pre>
    <p class="normal">If the state is the terminal state, then break:</p>
    <pre class="programlisting code"><code class="hljs-code">        <span class="hljs-keyword">if</span> done:
            <span class="hljs-keyword">break</span>
</code></pre>
    <p class="normal">Update the state to the next state:</p>
    <pre class="programlisting code"><code class="hljs-code">        state = next_state
</code></pre>
    <p class="normal">Print the return for every 10 episodes:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">if</span> i %<span class="hljs-number">10</span> ==<span class="hljs-number">0</span>:
         print(<span class="hljs-string">"Episode:{}, Return: {}"</span>.format(i,Return))
</code></pre>
    <p class="normal">By rendering the environment, we can observe how the agent learns to swing up the pendulum:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 12.5: The Gym pendulum environment</p>
    <p class="normal">Now that <a id="_idIndexMarker1098"/>we have learned how DDPG works and how to implement it, in the next section, we will learn about another interesting algorithm called twin delayed DDPG.</p>
    <h1 id="_idParaDest-321" class="title">Twin delayed DDPG </h1>
    <p class="normal">Now, we <a id="_idIndexMarker1099"/>will look into another interesting actor-critic algorithm, known as TD3. TD3 is an improvement (and basically a successor) to the DDPG algorithm we just covered.</p>
    <p class="normal">In the previous section, we learned how DDPG uses a deterministic policy to work on the continuous action space. DDPG has several advantages and has been successfully used in a variety of continuous action space environments. </p>
    <p class="normal">We understood that DDPG is an actor-critic method where an actor is a policy network and it finds the optimal policy, while the critic evaluates the policy produced by the actor by estimating the Q function using a DQN.</p>
    <p class="normal">One of the problems with DDPG is that the critic overestimates the target Q value. This overestimation causes several issues. We learned that the policy is improved based on the Q value given by the critic, but when the Q value has an approximation error, it causes stability issues to our policy and the policy may converge to local optima.</p>
    <p class="normal">Thus, to <a id="_idIndexMarker1100"/>combat this, TD3 proposes three important features, which are as follows:</p>
    <ol>
      <li class="numbered" value="1">Clipped double Q learning</li>
      <li class="numbered">Delayed policy updates</li>
      <li class="numbered">Target policy smoothing </li>
    </ol>
    <p class="normal">First, we will understand how TD3 works intuitively, and then we will look at the algorithm in detail.</p>
    <h2 id="_idParaDest-322" class="title">Key features of TD3</h2>
    <p class="normal">TD3 is <a id="_idIndexMarker1101"/>essentially the same as DDPG, except that it proposes three important features to mitigate the problems in DDPG. In this section, let's first get a basic understanding of the key features of TD3. The three key features of TD3 are:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Clipped double Q learning</strong>:<strong class="keyword"> </strong>Instead of using one critic network, we use two main critic networks <a id="_idIndexMarker1102"/>to compute the Q value and also use two target critic networks to compute the target value.<p class="bullet-para">We compute two target Q values using two target critic networks and use the minimum value of these two while computing the loss. This helps to prevent overestimation of the target Q value. We will learn more about this in detail in the next section. </p>
      </li>
      <li class="bullet"><strong class="keyword">Delayed policy updates</strong>: In <a id="_idIndexMarker1103"/>DDPG, we learned that we update the parameter of both the actor (policy network) and critic (DQN) network at every step of the episode. Unlike DDPG, here we delay updating the parameter of the actor network. <p class="bullet-para">That is, the critic network parameter is updated at every step of the episode, but the actor network (policy network) parameter is delayed and updated only after every two steps of the episode. </p>
      </li>
      <li class="bullet"><strong class="keyword">Target policy smoothing</strong>:<strong class="keyword"> </strong>The<strong class="keyword"> </strong><strong class="keyword"><a id="_idIndexMarker1104"/></strong>DDPG method produces different target values even for the same action. Hence, the variance of the target value will be high even for the same action, so we reduce this variance by adding some noise to the target action. <p class="bullet-para">Now that we have a basic idea of the key features of TD3, we will get into more detail and learn <a id="_idIndexMarker1105"/>how exactly these three key features work and how they solve the problems associated with DDPG.</p>
      </li>
    </ul>
    <h3 id="_idParaDest-323" class="title">Clipped double Q learning</h3>
    <p class="normal">Remember in <em class="chapterRef">Chapter 9</em>,<em class="italic"> Deep Q Network and Its Variants</em>, while learning about the DQN, we <a id="_idIndexMarker1106"/>discovered that it tends to overestimate the Q value of the next state-action pair in the target? It is shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_07.png" alt="" style="height:6em;"/></figure>
    <p class="normal">In order to mitigate the overestimation, we used double Q learning. With double Q learning, we use two different networks, in other words, two different Q functions, one for selecting an action and the other to compute the Q value, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_146.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Thus, computing the target value by using the preceding equation prevents the overestimation of the Q value in the DQN.</p>
    <p class="normal">We learned that in DDPG, the critic network is the DQN, and so it also suffers from the overestimation of the Q value in the target. Can we employ double Q learning in DDPG and try to solve the overestimation bias? Yes! But the problem is that in the actor-critic method, the policy and target network parameter updates happen slowly, and this will not help us in removing the overestimation bias. </p>
    <p class="normal">So, we will use a slightly different version of double Q learning called <em class="italic">clipped double Q learning</em>. In clipped double Q learning, we use two target critic networks to compute the Q value.</p>
    <p class="normal">We use the two target critic networks and compute the two Q values and select the minimum value out of these two to compute the target value. This helps to prevent overestimation bias. Let's understand this in more detail. </p>
    <p class="normal">If we need two target critic networks, then we also need two main critic networks. We know that the target network parameter is just a time-delayed copy of the main network parameter. So, we define two main critic networks with the parameters <img src="../Images/B15558_12_147.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_148.png" alt="" style="height: 1.11em;"/> to compute the two Q values, that is, <img src="../Images/B15558_12_149.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_150.png" alt="" style="height: 1.2em;"/>, respectively.</p>
    <p class="normal">We also <a id="_idIndexMarker1107"/>define the two target critic networks with parameters <img src="../Images/B15558_12_151.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_152.png" alt="" style="height: 1.2em;"/> to compute the two Q values of next state-action pair in the target, that is, <img src="../Images/B15558_12_153.png" alt="" style="height: 1.4em;"/> and <img src="../Images/B15558_12_154.png" alt="" style="height: 1.4em;"/>, respectively. Let's understand this clearly step by step.</p>
    <p class="normal">In DDPG, we learned that we compute the target value as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_155.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Computing the Q value of the next state-action pair in the target in this way creates overestimation bias:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_08.png" alt="" style="height:5em;"/></figure>
    <p class="normal">So, to avoid this, in TD3, first, we compute the Q value of the next state-action pair in the target using the first target critic network with a parameter <img src="../Images/B15558_12_151.png" alt="" style="height: 1.2em;"/>, that is, <img src="../Images/B15558_12_153.png" alt="" style="height: 1.4em;"/>, and then we compute the Q value of the next state-action pair in the target using the second target critic network with a parameter <img src="../Images/B15558_12_152.png" alt="" style="height: 1.2em;"/>, that is, <img src="../Images/B15558_12_154.png" alt="" style="height: 1.4em;"/>. Then, we use the minimum of these two Q values to compute the target value as expressed here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_160.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <a id="_idIndexMarker1108"/>the action <img src="../Images/B15558_12_161.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">We can express the preceding equation simply as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_162.png" alt="" style="height: 1.67em;"/></figure>
    <p class="normal">Where the action <img src="../Images/B15558_12_161.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">Computing the target value in this way prevents overestimation of the Q value of the next state-action pair.</p>
    <p class="normal">Okay, we computed the target value. How do we compute the loss and update the critic network parameter? We learned that we use two main critic networks, so, first, we compute the loss of the first main critic network, parameterized by <img src="../Images/B15558_12_147.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_165.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">After computing the loss, we compute the gradients and update the parameter <img src="../Images/B15558_12_147.png" alt="" style="height: 1.11em;"/><strong class="keyword"> </strong>using gradient descent as <img src="../Images/B15558_12_167.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Next, we compute the loss of the second main critic network, parameterized by <img src="../Images/B15558_12_148.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_169.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">After <a id="_idIndexMarker1109"/>computing the loss, we compute the gradients and update the parameter <img src="../Images/B15558_12_148.png" alt="" style="height: 1.11em;"/><strong class="keyword"> </strong>using gradient descent as <img src="../Images/B15558_12_171.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">We can simply express the preceding updates as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_172.png" alt="" style="height: 4.44em;"/></figure>
    <p class="normal">After updating the two main critic network parameters, <img src="../Images/B15558_12_147.png" alt="" style="height: 1.11em;"/><strong class="keyword"> </strong>and<strong class="keyword"> </strong><img src="../Images/B15558_12_148.png" alt="" style="height: 1.11em;"/>,<strong class="keyword"> </strong>we can update the two target critic network parameters, <img src="../Images/B15558_12_151.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_152.png" alt="" style="height: 1.2em;"/>, by soft replacement, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_177.png" alt="" style="height: 1.2em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_12_178.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">We <a id="_idIndexMarker1110"/>can simply express the preceding updates as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_179.png" alt="" style="height: 1.29em;"/></figure>
    <h3 id="_idParaDest-324" class="title">Delayed policy updates</h3>
    <p class="normal">Delayed policy updates imply that we update the parameters of our actor network (policy network) less frequently than the critic networks. But why do we want to do that? We <a id="_idIndexMarker1111"/>learned that in DDPG, actor and critic network parameters are updated at every step of the episode. </p>
    <p class="normal">When the critic network parameter is not good, then it estimates the incorrect Q values. If the Q value estimated by the critic network is not correct, then the actor network cannot update its parameter correctly. That is, we learned that the actor network learns based on feedback from the critic network. This feedback is just the Q value. When the critic network gives incorrect feedback (incorrect Q value), then the actor network cannot learn the correct action and cannot update its parameter correctly. </p>
    <p class="normal">Thus, to avoid this, we hold updating the parameter of the actor network for a while and only update the critic network to make the critic estimate the correct Q value. That is, we update the parameter of the critic network at every step of the episode, and we delay updating the parameter of the actor network, and only update it for some specific steps of the episode because we don't want our actor to learn from the incorrect critic's feedback.</p>
    <p class="normal">In a nutshell, the critic network parameter is updated at every step of the episode, but the actor network parameter update is delayed. We generally delay the update by two steps.</p>
    <p class="normal">Okay, in DDPG, we learned that the objective of the actor network (policy network) is to maximize the Q value:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_100.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">The preceding objective of the actor network is the same in TD3 as well. That is, similar to DDPG, here, the objective of the actor is to generate actions in such a way that it maximizes the Q value produced by the critic. But wait! Unlike DDPG, here we have two Q values, <img src="../Images/B15558_12_149.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_150.png" alt="" style="height: 1.2em;"/>, since we use two critic networks <a id="_idIndexMarker1112"/>with parameters <img src="../Images/B15558_12_147.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_148.png" alt="" style="height: 1.11em;"/>, respectively. So which Q value should our actor network maximize? Should it be <img src="../Images/B15558_12_149.png" alt="" style="height: 1.2em;"/> or <img src="../Images/B15558_12_150.png" alt="" style="height: 1.2em;"/>? We can take either of these and maximize one. So, we can take <img src="../Images/B15558_12_149.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Thus, in TD3, the objective of the actor network is to maximize the Q value, <img src="../Images/B15558_12_149.png" alt="" style="height: 1.2em;"/>, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_189.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Remember that in the above equation, the action <em class="italic">a</em> is selected by actor network, <img src="../Images/B15558_12_074.png" alt="" style="height: 1.29em;"/>. In order to maximize the objective function, we compute the gradients of our objective function, <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/>, and update the parameter of the network using gradient ascent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_103.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Now, instead of doing this parameter update of the actor network at every time step of the episode, we delay the updates and update the parameter only on every other step (every two steps). Let <em class="italic">t</em> be the time step of the episode and <em class="italic">d</em> denotes the number of time steps we <a id="_idIndexMarker1113"/>want to delay the update by (usually <em class="italic">d</em> is set to 2); then we can write the following: </p>
    <ol>
      <li class="numbered" value="1">If <em class="italic">t</em> mod <em class="italic">d</em> =0, then:<ol>
          <li class="numbered-l2">Compute the gradient of the objective function <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update the actor network parameter using gradient ascent <img src="../Images/B15558_12_103.png" alt="" style="height: 1.29em;"/></li>
        </ol>
      </li>
    </ol>
    <p class="normal">This will be made clearer when we look at the final algorithm.</p>
    <h3 id="_idParaDest-325" class="title">Target policy smoothing</h3>
    <p class="normal">To understand this, let's first recollect how we compute the target value in TD3. We learned <a id="_idIndexMarker1114"/>that in TD3, we update the target value using clipped double Q learning with two target critic networks:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_162.png" alt="" style="height: 1.67em;"/></figure>
    <p class="normal">Where the action <img src="../Images/B15558_12_161.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">As we can notice, we compute the target values with action <img src="../Images/B15558_12_031.png" alt="" style="height: 1.2em;"/> generated by the target actor network, <img src="../Images/B15558_12_199.png" alt="" style="height: 1.2em;"/>. Instead of using the action given by the target actor network directly, we add some noise <img src="../Images/B15558_12_200.png" alt="" style="height: 0.93em;"/> to the action and modify the action to <img src="../Images/B15558_12_201.png" alt="" style="height: 1.11em;"/>, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_202.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Here, −<em class="italic">c</em> to +<em class="italic">c</em> indicates that noise is clipped, so that we can keep the target close to the actual action. Thus, our target value computation now becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_203.png" alt="" style="height: 1.67em;"/></figure>
    <p class="normal">In the preceding equation, <a id="_idIndexMarker1115"/>the action <img src="../Images/B15558_12_204.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">But why are we doing this? Why do we need to add noise to the action and use it to compute the target value? Similar actions should have similar target values, right? However, the DDPG method produces target values with high variance even for similar actions. This is because deterministic policies overfit to the sharp peaks in the value estimate. So, we can smooth out these peaks for similar actions by adding some noise. Thus, target policy smoothing basically acts as a regularizer and reduces the variance in the target values.</p>
    <p class="normal">Now that we have understood the key features of the TD3 algorithm, let's get clarity on what we have learned so far and how the TD3 algorithm works by putting all the concepts together. </p>
    <h2 id="_idParaDest-326" class="title">Putting it all together</h2>
    <p class="normal">First, let's <a id="_idIndexMarker1116"/>recollect the notations to understand TD3 better. We use six networks—four critic networks and two actor networks:</p>
    <ul>
      <li class="bullet">The two main critic network parameters are represented by <img src="../Images/B15558_12_205.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_206.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet">The two target critic network parameters are represented by <img src="../Images/B15558_12_207.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_208.png" alt="" style="height: 1.2em;"/></li>
      <li class="bullet">The main actor network parameter is represented by <img src="../Images/B15558_12_209.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet">The target actor network parameter is represented by <img src="../Images/B15558_12_210.png" alt="" style="height: 1.2em;"/></li>
    </ul>
    <p class="normal">TD3 is an actor-critic method, and so the parameters of TD3 will get updated at every step of the episode, unlike the policy gradient method where we generate complete episodes and then update the parameter. Now, let's get started and understand how TD3 works.</p>
    <p class="normal">First, we initialize the two main critic network parameters, <img src="../Images/B15558_12_211.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_148.png" alt="" style="height: 1.11em;"/>, and the main actor network parameter <img src="../Images/B15558_12_213.png" alt="" style="height: 1.11em;"/> with random values. We know that the target network parameter is just a copy of the main network parameter. So, we initialize the two target critic network parameters <img src="../Images/B15558_12_214.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_208.png" alt="" style="height: 1.2em;"/> by just copying <img src="../Images/B15558_12_216.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/>, respectively. Similarly, we initialize the target actor network parameter <img src="../Images/B15558_12_218.png" alt="" style="height: 1.2em;"/> by just copying the main actor network parameter <img src="../Images/B15558_12_219.png" alt="" style="height: 1.11em;"/>. We also initialize the replay buffer <img src="../Images/B15558_12_220.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, for each step in the episode, first, we select an action <em class="italic">a</em> using the actor network:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_059.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">But <a id="_idIndexMarker1117"/>instead of using the action <em class="italic">a</em> directly, to ensure exploration, we add some noise <img src="../Images/B15558_12_222.png" alt="" style="height: 0.93em;"/>, where <img src="../Images/B15558_12_223.png" alt="" style="height: 1.11em;"/>. Thus, our action now becomes:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_224.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Then, we perform the action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_02_004.png" alt="" style="height: 1.2em;"/>, and get the reward <em class="italic">r</em>. We store this transition information in a replay buffer <img src="../Images/B15558_12_226.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Next, we randomly sample a minibatch of <em class="italic">K</em> transitions (<em class="italic">s</em>, <em class="italic">a</em>, <em class="italic">r</em>, <em class="italic">s'</em>) from the replay buffer. These <em class="italic">K</em> transitions will be used for updating both our critic and actor network.</p>
    <p class="normal">First, let us compute the loss of the critic networks. We learned that the loss function of the critic networks is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_227.png" alt="" style="height: 2.96em;"/></figure>
    <p class="normal">In the preceding equation, the following applies: </p>
    <ul>
      <li class="bullet">The action <em class="italic">a</em><sub class="" style="font-style: italic;">i</sub> is the action produced by the actor network, that is, <img src="../Images/B15558_12_228.png" alt="" style="height: 1.29em;"/></li>
      <li class="bullet"><em class="italic">y</em><sub class="" style="font-style: italic;">i</sub> is the target value of the critic, that is, <img src="../Images/B15558_12_230.png" alt="" style="height: 1.67em;"/>, and the action <img src="../Images/B15558_12_231.png" alt="" style="height: 1.11em;"/> is the action produced by the target actor network, that is, <img src="../Images/B15558_12_232.png" alt="" style="height: 1.29em;"/> where <img src="../Images/B15558_12_233.png" alt="" style="height: 1.11em;"/></li>
    </ul>
    <p class="normal">After <a id="_idIndexMarker1118"/>computing the loss of the critic network, we compute the gradients <img src="../Images/B15558_12_234.png" alt="" style="height: 1.29em;"/> and update the critic network parameter using gradient descent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_235.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">Now, let us update the actor network. We learned that the objective function of the actor network is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_189.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Note that in the above equation, we are only using the state (<em class="italic">s</em><sub class="" style="font-style: italic;">i</sub>) from the sampled <em class="italic">K</em> transitions (<em class="italic">s</em>, <em class="italic">a</em>, <em class="italic">r</em>, <em class="italic">s'</em>). The action <em class="italic">a</em> is selected by actor network, <img src="../Images/B15558_12_074.png" alt="" style="height: 1.29em;"/>. In order to maximize the objective function, we compute gradients of our objective function <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> and update the parameters of the network using gradient ascent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_103.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Instead of doing this parameter update of the actor network at every time step of the episode, we delay the updates. Let <em class="italic">t</em> be the time step of the episode and <em class="italic">d</em> denotes the number of time steps we want to delay the update by (usually <em class="italic">d</em> is set to 2); then we can write the following:</p>
    <ol>
      <li class="numbered" value="1">If <em class="italic">t</em> mod <em class="italic">d</em> = 0, then:<ol>
          <li class="numbered-l2">Compute the gradient of the objective function <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update the actor network parameter using gradient ascent <img src="../Images/B15558_12_103.png" alt="" style="height: 1.29em;"/></li>
        </ol>
      </li>
    </ol>
    <p class="normal">Finally, we update the parameter of the target critic networks <img src="../Images/B15558_12_214.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_208.png" alt="" style="height: 1.2em;"/> and the parameter of the target actor network <img src="../Images/B15558_12_243.png" alt="" style="height: 1.2em;"/> by soft replacement:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_244.png" alt="" style="height: 1.29em;"/></figure>
    <figure class="mediaobject"><img src="../Images/B15558_12_245.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">There <a id="_idIndexMarker1119"/>is a small change in updating the parameter of the target networks. Just like we delay updating the actor network parameter for <em class="italic">d</em> steps, we update the target network parameter for every <em class="italic">d</em> step; hence, we can write:</p>
    <ol>
      <li class="numbered" value="1">If <em class="italic">t</em> mod <em class="italic">d</em> = 0, then:<ol>
          <li class="numbered-l2">Compute the gradient of the objective function <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> and update the actor network parameter using gradient ascent <img src="../Images/B15558_12_247.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Update the target critic network parameter and target actor network parameter as <img src="../Images/B15558_12_248.png" alt="" style="height: 1.29em;"/>, and <img src="../Images/B15558_12_107.png" alt="" style="height: 1.2em;"/>, respectively</li>
        </ol>
      </li>
    </ol>
    <p class="normal">We <a id="_idIndexMarker1120"/>repeat the preceding steps for several episodes and improve the policy. To get a better understanding of how TD3 works, let's look into the TD3 algorithm in the next section. </p>
    <h2 id="_idParaDest-327" class="title">Algorithm – TD3</h2>
    <p class="normal">The TD3 <a id="_idIndexMarker1121"/>algorithm is exactly similar to the DDPG algorithm except that it includes the three key features we learned in the previous sections. So, before looking into the TD3 algorithm directly, you can revise all the key features of TD3.</p>
    <p class="normal">The algorithm of TD3 is given as follows: </p>
    <ol>
      <li class="numbered" value="1">Initialize the two main critic network parameters <img src="../Images/B15558_12_211.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/> and the main actor network parameter <img src="../Images/B15558_12_252.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the two target critic network parameters <img src="../Images/B15558_12_214.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_152.png" alt="" style="height: 1.2em;"/> by copying the main critic network parameters <img src="../Images/B15558_12_255.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_148.png" alt="" style="height: 1.11em;"/>, respectively</li>
      <li class="numbered">Initialize the target actor network parameter <img src="../Images/B15558_12_218.png" alt="" style="height: 1.2em;"/> by copying the main actor network parameter <img src="../Images/B15558_12_218.png" alt="" style="height: 1.2em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat step 6</li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0,…,<em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Select the action <em class="italic">a</em> based on the policy <img src="../Images/B15558_12_062.png" alt="" style="height: 1.29em;"/> and with exploration noise <img src="../Images/B15558_12_261.png" alt="" style="height: 0.93em;"/>, that is, <img src="../Images/B15558_12_224.png" alt="" style="height: 1.29em;"/> where, <img src="../Images/B15558_12_263.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Perform the selected action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_12_264.png" alt="" style="height: 1.2em;"/>, get the reward <em class="italic">r</em>, and store the transition information in the replay buffer <img src="../Images/B15558_09_088.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer <img src="../Images/B15558_12_266.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Select the action <img src="../Images/B15558_12_267.png" alt="" style="height: 1.11em;"/> to compute the target value, <img src="../Images/B15558_12_268.png" alt="" style="height: 1.29em;"/>, where <img src="../Images/B15558_12_269.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Compute the target value of the critic, that is, <img src="../Images/B15558_12_230.png" alt="" style="height: 1.67em;"/></li>
          <li class="numbered-l2">Compute the loss of the critic network, <img src="../Images/B15558_12_227.png" alt="" style="height: 2.96em;"/></li>
          <li class="numbered-l2">Compute the gradients of the loss <img src="../Images/B15558_12_234.png" alt="" style="height: 1.29em;"/> and minimize the loss using gradient descent, <img src="../Images/B15558_12_273.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">If <em class="italic">t</em> mod <em class="italic">d</em> =0, then:
        <ol>
          <li class="numbered-l2" value="1">Compute the gradient of the objective function <img src="../Images/B15558_12_274.png" alt="" style="height: 1.29em;"/> and update the actor network parameter using gradient ascent, <img src="../Images/B15558_12_126.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Update <a id="_idIndexMarker1122"/>the target critic network parameter and target actor network parameter as <img src="../Images/B15558_12_248.png" alt="" style="height: 1.29em;"/>, and <img src="../Images/B15558_12_107.png" alt="" style="height: 1.2em;"/>, respectively</li>
        </ol>
      </li>
    </ol>
  </li>
</ol>
    <p class="normal">Now <a id="_idIndexMarker1123"/>that we have learned how TD3 works, in the next section, we will learn about another interesting algorithm, called SAC. </p>
    <h1 id="_idParaDest-328" class="title">Soft actor-critic</h1>
    <p class="normal">Now, we will <a id="_idIndexMarker1124"/>look into another interesting actor-critic algorithm, called SAC. This is an off-policy algorithm and it borrows several features from the TD3 algorithm. But unlike TD3, it uses a stochastic policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/>. SAC is based on the concept of entropy. So first, let's understand what is meant by entropy. Entropy is a measure of the randomness of a variable. It basically tells us the uncertainty or unpredictability of the random variable and is denoted by <img src="../Images/B15558_12_279.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">If the random variable always gives the same value every time, then we can say that its entropy is low because there is no randomness. But if the random variable gives different values, then we can say that its entropy is high.</p>
    <p class="normal">For an example, consider a dice throw experiment. Every time a dice is thrown, if we get a different number, then we can say that the entropy is high because we are getting a different number every time and there is high uncertainty since we don't know which number will come up on the next throw. But if we are getting the same number, say 3, every time the dice is thrown, then we can say that the entropy is low, since there is no randomness here as we are getting the same number on every throw.</p>
    <p class="normal">We know that the policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/> tells what action to perform in a given state. What happens when the entropy of the policy <img src="../Images/B15558_12_281.png" alt="" style="height: 1.11em;"/> is high or low? If the entropy of the policy is high, then this means that our policy performs different actions instead of performing the same action every time. But if the entropy of the policy is low, then this means that our policy performs the same action every time. As you may have guessed, increasing the entropy of a policy promotes exploration, while decreasing the entropy of the policy means less exploration.</p>
    <p class="normal">We know <a id="_idIndexMarker1125"/>that, in reinforcement learning, our goal is to maximize the return. So, we can define our objective function as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_282.png" alt="" style="height: 1.4em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_12_283.png" alt="" style="height: 1.11em;"/> is the parameter of our stochastic policy <img src="../Images/B15558_03_139.png" alt="" style="height: 0.84em;"/>.</p>
    <p class="normal">We know that the return of the trajectory is just the sum of rewards, that is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_10_115.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">So, we can rewrite our objective function by expanding the return as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_286.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Maximizing the preceding objective function maximizes the return. In the SAC method, we use a slightly modified version of the objective function with the entropy term as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_287.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">As we can see, our objective function now has two terms; one is the reward and the other is the entropy of the policy. Thus, instead of maximizing only the reward, we also maximize the entropy of a policy. But what is the point of this? Maximizing the entropy of the policy allows us to explore new actions. But we don't want to explore actions that give us <a id="_idIndexMarker1126"/>a bad reward. Hence, maximizing entropy along with maximizing reward means that we can explore new actions along with maintaining maximum <a id="_idIndexMarker1127"/>reward. The preceding objective function is often referred to as <strong class="keyword">maximum entropy reinforcement learning</strong>, or <strong class="keyword">entropy regularized reinforcement learning.</strong> Adding an entropy term is also often referred <a id="_idIndexMarker1128"/>to as an entropy bonus.</p>
    <p class="normal">Also, the term <img src="../Images/B15558_09_143.png" alt="" style="height: 0.93em;"/> in the objective function is called temperature and is used to set the importance of our entropy term, or we can say that it is used to control exploration. When <img src="../Images/B15558_09_143.png" alt="" style="height: 0.93em;"/> is high, we allow exploration in the policy, but when it is low, then we don't allow exploration.</p>
    <p class="normal">Okay, now that we have a basic idea of SAC, let's get into some more details.</p>
    <h2 id="_idParaDest-329" class="title">Understanding soft actor-critic</h2>
    <p class="normal">SAC, as the <a id="_idIndexMarker1129"/>name suggests, is an actor-critic method similar to DDPG and TD3 we learned in the previous sections. Unlike DDPG and TD3, which use deterministic policies, SAC uses a stochastic policy. SAC works in a very similar manner to TD3. We learned that in actor-critic architecture, the actor uses the policy gradient to find the optimal policy and the critic evaluates the policy produced by the actor using the Q function.</p>
    <p class="normal">Similarly, in SAC, the actor uses the policy gradient to find the optimal policy and the critic evaluates the policy produced by the actor. However, instead of using only the Q function to <a id="_idIndexMarker1130"/>evaluate the actor's policy, the critic uses both the Q function and the value function. But why exactly do we need both the Q function and the value function to evaluate the actor's policy? This will be explained in detail in the upcoming sections. </p>
    <p class="normal">So, in SAC, we have three networks, one actor network (policy network) to find the optimal policy, and two critic networks—a value network and a Q network, to compute the value function and the Q function, respectively, to evaluate the policy produced by the actor. </p>
    <p class="normal">Before moving on, let's look at the modified version of the value function and the Q function with the entropy term.</p>
    <h3 id="_idParaDest-330" class="title">V and Q functions with the entropy term</h3>
    <p class="normal">We know <a id="_idIndexMarker1131"/>that the value <a id="_idIndexMarker1132"/>function (state value) is the expected return of the trajectory starting from state <em class="italic">s</em> following a policy <img src="../Images/B15558_03_008.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_289.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">We learned that the return is the sum of rewards of the trajectory, so we can rewrite the preceding equation by expanding the return as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_290.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Now, we can rewrite the value function by adding the entropy term as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_291.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">We know that the Q function (state-action value) is the expected return of the trajectory starting from state <em class="italic">s</em> and action <em class="italic">a</em> following a policy <img src="../Images/B15558_03_140.png" alt="" style="height: 0.84em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_293.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Expanding <a id="_idIndexMarker1133"/>the return of the trajectory, we can write the following:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_294.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">Now, we <a id="_idIndexMarker1134"/>can rewrite the Q function by adding the entropy term as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_295.png" alt="" style="height: 3.33em;"/></figure>
    <p class="normal">The modified Bellman equation for the preceding Q function with the entropy term is given as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_296.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Here, the value function can be computed using the relation between the Q function and the value function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_297.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">To learn <a id="_idIndexMarker1135"/>how exactly <a id="_idIndexMarker1136"/>we obtained the equations (2) and (3), you can check the derivation of soft policy iteration in the paper <em class="italic">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</em>, by Tuomas Haarnoja et.al.: <a href="https://arxiv.org/pdf/1801.01290.pdf"><span class="url">https://arxiv.org/pdf/1801.01290.pdf</span></a></p>
    <h2 id="_idParaDest-331" class="title">Components of SAC</h2>
    <p class="normal">Now that <a id="_idIndexMarker1137"/>we have a basic idea of SAC, let's go into more detail and understand how exactly each component of SAC works by looking at them separately.</p>
    <h3 id="_idParaDest-332" class="title">Critic network</h3>
    <p class="normal">We learned <a id="_idIndexMarker1138"/>that unlike other actor-critic methods we have seen earlier, the critic in SAC uses both the value function and the Q function to evaluate the policy produced by the actor network. But why is that? </p>
    <p class="normal">In the previous algorithms, we used the critic network to compute the Q function for evaluating the action produced by the actor. Also, the target Q value in the critic is computed using the Bellman equation. We can do the same here. However, here we have modified the Bellman equation of the Q function due to the entropy term, as we learned in equation (2):</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_298.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">From the preceding equation, we can observe that in order to compute the Q function, first we need to compute the value function. So, we need to compute both the Q function and the value function in order to evaluate the policy produced by the actor. We can use a single network to approximate both the Q function and the value function. However, instead of using a single network, we use two different networks, the Q network to estimate the Q function, and the value network to estimate the value function. Using two different networks to compute the Q function and value function stabilizes the training.</p>
    <p class="normal">As mentioned in the SAC paper, "<em class="italic">There is no need in principle to include a separate function approximator (neural network) for the state value since it is related to the Q function and policy according to Equation 2. But in practice, including a separate function approximator (neural network) for the state value can stabilize training and is convenient to train simultaneously with the other networks</em>."</p>
    <p class="normal">First, we will <a id="_idIndexMarker1139"/>learn how the value network works and then we will learn about the Q network. </p>
    <h4 class="title">Value network</h4>
    <p class="normal">The value <a id="_idIndexMarker1140"/>network is denoted by <em class="italic">V</em>, the parameter of the value network is denoted by <img src="../Images/B15558_12_299.png" alt="" style="height: 1.11em;"/>, and the parameter of the target value network is denoted by <img src="../Images/B15558_12_300.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Thus, <img src="../Images/B15558_12_301.png" alt="" style="height: 1.29em;"/> implies that we approximate the value function (state value) using the neural network parameterized by <img src="../Images/B15558_12_302.png" alt="" style="height: 1.11em;"/>. Okay, how can we train the value network? We can train the network by minimizing the loss between the target state value and the state value predicted by our network. How can we obtain the target state value? We can use the value function given in equation (3) to compute the target state value.</p>
    <p class="normal">We learned that according to equation (3), the value of the state is computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_303.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">In the preceding equation, we can remove the expectation. We will approximate the expectation by sampling <em class="italic">K</em> number of transitions from the replay buffer. So, we can compute the target state value <em class="italic">y</em><sub class="" style="font-style: italic;">v</sub> using the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_304.png" alt="" style="height: 1.11em;"/></figure>
    <p class="normal">If we look at the preceding equation, we have a Q function. In order to compute the Q function, we use a Q network parameterized by <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>, and similarly, our policy is parameterized by <img src="../Images/B15558_12_306.png" alt="" style="height: 1.11em;"/>, so we can rewrite the preceding equation with the parameterized Q function and policy as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_307.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">But if we use the preceding equation to compute the target value, the Q value will overestimate. So, to avoid this overestimation, we use clipped double Q learning, just like we <a id="_idIndexMarker1141"/>learned in TD3. That is, we compute the two Q values using two Q networks parameterized by <img src="../Images/B15558_12_211.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/> and take the minimum value of these two, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_310.png" alt="" style="height: 2.69em;"/></figure>
    <p class="normal">As we can observe in the preceding equation, for clipped double Q learning, we are using the two main Q networks parameterized by <img src="../Images/B15558_12_311.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/>, but in TD3, we used two target Q networks parameterized by <img src="../Images/B15558_12_214.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_152.png" alt="" style="height: 1.2em;"/>. Why is that?</p>
    <p class="normal">Because here, we are computing the Q value of a state-action pair <img src="../Images/B15558_12_315.png" alt="" style="height: 1.11em;"/> so we can use the two main Q networks parameterized by <img src="../Images/B15558_12_216.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/>, but in TD3, we compute the Q value of the next state-action pair <img src="../Images/B15558_05_091.png" alt="" style="height: 1.2em;"/>, so we used the two target Q networks parameterized by <img src="../Images/B15558_12_214.png" alt="" style="height: 1.2em;"/> and <img src="../Images/B15558_12_320.png" alt="" style="height: 1.2em;"/>. Thus, here, we don't need target Q networks.</p>
    <p class="normal">We can simply express the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_321.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">Now, we can define our objective function <img src="../Images/B15558_12_322.png" alt="" style="height: 1.11em;"/> of the value network as the mean squared <a id="_idIndexMarker1142"/>difference between the target state value and the state value predicted by our network, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_323.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <em class="italic">K</em> denotes the number of transitions we sample from the replay buffer.</p>
    <p class="normal">We can calculate the gradients of our objective function and then update our main value network parameter <img src="../Images/B15558_12_302.png" alt="" style="height: 1.11em;"/> as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_325.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Note that we are using <img src="../Images/B15558_12_326.png" alt="" style="height: 1.11em;"/> to represent the learning rate since we are already using <img src="../Images/B15558_07_025.png" alt="" style="height: 0.93em;"/> to denote the temperature.</p>
    <p class="normal">We can update the parameter of the target value network <img src="../Images/B15558_12_328.png" alt="" style="height: 1.2em;"/> using soft replacement:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_329.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">We will <a id="_idIndexMarker1143"/>learn where exactly the target value network is used in the next section.</p>
    <h4 class="title">Q network</h4>
    <p class="normal">The Q network is <a id="_idIndexMarker1144"/>denoted by <em class="italic">Q</em> and it is parameterized by <img src="../Images/B15558_12_330.png" alt="" style="height: 1.11em;"/>. Thus, <img src="../Images/B15558_12_331.png" alt="" style="height: 1.11em;"/> implies that we approximate the Q function using the neural network parameterized by <img src="../Images/B15558_09_098.png" alt="" style="height: 1.11em;"/>. How can we train the Q network? We can train the network by minimizing the loss between the target Q value and the Q value predicted by the network. How can we obtain the target Q value? Here is where we use the Bellman equation.</p>
    <p class="normal">We learned that according to the Bellman equation (2), the Q value can be computed as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_298.png" alt="" style="height: 1.58em;"/></figure>
    <p class="normal">We can remove the expectation in the preceding equation. We will approximate the expectation by sampling <em class="italic">K</em> number of transitions from the replay buffer. So, we can compute the target Q value <em class="italic">y</em><sub class="" style="font-style: italic;">q</sub> using the preceding equation as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_334.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">If we look at the preceding equation, we have a value of next state <img src="../Images/B15558_12_335.png" alt="" style="height: 1.2em;"/>. In order to compute the value of next state <img src="../Images/B15558_12_336.png" alt="" style="height: 1.2em;"/>, we use a target value network parameterized by <img src="../Images/B15558_12_337.png" alt="" style="height: 1.2em;"/>, so we can rewrite the preceding equation with the parameterized value function as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_338.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Now, we can define our objective function <img src="../Images/B15558_12_339.png" alt="" style="height: 1.11em;"/> of the Q network as the mean squared difference between the target Q value and the Q value predicted by the network, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_340.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <em class="italic">K</em> denotes the number of transitions we sample from the replay buffer.</p>
    <p class="normal">In the <a id="_idIndexMarker1145"/>previous section, we learned that we use two Q networks parameterized by <img src="../Images/B15558_12_216.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/> to prevent overestimation bias. So, first, we compute the loss of the first Q network, parameterized by <img src="../Images/B15558_12_211.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_344.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Then, we compute the gradients and update the parameter <img src="../Images/B15558_12_211.png" alt="" style="height: 1.11em;"/><strong class="keyword"> </strong>using gradient descent as <img src="../Images/B15558_12_346.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Next, we compute the loss of the second Q network, parameterized by <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/>:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_348.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Then, we <a id="_idIndexMarker1146"/>compute the gradients and update the parameter <img src="../Images/B15558_12_217.png" alt="" style="height: 1.11em;"/><strong class="keyword"> </strong>using gradient descent as <img src="../Images/B15558_12_350.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">We can simply express the preceding updates as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_351.png" alt="" style="height: 4.44em;"/></figure>
    <h3 id="_idParaDest-333" class="title">Actor network</h3>
    <p class="normal">The actor <a id="_idIndexMarker1147"/>network (policy network) is parameterized by <img src="../Images/B15558_11_043.png" alt="" style="height: 1.11em;"/>. Let's recall the objective function of the actor network we learned in TD3:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_100.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <em class="italic">a</em> is the action produced by the actor.</p>
    <p class="normal">The preceding objective function means that the goal of the actor is to generate action in such a way that it maximizes the Q value computed by the critic.</p>
    <p class="normal">The objective function of the actor network in SAC is the same as what we learned in TD3, except that here we use a stochastic policy <img src="../Images/B15558_12_413.png" alt="" style="height: 1.02em;"/>, and also, we maximize the entropy. So, we can write the objective function of the actor network in SAC as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_354.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Now, how can we compute the derivative of the preceding objective function? Because, unlike TD3, here, our action is computed using a stochastic policy. It will be difficult to apply backpropagation and compute gradients of the preceding objective function with the action computed using a stochastic policy. So, we use the reparameterization trick. The <a id="_idIndexMarker1148"/>reparameterization trick guarantees that sampling from our policy is differentiable. Thus, we can rewrite our action as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_355.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">In the preceding equation, we can observe that we parameterize the policy with a neural network <em class="italic">f</em> and <img src="../Images/B15558_12_200.png" alt="" style="height: 0.93em;"/> is the noise sampled from a spherical Gaussian distribution.</p>
    <p class="normal">Thus, we can rewrite our objective function as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_358.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Note that in the preceding equation, our action is <img src="../Images/B15558_12_355.png" alt="" style="height: 1.2em;"/>. Remember how we used two Q functions parameterized by <img src="../Images/B15558_12_359.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_360.png" alt="" style="height: 1.11em;"/> to avoid overestimation bias? Now, which Q function should we use in the preceding objective function? We can use either of the functions and so we use the Q function parameterized by <img src="../Images/B15558_12_216.png" alt="" style="height: 1.11em;"/> and write our final objective function as:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_362.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Now that <a id="_idIndexMarker1149"/>we have understood how the SAC algorithm works, let's recap what we have learned so far and how the SAC algorithm works exactly by putting all the concepts together. </p>
    <h2 id="_idParaDest-334" class="title">Putting it all together </h2>
    <p class="normal">First, let's <a id="_idIndexMarker1150"/>recall the notations to understand SAC better. We use five networks—four critic networks (two value networks and two Q networks) and one actor network:</p>
    <ul>
      <li class="bullet">The main value network parameter is represented by <img src="../Images/B15558_12_363.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet">The target value network parameter is represented by <img src="../Images/B15558_12_364.png" alt="" style="height: 1.2em;"/></li>
      <li class="bullet">The two main Q network parameters are represented by <img src="../Images/B15558_12_216.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_206.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet">The actor network (policy network) parameter is represented by <img src="../Images/B15558_10_152.png" alt="" style="height: 1.11em;"/></li>
      <li class="bullet">The target state value is represented by <em class="italic">y</em><sub class="" style="font-style: italic;">v</sub>, and the target Q value is represented by <em class="italic">y</em><sub class="" style="font-style: italic;">q</sub></li>
    </ul>
    <p class="normal">SAC is an actor-critic method, and so the parameters of SAC will get updated at every step of the episode. Now, let's get started and understand how SAC works.</p>
    <p class="normal">First, we initialize the main network parameter of the value network <img src="../Images/B15558_12_363.png" alt="" style="height: 1.11em;"/>, two Q network parameters <img src="../Images/B15558_12_216.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_206.png" alt="" style="height: 1.11em;"/>, and the actor network parameter <img src="../Images/B15558_12_371.png" alt="" style="height: 1.11em;"/>. Next, we initialize the target value network parameter <img src="../Images/B15558_12_364.png" alt="" style="height: 1.2em;"/> by just copying the main network parameter <img src="../Images/B15558_12_302.png" alt="" style="height: 1.11em;"/> and then we initialize the replay buffer <img src="../Images/B15558_12_374.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Now, for each step in the episode, first, we select an action <em class="italic">a</em> using the actor network:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_375.png" alt="" style="height: 1.29em;"/></figure>
    <p class="normal">Then, we <a id="_idIndexMarker1151"/>perform the action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_12_376.png" alt="" style="height: 1.2em;"/>, and get the reward <em class="italic">r</em>. We store this transition information in a replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/>.</p>
    <p class="normal">Next, we randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer. These <em class="italic">K</em> transitions (<em class="italic">s</em>, <em class="italic">a</em>, <em class="italic">r</em>, <em class="italic">s'</em>) are used for updating our value, Q, and actor network.</p>
    <p class="normal">First, let us compute the loss of the value network. We learned that the loss function of the value network is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_323.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_12_379.png" alt="" style="height: 1.02em;"/> is the target state value and it is given as <img src="../Images/B15558_12_380.png" alt="" style="height: 1.58em;"/>.</p>
    <p class="normal">After computing the loss, we calculate the gradients and update the parameter <img src="../Images/B15558_12_302.png" alt="" style="height: 1.11em;"/> of the value network using gradient descent: <img src="../Images/B15558_12_325.png" alt="" style="height: 1.2em;"/>.</p>
    <p class="normal">Now, we compute the loss of the Q networks. We learned that the loss function of the Q network is: </p>
    <figure class="mediaobject"><img src="../Images/B15558_12_383.png" alt="" style="height: 2.96em;"/></figure>
    <p class="normal">Where <img src="../Images/B15558_12_384.png" alt="" style="height: 1.02em;"/> is the target Q value and it is given as <img src="../Images/B15558_12_385.png" alt="" style="height: 1.29em;"/>.</p>
    <p class="normal">After <a id="_idIndexMarker1152"/>computing the loss, we calculate the gradients and update the parameter of the Q networks using gradient descent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_386.png" alt="" style="height: 1.49em;"/></figure>
    <p class="normal">Next, we update the actor network. We learned that the objective of the actor network is:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_362.png" alt="" style="height: 2.87em;"/></figure>
    <p class="normal">Now, we calculate gradients and update the parameter <img src="../Images/B15558_12_283.png" alt="" style="height: 1.11em;"/> of the actor network using gradient ascent:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_389.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">Finally, in the end, we update the target value network parameter by soft replacement, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15558_12_390.png" alt="" style="height: 1.2em;"/></figure>
    <p class="normal">We repeat <a id="_idIndexMarker1153"/>the preceding steps for several episodes and improve the policy. To get a better understanding of how SAC works, let's look into the SAC algorithm in the next section. </p>
    <h2 id="_idParaDest-335" class="title">Algorithm – SAC</h2>
    <p class="normal">The SAC <a id="_idIndexMarker1154"/>algorithm is given as follows: </p>
    <ol>
      <li class="numbered" value="1">Initialize the main value network parameter <img src="../Images/B15558_12_302.png" alt="" style="height: 1.11em;"/>, the Q network parameters <img src="../Images/B15558_12_211.png" alt="" style="height: 1.11em;"/> and <img src="../Images/B15558_12_206.png" alt="" style="height: 1.11em;"/>, and the actor network parameter <img src="../Images/B15558_12_371.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the target value network <img src="../Images/B15558_12_395.png" alt="" style="height: 1.2em;"/> by just copying the main value network parameter <img src="../Images/B15558_12_302.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">Initialize the replay buffer <img src="../Images/B15558_12_259.png" alt="" style="height: 1.11em;"/></li>
      <li class="numbered">For <em class="italic">N</em> number of episodes, repeat step 5</li>
      <li class="numbered">For each step in the episode, that is, for <em class="italic">t</em> = 0,…, <em class="italic">T</em> – 1:<ol>
          <li class="numbered-l2">Select an action <em class="italic">a</em> based on the policy <img src="../Images/B15558_12_398.png" alt="" style="height: 1.29em;"/>, that is, <img src="../Images/B15558_12_399.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Perform the selected action <em class="italic">a</em>, move to the next state <img src="../Images/B15558_12_376.png" alt="" style="height: 1.2em;"/>, get the reward <em class="italic">r</em>, and store the transition information in the replay buffer <img src="../Images/B15558_09_124.png" alt="" style="height: 1.11em;"/></li>
          <li class="numbered-l2">Randomly sample a minibatch of <em class="italic">K</em> transitions from the replay buffer</li>
          <li class="numbered-l2">Compute the target state value <img src="../Images/B15558_12_380.png" alt="" style="height: 1.58em;"/></li>
          <li class="numbered-l2">Compute the loss of value network <img src="../Images/B15558_12_323.png" alt="" style="height: 2.87em;"/> and update the parameter using gradient descent, <img src="../Images/B15558_12_325.png" alt="" style="height: 1.2em;"/></li>
          <li class="numbered-l2">Compute the target Q value <img src="../Images/B15558_12_385.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Compute the loss of the Q networks <img src="../Images/B15558_12_383.png" alt="" style="height: 2.96em;"/> and update the parameter using gradient descent, <img src="../Images/B15558_12_386.png" alt="" style="height: 1.49em;"/></li>
          <li class="numbered-l2">Compute gradients of the actor objective function, <img src="../Images/B15558_10_093.png" alt="" style="height: 1.2em;"/> and update the parameter using gradient ascent, <img src="../Images/B15558_12_409.png" alt="" style="height: 1.29em;"/></li>
          <li class="numbered-l2">Update <a id="_idIndexMarker1155"/>the target value network parameter as <img src="../Images/B15558_12_410.png" alt="" style="height: 1.2em;"/></li>
        </ol>
      </li>
    </ol>
    <p class="normal">Many congratulations on learning the several important state-of-the-art actor-critic algorithms, including DDPG, twin delayed DDPG, and SAC. In the next chapter, we will examine several state-of-the-art policy gradient algorithms. </p>
    <h1 id="_idParaDest-336" class="title">Summary</h1>
    <p class="normal">We started off the chapter by understanding the DDPG algorithm. We learned that DDPG is an actor-critic algorithm where the actor estimates the policy using policy gradient and the critic evaluates the policy produced by the actor using the Q function. We learned how DDPG uses a deterministic policy and how it is used in environments with a continuous action space.</p>
    <p class="normal">Later, we looked into the actor and critic components of DDPG in detail and understood how they work, before finally learning about the DDPG algorithm.</p>
    <p class="normal">Moving on, we learned about the twin delayed DDPG, which is the successor to DDPG and constitutes an improvement to the DDPG algorithm. We learned the key features of TD3, including clipped double Q learning, delayed policy updates, and target policy smoothing, in detail and finally, we looked into the TD3 algorithm.</p>
    <p class="normal">At the end of the chapter, we learned about the SAC algorithm. We learned that, unlike DDPG and TD3, the SAC method uses a stochastic policy. We also understood how SAC works with the entropy bonus in the objective function, and we learned what is meant by maximum entropy reinforcement learning.</p>
    <p class="normal">In the next chapter, we will learn the state-of-the-art policy gradient algorithms such as trust region policy optimization, proximal policy optimization, and actor-critic using Kronecker-factored trust region.</p>
    <h1 id="_idParaDest-337" class="title">Questions</h1>
    <p class="normal">Let's put our knowledge of actor-critic methods to the test. Try answering the following questions:</p>
    <ol>
      <li class="numbered" value="1">What is the role of actor and critic networks in DDPG?</li>
      <li class="numbered">How does the critic in DDPG work?</li>
      <li class="numbered">What are the key features of TD3?</li>
      <li class="numbered">Why do we need clipped double Q learning?</li>
      <li class="numbered">What is target policy smoothing?</li>
      <li class="numbered">What is maximum entropy reinforcement learning?</li>
      <li class="numbered">What is the role of the critic network in SAC? </li>
    </ol>
    <h1 id="_idParaDest-338" class="title">Further reading</h1>
    <p class="normal">For more information, refer to the following papers:</p>
    <ul>
      <li class="bullet"><strong class="keyword">Continuous Control with Deep Reinforcement Learning</strong> by <em class="italic">Timothy P. Lillicrap, et al.</em>, <a href="https://arxiv.org/pdf/1509.02971.pdf"><span class="url">https://arxiv.org/pdf/1509.02971.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Addressing Function Approximation Error in Actor-Critic Methods </strong>by <em class="italic">Scott Fujimoto, Herke van Hoof, David Meger, </em><a href="https://arxiv.org/pdf/1802.09477.pdf"><span class="url">https://arxiv.org/pdf/1802.09477.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor</strong> by <em class="italic">Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine</em>, <a href="https://arxiv.org/pdf/1801.01290.pdf"><span class="url">https://arxiv.org/pdf/1801.01290.pdf</span></a></li>
    </ul>
  </div>
</body></html>