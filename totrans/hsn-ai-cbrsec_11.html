<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">GANs - Attacks and Defenses</span></h1>
                </header>
            
            <article>
                
<p><strong><span class="koboSpan" id="kobo.2.1">Generative adversarial networks</span></strong><span class="koboSpan" id="kobo.3.1"> (</span><strong><span class="koboSpan" id="kobo.4.1">GANs</span></strong><span class="koboSpan" id="kobo.5.1">) represent the most advanced example of neural networks that deep learning makes available to us in the context of cybersecurity. </span><span class="koboSpan" id="kobo.5.2">GANs can be used for legitimate purposes, such as authentication procedures, but they can also be exploited to violate these procedures.Â </span></p>
<p><span class="koboSpan" id="kobo.6.1">In this chapter, we will look at the following topics:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.7.1">The fundamental concepts of GANs and their use in attack and defense scenarios</span></li>
<li><span class="koboSpan" id="kobo.8.1">The main libraries and tools for developing adversarial examples</span></li>
<li><span class="koboSpan" id="kobo.9.1">Attacks against </span><strong><span class="koboSpan" id="kobo.10.1">deep neural networks</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong><span class="koboSpan" id="kobo.12.1">DNNs</span></strong><span class="koboSpan" id="kobo.13.1">) via model substitution</span></li>
<li><span class="koboSpan" id="kobo.14.1">Attacks against </span><strong><span class="koboSpan" id="kobo.15.1">intrusion detection systems</span></strong><span class="koboSpan" id="kobo.16.1"> (</span><strong><span class="koboSpan" id="kobo.17.1">IDS</span></strong><span class="koboSpan" id="kobo.18.1">) via GANs</span></li>
<li><span class="koboSpan" id="kobo.19.1">Attacks against facial recognition procedures using adversarial examples</span></li>
</ul>
<p><span class="koboSpan" id="kobo.20.1">We will now begin the chapter by introducing the basic concepts of GANs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">GANs in a nutshell</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">GANs were theorized in a famous paper that dates back to 2014 (</span><a href="https://arxiv.org/abs/1406.2661"><span class="koboSpan" id="kobo.3.1">https://arxiv.org/abs/1406.2661</span></a><span class="koboSpan" id="kobo.4.1">), written by a team of researchers including Ian Goodfellow and Yoshua Bengio, which described the potential and characteristics of a special category of adversarial processes, called GANs.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The basic idea behind GANs is simple, as they consist of putting two neural networks in competition with one another, until a balanced condition of results is achieved; however at the same time, the possibilities of using these intuitions are almost unlimited, since GANs are able to learn how to imitate and artificially reproduce any data distribution, whether it represents faces, voices, texts, or even works of art.</span></p>
<p><span class="koboSpan" id="kobo.6.1">In this chapter, we will extend the use of GANs in the field of cybersecurity, learning how it is possible to use them to both carry out attacks (such as attacks against security procedures based on the recognition of biometric evidences) and to defend neural networks from attacks conducted through GANs. </span><span class="koboSpan" id="kobo.6.2">In order to fully understand the characteristics and potential of GANs, we need to introduce a number of fundamental concepts concerning </span><strong><span class="koboSpan" id="kobo.7.1">neural networks</span></strong><span class="koboSpan" id="kobo.8.1"> (</span><strong><span class="koboSpan" id="kobo.9.1">NNs</span></strong><span class="koboSpan" id="kobo.10.1">) and </span><strong><span class="koboSpan" id="kobo.11.1">deep learning</span></strong><span class="koboSpan" id="kobo.12.1"> (</span><strong><span class="koboSpan" id="kobo.13.1">DL</span></strong><span class="koboSpan" id="kobo.14.1">).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">A glimpse into deep learning</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have already encountered NNs in </span><a href="3311e837-18a2-4a50-8322-f7b9c12bcbc8.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 4</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Malware Threat Detection</span></em><span class="koboSpan" id="kobo.6.1">, and </span><a href="f467340a-244d-4714-8a39-68b230db2404.xhtml"><span class="koboSpan" id="kobo.7.1">Chapter 6</span></a><span class="koboSpan" id="kobo.8.1">, </span><em><span class="koboSpan" id="kobo.9.1">Securing User Authentication</span></em><span class="koboSpan" id="kobo.10.1">, and now, we will extend the topic further by treating DL in a more systematic way. </span><span class="koboSpan" id="kobo.10.2">DL is a branch of </span><strong><span class="koboSpan" id="kobo.11.1">machine learning</span></strong><span class="koboSpan" id="kobo.12.1"> (</span><strong><span class="koboSpan" id="kobo.13.1">ML</span></strong><span class="koboSpan" id="kobo.14.1">) that aims to emulate the cognitive abilities of the human brain in an attempt to perform those typically higher-level human tasks characterized by high complexity, such as facial recognition and speech recognition. </span><span class="koboSpan" id="kobo.14.2">DL therefore seeks to emulate the behavior of the human brain by introducing networks based on artificial neurons that are stratified on multiple levels and connected to one another, and that are characterized by a more or less high degree of depth which is where the </span><strong><span class="koboSpan" id="kobo.15.1">deep</span></strong><span class="koboSpan" id="kobo.16.1"> adjective in the phrase deep learning has its origins.</span></p>
<p><span class="koboSpan" id="kobo.17.1">The concepts of DL and NNs are not new, but only in recent years have they found concrete practical, as well as theoretical, application, thanks to the progress achieved in the field of digital architectures, which have benefited from increased computational capacity, as well as the possibility of fully exploiting distributed computing through cloud computing, together with the almost unlimited availability of training data made possible by big data analytics.</span></p>
<p><span class="koboSpan" id="kobo.18.1">The potential of DL has been recognized not only in the research and business sector, but also in the field of cybersecurity, where it is increasingly essential to use solutions capable of dynamically adapting to changes in context, adopting not only static detection tools, but algorithms that are able to dynamically learn how to recognize new types of attacks autonomously, finding possible threats by analyzing the most representative features within the often noisy datasets.</span></p>
<p><span class="koboSpan" id="kobo.19.1">Compared to traditional ML, DL is also characterized by a greater complexity from a mathematical point of view, especially regarding its widespread use of calculus and linear algebra. </span><span class="koboSpan" id="kobo.19.2">However, compared to ML, DL is able to achieve much better results in terms of accuracy and the potential reuse of algorithms in different application sectors.</span></p>
<p><span class="koboSpan" id="kobo.20.1">Through the use of layers of NNs that are connected to one another, DL does not limit itself to analyzing the features of the original datasets, but is also able to recombine them by creating new ones, thereby adapting to the complexity of the analysis that is to be conducted.</span></p>
<p><span class="koboSpan" id="kobo.21.1">The layers of artificial neurons that constitute DL analyze the data and features received as input and share them with the various inner layers, and these, in turn, process the output data of the outer layers. </span><span class="koboSpan" id="kobo.21.2">In this way, the original features extracted from the datasets are recombined, giving rise to new features that are optimized for analysis.</span></p>
<p><span class="koboSpan" id="kobo.22.1">The greater the number of internal layers that are interconnected, the greater the depth and ability to recombine the features and adapt to the complexity of the problem, thereby reducing it to more specific and more manageable subtasks.</span></p>
<p><span class="koboSpan" id="kobo.23.1">We have already mentioned that the constitutive elements of DL are the layers of NNs composed of artificial neurons. </span><span class="koboSpan" id="kobo.23.2">Now, we will examine the characteristics of these constituent elements in </span><span><span class="koboSpan" id="kobo.24.1">greater detail</span></span><span class="koboSpan" id="kobo.25.1">, starting with artificial neurons.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Artificial neurons and activation functions</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have already encountered (in </span><a href="aaf59353-00b3-4625-8732-63aad02cc8e5.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 3</span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Ham or Spam? </span><span class="koboSpan" id="kobo.5.2">Detecting Email Cybersecurity Threats with AI</span></em><span class="koboSpan" id="kobo.6.1">) a particular type of artificial neuron, Rosenblatt's Perceptron, and we have seen that this artificial neuron emulates the behavior of neurons in the human brain by activating itself in the presence of a positive signal beyond a threshold.</span></p>
<p><span class="koboSpan" id="kobo.7.1">To verify the presence of a positive signal beyond a threshold, a special function is used, known as the </span><strong><span class="koboSpan" id="kobo.8.1">activation</span></strong><span class="koboSpan" id="kobo.9.1"> function, which, in the case of a Perceptron, has the following characteristics:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.10.1"><img class="fm-editor-equation" src="assets/893080af-61b0-4cd9-9b91-d306614030ee.png" style="width:46.67em;height:3.00em;"/></span></p>
<p><span class="koboSpan" id="kobo.11.1">In practice, if the product of the </span><em><span class="koboSpan" id="kobo.12.1">wx</span></em><span class="koboSpan" id="kobo.13.1"> valuesâconsisting of the input data multiplied by the corresponding weights</span><span><span class="koboSpan" id="kobo.14.1">â</span></span><span class="koboSpan" id="kobo.15.1">exceeds a certain threshold </span><span class="koboSpan" id="kobo.16.1"><img class="fm-editor-equation" src="assets/ab446c36-e490-4d55-b41c-08b080a54fef.png" style="width:0.50em;height:0.92em;"/></span><span class="koboSpan" id="kobo.17.1">, then the Perceptron is activated; otherwise, it remains inert. </span><span class="koboSpan" id="kobo.17.2">Therefore, the task of the activation function is precisely to activate or not activate the artificial neuron following the verification of certain conditions.</span></p>
<p><span class="koboSpan" id="kobo.18.1">Different types of activation functions are possible, but perhaps the most common is the </span><strong><span class="koboSpan" id="kobo.19.1">rectified linear unit</span></strong><span class="koboSpan" id="kobo.20.1"> (</span><strong><span class="koboSpan" id="kobo.21.1">ReLU</span></strong><span class="koboSpan" id="kobo.22.1">), which, in its simplest version, entails assuming, as the activation value, the result obtained by applying the function </span><em><span class="koboSpan" id="kobo.23.1">max(0, wx)</span></em><span class="koboSpan" id="kobo.24.1"> to the input values (multiplied by the respective weights).</span></p>
<p><span class="koboSpan" id="kobo.25.1">In formal terms, this can be expressed as the following equation:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.26.1"><img class="fm-editor-equation" src="assets/6d82dfa4-04d7-4444-b1e5-8140e51eb28a.png" style="width:25.50em;height:1.42em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.27.1">There is also a variant known as </span><em><span class="koboSpan" id="kobo.28.1">LeakyReLU,</span></em><span class="koboSpan" id="kobo.29.1"> as shown in the following equation:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.30.1"><img class="fm-editor-equation" src="assets/a7f0552e-90c4-4272-ac4e-d4f70f2b0947.png" style="width:26.50em;height:1.25em;"/></span></p>
<p><span class="koboSpan" id="kobo.31.1">Unlike the plain ReLU, the leaky version of the activation function returns a softened value of the product </span><em><span class="koboSpan" id="kobo.32.1">wx</span></em><span class="koboSpan" id="kobo.33.1"> (instead of </span><em><span class="koboSpan" id="kobo.34.1">0</span></em><span class="koboSpan" id="kobo.35.1">, for negative values ââof </span><em><span class="koboSpan" id="kobo.36.1">wx</span></em><span class="koboSpan" id="kobo.37.1">), determined by the application of a multiplicative constant,Â </span><span class="koboSpan" id="kobo.38.1"><img class="fm-editor-equation" src="assets/666e3062-4473-4fc6-99e6-256c9b4cd643.png" style="width:0.75em;height:1.00em;"/></span><span class="koboSpan" id="kobo.39.1">, which usually assumes reduced values ââclose to </span><em><span class="koboSpan" id="kobo.40.1">0</span></em><span class="koboSpan" id="kobo.41.1"> (but not equal to </span><em><span class="koboSpan" id="kobo.42.1">0</span></em><span class="koboSpan" id="kobo.43.1">).</span></p>
<p><span class="koboSpan" id="kobo.44.1">From a mathematical point of view, the ReLU activation function represents a nonlinear transformation of a linear relationship consisting of the product of the input values ââfor their respective weights.</span></p>
<p><span class="koboSpan" id="kobo.45.1">In this way, we are able to approximate any kind of behavior without having to limit ourselves to simple linear relationships. </span><span class="koboSpan" id="kobo.45.2">We mentioned this in </span><a href="f467340a-244d-4714-8a39-68b230db2404.xhtml"><span class="koboSpan" id="kobo.46.1">Chapter 6</span></a><span class="koboSpan" id="kobo.47.1">, </span><em><span class="koboSpan" id="kobo.48.1">Securing User Authentication</span></em><span class="koboSpan" id="kobo.49.1">,</span><em><span class="koboSpan" id="kobo.50.1">Â </span></em><span class="koboSpan" id="kobo.51.1">when we introduced the section titledÂ </span><em><span class="koboSpan" id="kobo.52.1">User detection with multilayer perceptrons</span></em><span class="koboSpan" id="kobo.53.1">, showing how a </span><strong><span class="koboSpan" id="kobo.54.1">multilayer perceptron</span></strong><span class="koboSpan" id="kobo.55.1"> (</span><strong><span class="koboSpan" id="kobo.56.1">MLP</span></strong><span class="koboSpan" id="kobo.57.1">), being made up of multiple layers of artificial neurons implemented by Perceptrons, is able to overcome the limitations of the single Perceptron, approximating any continuous mathematical function by introducing an adequate number of neurons in the neural network. </span><span class="koboSpan" id="kobo.57.2">This ability to approximate any continuous mathematical function is what characterizes neural networks, and this determines their power in terms of learning.</span></p>
<p><span class="koboSpan" id="kobo.58.1">Now, let's see how we get to neural networks from individual artificial neurons.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">From artificial neurons to neural networks</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have seen the characteristics of artificial neurons and the tasks performed by the activation functions. </span><span class="koboSpan" id="kobo.2.2">Now let's look more closely at the characteristics of NNs. </span><span class="koboSpan" id="kobo.2.3">NNs are made up of layers of neurons, which together form a network. </span><span class="koboSpan" id="kobo.2.4">NNs can also be interpreted as artificial neuron graphs in which a weight is associated with each connection.</span></p>
<p><span class="koboSpan" id="kobo.3.1">We have said that by adding an adequate number of neurons to the NNs, it is possible to emulate the behavior of any continuous mathematical function. </span><span class="koboSpan" id="kobo.3.2">In practice, NNs are nothing but an alternative way of representing mathematical functions of arbitrary complexity. </span><span class="koboSpan" id="kobo.3.3">The power of NNs manifests itself in their ability to assemble the original features extracted from the datasets by creating new ones.</span></p>
<p><span class="koboSpan" id="kobo.4.1">Layers (hidden layers) are added to a neural network in order to perform such a combination of features. </span><span class="koboSpan" id="kobo.4.2">More layers are added, thereby enhancing the power of the network to generate new features.Â Particular attention must be given to the training procedure for </span><span><span class="koboSpan" id="kobo.5.1">NNs</span></span><span class="koboSpan" id="kobo.6.1">.</span></p>
<p><span class="koboSpan" id="kobo.7.1">One of the most common approaches to training NNs is </span><strong><span class="koboSpan" id="kobo.8.1">forward propagation.</span></strong><span class="koboSpan" id="kobo.9.1"> Training data is fed as input to the outer layers of the network, which, in turn, pass on their own partial processing output to the inner layers, and so on. </span><span class="koboSpan" id="kobo.9.2">The inner layers will carry out their elaborations on the input data received from the external layers, propagating the partial output returned from their processing forward to successive layers</span></p>
<p><span class="koboSpan" id="kobo.10.1">The processing carried out by the various layers usually entails evaluating the goodness of the weights associated with the individual predictions, based on the anticipated values. </span><span class="koboSpan" id="kobo.10.2">In the case of supervised learning, for example, we already know the expected values ââof the labeled samples in advance, and adjust the weights </span><span><span class="koboSpan" id="kobo.11.1">accordingly</span></span><span class="koboSpan" id="kobo.12.1">, based on the chosen learning algorithm.Â This results in a series of calculations, usually represented by the partial derivatives of the parameters associated with the individual neurons of which the different layers are composed, to be performed iteratively within the individual layers, thus resulting in a considerable load in computational terms.</span></p>
<p><span class="koboSpan" id="kobo.13.1">As the number of layers in the NNs increases, the number of steps that the data must make within the network increases exponentially. </span><span class="koboSpan" id="kobo.13.2">To get an idea of this, just think of the number of paths taken by the output of a neuron that gets forwarded to an inner layer consisting of 100 neurons, whose output is then, in turn, propagated to another layer composed of as many as 100 neurons, and so on, until it reaches the outer layers of neurons that return the final network output.</span></p>
<p><span class="koboSpan" id="kobo.14.1">An alternative training strategy, which significantly reduces the computational load, involvesÂ </span><strong><span class="koboSpan" id="kobo.15.1">backpropagation</span></strong><span class="koboSpan" id="kobo.16.1">. </span><span class="koboSpan" id="kobo.16.2">Instead of propagating the partial outputs obtained from the single layers toward the subsequent layers, the final outputs are computed at the level of the individual layers by consolidating the values ââobtained, by memorizing the outputs obtained at the individual layers. </span><span class="koboSpan" id="kobo.16.3">In this way, training is affected by propagating back the output of the entire network. </span><span class="koboSpan" id="kobo.16.4">Instead of the single outputs returned by the individual layers, the weights are updated accordingly to minimize the error rate.</span></p>
<p><span class="koboSpan" id="kobo.17.1">In mathematical terms,Â </span><strong><span class="koboSpan" id="kobo.18.1">backpropagation</span></strong><span class="koboSpan" id="kobo.19.1"> is as a product of theÂ </span><strong><span class="koboSpan" id="kobo.20.1">matrices</span></strong><span class="koboSpan" id="kobo.21.1"> and </span><strong><span class="koboSpan" id="kobo.22.1">vectors</span></strong><span class="koboSpan" id="kobo.23.1"> (which is less demanding in computational terms), rather than of a </span><strong><span class="koboSpan" id="kobo.24.1">matrix</span></strong><span class="koboSpan" id="kobo.25.1">â</span><strong><span class="koboSpan" id="kobo.26.1">matrix multiplication</span></strong><span class="koboSpan" id="kobo.27.1">, as in the case of forward propagation (for further details, refer toÂ </span><em><span class="koboSpan" id="kobo.28.1">Python Machine Learning â Second Edition</span></em><span class="koboSpan" id="kobo.29.1">,Â by Sebastian Raschka, Packt Publishing).</span></p>
<p><span class="koboSpan" id="kobo.30.1">Now, let's look at some of the most common types of NNs:</span></p>
<ul>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.31.1">Feedforward neural networks (FFNNs)</span></strong><span><span class="koboSpan" id="kobo.32.1">:Â FFNNs represent the basic typology of NNs. </span><span class="koboSpan" id="kobo.32.2">The individual layers of neurons are connected to some (or all) of the neurons present in the next layer. </span><span class="koboSpan" id="kobo.32.3">The peculiarity of</span></span> <span><span class="koboSpan" id="kobo.33.1">FFNNs</span></span><span><span class="koboSpan" id="kobo.34.1">Â is that the connections between the neurons of the individual layers</span></span><span class="koboSpan" id="kobo.35.1"> go only in one direction</span><span><span class="koboSpan" id="kobo.36.1">, and there are no cyclical or backward connections.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.37.1">Recurrent neural network (RNNs)</span></strong><span><span class="koboSpan" id="kobo.38.1">: These networks are characterized by the fact that the connections between neurons take the form of directed cycles in which the inputs and outputs consist of time series. </span><span class="koboSpan" id="kobo.38.2">RNNs facilitate the identification of patterns within the data as the data is accumulated and analyzed, and are therefore particularly useful for performing dynamic tasks such as speech recognition and language translation. </span><span class="koboSpan" id="kobo.38.3">In cybersecurity, RNNs are widely used in network traffic analysis, in static analysis, and so on. </span><span class="koboSpan" id="kobo.38.4">One example of an RNN is</span></span> <strong><span class="koboSpan" id="kobo.39.1">long short-term memory</span></strong> <span><span class="koboSpan" id="kobo.40.1">(</span></span><strong><span class="koboSpan" id="kobo.41.1">LSTM</span></strong><span><span class="koboSpan" id="kobo.42.1">) networks.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.43.1">Convolutional neural networks</span></strong> <span><span class="koboSpan" id="kobo.44.1">(</span></span><strong><span class="koboSpan" id="kobo.45.1">CNNs</span></strong><span><span class="koboSpan" id="kobo.46.1">):</span></span> <span><span class="koboSpan" id="kobo.47.1">These networks are particularlyÂ </span></span><span><span class="koboSpan" id="kobo.48.1">used to perform image-recognition tasks. </span><span class="koboSpan" id="kobo.48.2">CNNs are characterized by their ability to identify the presence of specific features within the data. </span><span class="koboSpan" id="kobo.48.3">The layers that make up the CNNs are associated with specific filters that represent the features of interest (such as, for example, a set of pixels representing digits within an image). </span><span class="koboSpan" id="kobo.48.4">These filters have the characteristic of being</span></span> <strong><span class="koboSpan" id="kobo.49.1">invariant</span></strong> <span><span class="koboSpan" id="kobo.50.1">with respect to the translations in space, thereby enabling the presence of features of interest in different areas of the search space to be detected (for example, the presence of the same digit in different areas of the image). </span><span class="koboSpan" id="kobo.50.2">The typical architecture of a CNN includes a series of</span></span> <strong><span class="koboSpan" id="kobo.51.1">convolution layers</span></strong><span><span class="koboSpan" id="kobo.52.1">,</span></span> <strong><span class="koboSpan" id="kobo.53.1">activation layers</span></strong><span><span class="koboSpan" id="kobo.54.1">,</span></span> <strong><span class="koboSpan" id="kobo.55.1">pooling layers</span></strong><span><span class="koboSpan" id="kobo.56.1">, and</span></span> <strong><span class="koboSpan" id="kobo.57.1">fully connected layers</span></strong><span class="koboSpan" id="kobo.58.1">. </span><span><span class="koboSpan" id="kobo.59.1">The pooling layer has the function of reducing the size of the features of interest to facilitate the search for the presence of the features within the search space.Â </span></span><span class="koboSpan" id="kobo.60.1">The following diagram shows CNN filters in action, within the different layers:</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.61.1"><img class="aligncenter size-full wp-image-559 image-border" src="assets/d71eec3a-9d49-4bce-b136-d5c5f45cfc91.png" style="width:38.17em;height:22.00em;"/></span></p>
<div class="packt_figref CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.62.1">(</span><em><span class="koboSpan" id="kobo.63.1">Image credits: https://commons.wikimedia.org/wiki/File:3_filters_in_a_Convolutional_Neural_Network.gif</span></em><span class="koboSpan" id="kobo.64.1">)</span></div>
<p><span class="koboSpan" id="kobo.65.1">Â </span></p>
<p><span class="koboSpan" id="kobo.66.1">Following this quick review of NNs, we are now ready to get acquainted with GANs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting to know GANs</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have said that the intuition on which GANs are based entails putting two NNs in competition with one another in order to improve the overall results. </span><span class="koboSpan" id="kobo.2.2">The term </span><strong><span class="koboSpan" id="kobo.3.1">adversarial</span></strong><span class="koboSpan" id="kobo.4.1"> refers specifically to the fact that the two NNs compete between themselves in completing their respective tasks. </span><span class="koboSpan" id="kobo.4.2">The outcome of this competition is an overall result that cannot be further improved, thereby attaining an equilibrium condition.</span></p>
<p><span class="koboSpan" id="kobo.5.1">A typical example of using GANs is the implementation of a particular NN, called aÂ </span><strong><span class="koboSpan" id="kobo.6.1">generativeÂ network</span></strong><span class="koboSpan" id="kobo.7.1">, with the task of creating an artificial image that simulates the characteristics of a real image. </span><span class="koboSpan" id="kobo.7.2">A second NN, called the </span><strong><span class="koboSpan" id="kobo.8.1">discriminatorÂ network</span></strong><span class="koboSpan" id="kobo.9.1">, is placed in competition with the first one (the generator) in order to distinguish the artificially simulated image from the real one.</span></p>
<p><span class="koboSpan" id="kobo.10.1">An interesting aspect is the fact that the two networks collaborate in achieving a situation of equilibrium (condition of indifference), putting in competition with one another the optimization of their respective objective functions. </span><span class="koboSpan" id="kobo.10.2">The generator network bases its optimization process on its ability to deceive the discriminator network.</span></p>
<p><span class="koboSpan" id="kobo.11.1">The discriminator network, in turn, carries out its optimization process, based on the accuracy achieved in distinguishing the real image from the artificially generated image from the generator network. </span><span class="koboSpan" id="kobo.11.2">Now, let's look at the differences between the two NNs in more detail.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Generative versus discriminative networks</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One way to intuitively understand the different tasks associated with individual NNs involved in a GAN is to consider the scenario in which the discriminator network tries to correctly classify spam messages artificially generated by the generator network. </span><span class="koboSpan" id="kobo.2.2">To demonstrate the different objective functions that the individual NNs must optimize, we will resort to conditional probabilities (which are the basis of the Bayes' rule), which we have already encountered in </span><a href="aaf59353-00b3-4625-8732-63aad02cc8e5.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 3</span></a><span class="koboSpan" id="kobo.4.1">,Â </span><em><span class="koboSpan" id="kobo.5.1">Ham or Spam? </span><span class="koboSpan" id="kobo.5.2">Detecting Email Cybersecurity Threats with AI</span></em><span class="koboSpan" id="kobo.6.1">, in the section </span><em><span class="koboSpan" id="kobo.7.1">Spam detection with Naive Bayes</span></em><span class="koboSpan" id="kobo.8.1">.</span></p>
<p><span class="koboSpan" id="kobo.9.1">We defineÂ </span><strong><em><span class="koboSpan" id="kobo.10.1">P</span></em></strong><span class="koboSpan" id="kobo.11.1">(</span><em><strong><span class="koboSpan" id="kobo.12.1">S</span></strong></em><span class="koboSpan" id="kobo.13.1">|</span><em><strong><span class="koboSpan" id="kobo.14.1">W</span></strong></em><span class="koboSpan" id="kobo.15.1">) as the probability that a given email message represents spam (</span><em><strong><span class="koboSpan" id="kobo.16.1">S</span></strong></em><span class="koboSpan" id="kobo.17.1">), based on the presence within the text of occurrences of suspect words (</span><em><strong><span class="koboSpan" id="kobo.18.1">W</span></strong></em><span class="koboSpan" id="kobo.19.1">). </span><span class="koboSpan" id="kobo.19.2">The task of the discriminator network therefore entails correctly estimating the probability </span><em><strong><span class="koboSpan" id="kobo.20.1">P</span></strong></em><span class="koboSpan" id="kobo.21.1">(</span><em><strong><span class="koboSpan" id="kobo.22.1">S</span></strong></em><span class="koboSpan" id="kobo.23.1">|</span><em><strong><span class="koboSpan" id="kobo.24.1">W</span></strong></em><span class="koboSpan" id="kobo.25.1">) associated with each single email analyzed.</span></p>
<p><span class="koboSpan" id="kobo.26.1">Symmetrically, the task of the generative network is the opposite: namely, to estimate the probability </span><em><strong><span class="koboSpan" id="kobo.27.1">P</span></strong></em><span class="koboSpan" id="kobo.28.1">(</span><em><strong><span class="koboSpan" id="kobo.29.1">W</span></strong></em><span class="koboSpan" id="kobo.30.1">|</span><em><strong><span class="koboSpan" id="kobo.31.1">S</span></strong></em><span class="koboSpan" id="kobo.32.1">)âthat is,Â given a spam message, how conceivable it is that the text contains the occurrences of the suspect words (</span><em><strong><span class="koboSpan" id="kobo.33.1">W</span></strong></em><span class="koboSpan" id="kobo.34.1">).Â You will recall from the theory of conditional probabilities that the valueÂ </span><em><strong><span class="koboSpan" id="kobo.35.1">P</span></strong></em><span><span class="koboSpan" id="kobo.36.1">(</span></span><em><strong><span class="koboSpan" id="kobo.37.1">S</span></strong></em><span><span class="koboSpan" id="kobo.38.1">|</span></span><em><strong><span class="koboSpan" id="kobo.39.1">W</span></strong></em><span><span class="koboSpan" id="kobo.40.1">)Â </span></span><span class="koboSpan" id="kobo.41.1">is different from the value </span><em><strong><span class="koboSpan" id="kobo.42.1">P</span></strong></em><span><span class="koboSpan" id="kobo.43.1">(</span></span><em><strong><span class="koboSpan" id="kobo.44.1">W</span></strong></em><span><span class="koboSpan" id="kobo.45.1">|</span></span><em><strong><span class="koboSpan" id="kobo.46.1">S</span></strong></em><span><span class="koboSpan" id="kobo.47.1">)</span></span><span class="koboSpan" id="kobo.48.1">, so the two neural networks have different objective functions to optimize, even if they are correlated.</span></p>
<p><span class="koboSpan" id="kobo.49.1">The discriminator network will therefore seek to optimize its objective function, which involves estimating </span><span><span class="koboSpan" id="kobo.50.1">appropriatelyÂ </span></span><span class="koboSpan" id="kobo.51.1">the probability </span><em><strong><span class="koboSpan" id="kobo.52.1">P</span></strong></em><span><span class="koboSpan" id="kobo.53.1">(</span></span><em><strong><span class="koboSpan" id="kobo.54.1">S</span></strong></em><span><span class="koboSpan" id="kobo.55.1">|</span></span><em><strong><span class="koboSpan" id="kobo.56.1">W</span></strong></em><span><span class="koboSpan" id="kobo.57.1">)</span></span><span class="koboSpan" id="kobo.58.1">Â by correctly classifying the spam messages artificially generated by the generative network, which in turn optimizes its objective function by generating spam emailÂ </span><span><span class="koboSpan" id="kobo.59.1">messages</span></span><span class="koboSpan" id="kobo.60.1">Â based on the probabilityÂ </span><em><strong><span class="koboSpan" id="kobo.61.1">P</span></strong></em><span><span class="koboSpan" id="kobo.62.1">(</span></span><em><strong><span class="koboSpan" id="kobo.63.1">W</span></strong></em><span><span class="koboSpan" id="kobo.64.1">|</span></span><em><strong><span class="koboSpan" id="kobo.65.1">S</span></strong></em><span><span class="koboSpan" id="kobo.66.1">)</span></span><span class="koboSpan" id="kobo.67.1">Â associated with each message. </span><span class="koboSpan" id="kobo.67.2">The generator network will then try to simulate spam messages, trying to deceive the discriminator network. </span><span class="koboSpan" id="kobo.67.3">At the same time, the discriminator network tries to correctly identify authentic spam messages, distinguishing them from those artificially created by the generator network by comparing them against the samples of genuine spam messages previously classified.</span></p>
<p><span class="koboSpan" id="kobo.68.1">Both networks learn from mutual interaction. </span><span class="koboSpan" id="kobo.68.2">The fake spam messages generated by the generative network are passed as input to the discriminative network, which analyzes them together with real spam messages, progressively refining the estimate of the probability constituted by the</span><span><span class="koboSpan" id="kobo.69.1">Â </span></span><em><strong><span class="koboSpan" id="kobo.70.1">P</span></strong></em><span><span class="koboSpan" id="kobo.71.1">(</span></span><em><strong><span class="koboSpan" id="kobo.72.1">S</span></strong></em><span><span class="koboSpan" id="kobo.73.1">|</span></span><em><strong><span class="koboSpan" id="kobo.74.1">W</span></strong></em><span><span class="koboSpan" id="kobo.75.1">)</span></span><span><span class="koboSpan" id="kobo.76.1">Â </span></span><span class="koboSpan" id="kobo.77.1">Â estimated value. </span><span class="koboSpan" id="kobo.77.2">This establishes a symbiotic relationshipÂ  between the two neural networks, in which both networks try to optimize their opposite objective functions.</span></p>
<p><span class="koboSpan" id="kobo.78.1">This situation is defined by game theory as aÂ </span><strong><span class="koboSpan" id="kobo.79.1">zero-sum game</span></strong><span class="koboSpan" id="kobo.80.1">, and the dynamic equilibrium that is progressively reached, which puts an end to the optimization process of both networks, is known as the </span><strong><span class="koboSpan" id="kobo.81.1">Nash equilibrium</span></strong><span class="koboSpan" id="kobo.82.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The Nash equilibrium</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the mathematical theory of games, the Nash equilibrium is defined as the condition in which two competing players consider their respective game strategies as the best possible options available to them. </span><span class="koboSpan" id="kobo.2.2">This condition of equilibrium is the result of the learning performed by the players by iteratively repeating playing sessions.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In a Nash equilibrium condition, each player will then choose to perform the same action without modifying it.</span></p>
<p><span class="koboSpan" id="kobo.4.1">The conditions under which this balance is determined are particularly restrictive. </span><span class="koboSpan" id="kobo.4.2">In fact, they imply the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">All players are rational (that is, they must maximize their own objective function)</span></li>
<li><span class="koboSpan" id="kobo.6.1">All the players know that the other players are, in turn, rational, and know the respective objective functions to be maximized</span></li>
<li><span class="koboSpan" id="kobo.7.1">All players play their game simultaneously, without being aware of the choices made by the others</span></li>
</ul>
<p><span class="koboSpan" id="kobo.8.1">Now, let's look at how to represent GANs in mathematical terms.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The math behind GANs</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">We have said that the purpose of a GAN is to achieve a condition of equilibrium between the two NNs. </span><span class="koboSpan" id="kobo.2.2">The search for this equilibrium involves solving the following equation, a minimax condition:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img class="fm-editor-equation" src="assets/086f1ba1-e063-4bd4-850e-0ee0d62e0194.png" style="width:44.83em;height:1.67em;"/></span></p>
<p><span class="koboSpan" id="kobo.4.1">From the preceding formula, you can see the antagonistic goal that characterizes the two neural networks. </span><span class="koboSpan" id="kobo.4.2">We try to maximize </span><em><span class="koboSpan" id="kobo.5.1">D</span></em><span class="koboSpan" id="kobo.6.1"> while minimizing </span><em><span class="koboSpan" id="kobo.7.1">G</span></em><span class="koboSpan" id="kobo.8.1">. </span><span class="koboSpan" id="kobo.8.2">In other words, the neural network </span><em><span class="koboSpan" id="kobo.9.1">D</span></em><span class="koboSpan" id="kobo.10.1">, which represents the discriminator, aims to maximize the equation, which translates into maximizing the output associated with real samples while minimizing the output associated with fake samples. </span><span class="koboSpan" id="kobo.10.2">On the other side, the neural network </span><em><span class="koboSpan" id="kobo.11.1">G</span></em><span class="koboSpan" id="kobo.12.1">, which represents the generator, has the inverse goal, which is to minimize the number of failures of </span><em><span class="koboSpan" id="kobo.13.1">G</span></em><span class="koboSpan" id="kobo.14.1">, which results in maximizing the output returned by </span><em><span class="koboSpan" id="kobo.15.1">D</span></em><span class="koboSpan" id="kobo.16.1"> when it is put in front of the fake samples.</span></p>
<p><span class="koboSpan" id="kobo.17.1">The overall objective of the GAN is to achieve a balance in a zero-sum game (Nash equilibrium), characterized by a condition of indifference in which the output of </span><em><span class="koboSpan" id="kobo.18.1">D</span></em><span class="koboSpan" id="kobo.19.1"> will consist of a probability estimate of 50% assigned to each categorized sample. </span><span class="koboSpan" id="kobo.19.2">In other words, the discriminator cannot reliably distinguish between genuine samples and fake samples.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">How to train a GAN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Training a GAN may require high computational capacity; otherwise, the time required to carry out the training may vary from a few hours to a few days. </span><span class="koboSpan" id="kobo.2.2">Given the mutual dependency that is established between the two NNs, it is advisable to keep the values returned by the generator network constant while training the discriminator network. </span><span class="koboSpan" id="kobo.2.3">At the same time, it can be useful to perform the pretraining of the discriminator network using the training data available, before training the generator network.</span></p>
<p><span class="koboSpan" id="kobo.3.1">It is also important to adequately set the learning rates of the two NNs, so as to avoid a situation where the learning rate of the discriminator network exceeds that of the generator network and vice versa, thereby preventing the respective NNs from achieving their optimization goals.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">An example of a GANâemulating MNIST handwritten digits</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In the following example, adapted from the original code available atÂ </span><a href="https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py"><span class="koboSpan" id="kobo.3.1">https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py</span></a><span class="koboSpan" id="kobo.4.1"> (released under the MIT license atÂ </span><a href="https://github.com/eriklindernoren/ML-From-Scratch/blob/master/LICENSE"><span class="koboSpan" id="kobo.5.1">https://github.com/eriklindernoren/ML-From-Scratch/blob/master/LICENSE</span></a><span class="koboSpan" id="kobo.6.1">), we see an example of a GAN that is able to artificially generate, from some input noise, the reproduction of handwritten digit images by comparing them against the MNIST dataset (available for download at</span><a href="http://yann.lecun.com/exdb/mnist/"><span class="koboSpan" id="kobo.7.1">Â http://yann.lecun.com/exdb/mnist/</span></a><span class="koboSpan" id="kobo.8.1">).</span></p>
<p><span class="koboSpan" id="kobo.9.1">The activation functions of the GAN's NNs, implemented by the </span><kbd><span class="koboSpan" id="kobo.10.1">build_generator()</span></kbd><span class="koboSpan" id="kobo.11.1"> and </span><kbd><span class="koboSpan" id="kobo.12.1">build_discriminator()</span></kbd><span class="koboSpan" id="kobo.13.1"> functions, are both based on Leaky ReLU (in order to improve the stability of the GAN, which can be affected by the presence of sparse gradients).</span></p>
<p><span class="koboSpan" id="kobo.14.1">We will make use of sample noise as generator input by leveraging the </span><kbd><span class="koboSpan" id="kobo.15.1">normal()</span></kbd><span class="koboSpan" id="kobo.16.1"> function from theÂ </span><kbd><span class="koboSpan" id="kobo.17.1">random</span></kbd><span class="koboSpan" id="kobo.18.1"> library as follows:</span></p>
<pre><span class="koboSpan" id="kobo.19.1">noise = np.random.normal(0, 1, (half_batch, self.latent_dim))</span></pre>
<p><span><span class="koboSpan" id="kobo.20.1">The training phase of both NNs is implemented by the </span><kbd><span class="koboSpan" id="kobo.21.1">train()</span></kbd><span class="koboSpan" id="kobo.22.1"> method:</span></span></p>
<pre><span class="koboSpan" id="kobo.23.1">train(self, n_epochs, batch_size=128, save_interval=50)</span></pre>
<p><span class="koboSpan" id="kobo.24.1">Finally, in the </span><kbd><span class="koboSpan" id="kobo.25.1">train()</span></kbd><span class="koboSpan" id="kobo.26.1"> method, the link between the two NNs is evident:</span></p>
<pre><span class="koboSpan" id="kobo.27.1"># The generator wants the discriminator to label the generated samples as valid</span><br/><br/><span class="koboSpan" id="kobo.28.1">valid = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))), axis=1)</span><br/><br/><span class="koboSpan" id="kobo.29.1"># Train the generator</span><br/><span class="koboSpan" id="kobo.30.1">g_loss, g_acc = self.combined.train_on_batch(noise, valid)</span></pre>
<p class="CDPAlignLeft CDPAlign"><span><span class="koboSpan" id="kobo.31.1">In the following image, we see the progressive learning of GANs in relation to the different epochs. </span><span class="koboSpan" id="kobo.31.2">The progress achieved by the GAN in generating the representative images of the numbers is clearly visible:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.32.1"><img class="aligncenter size-full wp-image-561 image-border" src="assets/ee6d925d-0113-4f8d-987d-380e93f968d2.png" style="width:58.67em;height:20.75em;"/></span></p>
<p><span class="koboSpan" id="kobo.33.1">The following is the code example, adapted from the original, that is available at </span><a href="https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py"><span class="koboSpan" id="kobo.34.1">https://github.com/eriklindernoren/ML-From-Scratch/blob/master/mlfromscratch/unsupervised_learning/generative_adversarial_network.py</span></a><span class="koboSpan" id="kobo.35.1"> (released under the MIT license atÂ </span><a href="https://github.com/eriklindernoren/ML-From-Scratch/blob/master/LICENSE"><span class="koboSpan" id="kobo.36.1">https://github.com/eriklindernoren/ML-From-Scratch/blob/master/LICENSE</span></a><span class="koboSpan" id="kobo.37.1">):</span></p>
<pre><span class="koboSpan" id="kobo.38.1">from __future__ import print_function, division</span><br/><span class="koboSpan" id="kobo.39.1">from sklearn import datasets</span><br/><span class="koboSpan" id="kobo.40.1">import math</span><br/><span class="koboSpan" id="kobo.41.1">import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.42.1">import numpy as np</span><br/><span class="koboSpan" id="kobo.43.1">import progressbar</span><br/><br/><span class="koboSpan" id="kobo.44.1">from sklearn.datasets import fetch_openml</span><br/><span class="koboSpan" id="kobo.45.1">from mlxtend.data import loadlocal_mnist</span><br/><br/><span class="koboSpan" id="kobo.46.1">from mlfromscratch.deep_learning.optimizers import Adam</span><br/><span class="koboSpan" id="kobo.47.1">from mlfromscratch.deep_learning.loss_functions import CrossEntropy</span><br/><span class="koboSpan" id="kobo.48.1">from mlfromscratch.deep_learning.layers import Dense, Dropout, Flatten, Activation, Reshape, BatchNormalization</span><br/><span class="koboSpan" id="kobo.49.1">from mlfromscratch.deep_learning import NeuralNetwork</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.50.1">After importing the necessary libraries, we are now ready to address the </span><kbd><span class="koboSpan" id="kobo.51.1">GAN</span></kbd><span class="koboSpan" id="kobo.52.1"> class definition, which implements our GAN, deploying deep, fully-connected neural networks in the form of generator and discriminator components, instantiated in the class constructor (the </span><kbd><span class="koboSpan" id="kobo.53.1">__init__()</span></kbd><span class="koboSpan" id="kobo.54.1"> method):</span></p>
<pre><span class="koboSpan" id="kobo.55.1">class GAN():</span><br/><br/><span class="koboSpan" id="kobo.56.1">    def __init__(self):</span><br/><span class="koboSpan" id="kobo.57.1">        self.img_rows = 28 </span><br/><span class="koboSpan" id="kobo.58.1">        self.img_cols = 28</span><br/><span class="koboSpan" id="kobo.59.1">        self.img_dim = self.img_rows * self.img_cols</span><br/><span class="koboSpan" id="kobo.60.1">        self.latent_dim = 100</span><br/><br/><span class="koboSpan" id="kobo.61.1">        optimizer = Adam(learning_rate=0.0002, b1=0.5)</span><br/><span class="koboSpan" id="kobo.62.1">        loss_function = CrossEntropy</span><br/><br/><span class="koboSpan" id="kobo.63.1">        # Build the discriminator</span><br/><span class="koboSpan" id="kobo.64.1">        self.discriminator = self.build_discriminator(optimizer, loss_function)</span><br/><br/><span class="koboSpan" id="kobo.65.1">        # Build the generator</span><br/><span class="koboSpan" id="kobo.66.1">        self.generator = self.build_generator(optimizer, loss_function)</span><br/><br/><span class="koboSpan" id="kobo.67.1">        # Build the combined model</span><br/><span class="koboSpan" id="kobo.68.1">        self.combined = NeuralNetwork(optimizer=optimizer, loss=loss_function)</span><br/><span class="koboSpan" id="kobo.69.1">        self.combined.layers.extend(self.generator.layers)</span><br/><span class="koboSpan" id="kobo.70.1">        self.combined.layers.extend(self.discriminator.layers)</span><br/><br/><span class="koboSpan" id="kobo.71.1">        print ()</span><br/><span class="koboSpan" id="kobo.72.1">        self.generator.summary(name="Generator")</span><br/><span class="koboSpan" id="kobo.73.1">        self.discriminator.summary(name="Discriminator")</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.74.1">The generator and discriminator components are defined in the </span><kbd><span class="koboSpan" id="kobo.75.1">build_generator()</span></kbd><span class="koboSpan" id="kobo.76.1">Â and the </span><kbd><span class="koboSpan" id="kobo.77.1">build_discriminator()</span></kbd><span class="koboSpan" id="kobo.78.1"> class methods, respectively:</span></p>
<pre><span class="koboSpan" id="kobo.79.1">    def build_generator(self, optimizer, loss_function):</span><br/>        <br/><span class="koboSpan" id="kobo.80.1">        model = NeuralNetwork(optimizer=optimizer, loss=loss_function)</span><br/><br/><span class="koboSpan" id="kobo.81.1">        model.add(Dense(256, input_shape=(self.latent_dim,)))</span><br/><span class="koboSpan" id="kobo.82.1">        model.add(Activation('leaky_relu'))</span><br/><span class="koboSpan" id="kobo.83.1">        model.add(BatchNormalization(momentum=0.8))</span><br/><span class="koboSpan" id="kobo.84.1">        model.add(Dense(512))</span><br/><span class="koboSpan" id="kobo.85.1">        model.add(Activation('leaky_relu'))</span><br/><span class="koboSpan" id="kobo.86.1">        model.add(BatchNormalization(momentum=0.8))</span><br/><span class="koboSpan" id="kobo.87.1">        model.add(Dense(1024))</span><br/><span class="koboSpan" id="kobo.88.1">        model.add(Activation('leaky_relu'))</span><br/><span class="koboSpan" id="kobo.89.1">        model.add(BatchNormalization(momentum=0.8))</span><br/><span class="koboSpan" id="kobo.90.1">        model.add(Dense(self.img_dim))</span><br/><span class="koboSpan" id="kobo.91.1">        model.add(Activation('tanh'))</span><br/><br/><span class="koboSpan" id="kobo.92.1">        return model</span><br/><br/><span class="koboSpan" id="kobo.93.1">    def build_discriminator(self, optimizer, loss_function):</span><br/>        <br/><span class="koboSpan" id="kobo.94.1">        model = NeuralNetwork(optimizer=optimizer, loss=loss_function)</span><br/><br/><span class="koboSpan" id="kobo.95.1">        model.add(Dense(512, input_shape=(self.img_dim,)))</span><br/><span class="koboSpan" id="kobo.96.1">        model.add(Activation('leaky_relu'))</span><br/><span class="koboSpan" id="kobo.97.1">        model.add(Dropout(0.5))</span><br/><span class="koboSpan" id="kobo.98.1">        model.add(Dense(256))</span><br/><span class="koboSpan" id="kobo.99.1">        model.add(Activation('leaky_relu'))</span><br/><span class="koboSpan" id="kobo.100.1">        model.add(Dropout(0.5))</span><br/><span class="koboSpan" id="kobo.101.1">        model.add(Dense(2))</span><br/><span class="koboSpan" id="kobo.102.1">        model.add(Activation('softmax'))</span><br/><br/><span class="koboSpan" id="kobo.103.1">        return model</span></pre>
<p><span class="koboSpan" id="kobo.104.1">To train the GAN, we define the </span><kbd><span class="koboSpan" id="kobo.105.1">train()</span></kbd><span class="koboSpan" id="kobo.106.1">Â class method, which takes care of training both the generator and discriminator components:</span></p>
<pre><span class="koboSpan" id="kobo.107.1">    def train(self, n_epochs, batch_size=128, save_interval=50):  </span><br/><br/><span class="koboSpan" id="kobo.108.1">        X, y = loadlocal_mnist(images_path='./MNIST/train-images.idx3-ubyte', labels_path='./MNIST/train-labels.idx1-ubyte')  </span><br/><br/><br/><span class="koboSpan" id="kobo.109.1">        # Rescale [-1, 1]</span><br/><span class="koboSpan" id="kobo.110.1">        X = (X.astype(np.float32) - 127.5) / 127.5</span><br/><br/><span class="koboSpan" id="kobo.111.1">        half_batch = int(batch_size / 2)</span><br/><br/><span class="koboSpan" id="kobo.112.1">        for epoch in range(n_epochs):</span><br/><br/><span class="koboSpan" id="kobo.113.1">            # ---------------------</span><br/><span class="koboSpan" id="kobo.114.1">            #  Train Discriminator</span><br/><span class="koboSpan" id="kobo.115.1">            # ---------------------</span><br/><br/><span class="koboSpan" id="kobo.116.1">            self.discriminator.set_trainable(True)</span><br/><br/><span class="koboSpan" id="kobo.117.1">            # Select a random half batch of images</span><br/><span class="koboSpan" id="kobo.118.1">            idx = np.random.randint(0, X.shape[0], half_batch)</span><br/><span class="koboSpan" id="kobo.119.1">            imgs = X[idx]</span><br/><br/><span class="koboSpan" id="kobo.120.1">            # Sample noise to use as generator input</span><br/><span class="koboSpan" id="kobo.121.1">            noise = np.random.normal(0, 1, (half_batch, self.latent_dim))</span><br/><br/><span class="koboSpan" id="kobo.122.1">            # Generate a half batch of images</span><br/><span class="koboSpan" id="kobo.123.1">            gen_imgs = self.generator.predict(noise)</span><br/><br/><span class="koboSpan" id="kobo.124.1">            # Valid = [1, 0], Fake = [0, 1]</span><br/><span class="koboSpan" id="kobo.125.1">            valid = np.concatenate((np.ones((half_batch, 1)), np.zeros((half_batch, 1))), axis=1)</span><br/><span class="koboSpan" id="kobo.126.1">            fake = np.concatenate((np.zeros((half_batch, 1)), np.ones((half_batch, 1))), axis=1)</span><br/><br/><span class="koboSpan" id="kobo.127.1">            # Train the discriminator</span><br/><span class="koboSpan" id="kobo.128.1">            d_loss_real, d_acc_real = self.discriminator.train_on_batch(imgs, valid)</span><br/><span class="koboSpan" id="kobo.129.1">            d_loss_fake, d_acc_fake = self.discriminator.train_on_batch(gen_imgs, fake)</span><br/><span class="koboSpan" id="kobo.130.1">            d_loss = 0.5 * (d_loss_real + d_loss_fake)</span><br/><span class="koboSpan" id="kobo.131.1">            d_acc = 0.5 * (d_acc_real + d_acc_fake)</span><br/><br/><br/><span class="koboSpan" id="kobo.132.1">            # ---------------------</span><br/><span class="koboSpan" id="kobo.133.1">            #  Train Generator</span><br/><span class="koboSpan" id="kobo.134.1">            # ---------------------</span><br/><br/><span class="koboSpan" id="kobo.135.1">            # We only want to train the generator for the combined model</span><br/><span class="koboSpan" id="kobo.136.1">            self.discriminator.set_trainable(False)</span><br/><br/><span class="koboSpan" id="kobo.137.1">            # Sample noise and use as generator input</span><br/><span class="koboSpan" id="kobo.138.1">            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))</span><br/><br/><span class="koboSpan" id="kobo.139.1">            # The generator wants the discriminator to label the generated samples as valid</span><br/><span class="koboSpan" id="kobo.140.1">            valid = np.concatenate((np.ones((batch_size, 1)), np.zeros((batch_size, 1))), axis=1)</span><br/><br/><span class="koboSpan" id="kobo.141.1">            # Train the generator</span><br/><span class="koboSpan" id="kobo.142.1">            g_loss, g_acc = self.combined.train_on_batch(noise, valid)</span><br/><br/><span class="koboSpan" id="kobo.143.1">            # Display the progress</span><br/><span class="koboSpan" id="kobo.144.1">            print ("%d [D loss: %f, acc: %.2f%%] [G loss: %f, acc: %.2f%%]" % (epoch, d_loss, 100*d_acc, g_loss, 100*g_acc))</span><br/><br/><span class="koboSpan" id="kobo.145.1">            # If at save interval =&gt; save generated image samples</span><br/><span class="koboSpan" id="kobo.146.1">            if epoch % save_interval == 0:</span><br/><span class="koboSpan" id="kobo.147.1">                self.save_imgs(epoch)</span></pre>
<p><span class="koboSpan" id="kobo.148.1">After training the GAN, we can save the newly created adversarial sample images with the </span><kbd><span class="koboSpan" id="kobo.149.1">save_imgs()</span></kbd><span class="koboSpan" id="kobo.150.1"> class method, which is defined as follows:</span></p>
<pre><span class="koboSpan" id="kobo.151.1">    def save_imgs(self, epoch):</span><br/><span class="koboSpan" id="kobo.152.1">        r, c = 5, 5 # Grid size</span><br/><span class="koboSpan" id="kobo.153.1">        noise = np.random.normal(0, 1, (r * c, self.latent_dim))</span><br/><span class="koboSpan" id="kobo.154.1">        # Generate images and reshape to image shape</span><br/><span class="koboSpan" id="kobo.155.1">        gen_imgs = self.generator.predict(noise).reshape((-1, self.img_rows, self.img_cols))</span><br/><br/><span class="koboSpan" id="kobo.156.1">        # Rescale images 0 - 1</span><br/><span class="koboSpan" id="kobo.157.1">        gen_imgs = 0.5 * gen_imgs + 0.5</span><br/><br/><span class="koboSpan" id="kobo.158.1">        fig, axs = plt.subplots(r, c)</span><br/><span class="koboSpan" id="kobo.159.1">        plt.suptitle("Generative Adversarial Network")</span><br/><span class="koboSpan" id="kobo.160.1">        cnt = 0</span><br/><span class="koboSpan" id="kobo.161.1">        for i in range(r):</span><br/><span class="koboSpan" id="kobo.162.1">            for j in range(c):</span><br/><span class="koboSpan" id="kobo.163.1">                axs[i,j].imshow(gen_imgs[cnt,:,:], cmap='gray')</span><br/><span class="koboSpan" id="kobo.164.1">                axs[i,j].axis('off')</span><br/><span class="koboSpan" id="kobo.165.1">                cnt += 1</span><br/><span class="koboSpan" id="kobo.166.1">        fig.savefig("mnist_%d.png" % epoch)</span><br/><span class="koboSpan" id="kobo.167.1">        plt.close()</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.168.1">To launch the script, we just need to define the </span><kbd><span class="koboSpan" id="kobo.169.1">__main__</span></kbd><span class="koboSpan" id="kobo.170.1"> entry point as follows:</span></p>
<pre><span class="koboSpan" id="kobo.171.1">if __name__ == '__main__':</span><br/><br/><span class="koboSpan" id="kobo.172.1">    gan = GAN()</span><br/><span class="koboSpan" id="kobo.173.1">    gan.train(n_epochs=200000, batch_size=64, save_interval=400)</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.174.1">Now, let's move on and have a look at the GAN tools and libraries developed in Python.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">GAN Python tools and libraries</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The number of tools and libraries (both to carry out attacks and to defend from attacks) for developing adversarial examples is constantly growing. </span><span class="koboSpan" id="kobo.2.2">We will look at some of the most common examples of these. </span><span class="koboSpan" id="kobo.2.3">In this section, we will consolidate the general-use libraries and tools, and in the following sections, we will deal with libraries and specific tools based on the individual strategies and scenarios of attack and defense.</span></p>
<p><span class="koboSpan" id="kobo.3.1">To fully understand the usefulness of these tools and libraries, we need to analyze the vulnerabilities of the cybersecurity solutions based on neural networks, the possibilities involved in the implementation of the attacks, and the difficulties in preparing an appropriate defense.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Neural network vulnerabilities</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Despite the fact, as we have seen previously, that NNs have acquired particular relevance in recent times (as we have seen previously), due to their significant potential when it comes to resolving more complex problems that are usually the prerogative of human cognitive abilities, such as facial recognition and speech recognition, NNs, especially DNNs, suffer from a number of rather important vulnerabilities, which can be exploited through the use of GANs.Â This implies the possibility, for example, of deceiving biometric authentication procedures based on facial recognition or other biometric evidence made possible by the artificial creation of adversarial examples.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Apparently harmless devices such as 3D medical imagery scanners have been exploited as attack vectors, as shown in a recent paper,Â </span><em><span class="koboSpan" id="kobo.4.1">CT-GAN: Malicious Tampering of 3D Medical Imagery using Deep Learning</span></em><span class="koboSpan" id="kobo.5.1">, by Yisroel Mirsky, Tom Mahler, Ilan Shelef, and Yuval Elovici of the Department of Information Systems Engineering, Ben-Gurion University, Israel Soroka University Medical Center, arXiv: 1901.03597v2).</span></p>
<p><span class="koboSpan" id="kobo.6.1">In the study, the authors focused on the possibility of injecting and removing cancer images from CT scans, demonstrating how DNNs are highly susceptible to attack.</span></p>
<p><span class="koboSpan" id="kobo.7.1">By adding fake evidence or removing some genuine evidence of a medical condition, an attacker with access to medical imageryÂ can change the outcome of a patient's diagnosis.</span></p>
<p><span class="koboSpan" id="kobo.8.1">For example, an attacker can add or remove evidence of aneurysms, tumors in the brain, and other forms of pathological evidences, such as heart disease. </span><span class="koboSpan" id="kobo.8.2">This type of threat shows how the use of DNNs to manage sensitive information, such as those pertaining to health conditions, can determine the expansion of the potential attack surface by several orders of magnitude, up to the possibility of committing crimes such as murder, which could involve politicians, heads of state, and so on as potential victims, without the need for the attacker to get their hands dirty, simply by exploiting the vulnerabilities of digital devices and procedures as </span><strong><span class="koboSpan" id="kobo.9.1">aseptic</span></strong><span class="koboSpan" id="kobo.10.1"> attack vectors.</span></p>
<p><span class="koboSpan" id="kobo.11.1">From what we have said, we can easily understand the severity level caused by the lack of robustness to adversarial attacks by DNNs, which can determine the compromising of the procedures and the applications they rely on.</span></p>
<p><span class="koboSpan" id="kobo.12.1">Nevertheless, among the applications that exploit DNNs, there are also mission-critical applications (such as those that manage the functions of self-driving cars).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Deep neural network attacks</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">There are basically two main ways to carry out an attack against DNNs:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">White-box attacks</span></strong><span class="koboSpan" id="kobo.4.1">: This type of attack presupposes the model transparency of the DNN's target, which grants the ability toÂ </span><span><span class="koboSpan" id="kobo.5.1">directly verifyÂ </span></span><span class="koboSpan" id="kobo.6.1">the sensitivity of the response to the adversarial examples.</span></li>
<li><strong><span class="koboSpan" id="kobo.7.1">Black-box attacks</span></strong><span class="koboSpan" id="kobo.8.1">: Unlike the previous case, the sensitivity check of the </span><span><span class="koboSpan" id="kobo.9.1">adversarial exampleÂ </span></span><span class="koboSpan" id="kobo.10.1">is implemented indirectly, not having available the configuration details of the targeted neural network; the only information available is the output values ââreturned by the neural networks to the respective inputs sent to them.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.11.1">Irrespective of the type of attack, the attacker is, in any case, able to exploit some general characteristics concerning neural networks. </span><span class="koboSpan" id="kobo.11.2">As we have seen, among the most widespread adversarial attacks are those that aim to deceive the image classification algorithms, exploiting artificially created image samples. </span><span class="koboSpan" id="kobo.11.3">Therefore, knowing that image classification applications prefer to use </span><strong><span class="koboSpan" id="kobo.12.1">convolutional neural networks</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong><span class="koboSpan" id="kobo.14.1">CNNs</span></strong><span class="koboSpan" id="kobo.15.1">), an attacker will focus more on the vulnerabilities of such neural networks to conduct their own attacks.</span></p>
<p><span class="koboSpan" id="kobo.16.1">Even the learning strategies used by DNNs can indirectly constitute vectors of attack. </span><span class="koboSpan" id="kobo.16.2">We have previously seen how the use of the backpropagation technique is preferred in carrying out the training of the algorithms by virtue of its greater efficiency in computational terms. </span><span class="koboSpan" id="kobo.16.3">Being aware of this preferential learning choice, an attacker can, in turn, exploit algorithms such as gradient descent to attack DNNs, trusting that the backpropagation strategy allows the gradient computation of the output returned by the entire DNN.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Adversarial attack methodologies</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The following are some of the most commonly used methods to develop adversarial attacks:</span></p>
<ul>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.3.1">Fast gradient sign method</span></strong><span class="koboSpan" id="kobo.4.1"> (</span><strong><span class="koboSpan" id="kobo.5.1">FGSM</span></strong><span class="koboSpan" id="kobo.6.1">)</span><span><span class="koboSpan" id="kobo.7.1">: To generate adversarial examples, this method exploits the sign of the gradient associated with the backpropagation method used by the DNN's victim.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.8.1">Jacobian-based saliency map attack</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong><span class="koboSpan" id="kobo.10.1">JSMA</span></strong><span class="koboSpan" id="kobo.11.1">)</span><span><span class="koboSpan" id="kobo.12.1">: This attack methodology iteratively modifies information (such as the most significant pixel of an image) to create adversarial examples, based on a JSMA that characterizes the existing relationship between the input and output returned by the target neural network.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.13.1">Carlini and Wagner</span></strong><span class="koboSpan" id="kobo.14.1"> (</span><strong><span class="koboSpan" id="kobo.15.1">C and W</span></strong><span class="koboSpan" id="kobo.16.1">)</span><span><span class="koboSpan" id="kobo.17.1">: This adversarial attack methodology is perhaps the most reliable, and the most difficult to detect. </span><span class="koboSpan" id="kobo.17.2">The adversarial attack is treated as an optimization problem that uses a predefined measure (such as the Euclidean distance) to determine the gap between the original and the adversarial examples.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.18.1">However, adversarial examples also show an interesting feature: </span><strong><span class="koboSpan" id="kobo.19.1">attack transferability</span></strong><span class="koboSpan" id="kobo.20.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Adversarial attack transferability</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A typical feature of adversarial attacks has to do with theirÂ </span><strong><span class="koboSpan" id="kobo.3.1">transferability</span></strong><span class="koboSpan" id="kobo.4.1">.</span></p>
<p><span class="koboSpan" id="kobo.5.1">This feature refers to the possibility that the adversarial examples generated for a given DNN can also be transferred to another DNN, due to the high generalization capacity that characterizes the neural networks, and that constitutes their power (but also their fragility).</span></p>
<p><span class="koboSpan" id="kobo.6.1">Taking advantage of the transferability of adversarial attacks, an attacker is able to create reusable adversarial examples without needing to know the exact parameters of the individual configurations of the neural networks.</span></p>
<p><span class="koboSpan" id="kobo.7.1">It is therefore very likely that a set of adversarial examples developed to successfully deceive a specific DNN for image classification, for example, can be exploited to deceive other neural networks with similar classification tasks.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Defending against adversarial attacks</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Following the growing diffusion of adversarial attacks, many attempts have been made to provide adequate defense measures, based mainly on the following methods:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.3.1">Statistical-based detection defense</span></strong><span class="koboSpan" id="kobo.4.1">: This method tries to detect the presence of adversarial examples by exploiting statistical tests and outlier detection. </span><span class="koboSpan" id="kobo.4.2">It assumes that the statistical distributions characterizing the real examples and the adversarial examples are fundamentally distinct from one another. </span><span class="koboSpan" id="kobo.4.3">However, the effectiveness of the C and W attack methodology shows that this assumption is not at all obvious or reliable.</span></li>
<li><strong><span class="koboSpan" id="kobo.5.1">Gradient masking defense</span></strong><span class="koboSpan" id="kobo.6.1">: We have seen how adversarial attacks exploit the backpropagation optimization strategy adopted by most DNNs to their advantage, relying on information pertaining to gradient calculations performed by the target neural network. </span><span class="koboSpan" id="kobo.6.2">One form of defense, gradient masking, therefore involves hiding information specifically pertaining toÂ gradients during neural network training.</span></li>
<li><strong><span class="koboSpan" id="kobo.7.1">Adversarial training defense</span></strong><span class="koboSpan" id="kobo.8.1">: This method of defense aims to make the learning algorithm more robust with regard to possible perturbations present in the training data by inserting the adversarial samples, as well as the genuine samples, in the training dataset. </span><span class="koboSpan" id="kobo.8.2">This defense methodology also appears to be the most promising against C and W adversarial attacks. </span><span class="koboSpan" id="kobo.8.3">However, it does have a cost associated with it, involving the increased complexity of the network and of the increase in model parameters.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.9.1">Now that the vulnerabilities of the DNNsâalong with the adversarial attacks and defense methodologiesâhave been introduced, we can now analyze the main libraries used to develop the adversarial examples.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">CleverHansÂ library of adversarial examples</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">One of the Python libraries that is garnering the most attention is definitely the CleverHans library, which is often the basis of other libraries and tools for developing adversarial examples.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The CleverHans library is available atÂ </span><a href="https://github.com/tensorflow/cleverhans"><span class="koboSpan" id="kobo.4.1">https://github.com/tensorflow/cleverhans</span></a><span class="koboSpan" id="kobo.5.1">, and is released under the MIT license (</span><a href="https://github.com/tensorflow/cleverhans/blob/master/LICENSE"><span class="koboSpan" id="kobo.6.1">https://github.com/tensorflow/cleverhans/blob/master/LICENSE</span></a><span class="koboSpan" id="kobo.7.1">).</span></p>
<p><span class="koboSpan" id="kobo.8.1">This library is particularly suitable for constructing attacks, building defenses, and benchmarking machine learning systems' vulnerability to adversarial attacks.</span></p>
<p><span class="koboSpan" id="kobo.9.1">To install the CleverHans library, we must first proceed with the installation of the TensorFlow library (</span><a href="https://www.tensorflow.org/install/"><span class="koboSpan" id="kobo.10.1">https://www.tensorflow.org/install/</span></a><span class="koboSpan" id="kobo.11.1">), which is used to perform the graph computations necessary in the implementation of learning models.</span></p>
<p><span class="koboSpan" id="kobo.12.1">After installing TensorFlow, we can proceed with the installation of CleverHans using the usual command:</span></p>
<pre><strong><span class="koboSpan" id="kobo.13.1">pip install cleverhans</span></strong></pre>
<p><span class="koboSpan" id="kobo.14.1">One of the many advantages of the CleverHans library is that it offers several examples and tutorials in which the many different methods of using the models for the development of adversarial examples are shown.</span></p>
<p><span class="koboSpan" id="kobo.15.1">In particular, the CleverHans library provides us with the following tutorials (based on the MNIST training handwritten digits dataset, available for download atÂ </span><a href="http://yann.lecun.com/exdb/mnist/"><span class="koboSpan" id="kobo.16.1">http://yann.lecun.com/exdb/mnist/</span></a><span class="koboSpan" id="kobo.17.1"> ):</span></p>
<ul>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.18.1">MNIST with FGSM</span></strong><span><span class="koboSpan" id="kobo.19.1">: This tutorial covers how to train a MNIST model to craft adversarial examples using the FGSM and make the model more robust to adversarial examples using adversarial training.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.20.1">MNIST with JSMA</span></strong><span><span class="koboSpan" id="kobo.21.1">: This tutorial covers how to define a MNIST model to craft adversarial examples using the JSMA approach.</span></span></li>
<li class="mce-root"><strong><span class="koboSpan" id="kobo.22.1">MNIST using a black-box attack</span></strong><span><span class="koboSpan" id="kobo.23.1">: This tutorial implements a black-box attack based on the adversarialÂ training of a substitute model (that is, a copy that imitates the black-box model by observing the labels that the black-box model assigns to inputs chosen carefully by the adversary). </span><span class="koboSpan" id="kobo.23.2">The adversary then uses the substitute model's gradients to find adversarial examples that are incorrectly classified by the black-box model as well.</span></span></li>
</ul>
<p><span><span class="koboSpan" id="kobo.24.1">During this chapter, we will encounter some examples that use the CleverHans library to develop adversarial attack and defense scenarios.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">EvadeML-Zoo library of adversarial examples</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Another library of particular interest is EvadeML-Zoo. </span><span class="koboSpan" id="kobo.2.2">EvadeML-Zoo is a benchmarking and visualization tool for adversarial machine learning, developed by the machine learning group and the security research group at the University of Virginia.</span></p>
<p><span class="koboSpan" id="kobo.3.1">EvadeML-Zoo is released under the MIT license (</span><a href="https://github.com/mzweilin/EvadeML-Zoo/blob/master/LICENSE"><span class="koboSpan" id="kobo.4.1">https://github.com/mzweilin/EvadeML-Zoo/blob/master/LICENSE</span></a><span class="koboSpan" id="kobo.5.1">) and is freely available for download atÂ </span><a href="https://github.com/mzweilin/EvadeML-Zoo"><span class="koboSpan" id="kobo.6.1">https://github.com/mzweilin/EvadeML-Zoo</span></a><span class="koboSpan" id="kobo.7.1">.</span><a href="https://github.com/mzweilin/EvadeML-Zoo"/></p>
<p><span class="koboSpan" id="kobo.8.1">The EvadeML-Zoo library provides a series of tools and models, including the following:</span></p>
<ul>
<li class="mce-root"><span><span class="koboSpan" id="kobo.9.1">Attacking methods such as FGSM, BIM, JSMA, Deepfool, Universal Perturbations, and Carlini/Wagner-L2/Li/L0</span></span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.10.1">Pretrained state-of-the-art models to attack</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.11.1">Visualization of adversarial examples</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.12.1">Defense methods</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.13.1">Several ready-to-use datasets, such as, MNIST, CIFAR-10, and ImageNet-ILSVRC</span></li>
</ul>
<p><span class="koboSpan" id="kobo.14.1">Once the package has been downloaded, you can install the EvadeML-Zoo library on a machine that only uses a CPU with the following command:</span></p>
<pre><strong><span class="koboSpan" id="kobo.15.1">pip install -r requirements_cpu.txt</span></strong></pre>
<p><span class="koboSpan" id="kobo.16.1">Also, if you have a compatible GPU available, you can execute the following command:</span></p>
<pre><strong><span class="koboSpan" id="kobo.17.1">pip install -r requirements_gpu.txt</span></strong></pre>
<p><span class="koboSpan" id="kobo.18.1">We have seen that the features offered by the EvadeML-Zoo library also include the pretrained models, particularly useful for accelerating the development process of adversarial examples, which are notoriously rather heavy in computational terms.</span></p>
<p><span class="koboSpan" id="kobo.19.1">To download the pretrained models, run the following command:</span></p>
<pre><strong><span class="koboSpan" id="kobo.20.1">mkdir downloads; curl -sL https://github.com/mzweilin/EvadeML-Zoo/releases/download/v0.1/downloads.tar.gz | tar xzv -C downloads</span></strong></pre>
<p><span class="koboSpan" id="kobo.21.1">Another interesting feature of the EvadeML-Zoo library is that it can be executed by running the </span><kbd><span class="koboSpan" id="kobo.22.1">main.py</span></kbd><span class="koboSpan" id="kobo.23.1">Â </span><span><span class="koboSpan" id="kobo.24.1">utility.</span></span></p>
<p><span class="koboSpan" id="kobo.25.1">In the following code block, you can see the usage menu of </span><kbd><span class="koboSpan" id="kobo.26.1">main.py</span></kbd><span class="koboSpan" id="kobo.27.1">, along with an example of execution of the tool:</span></p>
<pre><strong><span class="koboSpan" id="kobo.28.1">usage: python main.py [-h] [--dataset_name DATASET_NAME] [--model_name MODEL_NAME]</span></strong><br/><strong><span class="koboSpan" id="kobo.29.1">                [--select [SELECT]] [--noselect] [--nb_examples NB_EXAMPLES]</span></strong><br/><strong><span class="koboSpan" id="kobo.30.1">                [--balance_sampling [BALANCE_SAMPLING]] [--nobalance_sampling]</span></strong><br/><strong><span class="koboSpan" id="kobo.31.1">                [--test_mode [TEST_MODE]] [--notest_mode] [--attacks ATTACKS]</span></strong><br/><strong><span class="koboSpan" id="kobo.32.1">                [--clip CLIP] [--visualize [VISUALIZE]] [--novisualize]</span></strong><br/><strong><span class="koboSpan" id="kobo.33.1">                [--robustness ROBUSTNESS] [--detection DETECTION]</span></strong><br/><strong><span class="koboSpan" id="kobo.34.1">                [--detection_train_test_mode [DETECTION_TRAIN_TEST_MODE]]</span></strong><br/><strong><span class="koboSpan" id="kobo.35.1">                [--nodetection_train_test_mode] [--result_folder RESULT_FOLDER]</span></strong><br/><strong><span class="koboSpan" id="kobo.36.1">                [--verbose [VERBOSE]] [--noverbose]</span></strong><br/> <br/><strong><span class="koboSpan" id="kobo.37.1"> optional arguments:</span></strong><br/><strong><span class="koboSpan" id="kobo.38.1">   -h, --help            show this help message and exit</span></strong><br/><strong><span class="koboSpan" id="kobo.39.1">   --dataset_name DATASET_NAME</span></strong><br/><strong><span class="koboSpan" id="kobo.40.1">                         Supported: MNIST, CIFAR-10, ImageNet, SVHN.</span></strong><br/><strong><span class="koboSpan" id="kobo.41.1">   --model_name MODEL_NAME</span></strong><br/><strong><span class="koboSpan" id="kobo.42.1">                         Supported: cleverhans, cleverhans_adv_trained and</span></strong><br/><strong><span class="koboSpan" id="kobo.43.1">                         carlini for MNIST; carlini and DenseNet for CIFAR-10;</span></strong><br/><strong><span class="koboSpan" id="kobo.44.1">                         ResNet50, VGG19, Inceptionv3 and MobileNet for</span></strong><br/><strong><span class="koboSpan" id="kobo.45.1">                         ImageNet; tohinz for SVHN.</span></strong><br/><strong><span class="koboSpan" id="kobo.46.1">   --select [SELECT]     Select correctly classified examples for the</span></strong><br/><strong><span class="koboSpan" id="kobo.47.1">                         experiment.</span></strong><br/><strong><span class="koboSpan" id="kobo.48.1">   --noselect</span></strong><br/><strong><span class="koboSpan" id="kobo.49.1">   --nb_examples NB_EXAMPLES</span></strong><br/><strong><span class="koboSpan" id="kobo.50.1">                         The number of examples selected for attacks.</span></strong><br/><strong><span class="koboSpan" id="kobo.51.1">   --balance_sampling [BALANCE_SAMPLING]</span></strong><br/><strong><span class="koboSpan" id="kobo.52.1">                         Select the same number of examples for each class.</span></strong><br/><strong><span class="koboSpan" id="kobo.53.1">   --nobalance_sampling</span></strong><br/><strong><span class="koboSpan" id="kobo.54.1">   --test_mode [TEST_MODE]</span></strong><br/><strong><span class="koboSpan" id="kobo.55.1">                         Only select one sample for each class.</span></strong><br/><strong><span class="koboSpan" id="kobo.56.1">   --notest_mode</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1">   --attacks ATTACKS     Attack name and parameters in URL style, separated by</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1">                         semicolon.</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1">   --clip CLIP           L-infinity clip on the adversarial perturbations.</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1">   --visualize [VISUALIZE]</span></strong><br/><strong><span class="koboSpan" id="kobo.61.1">                         Output the image examples for each attack, enabled by</span></strong><br/><strong><span class="koboSpan" id="kobo.62.1">                         default.</span></strong><br/><strong><span class="koboSpan" id="kobo.63.1">   --novisualize</span></strong><br/><strong><span class="koboSpan" id="kobo.64.1">   --robustness ROBUSTNESS</span></strong><br/><strong><span class="koboSpan" id="kobo.65.1">                         Supported: FeatureSqueezing.</span></strong><br/><strong><span class="koboSpan" id="kobo.66.1">   --detection DETECTION</span></strong><br/><strong><span class="koboSpan" id="kobo.67.1">                         Supported: feature_squeezing.</span></strong><br/><strong><span class="koboSpan" id="kobo.68.1">   --detection_train_test_mode [DETECTION_TRAIN_TEST_MODE]</span></strong><br/><strong><span class="koboSpan" id="kobo.69.1">                         Split into train/test datasets.</span></strong><br/><strong><span class="koboSpan" id="kobo.70.1">   --nodetection_train_test_mode</span></strong><br/><strong><span class="koboSpan" id="kobo.71.1">   --result_folder RESULT_FOLDER</span></strong><br/><strong><span class="koboSpan" id="kobo.72.1">                         The output folder for results.</span></strong><br/><strong><span class="koboSpan" id="kobo.73.1">   --verbose [VERBOSE]   Stdout level. </span><span class="koboSpan" id="kobo.73.2">The hidden content will be saved to log</span></strong><br/><strong><span class="koboSpan" id="kobo.74.1">                         files anyway.</span></strong><br/><strong><span class="koboSpan" id="kobo.75.1">   --noverbose</span></strong></pre>
<p><span class="koboSpan" id="kobo.76.1">The EvadeML-Zoo library is executed using the Carlini model and an FGSM adversarial attack on the MNIST dataset, as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.77.1">python main.py --dataset_name MNIST --model_name carlini \</span></strong><br/><strong><span class="koboSpan" id="kobo.78.1"> --nb_examples 2000 --balance_sampling \</span></strong><br/><strong><span class="koboSpan" id="kobo.79.1"> --attacks "FGSM?eps=0.1;" \</span></strong><br/><strong><span class="koboSpan" id="kobo.80.1"> --robustness "none;FeatureSqueezing?squeezer=bit_depth_1;" \</span></strong><br/><strong><span class="koboSpan" id="kobo.81.1"> --detection "FeatureSqueezing?squeezers=bit_depth_1,median_filter_2_2&amp;distance_measure=l1&amp;fpr=0.05;"</span><br/><br/><span class="koboSpan" id="kobo.82.1">Defense-GAN library</span></strong></pre>
<p><span class="koboSpan" id="kobo.83.1">Finally, we will learn how to develop defense models against adversarial attacks using the </span><kbd><span class="koboSpan" id="kobo.84.1">Defense-GAN</span></kbd><span class="koboSpan" id="kobo.85.1"> library.</span></p>
<p><span class="koboSpan" id="kobo.86.1">Before analyzing the details of the Defense-GAN library, let's try to understand the assumptions thatÂ it is based on, along with the features it offers to implement an adequate defense against adversarial attacks.</span></p>
<p><span class="koboSpan" id="kobo.87.1">As we have seen, adversarial attacks are categorized as either white-box attacks or black-box attacks; in the case of white-box attacks, the attacker has complete access to the model architecture and parameters, while in the case of black-box attacks, the attacker does not have access to the model parameters.</span></p>
<p><span class="koboSpan" id="kobo.88.1">We also know that many methods of defense against adversarial attacks have been proposed that are essentially based on the ability to distinguish the statistical distributions of adversarial examples from genuine samples (statistical detection), on the ability to hide sensitive information relating to the neural learning phase network (gradient masking), or on the possibility of training the learning algorithm using the adversarial examples together with the other training samples (adversarial training).</span></p>
<p><span class="koboSpan" id="kobo.89.1">All these defense methods present limitations, as they are effective against either white-box attacks or black-box attacks, but not both.</span></p>
<p><span class="koboSpan" id="kobo.90.1">Defense-GAN can </span><span><span class="koboSpan" id="kobo.91.1">insteadÂ </span></span><span class="koboSpan" id="kobo.92.1">be used as a defense against any attack, since it does not assume an attack model, but simply leverages the generative power of GANs to reconstruct adversarial examples.</span></p>
<p><span class="koboSpan" id="kobo.93.1">Defense-GAN proposes a new defense strategy based on a GAN trained in an unsupervised manner on legitimate (unperturbed) training samples in order to denoise adversarial examples.</span></p>
<p><span class="koboSpan" id="kobo.94.1">The Defense-GAN library is released under the Apache 2.0 license (</span><a href="https://github.com/kabkabm/defensegan/blob/master/LICENSE"><span class="koboSpan" id="kobo.95.1">https://github.com/kabkabm/defensegan/blob/master/LICENSE</span></a><span class="koboSpan" id="kobo.96.1">), and is freely available for download atÂ </span><a href="https://github.com/kabkabm/defensegan"><span class="koboSpan" id="kobo.97.1">https://github.com/kabkabm/defensegan</span></a><span class="koboSpan" id="kobo.98.1">.</span><a href="https://github.com/kabkabm/defensegan"/></p>
<p><span class="koboSpan" id="kobo.99.1">Once the library is downloaded, you can install it by launching the following command:</span></p>
<pre><strong><span class="koboSpan" id="kobo.100.1">pip install -r requirements.txt</span></strong></pre>
<p><span class="koboSpan" id="kobo.101.1">To download the dataset and prepare the data directory, launch the </span><kbd><span class="koboSpan" id="kobo.102.1">download_dataset.py</span></kbd><span class="koboSpan" id="kobo.103.1">Â Python script with the following command:</span></p>
<pre><strong><span class="koboSpan" id="kobo.104.1">python download_dataset.py [mnist|f-mnist|celeba]</span></strong></pre>
<p><span class="koboSpan" id="kobo.105.1">Train a GAN model by launching </span><kbd><span class="koboSpan" id="kobo.106.1">train.py script</span></kbd><span class="koboSpan" id="kobo.107.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.108.1">python train.py --cfg  --is_train</span></strong><br/> <br/><strong><span class="koboSpan" id="kobo.109.1">     --cfg This can be set to either a .yml configuration file like the ones in experiments/cfgs, or an output directory path.</span></strong><br/><strong><span class="koboSpan" id="kobo.110.1">      can be any parameter that is defined in the config file.</span></strong></pre>
<p><span class="koboSpan" id="kobo.111.1">The script execution will create:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.112.1">A directory in the output directory for each experiment with the same name as the directory where the model checkpoints are saved</span></li>
<li><span class="koboSpan" id="kobo.113.1">A configuration file is saved in each experiment directory so that it can be loaded as the address to that directory</span></li>
<li><span class="koboSpan" id="kobo.114.1">A training directory in the output directory </span><span><span class="koboSpan" id="kobo.115.1">for each experiment with the same name as the directory where the model checkpoints are saved</span></span></li>
<li><span class="koboSpan" id="kobo.116.1">A training configuration file is saved in each experiment directory so that it can be loaded as the address to that directory</span></li>
</ul>
<p><span class="koboSpan" id="kobo.117.1">The Defense-GAN library also offers tools that you can use to experiment with the different attack modes, thereby allowing the effectiveness of defense models to be verified.</span></p>
<p><span class="koboSpan" id="kobo.118.1">To perform black-box attacks, we can launch the </span><kbd><span class="koboSpan" id="kobo.119.1">blackbox.py</span></kbd><span class="koboSpan" id="kobo.120.1"> tool:</span></p>
<pre class="mce-root"><span class="koboSpan" id="kobo.121.1">python blackbox.py --cfg  \</span><br/><span class="koboSpan" id="kobo.122.1">     --results_dir  \</span><br/><span class="koboSpan" id="kobo.123.1">     --bb_model {A, B, C, D, E} \</span><br/><span class="koboSpan" id="kobo.124.1">     --sub_model {A, B, C, D, E} \</span><br/><span class="koboSpan" id="kobo.125.1">     --fgsm_eps  \</span><br/><span class="koboSpan" id="kobo.126.1">     --defense_type {none|defense_gan|adv_tr}</span><br/><span class="koboSpan" id="kobo.127.1">     [--train_on_recs or --online_training]  </span></pre>
<p><span class="koboSpan" id="kobo.128.1">Let's take a look at each parameter here:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.129.1">The </span><kbd><span class="koboSpan" id="kobo.130.1">--cfg</span></kbd><span class="koboSpan" id="kobo.131.1"> parameter is the path to the configuration file for training the iWGAN. </span><span class="koboSpan" id="kobo.131.2">This can also be the path to the output directory of the model.</span></li>
<li><span class="koboSpan" id="kobo.132.1">The </span><kbd><span class="koboSpan" id="kobo.133.1">--results_dir</span></kbd><span class="koboSpan" id="kobo.134.1"> parameter isÂ the path where the final results are saved in text files.</span></li>
<li><span class="koboSpan" id="kobo.135.1">TheÂ </span><kbd><span class="koboSpan" id="kobo.136.1">--bb_model</span></kbd><span class="koboSpan" id="kobo.137.1"> parameterÂ represents the black-box model architectures that are used in tables 1 and 2.</span></li>
<li><span class="koboSpan" id="kobo.138.1">The </span><kbd><span class="koboSpan" id="kobo.139.1">--sub_model</span></kbd><span class="koboSpan" id="kobo.140.1"> parameter representsÂ the substitute model architectures that are used in tables 1 and 2.</span></li>
<li><span class="koboSpan" id="kobo.141.1">The </span><kbd><span class="koboSpan" id="kobo.142.1">--defense_type</span></kbd><span class="koboSpan" id="kobo.143.1"> parameter specifies the type of defense to protect the classifier.</span></li>
<li><span class="koboSpan" id="kobo.144.1">The </span><kbd><span class="koboSpan" id="kobo.145.1">--train_on_recs</span></kbd><span class="koboSpan" id="kobo.146.1">Â andÂ </span><kbd><span class="koboSpan" id="kobo.147.1">--online_training</span></kbd><span class="koboSpan" id="kobo.148.1"> parameters are optional. </span><span class="koboSpan" id="kobo.148.2">If they are set, the classifier will be trained on the reconstructions of Defense-GAN (for example, in the </span><kbd><span class="koboSpan" id="kobo.149.1">Defense-GAN-Rec</span></kbd> <span><span class="koboSpan" id="kobo.150.1">columnÂ </span></span><span class="koboSpan" id="kobo.151.1">of tables 1 and 2); otherwise, the results are for </span><kbd><span class="koboSpan" id="kobo.152.1">Defense-GAN-Orig</span></kbd><span class="koboSpan" id="kobo.153.1">. </span><span class="koboSpan" id="kobo.153.2">Note thatÂ </span><kbd><span class="koboSpan" id="kobo.154.1">--online_training</span></kbd><span class="koboSpan" id="kobo.155.1"> will take a while if </span><kbd><span class="koboSpan" id="kobo.156.1">--rec_iters</span></kbd><span class="koboSpan" id="kobo.157.1">, or </span><kbd><span class="koboSpan" id="kobo.158.1">L</span></kbd><span class="koboSpan" id="kobo.159.1"> in the paper, is set to a large value.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.160.1">There is also a list of </span><kbd><span class="koboSpan" id="kobo.161.1">--</span></kbd><span class="koboSpan" id="kobo.162.1"> that are the same as the hyperparameters that are defined in configuration files (all lowercase), along with a list of flags in </span><kbd><span class="koboSpan" id="kobo.163.1">blackbox.py</span></kbd><span class="koboSpan" id="kobo.164.1">. </span><span class="koboSpan" id="kobo.164.2">The most important ones are as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.165.1">--rec_iters</span></kbd><span class="koboSpan" id="kobo.166.1">: The number of </span><strong><span class="koboSpan" id="kobo.167.1">gradient descent</span></strong><span class="koboSpan" id="kobo.168.1"> (</span><strong><span class="koboSpan" id="kobo.169.1">GD</span></strong><span class="koboSpan" id="kobo.170.1">) reconstruction iterations for Defense-GAN, or </span><kbd><span class="koboSpan" id="kobo.171.1">L</span></kbd><span class="koboSpan" id="kobo.172.1"> in the paper.</span></li>
<li><kbd><span class="koboSpan" id="kobo.173.1">--rec_lr</span></kbd><span class="koboSpan" id="kobo.174.1">: The learning rate of the reconstruction step.</span></li>
<li><kbd><span class="koboSpan" id="kobo.175.1">--rec_rr</span></kbd><span class="koboSpan" id="kobo.176.1">: The number of random restarts for the reconstruction step, or </span><kbd><span class="koboSpan" id="kobo.177.1">R</span></kbd><span class="koboSpan" id="kobo.178.1"> in the paper.</span></li>
</ul>
<ul>
<li><kbd><span class="koboSpan" id="kobo.179.1">--num_train</span></kbd><span class="koboSpan" id="kobo.180.1">: The number of images on which to train the black-box model. </span><span class="koboSpan" id="kobo.180.2">For debugging purposes, set this to a small value.</span></li>
<li><kbd><span class="koboSpan" id="kobo.181.1">--num_test</span></kbd><span class="koboSpan" id="kobo.182.1">: The number of images to test on. </span><span class="koboSpan" id="kobo.182.2">For debugging purposes, set this to a small value.</span></li>
<li><kbd><span class="koboSpan" id="kobo.183.1">--debug</span></kbd><span class="koboSpan" id="kobo.184.1">: This will save qualitative attack and reconstruction results in the debug directory and will not run the adversarial attack part of the code.</span></li>
</ul>
<p><span><span class="koboSpan" id="kobo.185.1">An example of </span><kbd><span class="koboSpan" id="kobo.186.1">blackbox.py</span></kbd><span class="koboSpan" id="kobo.187.1"> execution with parameters is as follows:</span></span></p>
<pre><span class="koboSpan" id="kobo.188.1">python blackbox.py --cfg output/gans/mnist \</span><br/><span class="koboSpan" id="kobo.189.1"> --results_dir defensegan \</span><br/><span class="koboSpan" id="kobo.190.1"> --bb_model A \</span><br/><span class="koboSpan" id="kobo.191.1"> --sub_model B \</span><br/><span class="koboSpan" id="kobo.192.1"> --fgsm_eps 0.3 \</span><br/><span class="koboSpan" id="kobo.193.1"> --defense_type defense_gan</span></pre>
<p><span class="koboSpan" id="kobo.194.1">We can, of course, test Defense-GAN for white-box attacks by launching the </span><kbd><span class="koboSpan" id="kobo.195.1">whitebox.py</span></kbd><span class="koboSpan" id="kobo.196.1"> tool:</span></p>
<pre><span class="koboSpan" id="kobo.197.1">python whitebox.py --cfg  \</span><br/><span class="koboSpan" id="kobo.198.1">        --results_dir  \</span><br/><span class="koboSpan" id="kobo.199.1">        --attack_type {fgsm, rand_fgsm, cw} \</span><br/><span class="koboSpan" id="kobo.200.1">        --defense_type {none|defense_gan|adv_tr} \</span><br/><span class="koboSpan" id="kobo.201.1">        --model {A, B, C, D} \</span><br/><span class="koboSpan" id="kobo.202.1">        [--train_on_recs or --online_training]</span><br/> <br/>     </pre>
<p><span><span class="koboSpan" id="kobo.203.1">An example of </span><kbd><span class="koboSpan" id="kobo.204.1">whitebox.py</span></kbd><span class="koboSpan" id="kobo.205.1"> execution with parameters is as follows:</span></span></p>
<pre><span class="koboSpan" id="kobo.206.1">python whitebox.py --cfg  \</span><br/><span class="koboSpan" id="kobo.207.1">        --results_dir whitebox \</span><br/><span class="koboSpan" id="kobo.208.1">        --attack_type fgsm \</span><br/><span class="koboSpan" id="kobo.209.1">        --defense_type defense_gan \</span><br/><span class="koboSpan" id="kobo.210.1">        --model A</span></pre>
<p><span class="koboSpan" id="kobo.211.1">As for </span><kbd><span><span class="koboSpan" id="kobo.212.1">blackbox.py</span></span></kbd><span class="koboSpan" id="kobo.213.1"> , there is also a list of </span><kbd><span class="koboSpan" id="kobo.214.1">--</span></kbd><span class="koboSpan" id="kobo.215.1"> that are the same as the hyperparameters that are defined in the configuration files (all lowercase), along with a list of flags in </span><kbd><span class="koboSpan" id="kobo.216.1">whitebox.py</span></kbd><span class="koboSpan" id="kobo.217.1">. </span><span class="koboSpan" id="kobo.217.2">The most important ones are as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.218.1">--rec_iters</span></kbd><span class="koboSpan" id="kobo.219.1">: The number of GD reconstruction iterations for Defense-GAN, or </span><kbd><span class="koboSpan" id="kobo.220.1">L</span></kbd><span class="koboSpan" id="kobo.221.1"> in the paper</span></li>
<li><kbd><span class="koboSpan" id="kobo.222.1">--rec_lr</span></kbd><span><span class="koboSpan" id="kobo.223.1">: The learning rate of the reconstruction step</span></span></li>
</ul>
<ul>
<li><kbd><span class="koboSpan" id="kobo.224.1">--rec_rr</span></kbd><span><span class="koboSpan" id="kobo.225.1">: The number of random restarts for the reconstruction step, or </span><kbd><span class="koboSpan" id="kobo.226.1">R</span></kbd><span class="koboSpan" id="kobo.227.1"> in the paper</span></span></li>
<li><kbd><span class="koboSpan" id="kobo.228.1">--num_test</span></kbd><span><span class="koboSpan" id="kobo.229.1">: The number of images to test on. </span><span class="koboSpan" id="kobo.229.2">For debugging purposes, set this to a small value</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.230.1">Let's now move on and see how attacks against neural networks can be performed via model substitution.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Network attack via model substitution</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">An interesting demonstration of the potential offered by adversarial attacks conducted in black-box mode is the one described in the paper </span><em><span class="koboSpan" id="kobo.3.1">Practical Black-Box Attacks against Machine Learning</span></em><span class="koboSpan" id="kobo.4.1">Â (arXiv: 1602.02697v4), in which the possibility of carrying out an attack against remotely hosted DNNs is demonstrated, without the attacker being aware of the configuration characteristics of the target NN.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">In these cases, the only information available to the attacker is that of the output returned by the neural network based on the type of input provided by the attacker. </span><span class="koboSpan" id="kobo.5.2">In practice, the attacker observes the classification labels returned by the DNN in relation to the attacking inputs. </span><span class="koboSpan" id="kobo.5.3">And it is here that an attack strategy becomes interesting. </span><span class="koboSpan" id="kobo.5.4">A local substitute model is, in fact, trained in place of the remotely hosted NN, using inputs synthetically generated by an adversary model and labeled by the target NN.</span></p>
<p><span class="koboSpan" id="kobo.6.1">A neural network hosted by MetaMind is used as a remote hosted network target, which exposes a DL API on the internet. </span><span class="koboSpan" id="kobo.6.2">By submitting to the hosted network, the adversarial examples trained on the local substitute, the authors verify that the RNN wrongly classifies over 80% of the adversarial examples. </span><span class="koboSpan" id="kobo.6.3">Furthermore, this attack strategy is also verified against similar services made available online by Amazon and Google, with even worse results in terms of the misclassification rate, which goes up to 96%.</span></p>
<p><span class="koboSpan" id="kobo.7.1">In this way, the authors demonstrate that their black-box adversarial attacks strategy is of general validity, and not limited to the specific target neural network chosen. </span><span class="koboSpan" id="kobo.7.2">The result obtained also demonstrates the validity of the principle of theÂ </span><strong><span class="koboSpan" id="kobo.8.1">transferability of adversarial attacks,</span></strong><span class="koboSpan" id="kobo.9.1">Â using the synthetic dataset tested on the local model. </span><span class="koboSpan" id="kobo.9.2">The attacker is actually replacing the local model with the target model by approximating the characteristics sufficiently to be able to exploit the vulnerabilities identified on the local model to the target model.</span></p>
<p><span class="koboSpan" id="kobo.10.1">Therefore, the critical elements of the model-substitution-based adversarial attack methodology are substitute model training and synthetic dataset generation.</span></p>
<p><span class="koboSpan" id="kobo.11.1">Let's take a closer look at both features.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Substitute model training</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As we said previously, the model-substitution-based adversarial attack methodology is aimed at training a </span><strong><span class="koboSpan" id="kobo.3.1">substitute model</span></strong><span class="koboSpan" id="kobo.4.1"> that resembles the original target NN in order to find viable vulnerabilities on the target NN.</span></p>
<p><span class="koboSpan" id="kobo.5.1">The training phase of the substitute model is therefore characterized by a number of important peculiarities, which involves the following:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.6.1">Selecting an architecture for the substitute model without knowledge of the targeted model</span></li>
<li><span class="koboSpan" id="kobo.7.1">Limiting the number of queries made to the targeted model in order to ensure that the approach is tractable</span></li>
</ul>
<p><span class="koboSpan" id="kobo.8.1">In order to address these difficult tasks, the proposed attack strategy is based on the generation of synthetic data (using the technique known as </span><strong><span class="koboSpan" id="kobo.9.1">Jacobian-based dataset augmentation</span></strong><span class="koboSpan" id="kobo.10.1">).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Generating the synthetic dataset</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The approach followed in the generation of the synthetic dataset is of central importance in the attack strategy based on model substitution.</span></p>
<p><span class="koboSpan" id="kobo.3.1">To understand it, you only need to consider the fact that, although, in principle, it is possible to carry out an indefinite (even infinite) number of different queries toward the targeted model (to verify the output that the target model generates in relation to the input </span><span><span class="koboSpan" id="kobo.4.1">containedÂ </span></span><span class="koboSpan" id="kobo.5.1">in the individual queries), this approach is not viable from a practical point of view.</span></p>
<p><span class="koboSpan" id="kobo.6.1">It is unsustainable in the first place because the high number of queries would make the adversarial attack easily detectable, but it is also unsustainable because we would increase the number of requests to be sent to the target model in proportion to the number of potential input components of the target neural network.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The alternative solution involves using an appropriate heuristic to generate the synthetic dataset, based on identifying how the directions in the target model's output vary around an initial set of training points. </span><span class="koboSpan" id="kobo.7.2">These directions are identified with the substitute model's Jacobian matrix to accurately approximate the target model's decision boundaries by prioritizing the samples when querying the target model for labels.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Fooling malware detectors with MalGAN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The black-box adversarial attack strategy can also be validly used to deceive the next-generation antimalware systems, based on NNs.</span></p>
<p><span class="koboSpan" id="kobo.3.1">A useful library for developing black-box adversarial attacks with malware examples is MalGAN, available for download atÂ </span><a href="https://github.com/yanminglai/Malware-GAN/"><span class="koboSpan" id="kobo.4.1">https://github.com/yanminglai/Malware-GAN/</span></a><span class="koboSpan" id="kobo.5.1">, and released under the GPL 3.0 license (</span><a href="https://github.com/yanminglai/Malware-GAN/blob/master/LICENSE"><span class="koboSpan" id="kobo.6.1">https://github.com/yanminglai/Malware-GAN/blob/master/LICENSE</span></a><span class="koboSpan" id="kobo.7.1">). </span><span class="koboSpan" id="kobo.7.2">The fundamental idea behind </span><span><span class="koboSpan" id="kobo.8.1">MalGANÂ </span></span><span class="koboSpan" id="kobo.9.1">is to use a GAN to generate adversarial malware examples, which are able to bypass black-box machine-learning-based detection models. </span><span class="koboSpan" id="kobo.9.2">To install the MalGAN library, you need to install the TensorFlow 1.80, Keras 2.0, and Cuckoo Sandbox 2.03 (</span><a href="https://cuckoo.readthedocs.io/en/2.0.3/"><span class="koboSpan" id="kobo.10.1">https://cuckoo.readthedocs.io/en/2.0.3/</span></a><span class="koboSpan" id="kobo.11.1">) libraries. </span><span class="koboSpan" id="kobo.11.2">Cuckoo Sandbox is used to extract API features from malware samples acquired from </span><a href="https://virusshare.com/"><span class="koboSpan" id="kobo.12.1">https://virusshare.com/</span></a><span class="koboSpan" id="kobo.13.1"> (128 API features are selected as dimensional vectors to be input to the NN).Â </span></p>
<p><span class="koboSpan" id="kobo.14.1">The following is the code of the main MalGAN class (version 2):</span></p>
<pre><span class="koboSpan" id="kobo.15.1">"""</span><br/><span class="koboSpan" id="kobo.16.1"> MalGAN v2 Class definition</span><br/><span class="koboSpan" id="kobo.17.1"> https://github.com/yanminglai/Malware-GAN/blob/master/MalGAN_v2.py</span><br/><span class="koboSpan" id="kobo.18.1"> Released under GPL 3.0 LICENSE: https://github.com/yanminglai/Malware-GAN/blob/master/LICENSE  </span><br/> <br/><span class="koboSpan" id="kobo.19.1"> """</span><br/> <br/><span class="koboSpan" id="kobo.20.1"> from keras.layers import Input, Dense, Activation</span><br/><span class="koboSpan" id="kobo.21.1"> from keras.layers.merge import Maximum, Concatenate</span><br/><span class="koboSpan" id="kobo.22.1"> from keras.models import Model</span><br/><span class="koboSpan" id="kobo.23.1"> from keras.optimizers import Adam</span><br/><span class="koboSpan" id="kobo.24.1"> from numpy.lib import format</span><br/><span class="koboSpan" id="kobo.25.1"> from sklearn.ensemble import RandomForestClassifier</span><br/><span class="koboSpan" id="kobo.26.1"> from sklearn import linear_model, svm</span><br/><span class="koboSpan" id="kobo.27.1"> from sklearn.model_selection import train_test_split</span><br/><span class="koboSpan" id="kobo.28.1"> import matplotlib.pyplot as plt</span><br/><span class="koboSpan" id="kobo.29.1"> from load_data import *</span><br/><span class="koboSpan" id="kobo.30.1"> import numpy as np</span></pre>
<p><span class="koboSpan" id="kobo.31.1">After importing the necessary libraries, let's look at the </span><kbd><span class="koboSpan" id="kobo.32.1">MalGAN()</span></kbd><span class="koboSpan" id="kobo.33.1"> class definition, beginning with its constructor (the </span><kbd><span class="koboSpan" id="kobo.34.1">__init__()</span></kbd><span class="koboSpan" id="kobo.35.1"> method):</span></p>
<pre><span class="koboSpan" id="kobo.36.1"> class MalGAN():</span><br/><span class="koboSpan" id="kobo.37.1">     def __init__(self):</span><br/><span class="koboSpan" id="kobo.38.1">         self.apifeature_dims = 74</span><br/><span class="koboSpan" id="kobo.39.1">         self.z_dims = 10</span><br/><span class="koboSpan" id="kobo.40.1">         self.hide_layers = 256</span><br/><span class="koboSpan" id="kobo.41.1">         self.generator_layers = [self.apifeature_dims+self.z_dims, self.hide_layers, self.apifeature_dims]</span><br/><span class="koboSpan" id="kobo.42.1">         self.substitute_detector_layers = [self.apifeature_dims, self.hide_layers, 1]</span><br/><span class="koboSpan" id="kobo.43.1">         self.blackbox = 'RF'</span><br/><span class="koboSpan" id="kobo.44.1">         optimizer = Adam(lr=0.001)</span><br/> <br/><span class="koboSpan" id="kobo.45.1">         # Build and Train blackbox_detector</span><br/><span class="koboSpan" id="kobo.46.1">         self.blackbox_detector = self.build_blackbox_detector()</span><br/> <br/><span class="koboSpan" id="kobo.47.1">         # Build and compile the substitute_detector</span><br/><span class="koboSpan" id="kobo.48.1">         self.substitute_detector = self.build_substitute_detector()</span><br/><span class="koboSpan" id="kobo.49.1">         self.substitute_detector.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])</span><br/> <br/><span class="koboSpan" id="kobo.50.1">         # Build the generator</span><br/><span class="koboSpan" id="kobo.51.1">         self.generator = self.build_generator()</span><br/> <br/><span class="koboSpan" id="kobo.52.1">         # The generator takes malware and noise as input and generates adversarial malware examples</span><br/><span class="koboSpan" id="kobo.53.1">         example = Input(shape=(self.apifeature_dims,))</span><br/><span class="koboSpan" id="kobo.54.1">         noise = Input(shape=(self.z_dims,))</span><br/><span class="koboSpan" id="kobo.55.1">         input = [example, noise]</span><br/><span class="koboSpan" id="kobo.56.1">         malware_examples = self.generator(input)</span><br/> <br/><span class="koboSpan" id="kobo.57.1">         # For the combined model we will only train the generator</span><br/><span class="koboSpan" id="kobo.58.1">         self.substitute_detector.trainable = False</span><br/> <br/><span class="koboSpan" id="kobo.59.1">         # The discriminator takes generated images as input and determines validity</span><br/><span class="koboSpan" id="kobo.60.1">         validity = self.substitute_detector(malware_examples)</span><br/> <br/><span class="koboSpan" id="kobo.61.1">         # The combined model  (stacked generator and substitute_detector)</span><br/><span class="koboSpan" id="kobo.62.1">         # Trains the generator to fool the discriminator</span><br/><span class="koboSpan" id="kobo.63.1">         self.combined = Model(input, validity)</span><br/><span class="koboSpan" id="kobo.64.1">         self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.65.1">The </span><kbd><span class="koboSpan" id="kobo.66.1">MalGAN</span></kbd><span class="koboSpan" id="kobo.67.1"> class then provides the methods for building the generator component and the substitute detector, along with the </span><kbd><span class="koboSpan" id="kobo.68.1">blackbox_detector</span></kbd><span class="koboSpan" id="kobo.69.1">:</span></p>
<pre><span class="koboSpan" id="kobo.70.1">     def build_blackbox_detector(self):</span><br/> <br/><span class="koboSpan" id="kobo.71.1">         if self.blackbox is 'RF':</span><br/><span class="koboSpan" id="kobo.72.1">             blackbox_detector = RandomForestClassifier(n_estimators=50, max_depth=5, random_state=1)</span><br/><span class="koboSpan" id="kobo.73.1">         return blackbox_detector</span><br/> <br/><span class="koboSpan" id="kobo.74.1">     def build_generator(self):</span><br/> <br/><span class="koboSpan" id="kobo.75.1">         example = Input(shape=(self.apifeature_dims,))</span><br/><span class="koboSpan" id="kobo.76.1">         noise = Input(shape=(self.z_dims,))</span><br/><span class="koboSpan" id="kobo.77.1">         x = Concatenate(axis=1)([example, noise])</span><br/><span class="koboSpan" id="kobo.78.1">         for dim in self.generator_layers[1:]:</span><br/><span class="koboSpan" id="kobo.79.1">             x = Dense(dim)(x)</span><br/><span class="koboSpan" id="kobo.80.1">         x = Activation(activation='sigmoid')(x)</span><br/><span class="koboSpan" id="kobo.81.1">         x = Maximum()([example, x])</span><br/><span class="koboSpan" id="kobo.82.1">         generator = Model([example, noise], x, name='generator')</span><br/><span class="koboSpan" id="kobo.83.1">         generator.summary()</span><br/><span class="koboSpan" id="kobo.84.1">         return generator</span><br/> <br/><span class="koboSpan" id="kobo.85.1">     def build_substitute_detector(self):</span><br/> <br/><span class="koboSpan" id="kobo.86.1">         input = Input(shape=(self.substitute_detector_layers[0],))</span><br/><span class="koboSpan" id="kobo.87.1">         x = input</span><br/><span class="koboSpan" id="kobo.88.1">         for dim in self.substitute_detector_layers[1:]:</span><br/><span class="koboSpan" id="kobo.89.1">             x = Dense(dim)(x)</span><br/><span class="koboSpan" id="kobo.90.1">         x = Activation(activation='sigmoid')(x)</span><br/><span class="koboSpan" id="kobo.91.1">         substitute_detector = Model(input, x, name='substitute_detector')</span><br/><span class="koboSpan" id="kobo.92.1">         substitute_detector.summary()</span><br/><span class="koboSpan" id="kobo.93.1">         return substitute_detector</span></pre>
<p><span class="koboSpan" id="kobo.94.1">The training of the generator component, along with the training of the </span><kbd><span class="koboSpan" id="kobo.95.1">blackbox</span></kbd><span class="koboSpan" id="kobo.96.1"> and substitute detectors, is implemented in the </span><kbd><span class="koboSpan" id="kobo.97.1">train()</span></kbd><span class="koboSpan" id="kobo.98.1"> method:</span></p>
<pre><span class="koboSpan" id="kobo.99.1">     def train(self, epochs, batch_size=32):</span><br/> <br/><span class="koboSpan" id="kobo.100.1">         # Load the dataset</span><br/><span class="koboSpan" id="kobo.101.1">         (xmal, ymal), (xben, yben) = self.load_data('mydata.npz')</span><br/><span class="koboSpan" id="kobo.102.1">         xtrain_mal, xtest_mal, ytrain_mal, ytest_mal = train_test_split(xmal, ymal, test_size=0.20)</span><br/><span class="koboSpan" id="kobo.103.1">         xtrain_ben, xtest_ben, ytrain_ben, ytest_ben = train_test_split(xben, yben, test_size=0.20)</span><br/> <br/><span class="koboSpan" id="kobo.104.1">         # Train blackbox_detector</span><br/><span class="koboSpan" id="kobo.105.1">         self.blackbox_detector.fit(np.concatenate([xmal, xben]),</span><br/><span class="koboSpan" id="kobo.106.1">                                    np.concatenate([ymal, yben]))</span><br/> <br/><span class="koboSpan" id="kobo.107.1">         ytrain_ben_blackbox = self.blackbox_detector.predict(xtrain_ben)</span><br/><span class="koboSpan" id="kobo.108.1">         Original_Train_TPR = self.blackbox_detector.score(xtrain_mal, ytrain_mal)</span><br/><span class="koboSpan" id="kobo.109.1">         Original_Test_TPR = self.blackbox_detector.score(xtest_mal, ytest_mal)</span><br/><span class="koboSpan" id="kobo.110.1">         Train_TPR, Test_TPR = [Original_Train_TPR], [Original_Test_TPR]</span><br/><span class="koboSpan" id="kobo.111.1">         best_TPR = 1.0</span><br/><span class="koboSpan" id="kobo.112.1">         for epoch in range(epochs):</span><br/> <br/><span class="koboSpan" id="kobo.113.1">             for step in range(xtrain_mal.shape[0] // batch_size):</span><br/><span class="koboSpan" id="kobo.114.1">                 # ---------------------</span><br/><span class="koboSpan" id="kobo.115.1">                 #  Train substitute_detector</span><br/><span class="koboSpan" id="kobo.116.1">                 # ---------------------</span><br/> <br/><span class="koboSpan" id="kobo.117.1">                 # Select a random batch of malware examples</span><br/><span class="koboSpan" id="kobo.118.1">                 idx = np.random.randint(0, xtrain_mal.shape[0], batch_size)</span><br/><span class="koboSpan" id="kobo.119.1">                 xmal_batch = xtrain_mal[idx]</span><br/><span class="koboSpan" id="kobo.120.1">                 noise = np.random.uniform(0, 1, (batch_size, self.z_dims))   #noise as random uniform</span><br/><span class="koboSpan" id="kobo.121.1">                 idx = np.random.randint(0, xmal_batch.shape[0], batch_size)</span><br/><span class="koboSpan" id="kobo.122.1">                 xben_batch = xtrain_ben[idx]</span><br/><span class="koboSpan" id="kobo.123.1">                 yben_batch = ytrain_ben_blackbox[idx]</span><br/> <br/><span class="koboSpan" id="kobo.124.1">                 # Generate a batch of new malware examples</span><br/><span class="koboSpan" id="kobo.125.1">                 gen_examples = self.generator.predict([xmal_batch, noise])</span><br/><span class="koboSpan" id="kobo.126.1">                 ymal_batch = self.blackbox_detector.predict(np.ones(gen_examples.shape)*(gen_examples &gt; 0.5))</span><br/> <br/><span class="koboSpan" id="kobo.127.1">                 # Train the substitute_detector</span><br/><span class="koboSpan" id="kobo.128.1">                 d_loss_real = self.substitute_detector.train_on_batch(gen_examples, ymal_batch)</span><br/><span class="koboSpan" id="kobo.129.1">                 d_loss_fake = self.substitute_detector.train_on_batch(xben_batch, yben_batch)</span><br/><span class="koboSpan" id="kobo.130.1">                 d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)               </span></pre>
<p><span class="koboSpan" id="kobo.131.1">We'll train the generator as follows:</span></p>
<pre><br/><span class="koboSpan" id="kobo.132.1">                 idx = np.random.randint(0, xtrain_mal.shape[0], batch_size)</span><br/><span class="koboSpan" id="kobo.133.1">                 xmal_batch = xtrain_mal[idx]</span><br/><span class="koboSpan" id="kobo.134.1">                 noise = np.random.uniform(0, 1, (batch_size, self.z_dims))</span><br/> <br/><span class="koboSpan" id="kobo.135.1">                 # Train the generator</span><br/><span class="koboSpan" id="kobo.136.1">                 g_loss = self.combined.train_on_batch([xmal_batch, noise], np.zeros((batch_size, 1)))</span><br/> <br/><span class="koboSpan" id="kobo.137.1">             # Compute Train TPR</span><br/><span class="koboSpan" id="kobo.138.1">             noise = np.random.uniform(0, 1, (xtrain_mal.shape[0], self.z_dims))</span><br/><span class="koboSpan" id="kobo.139.1">             gen_examples = self.generator.predict([xtrain_mal, noise])</span><br/><span class="koboSpan" id="kobo.140.1">             TPR = self.blackbox_detector.score(np.ones(gen_examples.shape) * (gen_examples &gt; 0.5), ytrain_mal)</span><br/><span class="koboSpan" id="kobo.141.1">             Train_TPR.append(TPR)</span><br/> <br/><span class="koboSpan" id="kobo.142.1">             # Compute Test TPR</span><br/><span class="koboSpan" id="kobo.143.1">             noise = np.random.uniform(0, 1, (xtest_mal.shape[0], self.z_dims))</span><br/><span class="koboSpan" id="kobo.144.1">             gen_examples = self.generator.predict([xtest_mal, noise])</span><br/><span class="koboSpan" id="kobo.145.1">             TPR = self.blackbox_detector.score(np.ones(gen_examples.shape) * (gen_examples &gt; 0.5), ytest_mal)</span><br/><span class="koboSpan" id="kobo.146.1">             Test_TPR.append(TPR)</span><br/> <br/><span class="koboSpan" id="kobo.147.1">             # Save best model</span><br/><span class="koboSpan" id="kobo.148.1">             if TPR &lt; best_TPR:</span><br/><span class="koboSpan" id="kobo.149.1">                 self.combined.save_weights('saves/malgan.h5')</span><br/><span class="koboSpan" id="kobo.150.1">                 best_TPR = TPR</span></pre>
<p><span class="koboSpan" id="kobo.151.1">To launch the script, we just need to define the </span><kbd><span class="koboSpan" id="kobo.152.1">__main__</span></kbd><span class="koboSpan" id="kobo.153.1"> entry point:</span></p>
<pre><span class="koboSpan" id="kobo.154.1"> if __name__ == '__main__':</span><br/><span class="koboSpan" id="kobo.155.1">     malgan = MalGAN()</span><br/><span class="koboSpan" id="kobo.156.1">     malgan.train(epochs=50, batch_size=64)</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.157.1">Let's now continue illustrating the IDS evasion techniques that leverage the GANs.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">IDS evasion via GAN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have dealt extensively with IDS in </span><a href="a6eab48a-f031-44c9-ae4a-0cfd5db2e05e.xhtml"><span class="koboSpan" id="kobo.3.1">Chapter 5</span></a><span class="koboSpan" id="kobo.4.1">,Â </span><em><span class="koboSpan" id="kobo.5.1">Network Anomaly Detection with AI</span></em><span class="koboSpan" id="kobo.6.1">, where we learned about the delicate role played by these devices in a context like the current one, characterized by a growing explosion of malware threats spread through network attacks.</span></p>
<p><span class="koboSpan" id="kobo.7.1">It is therefore necessary to introduce tools capable of promptly detecting possible malware threats, preventing them from spreading across the entire corporate network, and thereby compromising both the software and the integrity of the data (just think, for example, of the growing diffusion of ransomware attacks).</span></p>
<p><span class="koboSpan" id="kobo.8.1">In order to be able to promptly and effectively carry outâthat is, reduceâthe number of false positives, it is therefore necessary to equip IDS systems with automated procedures capable of adequately classifying the traffic analyzed. </span><span class="koboSpan" id="kobo.8.2">It is no coincidence, therefore, that modern IDSes employ machine learning algorithms, also increasingly resorting to DNNs (such as CNNs, and RNNs) to improve intrusion detection accuracy.</span></p>
<p><span class="koboSpan" id="kobo.9.1">Consequently, not even </span><strong><span class="koboSpan" id="kobo.10.1">intrusion detection systems</span></strong><span class="koboSpan" id="kobo.11.1"> (</span><strong><span class="koboSpan" id="kobo.12.1">IDSes</span></strong><span class="koboSpan" id="kobo.13.1">) can be considered immune to adversarial attacks, generated specifically to deceive the underlying models of the IDS, thereby reducing (or even eliminating) the ability to correctly classify the anomalous traffic.</span></p>
<p><span class="koboSpan" id="kobo.14.1">Despite this, to date, there are still few theoretical studies and software implementations that use adversarial examples to carry out attacks against IDSes.</span></p>
<p><span class="koboSpan" id="kobo.15.1">One of the demonstrations of the possibility of evading IDS detection using GANs is described in the paper </span><em><span class="koboSpan" id="kobo.16.1">IDSGAN: Generative Adversarial Networks for Attack Generation against Intrusion Detection</span></em><span class="koboSpan" id="kobo.17.1">Â (</span><a href="https://arxiv.org/pdf/1809.02077"><span class="koboSpan" id="kobo.18.1">https://arxiv.org/pdf/1809.02077</span></a><span class="koboSpan" id="kobo.19.1">).</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Introducing IDSGAN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Also, in the case of IDSGAN, the type of attack is based on a black-box strategy, in which the implementation details and configuration of the target IDS are unknown.</span></p>
<p><span class="koboSpan" id="kobo.3.1">The underlying GAN of IDSGAN usually </span><span><span class="koboSpan" id="kobo.4.1">includesÂ </span></span><span class="koboSpan" id="kobo.5.1">two antagonistic neural networks in which the generator component takes care of transforming the original network traffic into malicious traffic through the crafting of adversarial examples.</span></p>
<p><span class="koboSpan" id="kobo.6.1">The discriminator component of IDSGAN, on the other hand, deals with correctly classifying the traffic, simulating the black-box detection system, thereby providing the necessary feedback to the generator component for the creation of adversarial examples.</span></p>
<p><span class="koboSpan" id="kobo.7.1">Even in the case of IDSGAN, the adversarial examples generated using the NSL-KDD dataset (</span><a href="http://www.unb.ca/cic/datasets/nsl.html"><span class="koboSpan" id="kobo.8.1">http://www.unb.ca/cic/datasets/nsl.html</span></a><span class="koboSpan" id="kobo.9.1">) show the characteristics of </span><strong><span class="koboSpan" id="kobo.10.1">attack transportability</span></strong><span class="koboSpan" id="kobo.11.1">; that is, they can be reused to attack many detection systems, thereby demonstrating the robustness of the underlying model.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Features of IDSGAN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The main features offered by IDSGAN are as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.3.1">The ability to develop attacks against IDS by emulating their behavior</span></li>
<li><span class="koboSpan" id="kobo.4.1">The ability to take advantage of adversarial examples to make attacks against IDS in black-box mode</span></li>
<li><span class="koboSpan" id="kobo.5.1">The ability to reduce the detection rate of artificially produced traffic to zero</span></li>
<li><span class="koboSpan" id="kobo.6.1">The ability to reuse the adversarial examples generated to attack different types of IDS</span></li>
</ul>
<p><span class="koboSpan" id="kobo.7.1">Now, let's look at the structure of IDSGAN.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The IDSGAN training dataset</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">First of all, IDSGAN uses the NSL-KDD dataset (</span><a href="http://www.unb.ca/cic/datasets/nsl.html"><span class="koboSpan" id="kobo.3.1">http://www.unb.ca/cic/datasets/nsl.html</span></a><span class="koboSpan" id="kobo.4.1">), which contains both malicious and genuine </span><span><span class="koboSpan" id="kobo.5.1">traffic samples</span></span><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">These samples are particularly useful for checking the performance of IDSGAN, as they are also used by common IDS.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The NSL-KDD dataset is then used as a benchmark both to verify the effectiveness of the generator component and to allow the discriminator component to return the feedback required to create the adversarial examples. </span><span class="koboSpan" id="kobo.7.2">Therefore, the choice of the NSL-KDD dataset is not by chance, as the traffic data samples contain both normal and malicious traffic, subdivided into four main categories, such as probing (probe), </span><strong><span class="koboSpan" id="kobo.8.1">denial of service</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong><span class="koboSpan" id="kobo.10.1">DoS</span></strong><span class="koboSpan" id="kobo.11.1">), </span><strong><span class="koboSpan" id="kobo.12.1">user to root</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong><span class="koboSpan" id="kobo.14.1">U2R</span></strong><span class="koboSpan" id="kobo.15.1">), and </span><strong><span class="koboSpan" id="kobo.16.1">root to local</span></strong><span class="koboSpan" id="kobo.17.1"> (</span><strong><span class="koboSpan" id="kobo.18.1">R2L</span></strong><span class="koboSpan" id="kobo.19.1">).</span></p>
<p><span class="koboSpan" id="kobo.20.1">Moreover, the dataset exposes the traffic according to 41 complex features, of which 9 are characterized by discrete values, while the remaining 32 features take continuous values.</span></p>
<p><span class="koboSpan" id="kobo.21.1">These features, in turn, can be divided into the following four types:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.22.1">Intrinsic</span></strong><span class="koboSpan" id="kobo.23.1">: The features reflect the inherent characteristics of a single connection</span></li>
<li><strong><span class="koboSpan" id="kobo.24.1">Content</span></strong><span class="koboSpan" id="kobo.25.1">: The features mark the content of connections that relate to possible attacks</span></li>
<li><strong><span class="koboSpan" id="kobo.26.1">Time based</span></strong><span class="koboSpan" id="kobo.27.1">: The features examine the connections established in the past 2 seconds that have the same destination host or the same service as the current connection</span></li>
<li><strong><span class="koboSpan" id="kobo.28.1">Hosted based</span></strong><span class="koboSpan" id="kobo.29.1">: The features monitor the connections in the past 100 connections that have the same destination host or the same service as the current connection</span></li>
</ul>
<p><span class="koboSpan" id="kobo.30.1">In the data preprocessing phase, particular attention is given to the dimensional impact reduction between feature values. </span><span class="koboSpan" id="kobo.30.2">A normalization method based on the minâmax criterion is used to convert the input data and make it fall within the interval [0, 1], thereby being able to manage both the discrete features and the continuous features.</span></p>
<p><span class="koboSpan" id="kobo.31.1">The mathematical formula used to carry out this normalization is as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.32.1"><img class="fm-editor-equation" src="assets/518c36f9-c301-4cb8-a21a-3c0d01e002be.png" style="width:20.58em;height:1.92em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.33.1">Here, </span><em><span class="koboSpan" id="kobo.34.1">x</span></em><span class="koboSpan" id="kobo.35.1"> represents the feature value before normalization, and </span><em><span class="koboSpan" id="kobo.36.1">xâ²</span></em><span class="koboSpan" id="kobo.37.1"> is the feature value after normalization.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">Once we have analyzed the training dataset and data normalization we can move on to examine the characteristics of the IDSGAN components.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Generator network</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As in all GANs, the generator network is the component responsible for generating the adversarial examples.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In IDSGAN, the generator transforms an original sample of the input traffic, associated with the vector of size </span><em><span class="koboSpan" id="kobo.4.1">m</span></em><span class="koboSpan" id="kobo.5.1">, which represents the characteristics of the original sample, a vector of dimension </span><em><span class="koboSpan" id="kobo.6.1">n</span></em><span class="koboSpan" id="kobo.7.1">, containing noiseâthat is, random numbers extracted from a uniform distribution whose values fall within the range [</span><em><strong><span class="koboSpan" id="kobo.8.1">0</span></strong></em><span class="koboSpan" id="kobo.9.1">, </span><em><strong><span class="koboSpan" id="kobo.10.1">1</span></strong></em><span class="koboSpan" id="kobo.11.1">].</span></p>
<p><span class="koboSpan" id="kobo.12.1">The generator network consists of five layers (with which the ReLU activation function is associated) to manage the output of the internal layers, while the output layer has sufficient units to meet the original </span><em><span class="koboSpan" id="kobo.13.1">m</span></em><span class="koboSpan" id="kobo.14.1">-dimensional sample vector.</span></p>
<p><span class="koboSpan" id="kobo.15.1">As we anticipated, the generator network adjusts its parameters based on the feedback received from the discriminator network (that emulates the behavior of IDS in black-box mode).</span></p>
<p><span class="koboSpan" id="kobo.16.1">Now, let's look at the features of IDSGAN's discriminator component in more detail.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Discriminator network</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We have said that the attack strategy implemented by IDSGAN follows the black-box mode, which means that it is assumed that the attacker has no knowledge of the implementations of the target IDS. </span><span class="koboSpan" id="kobo.2.2">In this sense, the discriminator component of IDSGAN tries to mimic the attacked IDS, classifying the output generated by the generator component by comparing it with the normal traffic examples.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In this way, the discriminator is able to provide the necessary feedback to the generator in order to craft the adversarial examples. </span><span class="koboSpan" id="kobo.3.2">Therefore, the discriminator component consists of a multilayer neural network whose training dataset contains both the normal traffic and the adversarial examples.</span></p>
<p><span class="koboSpan" id="kobo.4.1">The training phases of the discriminator network are therefore as follows:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">The normal samples and the adversarial examples are classified by the IDS</span></li>
<li><span class="koboSpan" id="kobo.6.1">The results of the IDS are used as the target labels of the discriminator</span></li>
<li><span class="koboSpan" id="kobo.7.1">The discriminator mimics the IDS classification using the resulting training dataset</span></li>
</ul>
<p><span class="koboSpan" id="kobo.8.1">The algorithms used to train the generator and discriminator components are outlined in the following sections.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Understanding IDSGAN's algorithm training</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">To train the generator network, the gradients of the results obtained from the classification of adversarial examples by the discriminator network are used. </span><span class="koboSpan" id="kobo.2.2">The objective function, also known as the </span><strong><span class="koboSpan" id="kobo.3.1">loss function</span></strong><span class="koboSpan" id="kobo.4.1">ârepresented by,Â </span><em><span class="koboSpan" id="kobo.5.1">L</span></em><span class="koboSpan" id="kobo.6.1"> in the following equation that the generator network must minimizeâconsists of the following equation:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img class="fm-editor-equation" src="assets/199fefbe-6687-418c-a9c1-c32c1a30b175.png" style="width:20.08em;height:1.83em;"/></span></p>
<p><span><span class="koboSpan" id="kobo.8.1">Here,Â </span><em><span class="koboSpan" id="kobo.9.1">G</span></em><span class="koboSpan" id="kobo.10.1"> and </span><em><span class="koboSpan" id="kobo.11.1">D</span></em><span class="koboSpan" id="kobo.12.1"> represent the generator and the discriminator networks, respectively, while </span><em><span class="koboSpan" id="kobo.13.1">S</span></em></span><em><sub><span class="koboSpan" id="kobo.14.1">attack</span></sub></em> <span><span class="koboSpan" id="kobo.15.1">represents the original malicious samples, with </span><em><span class="koboSpan" id="kobo.16.1">M</span></em><span class="koboSpan" id="kobo.17.1"> and </span><em><span class="koboSpan" id="kobo.18.1">N</span></em><span class="koboSpan" id="kobo.19.1"> representing the </span><em><span class="koboSpan" id="kobo.20.1">m</span></em><span class="koboSpan" id="kobo.21.1">-dimensional vector that matches the original traffic sample and the </span><em><span class="koboSpan" id="kobo.22.1">n</span></em><span class="koboSpan" id="kobo.23.1">-dimensional vector matching the noisy part, respectively.</span></span></p>
<p><span><span class="koboSpan" id="kobo.24.1">In the case of the discriminator network, training takes place by optimizing the objective function represented by the following equation:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.25.1"><img class="fm-editor-equation" src="assets/0be233a6-d951-40e8-b61e-12024feb99b9.png" style="width:23.17em;height:1.67em;"/></span></p>
<p><span class="koboSpan" id="kobo.26.1">As we have seen, the training dataset of the discriminator network consists of both normal samples and adversarial examples, while the target labels are represented by the outputs returned by the IDS.</span></p>
<p><span class="koboSpan" id="kobo.27.1">In the objective function, therefore, </span><em><span class="koboSpan" id="kobo.28.1">s</span></em><span class="koboSpan" id="kobo.29.1"> represents the traffic examples used for the discriminator's training, while </span><em><span class="koboSpan" id="kobo.30.1">B</span><sub><span class="koboSpan" id="kobo.31.1">normal</span></sub></em><span class="koboSpan" id="kobo.32.1"> and </span><em><span class="koboSpan" id="kobo.33.1">B</span></em><sub><em><span class="koboSpan" id="kobo.34.1">attack</span></em><span class="koboSpan" id="kobo.35.1">Â </span></sub><span class="koboSpan" id="kobo.36.1">represent the normal examples and the adversarial examples correctly predicted by the IDS, </span><span><span class="koboSpan" id="kobo.37.1">respectively</span></span><span class="koboSpan" id="kobo.38.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Facial recognition attacks with GAN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As a last example of the use of GANs, we will look at what is perhaps the most symptomatic and well-known case, which involves generating adversarial examples representative of human faces.</span></p>
<p><span class="koboSpan" id="kobo.3.1">Apart from the surprising effect that this technique can have on those who examine the results, which are often very realistic, this technique, when used as an attack tool, constitutes a serious threat to all those cybersecurity procedures based on the verification of biometric evidence (often used to access, for example, online banking services, or, more recently, to log in to social networks, and even access your own smartphone).</span></p>
<p><span class="koboSpan" id="kobo.4.1">Moreover, it can be used to deceive even the AI-empowered facial-recognition tools used by the police to identify suspects, consequently reducing their overall reliability.</span></p>
<p><span class="koboSpan" id="kobo.5.1">As demonstrated in the paper </span><em><span class="koboSpan" id="kobo.6.1">Explaining and Harnessing Adversarial Examples</span></em><span class="koboSpan" id="kobo.7.1">Â (arxiv: 1412.6572, whose authors include Ian Goodfellow, who first introduced GANs to the world), you only need to introduces small perturbation (imperceptible to the human eye) to build artificial images that can fool neural network classifiers.</span></p>
<p><span class="koboSpan" id="kobo.8.1">The following is reproduced from a famous image in which a panda is erroneously classified as a gibbon, due to the effect of the small perturbation injected into the original sample:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.9.1"><img class="aligncenter size-full wp-image-565 image-border" src="assets/b2f8fe03-3026-448f-a10b-23b7297278b8.png" style="width:38.50em;height:15.67em;"/></span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><span class="koboSpan" id="kobo.10.1">(Image taken from the paper entitled </span><em><span class="koboSpan" id="kobo.11.1">Explaining and Harnessing Adversarial Examples</span></em><span class="koboSpan" id="kobo.12.1">Â â 1412.6572)</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Facial recognition vulnerability to adversarial attacks</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The reason why common facial-recognition </span><span><span class="koboSpan" id="kobo.3.1">modelsÂ </span></span><span class="koboSpan" id="kobo.4.1">are vulnerable to adversarial attacks is that two identical CNNs are used, which together constitute a </span><strong><span class="koboSpan" id="kobo.5.1">Siamese network</span></strong><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">In an attempt to calculate the distance between two representative images of the faces to be compared, a CNN is combined with the first image and another CNN is combined with the second image.</span></p>
<p><span class="koboSpan" id="kobo.7.1">The distance calculated between the representationsâalso known as </span><strong><span class="koboSpan" id="kobo.8.1">output embeddings</span></strong><span class="koboSpan" id="kobo.9.1">, formulated by the CNNs in relation to the respective images</span><span><span class="koboSpan" id="kobo.10.1">â</span></span><span class="koboSpan" id="kobo.11.1">is evaluated based on the exceedance of a given threshold value.</span></p>
<p><span class="koboSpan" id="kobo.12.1">The weak link of this facial recognition method is constituted precisely by a correct evaluation of the distance existing between the embedding outputs associated with the individual images, in order to verify the exceedance of the threshold that determines, consequently, the failed matching of the images.</span></p>
<p><span class="koboSpan" id="kobo.13.1">Therefore, an attacker who wants to be recognized in place of the legitimate user, for example, in order to log in to an online banking website or a social network should try to obtain CNN output embeddings by performing an unauthorized access of the database where they are stored.Â Alternatively, the attacker can identify themselves as any user, fooling the Siamese network by leveraging an adversarial example attack.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Adversarial examples against FaceNet</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">An example of an attack that uses adversarial examples to deceive a CNN implementing a facial-recognition model is contained in the CleverHans library (under the examples directory; it is freely available for download atÂ </span><a href="https://github.com/tensorflow/cleverhans/blob/master/examples/facenet_adversarial_faces/facenet_fgsm.py"><span class="koboSpan" id="kobo.3.1">https://github.com/tensorflow/cleverhans/blob/master/examples/facenet_adversarial_faces/facenet_fgsm.py</span></a><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">The example code is released under the MIT license atÂ </span><a href="https://github.com/tensorflow/cleverhans/blob/master/LICENSE"><span class="koboSpan" id="kobo.5.1">https://github.com/tensorflow/cleverhans/blob/master/LICENSE</span></a><span class="koboSpan" id="kobo.6.1">).</span></p>
<p><span class="koboSpan" id="kobo.7.1">The example code shows how to perform an adversarial attack against the FaceNet library, using the </span><kbd><span class="koboSpan" id="kobo.8.1">FGSM</span></kbd><span class="koboSpan" id="kobo.9.1">Â method, obtaining an accuracy in excess of 99%.</span></p>
<p><span class="koboSpan" id="kobo.10.1">Here is the code for the adversarial attack example against the facial-recognition model implemented by the FaceNet library:</span></p>
<pre class="mce-root"><span class="koboSpan" id="kobo.11.1">"""</span><br/><span class="koboSpan" id="kobo.12.1"> Script name: facenet_fgsm.py  </span><br/><span class="koboSpan" id="kobo.13.1"> https://github.com/tensorflow/cleverhans/blob/master/examples/facenet_adversarial_faces/facenet_fgsm.py</span><br/><span class="koboSpan" id="kobo.14.1"> Released under MIT LICENSE:  </span><br/><span class="koboSpan" id="kobo.15.1"> https://github.com/tensorflow/cleverhans/blob/master/LICENSE</span><br/><span class="koboSpan" id="kobo.16.1"> """</span><br/> <br/> <br/><span class="koboSpan" id="kobo.17.1"> import facenet</span><br/><span class="koboSpan" id="kobo.18.1"> import tensorflow as tf</span><br/><span class="koboSpan" id="kobo.19.1"> import numpy as np</span><br/><span class="koboSpan" id="kobo.20.1"> from cleverhans.model import Model</span><br/><span class="koboSpan" id="kobo.21.1"> from cleverhans.attacks import FastGradientMethod</span><br/> <br/><span class="koboSpan" id="kobo.22.1"> import set_loader</span><br/> </pre>
<p><span class="koboSpan" id="kobo.23.1">After loading the necessary libraries, we can delve deeper with the </span><kbd><span class="koboSpan" id="kobo.24.1">InceptionResnetV1Model</span></kbd><span class="koboSpan" id="kobo.25.1"> class definition, which provides us with all the requested methods we need to perform the adversarial attack against the FaceNet library:</span></p>
<pre class="mce-root"><span class="koboSpan" id="kobo.26.1"> class InceptionResnetV1Model(Model):</span><br/><span class="koboSpan" id="kobo.27.1">   model_path = "models/facenet/20170512-110547/20170512-110547.pb"</span><br/> <br/><span class="koboSpan" id="kobo.28.1">   def __init__(self):</span><br/><span class="koboSpan" id="kobo.29.1">     super(InceptionResnetV1Model, self).__init__(scope='model')</span><br/> <br/><span class="koboSpan" id="kobo.30.1">     # Load Facenet CNN</span><br/><span class="koboSpan" id="kobo.31.1">     facenet.load_model(self.model_path)</span><br/><span class="koboSpan" id="kobo.32.1">     # Save input and output tensors references</span><br/><span class="koboSpan" id="kobo.33.1">     graph = tf.get_default_graph()</span><br/><span class="koboSpan" id="kobo.34.1">     self.face_input = graph.get_tensor_by_name("input:0")</span><br/><span class="koboSpan" id="kobo.35.1">     self.embedding_output = graph.get_tensor_by_name("embeddings:0")</span><br/> <br/><span class="koboSpan" id="kobo.36.1">   def convert_to_classifier(self):</span><br/><span class="koboSpan" id="kobo.37.1">     # Create victim_embedding placeholder</span><br/><span class="koboSpan" id="kobo.38.1">     self.victim_embedding_input = tf.placeholder(</span><br/><span class="koboSpan" id="kobo.39.1">         tf.float32,</span><br/><span class="koboSpan" id="kobo.40.1">         shape=(None, 128))</span><br/> <br/><span class="koboSpan" id="kobo.41.1">     # Squared Euclidean Distance between embeddings</span><br/><span class="koboSpan" id="kobo.42.1">     distance = tf.reduce_sum(</span><br/><span class="koboSpan" id="kobo.43.1">         tf.square(self.embedding_output - self.victim_embedding_input),</span><br/><span class="koboSpan" id="kobo.44.1">         axis=1)</span><br/> <br/><span class="koboSpan" id="kobo.45.1">     # Convert distance to a softmax vector</span><br/><span class="koboSpan" id="kobo.46.1">     # 0.99 out of 4 is the distance threshold for the Facenet CNN</span><br/><span class="koboSpan" id="kobo.47.1">     threshold = 0.99</span><br/><span class="koboSpan" id="kobo.48.1">     score = tf.where(</span><br/><span class="koboSpan" id="kobo.49.1">         distance &gt; threshold,</span><br/><span class="koboSpan" id="kobo.50.1">         0.5 + ((distance - threshold) * 0.5) / (4.0 - threshold),</span><br/><span class="koboSpan" id="kobo.51.1">         0.5 * distance / threshold)</span><br/><span class="koboSpan" id="kobo.52.1">     reverse_score = 1.0 - score</span><br/><span class="koboSpan" id="kobo.53.1">     self.softmax_output = tf.transpose(tf.stack([reverse_score, score]))</span><br/> <br/><span class="koboSpan" id="kobo.54.1">     # Save softmax layer</span><br/><span class="koboSpan" id="kobo.55.1">     self.layer_names = []</span><br/><span class="koboSpan" id="kobo.56.1">     self.layers = []</span><br/><span class="koboSpan" id="kobo.57.1">     self.layers.append(self.softmax_output)</span><br/><span class="koboSpan" id="kobo.58.1">     self.layer_names.append('probs')</span><br/> <br/><span class="koboSpan" id="kobo.59.1">   def fprop(self, x, set_ref=False):</span><br/><span class="koboSpan" id="kobo.60.1">     return dict(zip(self.layer_names, self.layers))</span><br/><br/></pre>
<p><span class="koboSpan" id="kobo.61.1">We are now ready to perform our attack, leveraging the FGSM method:</span></p>
<pre class="mce-root"> <br/><span class="koboSpan" id="kobo.62.1"> with tf.Graph().as_default():</span><br/><span class="koboSpan" id="kobo.63.1">   with tf.Session() as sess:</span><br/><span class="koboSpan" id="kobo.64.1">     # Load model</span><br/><span class="koboSpan" id="kobo.65.1">     model = InceptionResnetV1Model()</span><br/><span class="koboSpan" id="kobo.66.1">     # Convert to classifier</span><br/><span class="koboSpan" id="kobo.67.1">     model.convert_to_classifier()</span><br/> <br/><span class="koboSpan" id="kobo.68.1">     # Load pairs of faces and their labels in one-hot encoding</span><br/><span class="koboSpan" id="kobo.69.1">     faces1, faces2, labels = set_loader.load_testset(1000)</span><br/> <br/><span class="koboSpan" id="kobo.70.1">     # Create victims' embeddings using Facenet itself</span><br/><span class="koboSpan" id="kobo.71.1">     graph = tf.get_default_graph()</span><br/><span class="koboSpan" id="kobo.72.1">     phase_train_placeholder = graph.get_tensor_by_name("phase_train:0")</span><br/><span class="koboSpan" id="kobo.73.1">     feed_dict = {model.face_input: faces2,</span><br/><span class="koboSpan" id="kobo.74.1">                  phase_train_placeholder: False}</span><br/><span class="koboSpan" id="kobo.75.1">     victims_embeddings = sess.run(</span><br/><span class="koboSpan" id="kobo.76.1">         model.embedding_output, feed_dict=feed_dict)</span><br/> <br/><span class="koboSpan" id="kobo.77.1">     # Define FGSM for the model</span><br/><span class="koboSpan" id="kobo.78.1">     steps = 1</span><br/><span class="koboSpan" id="kobo.79.1">     eps = 0.01</span><br/><span class="koboSpan" id="kobo.80.1">     alpha = eps / steps</span><br/><span class="koboSpan" id="kobo.81.1">     fgsm = FastGradientMethod(model)</span><br/><span class="koboSpan" id="kobo.82.1">     fgsm_params = {'eps': alpha,</span><br/><span class="koboSpan" id="kobo.83.1">                    'clip_min': 0.,</span><br/><span class="koboSpan" id="kobo.84.1">                    'clip_max': 1.}</span><br/><span class="koboSpan" id="kobo.85.1">     adv_x = fgsm.generate(model.face_input, **fgsm_params)</span><br/> <br/><span class="koboSpan" id="kobo.86.1">     # Run FGSM</span><br/><span class="koboSpan" id="kobo.87.1">     adv = faces1</span><br/><span class="koboSpan" id="kobo.88.1">     for i in range(steps):</span><br/><span class="koboSpan" id="kobo.89.1">       print("FGSM step " + str(i + 1))</span><br/><span class="koboSpan" id="kobo.90.1">       feed_dict = {model.face_input: adv,</span><br/><span class="koboSpan" id="kobo.91.1">                    model.victim_embedding_input: victims_embeddings,</span><br/><span class="koboSpan" id="kobo.92.1">                    phase_train_placeholder: False}</span><br/><span class="koboSpan" id="kobo.93.1">       adv = sess.run(adv_x, feed_dict=feed_dict)</span><br/> </pre>
<p><span class="koboSpan" id="kobo.94.1">As a result, the FGSM will follow two different attack strategies:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.95.1">Impersonation attack (the attack is aimed at impersonating a specific user), using pairs of faces belonging to different individuals</span></li>
<li><span class="koboSpan" id="kobo.96.1">Dodging attack (the attack is aimed at being identified as any possible user), using pairs of faces belonging to the same person</span></li>
</ul>
<p><span class="koboSpan" id="kobo.97.1">Let's now look at how to launch the adversarial attack against FaceNet's CNN.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Launching the adversarial attack against FaceNet's CNN</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In order to run the adversarial attack example against FaceNet's CNN, go through the following steps:</span></p>
<ol>
<li><span class="koboSpan" id="kobo.3.1">Install the FaceNet library, download and align the LFW faces, and download a pretrained FaceNet model as described in the FaceNet tutorial available atÂ </span><a href="https://github.com/davidsandberg/facenet/wiki/Validate-on-LFW"><span class="koboSpan" id="kobo.4.1">https://github.com/davidsandberg/facenet/wiki/Validate-on-LFW</span></a><span class="koboSpan" id="kobo.5.1">.</span></li>
</ol>
<ol start="2">
<li><span class="koboSpan" id="kobo.6.1">Verify that the downloaded datasets and the models' folders are in the same folder of the example.</span></li>
<li><span class="koboSpan" id="kobo.7.1">Edit the following line in the example code, verifying that the name and path of the </span><kbd><span class="koboSpan" id="kobo.8.1">.pb</span></kbd> <span><span class="koboSpan" id="kobo.9.1">file match the path and the filename of the FaceNet model downloaded previously:</span></span></li>
</ol>
<pre style="padding-left: 60px"><strong><span class="koboSpan" id="kobo.10.1">model_path = "models/facenet/20170512-110547/20170512-110547.pb"</span></strong></pre>
<ol start="4">
<li><span class="koboSpan" id="kobo.11.1">Launch the Python script with the following command:</span></li>
</ol>
<pre style="padding-left: 60px"><strong><span class="koboSpan" id="kobo.12.1">python facenet_fgsm.py</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In this chapter, we looked at the attack and defense techniques that exploit the adversarial examples created with GANs.</span></p>
<p><span class="koboSpan" id="kobo.3.1">We looked at the concrete threats that can arise from the use of GANs against DNNs that are increasingly at the heart of cybersecurity procedures, such as malware-detection tools, and biometric authentication. </span><span class="koboSpan" id="kobo.3.2">In addition to the risks associated with the widespread use of NNs in the management of sensitive data, such as health data, these threats lead to new forms of GAN-based attacks that can compromise even the health and physical safety of citizens.</span></p>
<p><span class="koboSpan" id="kobo.4.1">In the next chapter, we will learn how to evaluate algorithms with the help of several examples.</span></p>


            </article>

            
        </section>
    </body></html>