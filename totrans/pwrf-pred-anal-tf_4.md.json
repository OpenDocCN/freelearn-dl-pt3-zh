["```\n    import tensorflow as tf\n    import tensorflow.contrib.slim as slim\n    import numpy as np\n    ```", "```\n    def getBandit(bandit):\n        ''\n        This function creates the reward to the bandits on the basis of randomly generated numbers. It then returns either a positive or negative reward.\n        '' \n        random_number = np.random.randn(1)\n        if random_number > bandit:   \n            return 1\n        else:\n            return -1\n    ```", "```\n    tf.reset_default_graph()\n    ```", "```\n    weight_op = tf.Variable(tf.ones([num_bandits]))\n    action_op = tf.argmax(weight_op,0)\n    ```", "```\n    reward_holder = tf.placeholder(shape=[1],dtype=tf.float32)\n    action_holder = tf.placeholder(shape=[1],dtype=tf.int32)\n    responsible_weight = tf.slice(weight_op,action_holder,[1])\n    ```", "```\n    loss = -(tf.log(responsible_weight)*reward_holder)\n    ```", "```\n    LR = 0.001\n    ```", "```\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate=LR)\n    training_op = optimizer.minimize(loss)\n    ```", "```\n    total_episodes = 10000\n    total_reward = np.zeros(num_bandits) \n    chance_of_random_action = 0.1 \n    ```", "```\n    init_op = tf.global_variables_initializer() \n    ```", "```\n    with tf.Session() as sess:\n        sess.run(init_op)\n        i = 0\n        while i < total_episodes:        \n            if np.random.rand(1) < chance_of_random_action:\n                action = np.random.randint(num_bandits)\n            else:\n                action = sess.run(action_op)\n                    reward = getBandit(bandits[action])         \n                _,resp,ww = sess.run([training_op,responsible_weight,weight_op], feed_dict={reward_holder:[reward],action_holder:[action]})\n            total_reward[action] += reward\n            if i % 50 == 0:\n                print(\"Running reward for all the \" + str(num_bandits) + \" bandits: \" + str(total_reward))\n            i+=1\n    ```", "```\n    print(\"The agent thinks bandit \" + str(np.argmax(ww)+1) + \" would be the most efficient one.\")\n    if np.argmax(ww) == np.argmax(-np.array(bandits)):\n        print(\" and it was right at the end!\")\n    else:\n        print(\" and it was wrong at the end!\")\n    >>>\n    ```", "```\n    Running reward for all the 4 bandits: [-1\\. 0\\. 0\\. 0.]\n    Running reward for all the 4 bandits: [ -1\\. -2\\. 14\\. 0.]\n    …\n    Running reward for all the 4 bandits: [ -15\\. -7\\. 340\\. 21.]\n    Running reward for all the 4 bandits: [ -15\\. -10\\. 364\\. 22.]\n    The agent thinks Bandit 3 would be the most efficient one and it was wrong at the end!\n    ```", "```\n    Running reward for all the 4 bandits: [ 1\\. 0\\. 0\\. 0.]\n    Running reward for all the 4 bandits: [ -1\\. 11\\. -3\\. 0.]\n    Running reward for all the 4 bandits: [ -2\\. 1\\. -2\\. 20.]\n    …\n    Running reward for all the 4 bandits: [ -7\\. -2\\. 8\\. 762.]\n    Running reward for all the 4 bandits: [ -8\\. -3\\. 8\\. 806.]\n    The agent thinks Bandit 4 would be the most efficient one and it was right at the end!\n    ```", "```\n    class contextualBandit():\n        def __init__(self):\n            self.state = 0        \n            self.bandits = np.array([[0.2,0,-0.0,-5], [0.1,-5,1,0.25], [0.3,0.4,-5,0.5], [-5,5,5,5]])\n            self.num_bandits = self.bandits.shape[0]\n            self.num_actions = self.bandits.shape[1]\n            def getBandit(self):        \n            '''\n            This function returns a random state for each episode.\n            '''\n            self.state = np.random.randint(0, len(self.bandits)) \n            return self.state\n            def pullArm(self,action):        \n            '''\n            This function creates the reward to the bandits on the basis of randomly generated numbers. It then returns either a positive or negative reward that is action\n            ''' \n            bandit = self.bandits[self.state, action]\n            result = np.random.randn(1)\n            if result > bandit:\n                return 1\n            else:\n                return -1\n    ```", "```\n    class ContextualAgent():\n        def __init__(self, lr, s_size,a_size):\n            '''\n            This function establishes the feed-forward part of the network. The agent takes a state and produces an action -that is. contextual agent\n            ''' \n            self.state_in= tf.placeholder(shape=[1], dtype=tf.int32)\n            state_in_OH = slim.one_hot_encoding(self.state_in, s_size)\n            output = slim.fully_connected(state_in_OH, a_size,biases_initializer=None, activation_fn=tf.nn.sigmoid, weights_initializer=tf.ones_initializer())\n            self.output = tf.reshape(output,[-1])\n            self.chosen_action = tf.argmax(self.output,0)\n            self.reward_holder = tf.placeholder(shape=[1], dtype=tf.float32)\n            self.action_holder = tf.placeholder(shape=[1], dtype=tf.int32)\n            self.responsible_weight = tf.slice(self.output, self.action_holder,[1])\n            self.loss = -(tf.log(self.responsible_weight)*self.reward_holder)\n            optimizer = tf.train.GradientDescentOptimizer(learning_rate=lr)\n            self.update = optimizer.minimize(self.loss)\n    ```", "```\n    tf.reset_default_graph()\n    ```", "```\n    lrarning_rate = 0.001 # learning rate \n    chance_of_random_action = 0.1 # Chance of a random action.\n    max_iteration = 10000 #Max iteration to train the agent.\n    ```", "```\n    contextualBandit = contextualBandit() #Load the bandits.\n    contextualAgent = ContextualAgent(lr=lrarning_rate, s_size=contextualBandit.num_bandits, a_size=contextualBandit.num_actions) #Load the agent.\n    ```", "```\n    weights = tf.trainable_variables()[0] \n    total_reward = np.zeros([contextualBandit.num_bandits,contextualBandit.num_actions])\n    ```", "```\n    init_op = tf.global_variables_initializer()\n    ```", "```\n    with tf.Session() as sess:\n        sess.run(init_op)\n        i = 0\n        while i < max_iteration:\n            s = contextualBandit.getBandit() #Get a state from the environment.\n            #Choose a random action or one from our network.\n            if np.random.rand(1) < chance_of_random_action:\n                action = np.random.randint(contextualBandit.num_actions)\n            else:\n                action = sess.run(contextualAgent.chosen_action,feed_dict={contextualAgent.state_in:[s]})\n            reward = contextualBandit.pullArm(action) #Get our reward for taking an action given a bandit.\n            #Update the network.\n            feed_dict={contextualAgent.reward_holder:[reward],contextualAgent.action_\n    holder:[action],contextualAgent.state_in:[s]}\n            _,ww = sess.run([contextualAgent.update,weights], feed_dict=feed_dict)        \n            #Update our running tally of scores.\n            total_reward[s,action] += reward\n            if i % 500 == 0:\n                print(\"Mean reward for each of the \" + str(contextualBandit.num_bandits) + \" bandits: \" + str(np.mean(total_reward,axis=1)))\n            i+=1\n    >>>\n    Mean reward for each of the 4 bandits: [ 0\\. 0\\. -0.25 0\\. ]\n    Mean reward for each of the 4 bandits: [ 25.75 28.25 25.5 28.75]\n    …\n    Mean reward for each of the 4 bandits: [ 488.25 489\\. 473.5 440.5 ]\n    Mean reward for each of the 4 bandits: [ 518.75 520\\. 499.25 465.25]\n    Mean reward for each of the 4 bandits: [ 546.5 547.75 525.25 490.75]\n    ```", "```\n    right_flag = 0\n    wrong_flag = 0\n    ```", "```\n    for a in range(contextualBandit.num_bandits):\n        print(\"The agent thinks action \" + str(np.argmax(ww[a])+1) + \" for bandit \" + str(a+1) + \" would be the most efficient one.\")\n        if np.argmax(ww[a]) == np.argmin(contextualBandit.bandits[a]):\n            right_flag += 1\n            print(\" and it was right at the end!\")\n        else:\t\n            print(\" and it was wrong at the end!\")\n            wrong_flag += 1\n    >>>\n    The agent thinks action 4 for Bandit 1 would be the most efficient one and it was right at the end!\n    The agent thinks action 2 for Bandit 2 would be the most efficient one and it was right at the end!\n    The agent thinks action 3 for Bandit 3 would be the most efficient \n    ne and it was right at the end!\n    The agent thinks action 1 for Bandit 4 would be the most efficient one and it was right at the end!\n    ```", "```\n    prediction_accuracy = (right_flag/right_flag+wrong_flag)\n    print(\"Prediction accuracy (%):\", prediction_accuracy * 100)\n    >>>\n    Prediction accuracy (%): 100.0\n    ```", "```\n>>> from yahoo_finance import Share\n>>> msoft = Share('MSFT')\n>>> print(msoft.get_open())\n72.24=\n>>> print(msoft.get_price())\n72.78\n>>> print(msoft.get_trade_datetime())\n2017-07-14 20:00:00 UTC+0000\n>>>\n```", "```\n$ sudo pip3 install yahoo_finance \n\n```", "```\ndef get_prices(share_symbol, start_date, end_date, cache_filename):\n    try:\n        stock_prices = np.load(cache_filename)\n    except IOError:\n        share = Share(share_symbol)\n        stock_hist = share.get_historical(start_date, end_date)\n        stock_prices = [stock_price['Open'] for stock_price in stock_hist]\n        np.save(cache_filename, stock_prices)\n    return stock_prices\n```", "```\ndef plot_prices(prices):\n    plt.title('Opening stock prices')\n    plt.xlabel('day')\n    plt.ylabel('price ($)')\n    plt.plot(prices)\n    plt.savefig('prices.png')\n```", "```\nif __name__ == '__main__':\n    prices = get_prices('MSFT', '2000-07-01', '2017-07-01', 'historical_stock_prices.npy')\n    plot_prices(prices)\n```", "```\nclass DecisionPolicy:\n    def select_action(self, current_state, step):\n        pass\n    def update_q(self, state, action, reward, next_state):\n        pass\n```", "```\nclass RandomDecisionPolicy(DecisionPolicy):\n    def __init__(self, actions):\n        self.actions = actions\n    def select_action(self, current_state, step):\n        action = self.actions[random.randint(0, len(self.actions) - 1)]\n        return action\n```", "```\nportfolio = budget + number of stocks * share value\nreward = new_portfolio - current_portfolio\n```", "```\ndef run_simulation(policy, initial_budget, initial_num_stocks, prices, hist, debug=False):\n    budget = initial_budget\n    num_stocks = initial_num_stocks\n    share_value = 0\n    transitions = list()\n    for i in range(len(prices) - hist - 1):\n        if i % 100 == 0:\n            print('progress {:.2f}%'.format(float(100*i) / (len(prices) - hist - 1)))\n        current_state = np.asmatrix(np.hstack((prices[i:i+hist], budget, num_stocks)))\n        current_portfolio = budget + num_stocks * share_value\n        action = policy.select_action(current_state, i)\n        share_value = float(prices[i + hist + 1])\n        if action == 'Buy' and budget >= share_value:\n            budget -= share_value\n            num_stocks += 1\n        elif action == 'Sell' and num_stocks > 0:\n            budget += share_value\n            num_stocks -= 1\n        else:\n            action = 'Hold'\n        new_portfolio = budget + num_stocks * share_value\n        reward = new_portfolio - current_portfolio\n        next_state = np.asmatrix(np.hstack((prices[i+1:i+hist+1], budget, num_stocks)))\n        transitions.append((current_state, action, reward, next_state))\n        policy.update_q(current_state, action, reward, next_state)\n    portfolio = budget + num_stocks * share_value\n    if debug:\n        print('${}\\t{} shares'.format(budget, num_stocks))\n    return portfolio\n```", "```\ndef run_simulations(policy, budget, num_stocks, prices, hist):\n    num_tries = 100\n    final_portfolios = list()\n    for i in range(num_tries):\n        final_portfolio = run_simulation(policy, budget, num_stocks, prices, hist)\n        final_portfolios.append(final_portfolio)\n    avg, std = np.mean(final_portfolios), np.std(final_portfolios)\n    return avg, std\n```", "```\nactions = ['Buy', 'Sell', 'Hold']\n    hist = 200\n    policy = RandomDecisionPolicy(actions)\n    budget = 1000.0\n    num_stocks = 0\n    avg,std=run_simulations(policy,budget,num_stocks,prices, hist)\n    print(avg, std)\n>>>\n1512.87102405 682.427384814\n```", "```\n>>> \n1518.12039077 603.15350649 \n```", "```\n    class QLearningDecisionPolicy(DecisionPolicy):\n        def __init__(self, actions, input_dim):\n            self.epsilon = 0.9\n            self.gamma = 0.001\n            self.actions = actions\n            output_dim = len(actions)\n            h1_dim = 200\n            self.x = tf.placeholder(tf.float32, [None, input_dim])\n            self.y = tf.placeholder(tf.float32, [output_dim])\n            W1 = tf.Variable(tf.random_normal([input_dim, h1_dim]))\n            b1 = tf.Variable(tf.constant(0.1, shape=[h1_dim]))\n            h1 = tf.nn.relu(tf.matmul(self.x, W1) + b1)\n            W2 = tf.Variable(tf.random_normal([h1_dim, output_dim]))\n            b2 = tf.Variable(tf.constant(0.1, shape=[output_dim]))\n            self.q = tf.nn.relu(tf.matmul(h1, W2) + b2)\n            loss = tf.square(self.y - self.q)\n            self.train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n            self.sess = tf.Session()\n            self.sess.run(tf.initialize_all_variables())\n        def select_action(self, current_state, step):\n            threshold = min(self.epsilon, step / 1000.)\n            if random.random() < threshold:\n                # Exploit best option with probability epsilon\n                action_q_vals = self.sess.run(self.q, feed_dict={self.x: current_state})\n                action_idx = np.argmax(action_q_vals)  \n                action = self.actions[action_idx]\n            else:\n                # Random option with probability 1 - epsilon\n                action = self.actions[random.randint(0, len(self.actions) - 1)]\n            return action\n        def update_q(self, state, action, reward, next_state):\n            action_q_vals = self.sess.run(self.q, feed_dict={self.x: state})\n            next_action_q_vals = self.sess.run(self.q, feed_dict={self.x: next_state})\n            next_action_idx = np.argmax(next_action_q_vals)\n            action_q_vals[0, next_action_idx] = reward + self.gamma * next_action_q_vals[0, next_action_idx]\n    ```", "```\n        action_q_vals = np.squeeze(np.asarray(action_q_vals))\n        self.sess.run(self.train_op, feed_dict={self.x: state, self.y: action_q_vals})\n```"]