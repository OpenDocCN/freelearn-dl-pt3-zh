- en: Solving Problems with Dynamic Programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purposes of this chapter are manifold. We will introduce many topics that
    are essential to the understanding of reinforcement problems and the first algorithms
    that are used to solve them. Whereas, in the previous chapters, we talked about
    **reinforcement learning** (**RL**) from a broad and non-technical point of view,
    here, we will formalize this understanding to develop the first algorithms to
    solve a simple game.
  prefs: []
  type: TYPE_NORMAL
- en: The RL problem can be formulated as a **Markov decision process** (**MDP**),
    a framework that provides a formalization of the key elements of RL, such as value
    functions and the expected reward. RL algorithms can then be created using these
    mathematical components. They differ from each other by how these components are
    combined and on the assumptions made while designing them.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, as we'll see in this chapter, RL algorithms can be categorized
    into three main categories that can overlap each other. This is because some algorithms
    can unify characteristics from more than one category. Once these pivotal concepts
    have been explained, we'll present the first type of algorithm, called dynamic
    programming, which can solve problems when given complete information about the
    environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: MDP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Categorizing RL algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MDP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An MDP expresses the problem of sequential decision-making, where actions influence
    the next states and the results. MDPs are general and flexible enough to provide
    a formalization of the problem of learning a goal through interactions, the same
    problem that is addressed with RL. Thus we can express and reason with RL problems
    in terms of MDPs.
  prefs: []
  type: TYPE_NORMAL
- en: 'An MDP is four-tuple (S,A,P,R):'
  prefs: []
  type: TYPE_NORMAL
- en: '*S* is the state space, with a finite set of states.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A* is the action space, with a finite set of actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*P* is a transition function, which defines the probability of reaching a state,
    *s′*, from *s* through an action, *a*. In *P(s′, s, a) = p(s′| s, a)*, the transition
    function is equal to the conditional probability of *s′* given *s* and *a*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*R* is the reward function, which determines the value received for transitioning
    to state *s′* after taking action *a* from state *s*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An illustration of an MDP is given in the following diagram. The arrows represent
    the transitions between two states, with the transition probabilities attached
    to the tail of the arrows and the rewards on the body of the arrows. For their
    properties, the transition probabilities of a state must add up to 1\. In this
    example, the final state is represented with a square (state *S[5]*) and for simplicity,
    we have represented an MDP with a single action:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aacec67b-f1bb-4c5b-b003-716c51e2b48e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 Example of an MDP with five states and one action
  prefs: []
  type: TYPE_NORMAL
- en: The MDP is controlled by a sequence of discrete time steps that create a trajectory
    of states and actions (*S[0], A[0], S[1], A[1], ...*), where the states follow
    the dynamics of the MDP, namely the state transition function, *p(s′|s, a)*. In
    this way, the transition function fully characterizes the environment's dynamics.
  prefs: []
  type: TYPE_NORMAL
- en: By definition, the transition function and the reward function are determined
    only by the current state, and not from the sequence of the previous states visited.
    This property is called the **Markov property**, which means that the process
    is memory-less and the future state depends only on the current one, and not on
    its history. Thus, a state holds all the information. A system with such a property
    is called **fully observable**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In many practical RL cases, the Markov property does not hold up, and for practicality,
    we can get around the problem by assuming it is an MDP and using a finite number
    of previous states (a finite history): *S[t]*, *S[t-1]*, *S[t-2]*, ..., *S*[*t-k*]. In
    this case, the system is **partially observable** and the states are called **observations**.
    We''ll use this strategy in the Atari games, where we''ll use row pixels as the
    input of the agent. This is because the single frame is static and does not carry
    information about the speed or direction of the objects. Instead, these values
    can be retrieved using the previous three or four frames (it is still an approximation).'
  prefs: []
  type: TYPE_NORMAL
- en: The final objective of an MDP is to find a policy, π, that maximizes the cumulative
    reward, [![](img/b27acbaf-de95-4dc7-b6e9-bd508392ce23.png)], where *R[π]* is the
    reward obtained at each step by following the policy, π. A solution of an MDP
    is found when a policy takes the best possible action in each state of the MDP.
    This policy is known as the **optimal** **policy**.
  prefs: []
  type: TYPE_NORMAL
- en: Policy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The policy chooses the actions to be taken in a given situation and can be categorized
    as deterministic or stochastic.
  prefs: []
  type: TYPE_NORMAL
- en: A deterministic policy is denoted as *a[t] = µ(st)*, while a stochastic policy
    can be denoted as *a[t] ~ π(.|s[t])*, where the tilde symbol (~) means **has distribution**.
    Stochastic policies are used when it is better to consider an action distribution;
    for example, when it is preferable to inject a noisy action into the system.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, stochastic policies can be categorical or Gaussian. The former case
    is similar to a classification problem and is computed as a softmax function across
    the categories. In the latter case, the actions are sampled from a Gaussian distribution,
    described by a mean and a standard deviation (or variance). These parameters can
    also be functions of states.
  prefs: []
  type: TYPE_NORMAL
- en: When using parameterized policies, we'll define them with the letter *θ*. For
    example, in the case of a deterministic policy, it would be written as *µ[θ] (s[t])*.
  prefs: []
  type: TYPE_NORMAL
- en: Policy, decision-maker, and agent are three terms that express the same concept,
    so, in this book, we'll use these terms interchangeably.
  prefs: []
  type: TYPE_NORMAL
- en: Return
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When running a policy in an MDP, the sequence of state and action (*S[0]*, *A[0]*, *S[1]*, *A[1]*,
    ...) is called **trajectory** or **rollout***,* and is denoted by ![](img/7e13ad77-456c-493a-8343-9ffc8a8bf955.png). In
    each trajectory, a sequence of rewards will be collected as a result of the actions.
    A function of these rewards is called **return** and in its most simplified version,
    it is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/263dd699-3fe4-4a42-a6ec-3c5ee28e18e4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this point, the return can be analyzed separately for trajectories with
    infinite and finite horizons. This distinction is needed because in the case of
    interactions within an environment that do not terminate, the sum previously presented
    will always have an infinite value. This situation is dangerous because it doesn''t
    provide any information. Such tasks are called continuing tasks and need another
    formulation of the reward. The best solution is to give more weight to the short-term
    rewards while giving less importance to those in the distant future. This is accomplished
    by using a value between 0 and 1 called the **discount factor** denoted with the
    symbol λ*. *Thus, the return **G** can be reformulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e72838f1-d23d-4c2d-a455-08853a791dc4.png)'
  prefs: []
  type: TYPE_IMG
- en: This formula can be viewed as a way to prefer actions that are closer in time
    with respect to those that will be encountered in the distant future. Take this
    example—you win the lottery and you can decide when you would like to collect
    the prize. You would probably prefer to collect it within a few days rather than
    in a few years. ![](img/f64405b5-36bf-47d8-8d63-38e0da4636d5.png) is the value
    that defines how long you are willing to wait to collect the prize. If ![](img/15638a94-11a5-4f72-8968-90fede169fa4.png), that
    means that you are not bothered about when you collect the prize. If [![](img/aa898182-0ce4-47e4-82fc-51be617d1fe8.png)],
    that means that you want it immediately.
  prefs: []
  type: TYPE_NORMAL
- en: 'In cases of trajectories with a finite horizon, meaning trajectories with a
    natural ending, tasks are called **episodic** (it derives from the term episode,
    which is another word for trajectory). In episodic tasks, the original formula
    (1) works, but nevertheless, it is preferred to have a variation of it with the
    discount factor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2ced63d-7d0b-4d81-8688-db95ece7a973.png)'
  prefs: []
  type: TYPE_IMG
- en: With a finite but long horizon, the use of a discount factor increases the stability
    of algorithms, considering that long future rewards are only partially considered. In
    practice, discount factor values between 0.9 and 0.999 are used.
  prefs: []
  type: TYPE_NORMAL
- en: 'A trivial but very useful decomposition of formula (3) is the definition of
    return in terms of the return at timestep *t + 1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32c5321a-1dee-4421-ad5c-b9c7c7efc629.png)'
  prefs: []
  type: TYPE_IMG
- en: 'When simplifying the notation, it becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c1de6332-c6af-457a-b6cd-f721ea802575.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, using the return notation, we can define the goal of RL to find an optimal
    policy, ![](img/14497f73-ee08-44a6-b1e7-74e633c68357.png), that maximizes the
    expected return as [![](img/36d54163-a38b-424c-84d1-5bb6dccabbec.png)], where [![](img/abb94ffb-0ccf-4754-b31e-eed001662724.png)] is
    the expected value of a random variable.
  prefs: []
  type: TYPE_NORMAL
- en: Value functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The return [![](img/e9046fe7-5a5d-46e6-90d1-c519d768a125.png)] provides a good
    insight into the trajectory''s value, but still, it doesn''t give any indication
    of the quality of the single states visited. This quality indicator is important
    because it can be used by the policy to choose the next best action. The policy
    has to just choose the action that will result in the next state with the highest
    quality. The **value function** does exactly this: it estimates the **quality**
    in terms of the expected return from a state following a policy. Formally, the
    value function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/803e2f8b-f844-4f77-976a-56019065ffbf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The **action-value function**, similar to the value function, is the expected
    return from a state but is also conditioned on the first action. It is defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a4ec12a-7d62-4c8c-b3b5-8c19daafaddc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The value function and action-value function are also called the **V-function**
    and **Q-function **respectively, and are strictly correlated with each other since
    the value function can also be defined in terms of the action-value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/488cf9ca-f963-4c79-a1e5-5523c6c50d75.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Knowing the optimal [![](img/8a9c0fe1-da0e-43e5-9e66-e7b81af625a9.png)], the
    optimal value function is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/538c8450-86a3-4138-a125-112d7dd8b047.png)'
  prefs: []
  type: TYPE_IMG
- en: That's because the optimal action is [![](img/9e78caff-8401-4f9a-a736-3076e1ec787a.png)].
  prefs: []
  type: TYPE_NORMAL
- en: Bellman equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**V** and **Q** can be estimated by running trajectories that follow the policy, ![](img/28b05043-d213-4654-8fb0-f170fdcdb514.png),
    and then averaging the values obtained. This technique is effective and is used
    in many contexts, but is very expensive considering that the return requires the
    rewards from the full trajectory.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Luckily, the Bellman equation defines the action-value function and the value
    function recursively, enabling their estimations from subsequent states. The Bellman
    equation does that by using the reward obtained in the present state and the value
    of its successor state. We already saw the recursive formulation of the return
    (in formula (5)) and we can apply it to the state value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11c116e3-4fc4-42f3-830a-e709ef88eb8e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Similarly, we can adapt the Bellman equation for the action-value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fd00dc43-1da2-4a23-9d9d-224e4e9d5294.png)'
  prefs: []
  type: TYPE_IMG
- en: Now, with (6) and (7), [![](img/14df8976-b491-4874-bf26-1879dfdc30fb.png)] and [![](img/825e2f3a-c490-4ee0-9c32-aaed59534c36.png)]
    are updated only with the values of the successive states, without the need to
    unroll the trajectory to the end, as required in the old definition.
  prefs: []
  type: TYPE_NORMAL
- en: Categorizing RL algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before deep diving into the first RL algorithm that solves the optimal Bellman
    equation, we want to give a broad but detailed overview of RL algorithms. We need
    to do this because their distinctions can be quite confusing. There are many parts
    involved in the design of algorithms, and many characteristics have to be considered
    before deciding which algorithm best fits the actual needs of the user. The scope
    of this overview presents the big picture of RL so that in the next chapters,
    where we'll give a comprehensive theoretical and practical view of these algorithms,
    you will already see the general objective and have a clear idea of their location
    in the map of RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The first distinction is between model-based and model-free algorithms. As the
    name suggests, the first requires a model of the environment, while the second
    is free from this condition. The model of the environment is highly valuable because
    it carries precious information that can be used to find the desired policies;
    however, in most cases, the model is almost impossible to obtain. For example,
    it can be quite easy to model the game tic-tac-toe, while it can be difficult
    to model the waves of the sea. To this end, model-free algorithms can learn information
    without any assumptions about the environment. A representation of the categories
    of RL algorithms is visible in figure 3.2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here the distinction is shown between model-based and model-free, and two widely
    known model-free approaches, namely policy gradient and value-based. Also, as
    we''ll see in later chapters, a combination of those is possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a612c49f-f471-4f07-9d64-dc5dbe0377ca.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2\. Categorization of RL algorithms
  prefs: []
  type: TYPE_NORMAL
- en: The first distinction is between model-free and model-based. Model-free RL algorithms
    can be further decomposed in policy gradient and value-based algorithms. Hybrids
    are methods that combine important characteristics of both methods.
  prefs: []
  type: TYPE_NORMAL
- en: Model-free algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the absence of a model, **model-free** (**MF**) algorithms run trajectories
    within a given policy to gain experience and to improve the agent. MF algorithms
    are made up of three main steps that are repeated until a good policy is created:'
  prefs: []
  type: TYPE_NORMAL
- en: The generation of new samples by running the policy in the environment. The
    trajectories are run until a final state is reached or for a fixed number of steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The estimation of the return function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The improvement of the policy using the samples collected, and the estimation
    done in step 2.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These three components are at the heart of this type of algorithm, but based
    on how each step is performed, they generate different algorithms. Value-based
    algorithms and policy gradient algorithms are two such examples. They seem to
    be very different, but they are based on similar principles and both use the three-step
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: Value-based algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Value-based algorithms, also known as **value function algorithms**, use a paradigm
    that's very similar to the one we saw in the previous section. That is, they use
    the Bellman equation to learn the Q-function, which in turn is used to learn a
    policy. In the most common setting, they use deep neural networks as a function
    approximator and other tricks to deal with high variance and general instabilities.
    To a certain degree, value-based algorithms are closer to supervised regression
    algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, these algorithms are off-policy, meaning they are not required to
    optimize the same policy that was used to generate the data. This means that these
    methods can learn from previous experience, as they can store the sampled data
    in a replay buffer. The ability to use previous samples makes the value function
    more sample-efficient than other model-free algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Policy gradient algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The other family of MF algorithms is that of the policy gradient methods (or
    policy optimization methods). They have a more direct and obvious interpretation
    of the RL problem, as they learn directly from a parametric policy by updating
    the parameters in the direction of the improvements. It's based on the RL principle
    that good actions should be encouraged (by boosting the gradient of the policy
    upward) while discouraging bad actions.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to value function algorithms, policy optimization mainly requires on-policy
    data, making these algorithms more sample inefficient. Policy optimization methods
    can be quite unstable due to the fact that taking the steepest ascent in the presence
    of surfaces with high curvature can easily result in moving too far in any given
    direction, falling down into a bad region. To address this problem, many algorithms
    have been proposed, such as optimizing the policy only within a trust region,
    or optimizing a surrogate clipped objective function to limit changes to the policy.
  prefs: []
  type: TYPE_NORMAL
- en: A major advantage of policy gradient methods is that they easily handle environments
    with continuous action spaces. This is a very difficult thing to approach with
    value function algorithms as they learn Q-values for discrete pairs of states
    and actions.
  prefs: []
  type: TYPE_NORMAL
- en: Actor-Critic algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Actor-Critic** (**AC**) algorithms are on-policy policy gradient algorithms
    that also learn a value function (generally a Q-function) called a critic to provide
    feedback to the policy, the actor. Imagine that you, the actor, want to go to
    the supermarket via a new route. Unfortunately, before arriving at the destination,
    your boss calls you requiring you to go back to work. Because you didn''t reach
    the supermarket, you don''t know if the new road is actually faster than the old
    one. But if you reached a familiar location, you can estimate the time you''ll
    need to go from there to the supermarket and calculate whether the new path is
    preferable. This estimate is what the critic does. In this way, you can improve
    the actor even though you didn''t reach the final goal.'
  prefs: []
  type: TYPE_NORMAL
- en: Combining a critic with an actor has been shown to be very effective and is
    commonly used in policy gradient algorithms. This technique can also be combined
    with other ideas used in policy optimization, such as trust-region algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advantages of both value functions and policy gradient algorithms can be merged,
    creating hybrid algorithms that can be more sample efficient and robust.
  prefs: []
  type: TYPE_NORMAL
- en: Hybrid approaches combine Q-functions and policy gradients to symbiotically
    and mutually improve each other. These methods estimate the expected Q-function
    of deterministic actions to directly improve the policy.
  prefs: []
  type: TYPE_NORMAL
- en: Be aware that because AC algorithms learn and use a value function, they are
    categorized as policy gradients and not as hybrid algorithms. This is because
    the main underlying objective is that of policy gradient methods. The value function
    is only an upgrade to provide additional information.
  prefs: []
  type: TYPE_NORMAL
- en: Model-based RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having a model of the environment means that the state transitions and the rewards
    can be predicted for each state-action tuple (without any interaction with the
    real environment). As we already mentioned, the model is known only in limited
    cases, but when it is known, it can be used in many different ways. The most obvious
    application of the model is to use it to plan future actions. Planning is a concept
    used to express the organization of future moves when the consequences of the
    next actions are already known. For example, if you know exactly what moves your
    enemy will make, you can think ahead and plan all your actions before executing
    the first one. As a downside, planning can be very expensive and isn't a trivial
    process.
  prefs: []
  type: TYPE_NORMAL
- en: A model can also be learned through interactions with the environment, assimilating
    the consequences (both in terms of the states and rewards) of an action. This
    solution is not always the best one because teaching a model could be terribly
    expensive in the real world. Moreover, if only a rough approximation of the environment
    is understood by the model, it could lead to disastrous results.
  prefs: []
  type: TYPE_NORMAL
- en: A model, whether known or learned, can be used both to plan and to improve the
    policy, and can be integrated into different phases of an RL algorithm. Well-known
    cases of model-based RL involve pure planning, embedded planning to improve the
    policy, and generated samples from an approximate model.
  prefs: []
  type: TYPE_NORMAL
- en: A set of algorithms that use a model to estimate a value function is called
    **dynamic programming** (**DP**) and will be studied later in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm diversity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why are there so many types of RL algorithms? This is because there isn't one
    that is better than all the others in every context. Each one is designed for
    different needs and to take care of different aspects. The most notable differences
    are stability, sample efficiency, and wall clock time (training time). These will
    be more clear as we progress through the book but as a rule of thumb, policy gradient
    algorithms are more stable and reliable than value function algorithms. On the
    other hand, value function methods are more sample efficient as they are off-policy
    and can use prior experience. In turn, model-based algorithms are more sample
    efficient than Q-learning algorithms but their computational cost is much higher
    and they are slower.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the ones just presented, there are other trade-offs that have to be
    taken into consideration while designing and deploying an algorithm (such as ease
    of use and robustness), which is not a trivial process.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic programming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DP is a general algorithmic paradigm that breaks up a problem into smaller chunks
    of overlapping subproblems, and then finds the solution to the original problem
    by combining the solutions of the subproblems.
  prefs: []
  type: TYPE_NORMAL
- en: DP can be used in reinforcement learning and is among one of the simplest approaches.
    It is suited to computing optimal policies by being provided with a perfect model
    of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: DP is an important stepping stone in the history of RL algorithms and provides
    the foundation for the next generation of algorithms, but it is computationally
    very expensive. DP works with MDPs with a limited number of states and actions
    as it has to update the value of each state (or action-value), taking into consideration
    all the other possible states. Moreover, DP algorithms store value functions in
    an array or in a table. This way of storing information is effective and fast
    as there isn't any loss of information, but it does require the storage of large
    tables. Since DP algorithms use tables to store value functions, it is called
    tabular learning. This is opposed to approximated learning, which uses approximated
    value functions to store the values in a fixed size function, such as an artificial
    neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'DP uses **bootstrapping**, meaning that it improves the estimation value of
    a state by using the expected value of the following states. As we have already
    seen, bootstrapping is used in the Bellman equation. Indeed, DP applies the Bellman
    equations, (6) and (7), to estimate ![](img/d1e5a85d-b3db-40e3-8673-6e550b0970cc.png) and/or
    ![](img/c3f1b6e5-7143-4c81-a8e4-e18a3b34132f.png). This can be done using the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b748b0d7-08fe-47f8-8b6f-3f2d8d554826.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Or by using the Q-function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae17b27a-87c2-4b74-9d79-82d604815cae.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, once the optimal value and action-value function are found, the optimal
    policy can be found by just taking the actions that maximize the expectation.
  prefs: []
  type: TYPE_NORMAL
- en: Policy evaluation and policy improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To find the optimal policy, you first need to find the optimal value function.
    An iterative procedure that does this is called **policy evaluation**—it creates
    a ![](img/e066f1bb-6f23-4534-a641-d2c5b5744803.png) sequence that iteratively
    improves the value function for a policy, ![](img/efe619a7-adea-4fca-8f5c-0526379caee6.png),
    using the state value transition of the model, the expectation of the next state,
    and the immediate reward. Therefore, it creates a sequence of improving value
    functions using the Bellman equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/35ce6c80-4531-418a-89ee-d95018a5eb01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This sequence will converge to the optimal value as ![](img/2de8443b-3c7c-4562-81a4-8c8909dc7721.png).
    Figure 3.3 shows the update of ![](img/fcc876d2-ff8c-43bd-b4ab-94ca788e93c7.png) using
    the successive state values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/750b984e-76e9-4fb4-8565-bbf6e0dc1e0f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3\. The update of ![](img/772727b6-3e7b-41bf-a9c3-2b329fc37397.png) using formula
    (8)
  prefs: []
  type: TYPE_NORMAL
- en: The value function (8) can be updated only if the state transition function, `p`,
    and the reward function, `r`, for every state and action are known, so only if
    the model of the environment is completely known.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the first summation of the actions in (8) is needed for stochastic
    policies because the policy outputs a probability for each action. For simplicity
    from now on, we'll consider only deterministic policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the value functions are improved, it can be used to find a better policy.
    This procedure is called *policy improvement* and is about finding a policy, ![](img/9c7a1dd0-2b3e-4a41-98fd-25aafdd44903.png), as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/76d1e369-d645-4f9f-86df-577aa539974b.png)'
  prefs: []
  type: TYPE_IMG
- en: It creates a policy, ![](img/bc4324c4-27a9-4444-9262-f5739c2b6eae.png), from
    the value function, ![](img/54caf5aa-e5a1-4022-a5ed-3e801909c297.png), of the
    original policy, ![](img/b07eeebf-18a3-489d-9769-5c9736bbb58c.png). As can be
    formally demonstrated, the new policy, ![](img/e5fc8c16-f6c2-4c49-a973-0fbc3d372902.png), is
    always better than ![](img/a999e245-6d4c-4c0f-8ee9-5b1020f8eb35.png), and the
    policy is optimal if and only if ![](img/46dec1a9-9029-46fb-87ff-c75694ccf873.png) is
    optimal. The combination of policy evaluation and policy improvement gives rise
    to two algorithms to compute the optimal policy. One is called **policy iteration**
    and the other is called **value iteration**. Both use policy evaluation to monotonically improve
    the value function and policy improvement to estimate the new policy. The only
    difference is that policy iteration executes the two phases cyclically, while
    value iteration combines them in a single update.
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Policy iteration cycles between policy evaluation, which updates ![](img/385c7c7e-e70d-4c66-b996-ce801f533505.png) under
    the current policy, ![](img/147da112-7f02-4562-8164-d6d642b64073.png), using formula (8),
    and policy improvement (9), which computes ![](img/1fefbb96-584a-47d3-8820-6b078fc78d85.png) using
    the improved value function, ![](img/2c92dcf9-5506-484f-8ee1-6e799304cca7.png). Eventually,
    after ![](img/61d2c68a-4610-4ead-923f-9ea65c7f0c41.png) cycles, the algorithm
    will result in an optimal policy, ![](img/a65f1839-d684-4a3e-85bb-741fb82da1cc.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The pseudocode is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: After an initialization phase, the outer loop iterates through policy evaluation
    and policy iteration until a stable policy is found. On each of these iterations,
    policy evaluation evaluates the policy found during the preceding policy improvement
    steps, which in turn use the estimated value function.
  prefs: []
  type: TYPE_NORMAL
- en: Policy iteration applied to FrozenLake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To consolidate the ideas behind policy iteration, we''ll apply it to a game
    called FrozenLake. Here, the environment consists of a 4 x 4 grid. Using four
    actions that correspond to the directions (0 is left, 1 is down, 2 is right, and
    3 is up), the agent has to move to the opposite side of the grid without falling
    in the holes. Moreover, movement is uncertain, and the agent has the possibility
    of movement in other directions. So, in such a situation, it could be beneficial
    not to move in the intended direction. A reward of +1 is assigned when the end
    goal is reached. The map of the game is shown in figure 3.4\. S is the start position,
    the star is the end position, and the spirals are the holes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c9d98d17-74ab-42f6-80eb-d542848be9d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 Map of the FrozenLake game
  prefs: []
  type: TYPE_NORMAL
- en: With all the tools needed, let's see how to solve it.
  prefs: []
  type: TYPE_NORMAL
- en: 'All the code explained in this chapter is available on the GitHub repository
    of this book, using the following link: [https://https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python](https://github.com/PacktPublishing/Reinforcement-Learning-Algorithms-with-Python)'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to create the environment, initializing the value function and
    the policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we have to create the main cycle that does one step of policy evaluation
    and one step of policy improvement. This cycle finishes whenever the policy is
    stable. To do this, use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'In the end, we can print the number of iterations completed, the value function,
    the policy, and the score reached running some test games:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, before defining `policy_evaluation`, we can create a function to evaluate
    the expected action-value that will also be used in `policy_improvement`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, `env.P` is a dictionary that contains all the information about the dynamics
    of the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '`gamma` is the discount factor, with 0.99 being a standard value to use for
    simple and medium difficulty problems. The higher it is, the more difficult it
    is for the agent to predict the value of a state because it should look further
    into the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can define the `policy_evaluation` function. `policy_evaluation` has
    to calculate formula (8) under the current policy for every state until it reaches steady
    values. Because the policy is deterministic, we only evaluate one action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: We consider the value function stable whenever `delta` is lower than the threshold, `eps`.
    When these conditions are met, the `while` loop statement is stopped.
  prefs: []
  type: TYPE_NORMAL
- en: '`policy_improvement` takes the value function and the policy and iterates them
    across all of the states to update the policy based on the new value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`policy_improvement(V, policy)` returns `False` until the policy changes. That''s
    because it means that the policy isn''t stable yet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The final snippet of code runs some games to test the new policy and prints
    the number of games won:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: That's it.
  prefs: []
  type: TYPE_NORMAL
- en: 'It converges in about 7 iterations and wins approximately 85% of games:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d88776b-8ea4-4c76-abdf-e61bc3a217f1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 Results of the FrozenLake game. The optimal policy is on the left
    and the optimal state values are on the right
  prefs: []
  type: TYPE_NORMAL
- en: The policy resulting from the code is shown on the left of figure 3.5\. You
    can see that it takes strange directions, but it's only because it follows the
    dynamics of the environment. On the right of figure 3.5, the final state's values
    are presented.
  prefs: []
  type: TYPE_NORMAL
- en: Value iteration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Value iteration is the other dynamic programming algorithm to find optimal
    values in an MDP, but unlike policy iterations that execute policy evaluations
    and policy iterations in a loop, value iteration combines the two methods in a
    single update. In particular, it updates the value of a state by selecting the
    best action immediately:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4ac987cf-3e3e-4ae7-9158-2de7a6806d5a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The code for value iteration is even simpler than the policy iteration code,
    summarized in the following pseudocode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The only difference is in the new value estimation update and in the absence
    of a proper policy iteration module. The resulting optimal policy is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63edbf60-b522-4259-a519-9a0c94167b59.png)'
  prefs: []
  type: TYPE_IMG
- en: Value iteration applied to FrozenLake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can now apply value iteration to the FrozenLake game in order to compare
    the two DP algorithms and to see whether they converge to the same policy and
    value function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s define `eval_state_action` as before to estimate the action state value
    for a state-action pair:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create the main body of the value iteration algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: It loops until it reaches a steady value function (determined by the threshold, `eps`)
    and for each iteration, it updates the value of each state using formula (10).
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the policy iteration, `run_episodes` executes some games to test the
    policy. The only difference is that in this case, the policy is determined at
    the same time that `run_episodes` is executed (for policy iteration, we defined
    the action for every state beforehand):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can create the environment, unwrap it, run the value iteration,
    and execute some test games:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The value iteration algorithm converges after 130 iterations. The resulting
    value function and policy are the same as the policy iteration algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An RL problem can be formalized as an MDP, providing an abstract framework for
    learning goal-based problems. An MDP is defined by a set of states, actions, rewards,
    and transition probabilities, and solving an MDP means finding a policy that maximizes
    the expected reward in each state. The Markov property is intrinsic to the MDP
    and ensures that the future states depend only on the current one, not on its
    history.
  prefs: []
  type: TYPE_NORMAL
- en: Using the definition of MDP, we formulated the concepts of policy, return function,
    expected return, action-value function, and value function. The latter two can
    be defined in terms of the values of the subsequent states, and the equations
    are called Bellman equations. These equations are useful because they provide
    a method to compute value functions in an iterative way. The optimal value functions
    can then be used to find the optimal policy.
  prefs: []
  type: TYPE_NORMAL
- en: RL algorithms can be categorized as model-based or model-free. While the former
    requires a model of the environment to plan the next actions, the latter is independent
    of the model and can learn by direct interaction with the environment. Model-free
    algorithms can be further divided into policy gradient and value function algorithms.
    Policy gradient algorithms learn directly from the policy through gradient ascent
    and are typically on-policy. Value function algorithms are usually off-policy,
    and learn an action-value function or value function in order to create the policy.
    These two methods can be brought together to give rise to methods that combine
    the advantages of both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: 'DP is the first set of model-based algorithms that we looked at in depth. It
    is used whenever the full model of the environment is known and when it is constituted
    by a limited number of states and actions. DP algorithms use bootstrapping to
    estimate the value of a state and they learn the optimal policy through two processes:
    policy evaluation and policy improvement. Policy evaluation computes the state
    value function for an arbitrary policy, while policy improvement improves the
    policy using the value function obtained from the policy evaluation process.'
  prefs: []
  type: TYPE_NORMAL
- en: By combining policy improvement and policy evaluation, the policy iteration
    algorithm and the value iteration algorithm can be created. The main difference
    between the two is that while policy iteration runs iteratively of policy evaluation
    and policy improvement, value iteration combines the two processes in a single
    update.
  prefs: []
  type: TYPE_NORMAL
- en: Though DP suffers from the curse of dimensionality (the complexity grows exponentially
    with the number of states), the ideas behind policy evaluation and policy iteration
    are key in almost all RL algorithms because they use a generalized version of
    them.
  prefs: []
  type: TYPE_NORMAL
- en: Another disadvantage of DP is that it requires the exact model of the environment,
    limiting its applicability to many other problems.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you'll see how V-functions and Q-functions can be used
    to learn a policy, using problems where the model is unknown by sampling directly
    from the environment.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What's an MDP?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a stochastic policy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can a return function be defined in terms of the return at the next time
    step?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is the Bellman equation so important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the limiting factors of DP algorithms?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is policy evaluation?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do policy iteration and value iteration differ?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sutton and Barto, *Reinforcement Learning*, Chapters 3 and 4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
