["```\ndf = pd.read_csv('path/to/dataset')\n```", "```\nscaler = StandardScaler()\ntransformed_df = scaler.fit_transform(df)\n```", "```\n    import pandas as pd\n    ```", "```\n    df = pd.read_csv('Bias_correction_ucl.csv')\n    df\n    ```", "```\n    df.drop('Date', inplace=True, axis=1)\n    ```", "```\n    ax = df['Present_Tmax'].hist(color='gray')\n    ax.set_xlabel(\"Temperature\")\n    ax.set_ylabel(\"Frequency\")\n    ```", "```\n    from sklearn.preprocessing import StandardScaler\n    scaler = StandardScaler()\n    df2 = scaler.fit_transform(df)\n    df2 = pd.DataFrame(df2, columns=df.columns)\n    ```", "```\n    ax = df2['Present_Tmax'].hist(color='gray')\n    ax.set_xlabel(\"Normalized Temperature\")\n    ax.set_ylabel(\"Frequency\")\n    ```", "```\ndummies = pd.get_dummies(df['feature1'], prefix='feature1')\n```", "```\n    import pandas as pd\n    ```", "```\n    df = pd.read_csv('Bias_correction_ucl.csv')\n    ```", "```\n    df['Date'] = pd.to_datetime(df['Date'])\n    ```", "```\n    year_dummies = pd.get_dummies(df['Date'].dt.year, \\\n                                  prefix='year')\n    year_dummies\n    ```", "```\n    month_dummies = pd.get_dummies(df['Date'].dt.month, \\\n                                   prefix='month')\n    month_dummies\n    ```", "```\n    df = pd.concat([df, month_dummies, year_dummies], \\\n                   axis=1)\n    ```", "```\n    df.drop('Date', axis=1, inplace=True)\n    ```", "```\n    df.dtypes\n    ```", "```\ndatagenerator = ImageDataGenerator(rescale = 1./255)\n```", "```\ndataset = datagenerator.flow_from_directory\\\n          ('path/to/data',\\\n           target_size = (64, 64),\\\n           batch_size = 25,\\\n           class_mode = 'binary')\n```", "```\n    from tensorflow.keras.preprocessing.image \\\n         import ImageDataGenerator\n    ```", "```\n    train_datagen = ImageDataGenerator(rescale =  1./255)\n    ```", "```\n    training_set = train_datagen.flow_from_directory\\\n                   ('image_data',\\\n                    target_size = (64, 64),\\\n                    batch_size = 25,\\\n                    class_mode = 'binary')\n    ```", "```\n    import matplotlib.pyplot as plt\n    def show_batch(image_batch, label_batch):\\\n        lookup = {v: k for k, v in \\\n                  training_set.class_indices.items()}\n        label_batch = [lookup[label] for label in \\\n                       label_batch]\n        plt.figure(figsize=(10,10))\n        for n in range(25):\n            ax = plt.subplot(5,5,n+1)\n            plt.imshow(image_batch[n])\n            plt.title(label_batch[n].title())\n            plt.axis('off')\n    ```", "```\n    image_batch, label_batch = next(training_set)\n    show_batch(image_batch, label_batch)\n    ```", "```\ndatagenerator = ImageDataGenerator(rescale = 1./255,\\\n                                   shear_range = 0.2,\\\n                                   rotation_range= 180,\\\n                                   zoom_range = 0.2,\\\n                                   horizontal_flip = True)\n```", "```\nimport tensorflow_hub as hub\nmodel_url = \"url_of_model\"\nhub_layer = hub.KerasLayer(model_url, \\\n                           input_shape=[], dtype=tf.string, \\\n                           trainable=True)\n```", "```\nhub_layer(data)\n```", "```\n    import tensorflow as tf\n    ```", "```\n    df = tf.data.experimental.make_csv_dataset\\\n         ('../Datasets/drugsComTest_raw.tsv', \\\n          batch_size=1, field_delim='\\t')\n    ```", "```\n    def prep_ds(ds, shuffle_buffer_size=1024, \\\n                batch_size=32):\n        # Shuffle the dataset\n        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n        # Repeat the dataset\n        ds = ds.repeat()\n        # Batch the dataset\n        ds = ds.batch(batch_size)\n        return ds\n    ```", "```\n    ds = prep_ds(df, batch_size=5)\n    ```", "```\n    for x in ds.take(1):\\\n        print(x)\n    ```", "```\n    import tensorflow_hub as hub\n    embedding = \"https://tfhub.dev/google/tf2-preview\"\\\n                \"/gnews-swivel-20dim/1\"\n    hub_layer = hub.KerasLayer(embedding, input_shape=[], \\\n                               dtype=tf.string, \\\n                               trainable=True)\n    ```", "```\n    for x in ds.take(1):\\\n        print(hub_layer(tf.reshape(x['review'],[-1])))\n    ```", "```\ndataset = tf.data.Dataset\\\n            .from_tensor_slices([1, 2, 3, 4, 5])\n```", "```\nsample_rate = 44100\naudio_data = tf.io.read_file('path/to/file')\naudio, sample_rate = tf.audio.decode_wav\\\n                     (audio_data,\\\n                      desired_channels=-1,\\\n                      desired_samples=sample_rate)\n```", "```\nstfts = tf.signal.stft(audio, frame_length=1024,\\\n                       frame_step=256,\\\n                       fft_length=1024)\nspectrograms = tf.abs(stfts)\n```", "```\nlower_edge_hertz, upper_edge_hertz, num_mel_bins \\\n    = 80.0, 7600.0, 80\nlinear_to_mel_weight_matrix \\\n    = tf.signal.linear_to_mel_weight_matrix\\\n      (num_mel_bins, num_spectrogram_bins, sample_rate, \\\n       lower_edge_hertz, upper_edge_hertz)\nmel_spectrograms = tf.tensordot\\\n                   (spectrograms, \\\n                    linear_to_mel_weight_matrix, 1)\nmel_spectrograms.set_shape\\\n    (spectrograms.shape[:-1].concatenate\\\n    (linear_to_mel_weight_matrix.shape[-1:]))\nlog_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)\nmfccs = tf.signal.mfccs_from_log_mel_spectrograms\\\n        (log_mel_spectrograms)[..., :num_mfccs]\n```", "```\n    import tensorflow as tf\n    import os\n    ```", "```\n    def load_audio(file_path, sample_rate=44100):\n        # Load audio at 44.1kHz sample-rate\n        audio = tf.io.read_file(file_path)\n        audio, sample_rate = tf.audio.decode_wav\\\n                             (audio,\\\n                              desired_channels=-1,\\\n                              desired_samples=sample_rate)\n        return tf.transpose(audio)\n    ```", "```\n    prefix = \" ../Datasets/data_speech_commands_v0.02\"\\\n            \"/zero/\"\n    paths = [os.path.join(prefix, path) for path in \\\n             os.listdir(prefix)]\n    ```", "```\n    import matplotlib.pyplot as plt\n    audio = load_audio(paths[0])\n    plt.plot(audio.numpy().T)\n    plt.xlabel('Sample')\n    plt.ylabel('Value')\n    ```", "```\n    def apply_mfccs(audio, sample_rate=44100, num_mfccs=13):\n        stfts = tf.signal.stft(audio, frame_length=1024, \\\n                               frame_step=256, \\\n                               fft_length=1024)\n        spectrograms = tf.abs(stfts)\n        num_spectrogram_bins = stfts.shape[-1]#.value\n        lower_edge_hertz, upper_edge_hertz, \\\n        num_mel_bins = 80.0, 7600.0, 80\n        linear_to_mel_weight_matrix = \\\n          tf.signal.linear_to_mel_weight_matrix\\\n          (num_mel_bins, num_spectrogram_bins, \\\n           sample_rate, lower_edge_hertz, upper_edge_hertz)\n        mel_spectrograms = tf.tensordot\\\n                           (spectrograms, \\\n                            linear_to_mel_weight_matrix, 1)\n        mel_spectrograms.set_shape\\\n        (spectrograms.shape[:-1].concatenate\\\n        (linear_to_mel_weight_matrix.shape[-1:]))\n        log_mel_spectrograms = tf.math.log\\\n                               (mel_spectrograms + 1e-6)\n        #Compute MFCCs from log_mel_spectrograms\n        mfccs = tf.signal.mfccs_from_log_mel_spectrograms\\\n                (log_mel_spectrograms)[..., :num_mfccs]\n        return mfccs\n    ```", "```\n    mfcc = apply_mfccs(audio)\n    plt.pcolor(mfcc.numpy()[0])\n    plt.xlabel('MFCC log coefficient')\n    plt.ylabel('Sample Value')\n    ```", "```\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    def prep_ds(ds, shuffle_buffer_size=1024, \\\n                batch_size=64):\n        # Randomly shuffle (file_path, label) dataset\n        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n        # Load and decode audio from file paths\n        ds = ds.map(load_audio, num_parallel_calls=AUTOTUNE)\n        # generate MFCCs from the audio data\n        ds = ds.map(apply_mfccs)\n        # Repeat dataset forever\n        ds = ds.repeat()\n        # Prepare batches\n        ds = ds.batch(batch_size)\n        # Prefetch\n        ds = ds.prefetch(buffer_size=AUTOTUNE)\n        return ds\n    ```", "```\n    ds = tf.data.Dataset.from_tensor_slices(paths)\n    train_ds = prep_ds(ds)\n    ```", "```\n    for x in train_ds.take(1):\\\n        print(x)\n    ```"]