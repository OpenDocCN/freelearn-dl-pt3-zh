- en: '*Chapter 10*: Applying the Power of Deep Learning to Videos'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Computer vision is focused on the understanding of visual data. Of course, that
    includes videos, which, at their core, are a sequence of images, which means we
    can leverage most of our knowledge regarding deep learning for image processing
    to videos and reap great results.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll start training a convolutional neuronal network to detect
    emotions in human faces, and then we'll learn how to apply it in a real-time context
    using our webcam.
  prefs: []
  type: TYPE_NORMAL
- en: Then, in the remaining recipes, we'll use very advanced implementations of architectures,
    hosted in **TensorFlow Hub** (**TFHub**), specially tailored to tackle interesting
    video-related problems such as action recognition, frames generation, and text-to-video
    retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the recipes that we will be covering shortly:'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting emotions in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing actions with TensorFlow Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating the middle frames of a video with TensorFlow Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing text-to-video retrieval with TensorFlow Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As usual, having access to a GPU is a great plus, particularly for the first
    recipe, where we''ll implement a network from scratch. Because the rest of the
    chapter leverages models in TFHub, your CPU should be enough, although a GPU will
    give you a pretty nice speed boost! In the *Getting ready* section, you''ll find
    the preparatory steps for each recipe. You can find the code for this chapter
    here: [https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch10](https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch10).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/3qkTJ2l](https://bit.ly/3qkTJ2l).'
  prefs: []
  type: TYPE_NORMAL
- en: Detecting emotions in real time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At its most basic form, a video is just a series of images. By leveraging this
    seemingly simple or trivial fact, we can adapt what we know about image classification
    to create very interesting video processing pipelines powered by deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll build an algorithm to detect emotions in real time (webcam
    streaming) or from video files. Pretty interesting, right?
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we must install several external libraries, such as `OpenCV` and `imutils`.
    Execute the following command to install them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To train an emotion classifier network, we'll use the dataset from the Kaggle
    competition `~/.keras/datasets` folder), extract it as `emotion_recognition`,
    and then unzip the `fer2013.tar.gz` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some sample images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Sample images. Emotions from left to right: sad, angry, scared,
    surprised, happy, and neutral](img/B14768_10_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.1 – Sample images. Emotions from left to right: sad, angry, scared,
    surprised, happy, and neutral'
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By the end of this recipe, you'll have your own emotion detector!
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a list of all possible emotions in our dataset, along with a color associated
    with each one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a method to build the emotion classifier architecture. It receives the
    input shape and the number of classes in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Each block in the network is comprised of two ELU activated, batch-normalized
    convolutions, followed by a max pooling layer, and ending with a dropout layer.
    The block defined previously had 32 filters per convolution, while the following
    one has 64 filters per convolution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The third block has 128 filters per convolution:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we have two dense, ELU activated, batch-normalized layers, also followed
    by a dropout, each with 64 units:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we encounter the output layer, with as many neurons as classes in
    the dataset. Of course, it''s softmax-activated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`load_dataset()` loads both the images and labels for the training, validation,
    and test datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The data in this dataset is in a CSV file, separated into `emotion`, `pixels`,
    and `Usage` columns. Let''s parse the `emotion` column first. Although the dataset
    contains faces for seven classes, we''ll combine *disgust* and *angry* (encoded
    as `0` and `1`, respectively) because both share most of the facial features,
    and merging them leads to better results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we parse the `pixels` column, which is 2,034 whitespace-separated integers,
    corresponding to the grayscale pixels for the image (48x48=2034):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, to figure out to which subset this image and label belong, we must look
    at the `Usage` column:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Convert all the images to NumPy arrays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, one-hot encode all the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Return all the images and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to compute the area of a rectangle. We''ll use this later
    to get the largest face detection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll now create a bar plot to display the probability distribution of the
    emotions detected in each frame. The following function is used to plot each bar,
    corresponding to a particular emotion, in said plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll also draw a bounding box around the detected face, captioned with the
    recognized emotion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `predict_emotion()` function, which takes the emotion classifier
    and an input image and returns the predictions output by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load a saved model if there is one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Otherwise, train the model from scratch. First, build the path to the CSV with
    the data and then compute the number of classes in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, load each subset of data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the network and compile it. Also, define a `ModelCheckpoint` callback
    to save the best performing model, based on the validation loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the augmenters and generator for the training and validation sets. Notice
    that we''re only augmenting the training set, while we just rescale the images
    in the validation set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model for 300 epochs and then evaluate it on the test set (we only
    rescale the images in this subset):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a `cv2.VideoCapture()` object to fetch the frames in a test video.
    If you want to use your webcam, replace `video_path` with `0`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a **Haar Cascades** face detector (this is a topic outside the scope
    of this book. If you want to learn more about Haar Cascades, refer to the *See
    also* section in this recipe):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate over each frame in the video (or webcam stream), exiting only if there
    are no more frames to read, or if the user presses the Q key:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Resize the frame to have a width of 380 pixels (the height will be computed
    automatically to preserve the aspect ratio). Also, create a canvas of where to
    draw the emotions bar plot, and a copy of the input frame in terms of where to
    plot the detected faces:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Because Haar Cascades work on grayscale images, we must convert the input frame
    to black and white. Then, we run the face detector on it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify whether there are any detections and fetch the one with the largest
    area:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Extract the region of interest (`roi`) corresponding to the detected face and
    extract the emotions from it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the emotion distribution plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Plot the detected face along with the emotion it displays:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Show the result:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check whether the user pressed Q, and if they did, break out of the loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, release the resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After 300 epochs, I obtained a test accuracy of 65.74%. Here you can see some
    snapshots of the emotions detected in the test video:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Emotions detected in two different snapshots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Emotions detected in two different snapshots
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that the network correctly identifies sadness in the top frame,
    and happiness in the bottom one. Let''s take a look at another example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Emotions detected in three different snapshots'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Emotions detected in three different snapshots
  prefs: []
  type: TYPE_NORMAL
- en: In the first frame, the girl clearly has a neutral expression, which was correctly
    picked up by the network. In the second frame, her face shows anger, which the
    classifier also detects. The third frame is more interesting, because her expression
    displays surprise, but it could also be interpreted as fear. Our detector seems
    to be split between these two emotions as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let's head over to the next section, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we implemented a fairly capable emotion detector for video streams,
    either from a built-in webcam, or a stored video file. We started by parsing the
    `FER 2013` data, which, unlike most other image datasets, is in CSV format. Then,
    we trained an emotion classifier on its images, achieving a respectable 65.74%
    accuracy on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: We must take into consideration the fact that facial expressions are tricky
    to interpret, even for humans. At a given time, we might display mixed emotions.
    Also, there are many expressions that share traits, such as *anger* and *disgust*,
    and *fear* and *surprise*, among others.
  prefs: []
  type: TYPE_NORMAL
- en: The last step in this first recipe consisted of passing each frame in the input
    video stream to a Haar Cascade face detector, and then getting the emotions, using
    the trained classifier, from the regions of interest corresponding to the detected
    faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this approach works well for this particular problem, we must take
    into account that we overlooked a crucial assumption: each frame is independent.
    Simply put, we treated each frame in the video as an isolated image, but in reality,
    that''s not the case when we''re dealing with videos, because there''s a time
    dimension that, when accounted for, yields more stable and better results.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here''s a great resource for understanding the Haar Cascade classifier: [https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html](https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing actions with TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A very interesting application of deep learning to video processing involves
    action recognition. This is a challenging problem, because it not only presents
    the typical difficulties associated with classifying the contents of an image,
    but also includes a temporal component. An action in a video can vary depending
    on the order in which the frames are presented to us.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that there is an architecture that is perfectly suited to this
    kind of problem, known as **Inflated 3D Convnet** (**I3D**), and in this recipe
    we'll use a trained version hosted in TFHub to recognize actions in a varied selection
    of videos!
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to install several supplementary libraries, such as `OpenCV`, `TFHub`,
    and `imageio`. Execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: That's it! Let's begin implementing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to complete the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the required dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the path to the `UCF101 – Action Recognition` dataset, from where we''ll
    fetch the test videos that we will pass to the model later on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the path to the labels file of the `Kinetics` dataset, the one used
    to train the 3D convolutional network we''ll use shortly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a temporary directory to cache the downloaded resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an unverified SSL context. We need this to be able to download data
    from UCF''s site (at the time of writing this book, it appears that their certificate
    has expired):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `fetch_ucf_videos()` function, which downloads the list of the possible
    videos we''ll choose from to test our action recognizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `fetch_kinetics_labels()` function, used to download and parse the
    labels of the `Kinetics` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `fetch_random_video()` function, which selects a random video from
    our list of `UCF101` videos and downloads it to the temporary directory created
    in *Step 4*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `crop_center()` function, which takes an image and crops a squared
    selection corresponding to the center of the received frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `read_video()` function, which reads up to `max_frames` from a video
    stored in our cache and returns a list of all the read frames. It also crops the
    center of each frame, resizes it to 224x224x3 (the input shape expected by the
    network), and normalizes it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `predict()` function, used to get the top five most likely actions
    recognized by the model in the input video:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `save_as_gif()` function, which takes a list of frames corresponding
    to a video, and uses them to create a GIF representation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch the videos and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch a random video and read its frames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the I3D from TFHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, pass the video through the network to obtain the predictions, and
    then save the video as a GIF:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the first frame of the random video I obtained:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Frame of the random UCF101 video'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Frame of the random UCF101 video
  prefs: []
  type: TYPE_NORMAL
- en: 'And here are the top five predictions produced by the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: It appears that the network understands that the action portrayed in the video
    has to do with the floor, because four out of five predictions have to do with
    it. However, `mopping floor` is the correct one.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now move to the *How it works…* section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we leveraged the power of a 3D convolutional network to recognize
    actions in videos. A 3D convolution, as the name suggests, is a natural extension
    of a bi-dimensional convolution, which moves in two directions. Naturally, 3D
    convolutions consider width and height, but also depth, making them the perfect
    fit for special kinds of images, such as Magnetic Resonance Imaging (MRI) or,
    in this case, videos, which are just a series of images stacked together.
  prefs: []
  type: TYPE_NORMAL
- en: We started by fetching a series of videos from the `UCF101` dataset and a set
    of action labels from the `Kinetics` dataset. It's important to remember that
    the I3D we downloaded from TFHub was trained on Kinetics. Therefore, the videos
    we passed to it are unseen.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we implemented a series of helper functions to obtain, preprocess, and
    shape each input video in the way the I3D expects. Then, we loaded the aforementioned
    network from TFHub and used it to display the top five actions it recognized in
    the video.
  prefs: []
  type: TYPE_NORMAL
- en: One interesting extension you can make to this solution is to read custom videos
    from your filesystem, or better yet, pass a stream of images from your webcam
    to the network in order to see how well it performs!
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I3D is a groundbreaking architecture for video processing, so I highly recommend
    you read the original paper here: [https://arxiv.org/abs/1705.07750](https://arxiv.org/abs/1705.07750).
    Here''s a pretty interesting article that explains the difference between 1D,
    2D, and 3D convolutions: [https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610](https://towardsdatascience.com/understanding-1d-and-3d-convolution-neural-network-keras-9d8f76e29610).
    You can learn more about the `UCF101` dataset here: https://www.crcv.ucf.edu/data/UCF101.php.
    If you''re interested in the `Kinetics` dataset, access this link: https://deepmind.com/research/open-source/kinetics.
    Lastly, you can find more details about the I3D implementation we used here: [https://tfhub.dev/deepmind/i3d-kinetics-400/1](https://tfhub.dev/deepmind/i3d-kinetics-400/1).'
  prefs: []
  type: TYPE_NORMAL
- en: Generating the middle frames of a video with TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another interesting application of deep learning to videos involves frame generation.
    A fun and practical example of this technique is slow motion, where a network
    decides, based on the context, how to create intervening frames, thus expanding
    the length of a video and creating the illusion it was recorded with a high-speed
    camera (if you want to read more about it, refer to the *See also…* section).
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll use a 3D convolutional network to produce the middle frames
    of a video, given only its first and last frames.
  prefs: []
  type: TYPE_NORMAL
- en: For this purpose, we'll rely on TFHub.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We must install TFHub and `TensorFlow Datasets`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The model we''ll use was trained on the `BAIR Robot Pushing Videos` dataset,
    which is available in `TensorFlow Datasets`. However, if we access it through
    the library, we''ll download way more data than we need for the purposes of this
    recipe. Instead, we''ll use a smaller subset of the test set. Execute the following
    command to download it and place it inside the `~/.keras/datasets/bair_robot_pushing`
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: Now we're all set! Let's begin implementing.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to learn how to generate middle frames using **Direct
    3D Convolutions**, through a model hosted in TFHub:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the dependencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `plot_first_and_last_for_sample()` function, which creates a plot
    of the first and last frames of a sample of four videos:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `plot_generated_frames_for_sample()` function, which graphs the
    middle frames generated for a sample of four videos:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We need to patch the `BarRobotPushingSmall()` (see *Step 6*) dataset builder
    to only expect the test split to be available, instead of both the training and
    test ones. Therefore, we must create a custom `SplitGenerator()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the path to the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `BarRobotPushingSmall()` builder, pass it the custom split generator
    created in *Step 4*, and then prepare the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the first batch of videos:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Keep only the first and last frame of each video in the batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the generator model from TFHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the batch of videos through the model to generate the middle frames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Concatenate the first and last frames of each video in the batch with the corresponding
    middle frames produced by the network in *Step 10*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, plot the first and last frames, and also the middle frames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In *Figure 10.5*, we can observe the first and last frame of each video in
    our sample of four:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.5 – First and last frame of each video in the sample'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – First and last frame of each video in the sample
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 10.6*, we observe the 14 middle frames generated by the model for
    each video. Close inspection reveals they are coherent with the first and last
    real frames passed to the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Middle frames produced by the model for each sample video'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Middle frames produced by the model for each sample video
  prefs: []
  type: TYPE_NORMAL
- en: Let's go to the *How it works…* section to review what we did.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we learned about another useful and interesting application
    of deep learning to videos, particularly 3D convolutional networks, in the context
    of generative models.
  prefs: []
  type: TYPE_NORMAL
- en: We took a state-of-the-art architecture trained on the `BAIR Robot Pushing Videos`
    dataset, hosted in TFHub, and used it to produce an entirely new video sequence,
    taking only as seeds the first and last frames of a video.
  prefs: []
  type: TYPE_NORMAL
- en: Because downloading the entire 30 GBs of the `BAIR` dataset would have been
    an overkill, given we only needed a way smaller subset to test our solution, we
    couldn't rely directly on the TensorFlow dataset's `load()` method. Instead, we
    downloaded a subset of the test videos and made the necessary adjustments to the
    `BairRobotPushingSmall()` builder to load and prepare the sample videos.
  prefs: []
  type: TYPE_NORMAL
- en: It must be mentioned that this model was trained on a very specific dataset,
    but it certainly showcases the powerful generation capabilities of this architecture.
    I encourage you to check out the *See also* section for a list of useful resources
    that could be of help if you want to implement a video generation network on your
    own data.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can learn more about the `BAIR Robot Pushing Videos` dataset here: [https://arxiv.org/abs/1710.05268](https://arxiv.org/abs/1710.05268).
    I encourage you to read the paper entitled **Video Inbetweening Using Direct 3D
    Convolutions**, where the network we used in this recipe was proposed: https://arxiv.org/abs/1905.10240\.
    You can find the TFHub model we relied on at the following link: https://tfhub.dev/google/tweening_conv3d_bair/1\.
    Lastly, here''s an interesting read about an AI that transforms regular footage
    into slow motion: [https://petapixel.com/2020/09/08/this-ai-can-transform-regular-footage-into-slow-motion-with-no-artifacts/](https://petapixel.com/2020/09/08/this-ai-can-transform-regular-footage-into-slow-motion-with-no-artifacts/).'
  prefs: []
  type: TYPE_NORMAL
- en: Performing text-to-video retrieval with TensorFlow Hub
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The applications of deep learning to videos are not limited to classification,
    categorization, or even generation. One of the biggest resources of neural networks
    is their internal representation of data features. The better a network is at
    a given task, the better their internal mathematical model is. We can take advantage
    of the inner workings of state-of-the-art models to build interesting applications
    on top of them.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll create a small search engine based on the embeddings produced
    by an **S3D** model, trained and ready to be used, which lives in TFHub.
  prefs: []
  type: TYPE_NORMAL
- en: Are you ready? Let's begin!
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we must install `OpenCV` and TFHub, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: That's all we need, so let's start this recipe!
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Perform the following steps to learn how to perform text-to-video retrieval
    using TFHub:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to import all the dependencies that we''ll use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define a function to produce the text and video embeddings using an instance
    of S3D:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `crop_center()` function, which takes an image and crops a squared
    selection corresponding to the center of the received frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the `fetch_and_read_video()` function, which, as its name indicates,
    downloads a video and then reads it. For this last part, we use OpenCV. Let''s
    start by getting the video from a given URL:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We extract the video format from the URL. Then, we save the video in the current
    folder, with a random UUID as its name.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we''ll load `max_frames` of this fetched video:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the video doesn''t have enough frames, we''ll repeat the process until we
    reach the desired capacity:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Return the normalized frames:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the URLs of the videos:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fetch and read each video:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the queries (captions) associated with each video. Notice that they
    must be in the correct order:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load S3D from TFHub:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Obtain the text and video embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the similarity scores between the text and video embeddings:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Take the first frame of each video, rescale it back to [0, 255], and then convert
    it to BGR space so that we can display it with OpenCV. We do this to display the
    results of our experiment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Iterate over each (query, video, score) triplet and display the most similar
    videos for each query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'First, we''ll see the result of the *beach* query:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Ranked results for the BEACH query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Ranked results for the BEACH query
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected, the first result, which is the highest score, is an image of a
    beach. Let''s now try with *playing drums*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Ranked results for the PLAYING DRUMS query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – Ranked results for the PLAYING DRUMS query
  prefs: []
  type: TYPE_NORMAL
- en: 'Awesome! It seems that the similarity between the query text and the images
    is stronger in this instance. Up next, a more difficult one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Ranked results for the AIRPLANE TAKING OFF query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Ranked results for the AIRPLANE TAKING OFF query
  prefs: []
  type: TYPE_NORMAL
- en: 'Although *airplane taking off* is a somewhat more complex query, our solution
    had no problem producing the correct results. Let''s now try with *biking*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Ranked results for the BIKING query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 – Ranked results for the BIKING query
  prefs: []
  type: TYPE_NORMAL
- en: Another match! How about *dog catching frisbee*?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Ranked results for the DOG CATCHING FRISBEE query'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14768_10_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.11 – Ranked results for the DOG CATCHING FRISBEE query
  prefs: []
  type: TYPE_NORMAL
- en: No problem at all! The satisfying results we've seen are due to the great job
    S3D does at mapping images with the words that best describe them. If you have
    read the paper where S3D was introduced, you won't be surprised by this fact,
    given the humongous amount of data it was trained on.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now proceed to the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we exploited the ability of the S3D model to generate embeddings,
    both for text and video, to create a small database we used as the basis of a
    toy search engine. This way, we demonstrated the usefulness of having a network
    capable of producing richly informative vectorial two-way mappings between images
    and text.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'I highly recommend that you read the paper where the model we used in this
    recipe was published as it''s very interesting! Here''s the link: https://arxiv.org/pdf/1912.06430.pdf.
    Speaking of the model, you''ll find it here: https://tfhub.dev/deepmind/mil-nce/s3d/1.'
  prefs: []
  type: TYPE_NORMAL
