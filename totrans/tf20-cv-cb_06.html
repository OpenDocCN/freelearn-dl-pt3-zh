<html><head></head><body>
		<div id="_idContainer071">
			<h1 id="_idParaDest-183"><em class="italic"><a id="_idTextAnchor214"/>Chapter 6</em>: Generative Models and Adversarial Attacks</h1>
			<p>Being able to differentiate between two or more classes is certainly impressive, and a healthy sign that deep neural networks do, in fact, learn. </p>
			<p>But if traditional classification is impressive, then producing new content is staggering! That definitely requires a superior understanding of the domain. So, are there neural networks capable of such a feat? You bet there are! </p>
			<p>In this chapter, we'll study one of the most captivating and promising types of neural networks: <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>). As the term implies, these networks are actually a system comprised of two sub-networks: the generator and the discriminator. The job of the generator is to produce images so good that they <em class="italic">could</em> come from the original distribution (but actually don't; they're generated from scratch), thereby fooling the discriminator, whose task is to discern between real and fake images. </p>
			<p><strong class="bold">GANs</strong> are the tip of the spear in areas such as semi-supervised learning and image-to-image translation, both topics that we will cover in this chapter. As a complement, the final recipe in this chapter teaches us how to perform an adversarial attack on a network using the <strong class="bold">Fast Gradient Signed Method</strong> (<strong class="bold">FGSM</strong>). </p>
			<p>The recipes that we will cover in this chapter are as follows:</p>
			<ul>
				<li>Implementing a deep convolutional GAN</li>
				<li><a id="_idTextAnchor215"/><a id="_idTextAnchor216"/>Using a DCGAN for semi-supervised learning</li>
				<li>Translating images with Pix2Pix</li>
				<li>Translating unpaired images with CycleGAN</li>
				<li>Implementing an adversarial attack using the Fast Gradient Signed Method</li>
			</ul>
			<h1 id="_idParaDest-184"><a id="_idTextAnchor217"/>Technical requirements</h1>
			<p>GANs are great, but also extremely taxing in terms of computing power. Therefore, a GPU is a must-have in order to work on these recipes (and even then, most will run for several hours). In the <em class="italic">Getting ready</em> section, you'll find the preparations that are necessary, if any, for each recipe. The code for this chapter is available here: <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6</a>.</p>
			<p>Check out the following link to see the Code in Action video: <a href="https://bit.ly/35Z8IYn">https://bit.ly/35Z8IYn</a>.<a id="_idTextAnchor218"/><a id="_idTextAnchor219"/></p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor220"/>Implementing a deep convolutional GAN</h1>
			<p>A <strong class="bold">GAN</strong> is comprised, in its <a id="_idIndexMarker466"/>simplest form, of two networks, a generator and a discriminator. The discriminator is just a regular <strong class="bold">Convolutional Neural Network</strong> (<strong class="bold">CNN</strong>) that must <a id="_idIndexMarker467"/>solve the binary classification problem of distinguishing real images from fakes. The generator, on the other hand, is similar to the decoder in an autoencoder because it has to produce an image from a <strong class="source-inline">seed</strong>, which is just a vector of Gaussian noise.</p>
			<p>In this recipe, we'll <a id="_idIndexMarker468"/>implement a <strong class="bold">Deep Convolutional Generative Adversarial Network (DCGAN)</strong> to produce images akin to the ones present in <strong class="source-inline">EMNIST</strong>, a dataset that extends the well-known <strong class="source-inline">MNIST</strong> dataset with uppercase and lowercase handwritten letters on top of the digits from 0 to 9. </p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor221"/>Getting ready</h2>
			<p>We'll need to install <strong class="source-inline">tensorflow-datasets</strong> to access <strong class="source-inline">EMNIST</strong> more easily. Also, in order to display a nice progress bar during the training of our GAN, we'll use <strong class="source-inline">tqdm</strong>. </p>
			<p>Both dependencies can be installed as follows:</p>
			<p class="source-code">$&gt; pip install tensorflow-datasets tqdm</p>
			<p>We are good to go!</p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor222"/>How to do it…</h2>
			<p>Perform the following steps to implement a DCGAN on <strong class="source-inline">EMNIST</strong>:</p>
			<ol>
				<li>Import the necessary dependencies:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import BinaryCrossentropy</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">from tqdm import tqdm</p></li>
				<li>Define an <a id="_idIndexMarker469"/>alias for the <strong class="source-inline">AUTOTUNE</strong> setting, which we'll use later to determine the number of parallel calls when processing the images in the dataset:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p></li>
				<li>Define a <strong class="source-inline">DCGAN()</strong> class to encapsulate our implementation. The constructor creates the discriminator, generator, loss function, and the respective optimizers for both sub-networks:<p class="source-code">class DCGAN(object):</p><p class="source-code">    def __init__(self):</p><p class="source-code">        self.loss = BinaryCrossentropy(from_logits=True)</p><p class="source-code">        self.generator = self.create_generator()</p><p class="source-code">        self.discriminator = self.create_discriminator()</p><p class="source-code">        self.generator_opt = Adam(learning_rate=1e-4)</p><p class="source-code">        self.discriminator_opt = Adam(learning_rate=1e-4)</p></li>
				<li>Define a static method to create the generator network. It reconstructs a 28x28x1 image from an input tensor of 100 elements. Notice the use of transposed convolutions (<strong class="source-inline">Conv2DTranspose</strong>) to expand the output volumes as we go deeper into the network. Also, notice the activation is <strong class="source-inline">'tanh'</strong>, which means the outputs will be in the range [-1, 1]:<p class="source-code">   @staticmethod</p><p class="source-code">    def create_generator(alpha=0.2):</p><p class="source-code">        input = Input(shape=(100,))</p><p class="source-code">        x = Dense(units=7 * 7 * 256, </p><p class="source-code">                 use_bias=False)(input)</p><p class="source-code">        x = LeakyReLU(alpha=alpha)(x)</p><p class="source-code">        x = BatchNormalization()(x)</p><p class="source-code">        x = Reshape((7, 7, 256))(x)</p></li>
				<li>Add the first <a id="_idIndexMarker470"/>transposed convolution block, with 128 filters:<p class="source-code">        x = Conv2DTranspose(filters=128,</p><p class="source-code">                            strides=(1, 1),</p><p class="source-code">                            kernel_size=(5, 5),</p><p class="source-code">                            padding='same',</p><p class="source-code">                            use_bias=False)(x)</p><p class="source-code">        x = LeakyReLU(alpha=alpha)(x)</p><p class="source-code">        x = BatchNormalization()(x)</p></li>
				<li>Create the second transposed convolution block, with 64 filters:<p class="source-code">        x = Conv2DTranspose(filters=64,</p><p class="source-code">                            strides=(2, 2),</p><p class="source-code">                            kernel_size=(5, 5),</p><p class="source-code">                            padding='same',</p><p class="source-code">                            use_bias=False)(x)</p><p class="source-code">        x = LeakyReLU(alpha=alpha)(x)</p><p class="source-code">        x = BatchNormalization()(x)</p></li>
				<li>Add the last transposed convolution block, with only one filter, corresponding to the output of the network:<p class="source-code">        x = Conv2DTranspose(filters=1,</p><p class="source-code">                            strides=(2, 2),</p><p class="source-code">                            kernel_size=(5, 5),</p><p class="source-code">                            padding='same',</p><p class="source-code">                            use_bias=False)(x)</p><p class="source-code">        output = Activation('tanh')(x)</p><p class="source-code">        return Model(input, output)</p></li>
				<li>Define a static <a id="_idIndexMarker471"/>method to create the discriminator. This architecture is a regular CNN:<p class="source-code">    @staticmethod</p><p class="source-code">    def create_discriminator(alpha=0.2, dropout=0.3):</p><p class="source-code">        input = Input(shape=(28, 28, 1))</p><p class="source-code">        x = Conv2D(filters=64,</p><p class="source-code">                   kernel_size=(5, 5),</p><p class="source-code">                   strides=(2, 2),</p><p class="source-code">                   padding='same')(input)</p><p class="source-code">        x = LeakyReLU(alpha=alpha)(x)</p><p class="source-code">        x = Dropout(rate=dropout)(x)</p><p class="source-code">        x = Conv2D(filters=128,</p><p class="source-code">                   kernel_size=(5, 5),</p><p class="source-code">                   strides=(2, 2),</p><p class="source-code">                   padding='same')(x)</p><p class="source-code">        x = LeakyReLU(alpha=alpha)(x)</p><p class="source-code">        x = Dropout(rate=dropout)(x)</p><p class="source-code">        x = Flatten()(x)</p><p class="source-code">        output = Dense(units=1)(x)</p><p class="source-code">        return Model(input, output)</p></li>
				<li>Define a method to <a id="_idIndexMarker472"/>calculate the discriminator's loss, which is the sum of the real and fake losses:<p class="source-code">    def discriminator_loss(self, real, fake):</p><p class="source-code">        real_loss = self.loss(tf.ones_like(real), real)</p><p class="source-code">        fake_loss = self.loss(tf.zeros_like(fake), fake)</p><p class="source-code">        return real_loss + fake_loss</p></li>
				<li>Define a method to calculate the generator's loss:<p class="source-code">    def generator_loss(self, fake):</p><p class="source-code">        return self.loss(tf.ones_like(fake), fake)</p></li>
				<li>Define a method to perform a single training step. We'll start by generating a vector of random Gaussian noise:<p class="source-code">    @tf.function</p><p class="source-code">    def train_step(self, images, batch_size):</p><p class="source-code">        noise = tf.random.normal((batch_size,noise_dimension))</p></li>
				<li>Next, pass the random noise to the generator to produce fake images:<p class="source-code">        with tf.GradientTape() as gen_tape, \</p><p class="source-code">                tf.GradientTape() as dis_tape:</p><p class="source-code">            generated_images = self.generator(noise,</p><p class="source-code">                                        training=True)</p></li>
				<li>Pass the real and fake <a id="_idIndexMarker473"/>images to the discriminator and compute the losses of both sub-networks:<p class="source-code">            real = self.discriminator(images, </p><p class="source-code">                                      training=True)</p><p class="source-code">            fake = self.discriminator(generated_images,</p><p class="source-code">                                      training=True)</p><p class="source-code">            gen_loss = self.generator_loss(fake)</p><p class="source-code">            disc_loss = self.discriminator_loss(real, </p><p class="source-code">                                               fake)</p></li>
				<li>Compute the gradients:<p class="source-code">        generator_grad = gen_tape \</p><p class="source-code">            .gradient(gen_loss,</p><p class="source-code">                      self.generator.trainable_variables)</p><p class="source-code">        discriminator_grad = dis_tape \</p><p class="source-code">            .gradient(disc_loss,</p><p class="source-code">                self.discriminator.trainable_       variables)</p></li>
				<li>Next, apply the gradients using the respective optimizers:<p class="source-code">        opt_args = zip(generator_grad,</p><p class="source-code">                      self.generator.trainable_variables)</p><p class="source-code">        self.generator_opt.apply_gradients(opt_args)</p><p class="source-code">        opt_args = zip(discriminator_grad,</p><p class="source-code">                       </p><p class="source-code">               self.discriminator.trainable_variables)</p><p class="source-code">        self.discriminator_opt.apply_gradients(opt_args)</p></li>
				<li>Finally, define a <a id="_idIndexMarker474"/>method to train the whole architecture. Every 10 epochs, we will plot the images the generator produces in order to visually assess their quality:<p class="source-code">    def train(self, dataset, test_seed, epochs, </p><p class="source-code">               batch_size):</p><p class="source-code">        for epoch in tqdm(range(epochs)):</p><p class="source-code">            for image_batch in dataset:</p><p class="source-code">                self.train_step(image_batch, </p><p class="source-code">                                 batch_size)</p><p class="source-code">            if epoch == 0 or epoch % 10 == 0:</p><p class="source-code">                </p><p class="source-code">           generate_and_save_images(self.generator,</p><p class="source-code">                                         epoch,</p><p class="source-code">                                         test_seed)</p></li>
				<li>Define a function to produce new images, and then save a 4x4 mosaic of them to disk:<p class="source-code">def generate_and_save_images(model, epoch, test_input):</p><p class="source-code">    predictions = model(test_input, training=False)</p><p class="source-code">    plt.figure(figsize=(4, 4))</p><p class="source-code">    for i in range(predictions.shape[0]):</p><p class="source-code">        plt.subplot(4, 4, i + 1)</p><p class="source-code">        image = predictions[i, :, :, 0] * 127.5 + 127.5</p><p class="source-code">        image = tf.cast(image, tf.uint8)</p><p class="source-code">        plt.imshow(image, cmap='gray')</p><p class="source-code">        plt.axis('off')</p><p class="source-code">    plt.savefig(f'{epoch}.png')</p><p class="source-code">    plt.show()</p></li>
				<li>Define a <a id="_idIndexMarker475"/>function to scale the images that come from the <strong class="source-inline">EMNIST</strong> dataset to the [-1, 1] interval:<p class="source-code">def process_image(input):</p><p class="source-code">    image = tf.cast(input['image'], tf.float32)</p><p class="source-code">    image = (image - 127.5) / 127.5</p><p class="source-code">    return image</p></li>
				<li>Load the <strong class="source-inline">EMNIST</strong> dataset using <strong class="source-inline">tfds</strong>. We'll only use the <strong class="source-inline">'train'</strong> split, which contains more than 600,000 images. We will also make sure to scale each image to the <strong class="source-inline">'tanh'</strong> range:<p class="source-code">BUFFER_SIZE = 1000</p><p class="source-code">BATCH_SIZE = 512</p><p class="source-code">train_dataset = (tfds</p><p class="source-code">                 .load('emnist', split='train')</p><p class="source-code">                 .map(process_image,</p><p class="source-code">                      num_parallel_calls=AUTOTUNE)</p><p class="source-code">                 .shuffle(BUFFER_SIZE)</p><p class="source-code">                 .batch(BATCH_SIZE))</p></li>
				<li>Create a test <a id="_idIndexMarker476"/>seed that will be used throughout the training of the DCGAN to generate images:<p class="source-code">noise_dimension = 100</p><p class="source-code">num_examples_to_generate = 16</p><p class="source-code">seed_shape = (num_examples_to_generate, </p><p class="source-code">              noise_dimension)</p><p class="source-code">test_seed = tf.random.normal(seed_shape)</p></li>
				<li>Finally, instantiate and train a <strong class="source-inline">DCGAN()</strong> instance for 200 epochs:<p class="source-code">EPOCHS = 200</p><p class="source-code">dcgan = DCGAN()</p><p class="source-code">dcgan.train(train_dataset, test_seed, EPOCHS, BATCH_SIZE)</p><p>The first image generated by the GAN will look similar to this, just a collection of shapeless blobs:</p></li>
			</ol>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B14768_06_001.jpg" alt="Figure 6.1 – Images generated at epoch 0&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Images generated at epoch 0</p>
			<p>At the end of <a id="_idIndexMarker477"/>the training process, the results are much better:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B14768_06_002.jpg" alt="Figure 6.2 – Images generated at epoch 200&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.2 – Images generated at epoch 200</p>
			<p>In <em class="italic">Figure 6.2</em>, we can distinguish familiar letters and numbers, including <em class="italic">A</em>, <em class="italic">d</em>, <em class="italic">9</em>, <em class="italic">X</em>, and <em class="italic">B</em>. However, in the first row, we notice a couple of ambiguous forms, which is a sign that the generator has room for improvement. </p>
			<p>Let's see how it all works in the next section.</p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor223"/>How it works…</h2>
			<p>In this recipe, we learned <a id="_idIndexMarker478"/>that GANs work in tandem and, unlike autoencoders, they work against each other (hence the <em class="italic">adversarial</em> in the name) instead of cooperating. When our focus is on the generator, the discriminator is just a tool to train the latter, as is the case in this recipe. This means that after training, the discriminator is tossed out.</p>
			<p>Our generator is actually a decoder that takes random Gaussian vectors of 100 elements and produces 28x28x1 images that are then passed to the discriminator, a regular CNN, which has to guess whether they are real or fake. </p>
			<p>Because our goal is to create the best generator possible, the classification problem the discriminator tries to solve has nothing to do with the actual classes in <strong class="source-inline">EMNIST</strong>. For this reason, we don't explicitly label the images as real or fake beforehand, but in the <strong class="source-inline">discriminator_loss()</strong> method, where we know that all images in <strong class="source-inline">real</strong> come from <strong class="source-inline">EMNIST</strong>, and therefore we compute the loss against a tensor of ones (<strong class="source-inline">tf.ones_like(real)</strong>) and, analogously, all images in <strong class="source-inline">fake</strong> are synthetic, and we compute the loss against a tensor of zeros (<strong class="source-inline">tf.zeros_like(fake)</strong>).</p>
			<p>The generator, on the other hand, takes into consideration the feedback received from the discriminator when computing its loss to improve its outputs.</p>
			<p>It must be noted that the goal here is to achieve an equilibrium, instead of minimizing the loss. Therefore, visual inspection is crucial, and the reason why we save the images the generator produces every 10 epochs.</p>
			<p>In the end, we went from random, shapeless blobs at epoch 0 to recognizable digits and letters at epoch 200, although the network can be improved further.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor224"/>See also</h2>
			<p>You can read more <a id="_idIndexMarker479"/>about <strong class="source-inline">EMNIST</strong> here: <a href="https://arxiv.org/abs/1702.05373v1">https://arxiv.org/abs/1702.05373v1</a>.</p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor225"/>Using a DCGAN for semi-supervised learning</h1>
			<p>Data is the most <a id="_idIndexMarker480"/>important part of developing any <a id="_idIndexMarker481"/>deep learning model. However, good data is often scarce and expensive to acquire. The good news is that GANs can lend us a hand in these situations by artificially producing novel training examples, in a <a id="_idIndexMarker482"/>process known as <strong class="bold">semi-supervised learning</strong>.</p>
			<p>In this recipe, we'll develop a special DCGAN architecture to train a classifier on a very small subset of <strong class="source-inline">Fashion-MNIST</strong> and still achieve a decent performance. </p>
			<p>Let's begin, shall we?</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor226"/>Getting ready</h2>
			<p>We won't require anything extra to access <strong class="source-inline">Fashion-MNIST</strong> because it comes bundled with TensorFlow. In order to display a nice-looking progress bar, let's install <strong class="source-inline">tqdm</strong>:</p>
			<p class="source-code">$&gt; pip install tqdm</p>
			<p>Let's now move on to the next section to start the recipe's implementation.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor227"/>How to do it…</h2>
			<p>Perform the following steps to complete the recipe:</p>
			<ol>
				<li value="1">Let's start by importing the required packages:<p class="source-code">import numpy as np</p><p class="source-code">from numpy.random import *</p><p class="source-code">from tensorflow.keras import backend as K</p><p class="source-code">from tensorflow.keras.datasets import fashion_mnist as fmnist</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import Model</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">from tqdm import tqdm</p></li>
				<li>Define the <strong class="source-inline">pick_supervised_subset()</strong> function to pick a subset of the data. This will allow us to simulate a situation of scarce data, a perfect <a id="_idIndexMarker483"/>fit for semi-supervised <a id="_idIndexMarker484"/>learning:<p class="source-code">def pick_supervised_subset(feats,</p><p class="source-code">                           labels,</p><p class="source-code">                           n_samples=1000,</p><p class="source-code">                           n_classes=10):</p><p class="source-code">    samples_per_class = int(n_samples / n_classes)</p><p class="source-code">    X = []</p><p class="source-code">    y = []</p><p class="source-code">    for i in range(n_classes):</p><p class="source-code">        class_feats = feats[labels == i]</p><p class="source-code">        class_sample_idx = randint(low=0,</p><p class="source-code">                                   </p><p class="source-code">                               high=len(class_feats),</p><p class="source-code">                              size=samples_per_class)</p><p class="source-code">        X.extend([class_feats[j] for j in </p><p class="source-code">                  class_sample_idx])</p><p class="source-code">        y.extend([i] * samples_per_class)</p><p class="source-code">    return np.array(X), np.array(y)</p></li>
				<li>Now, define a function to select a random sample of data for classification. This means that we'll use the labels from the original dataset:<p class="source-code">def pick_samples_for_classification(feats, labels, </p><p class="source-code">                                     n_samples):</p><p class="source-code">    sample_idx = randint(low=0,</p><p class="source-code">                         high=feats.shape[0],</p><p class="source-code">                         size=n_samples)</p><p class="source-code">    X = np.array([feats[i] for i in sample_idx])</p><p class="source-code">    y = np.array([labels[i] for i in sample_idx])</p><p class="source-code">    return X, y</p></li>
				<li>Define the <strong class="source-inline">pick_samples_for_discrimination()</strong> function in order to select a random sample for discrimination. The main difference with the last function is that the labels <a id="_idIndexMarker485"/>here are all 1, indicating <a id="_idIndexMarker486"/>that all images are real, which clearly indicates that this sample is intended for the discriminator:<p class="source-code">def pick_samples_for_discrimination(feats, n_samples):</p><p class="source-code">    sample_idx = randint(low=0,</p><p class="source-code">                         high=feats.shape[0],</p><p class="source-code">                         size=n_samples)</p><p class="source-code">    X = np.array([feats[i] for i in sample_idx])</p><p class="source-code">    y = np.ones((n_samples, 1))</p><p class="source-code">    return X, y</p></li>
				<li>Implement the <strong class="source-inline">generate_fake_samples()</strong> function to produce a batch of latent points or, put another way, a sample of random noise vectors that the generator will use to generate fake images:<p class="source-code">def generate_fake_samples(model, latent_size, </p><p class="source-code">                          n_samples):</p><p class="source-code">    z_input = generate_latent_points(latent_size, </p><p class="source-code">                                      n_samples)</p><p class="source-code">    images = model.predict(z_input)</p><p class="source-code">    y = np.zeros((n_samples, 1))</p><p class="source-code">    return images, y</p></li>
				<li>Create the <strong class="source-inline">generate_fake_samples()</strong> function to generate fake data using the generator:<p class="source-code">def generate_fake_samples(model, latent_size, </p><p class="source-code">                          n_samples):</p><p class="source-code">    z_input = generate_latent_points(latent_size, </p><p class="source-code">                                      n_samples)</p><p class="source-code">    images = model.predict(z_input)</p><p class="source-code">    y = np.zeros((n_samples, 1))</p><p class="source-code">    return images, y</p></li>
				<li>We are ready to define <a id="_idIndexMarker487"/>our semi-supervised <a id="_idIndexMarker488"/>DCGAN, which we'll encapsulate in the <strong class="source-inline">SSGAN()</strong> class defined here. We'll start with the constructor:<p class="source-code">class SSGAN(object):</p><p class="source-code">    def __init__(self,</p><p class="source-code">                 latent_size=100,</p><p class="source-code">                 input_shape=(28, 28, 1),</p><p class="source-code">                 alpha=0.2):</p><p class="source-code">        self.latent_size = latent_size</p><p class="source-code">        self.input_shape = input_shape</p><p class="source-code">        self.alpha = alpha</p></li>
				<li>After storing the arguments as members, let's instantiate the discriminators:<p class="source-code">        (self.classifier,</p><p class="source-code">         self.discriminator) = self._create_discriminators()</p></li>
				<li>Now, compile both the <a id="_idIndexMarker489"/>classifier and discriminator <a id="_idIndexMarker490"/>models:<p class="source-code">        clf_opt = Adam(learning_rate=2e-4, beta_1=0.5)</p><p class="source-code">        self.classifier.compile(</p><p class="source-code">            loss='sparse_categorical_crossentropy',</p><p class="source-code">            optimizer=clf_opt,</p><p class="source-code">            metrics=['accuracy'])</p><p class="source-code">        dis_opt = Adam(learning_rate=2e-4, beta_1=0.5)</p><p class="source-code">        self.discriminator.compile(loss='binary_crossentropy',</p><p class="source-code">                                   optimizer=dis_opt)</p></li>
				<li>Create the generator:<p class="source-code">        self.generator = self._create_generator()</p></li>
				<li>Create the GAN and compile it:<p class="source-code">        self.gan = self._create_gan()</p><p class="source-code">        gan_opt = Adam(learning_rate=2e-4, beta_1=0.5)</p><p class="source-code">        self.gan.compile(loss='binary_crossentropy',</p><p class="source-code">                         optimizer=gan_opt)</p></li>
				<li>Define the private <strong class="source-inline">_create_discriminators()</strong> method to create the discriminators. The inner <strong class="source-inline">custom_activation()</strong> function is used to activate the outputs of the classifier model and generate a value between 0 and 1 that will be used to discern whether the image is real or fake:<p class="source-code">    def _create_discriminators(self, num_classes=10):</p><p class="source-code">        def custom_activation(x):</p><p class="source-code">            log_exp_sum = K.sum(K.exp(x), axis=-1,</p><p class="source-code">                                keepdims=True)</p><p class="source-code">            return log_exp_sum / (log_exp_sum + 1.0)</p></li>
				<li>Define the <a id="_idIndexMarker491"/>classifier architecture, which <a id="_idIndexMarker492"/>is just a regular softmax-activated CNN:<p class="source-code">        input = Input(shape=self.input_shape)</p><p class="source-code">        x = input</p><p class="source-code">        for _ in range(3):</p><p class="source-code">            x = Conv2D(filters=128,</p><p class="source-code">                       kernel_size=(3, 3),</p><p class="source-code">                       strides=2,</p><p class="source-code">                       padding='same')(x)</p><p class="source-code">            x = LeakyReLU(alpha=self.alpha)(x)</p><p class="source-code">        x = Flatten()(x)</p><p class="source-code">        x = Dropout(rate=0.4)(x)</p><p class="source-code">        x = Dense(units=num_classes)(x)</p><p class="source-code">        clf_output = Softmax()(x)</p><p class="source-code">        clf_model = Model(input, clf_output)</p></li>
				<li>The discriminator shares weights with the classifier, but instead of softmax activating the outputs, it uses the <strong class="source-inline">custom_activation()</strong> function defined previously:<p class="source-code">        dis_output = Lambda(custom_activation)(x)</p><p class="source-code">        discriminator_model = Model(input, dis_output)</p></li>
				<li>Return both the classifier and the discriminator:<p class="source-code">        return clf_model, discriminator_model</p></li>
				<li>Create <a id="_idIndexMarker493"/>the private <strong class="source-inline">_create_generator()</strong> method to implement the generator <a id="_idIndexMarker494"/>architecture, which is just a decoder, as explained in the first recipe in this chapter:<p class="source-code">    def _create_generator(self):</p><p class="source-code">        input = Input(shape=(self.latent_size,))</p><p class="source-code">        x = Dense(units=128 * 7 * 7)(input)</p><p class="source-code">        x = LeakyReLU(alpha=self.alpha)(x)</p><p class="source-code">        x = Reshape((7, 7, 128))(x)</p><p class="source-code">        for _ in range(2):</p><p class="source-code">            x = Conv2DTranspose(filters=128,</p><p class="source-code">                                kernel_size=(4, 4),</p><p class="source-code">                                strides=2,</p><p class="source-code">                                padding='same')(x)</p><p class="source-code">            x = LeakyReLU(alpha=self.alpha)(x)</p><p class="source-code">        x = Conv2D(filters=1,</p><p class="source-code">                   kernel_size=(7, 7),</p><p class="source-code">                   padding='same')(x)</p><p class="source-code">        output = Activation('tanh')(x)</p><p class="source-code">        return Model(input, output)</p></li>
				<li>Define the private <strong class="source-inline">_create_gan()</strong> method to create the GAN itself, which is just the connection between the generator and the discriminator:<p class="source-code">    def _create_gan(self):</p><p class="source-code">        self.discriminator.trainable = False</p><p class="source-code">        output = </p><p class="source-code">              self.discriminator(self.generator.output)</p><p class="source-code">        return Model(self.generator.input, output)</p></li>
				<li>Finally, define <strong class="source-inline">train()</strong>, a function <a id="_idIndexMarker495"/>to train the whole system. We'll start by selecting the <a id="_idIndexMarker496"/>subset of <strong class="source-inline">Fashion-MNIST</strong> that we'll train on, and then we'll define the number of batches and training steps required to fit the architecture:<p class="source-code">    def train(self, X, y, epochs=20, num_batches=100):</p><p class="source-code">        X_sup, y_sup = pick_supervised_subset(X, y)</p><p class="source-code">        batches_per_epoch = int(X.shape[0] / num_batches)</p><p class="source-code">        num_steps = batches_per_epoch * epochs</p><p class="source-code">        num_samples = int(num_batches / 2)</p></li>
				<li>Pick samples for classification, and use these to fit the classifier:<p class="source-code">        for _ in tqdm(range(num_steps)):</p><p class="source-code">            X_sup_real, y_sup_real = \</p><p class="source-code">                pick_samples_for_classification(X_sup,</p><p class="source-code">                                                y_sup,</p><p class="source-code">                                          num_samples)</p><p class="source-code">            self.classifier.train_on_batch(X_sup_real,</p><p class="source-code">                                           y_sup_real)</p></li>
				<li>Pick <a id="_idIndexMarker497"/>real samples for discrimination, and use <a id="_idIndexMarker498"/>these to fit the discriminator:<p class="source-code">            X_real, y_real = \</p><p class="source-code">                pick_samples_for_discrimination(X,</p><p class="source-code">                                          num_samples)</p><p class="source-code">        self.discriminator.train_on_batch(X_real, y_real)</p></li>
				<li>Use the generator to produce fake data, and use this to fit the discriminator:<p class="source-code">            X_fake, y_fake = \</p><p class="source-code">                generate_fake_samples(self.generator,</p><p class="source-code">                                    self.latent_size,</p><p class="source-code">                                      num_samples)</p><p class="source-code">            self.discriminator.train_on_batch(X_fake, </p><p class="source-code">                                             y_fake)</p></li>
				<li>Generate latent points, and use these to train the GAN:<p class="source-code">            X_gan = generate_latent_points(self.latent_size,</p><p class="source-code">                      num_batches)</p><p class="source-code">            y_gan = np.ones((num_batches, 1))</p><p class="source-code">            self.gan.train_on_batch(X_gan, y_gan)</p></li>
				<li>Load <strong class="source-inline">Fashion-MNIST</strong> and normalize both the training and test sets:<p class="source-code">(X_train, y_train), (X_test, y_test) = fmnist.load_data()</p><p class="source-code">X_train = np.expand_dims(X_train, axis=-1)</p><p class="source-code">X_train = (X_train.astype(np.float32) - 127.5) / 127.5</p><p class="source-code">X_test = np.expand_dims(X_test, axis=-1)</p><p class="source-code">X_test = (X_test.astype(np.float32) - 127.5) / 127.5</p></li>
				<li>Instantiate <a id="_idIndexMarker499"/>an <strong class="source-inline">SSCGAN()</strong> and train it <a id="_idIndexMarker500"/>for 30 epochs:<p class="source-code">ssgan = SSGAN()</p><p class="source-code">ssgan.train(X_train, y_train, epochs=30)</p></li>
				<li>Report the accuracy of the classifier on both the training and test sets:<p class="source-code">train_acc = ssgan.classifier.evaluate(X_train, </p><p class="source-code">                                      y_train)[1]</p><p class="source-code">train_acc *= 100</p><p class="source-code">print(f'Train accuracy: {train_acc:.2f}%')</p><p class="source-code">test_acc = ssgan.classifier.evaluate(X_test, y_test)[1]</p><p class="source-code">test_acc *= 100</p><p class="source-code">print(f'Test accuracy: {test_acc:.2f}%')</p></li>
			</ol>
			<p>After the training finishes, both the training and test accuracy should be around 83%, which is pretty satisfying if we consider we only used 1,000 examples out of 50,000!</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor228"/>How it works…</h2>
			<p>In this recipe, we implemented an architecture quite similar to the one implemented in the <em class="italic">Implementing a deep convolutional GAN</em> recipe that opened this chapter. The main difference <a id="_idIndexMarker501"/>resides in the fact that we have two <a id="_idIndexMarker502"/>discriminators: the first one is actually a classifier, which is trained on the small subset of labeled data at our disposal. The other is a regular discriminator, whose sole job is to not be fooled by the generator.</p>
			<p>How does the classifier achieve such a respectable performance with so little data? The answer is shared weights. Both the classifier and the discriminator share the same feature extraction layers, differing only in the final output layer, which is activated with a plain old softmax function in the case of the classifier, and with a <strong class="source-inline">Lambda()</strong> layer that wraps our <strong class="source-inline">custom_activation()</strong> function in the case of the discriminator.</p>
			<p>This means that these shared weights get updated each time the classifier trains on a batch of labeled data, and also when the discriminator trains on both real and fake images. In the end, we circumvent the data scarcity problem with the aid of the generator.</p>
			<p>Pretty impressive, right? </p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor229"/>See also</h2>
			<p>You can consolidate your understanding of the semi-supervised training approach used in this recipe by reading the paper where it was first proposed: <a href="https://arxiv.org/abs/1606.03498">https://arxiv.org/abs/1606.03498</a>.</p>
			<h1 id="_idParaDest-195"><a id="_idTextAnchor230"/>Translating images with Pix2Pix</h1>
			<p>One of the most <a id="_idIndexMarker503"/>interesting applications of GANs is image-to-image translation, which, as <a id="_idIndexMarker504"/>the name suggests, consists of translating the content from one image domain to another (for instance, sketches to photos, black and white images to RGB, and Google Maps to satellite views, among others). </p>
			<p>In this recipe, we'll implement a fairly complex conditional adversarial <a id="_idIndexMarker505"/>network known as Pix2Pix. We'll focus solely on the practical aspects of the solution, but if you want to get familiar with the literature, check out the <em class="italic">See also</em> section at the end of the recipe.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor231"/>Getting ready</h2>
			<p>We'll use the <strong class="source-inline">cityscapes</strong> dataset, which is available here: https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets/cityscapes.tar.gz. Download it and decompress it in a location of your choosing. For the purposes of this tutorial, we will assume that it's placed in the <strong class="source-inline">~/.keras/datasets</strong> directory, under the name <strong class="source-inline">cityscapes</strong>. To display a progress bar during training, install <strong class="source-inline">tqdm</strong>:</p>
			<p class="source-code">$&gt; pip install tqdm</p>
			<p>By the end of this recipe, we'll learn to generate the image on the left from the right one using Pix2Pix:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B14768_06_003.jpg" alt="Figure 6.3 – We will use the segmented images on the right to produce real-world images like the one on the left"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.3 – We will use the segmented images on the right to produce real-world images like the one on the left</p>
			<p>Let's get started!</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor232"/>How to do it…</h2>
			<p>After completing these steps, you'll have implemented Pix2Pix from scratch!</p>
			<ol>
				<li value="1">Import the dependencies:<p class="source-code">import pathlib</p><p class="source-code">import cv2</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tqdm</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import BinaryCrossentropy</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p></li>
				<li>Define <a id="_idIndexMarker506"/>constants for TensorFlow's autotuning and resizing <a id="_idIndexMarker507"/>options, as well as the dimensions. We will resize all the images in the dataset:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p><p class="source-code">NEAREST_NEIGHBOR = tf.image.ResizeMethod.NEAREST_NEIGHBOR</p><p class="source-code">IMAGE_WIDTH = 256</p><p class="source-code">IMAGE_HEIGHT = 256</p></li>
				<li>Each image in the dataset is comprised of both the input and target, so after processing it, we need to split them into separate images. The <strong class="source-inline">load_image()</strong> function does this:<p class="source-code">def load_image(image_path):</p><p class="source-code">    image = tf.io.read_file(image_path)</p><p class="source-code">    image = tf.image.decode_jpeg(image)</p><p class="source-code">    width = tf.shape(image)[1]</p><p class="source-code">    width = width // 2</p><p class="source-code">    real_image = image[:, :width, :]</p><p class="source-code">    input_image = image[:, width:, :]</p><p class="source-code">    input_image = tf.cast(input_image, tf.float32)</p><p class="source-code">    real_image = tf.cast(real_image, tf.float32)</p><p class="source-code">    return input_image, real_image</p></li>
				<li>Let's <a id="_idIndexMarker508"/>create the <strong class="source-inline">resize()</strong> function to resize both the input and <a id="_idIndexMarker509"/>target images:<p class="source-code"> def resize(input_image, real_image, height, width):</p><p class="source-code">    input_image = tf.image.resize(input_image,</p><p class="source-code">                              size=(height,width),</p><p class="source-code">                             method=NEAREST_NEIGHBOR)</p><p class="source-code">    real_image = tf.image.resize(real_image,</p><p class="source-code">                                 size=(height, width),</p><p class="source-code">                              method=NEAREST_NEIGHBOR)</p><p class="source-code">    return input_image, real_image</p></li>
				<li>Now, implement the <strong class="source-inline">random_crop()</strong> function to perform random cropping on the images:<p class="source-code">def random_crop(input_image, real_image):</p><p class="source-code">    stacked_image = tf.stack([input_image, </p><p class="source-code">                             real_image],axis=0)</p><p class="source-code">    size = (2, IMAGE_HEIGHT, IMAGE_WIDTH, 3)</p><p class="source-code">    cropped_image = tf.image.random_crop(stacked_image,</p><p class="source-code">                                         size=size)</p><p class="source-code">    input_image = cropped_image[0]</p><p class="source-code">    real_image = cropped_image[1]</p><p class="source-code">    return input_image, real_image</p></li>
				<li>Next, code <a id="_idIndexMarker510"/>up the <strong class="source-inline">normalize()</strong> function to normalize the <a id="_idIndexMarker511"/>images to the range [-1, 1]:<p class="source-code">def normalize(input_image, real_image):</p><p class="source-code">    input_image = (input_image / 127.5) - 1</p><p class="source-code">    real_image = (real_image / 127.5) - 1</p><p class="source-code">    return input_image, real_image</p></li>
				<li>Define the <strong class="source-inline">random_jitter()</strong> function, which performs random jittering on the input images (notice that it uses the functions defined in <em class="italic">Step 4</em> and <em class="italic">Step 5</em>):<p class="source-code">@tf.function</p><p class="source-code">def random_jitter(input_image, real_image):</p><p class="source-code">    input_image, real_image = resize(input_image, </p><p class="source-code">                                     real_image,</p><p class="source-code">                                     width=286, </p><p class="source-code">                                      height=286)</p><p class="source-code">    input_image, real_image = random_crop(input_image,</p><p class="source-code">                                          real_image)</p><p class="source-code">    if np.random.uniform() &gt; 0.5:</p><p class="source-code">        input_image = \</p><p class="source-code">              tf.image.flip_left_right(input_image)</p><p class="source-code">        real_image = \</p><p class="source-code">             tf.image.flip_left_right(real_image)</p><p class="source-code">    return input_image, real_image</p></li>
				<li>Create <a id="_idIndexMarker512"/>the <strong class="source-inline">load_training_image()</strong> function to load <a id="_idIndexMarker513"/>and augment the training images:<p class="source-code">def load_training_image(image_path):</p><p class="source-code">    input_image, real_image = load_image(image_path)</p><p class="source-code">    input_image, real_image = \</p><p class="source-code">        random_jitter(input_image, real_image)</p><p class="source-code">    input_image, real_image = \</p><p class="source-code">        normalize(input_image, real_image)</p><p class="source-code">    return input_image, real_image</p></li>
				<li>Let's now implement the <strong class="source-inline">load_test_image()</strong> function, which, as its name indicates, will be used to load test images:<p class="source-code">def load_test_image(image_path):</p><p class="source-code">    input_image, real_image = load_image(image_path)</p><p class="source-code">    input_image, real_image = resize(input_image, </p><p class="source-code">                                     real_image,</p><p class="source-code">                                   width=IMAGE_WIDTH,</p><p class="source-code">                                 height=IMAGE_HEIGHT)</p><p class="source-code">    input_image, real_image = \</p><p class="source-code">        normalize(input_image, real_image)</p><p class="source-code">    return input_image, real_image</p></li>
				<li>Now, let's proceed to create the <strong class="source-inline">generate_and_save_images()</strong> function to store <a id="_idIndexMarker514"/>synthetic images created by the generator model. The <a id="_idIndexMarker515"/>resulting images will be a concatenation of <strong class="source-inline">input</strong>, <strong class="source-inline">target</strong>, and <strong class="source-inline">prediction</strong>:<p class="source-code">def generate_and_save_images(model, input, target,epoch):</p><p class="source-code">    prediction = model(input, training=True)</p><p class="source-code">    display_list = [input[0], target[0], prediction[0]]</p><p class="source-code">    image = np.hstack(display_list)</p><p class="source-code">    image *= 0.5</p><p class="source-code">    image += 0.5</p><p class="source-code">    image *= 255.0</p><p class="source-code">    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)</p><p class="source-code">    cv2.imwrite(f'{epoch + 1}.jpg', image)</p></li>
				<li>Next, define the <strong class="source-inline">Pix2Pix()</strong> class, which encapsulates this architecture implementation. Start with the constructor:<p class="source-code">class Pix2Pix(object):</p><p class="source-code">    def __init__(self, output_channels=3, </p><p class="source-code">                 lambda_value=100):</p><p class="source-code">        self.loss = BinaryCrossentropy(from_logits=True)</p><p class="source-code">        self.output_channels = output_channels</p><p class="source-code">        self._lambda = lambda_value</p><p class="source-code">        self.generator = self.create_generator()</p><p class="source-code">        self.discriminator = self.create_discriminator()</p><p class="source-code">        self.gen_opt = Adam(learning_rate=2e-4, </p><p class="source-code">                             beta_1=0.5)</p><p class="source-code">        self.dis_opt = Adam(learning_rate=2e-4, </p><p class="source-code">                             beta_1=0.5)</p></li>
				<li>The <a id="_idIndexMarker516"/>constructor implemented in <em class="italic">Step 11</em> defines the loss <a id="_idIndexMarker517"/>function to be used (<strong class="bold">binary cross-entropy</strong>), the lambda value (used in <em class="italic">Step 18</em>), and instantiates the generator and the discriminator, as well as their respective optimizers. Our generator is a modified <strong class="bold">U-Net</strong>, which is a <a id="_idIndexMarker518"/>U-shaped network comprising downsampling and upsampling blocks. Let's create a static method to produce a downsample block: <p class="source-code">    @staticmethod</p><p class="source-code">    def downsample(filters, size, batch_norm=True):</p><p class="source-code">       initializer = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        layers = Sequential()</p><p class="source-code">        layers.add(Conv2D(filters=filters,</p><p class="source-code">                          kernel_size=size,</p><p class="source-code">                          strides=2,</p><p class="source-code">                          padding='same',</p><p class="source-code">                          </p><p class="source-code">                      kernel_initializer=initializer,</p><p class="source-code">                          use_bias=False))</p><p class="source-code">        if batch_norm:</p><p class="source-code">            layers.add(BatchNormalization())</p><p class="source-code">        layers.add(LeakyReLU())</p><p class="source-code">        return layers</p></li>
				<li>A downsample <a id="_idIndexMarker519"/>block is a convolution, optionally batch <a id="_idIndexMarker520"/>normalized, and activated with <strong class="source-inline">LeakyReLU()</strong>. Let's now implement a static method to create upsampling blocks:<p class="source-code">    @staticmethod</p><p class="source-code">    def upsample(filters, size, dropout=False):</p><p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        layers = Sequential()</p><p class="source-code">        layers.add(Conv2DTranspose(filters=filters,</p><p class="source-code">                                   kernel_size=size,</p><p class="source-code">                                   strides=2,</p><p class="source-code">                                   padding='same',</p><p class="source-code">                             kernel_initializer=init,</p><p class="source-code">                                   use_bias=False))</p><p class="source-code">        layers.add(BatchNormalization())</p><p class="source-code">        if dropout:</p><p class="source-code">            layers.add(Dropout(rate=0.5))</p><p class="source-code">        layers.add(ReLU())</p><p class="source-code">        return layers</p></li>
				<li>An upsampling <a id="_idIndexMarker521"/>block is a transposed convolution, optionally <a id="_idIndexMarker522"/>followed by dropout and with <strong class="source-inline">ReLU()</strong> activated. Let's now use these two convenience methods to implement the U-Net generator:<p class="source-code">    def create_generator(self, input_shape=(256, 256,3)):</p><p class="source-code">        down_stack = [self.downsample(64,4,batch_norm=False)]</p><p class="source-code">        for filters in (128, 256, 512, 512, 512, 512, </p><p class="source-code">                         512):</p><p class="source-code">            down_block = self.downsample(filters, 4)</p><p class="source-code">            down_stack.append(down_block)</p></li>
				<li>After defining the downsampling stack, let's do the same with the upsampling layers:<p class="source-code">        up_stack = []</p><p class="source-code">        for _ in range(3):</p><p class="source-code">            up_block = self.upsample(512, 4,dropout=True)</p><p class="source-code">            up_stack.append(up_block)</p><p class="source-code">        for filters in (512, 256, 128, 64):</p><p class="source-code">            up_block = self.upsample(filters, 4)</p><p class="source-code">            up_stack.append(up_block)</p></li>
				<li>Thread the input <a id="_idIndexMarker523"/>through the down and up stacks, and <a id="_idIndexMarker524"/>also add skip connections to prevent the depth of the network from impeding its learning:<p class="source-code">        inputs = Input(shape=input_shape)</p><p class="source-code">        x = inputs</p><p class="source-code">        skip_layers = []</p><p class="source-code">        for down in down_stack:</p><p class="source-code">            x = down(x)</p><p class="source-code">            skip_layers.append(x)</p><p class="source-code">        skip_layers = reversed(skip_layers[:-1])</p><p class="source-code">        for up, skip_connection in zip(up_stack, </p><p class="source-code">                                       skip_layers):</p><p class="source-code">            x = up(x)</p><p class="source-code">            x = Concatenate()([x, skip_connection])</p></li>
				<li>The output layers are a transposed convolution with <strong class="source-inline">'tanh'</strong> activated:<p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        output = Conv2DTranspose(</p><p class="source-code">            filters=self.output_channels,</p><p class="source-code">            kernel_size=4,</p><p class="source-code">            strides=2,</p><p class="source-code">            padding='same',</p><p class="source-code">            kernel_initializer=init,</p><p class="source-code">            activation='tanh')(x)</p><p class="source-code">        return Model(inputs, outputs=output)</p></li>
				<li>Define a <a id="_idIndexMarker525"/>method to compute <a id="_idIndexMarker526"/>the generator loss, as the authors of Pix2Pix recommend. Notice the use of the <strong class="source-inline">self._lambda</strong> constant:<p class="source-code">    def generator_loss(self,</p><p class="source-code">                       discriminator_generated_output,</p><p class="source-code">                       generator_output,</p><p class="source-code">                       target):</p><p class="source-code">        gan_loss = self.loss(</p><p class="source-code">            tf.ones_like(discriminator_generated_output),</p><p class="source-code">            discriminator_generated_output)</p><p class="source-code">        # MAE</p><p class="source-code">        error = target - generator_output</p><p class="source-code">        l1_loss = tf.reduce_mean(tf.abs(error))</p><p class="source-code">        total_gen_loss = gan_loss + (self._lambda * </p><p class="source-code">                                      l1_loss)</p><p class="source-code">        return total_gen_loss, gan_loss, l1_loss</p></li>
				<li>The <a id="_idIndexMarker527"/>discriminator, defined in this <a id="_idIndexMarker528"/>step, receives two images; the input and the target:<p class="source-code">    def create_discriminator(self):</p><p class="source-code">        input = Input(shape=(256, 256, 3))</p><p class="source-code">        target = Input(shape=(256, 256, 3))</p><p class="source-code">        x = Concatenate()([input, target])</p><p class="source-code">        x = self.downsample(64, 4, False)(x)</p><p class="source-code">        x = self.downsample(128, 4)(x)</p><p class="source-code">        x = self.downsample(256, 4)(x)</p><p class="source-code">        x = ZeroPadding2D()(x)</p></li>
				<li>Notice that the last couple of layers are convolutions, instead of <strong class="source-inline">Dense()</strong> layers. This is because the discriminator works on patches of images at a time, and tells whether each patch is real or fake:<p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        x = Conv2D(filters=512,</p><p class="source-code">                   kernel_size=4,</p><p class="source-code">                   strides=1,</p><p class="source-code">                   kernel_initializer=init,</p><p class="source-code">                   use_bias=False)(x)</p><p class="source-code">        x = BatchNormalization()(x)</p><p class="source-code">        x = LeakyReLU()(x)</p><p class="source-code">        x = ZeroPadding2D()(x)</p><p class="source-code">        output = Conv2D(filters=1,</p><p class="source-code">                        kernel_size=4,</p><p class="source-code">                        strides=1,</p><p class="source-code">                        kernel_initializer=init)(x)</p><p class="source-code">        return Model(inputs=[input, target], </p><p class="source-code">                    outputs=output)</p></li>
				<li>Define <a id="_idIndexMarker529"/>the discriminator <a id="_idIndexMarker530"/>loss:<p class="source-code">    def discriminator_loss(self,</p><p class="source-code">                           discriminator_real_output,</p><p class="source-code">                         discriminator_generated_output):</p><p class="source-code">        real_loss = self.loss(</p><p class="source-code">            tf.ones_like(discriminator_real_output),</p><p class="source-code">            discriminator_real_output)</p><p class="source-code">        fake_loss = self.loss(</p><p class="source-code">            tf.zeros_like(discriminator_generated_output),</p><p class="source-code">            discriminator_generated_output)</p><p class="source-code">        return real_loss + fake_loss</p></li>
				<li>Define a function to perform a single train step, named <strong class="source-inline">train_step()</strong>, consisting of taking the <a id="_idIndexMarker531"/>input image, passing through <a id="_idIndexMarker532"/>the generator, and then using the discriminator on the input image paired with the original target image, and then on the input imaged paired with the fake image output from the generator:<p class="source-code">    @tf.function</p><p class="source-code">    def train_step(self, input_image, target):</p><p class="source-code">        with tf.GradientTape() as gen_tape, \</p><p class="source-code">                tf.GradientTape() as dis_tape:</p><p class="source-code">            gen_output = self.generator(input_image,</p><p class="source-code">                                        training=True)</p><p class="source-code">            dis_real_output = self.discriminator(</p><p class="source-code">                [input_image, target], training=True)</p><p class="source-code">            dis_gen_output = self.discriminator(</p><p class="source-code">                [input_image, gen_output], </p><p class="source-code">                        training=True)</p></li>
				<li>Next, the losses are computed, along with the gradients:<p class="source-code">            (gen_total_loss, gen_gan_loss,   </p><p class="source-code">               gen_l1_loss) = \</p><p class="source-code">                self.generator_loss(dis_gen_output,</p><p class="source-code">                                    gen_output,</p><p class="source-code">                                    target)</p><p class="source-code">            dis_loss = \</p><p class="source-code">                self.discriminator_loss(dis_real_output,</p><p class="source-code">                                        </p><p class="source-code">                        dis_gen_output)</p><p class="source-code">        gen_grads = gen_tape. \</p><p class="source-code">            gradient(gen_total_loss,</p><p class="source-code">                     self.generator.trainable_variables)</p><p class="source-code">        dis_grads = dis_tape. \</p><p class="source-code">            gradient(dis_loss,</p><p class="source-code">                     self.discriminator.trainable_variables)</p></li>
				<li>Use the gradients <a id="_idIndexMarker533"/>to update the models through the <a id="_idIndexMarker534"/>respective optimizers:<p class="source-code">        opt_args = zip(gen_grads,</p><p class="source-code">                       self.generator.trainable_variables)</p><p class="source-code">        self.gen_opt.apply_gradients(opt_args)</p><p class="source-code">        opt_args = zip(dis_grads,</p><p class="source-code">                       self.discriminator.trainable_variables)</p><p class="source-code">        self.dis_opt.apply_gradients(opt_args)</p></li>
				<li>Implement <strong class="source-inline">fit()</strong>, a method to train the whole architecture. For each epoch, we'll save to disk the images generated to visually assess the performance of the model:<p class="source-code">    def fit(self, train, epochs, test):</p><p class="source-code">        for epoch in tqdm.tqdm(range(epochs)):</p><p class="source-code">            for example_input, example_target in </p><p class="source-code">                              test.take(1):</p><p class="source-code">                generate_and_save_images(self.generator,</p><p class="source-code">                                       example_input,</p><p class="source-code">                                       example_target,</p><p class="source-code">                                         epoch)</p><p class="source-code">            for input_image, target in train:</p><p class="source-code">                self.train_step(input_image, target)</p></li>
				<li>Assemble <a id="_idIndexMarker535"/>the path to the <a id="_idIndexMarker536"/>training and test splits of the dataset:<p class="source-code">dataset_path = (pathlib.Path.home() / '.keras' / </p><p class="source-code">                'datasets' /'cityscapes')</p><p class="source-code">train_dataset_pattern = str(dataset_path / 'train' / </p><p class="source-code">                             '*.jpg')</p><p class="source-code">test_dataset_pattern = str(dataset_path / 'val' / </p><p class="source-code">                           '*.jpg')</p></li>
				<li>Define the training and test datasets:<p class="source-code">BUFFER_SIZE = 400</p><p class="source-code">BATCH_SIZE = 1</p><p class="source-code">train_ds = (tf.data.Dataset</p><p class="source-code">            .list_files(train_dataset_pattern)</p><p class="source-code">            .map(load_training_image,</p><p class="source-code">                 num_parallel_calls=AUTOTUNE)</p><p class="source-code">            .shuffle(BUFFER_SIZE)</p><p class="source-code">            .batch(BATCH_SIZE))</p><p class="source-code">test_ds = (tf.data.Dataset</p><p class="source-code">           .list_files(test_dataset_pattern)</p><p class="source-code">           .map(load_test_image)</p><p class="source-code">           .batch(BATCH_SIZE))</p></li>
				<li>Instantiate <strong class="source-inline">Pix2Pix()</strong> and <a id="_idIndexMarker537"/>fit it over 150 <a id="_idIndexMarker538"/>epochs:<p class="source-code">pix2pix = Pix2Pix()</p><p class="source-code">pix2pix.fit(train_ds, epochs=150, test=test_ds)</p><p>Here's a generated image at epoch 1:</p></li>
			</ol>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B14768_06_004.jpg" alt="Figure 6.4 – At first, the generator only produces noise&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.4 – At first, the generator only produces noise</p>
			<p>And here's one at epoch 150:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B14768_06_005.jpg" alt="Figure 6.5 – At the end of its training run, the generator is capable of producing reasonable results&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.5 – At the end of its training run, the generator is capable of producing reasonable results</p>
			<p>When the training <a id="_idIndexMarker539"/>ends, our Pix2Pix architecture can translate <a id="_idIndexMarker540"/>segmented images to real scenes, as demonstrated in <em class="italic">Figure 6.5</em>, where the first image is the input, the second is the target, and the rightmost is the generated one. </p>
			<p>Let's connect the dots in the next section.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor233"/>How it works…</h2>
			<p>In this recipe, we <a id="_idIndexMarker541"/>implemented an architecture which was a bit hard, but was based, but <a id="_idIndexMarker542"/>based on the same ideas as all GANs. The main difference is that this time, the discriminator works on patches, instead of whole images. More specifically, the discriminator looks at patches of the original and fake images at a time and decides whether those patches belong to real or synthetized images.</p>
			<p>Because image-to-image translation is a form of image segmentation, our generator is a modified U-Net, a groundbreaking type of CNN first used for biomedical image segmentation. </p>
			<p>Because Pix2Pix is such a complex and deep network, the training process takes several hours to complete, but in the end, we obtained very good results translating the content of segmented city landscapes to real-looking predictions. Impressive! </p>
			<p>If you want to take a look at other produced images, as well as a graphical representation of the generator and discriminator, consult the official repository at <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe3">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe3</a>.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor234"/>See also</h2>
			<p>I recommend you read the original paper by Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros, the authors of <strong class="bold">Pix2Pix</strong>, here: <a href="https://arxiv.org/abs/1611.07004">https://arxiv.org/abs/1611.07004</a>. We used a U-Net as the generator, which you can read more about here: <a href="https://arxiv.org/abs/1505.04597">https<span id="_idTextAnchor235"/><span id="_idTextAnchor236"/>://arxiv.org/abs/1505.04597</a>.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor237"/>Translating unpaired images with CycleGAN</h1>
			<p>In the <em class="italic">Translating images with Pix2Pix</em> recipe, we <a id="_idIndexMarker543"/>discovered how to transfer <a id="_idIndexMarker544"/>images from one domain to another. However, in the end, it's supervised learning that requires a pairing of input and target images in order for Pix2Pix to learn the correct mapping. Wouldn't it be great if we could bypass this pairing condition, and let the network figure out on its own how to translate the characteristics from one domain to another, while preserving image consistency? </p>
			<p>Well, that's what <strong class="bold">CycleGAN</strong> does, and in this recipe, we'll implement one from scratch to convert pictures of Yosemite National Park taken during the summer into their winter counterparts! </p>
			<p>Let's get started.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor238"/>Getting ready</h2>
			<p>We'll use <strong class="source-inline">OpenCV</strong>, <strong class="source-inline">tqdm</strong>, and <strong class="source-inline">tensorflow-datasets</strong> in this recipe. </p>
			<p>Install <a id="_idIndexMarker545"/>these simultaneously <a id="_idIndexMarker546"/>with <strong class="source-inline">pip</strong>:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python tqdm tensorflow-datasets</p>
			<p>Through the TensorFlow datasets, we'll access the <strong class="source-inline">cyclegan/summer2winter_yosemite</strong> dataset.</p>
			<p>Here are some sample images of this dataset:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B14768_06_006.jpg" alt="Figure 6.6 – Left: Yosemite during summer; right: Yosemite during winter&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.6 – Left: Yosemite during summer; right: Yosemite during winter</p>
			<p class="callout-heading"> Tip</p>
			<p class="callout">The implementation of CycleGAN is very similar to Pix2Pix. Therefore, we won't explain most of it in detail. Instead, I encourage you to complete the <em class="italic">Translating images with Pix2Pix</em> recipe before tackling this one.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor239"/>How to do it…</h2>
			<p>Perform the following steps to complete the recipe:</p>
			<ol>
				<li value="1">Import the necessary dependencies:<p class="source-code">import cv2</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_datasets as tfds</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.losses import BinaryCrossentropy</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.optimizers import Adam</p><p class="source-code">from tqdm import tqdm</p></li>
				<li>Define <a id="_idIndexMarker547"/>an alias <a id="_idIndexMarker548"/>for <strong class="source-inline">tf.data.experimental.AUTOTUNE</strong>:<p class="source-code">AUTOTUNE = tf.data.experimental.AUTOTUNE</p></li>
				<li>Define a function to perform the random cropping of an image:<p class="source-code">def random_crop(image):</p><p class="source-code">    return tf.image.random_crop(image, size=(256, 256, </p><p class="source-code">                                               3))</p></li>
				<li>Define a function to normalize images to the range [-1, 1]:<p class="source-code">def normalize(image):</p><p class="source-code">    image = tf.cast(image, tf.float32)</p><p class="source-code">    image = (image / 127.5) - 1</p><p class="source-code">    return image</p></li>
				<li>Define a function to perform random jittering on an image:<p class="source-code">def random_jitter(image):</p><p class="source-code">    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR</p><p class="source-code">    image = tf.image.resize(image, (286, 286), </p><p class="source-code">                            method=method)</p><p class="source-code">    image = random_crop(image)</p><p class="source-code">    image = tf.image.random_flip_left_right(image)</p><p class="source-code">    return image</p></li>
				<li>Define a function to preprocess and augment training images: <p class="source-code">def preprocess_training_image(image, _):</p><p class="source-code">    image = random_jitter(image)</p><p class="source-code">    image = normalize(image)</p><p class="source-code">    return image</p></li>
				<li>Define a <a id="_idIndexMarker549"/>function to preprocess test <a id="_idIndexMarker550"/>images:<p class="source-code">def preprocess_test_image(image, _):</p><p class="source-code">    image = normalize(image)</p><p class="source-code">    return image</p></li>
				<li>Define a function to generate and save images using the generator model. The resulting images will be a concatenation of the input and the prediction:<p class="source-code">def generate_images(model, test_input, epoch):</p><p class="source-code">    prediction = model(test_input)</p><p class="source-code">    image = np.hstack([test_input[0], prediction[0]])</p><p class="source-code">    image *= 0.5</p><p class="source-code">    image += 0.5</p><p class="source-code">    image *= 255.0</p><p class="source-code">    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)</p><p class="source-code">    cv2.imwrite(f'{epoch + 1}.jpg', image)</p></li>
				<li>Define a custom instance normalization layer, starting with the constructor:<p class="source-code">class InstanceNormalization(Layer):</p><p class="source-code">    def __init__(self, epsilon=1e-5):</p><p class="source-code">        super(InstanceNormalization, self).__init__()</p><p class="source-code">        self.epsilon = epsilon</p></li>
				<li>Now, define <a id="_idIndexMarker551"/>the <strong class="source-inline">build()</strong> method, which creates the inner <a id="_idIndexMarker552"/>components of the <strong class="source-inline">InstanceNormalization()</strong> class:<p class="source-code">    def build(self, input_shape):</p><p class="source-code">        init = tf.random_normal_initializer(1.0, 0.02)</p><p class="source-code">        self.scale = self.add_weight(name='scale',</p><p class="source-code">                               shape=input_shape[-1:],</p><p class="source-code">                                     initializer=init,</p><p class="source-code">                                     trainable=True)</p><p class="source-code">        self.offset = self.add_weight(name='offset',</p><p class="source-code">                               shape=input_shape[-1:],</p><p class="source-code">                                  initializer='zeros',</p><p class="source-code">                                      trainable=True)</p></li>
				<li>Create the <strong class="source-inline">call()</strong> method, which implements the logic to instance-normalize the input tensor, <strong class="source-inline">x</strong>:<p class="source-code">    def call(self, x):</p><p class="source-code">        mean, variance = tf.nn.moments(x,</p><p class="source-code">                                       axes=(1, 2),</p><p class="source-code">                                       keepdims=True)</p><p class="source-code">        inv = tf.math.rsqrt(variance + self.epsilon)</p><p class="source-code">        normalized = (x - mean) * inv</p><p class="source-code">        return self.scale * normalized + self.offset</p></li>
				<li>Define a <a id="_idIndexMarker553"/>class to encapsulate the CycleGAN implementation. Start <a id="_idIndexMarker554"/>with the constructor:<p class="source-code">class CycleGAN(object):</p><p class="source-code">    def __init__(self, output_channels=3, </p><p class="source-code">                 lambda_value=10):</p><p class="source-code">        self.output_channels = output_channels</p><p class="source-code">        self._lambda = lambda_value</p><p class="source-code">        self.loss = BinaryCrossentropy(from_logits=True)</p><p class="source-code">        self.gen_g = self.create_generator()</p><p class="source-code">        self.gen_f = self.create_generator()</p><p class="source-code">        self.dis_x = self.create_discriminator()</p><p class="source-code">        self.dis_y = self.create_discriminator()</p><p class="source-code">        self.gen_g_opt = Adam(learning_rate=2e-4, </p><p class="source-code">                               beta_1=0.5)</p><p class="source-code">        self.gen_f_opt = Adam(learning_rate=2e-4, </p><p class="source-code">                              beta_1=0.5)</p><p class="source-code">        self.dis_x_opt = Adam(learning_rate=2e-4, </p><p class="source-code">                              beta_1=0.5)</p><p class="source-code">        self.dis_y_opt = Adam(learning_rate=2e-4, </p><p class="source-code">                              beta_1=0.5)</p><p>The main difference with Pix2Pix is that we have two generators (<strong class="source-inline">gen_g</strong> and<strong class="source-inline"> gen_f</strong>) and two discriminators (<strong class="source-inline">dis_x</strong> and <strong class="source-inline">dis_y</strong>). <strong class="source-inline">gen_g</strong> learns how to transform image X to image Y, and <strong class="source-inline">gen_f</strong> learns how to transform image Y to image Y. Analogously, <strong class="source-inline">dis_x</strong> learns to <a id="_idIndexMarker555"/>differentiate between the real image X and the one <a id="_idIndexMarker556"/>generated by <strong class="source-inline">gen_f</strong>, while <strong class="source-inline">dis_y</strong> learns to differentiate between the real image Y and the one generated by <strong class="source-inline">gen_g</strong>.</p></li>
				<li>Now, let's create a static method to produce downsampling blocks (this is the same as in the last recipe, only this time we use instance instead of batch normalization):<p class="source-code">    @staticmethod</p><p class="source-code">    def downsample(filters, size, norm=True):</p><p class="source-code">        initializer = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        layers = Sequential()</p><p class="source-code">        layers.add(Conv2D(filters=filters,</p><p class="source-code">                          kernel_size=size,</p><p class="source-code">                          strides=2,</p><p class="source-code">                          padding='same',</p><p class="source-code">                          </p><p class="source-code">                     kernel_initializer=initializer,</p><p class="source-code">                          use_bias=False))</p><p class="source-code">        if norm:</p><p class="source-code">            layers.add(InstanceNormalization())</p><p class="source-code">        layers.add(LeakyReLU())</p><p class="source-code">        return layers</p></li>
				<li>Now, define a <a id="_idIndexMarker557"/>static method to produce <a id="_idIndexMarker558"/>upsampling blocks (this is the same as in the last recipe, only this time we use instance instead of batch normalization):<p class="source-code">    @staticmethod</p><p class="source-code">    def upsample(filters, size, dropout=False):</p><p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        layers = Sequential()</p><p class="source-code">        layers.add(Conv2DTranspose(filters=filters,</p><p class="source-code">                                   kernel_size=size,</p><p class="source-code">                                   strides=2,</p><p class="source-code">                                   padding='same',</p><p class="source-code">                                   </p><p class="source-code">                             kernel_initializer=init,</p><p class="source-code">                                   use_bias=False))</p><p class="source-code">        layers.add(InstanceNormalization())</p><p class="source-code">        if dropout:</p><p class="source-code">            layers.add(Dropout(rate=0.5))</p><p class="source-code">        layers.add(ReLU())</p><p class="source-code">        return layers</p></li>
				<li>Define a method to <a id="_idIndexMarker559"/>build the <a id="_idIndexMarker560"/>generator. Start by creating the downsampling layers:<p class="source-code">    def create_generator(self):</p><p class="source-code">        down_stack = [</p><p class="source-code">            self.downsample(64, 4, norm=False),</p><p class="source-code">            self.downsample(128, 4),</p><p class="source-code">            self.downsample(256, 4)]</p><p class="source-code">        for _ in range(5):</p><p class="source-code">            down_block = self.downsample(512, 4)</p><p class="source-code">            down_stack.append(down_block)</p></li>
				<li>Now, create the upsampling layers:<p class="source-code">        for _ in range(3):</p><p class="source-code">            up_block = self.upsample(512, 4, </p><p class="source-code">                                   dropout=True)</p><p class="source-code">            up_stack.append(up_block)</p><p class="source-code">        for filters in (512, 256, 128, 64):</p><p class="source-code">            up_block = self.upsample(filters, 4)</p><p class="source-code">            up_stack.append(up_block)</p></li>
				<li>Thread the input <a id="_idIndexMarker561"/>through the <a id="_idIndexMarker562"/>downsampling and upsampling layers. Add skip connections to avoid the vanishing gradient problem:<p class="source-code">inputs = Input(shape=(None, None, 3))</p><p class="source-code">        x = inputs</p><p class="source-code">        skips = []</p><p class="source-code">        for down in down_stack:</p><p class="source-code">            x = down(x)</p><p class="source-code">            skips.append(x)</p><p class="source-code">        skips = reversed(skips[:-1])</p><p class="source-code">        for up, skip in zip(up_stack, skips):</p><p class="source-code">            x = up(x)</p><p class="source-code">            x = Concatenate()([x, skip])</p></li>
				<li>The output layers are a <strong class="source-inline">'tanh'</strong> activated transposed convolution:<p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        output = Conv2DTranspose(</p><p class="source-code">            filters=self.output_channels,</p><p class="source-code">            kernel_size=4,</p><p class="source-code">            strides=2,</p><p class="source-code">            padding='same',</p><p class="source-code">            kernel_initializer=init,</p><p class="source-code">            activation='tanh')(x)</p><p class="source-code">        return Model(inputs, outputs=output)</p></li>
				<li>Define a <a id="_idIndexMarker563"/>method to calculate the generator <a id="_idIndexMarker564"/>loss:<p class="source-code">    def generator_loss(self, generated):</p><p class="source-code">        return self.loss(tf.ones_like(generated), </p><p class="source-code">                         generated)</p></li>
				<li>Define a method to create the discriminator:<p class="source-code">    def create_discriminator(self):</p><p class="source-code">        input = Input(shape=(None, None, 3))</p><p class="source-code">        x = input</p><p class="source-code">        x = self.downsample(64, 4, False)(x)</p><p class="source-code">        x = self.downsample(128, 4)(x)</p><p class="source-code">        x = self.downsample(256, 4)(x)</p><p class="source-code">        x = ZeroPadding2D()(x)</p></li>
				<li>Add the last couple of layers, which are convolutional:<p class="source-code">        init = tf.random_normal_initializer(0.0, 0.02)</p><p class="source-code">        x = Conv2D(filters=512,</p><p class="source-code">                   kernel_size=4,</p><p class="source-code">                   strides=1,</p><p class="source-code">                   kernel_initializer=init,</p><p class="source-code">                   use_bias=False)(x)</p><p class="source-code">        x = InstanceNormalization()(x)</p><p class="source-code">        x = LeakyReLU()(x)</p><p class="source-code">        x = ZeroPadding2D()(x)</p><p class="source-code">        output = Conv2D(filters=1,</p><p class="source-code">                        kernel_size=4,</p><p class="source-code">                        strides=1,</p><p class="source-code">                        kernel_initializer=init)(x)</p><p class="source-code">        return Model(inputs=input, outputs=output)</p></li>
				<li>Define a <a id="_idIndexMarker565"/>method to compute the <a id="_idIndexMarker566"/>discriminator loss:<p class="source-code">    def discriminator_loss(self, real, generated):</p><p class="source-code">        real_loss = self.loss(tf.ones_like(real), </p><p class="source-code">                                     real)</p><p class="source-code">        generated_loss = </p><p class="source-code">              self.loss(tf.zeros_like(generated),</p><p class="source-code">                                   generated)</p><p class="source-code">        total_discriminator_loss = real_loss + generated_loss</p><p class="source-code">        return total_discriminator_loss * 0.5</p></li>
				<li>Define a method to compute the loss between the real and cycled images. This loss is in charge of quantifying the cycle consistency, which says that if you translate an image X to Y, and then Y to X, the result should be X, or close to X:<p class="source-code">    def calculate_cycle_loss(self, real_image, </p><p class="source-code">                             cycled_image):</p><p class="source-code">        error = real_image - cycled_image</p><p class="source-code">        loss1 = tf.reduce_mean(tf.abs(error))</p><p class="source-code">        return self._lambda * loss1</p></li>
				<li>Define a method to <a id="_idIndexMarker567"/>compute the identity loss. This loss establishes that if you pass image Y <a id="_idIndexMarker568"/>through <strong class="source-inline">gen_g</strong>, we should obtain the real image Y or something close to it (the same applies to <strong class="source-inline">gen_f</strong>):<p class="source-code">    def identity_loss(self, real_image, same_image):</p><p class="source-code">        error = real_image - same_image</p><p class="source-code">        loss = tf.reduce_mean(tf.abs(error))</p><p class="source-code">        return self._lambda * 0.5 * loss</p></li>
				<li>Define a method to perform a single training step. It receives images X and Y from different domains. Then, it uses <strong class="source-inline">gen_g</strong> to translate X to Y, and <strong class="source-inline">gen_f</strong> to translate Y to X:<p class="source-code">    @tf.function</p><p class="source-code">    def train_step(self, real_x, real_y):</p><p class="source-code">        with tf.GradientTape(persistent=True) as tape:</p><p class="source-code">            fake_y = self.gen_g(real_x, training=True)</p><p class="source-code">            cycled_x = self.gen_f(fake_y, </p><p class="source-code">                                 training=True)</p><p class="source-code">            fake_x = self.gen_f(real_y, training=True)</p><p class="source-code">            cycled_y = self.gen_g(fake_x, </p><p class="source-code">                                   training=True)</p></li>
				<li>Now, pass X through <strong class="source-inline">gen_f</strong> and Y through <strong class="source-inline">gen_y</strong> to later compute the identity loss:<p class="source-code">            same_x = self.gen_f(real_x, training=True)</p><p class="source-code">            same_y = self.gen_g(real_y, training=True)</p></li>
				<li>Pass real X <a id="_idIndexMarker569"/>and fake X to <strong class="source-inline">dis_x</strong>, and real Y, along <a id="_idIndexMarker570"/>with generated Y, to <strong class="source-inline">dis_y</strong>:<p class="source-code">            dis_real_x = self.dis_x(real_x, </p><p class="source-code">                                    training=True)</p><p class="source-code">            dis_real_y = self.dis_y(real_y, </p><p class="source-code">                                    training=True)</p><p class="source-code">            dis_fake_x = self.dis_x(fake_x,training=True)</p><p class="source-code">            dis_fake_y = self.dis_y(fake_y, </p><p class="source-code">                                   training=True)</p></li>
				<li>Compute the generators' losses:<p class="source-code">            gen_g_loss = self.generator_loss(dis_fake_y)</p><p class="source-code">            gen_f_loss = self.generator_loss(dis_fake_x)</p></li>
				<li>Compute the cycle loss:<p class="source-code">            cycle_x_loss = \</p><p class="source-code">                self.calculate_cycle_loss(real_x, </p><p class="source-code">                                          cycled_x)</p><p class="source-code">            cycle_y_loss = \</p><p class="source-code">                self.calculate_cycle_loss(real_y, </p><p class="source-code">                                         cycled_y)</p><p class="source-code">            total_cycle_loss = cycle_x_loss + </p><p class="source-code">                                   cycle_y_loss</p></li>
				<li>Compute the identity loss and the total generator G loss:<p class="source-code">            identity_y_loss = \</p><p class="source-code">                self.identity_loss(real_y, same_y)</p><p class="source-code">            total_generator_g_loss = (gen_g_loss +</p><p class="source-code">                                      total_cycle_loss +</p><p class="source-code">                                      identity_y_loss)</p></li>
				<li>Repeat <a id="_idIndexMarker571"/>for generator <a id="_idIndexMarker572"/>F:<p class="source-code">            identity_x_loss = \</p><p class="source-code">                self.identity_loss(real_x, same_x)</p><p class="source-code">            total_generator_f_loss = (gen_f_loss +</p><p class="source-code">                                      total_cycle_loss +</p><p class="source-code">                                      identity_x_loss)</p></li>
				<li>Compute the discriminators' losses:<p class="source-code">         dis_x_loss = \</p><p class="source-code">           self.discriminator_loss(dis_real_x,dis_fake_x)</p><p class="source-code">         dis_y_loss = \</p><p class="source-code">           self.discriminator_loss(dis_real_y,dis_fake_y)</p></li>
				<li>Compute the gradients for the generators:<p class="source-code">        gen_g_grads = tape.gradient(</p><p class="source-code">            total_generator_g_loss,</p><p class="source-code">            self.gen_g.trainable_variables)</p><p class="source-code">        gen_f_grads = tape.gradient(</p><p class="source-code">            total_generator_f_loss,</p><p class="source-code">            self.gen_f.trainable_variables)</p></li>
				<li>Compute <a id="_idIndexMarker573"/>the gradients for the <a id="_idIndexMarker574"/>discriminators:<p class="source-code">        dis_x_grads = tape.gradient(</p><p class="source-code">            dis_x_loss,</p><p class="source-code">            self.dis_x.trainable_variables)</p><p class="source-code">        dis_y_grads = tape.gradient(</p><p class="source-code">            dis_y_loss,</p><p class="source-code">            self.dis_y.trainable_variables)</p></li>
				<li>Apply the gradients to each generator using the respective optimizer:<p class="source-code">        gen_g_opt_params = zip(gen_g_grads,</p><p class="source-code">                         self.gen_g.trainable_variables)</p><p class="source-code">        self.gen_g_opt.apply_gradients(gen_g_opt_params)</p><p class="source-code">        gen_f_opt_params = zip(gen_f_grads,</p><p class="source-code">                               self.gen_f.trainable_variables)</p><p class="source-code">        self.gen_f_opt.apply_gradients(gen_f_opt_params)</p></li>
				<li>Apply the gradients to each discriminator using the respective optimizer:<p class="source-code">        dis_x_opt_params = zip(dis_x_grads,</p><p class="source-code">                          self.dis_x.trainable_variables)</p><p class="source-code">        self.dis_x_opt.apply_gradients(dis_x_opt_params)</p><p class="source-code">        dis_y_opt_params = zip(dis_y_grads,</p><p class="source-code">                          self.dis_y.trainable_variables)</p><p class="source-code">        self.dis_y_opt.apply_gradients(dis_y_opt_params)</p></li>
				<li>Define a <a id="_idIndexMarker575"/>method to fit the whole <a id="_idIndexMarker576"/>architecture. It will save to disk the images produced by generator G after each epoch:<p class="source-code">    def fit(self, train, epochs, test):</p><p class="source-code">        for epoch in tqdm(range(epochs)):</p><p class="source-code">            for image_x, image_y in train:</p><p class="source-code">                self.train_step(image_x, image_y)</p><p class="source-code">            test_image = next(iter(test))</p><p class="source-code">            generate_images(self.gen_g, test_image, </p><p class="source-code">                               epoch)</p></li>
				<li>Load the dataset:<p class="source-code">dataset, _ = tfds.load('cycle_gan/summer2winter_  yosemite',</p><p class="source-code">                       with_info=True,</p><p class="source-code">                       as_supervised=True)</p></li>
				<li>Unpack the training and test splits:<p class="source-code">train_summer = dataset['trainA']</p><p class="source-code">train_winter = dataset['trainB']</p><p class="source-code">test_summer = dataset['testA']</p><p class="source-code">test_winter = dataset['testB']</p></li>
				<li>Define <a id="_idIndexMarker577"/>the data processing pipelines for the <a id="_idIndexMarker578"/>training spit:<p class="source-code">BUFFER_SIZE = 400</p><p class="source-code">BATCH_SIZE = 1</p><p class="source-code">train_summer = (train_summer</p><p class="source-code">                .map(preprocess_training_image,</p><p class="source-code">                     num_parallel_calls=AUTOTUNE)</p><p class="source-code">                .cache()</p><p class="source-code">                .shuffle(BUFFER_SIZE)</p><p class="source-code">                .batch(BATCH_SIZE))</p><p class="source-code">train_winter = (train_winter</p><p class="source-code">                .map(preprocess_training_image,</p><p class="source-code">                     num_parallel_calls=AUTOTUNE)</p><p class="source-code">                .cache()</p><p class="source-code">                .shuffle(BUFFER_SIZE)</p><p class="source-code">                .batch(BATCH_SIZE))</p></li>
				<li>Define the data processing pipelines for the test split:<p class="source-code">test_summer = (test_summer</p><p class="source-code">               .map(preprocess_test_image,</p><p class="source-code">                    num_parallel_calls=AUTOTUNE)</p><p class="source-code">               .cache()</p><p class="source-code">               .shuffle(BUFFER_SIZE)</p><p class="source-code">               .batch(BATCH_SIZE))</p><p class="source-code">test_winter = (test_winter</p><p class="source-code">               .map(preprocess_test_image,</p><p class="source-code">                    num_parallel_calls=AUTOTUNE)</p><p class="source-code">               .cache()</p><p class="source-code">               .shuffle(BUFFER_SIZE)</p><p class="source-code">               .batch(BATCH_SIZE))</p></li>
				<li>Create <a id="_idIndexMarker579"/>an instance of <strong class="source-inline">CycleGAN()</strong> and train it <a id="_idIndexMarker580"/>for 40 epochs:<p class="source-code">cycle_gan = CycleGAN()</p><p class="source-code">train_ds = tf.data.Dataset.zip((train_summer, </p><p class="source-code">                                train_winter))</p><p class="source-code">cycle_gan.fit(train=train_ds,</p><p class="source-code">              epochs=40,</p><p class="source-code">              test=test_summer)</p><p>At epoch 1, we'll notice that the network hasn't learned much:</p></li>
			</ol>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B14768_06_007.jpg" alt="Figure 6.7 – Left: original image during summer; right: translated image (winter)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.7 – Left: original image during summer; right: translated image (winter)</p>
			<p>However, at <a id="_idIndexMarker581"/>epoch 40, the results are more <a id="_idIndexMarker582"/>promising:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="image/B14768_06_008.jpg" alt="Figure 6.8 – Left: original image during summer; right: translated image (winter)"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.8 – Left: original image during summer; right: translated image (winter)</p>
			<p>As we can see in the preceding image, our <strong class="source-inline">CycleGAN()</strong> added a little more white to certain parts of the trail and the trees to make the translated image seem like it was taken during winter. Of course, training for more epochs can potentially lead to better results, which I encourage you to do to solidify your understanding of CycleGANs!</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor240"/>How it works…</h2>
			<p>In this recipe, we learned that CycleGANs work in a very similar fashion to Pix2Pix. However, the biggest <a id="_idIndexMarker583"/>advantage is that a CycleGAN doesn't <a id="_idIndexMarker584"/>require a dataset of paired images to achieve its goal. Instead, it relies on two sets of generators and discriminators, which, in fact, create a learning cycle, hence the name.</p>
			<p>In particular, CycleGANs <a id="_idIndexMarker585"/>work as follows:</p>
			<ul>
				<li>A generator G must learn a mapping from an image X to an image Y.</li>
				<li>A generator F must learn a mapping from an image Y to an image X.</li>
				<li>A discriminator D(X) must distinguish the real image X from the fake one generated by G.</li>
				<li>A discriminator D(Y) must distinguish the real image Y from the fake one generated by F.</li>
			</ul>
			<p>There are two conditions that ensure that the translation preserves the meaning in both domains (very much like when we want to preserve the meaning of our words when we translate from English to Spanish, and vice versa):</p>
			<ul>
				<li>Cycle consistency: Going from X to Y and then from Y to X should produce the original X or something very similar to X. The same applies to Y.</li>
				<li>Identity consistency: Passing X to G should produce the same X or something very similar to X. The same applies to Y.</li>
			</ul>
			<p>Using these four components, CycleGAN tries to preserve the cycle and identity consistency in the translation, which generates very satisfying results without the need for supervised, paired data.</p>
			<h2 id="_idParaDest-204"><a id="_idTextAnchor241"/>See also</h2>
			<p>You can read the original paper on <a id="_idIndexMarker586"/>CycleGANs here: <a href="https://arxiv.org/abs/1703.10593">https://arxiv.org/abs/1703.10593</a>. Also, here is a very interesting thread to understand the difference between instance and <a id="_idIndexMarker587"/>batch normalization: <a href="https://intellipaat.com/community/1869/instance-normalisation-vs-batch-normalisation">https://intellipaat.com/community/1869/instance-normalisation-vs-batch-normalisation</a>.</p>
			<h1 id="_idParaDest-205"><a id="_idTextAnchor242"/>Implementing an adversarial attack using the Fast Gradient Signed Method</h1>
			<p>We often think of highly <a id="_idIndexMarker588"/>accurate deep neural networks as <a id="_idIndexMarker589"/>robust models, but the <strong class="bold">Fast Gradient Signed Method</strong> (<strong class="bold">FGSM</strong>), proposed by <a id="_idIndexMarker590"/>no other than the father of GANs himself, Ian Goodfellow, showed otherwise. In this recipe, we'll perform an FGSM attack on a pre-trained model to see how, by introducing seemingly imperceptible changes, we can completely fool a network.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor243"/>Getting ready</h2>
			<p>Let's install <strong class="source-inline">OpenCV</strong> with <strong class="source-inline">pip</strong>. </p>
			<p>We'll use it to save the perturbed images using the FGSM method:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python</p>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor244"/>How to do it</h2>
			<p>After completing the following steps, you'll have successfully performed an adversarial attack:</p>
			<ol>
				<li value="1">Import the dependencies:<p class="source-code">import cv2</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras.applications.nasnet import *</p><p class="source-code">from tensorflow.keras.losses import CategoricalCrossentropy</p></li>
				<li>Define a function to preprocess an image, which entails resizing it and applying the same treatment as the pre-trained network we'll use (in this case, <strong class="source-inline">NASNetMobile</strong>):<p class="source-code">def preprocess(image, target_shape):</p><p class="source-code">    image = tf.cast(image, tf.float32)</p><p class="source-code">    image = tf.image.resize(image, target_shape)</p><p class="source-code">    image = preprocess_input(image)</p><p class="source-code">    image = image[None, :, :, :]</p><p class="source-code">    return image</p></li>
				<li>Define a <a id="_idIndexMarker591"/>function to get the human-readable image <a id="_idIndexMarker592"/>from a set of probabilities:<p class="source-code">def get_imagenet_label(probabilities):</p><p class="source-code">    return decode_predictions(probabilities, top=1)[0][0]</p></li>
				<li>Define a function to save an image. This will use the pre-trained model to get the proper label and will utilize it as part of the filename of the image, which also contains the prediction confidence percentage. Prior to storing the image on disk, it ensures that it's in the expected [0, 255] range, as well as in BGR space, which is the one used by OpenCV:<p class="source-code">def save_image(image, model, description):</p><p class="source-code">    prediction = model.predict(image)</p><p class="source-code">    _, label, conf = get_imagenet_label(prediction)</p><p class="source-code">    image = image.numpy()[0] * 0.5 + 0.5</p><p class="source-code">    image = (image * 255).astype('uint8')</p><p class="source-code">    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)</p><p class="source-code">    conf *= 100</p><p class="source-code">    img_name = f'{description}, {label} ({conf:.2f}%).jpg'</p><p class="source-code">    cv2.imwrite(img_name, image)</p></li>
				<li>Define a function to create the adversarial pattern that will be used later on to perform the actual FGSM attack:                             <p class="source-code">def generate_adv_pattern(model,</p><p class="source-code">                         input_image,</p><p class="source-code">                         input_label,</p><p class="source-code">                         loss_function):</p><p class="source-code">    with tf.GradientTape() as tape:</p><p class="source-code">        tape.watch(input_image)</p><p class="source-code">        prediction = model(input_image)</p><p class="source-code">        loss = loss_function(input_label, prediction)</p><p class="source-code">    gradient = tape.gradient(loss, input_image)</p><p class="source-code">    signed_gradient = tf.sign(gradient)</p><p class="source-code">    return signed_gradient</p><p>The <a id="_idIndexMarker593"/>pattern is pretty simple: It consists of a tensor with the sign <a id="_idIndexMarker594"/>of the gradient in each element. More specifically, <strong class="source-inline">signed_gradient</strong> will contain a <strong class="source-inline">-1</strong> for gradient values below <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong> for values above <strong class="source-inline">0</strong>, and <strong class="source-inline">0</strong> if the gradient is, well, <strong class="source-inline">0</strong>.</p></li>
				<li>Instantiate the pre-trained <strong class="source-inline">NASNetMobile()</strong> model and freeze its weights:<p class="source-code">pretrained_model = NASNetMobile(include_top=True,</p><p class="source-code">                                weights='imagenet')</p><p class="source-code">pretrained_model.trainable = False</p></li>
				<li>Load the test image and pass it through the network:<p class="source-code">image = tf.io.read_file('dog.jpg')</p><p class="source-code">image = tf.image.decode_jpeg(image)</p><p class="source-code">image = preprocess(image, pretrained_model.input.shape[1:-1])</p><p class="source-code">image_probabilities = pretrained_model.predict(image)    </p></li>
				<li>One-hot <a id="_idIndexMarker595"/>encode the ground truth label of the <a id="_idIndexMarker596"/>original image, and use it to generate the adversarial pattern:<p class="source-code">cce_loss = CategoricalCrossentropy()</p><p class="source-code">pug_index = 254</p><p class="source-code">label = tf.one_hot(pug_index, image_probabilities.shape[-1])</p><p class="source-code">label = tf.reshape(label, (1, image_probabilities.shape[-1]))</p><p class="source-code">disturbances = generate_adv_pattern(pretrained_model,</p><p class="source-code">                                    image,</p><p class="source-code">                                    label,</p><p class="source-code">                                    cce_loss)</p></li>
				<li>Perform a series of adversarial attacks using increasing, yet small, values of <strong class="source-inline">epsilon</strong>, which will be applied in the direction of the gradient, leveraging the pattern present in <strong class="source-inline">disturbances</strong>:<p class="source-code">for epsilon in [0, 0.005, 0.01, 0.1, 0.15, 0.2]:</p><p class="source-code">    corrupted_image = image + epsilon * disturbances</p><p class="source-code">    corrupted_image = tf.clip_by_value(corrupted_image, -1, 1)</p><p class="source-code">    save_image(corrupted_image,</p><p class="source-code">               pretrained_model,</p><p class="source-code">               f'Epsilon = {epsilon:.3f}')</p><p>For epsilon = 0 (no attack), the image looks like this, and the label is <strong class="source-inline">pug</strong> with an 80% confidence:</p></li>
			</ol>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/B14768_06_009.jpg" alt="Figure 6.9 – Original image. Label: pug (80.23% confidence)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.9 – Original image. Label: pug (80.23% confidence)</p>
			<p>When <a id="_idIndexMarker597"/>epsilon = 0.005 (a very small <a id="_idIndexMarker598"/>perturbation), the label changes to <strong class="source-inline">Brabancon_griffon</strong>, with a 43.03% confidence:</p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="image/B14768_06_010.jpg" alt="Figure 6.10 – Epsilon = 0.005 applied in the gradient direction. Label: Brabancon_gritton (43.03% "/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.10 – Epsilon = 0.005 appli<a id="_idTextAnchor245"/>ed in the gradient direction. Label: Brabancon_gritton (43.03% confidence)</p>
			<p>As can be seen from the preceding image, an imperceptible variation in the pixel values produced a drastically different response from the network. However, the situation worsens the more we increment the magnitude of epsilon. For a complete list of results, refer to <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe5">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch6/recipe5</a>.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor246"/>How it works…</h2>
			<p>In this recipe, we <a id="_idIndexMarker599"/>implemented a fairly simple <a id="_idIndexMarker600"/>attack based on the FGSM proposed by Ian Goodfellow, which simply consists of determining the direction (sign) of the gradient at each location and using that information to create an adversarial pattern. The underlying principle is that this technique maximizes the loss at each pixel value.</p>
			<p>Next, we use this pattern to either add or subtract a small perturbation to each pixel in the image that gets passed to the network.</p>
			<p>Although these changes are often imperceptible to the human eye, they have the power to completely confuse a network, resulting in nonsensical predictions, as demonstrated in the last step of this recipe.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor247"/>See also</h2>
			<p>Fortunately, many defenses against this type of attack (and more sophisticated ones) have emerged. You can read a pretty interesting survey of adversarial attacks and defenses here: <a href="https://arxiv.org/abs/1810.00069">https://arxiv.org/abs/1810.00069</a>.</p>
		</div>
	</body></html>