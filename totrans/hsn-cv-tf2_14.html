<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Assessments</h1>
                </header>
            
            <article>
                


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Answers</h1>
                </header>
            
            <article>
                
<p class="mce-root">The answers to the assessment questions found at the end of each chapter are shared in the following sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 1</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>Which of the following tasks does not belong to computer vision: a web search of images similar to a query, a 3D scene reconstruction from image sequences, or the animation of a video character?</strong></li>
</ol>
<p style="padding-left: 90px">The <em>latter</em>, which instead belongs to the domain of <strong>computer graphics.</strong> Note, however, that increasingly, computer vision algorithms are helping artists to generate or animate content more efficiently (such as the <em>motion capture</em> methods, for instance, which record actors performing some actions and transfer the motions to virtual characters).</p>
<ol start="2">
<li><strong>Which</strong> <strong>activ</strong><strong>ation</strong> <strong>function did the original perceptrons use?</strong></li>
</ol>
<p style="padding-left: 90px">The <kbd>step</kbd> function.</p>
<ol start="3">
<li><strong>Suppose we want to train a method to detect whether a handwritten digit is a <em>4</em>. How should we adapt the network implemented in the chapter for this task?</strong></li>
</ol>
<p style="padding-left: 90px">In the chapter, we trained a classification network to identify pictures of digits from <kbd>0</kbd> to <kbd>9</kbd>. Therefore, the network had to predict the proper class among 10, hence, an output vector of 10 values (one for each class score/probability).</p>
<p style="padding-left: 90px">In this question, we define a different classification task. We want the network to identify whether an image contains a <em>4</em> or <em>not a 4</em>. This is a <strong>binary classification</strong>, and the network should, therefore, be edited to <em>output only two values</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 2</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>What is Keras compared to TensorFlow? What is its purpose?</strong><span class="underline"><strong><br/></strong></span></li>
</ol>
<p style="padding-left: 90px">Keras was designed as a wrapper around other deep learning libraries to make development easier. TensorFlow is now fully integrated with Keras through <kbd>tf.keras</kbd>. It is best practice to use this module to create models in TensorFlow 2.</p>
<ol start="2">
<li><strong>Why does TensorFlow use graphs? How can they be created manually?</strong></li>
</ol>
<p style="padding-left: 90px">TensorFlow relies on graphs to ensure model performance and portability. In TensorFlow 2, the best way to create graphs manually is to employ the <kbd>tf.function</kbd> decorator.</p>
<ol start="3">
<li><strong>What is the difference between eager execution mode and lazy execution mode?</strong></li>
</ol>
<p style="padding-left: 90px">In lazy execution mode, no computation is performed until the user specifically asks for a result. In eager execution mode, every operation is run when it is defined. While the former can be faster thanks to graph optimizations, the latter is easier to use and easier to debug. In TensorFlow 2, lazy execution mode has been deprecated in favor of eager execution mode.</p>
<ol start="4">
<li><strong>How do you log information in TensorBoard, and how do you display it? </strong></li>
</ol>
<p style="padding-left: 90px">To log information in TensorBoard, you can use the <kbd>tf.keras.callbacks.TensorBoard</kbd> callback and pass it to the <kbd>.fit</kbd> method when training a model. To log information manually, you can use the <kbd>tf.summary</kbd> module. To display information, launch the following command:</p>
<pre style="padding-left: 120px"><strong>$ tensorboard --logdir ./model_logs</strong></pre>
<p style="padding-left: 90px">Here, <kbd>model_logs</kbd> is the directory where TensorBoard logs are stored. This command will output a URL. Navigate to this URL to monitor training.</p>
<ol start="5">
<li><strong>What are the main differences between TensorFlow 1 and 2?</strong></li>
</ol>
<p style="padding-left: 90px">TensorFlow 2 focuses on simplicity by removing graph management from the hands of the user. It also uses eager execution by default, making models easier to debug. Nevertheless, it still maintains its performance thanks to AutoGraph and <kbd>tf.function</kbd>. It also integrates deeply with Keras, making model creation easier than ever.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 3</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>Why does the output of a convolutional layer have a smaller width and height than the input, unless it is padded?</strong><span class="underline"><strong><br/></strong></span></li>
</ol>
<p style="padding-left: 90px">The spatial dimensions of the output of a convolutional layer represent the number of valid positions the kernels could take when sliding over the input tensors, vertically and horizontally. Since kernels span over <em>k</em> <span class="ui_qtext_rendered_qtext"><span class="js-about-item-abstr">×</span> <em>k</em> pixels (if square), the number of positions they can take over the input image without being partially out of it can only be equal to (if <em>k</em> = 1), or less than, the image dimensions.</span></p>
<p style="padding-left: 90px">This is expressed by the equations presented in the chapter, to compute the output dimensions based on the layer's hyper parameters.</p>
<ol start="2">
<li><strong>What would be the output of a max-pooling layer with a receptive field of (2, 2) and a stride of 2 on the input matrix in Figure 3-6?</strong><span class="underline"><strong><br/></strong></span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b377c7f8-43ec-4690-b38b-7be67231f8a7.png" style="width:43.92em;height:10.67em;"/></p>
<p class="mce-root"/>
<ol start="3">
<li><strong>How could LeNet-5 be implemented using the Keras Functional API in a non-object-oriented manner ?</strong></li>
</ol>
<p style="padding-left: 90px">The code is as follows:</p>
<pre style="padding-left: 90px">from tensorflow.keras import Model<br/>from tensorflow.keras.layers import Inputs, Conv2D, MaxPooling2D, Flatten, Dense<br/><br/># "Layer" representing the network's inputs:<br/>inputs = Input(shape=input_shape)<br/># First block (conv + max-pool):<br/><span class="n">conv1</span> <span class="o">=</span> Conv2D(6, kernel_size=5, padding='same', activation='relu')(inputs)<br/>max_pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)<br/># 2nd block:<br/>conv2 = Conv2D(16, kernel_size=5, activation='relu')(max_pool1)<br/>max_pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)<br/># Dense layers:<br/>flatten = Flatten()(max_pool2)<br/>dense1 = Dense(120, activation='relu')(flatten)<br/>dense2 = Dense(84, activation='relu')(dense1)<br/>dense3 = Dense(num_classes, activation='softmax')(dense2)<br/><br/>lenet5_model = Model(inputs=inputs, outputs=dense3)</pre>
<ol start="4">
<li><strong>How does L1/L2 regularization affect the networks?</strong><span class="underline"><strong><br/></strong></span></li>
</ol>
<p style="padding-left: 90px"><strong>L1 regularization</strong> forces the layers to which it is applied to bring toward zero the values of the parameters linked to less important features; that is, to ignore less meaningful features (such as features tied to dataset noise).</p>
<p style="padding-left: 90px"><strong>L2 regularization</strong> compels the layers to keep their variables low, and, hence, more homogeneously distributed. It prevents the network from developing a small set of parameters with large values that overly influence its predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 4</h1>
                </header>
            
            <article>
                
<ol>
<li class="mce-root"><strong>Which TensorFlow Hub module can be used to instantiate an Inception classifier for ImageNet?</strong></li>
</ol>
<p style="padding-left: 90px">The model at <a href="https://tfhub.dev/google/tf2-preview/inception_v3/classification/2">https://tfhub.dev/google/tf2-preview/inception_v3/classification/2</a> can be directly used to classify ImageNet-like images, as this classification model was pretrained over this dataset.</p>
<p class="mce-root"/>
<ol start="2">
<li><strong>How can the first three residual macro-blocks of a ResNet-50 model from Keras Applications be frozen?</strong></li>
</ol>
<p style="padding-left: 90px"><span>The code is as follows:</span></p>
<pre style="padding-left: 90px">freeze_num = 3<br/># Looking at `resnet50.summary()`, we could observe that the 1st layer of the 4th macro-block is named "res5[...]":<br/>break_layer_name = <span>'res{}'</span>.format(freeze_num + <span>2</span>)<br/><span>for </span>layer <span>in </span>resnet50_finetune.layers:<br/>   <span>if </span>break_layer_name <span>in </span>layer.name:<br/>        <span>break<br/></span><span>    if </span><span>isinstance</span>(layer<span>, </span>tf.keras.layers.Conv2D):<br/>        <span># If the layer is a convolution, and isn't after <br/>        # the 1st layer not to train:<br/></span><span> </span>       layer.trainable = <span>False</span></pre>
<ol start="3">
<li><strong>When is transfer learning discouraged?</strong></li>
</ol>
<p style="padding-left: 90px">Transfer learning may not be beneficial when the <em>domains</em> are too dissimilar and the target data has a structure that is completely different to the source data structure. As mentioned in the chapter, while CNNs can be applied to images, text, and audio files, transferring weights trained for one modality to another is not encouraged.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 5</h1>
                </header>
            
            <article>
                
<ol>
<li class="mce-root"><strong>What is the difference between a bounding box, an anchor box, and a ground truth box?</strong></li>
</ol>
<p style="padding-left: 90px" class="mce-root">A <strong>bounding box</strong> is the smallest rectangle enclosing an object. An <strong>anchor box</strong> is a bounding box with a specific size. For each position in the image grid, there are usually several anchor boxes with different aspect ratios—square, vertical rectangle, and horizontal rectangle. By refining the size and the position of the anchor box, the object detection model generates predictions. A <strong>ground truth box</strong> is a bounding box corresponding to a specific object in the training set. If a model is trained perfectly, it generates predictions that are very close to ground truth boxes.</p>
<ol start="2">
<li class="mce-root"><strong>What is the role of the feature extractor?</strong></li>
</ol>
<p style="padding-left: 90px" class="mce-root">A feature extractor is a CNN that converts an image into a feature volume. The feature volume is usually smaller in dimension than the input image and contains meaningful features that can be passed to the remainder of the network in order to generate predictions.</p>
<ol start="3">
<li class="mce-root"><strong>Which of the following models should you choose: YOLO or Faster R-CNN?</strong></li>
</ol>
<p style="padding-left: 90px" class="mce-root">If speed is the priority, you should pick YOLO as it is the fastest architecture. If accuracy is paramount, you should choose Faster R-CNN as it generates the best predictions.</p>
<ol start="4">
<li class="mce-root"><strong>When are anchor boxes used?</strong></li>
</ol>
<p style="padding-left: 90px">Before anchor boxes, box prediction dimensions were generated using the output of the network. As object sizes vary (a person usually fits in a vertical rectangle, while a car fits in a horizontal rectangle), anchor boxes were introduced. Using this technique, each anchor box is able to specialize for one object ratio, leading to more precise predictions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 6</h1>
                </header>
            
            <article>
                
<ol>
<li class="mce-root"><strong>What is the partic</strong><strong>ularity of autoencoders?</strong></li>
</ol>
<p style="padding-left: 90px">Autoencoders are encoders-decoders whose <strong>inputs and targets are the same</strong>. Their goal is to properly encode and then decode images without impacting their quality, despite their <em>bottleneck</em> (that is, their latent space of lower dimensionality). </p>
<ol start="2">
<li><strong>Which classification architecture are fully convolutional networks<span> (</span>FCNs) based on?</strong></li>
</ol>
<p style="padding-left: 90px"><strong>FCNs</strong> use <strong>VGG-16</strong> as the feature extractor.</p>
<ol start="3">
<li><strong>How can a semantic segmentation model be trained so that it does not ignore small classes?</strong></li>
</ol>
<p style="padding-left: 90px"><strong>Per-class weighing</strong> can be applied to the cross-entropy loss, thereby penalizing more heavy pixels from smaller classes that are misclassified. Losses that are not affected by the classes' proportions can also be used instead, such as <strong>Dice</strong>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 7</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>Given an <kbd>a = [1, 2, 3]</kbd> tensor and a <kbd>b = [4, 5, 6]</kbd> tensor, how can a <kbd>tf.data</kbd> pipeline that would output each value separately, from <kbd>1</kbd> to <kbd>6</kbd>, be built?</strong></li>
</ol>
<p style="padding-left: 90px"><span>The code is as follows:</span></p>
<pre style="padding-left: 90px">dataset_a = tf.data.Dataset.from_tensor_slices(a)<br/>dataset_b = tf.data.Dataset.from_tensor_slices(b)<br/>dataset_ab = dataset_a.<strong>concatenate</strong>(dataset_b)<br/>for element in dataset_ab:<br/>    print(element) # will print 1, then 2, ... until 6</pre>
<ol start="2">
<li><strong>According to the documentation of <kbd>tf.data.Options</kbd>, how can you ensure that a dataset always returns samples in the same order, run after run?</strong></li>
</ol>
<p style="padding-left: 90px" class="mce-root">The <kbd>.experimental_deterministic</kbd> <span>attribute </span>of <kbd>tf.data.Options</kbd> should be set to <kbd>True</kbd> before being passed to the dataset.</p>
<ol start="3">
<li><strong>Which domain adaptation methods that we introduced can be used when no target annotations are available for trainin<span class="underline">g</span>?</strong></li>
</ol>
<p style="padding-left: 90px">Unsupervised domain adaptation methods should be considered, such as <em>Learning Transferable Features with Deep Adaptation Networks</em>, by Mingsheng Long et al. (from Tsinghua University, China), or <strong>Domain-Adversarial Neural Networks</strong> (<strong>DANN</strong>), by Yaroslav Ganin <span>et al.</span> (from Skoltech).</p>
<ol start="4">
<li><strong>What role does the discriminator play in GANs?</strong></li>
</ol>
<p style="padding-left: 90px">It plays against the generator, trying to distinguish fake images from real images. The discriminator can be considered as a <strong>trainable loss function</strong> to guide the generator—the generator tries to minimize how <em>correct</em> the discriminator is, with both networks becoming better and better at their task as the training proceeds.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 8</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>What are the main advantages of LSTMs over the simple RNN architecture?</strong></li>
</ol>
<p style="padding-left: 90px">LSTMs suffer less from gradient vanishing and are more capable of storing long-term relationships in recurrent data. While they require more computing power, this usually leads to better predictions.</p>
<ol start="2">
<li><strong>How is a CNN used when it is applied before the LSTM?</strong></li>
</ol>
<p style="padding-left: 90px">The CNN acts as a feature extractor and reduces the dimensionality of the input data. By applying a pretrained CNN, we extract meaningful features from the input images. The LSTM trains faster since those features have a much smaller dimensionality than the input image.</p>
<ol start="3">
<li><strong>What is vanishing gradient and why does it occur? Why is it a problem?</strong></li>
</ol>
<p style="padding-left: 90px">When backpropagating the error in RNNs, we need to go back through the time steps as well. If there are many time steps, the information slowly fades away due to the way in which the gradient is computed. It is a problem since it makes it harder for the network to learn how to generate good predictions.</p>
<ol start="4">
<li><strong>What are some of the workarounds for the vanishing gradient problem?</strong></li>
</ol>
<p style="padding-left: 90px">One workaround is to use truncated backpropagation, which is a technique described in the chapter. Another option is to use LSTMs instead of simple RNNs, as they suffer less from gradient vanishing.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Chapter 9</h1>
                </header>
            
            <article>
                
<ol>
<li><strong>When measuring a model's inference speed, should you measure with single or multiple images?</strong></li>
</ol>
<p style="padding-left: 90px">Multiple images should be used to avoid measure bias.</p>
<ol start="2">
<li><strong>Is a model with <kbd>float32</kbd> weights larger or smaller than one with <kbd>float16</kbd> weights?</strong></li>
</ol>
<p style="padding-left: 90px"><kbd>Float16</kbd> weights use about half the space of <kbd>float32</kbd> weights. On compatible devices, they can also be faster.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li><strong>On iOS devices, should you use Core ML or TensorFlow Lite? What about Android devices?</strong></li>
</ol>
<p style="padding-left: 90px">On iOS devices, we recommend using Core ML where possible as it is available natively and is tightly integrated with the hardware. On Android devices, TensorFlow Lite should be used as there is no alternative.</p>
<ol start="4">
<li><strong>What are the benefits and limitations of running a model in the browser?</strong></li>
</ol>
<p style="padding-left: 90px">It does not require any installation on the user side and does not require computing power on the server side, making the application almost infinitely scalable.</p>
<ol start="5">
<li><strong>What is the most important requirement for embedded devices running deep learning algorithms?</strong></li>
</ol>
<p style="padding-left: 90px">On top of computing power, the most important requirement is power consumption, since most embedded devices run on batteries.</p>


            </article>

            
        </section>
    </body></html>