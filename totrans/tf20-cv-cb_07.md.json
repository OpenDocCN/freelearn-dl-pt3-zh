["```\n$> pip install Pillow tqdm\n```", "```\n    import glob\n    import os\n    import pathlib\n    import pickle\n    from string import punctuation\n    import numpy as np\n    import tqdm\n    from tensorflow.keras.applications.vgg16 import *\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.preprocessing.image import *\n    from tensorflow.keras.preprocessing.sequence import \\\n        pad_sequences\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    from tensorflow.keras.utils import to_categorical\n    from tqdm import tqdm\n    ```", "```\n    class ImageCaptionFeatureExtractor(object):\n        def __init__(self,\n                     output_path,\n                     start_token='beginsequence',\n                     end_token='endsequence',\n                     feature_extractor=None,\n                     input_shape=(224, 224, 3)):\n    ```", "```\n            self.input_shape = input_shape\n            if feature_extractor is None:\n                input = Input(shape=input_shape)\n                self.feature_extractor = VGG16(input_ \n                                         tensor=input,\n                                       weights='imagenet',\n                                       include_top=False)\n            else:\n                self.feature_extractor = feature_extractor\n            self.output_path = output_path\n            self.start_token = start_token\n            self.end_token = end_token\n            self.tokenizer = Tokenizer()\n            self.max_seq_length = None\n    ```", "```\n        def extract_image_features(self, image_path):\n            image = load_img(image_path,\n                           target_size=self.input_shape[:2])\n            image = img_to_array(image)\n            image = np.expand_dims(image, axis=0)\n            image = preprocess_input(image)\n            return self.feature_extractor.predict(image)[0]\n    ```", "```\n        def _clean_captions(self, captions):\n            def remove_punctuation(word):\n                translation = str.maketrans('', '',\n                                            punctuation)\n                return word.translate(translation)\n            def is_valid_word(word):\n                return len(word) > 1 and word.isalpha()\n            cleaned_captions = []\n            for caption in captions:\n                caption = caption.lower().split(' ')\n                caption = map(remove_punctuation, caption)\n                caption = filter(is_valid_word, caption)\n                cleaned_caption = f'{self.start_token} ' \\\n                                  f'{“ “.join(caption)} ' \\\n                                  f'{self.end_token}'\n                cleaned_captions.append(cleaned_caption)\n            return cleaned_captions\n    ```", "```\n        def _get_max_seq_length(self, captions):\n            max_sequence_length = -1\n            for caption in captions:\n                caption_length = len(caption.split(' '))\n                max_sequence_length = \n                                max(max_sequence_length,\n                                          caption_length)\n            return max_sequence_length\n    ```", "```\n        def extract_features(self, images_path, captions):\n            assert len(images_path) == len(captions)\n    ```", "```\n            captions = self._clean_captions(captions)\n            self.max_seq_length=self._get_max_seq_ \n                                       length(captions) \n            self.tokenizer.fit_on_texts(captions)\n    ```", "```\n            data_mapping = {}\n            print('\\nExtracting features...')\n            for i in tqdm(range(len(images_path))):\n                image_path = images_path[i]\n                caption = captions[i]\n             feats = self.extract_image_features(image_ path)\n                image_id = image_path.split(os.path.sep)[-1]\n                image_id = image_id.split('.')[0]\n                data_mapping[image_id] = {\n                    'features': feats,\n                    'caption': caption\n                }\n    ```", "```\n            out_path = f'{self.output_path}/data_mapping.pickle'\n            with open(out_path, 'wb') as f:\n                pickle.dump(data_mapping, f, protocol=4)\n    ```", "```\n            self._create_sequences(data_mapping)\n    ```", "```\n        def _create_sequences(self, mapping):\n            num_classes = len(self.tokenizer.word_index) + 1\n            in_feats = []\n            in_seqs = []\n            out_seqs = []\n    ```", "```\n            print('\\nCreating sequences...')\n            for _, data in tqdm(mapping.items()):\n                feature = data['features']\n                caption = data['caption']\n                seq = self.tokenizer.texts_to_\n                           sequences([caption])\n                seq = seq[0]\n    ```", "```\n                for i in range(1, len(seq)):\n                    input_seq = seq[:i]\n                    input_seq, = \n                       pad_sequences([input_seq],\n\n                         self.max_seq_length)\n                    out_seq = seq[i]\n                    out_seq = to_categorical([out_seq],\n\n                                           num_classes)[0]\n    ```", "```\n                    in_feats.append(feature)\n                    in_seqs.append(input_seq)\n                    out_seqs.append(out_seq)\n    ```", "```\n            file_paths = [\n                f'{self.output_path}/input_features.pickle',\n                f'{self.output_path}/input_sequences.pickle',\n                f'{self.output_path}/output_sequences.\n                                                     pickle']\n            sequences = [in_feats,\n                         in_seqs,\n                         out_seqs]\n            for path, seq in zip(file_paths, sequences):\n                with open(path, 'wb') as f:\n                    pickle.dump(np.array(seq), f, \n                                protocol=4)\n    ```", "```\n    BASE_PATH = (pathlib.Path.home() / '.keras' / 'datasets'       \n                                          /'flickr8k')\n    IMAGES_PATH = str(BASE_PATH / 'Images')\n    CAPTIONS_PATH = str(BASE_PATH / 'captions.txt')\n    ```", "```\n    extractor = ImageCaptionFeatureExtractor(output_path='.')\n    ```", "```\n    image_paths = list(glob.glob(f'{IMAGES_PATH}/*.jpg'))\n    ```", "```\n    with open(CAPTIONS_PATH, 'r') as f:\n        text = f.read()\n        lines = text.split('\\n')\n    ```", "```\n    mapping = {}\n    for line in lines:\n        if '.jpg' not in line:\n            continue\n        tokens = line.split(',', maxsplit=1)\n        if len(line) < 2:\n            continue\n        image_id, image_caption = tokens\n        image_id = image_id.split('.')[0]\n        captions_per_image = mapping.get(image_id, [])\n        captions_per_image.append(image_caption)\n        mapping[image_id] = captions_per_image\n    ```", "```\n    captions = []\n    for image_path in image_paths:\n        image_id = image_path.split('/')[-1].split('.')[0]\n        captions.append(mapping[image_id][0])\n    ```", "```\n    extractor.extract_features(image_paths, captions)\n    ```", "```\n    data_mapping.pickle     input_features.pickle   input_sequences.pickle  output_sequences.pickle\n    ```", "```\n$> pip install Pillow nltk tqdm\n```", "```\n    import glob\n    import pathlib\n    import pickle\n    import numpy as np\n    from nltk.translate.bleu_score import corpus_bleu\n    from sklearn.model_selection import train_test_split\n    from tensorflow.keras.applications.vgg16 import *\n    from tensorflow.keras.callbacks import ModelCheckpoint\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.models import *\n    from tensorflow.keras.preprocessing.sequence import \\\n        pad_sequences\n    from ch7.recipe1.extractor import ImageCaptionFeatureExtractor\n    ```", "```\n    BASE_PATH = (pathlib.Path.home() / '.keras' / 'datasets'     \n                 /'flickr8k')\n    IMAGES_PATH = str(BASE_PATH / 'Images')\n    CAPTIONS_PATH = str(BASE_PATH / 'captions.txt')\n    OUTPUT_PATH = '.'\n    ```", "```\n    def load_paths_and_captions():\n        image_paths = list(glob.glob(f'{IMAGES_PATH}/*.jpg'))\n        with open(f'{CAPTIONS_PATH}', 'r') as f:\n            text = f.read()\n            lines = text.split('\\n')\n        mapping = {}\n        for line in lines:\n            if '.jpg' not in line:\n                continue\n            tokens = line.split(',', maxsplit=1)\n            if len(line) < 2:\n                continue\n            image_id, image_caption = tokens\n            image_id = image_id.split('.')[0]\n            captions_per_image = mapping.get(image_id, [])\n            captions_per_image.append(image_caption)\n            mapping[image_id] = captions_per_image\n    ```", "```\n        all_captions = []\n        for image_path in image_paths:\n            image_id = image_path.split('/')[-\n                       1].split('.')[0]\n            all_captions.append(mapping[image_id][0])\n        return image_paths, all_captions\n    ```", "```\n    def build_network(vocabulary_size,\n                      max_sequence_length,\n                      input_shape=(4096,)):\n    ```", "```\n        x = Dropout(rate=0.5)(feature_inputs)\n        x = Dense(units=256)(x)\n        feature_output = ReLU()(x)\n    ```", "```\n        sequence_inputs = \n                Input(shape=(max_sequence_length,))\n        y = Embedding(input_dim=vocabulary_size,\n                      output_dim=256,\n                      mask_zero=True)(sequence_inputs)\n        y = Dropout(rate=0.5)(y)\n        sequence_output = LSTM(units=256)(y)\n    ```", "```\n        z = Add()([feature_output, sequence_output])\n        z = Dense(units=256)(z)\n        z = ReLU()(z)\n        z = Dense(units=vocabulary_size)(z)\n        outputs = Softmax()(z)\n    ```", "```\n        return Model(inputs=[feature_inputs, \n                      sequence_inputs],\n                     outputs=outputs)\n    ```", "```\n    def get_word_from_index(tokenizer, index):\n        return tokenizer.index_word.get(index, None)\n    ```", "```\n    def produce_caption(model,\n                        tokenizer,\n                        image,\n                        max_sequence_length):\n        text = 'beginsequence'\n        for _ in range(max_sequence_length):\n           sequence = tokenizer.texts_to_sequences([text])[0]\n            sequence = pad_sequences([sequence],\n                   maxlen=max_sequence_length)\n            prediction = model.predict([[image], sequence])\n            index = np.argmax(prediction)\n            word = get_word_from_index(tokenizer, index)\n            if word is None:\n                break\n            text += f' {word}'\n            if word == 'endsequence':\n                break\n        return text\n    ```", "```\n    def evaluate_model(model, features, captions, \n                         tokenizer,\n                       max_seq_length):\n        actual = []\n        predicted = []\n        for feature, caption in zip(features, captions):\n            generated_caption = produce_caption(model,\n                                                tokenizer,\n                                                feature,\n                                         max_seq_length)\n            actual.append([caption.split(' ')])\n            predicted.append(generated_caption.split(' '))\n    ```", "```\n        for index, weights in enumerate([(1, 0, 0, 0),\n                                         (.5, .5, 0, 0),\n                                         (.3, .3, .3, 0),\n                                         (.25, .25, .25, \n                                            .25)],\n                                        start=1):\n            b_score = corpus_bleu(actual, predicted, weights)\n            print(f'BLEU-{index}: {b_score}')\n    ```", "```\n    image_paths, all_captions = load_paths_and_captions()\n    ```", "```\n    extractor_model = VGG16(weights='imagenet')\n    inputs = extractor_model.inputs\n    outputs = extractor_model.layers[-2].output\n    extractor_model = Model(inputs=inputs, outputs=outputs)\n    ```", "```\n    extractor = ImageCaptionFeatureExtractor(\n        feature_extractor=extractor_model,\n        output_path=OUTPUT_PATH)\n    extractor.extract_features(image_paths, all_captions)\n    ```", "```\n    pickled_data = []\n    for p in [f'{OUTPUT_PATH}/input_features.pickle',\n              f'{OUTPUT_PATH}/input_sequences.pickle',\n              f'{OUTPUT_PATH}/output_sequences.pickle']:\n        with open(p, 'rb') as f:\n            pickled_data.append(pickle.load(f))\n    input_feats, input_seqs, output_seqs = pickled_data\n    ```", "```\n    (train_input_feats, test_input_feats,\n     train_input_seqs, test_input_seqs,\n     train_output_seqs,\n     test_output_seqs) = train_test_split(input_feats,\n                                          input_seqs,\n                                          output_seqs,\n                                          train_size=0.8,\n                                          random_state=9)\n    ```", "```\n    vocabulary_size = len(extractor.tokenizer.word_index) + 1\n    model = build_network(vocabulary_size,\n                          extractor.max_seq_length)\n    model.compile(loss='categorical_crossentropy',\n                  optimizer='adam')\n    ```", "```\n    checkpoint_path = ('model-ep{epoch:03d}-\n                         loss{loss:.3f}-'\n                       'val_loss{val_loss:.3f}.h5')\n    checkpoint = ModelCheckpoint(checkpoint_path,\n                                 monitor='val_loss',\n                                 verbose=1,\n                                 save_best_only=True,\n                                 mode='min')\n    ```", "```\n    EPOCHS = 30\n    model.fit(x=[train_input_feats, train_input_seqs],\n              y=train_output_seqs,\n              epochs=EPOCHS,\n              callbacks=[checkpoint],\n              validation_data=([test_input_feats,test_input_\n                                                     seqs],\n                                           test_output_seqs))\n    ```", "```\n    model = load_model('model-ep003-loss3.847-\n                       val_loss4.328.h5')\n    ```", "```\n    with open(f'{OUTPUT_PATH}/data_mapping.pickle', 'rb') as f:\n        data_mapping = pickle.load(f)\n    feats = [v['features'] for v in data_mapping.values()]\n    captions = [v['caption'] for v in data_mapping.values()]\n    ```", "```\n    evaluate_model(model,\n                   features=feats,\n                   captions=captions,\n                   tokenizer=extractor.tokenizer,\n                   max_seq_length=extractor.max_seq_length)\n    ```", "```\n    BLEU-1: 0.35674398077995173\n    BLEU-2: 0.17030332240763874\n    BLEU-3: 0.12170338107914261\n    BLEU-4: 0.05493477725774873\n    ```", "```\n    import glob\n    import pickle\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from tensorflow.keras.applications.vgg16 import *\n    from tensorflow.keras.models import *\n    from tensorflow.keras.preprocessing.sequence import \\\n        pad_sequences\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    from ch7.recipe1.extractor import ImageCaptionFeatureExtractor\n    ```", "```\n    def get_word_from_index(tokenizer, index):\n        return tokenizer.index_word.get(index, None)\n    ```", "```\n    def produce_caption(model,\n                        tokenizer,\n                        image,\n                        max_sequence_length):\n        text = 'beginsequence'\n        for _ in range(max_sequence_length):\n           sequence = tokenizer.texts_to_sequences([text])[0]\n           sequence = pad_sequences([sequence],\n                                 maxlen=max_sequence_length)\n            prediction = model.predict([[image], sequence])\n            index = np.argmax(prediction)\n            word = get_word_from_index(tokenizer, index)\n            if word is None:\n                break\n            text += f' {word}'\n            if word == 'endsequence':\n                break\n        return text\n    ```", "```\n    extractor_model = VGG16(weights='imagenet')\n    inputs = extractor_model.inputs\n    outputs = extractor_model.layers[-2].output\n    extractor_model = Model(inputs=inputs, outputs=outputs)\n    ```", "```\n    extractor = ImageCaptionFeatureExtractor(\n        feature_extractor=extractor_model)\n    ```", "```\n    with open('data_mapping.pickle', 'rb') as f:\n        data_mapping = pickle.load(f)\n    captions = [v['caption'] for v in \n                data_mapping.values()]\n    ```", "```\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(captions)\n    max_seq_length = extractor._get_max_seq_length(captions)\n    ```", "```\n    model = load_model('model-ep003-loss3.847-\n                         val_loss4.328.h5')\n    ```", "```\n    for idx, image_path in enumerate(glob.glob('*.jpg'), \n                                       start=1):\n        img_feats = (extractor\n                     .extract_image_features(image_path))\n    ```", "```\n        description = produce_caption(model,\n                                      tokenizer,\n                                      img_feats,\n                                      max_seq_length)\n        description = (description\n                       .replace('beginsequence', '')\n                       .replace('endsequence', ''))\n    ```", "```\n        image = plt.imread(image_path)\n        plt.imshow(image)\n        plt.title(description)\n        plt.savefig(f'{idx}.jpg')\n    ```", "```\n    import json\n    import os\n    import time\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import tensorflow as tf\n    from sklearn.model_selection import train_test_split\n    from sklearn.utils import shuffle\n    from tensorflow.keras.applications.inception_v3 import *\n    from tensorflow.keras.layers import *\n    from tensorflow.keras.losses import \\\n        SparseCategoricalCrossentropy\n    from tensorflow.keras.models import Model\n    from tensorflow.keras.optimizers import Adam\n    from tensorflow.keras.preprocessing.sequence import \\\n        pad_sequences\n    from tensorflow.keras.preprocessing.text import Tokenizer\n    from tensorflow.keras.utils import get_file\n    ```", "```\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    ```", "```\n    def load_image(image_path):\n        image = tf.io.read_file(image_path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, (299, 299))\n        image = preprocess_input(image)\n        return image, image_path\n    ```", "```\n    def get_max_length(tensor):\n        return max(len(t) for t in tensor)\n    ```", "```\n    def load_image_and_caption(image_name, caption):\n        image_name = image_name.decode('utf-8').split('/')\n                                          [-1]\n        image_tensor = np.load(f'./{image_name}.npy')\n        return image_tensor, caption\n    ```", "```\n    class BahdanauAttention(Model):\n        def __init__(self, units):\n            super(BahdanauAttention, self).__init__()\n            self.W1 = Dense(units)\n            self.W2 = Dense(units)\n            self.V = Dense(1)\n    ```", "```\n        def call(self, features, hidden):\n            hidden_with_time_axis = tf.expand_dims(hidden, \n                                                     1)\n            score = tf.nn.tanh(self.W1(features) +\n                            self.W2(hidden_with_time_axis))\n            attention_w = tf.nn.softmax(self.V(score), \n                                         axis=1)\n            ctx_vector = attention_w * features\n            ctx_vector = tf.reduce_sum(ctx_vector, axis=1)\n            return ctx_vector, attention_w\n    ```", "```\n    class CNNEncoder(Model):\n        def __init__(self, embedding_dim):\n            super(CNNEncoder, self).__init__()\n            self.fc = Dense(embedding_dim)\n        def call(self, x):\n            x = self.fc(x)\n            x = tf.nn.relu(x)\n            return x\n    ```", "```\n    class RNNDecoder(Model):\n        def __init__(self, embedding_size, units, \n                         vocab_size):\n            super(RNNDecoder, self).__init__()\n            self.units = units\n            self.embedding = Embedding(vocab_size, \n                                        embedding_size)\n            self.gru = GRU(self.units,\n                           return_sequences=True,\n                           return_state=True,\n                           recurrent_initializer='glorot_\n                           uniform')\n            self.fc1 = Dense(self.units)\n            self.fc2 = Dense(vocab_size)\n            self.attention = BahdanauAttention(self.units)\n    ```", "```\n        def call(self, x, features, hidden):\n            context_vector, attention_weights = \\\n                self.attention(features, hidden)\n    ```", "```\n            x = self.embedding(x)\n            expanded_context = tf.expand_dims(context_vector, \n                                               1)\n            x = Concatenate(axis=-1)([expanded_context, x])\n    ```", "```\n            output, state = self.gru(x)\n            x = self.fc1(output)\n            x = tf.reshape(x, (-1, x.shape[2]))\n            x = self.fc2(x)\n    ```", "```\n        def reset_state(self, batch_size):\n            return tf.zeros((batch_size, self.units))\n    ```", "```\n    class ImageCaptioner(object):\n        def __init__(self, embedding_size, units, \n                     vocab_size,\n                     tokenizer):\n            self.tokenizer = tokenizer\n            self.encoder = CNNEncoder(embedding_size)\n            self.decoder = RNNDecoder(embedding_size, \n                                        units,\n                                      vocab_size)\n            self.optimizer = Adam()\n            self.loss = SparseCategoricalCrossentropy(\n                from_logits=True,\n                reduction='none')\n    ```", "```\n        def loss_function(self, real, predicted):\n            mask = tf.math.logical_not(tf.math.equal(real, \n                                                   0))\n            _loss = self.loss(real, predicted)\n            mask = tf.cast(mask, dtype=_loss.dtype)\n            _loss *= mask\n            return tf.reduce_mean(_loss)\n    ```", "```\n        @tf.function\n        def train_step(self, image_tensor, target):\n            loss = 0\n            hidden = \n           self.decoder.reset_state(target.shape[0])\n            start_token_idx = \n           self.tokenizer.word_index['<start>']\n            init_batch = [start_token_idx] * \n            target.shape[0]\n            decoder_input = tf.expand_dims(init_batch, 1)\n    ```", "```\n          with tf.GradientTape() as tape:\n                features = self.encoder(image_tensor)\n                for i in range(1, target.shape[1]):\n                    preds, hidden, _ = \n                    self.decoder(decoder_input,\n                                features,\n                                 hidden)\n                    loss += self.loss_function(target[:, i],\n                                               preds)\n                    decoder_input = \n                           tf.expand_dims(target[:, i],1)\n    ```", "```\n            total_loss = loss / int(target.shape[1])\n            trainable_vars = (self.encoder.trainable_\n                                variables +\n                              self.decoder.trainable_\n                                 variables)\n            gradients = tape.gradient(loss, trainable_vars)\n            self.optimizer.apply_gradients(zip(gradients,\n                                          trainable_vars))\n            return loss, total_loss\n    ```", "```\n        def train(self, dataset, epochs, num_steps):\n            for epoch in range(epochs):\n                start = time.time()\n                total_loss = 0\n                for batch, (image_tensor, target) \\\n                        in enumerate(dataset):\n                    batch_loss, step_loss = \\\n                        self.train_step(image_tensor, target)\n                    total_loss += step_loss\n    ```", "```\n                    if batch % 100 == 0:\n                        loss = batch_loss.numpy()\n                        loss = loss / int(target.shape[1])\n                        print(f'Epoch {epoch + 1}, batch \n                                                 {batch},'\n                              f' loss {loss:.4f}')\n                print(f'Epoch {epoch + 1},'\n                      f' loss {total_loss / \n                               num_steps:.6f}')\n                epoch_time = time.time() - start\n                print(f'Time taken: {epoch_time} seconds. \n                       \\n')\n    ```", "```\n    INPUT_DIR = os.path.abspath('.')\n    annots_folder = '/annotations/'\n    if not os.path.exists(INPUT_DIR + annots_folder):\n        origin_url = ('http://images.cocodataset.org/\n                annotations''/annotations_trainval2014.zip')\n        cache_subdir = os.path.abspath('.')\n        annots_zip = get_file('all_captions.zip',\n                              cache_subdir=cache_subdir,\n                              origin=origin_url,\n                              extract=True)\n        annots_file = (os.path.dirname(annots_zip) +\n                      '/annotations/captions_train2014.json')\n        os.remove(annots_zip)\n    else:\n        annots_file = (INPUT_DIR +\n                      '/annotations/captions_train2014.json')\n    ```", "```\n    image_folder = '/train2014/'\n    if not os.path.exists(INPUT_DIR + image_folder):\n        origin_url = ('http://images.cocodataset.org/zips/'\n                      'train2014.zip')\n        cache_subdir = os.path.abspath('.')\n        image_zip = get_file('train2014.zip',\n                             cache_subdir=cache_subdir,\n                             origin=origin_url,\n                             extract=True)\n        PATH = os.path.dirname(image_zip) + image_folder\n        os.remove(image_zip)\n    else:\n        PATH = INPUT_DIR + image_folder\n    ```", "```\n    with open(annots_file, 'r') as f:\n        annotations = json.load(f)\n    captions = []\n    image_paths = []\n    for annotation in annotations['annotations']:\n        caption = '<start>' + annotation['caption'] + ' <end>'\n        image_id = annotation['image_id']\n        image_path = f'{PATH}COCO_train2014_{image_id:012d}.jpg'\n        image_paths.append(image_path)\n        captions.append(caption)\n    ```", "```\n    train_captions, train_image_paths = shuffle(captions,\n                                            image_paths, \n                                          random_state=42)\n    SAMPLE_SIZE = 30000\n    train_captions = train_captions[:SAMPLE_SIZE]\n    train_image_paths = train_image_paths[:SAMPLE_SIZE]\n    train_images = sorted(set(train_image_paths))\n    ```", "```\n    feature_extractor = InceptionV3(include_top=False,\n                                    weights='imagenet')\n    feature_extractor = Model(feature_extractor.input,\n                              feature_extractor.layers[-\n                                            1].output)\n    ```", "```\n    BATCH_SIZE = 8\n    image_dataset = (tf.data.Dataset\n                     .from_tensor_slices(train_images)\n                     .map(load_image, \n                         num_parallel_calls=AUTOTUNE)\n                     .batch(BATCH_SIZE))\n    for image, path in image_dataset:\n        batch_features = feature_extractor.predict(image)\n        batch_features = tf.reshape(batch_features,\n                                 (batch_features.shape[0],\n                                     -1,\n                                batch_features.shape[3]))\n        for batch_feature, p in zip(batch_features, path):\n            feature_path = p.numpy().decode('UTF-8')\n            image_name = feature_path.split('/')[-1]\n            np.save(f'./{image_name}', batch_feature.numpy())\n    ```", "```\n    top_k = 5000\n    filters = '!”#$%&()*+.,-/:;=?@[\\]^_`{|}~ '\n    tokenizer = Tokenizer(num_words=top_k,\n                          oov_token='<unk>',\n                          filters=filters)\n    tokenizer.fit_on_texts(train_captions)\n    tokenizer.word_index['<pad>'] = 0\n    tokenizer.index_word[0] = '<pad>'\n    train_seqs = tokenizer.texts_to_sequences(train_captions)\n    captions_seqs = pad_sequences(train_seqs, \n                                  padding='post')\n    max_length = get_max_length(train_seqs)\n    ```", "```\n    (images_train, images_val, caption_train, caption_val) = \\\n        train_test_split(train_img_paths,\n                         captions_seqs,\n                         test_size=0.2,\n                         random_state=42)\n    ```", "```\n    BATCH_SIZE = 64\n    BUFFER_SIZE = 1000\n    dataset = (tf.data.Dataset\n               .from_tensor_slices((images_train, \n                                    caption_train))\n               .map(lambda i1, i2:\n                    tf.numpy_function(\n                        load_image_and_caption,\n                        [i1, i2],\n                        [tf.float32, tf.int32]),\n                    num_parallel_calls=AUTOTUNE)\n               .shuffle(BUFFER_SIZE)\n               .batch(BATCH_SIZE)\n               .prefetch(buffer_size=AUTOTUNE))\n    ```", "```\n    image_captioner = ImageCaptioner(embedding_size=256,\n                                     units=512,\n                                     vocab_size=top_k + 1,\n                                     tokenizer=tokenizer)\n    EPOCHS = 30\n    num_steps = len(images_train) // BATCH_SIZE\n    image_captioner.train(dataset, EPOCHS, num_steps)\n    ```", "```\n    def evaluate(encoder, decoder, tokenizer, image, \n                  max_length,\n                 attention_shape):\n        attention_plot = np.zeros((max_length,\n                                   attention_shape))\n    ```", "```\n        hidden = decoder.reset_state(batch_size=1)\n        temp_input = tf.expand_dims(load_image(image)[0], \n                                        0)\n        image_tensor_val = feature_extractor(temp_input)\n        image_tensor_val = tf.reshape(image_tensor_val,\n                               (image_tensor_val.shape[0],\n                                       -1,\n                              image_tensor_val.shape[3]))\n        feats = encoder(image_tensor_val)\n        start_token_idx = tokenizer.word_index['<start>']\n        dec_input = tf.expand_dims([start_token_idx], 0)\n        result = []\n    ```", "```\n        for i in range(max_length):\n            (preds, hidden, attention_w) = \\\n                decoder(dec_input, feats, hidden)\n            attention_plot[i] = tf.reshape(attention_w,\n                                           (-1,)).numpy()\n            pred_id = tf.random.categorical(preds,\n                                         1)[0][0].numpy()\n            result.append(tokenizer.index_word[pred_id])\n            if tokenizer.index_word[pred_id] == '<end>':\n                return result, attention_plot\n            dec_input = tf.expand_dims([pred_id], 0)\n        attention_plot = attention_plot[:len(result), :]\n        return result, attention_plot\n    ```", "```\n    def plot_attention(image, result,\n                       attention_plot, output_path):\n        tmp_image = np.array(load_image(image)[0])\n        fig = plt.figure(figsize=(10, 10))\n    ```", "```\n        for l in range(len(result)):\n            temp_att = np.resize(attention_plot[l], (8, 8))\n            ax = fig.add_subplot(len(result) // 2,\n                                 len(result) // 2,\n                                 l + 1)\n            ax.set_title(result[l])\n            image = ax.imshow(tmp_image)\n            ax.imshow(temp_att,\n                      cmap='gray',\n                      alpha=0.6,\n                      extent=image.get_extent())\n    ```", "```\n        plt.tight_layout()\n        plt.show()\n        plt.savefig(output_path)\n    ```", "```\n    attention_features_shape = 64\n    random_id = np.random.randint(0, len(images_val))\n    image = images_val[random_id]\n    ```", "```\n    actual_caption = ' '.join([tokenizer.index_word[i]\n                             for i in caption_val[random_id]\n                               if i != 0])\n    actual_caption = (actual_caption\n                      .replace('<start>', '')\n                      .replace('<end>', ''))\n    ```", "```\n    result, attention_plot = evaluate(image_captioner               \n                                    encoder,\n                       image_captioner.decoder,\n                                      tokenizer,\n                                      image,\n                                      max_length,\n                              attention_feats_shape)\n    ```", "```\n    predicted_caption = (' '.join(result)\n                         .replace('<start>', '')\n                         .replace('<end>', '')) \n    ```", "```\n    print(f'Actual caption: {actual_caption}')\n    print(f'Predicted caption: {predicted_caption}')\n    output_path = './attention_plot.png'\n    plot_attention(image, result, attention_plot, output_path)\n    ```", "```\n    Actual caption: a lone giraffe stands in the midst of a grassy area\n    Predicted caption: giraffe standing in a dry grass near trees\n    ```"]