<html><head></head><body>
		<div id="_idContainer166">
			<h1 id="_idParaDest-127"><em class="italic"><a id="_idTextAnchor136"/>Chapter 7</em>: High Fidelity Face Generation </h1>
			<p>As GANs began to become more stable to train, thanks to improvements to loss functions and normalization techniques, people started to shift their focus to trying to generate higher-resolution images. Previously, most GANs were only capable of generating images up to a resolution of 256x256, and simply adding more upscaling layers to the generator did not help. </p>
			<p>In this chapter, we will look at techniques that are capable of generating images of high resolutions of 1024x1024 and beyond. We will start by implementing a seminal GAN known as <strong class="bold">Progressive GAN</strong>, sometimes abbreviated to <strong class="bold">ProGAN</strong>. This was the first GAN that was successful at generating 1024x1024 high-fidelity face portraits. High-fidelity doesn't just mean high-resolution but also a high resemblance to a real face. We can have a high-resolution generated face image, but if it has four eyes, then it isn't high fidelity. </p>
			<p>After ProGAN, we will implement <strong class="bold">StyleGAN</strong>, which builds on top of ProGAN. StyleGAN incorporates AdaIN from style transfer to allow finer style control and style mixing to generate a variety of images.</p>
			<p>We will cover the following in this chapter:</p>
			<ul>
				<li>ProGAN overview</li>
				<li>Building a ProGAN</li>
				<li>Implementing StyleGAN</li>
			</ul>
			<h1 id="_idParaDest-128"><a id="_idTextAnchor137"/>Technical requirements</h1>
			<p>The Jupyter notebooks and code can be found here:</p>
			<p><a href="https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter07">https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter07</a></p>
			<p>The notebooks used in this chapter are listed here:</p>
			<ul>
				<li><strong class="source-inline">ch7_progressive_gan.ipynb</strong></li>
				<li><strong class="source-inline">ch7_style_gan.ipynb</strong></li>
			</ul>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor138"/>ProGAN overview</h1>
			<p>In a typical GAN <a id="_idIndexMarker481"/>setting, the generator output shape is fixed. In other words, the training image size does not change. If we want to try to double the image resolution, we add an additional upsampling layer to the generator architecture and start the training from scratch. People have tried and failed to increase image resolution by this brute-force method. The enlarged image resolution and network size increases the dimension space, making it more difficult to learn. </p>
			<p>CNNs faced the same problem and solved it by using a batch normalization layer, but this doesn't work well with GANs. The idea of ProGAN is to not train all the layers simultaneously but start by training the lowest layer in both the generator and the discriminator, so that the layer's weights are stabilized before adding new layers. We can see it as pre-training the network with lower resolutions. This idea is the core innovation brought by ProGAN, as detailed in the academic paper <em class="italic">Progressive Growing of GANs for Improved Quality, Stability, and Variation</em> by T. Karras et al. The following diagram illustrates the process of growing the network in ProGAN:</p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B14538_07_01.jpg" alt="Figure 7.1 – Illustration of the progressive growing of layers.&#13;&#10;(Redrawn from T. Karras et al. 2018, &quot;Progressive Growing of GANs for Improved Quality, Stability, &#13;&#10;and Variation,&quot; https://arxiv.org/abs/1710.10196)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Illustration of the progressive growing of layers.(Redrawn from T. Karras et al. 2018, "Progressive Growing of GANs for Improved Quality, Stability, and Variation," https://arxiv.org/abs/1710.10196)</p>
			<p>Like vanilla GANs, ProGAN's input is a latent vector sampled from random noise. As shown in the preceding <a id="_idIndexMarker482"/>diagram, we start with an image resolution of <strong class="bold">4x4</strong> and only have one block in both the generator and the discriminator. After training in the <strong class="bold">4x4</strong> resolution for a while, we add new layers for the <strong class="bold">8x8</strong> resolution. We then keep doing that until we reach the final image resolution of <strong class="bold">1024x1024</strong>. The following 256x256 images were generated using ProGAN and were released by NVIDIA. The image quality is breathtaking; they are literally indistinguishable from real faces:</p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B14538_07_02.jpg" alt="Figure 7.2 – High-fidelity images generated by ProGAN&#13;&#10;(Source: https://github.com/tkarras/progressive_growing_of_gans)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – High-fidelity images generated by ProGAN (Source: https://github.com/tkarras/progressive_growing_of_gans)</p>
			<p>It is fair to say that the <a id="_idIndexMarker483"/>superior image generation is mostly down to growing the networks progressively. The network architecture is quite simple, consisting only of convolutional layers and dense layers, rather than more complex architectures such as residual blocks or VAE-like architectures that were more common among GANs. </p>
			<p>It was not until two generations after the introduction of ProGAN, with StyleGAN 2, that the authors started exploring these network architectures. The loss function is also simple, just WGAN-GP loss, without any other losses such as content loss, reconstruction loss, or KL divergence loss. However, there are several minor innovations that we should go over before implementing the core part of growing layers progressively. These innovations are as follows:</p>
			<ul>
				<li>Pixel normalization</li>
				<li>Minibatch statistics</li>
				<li>Equalized learning rate</li>
			</ul>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor139"/>Pixel normalization</h2>
			<p>Batch <a id="_idIndexMarker484"/>normalization should reduce the covariate<a id="_idIndexMarker485"/> shift, but the ProGAN authors did not observe that in the network training. Therefore, they ditched batch normalization and used a custom normalization for the generator, known as <strong class="bold">pixel normalization</strong>. On a separate note, other researchers later found that batch normalization doesn't really solve the covariate problem despite stabilizing deep neural network training. </p>
			<p>Anyway, the purpose of normalization in ProGAN is to limit the weight values to prevent them from growing exponentially. Large weights could escalate signal magnitudes and result in unhealthy competition between the generator and the discriminator. Pixel normalization normalizes the feature in each pixel location (H, W) across the channel dimension to unit length. If the tensor is a batched RGB image with dimension (N, H, W, C), the RGB vector of any pixel will have a magnitude of 1. </p>
			<p>We can implement the equation using a custom layer as shown in the following code:</p>
			<p class="source-code">class PixelNorm(Layer):</p>
			<p class="source-code">    def __init__(self, epsilon=1e-8):</p>
			<p class="source-code">        super(PixelNorm, self).__init__()</p>
			<p class="source-code">        self.epsilon = epsilon</p>
			<p class="source-code">    def call(self, input_tensor):</p>
			<p class="source-code">        return input_tensor / tf.math.sqrt(                        	                            tf.reduce_mean(input_tensor**2,         	                            axis=-1, keepdims=True) + 		                            self.epsilon)</p>
			<p>Unlike other normalizations, pixel normalization doesn't have any learnable parameters; it only consists of simple arithmetic operations and hence is computationally efficient to run.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor140"/>Increasing image variation with minibatch statistics</h2>
			<p><strong class="bold">Mode collapse</strong> happens<a id="_idIndexMarker486"/> when <a id="_idIndexMarker487"/>a GAN <a id="_idIndexMarker488"/>generates similar-looking images as it captures only a subset of the variation found in the training data. One way to encourage more variation is to show the statistics of a minibatch to the discriminator. The statistics from a minibatch are more varied compared to only a single instance, and this encourages the generator to generate images that show similar statistics. </p>
			<p>Batch normalization uses minibatch statistics to normalize the activation, which in some way serves this purpose, but ProGAN doesn't use batch normalization. Instead, it uses a <strong class="bold">minibatch layer</strong> that<a id="_idIndexMarker489"/> calculates the minibatch standard deviation and appends it to the activation without changing the activation itself.</p>
			<p>The steps to calculate minibatch statistics are as follows:</p>
			<ol>
				<li>Calculate the standard deviation for each feature in each spatial location over the minibatch – in other words, across dimension <em class="italic">N</em>. </li>
				<li>Calculate the average of these standard deviations across the (<em class="italic">H, W, C</em>) dimensions to arrive at a single scale value.</li>
				<li>Replicate this value across the feature map of (<em class="italic">H, W</em>) and append it to the activation. As a result, the output activation has a shape of (<em class="italic">N, H, W, C+1</em>).</li>
			</ol>
			<p>The following is the code for a minibatch standard deviation custom layer:</p>
			<p class="source-code">class MinibatchStd(Layer):</p>
			<p class="source-code">    def __init__(self, group_size=4, epsilon=1e-8):</p>
			<p class="source-code">        super(MinibatchStd, self).__init__()</p>
			<p class="source-code">        self.epsilon = epsilon</p>
			<p class="source-code">        self.group_size = group_size</p>
			<p class="source-code">    def call(self, input_tensor):</p>
			<p class="source-code">        n, h, w, c = input_tensor.shape</p>
			<p class="source-code">        x = tf.reshape(input_tensor, [self.group_size,  				 -1, h, w, c])</p>
			<p class="source-code">        group_mean, group_var = tf.nn.moments(x,  							axes=(0), 							keepdims=False)</p>
			<p class="source-code">        group_std = tf.sqrt(group_var + self.epsilon)</p>
			<p class="source-code">        avg_std = tf.reduce_mean(group_std, axis=[1,2,3], 						keepdims=True)</p>
			<p class="source-code">        x = tf.tile(avg_std, [self.group_size, h, w, 1])</p>
			<p class="source-code">        return tf.concat([input_tensor, x], axis=-1)</p>
			<p>Before calculating the standard deviation, the activation is first split into groups of <strong class="source-inline">4</strong> or the batch <a id="_idIndexMarker490"/>size, whichever<a id="_idIndexMarker491"/> is lower. To simplify the code, we assume that the batch size is at least <strong class="source-inline">4</strong> during training. The minibatch layer can be inserted anywhere in the discriminator, but it was found to be more effective toward the end, which is the 4x4 layer. </p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor141"/>Equalized learning rate</h2>
			<p>The name<a id="_idIndexMarker492"/> can be misleading, as the <strong class="bold">equalized learning rate</strong> does<a id="_idIndexMarker493"/> not modify the learning rate like <strong class="bold">learning rate decay</strong>. In fact, the learning rate of the <a id="_idIndexMarker494"/>optimizer stays constant throughout the training. To understand this, let's recap how backpropagation works. When using a simple <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>) optimizer, the negative gradients are multiplied by the learning rate before updating<a id="_idIndexMarker495"/> the weights. Therefore, the layers closer to the generator input will receive less gradient (remember the vanishing gradient?). </p>
			<p>What if we want a layer to receive more gradient? Let's say we perform a simple matrix multiplication <em class="italic">y = w*x</em>, and now we add a constant <em class="italic">2</em> to make it <em class="italic">y = 2*w*x</em>. During backpropagation, the gradients will also be multiplied by <em class="italic">2</em>, hence becoming larger. We could then set different multiplier constants for different layers to effectively have different learning rates. </p>
			<p>In ProGAN, these multiplier constants are calculated from He's initializer. <strong class="bold">He</strong> or <strong class="bold">Kaiming</strong> initialization is named after Kaiming He, the inventor of ResNet. The <strong class="bold">weight</strong> initialization is designed specifically for networks that use the ReLU family of activation functions. Usually, weights are initialized using normal distribution with a specified standard deviation; for example, we used 0.02 in previous chapters. Instead of having to guess the standard deviation, He calculates it using the following equation:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/Formula_07_001.jpg" alt=""/>
				</div>
			</div>
			<p><strong class="bold">Fan in</strong> is the<a id="_idIndexMarker496"/> multiplication of the weights dimension <a id="_idIndexMarker497"/>except for the output channel. For a convolution weight with the shape (<strong class="source-inline">kernel, kernel, channel_in, channel_out</strong>), the <em class="italic">fan in</em> is the multiplication of <strong class="source-inline">kernel x kernel x channel_in</strong>. To use this in weight initialization, we can pass <strong class="source-inline">tf.keras.initializers.he_normal</strong> to the Keras layer. However, an equalized learning rate does this at runtime, so we will write custom layers to calculate the standard deviation. </p>
			<p>The default gain factor for the initialization is 2, but ProGAN uses a lower gain for the dense layer for the input of the 4x4 generator. ProGAN uses standard normal distribution to initialize the layer weights and scale them with their normalization constant. This deviates from the trend of careful weight initializations that was common among GANs. We now write a custom Conv2D layer that uses pixel normalization:</p>
			<p class="source-code">class Conv2D(layers.Layer):</p>
			<p class="source-code">    def build(self, input_shape):</p>
			<p class="source-code">        self.in_channels = input_shape[-1]</p>
			<p class="source-code">        fan_in = self.kernel*self.kernel*self.in_channels</p>
			<p class="source-code">        self.scale = tf.sqrt(self.gain/fan_in)</p>
			<p class="source-code">    def call(self, inputs):</p>
			<p class="source-code">        x = tf.pad(inputs, [[0, 0], [1, 1], [1, 1],  			  		[0, 0]], mode='REFLECT') \ 				if self.pad else inputs </p>
			<p class="source-code">        output = tf.nn.conv2d(x, self.scale*self.w, 					   strides=1,  					   padding="SAME") + self.b</p>
			<p class="source-code">        return output</p>
			<p>The official ProGAN uses zero padding in the convolutional layer, and you can see the border artifacts, especially when viewing low-resolution images. Therefore, we added reflective padding except for the 1x1 kernel, where no padding is needed. Larger layers have smaller scale factors, which effectively reduces the gradient and hence the learning rate. This causes the learning rate to be adjusted based on the layer size so that weights in big <a id="_idIndexMarker498"/>layers do not grow too quickly, hence<a id="_idIndexMarker499"/> the name equalized learning rate.</p>
			<p>The custom <strong class="source-inline">Dense</strong> layer can be written in a similar manner:</p>
			<p class="source-code">class Dense(layers.Layer):</p>
			<p class="source-code">    def __init__(self, units, gain=2, **kwargs):</p>
			<p class="source-code">        super(Dense, self).__init__(kwargs)</p>
			<p class="source-code">        self.units = units</p>
			<p class="source-code">        self.gain = gain</p>
			<p class="source-code">    def build(self, input_shape):</p>
			<p class="source-code">        self.in_channels = input_shape[-1]</p>
			<p class="source-code">        initializer = \ 			tf.keras.initializers.RandomNormal( 						mean=0., stddev=1.)        </p>
			<p class="source-code">        self.w = self.add_weight(shape=[self.in_channels,</p>
			<p class="source-code">                                        self.units],</p>
			<p class="source-code">                                initializer=initializer,</p>
			<p class="source-code">                                trainable=True,</p>
			<p class="source-code"> 					     name='kernel')</p>
			<p class="source-code">        self.b = self.add_weight(shape=(self.units,),</p>
			<p class="source-code">                                initializer='zeros',</p>
			<p class="source-code">                                trainable=True, </p>
			<p class="source-code"> 						name='bias')</p>
			<p class="source-code">        fan_in = self.in_channels</p>
			<p class="source-code">        self.scale = tf.sqrt(self.gain/fan_in)</p>
			<p class="source-code">    def call(self, inputs):</p>
			<p class="source-code">        output = tf.matmul(inputs,  					self.scale*self.w) + self.b</p>
			<p class="source-code">        return output</p>
			<p>Notice that the<a id="_idIndexMarker500"/> custom layer accepts <strong class="source-inline">**kwargs</strong> in<a id="_idIndexMarker501"/> the constructor, meaning we can pass in the usual Keras keyword arguments for the <strong class="source-inline">Dense</strong> layer. We now have all the ingredients required to start building a ProGAN in the next section.</p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor142"/>Building a ProGAN</h1>
			<p>We have now<a id="_idIndexMarker502"/> learned about the three features of ProGANs – pixel normalization, minibatch standard deviation statistics, and the equalized learning rate. Now, we are going to delve into the network architecture and look at how to grow the network progressively. ProGAN grows an image by growing the layers, starting from a resolution of 4x4, then doubling it to 8x8, 16x16, and so on to 1024x1024. Thus, we will first write the code to build the layer block at each scale. The building blocks of the generator and discriminator are trivially simple, as we will see. </p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor143"/>Building the generator blocks</h2>
			<p>We will start by <a id="_idIndexMarker503"/>building the 4x4 generator block, which forms the base of the generator and takes in the latent code as input. The input is normalized by <strong class="source-inline">PixelNorm</strong> before going to <strong class="source-inline">Dense</strong>. A lower gain is used for the equalized learning rate for that layer. Leaky ReLU and pixel normalization are used throughout all the generator blocks. We build the generator as follows:</p>
			<p class="source-code">def build_generator_base(self, input_shape):</p>
			<p class="source-code">    input_tensor = Input(shape=input_shape)</p>
			<p class="source-code">    x = PixelNorm()(input_tensor)</p>
			<p class="source-code">    x = Dense(8192, gain=1./8)(x)</p>
			<p class="source-code">    x = Reshape((4, 4, 512))(x)</p>
			<p class="source-code">    x = LeakyReLU(0.2)(x)        </p>
			<p class="source-code">    x = PixelNorm()(x)</p>
			<p class="source-code">    x = Conv2D(512, 3, name='gen_4x4_conv1')(x)</p>
			<p class="source-code">    x = LeakyReLU(0.2)(x)</p>
			<p class="source-code">    x = PixelNorm()(x)</p>
			<p class="source-code">    return Model(input_tensor, x, </p>
			<p class="source-code">                 name='generator_base')</p>
			<p>After the 4x4 generator block, all subsequent blocks have the same architecture, which involves an upsampling layer followed by two convolutional layers. The only difference is the convolutional filter size. In the ProGAN's default setting, a filter size of 512 is used up to the 32x32 generator block, then it is halved at each stage to finally reach 16 at 1024x1024, as follows:</p>
			<p class="source-code">self.log2_res_to_filter_size = {</p>
			<p class="source-code">    0: 512,</p>
			<p class="source-code">    1: 512,</p>
			<p class="source-code">    2: 512, # 4x4</p>
			<p class="source-code">    3: 512, # 8x8</p>
			<p class="source-code">    4: 512, # 16x16</p>
			<p class="source-code">    5: 512, # 32x32</p>
			<p class="source-code">    6: 256, # 64x64</p>
			<p class="source-code">    7: 128, # 128x128</p>
			<p class="source-code">    8: 64,  # 256x256</p>
			<p class="source-code">    9: 32,  # 512x512</p>
			<p class="source-code">    10: 16} # 1024x1024</p>
			<p>To make the coding<a id="_idIndexMarker504"/> easier, we can linearize the resolution by taking logarithm with base <strong class="source-inline">2</strong>. Hence, <em class="italic">log2(4)</em> is <em class="italic">2</em>, <em class="italic">log2(8)</em> is <em class="italic">3</em>, ... to <em class="italic">log2(1024)</em> is <em class="italic">10</em>. Then, we can loop through the resolution linearly in <strong class="source-inline">log2</strong> from 2 to 10 as follows:</p>
			<p class="source-code">def build_generator_block(self, log2_res, input_shape):</p>
			<p class="source-code">    res = 2**log2_res</p>
			<p class="source-code">    res_name = f'{res}x{res}'</p>
			<p class="source-code">    filter_n = self.log2_res_to_filter_size[log2_res]</p>
			<p class="source-code">    input_tensor = Input(shape=input_shape)</p>
			<p class="source-code">    x = UpSampling2D((2,2))(input_tensor)</p>
			<p class="source-code">    x = Conv2D(filter_n, 3,  		    name=f'gen_{res_name}_conv1')(x)</p>
			<p class="source-code">    x = PixelNorm()(LeakyReLU(0.2)(x))</p>
			<p class="source-code">    x = Conv2D(filter_n, 3, 			name=f'gen_{res_name}_conv2')(x)</p>
			<p class="source-code">    x = PixelNorm()(LeakyReLU(0.2)(x))</p>
			<p class="source-code">    return Model(input_tensor, x, </p>
			<p class="source-code">                 name=f'genblock_{res}_x_{res}')</p>
			<p>We can now use<a id="_idIndexMarker505"/> this code to build all the generator blocks from 4x4 all the way to the target resolution. </p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor144"/>Building the discriminator blocks</h2>
			<p>We can now <a id="_idIndexMarker506"/>shift our attention to the discriminator. The basic discriminator is at a 4x4 resolution, where it takes 4x4x3 images and predicts whether the image is real or fake. It uses one convolutional layer, followed by two dense layers. Unlike the generator, the discriminator does not use pixel normalization; in fact, no normalization is used at all. We will insert the minibatch standard deviation layer as follows:</p>
			<p class="source-code">def build_discriminator_base(self, input_shape):</p>
			<p class="source-code">    input_tensor = Input(shape=input_shape)</p>
			<p class="source-code">    x = <strong class="bold">MinibatchStd</strong>()(input_tensor)</p>
			<p class="source-code">    x = Conv2D(512, 3, name='gen_4x4_conv1')(x)</p>
			<p class="source-code">    x = LeakyReLU(0.2)(x)</p>
			<p class="source-code">    x = Flatten()(x)</p>
			<p class="source-code">    x = Dense(512, name='gen_4x4_dense1')(x)</p>
			<p class="source-code">    x = LeakyReLU(0.2)(x)</p>
			<p class="source-code">    x = Dense(1, name='gen_4x4_dense2')(x)</p>
			<p class="source-code">    return Model(input_tensor, x, </p>
			<p class="source-code">                 name='discriminator_base')</p>
			<p>After that, the discriminator uses two convolutional layers followed by downsampling, using average pooling at every stage: </p>
			<p class="source-code">def build_discriminator_block(self, log2_res, input_shape):</p>
			<p class="source-code">    filter_n = self.log2_res_to_filter_size[log2_res]        </p>
			<p class="source-code">    input_tensor = Input(shape=input_shape)</p>
			<p class="source-code">    x = Conv2D(filter_n, 3)(input_tensor)</p>
			<p class="source-code">    x = LeakyReLU(0.2)(x)</p>
			<p class="source-code">    filter_n = self.log2_res_to_filter_size[log2_res-1]        </p>
			<p class="source-code">    x = Conv2D(filter_n, 3)(x)</p>
			<p class="source-code">    x = LeakyReLU(0.2)(x)</p>
			<p class="source-code">    x = AveragePooling2D((2,2))(x)</p>
			<p class="source-code">    res = 2**log2_res</p>
			<p class="source-code">    return Model(input_tensor, x, </p>
			<p class="source-code">                 name=f'disc_block_{res}_x_{res}')</p>
			<p>We now have all<a id="_idIndexMarker507"/> the basic building blocks defined. Next, we will look at how to join them together to grow the network progressively. </p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor145"/>Progressively growing the network</h2>
			<p>This is the most<a id="_idIndexMarker508"/> important part of ProGAN – growing the network. We can use the preceding functions to create generator and discriminator blocks at different resolutions. All we need to do now is to join them together as we grow the layers. The following diagram shows the process of growing the network. Let's start from the left-hand side: </p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B14538_07_03.jpg" alt="Figure 7.3 – Illustration of progressively growing layers. &#13;&#10;Redrawn from T. Karras et al. 2018, &quot;Progressive Growing of GANs for Improved Quality, Stability, and Variation,&quot; https://arxiv.org/abs/1710.10196)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Illustration of progressively growing layers. </p>
			<p class="figure-caption">Redrawn from T. Karras et al. 2018, "Progressive Growing of GANs for Improved Quality, Stability, and Variation," https://arxiv.org/abs/1710.10196)</p>
			<p>In the generator and <a id="_idIndexMarker509"/>discriminator blocks that we have built, we assume both the input and output to be layer activations rather than RGB images. Thus, we will need to convert the activation from the generator block into an RGB image. Similarly, for the discriminator, we will need to convert the image into activation. This is shown by <strong class="bold">(a)</strong> in the preceding figure. </p>
			<p>We will create two more functions that build the blocks to convert into and from RGB images. Both blocks use a 1x1 convolutional layer; the <strong class="source-inline">to_rgb</strong> block uses a filter size of 3 to match the RGB channels, while the <strong class="source-inline">from_rgb</strong> blocks use a filter size that matches the input activation of the discriminator block at that scale. The code of the two functions is as follows:</p>
			<p class="source-code">def build_to_rgb(self, res, filter_n): </p>
			<p class="source-code">    return Sequential([Input(shape=(res, res, filter_n)),</p>
			<p class="source-code">                       Conv2D(3, 1, gain=1, </p>
			<p class="source-code"> 				 activation='tanh')])</p>
			<p class="source-code">def build_from_rgb(self, res, filter_n): </p>
			<p class="source-code">    return Sequential([Input(shape=(res, res, 3)),</p>
			<p class="source-code">                       Conv2D(filter_n, 1),</p>
			<p class="source-code">                       LeakyReLU(0.2)])</p>
			<p>Now, assume the<a id="_idIndexMarker510"/> network is at 16x16, meaning there are already layers at the lower resolutions of 8x8 and 4x4. Now we are about to grow the 32x32 layer. However, if we add a new untrained layer to the network, the newly generated images will look like noise and will result in a huge loss. This can in turn result in exploding gradients and destabilize the training. </p>
			<p>To minimize this disruption, the 32x32 image generated by the new layer is not used immediately. Instead, we upsample the 16x16 image from the previous stage and fade in with the new 32x32 image. Fade is a technical term in image processing that refers to gradually increasing the opacity of an image. This is implemented by using a weighted sum with the following equation:</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/Formula_07_002.jpg" alt=""/>
				</div>
			</div>
			<p>In this transition phase, alpha increases from 0 to 1. In other words, at the start of the phase, we discard the image from the new layer completely and use the one from the previous trained layer. We then increase alpha linearly to 1 when only the image generated by the new layer is used. The stable state is shown by <em class="italic">(c)</em> in the preceding diagram. We can implement a custom layer to perform the weighted sum as follows:</p>
			<p class="source-code">class FadeIn(Layer):</p>
			<p class="source-code">    @tf.function</p>
			<p class="source-code">    def call(self, input_alpha, a, b):</p>
			<p class="source-code">        alpha = tf.reduce_mean(input_alpha)</p>
			<p class="source-code">        y = alpha * a + (1. - alpha) * b</p>
			<p class="source-code">        return y</p>
			<p>When using a subclass to define a layer, we can pass in a scalar alpha to the function. However, it is not possible when we use <strong class="bold">Sequential</strong> to define our Keras model. The variable within the model, for instance, <strong class="source-inline">self.alpha = tf.Variable(1.0)</strong>, will be converted to a constant when we compile the model and can no longer be changed in the training. </p>
			<p>One way to pass in a scalar alpha is to write the entire model with subclassing, but I feel it is more convenient in this case to use the sequential or functional API to create the models. To address this problem, we define alpha as an input to the model. However, the model input is assumed to be a minibatch. To be concrete, if we define <strong class="source-inline">Input(shape=(1))</strong>, its actual shape will be (<em class="italic">None, 1</em>), where the first dimension is the batch size. Therefore, <strong class="source-inline">tf.reduce_mean()</strong> in <strong class="source-inline">FadeIN()</strong> is meant to convert the batched value to <a id="_idIndexMarker511"/>a scalar value.</p>
			<p>Now, we can look at the following steps to grow the generator to, say, 32x32:</p>
			<ol>
				<li value="1">Add a 4x4 generator, where the input is a latent vector.</li>
				<li>In a loop, add subsequent generators of gradually increasing resolutions until the one before the target resolution (in our example, 16x16).</li>
				<li>Add <strong class="source-inline">to_rgb</strong> from 16x16 and upsample it to 32x32.</li>
				<li>Add the 32x32 generator block.</li>
				<li>Fade in the two images to create one final RGB image.</li>
			</ol>
			<p>The code is as follows:</p>
			<p class="source-code">def grow_generator(self, log2_res):</p>
			<p class="source-code">    res = 2**log2_res</p>
			<p class="source-code">    alpha = Input(shape=(1))</p>
			<p class="source-code">    x = self.generator_blocks[2].input</p>
			<p class="source-code">    for i in range(2, log2_res):            </p>
			<p class="source-code">        x = self.generator_blocks[i](x)</p>
			<p class="source-code">    old_rgb = self.to_rgb[log2_res-1](x)</p>
			<p class="source-code">    old_rgb = UpSampling2D((2,2))(old_rgb)</p>
			<p class="source-code">    x = self.generator_blocks[log2_res](x)</p>
			<p class="source-code">    new_rgb = self.to_rgb[log2_res](x)</p>
			<p class="source-code">    rgb = FadeIn()(alpha, new_rgb, old_rgb)</p>
			<p class="source-code">    self.generator = Model([self.generator_blocks[2].input,</p>
			<p class="source-code"> 					alpha], rgb, </p>
			<p class="source-code">                               name=f'generator_{res}_x_{res}')</p>
			<p>The growing<a id="_idIndexMarker512"/> discriminator is done similarly but in the reverse direction, as follows:</p>
			<ol>
				<li value="1">At the resolution of the input image, say, 32x32, add <strong class="source-inline">from_rgb</strong> to the discriminator block of 32x32. The output is the activation with a 16x16 feature map.</li>
				<li>Parallelly, downsample the input image to 16x16, and add <strong class="source-inline">from_rgb</strong> to the 16x16 discriminator block.</li>
				<li>Fade in the two preceding features and feed that into the next discriminator block of 8x8.</li>
				<li>Continue adding the discriminator blocks to the base of 4x4, where the output is a single prediction value.</li>
			</ol>
			<p>The following is the code to grow the discriminator:</p>
			<p class="source-code">def grow_discriminator(self, log2_res):</p>
			<p class="source-code">    res = 2**log2_res </p>
			<p class="source-code">    input_image = Input(shape=(res, res, 3))</p>
			<p class="source-code">    alpha = Input(shape=(1))</p>
			<p class="source-code">    x = self.from_rgb[log2_res](input_image)</p>
			<p class="source-code">    x = self.discriminator_blocks[log2_res](x)        </p>
			<p class="source-code">    downsized_image = AveragePooling2D((2,2))(input_image)</p>
			<p class="source-code">    y = self.from_rgb[log2_res-1](downsized_image)</p>
			<p class="source-code">    x = FadeIn()(alpha, x, y)</p>
			<p class="source-code">    for i in range (log2_res-1, 1, -1):</p>
			<p class="source-code">        x = self.discriminator_blocks[i](x)</p>
			<p class="source-code">    self.discriminator =  Model([input_image, alpha], x,</p>
			<p class="source-code">                 name=f'discriminator_{res}_x_{res}')</p>
			<p>Finally, we build a <a id="_idIndexMarker513"/>model from the grown generator and discriminator as follows:</p>
			<p class="source-code">def grow_model(self, log2_res):</p>
			<p class="source-code">    self.grow_generator(log2_res)</p>
			<p class="source-code">    self.grow_discriminator(log2_res)</p>
			<p class="source-code">    self.discriminator.trainable = False</p>
			<p class="source-code">    latent_input = Input(shape=(self.z_dim))</p>
			<p class="source-code">    alpha_input = Input(shape=(1))</p>
			<p class="source-code">    fake_image = self.generator([latent_input, alpha_input])</p>
			<p class="source-code">    pred = self.discriminator([fake_image, alpha_input])</p>
			<p class="source-code">    self.model = Model(inputs=[latent_input, alpha_input],</p>
			<p class="source-code">                      outputs=pred)</p>
			<p class="source-code">    self.model.compile(loss=wasserstein_loss, </p>
			<p class="source-code">                       optimizer=Adam(**self.opt_init))</p>
			<p class="source-code">    self.optimizer_discriminator = Adam(**self.opt_init)</p>
			<p>We reset the optimizer states after the new layer is added. This is because optimizers such as Adam have internal states that store the gradient history for each layer. The easiest way to do <a id="_idIndexMarker514"/>that is probably to instantiate a new optimizer using the same parameters. </p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor146"/>Loss function</h2>
			<p>You may have <a id="_idIndexMarker515"/>noticed the <strong class="bold">Wasserstein loss</strong> in the preceding code snippet. That's right, the generator uses Wasserstein loss, where the loss function is a multiplication between predictions and the labels. The discriminator uses the WGAN-GP gradient penalty loss. We learned about WGAN-GP in <a href="B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a><em class="italic">, Generative Adversarial Network</em>, but let's recap the loss function here. </p>
			<p>WGAN-GP interpolates between fake and real images and feeds the interpolation to the discriminator. From there, gradients are calculated with respect to the input interpolation rather the usual optimization that calculate gradients against the weights. From there, we calculate the gradient penalty (loss) and add it to the discriminator loss for backpropagation. We will reuse the WGAN-GP that we developed in <a href="B14538_03_Final_JM_ePub.xhtml#_idTextAnchor060"><em class="italic">Chapter 3</em></a><em class="italic">, Generative Adversarial Network</em>. Unlike the original WGAN-GP, which trains the discriminator five times for every generator training step, ProGAN uses equal amounts of training for both the discriminator and the generator.</p>
			<p>Apart from the WGAN-GP losses, there is <a id="_idIndexMarker516"/>an additional loss type known as <strong class="bold">drift loss</strong>. The discriminator output is unbounded and can be large positive or negative values. This drift loss aims to keep the discriminator output from drifting too far away from zero toward the infinity. The following code snippet shows how to calculate drift loss from the discriminator output:</p>
			<p class="source-code"># drift loss</p>
			<p class="source-code">all_pred = tf.concat([pred_fake, pred_real], axis=0)</p>
			<p class="source-code">drift_factor = 0.001</p>
			<p class="source-code">drift_loss = drift_factor * tf.reduce_mean(all_pred**2)</p>
			<p>Now, we can <a id="_idIndexMarker517"/>start training our ProGAN!</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor147"/>Growing pains</h2>
			<p>ProGAN is extremely <a id="_idIndexMarker518"/>slow to train. It took the authors eight Tesla V100 GPUs and 4 days to train on the 1024x1024 <strong class="source-inline">CelebA-HQ</strong> dataset. If you have access to only one GPU, it would take you more than 1 month to train! Even for the relatively low resolution of 256x256, it would take a good 2 or 3 days to train with a single GPU. Take that into consideration before starting training. You might want to start with a lower target resolution, such as 64x64.</p>
			<p>Having said that, for a start, we don't have to use a high-resolution dataset. Datasets with a 256x256 resolution are sufficient. The notebook left out the input part, so feel free to fill in the input  to load your dataset. For your information, there are two popular 1024x1024 face datasets that are freely downloadable:</p>
			<ul>
				<li>CelebA-HQ on the official ProGAN TensorFlow 1 implementation: <a href="https://github.com/tkarras/progressive_growing_of_gans">https://github.com/tkarras/progressive_growing_of_gans</a>. It requires the download of the original <strong class="source-inline">CelebA</strong> dataset plus HQ-related files. The generation scripts also rely on some dated libraries. Therefore, I don't recommend you do it this way; you should try finding a dataset that is pre-converted.</li>
				<li>FFHQ: <a href="https://github.com/NVlabs/ffhq-dataset">https://github.com/NVlabs/ffhq-dataset</a>. This dataset was created for StyleGAN (a successor of ProGAN) and is more varied and diverse than the <strong class="source-inline">CelebA-HQ</strong> dataset. It can also be difficult to download due to the download limit set by the server.</li>
			</ul>
			<p>When we download high-resolution images, we will need to downscale them to lower resolutions to be used for training. You can do that at runtime, but it can slow down the training slightly due to the additional computation to perform the downsampling, and it requires more memory bandwidth to transfer the images. Alternatively, you can create the multiscale images from the original image resolution first, which can save time in memory transfer and image resizing.</p>
			<p>The other thing to note is the batch size. As the image resolution grows, so does the GPU memory required to store the images and the larger layer activations. We will run out of GPU memory if we set the batch size too high. Therefore, we use a batch size of 16 from 4x4 to 64x64, then halve the batch size as the resolution doubles. You should adjust the batch size accordingly to fit your GPU. </p>
			<p>The following figure<a id="_idIndexMarker519"/> shows the generated images from 16x16 resolution to 64x64 resolution using our ProGAN:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B14538_07_04.jpg" alt="Figure 7.4 – Images growing from 8x8 to 64x64 as generated by our ProGAN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Images growing from 8x8 to 64x64 as generated by our ProGAN</p>
			<p>ProGAN is a very <a id="_idIndexMarker520"/>delicate model. When reproducing the models in this book, I only implemented the key parts to match the details of the original implementation. I would leave out something that I thought was not that important and swap it for something that I hadn't covered. This applied to optimizers, learning rates, normalization techniques, and loss functions. </p>
			<p>However, I found that I had to implement everything to almost the exact original specification for ProGAN in order to make it work. This includes using the same batch size, drift loss, and equalized learning rate gain. Nevertheless, when we get the network to work, it does generate high-fidelity faces that are unmatched by any models that came before it! </p>
			<p>Now let's see how StyleGAN improves on ProGAN to allow for style mixing.</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor148"/>Implementing StyleGAN</h1>
			<p>ProGAN is great <a id="_idIndexMarker521"/>at generating high-resolution images by growing the network progressively, but the network architecture is quite primitive. The simple architecture resembles earlier GANs such as DCGAN that generate images from random noise but without fine control over the images to be generated. </p>
			<p>As we have seen in previous chapters, many innovations happened in image-to-image translation to allow better manipulation of the generator outputs. One of them is the use of the AdaIN layer (<a href="B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104"><em class="italic">Chapter 5</em></a><em class="italic">, Style Transfer</em>) to allow style transfer, mixing the content and style features from two different images. <strong class="bold">StyleGAN</strong> adopts this concept of style-mixing to come out with <em class="italic">a style-based generator architecture for generative adversarial networks</em> – this is the title of the paper written for <strong class="bold">FaceBid</strong>. The following figure shows that StyleGAN can mix the style features from two different images to generate a new one:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B14538_07_05.jpg" alt="Figure 7.5 – Mixing styles to produce new images&#13;&#10;(Source: T. Karras et al, 2019 &quot;A Style-Based Generator Architecture for Generative Adversarial Networks,&quot; https://arxiv.org/abs/1812.04948)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – Mixing styles to produce new images (Source: T. Karras et al, 2019 "A Style-Based Generator Architecture for Generative Adversarial Networks," https://arxiv.org/abs/1812.04948)</p>
			<p>We will now <a id="_idIndexMarker522"/>delve into the StyleGAN generator architecture.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor149"/>Style-based generator</h2>
			<p>The<a id="_idIndexMarker523"/> following<a id="_idIndexMarker524"/> diagram compares the generator architectures of ProGAN and StyleGAN:</p>
			<div>
				<div id="_idContainer162" class="IMG---Figure">
					<img src="image/B14538_07_06.jpg" alt="Figure 7.6 – Comparing generators between (a) ProGAN and (b) StyleGAN &#13;&#10;Redrawn from T. Karras et al, 2019 &quot;A Style-Based Generator Architecture for Generative Adversarial Networks,&quot; https://arxiv.org/abs/1812.04948)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Comparing generators between (a) ProGAN and (b) StyleGAN Redrawn from T. Karras et al, 2019 "A Style-Based Generator Architecture for Generative Adversarial Networks," https://arxiv.org/abs/1812.04948)</p>
			<p>The ProGAN <a id="_idIndexMarker525"/>architecture is a simple <a id="_idIndexMarker526"/>feedforward design, where the single input is the latent code. All the latent information, for example, content, style, and randomness, are included in the single latent code <em class="italic">z</em>. On the right in the preceding figure is the StyleGAN generator architecture, where the latent code no longer goes directly into the synthesis network. The latent code is mapped to style code that goes into a multi-scale synthesis network.</p>
			<p>We will go through the generation pipeline now, which has the following the major building blocks:</p>
			<ul>
				<li><strong class="bold">Mapping network, f</strong>: This is 8 dense layers with 512 dimensions. Its input is 512-dimensional latent code, and the output <em class="italic">w</em> is also a vector of 512. <em class="italic">w</em> is broadcast to every<a id="_idIndexMarker527"/> scale of the<a id="_idIndexMarker528"/> generator.</li>
				<li><strong class="bold">Affine transform, A</strong>: In every scale, there is a block that maps <em class="italic">w</em> into styles <em class="italic">y = (y</em><span class="subscript">s</span><em class="italic">, y</em><span class="subscript">b</span><em class="italic">).</em> In other words, the global latent vector is converted to localized style code at each image scale. The affine transform is implemented using dense layers.</li>
				<li><strong class="bold">AdaIN</strong>: AdaIN modulates the style code and content code. The content code <em class="italic">x</em> is the convolutional layer's activation, while <em class="italic">y</em> is the style code:</li>
			</ul>
			<div>
				<div id="_idContainer163" class="IMG---Figure">
					<img src="image/Formula_07_003.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li><strong class="bold">Synthesis network, g</strong>: This is essentially made up of the ProGAN multiscale generator blocks. The notable difference with ProGAN is that the input to the synthesis network is just some constant values. This is because the latent code presents itself as style codes in every generator layer, including the first 4x4 block, so there is no need to have another random input to the synthesis network. </li>
				<li><strong class="bold">Multiscale noise</strong>: There are many aspects of human portraits that can be seen as stochastic (random). For example, the exact placement of hairs and freckles can be random, but this does not change our perception of an image. This randomness comes from the noise that is injected into the generator. The Gaussian noise has a shape that matches the convolution layer activation map (H, W, 1). It is scaled per channel by <em class="italic">B</em> to (H, W, C) before being added to the convolutional activation.</li>
			</ul>
			<p>In most GANs that came before StyleGAN, the latent code was injected only at input or into one of the internal layer. The brilliance of the StyleGAN generator is that we can now inject style code and noise at every layer, meaning we can tweak images at different levels. Styles at coarse spatial resolutions (from 4x4 to 8x8) correspond to high-level aspects such as poses and face shapes. Middle resolutions (from 16x16 to 32x32) are to do <a id="_idIndexMarker529"/>with smaller-scale facial <a id="_idIndexMarker530"/>features, hairstyles, and whether eyes are open or closed. Finally, higher resolutions (from 64x64 to 1024x1024) mainly change the color scheme and microstructure. </p>
			<p>The StyleGAN generator may have looked complex at first, but hopefully it doesn't look that scary now. As with ProGAN, the individual blocks are simple. We will leverage code from ProGAN heavily; now let's start to build the StyleGAN generator!</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor150"/>Implementing the mapping network</h2>
			<p>The mapping<a id="_idIndexMarker531"/> network<a id="_idIndexMarker532"/> maps the 512-dimensional latent code into 512-dimensional features as follows:</p>
			<p class="source-code">def build_mapping(self):</p>
			<p class="source-code">    # Mapping Network</p>
			<p class="source-code">    z = Input(shape=(self.z_dim))</p>
			<p class="source-code">    w = PixelNorm()(z)</p>
			<p class="source-code">    for i in range(8):</p>
			<p class="source-code">        w = Dense(512, lrmul=0.01)(w)</p>
			<p class="source-code">        w = LeakyReLU(0.2)(w)</p>
			<p class="source-code">    w = tf.tile(tf.expand_dims(w, 0), (8,1,1))            </p>
			<p class="source-code">    self.mapping = Model(z, w, name='mapping')</p>
			<p>It is a straightforward implementation of dense layers with leaky ReLU activation. One thing to note is that the learning rate is multiplied by 0.01 to make it more stable to train. Therefore, the custom <strong class="source-inline">Dense</strong> layer is modified to take in an additional <strong class="source-inline">lrmul</strong> argument. At <a id="_idIndexMarker533"/>the end of the<a id="_idIndexMarker534"/> network, we create eight copies of <strong class="source-inline">w</strong>, which will go into eight layers of generator blocks. We could skip the tiling if we don't intend to use style mixing.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor151"/>Adding noise</h2>
			<p>We now create <a id="_idIndexMarker535"/>a custom layer to add noise to the convolution layer output, which includes the <em class="italic">B</em> block in the architectural diagram. The code is as follows:</p>
			<p class="source-code">class AddNoise(Layer):</p>
			<p class="source-code">    def build(self, input_shape):</p>
			<p class="source-code">        n, h, w, c = input_shape[0]</p>
			<p class="source-code">        initializer = \ 		tf.keras.initializers.RandomNormal( 						mean=0., stddev=1.)</p>
			<p class="source-code">        self.B = self.add_weight(shape=[1, 1, 1, c],</p>
			<p class="source-code">                                initializer=initializer,</p>
			<p class="source-code">                                trainable=True, </p>
			<p class="source-code"> 						name='kernel')</p>
			<p class="source-code">    def call(self, inputs):</p>
			<p class="source-code">        x, noise = inputs</p>
			<p class="source-code">        output = x + self.B * noise</p>
			<p class="source-code">        return output</p>
			<p>The noise is multiplied with the learnable <strong class="source-inline">B</strong> to scale it per channel, then it is added to the input <a id="_idIndexMarker536"/>activation.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor152"/>Implementing AdaIN</h2>
			<p>The AdaIN that <a id="_idIndexMarker537"/>we will implement for <a id="_idIndexMarker538"/>StyleGAN is different from the one for style transfer for the following reasons:</p>
			<ul>
				<li>We will include affine transformation <em class="italic">A</em>. This is implemented with two dense layers to predict <em class="italic">y</em><em class="italic">s</em> and <em class="italic">yb</em>, respectively. </li>
				<li>Original AdaIN involves the normalization of the input activation, but since the input activation to our AdaIN has undergone pixel normalization, we will not perform normalization within this custom layer. The code for the AdaIN layer is as follows:</li>
			</ul>
			<p class="source-code">class AdaIN(Layer):</p>
			<p class="source-code">    def __init__(self, gain=1, **kwargs):</p>
			<p class="source-code">        super(AdaIN, self).__init__(kwargs)</p>
			<p class="source-code">        self.gain = gain</p>
			<p class="source-code">    def build(self, input_shapes):</p>
			<p class="source-code">        x_shape = input_shapes[0]</p>
			<p class="source-code">        w_shape = input_shapes[1]</p>
			<p class="source-code">        self.w_channels = w_shape[-1]</p>
			<p class="source-code">        self.x_channels = x_shape[-1]</p>
			<p class="source-code">        self.dense_1 = Dense(self.x_channels, gain=1)</p>
			<p class="source-code">        self.dense_2 = Dense(self.x_channels, gain=1)</p>
			<p class="source-code">    def call(self, inputs):</p>
			<p class="source-code">        x, w = inputs</p>
			<p class="source-code">        ys = tf.reshape(self.dense_1(w), (-1, 1, 1,</p>
			<p class="source-code"> 					self.x_channels))</p>
			<p class="source-code">        yb = tf.reshape(self.dense_2(w), (-1, 1, 1, </p>
			<p class="source-code"> 					self.x_channels))</p>
			<p class="source-code">        output = ys*x + yb</p>
			<p class="source-code">        return output</p>
			<p class="callout-heading">Comparing AdaIN with style transfer</p>
			<p class="callout">The AdaIN in ProGAN is different from the original implementation for style transfer. In style transfer, the <a id="_idIndexMarker539"/>style feature is Gram <a id="_idIndexMarker540"/>matrix calculated from VGG features of an input image. In ProGAN, the 'style' is vector <em class="italic">w</em> generated from random noise.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor153"/>Building the generator block</h2>
			<p>Now, we can <a id="_idIndexMarker541"/>put <strong class="source-inline">AddNoise</strong> and <strong class="source-inline">AdaIN</strong> into<a id="_idIndexMarker542"/> the generator block, which looks similar to ProGAN's code to build generator block, as follows:</p>
			<p class="source-code">def build_generator_block(self, log2_res, input_shape):</p>
			<p class="source-code">    res = int(2**log2_res)</p>
			<p class="source-code">    res_name = f'{res}x{res}'</p>
			<p class="source-code">    filter_n = self.log2_res_to_filter_size[log2_res]</p>
			<p class="source-code">    input_tensor = Input(shape=input_shape)</p>
			<p class="source-code">    x = input_tensor</p>
			<p class="source-code">    w = Input(shape=512)</p>
			<p class="source-code">    noise = Input(shape=(res, res, 1))</p>
			<p class="source-code">    if log2_res &gt; 2:</p>
			<p class="source-code">        x = UpSampling2D((2,2))(x)</p>
			<p class="source-code">        x = Conv2D(filter_n, 3,  			   name=f'gen_{res_name}_conv1')(x)</p>
			<p class="source-code">    <strong class="bold">x = AddNoise()([x, noise])</strong></p>
			<p class="source-code">    x = PixelNorm()(LeakyReLU(0.2)(x))</p>
			<p class="source-code">    <strong class="bold">x = AdaIN()([x, w])</strong></p>
			<p class="source-code">    # ADD NOISE</p>
			<p class="source-code">    x = Conv2D(filter_n, 3, </p>
			<p class="source-code"> 			name=f'gen_{res_name}_conv2')(x)</p>
			<p class="source-code">    <strong class="bold">x = AddNoise()([x, noise])  </strong>  </p>
			<p class="source-code">    x = PixelNorm()(LeakyReLU(0.2)(x))</p>
			<p class="source-code">    <strong class="bold">x = AdaIN()([x, w])</strong></p>
			<p class="source-code">    return Model([input_tensor, x, noise], x, </p>
			<p class="source-code">                 name=f'genblock_{res}_x_{res}')</p>
			<p>The generator<a id="_idIndexMarker543"/> block takes three<a id="_idIndexMarker544"/> inputs. For a 4x4 generator block, the input is a constant tensor of 1 and we bypass the upsampling and convolutional blocks. The other two inputs are the vector <em class="italic">w</em> and random noise.  </p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor154"/>Training StyleGAN</h2>
			<p>As mentioned at <a id="_idIndexMarker545"/>the beginning of the section, the main changes from ProGAN to StyleGAN are to the generator. There are some minor differences in the discriminator and training details, but they don't affect performance as much. Therefore, we will keep the rest of the pipeline the same as ProGAN. </p>
			<p>The following figure shows 256x256 images generated by our StyleGAN. The same style <em class="italic">w</em> is used but with different randomly generated noise:</p>
			<div>
				<div id="_idContainer164" class="IMG---Figure">
					<img src="image/B14538_07_07.jpg" alt="Figure 7.7 – Portraits generated using the same style but different noise&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Portraits generated using the same style but different noise</p>
			<p>We can see that the faces belong to the same person but with varying details, such as length of hair and head pose. We can also mix styles by using <em class="italic">w</em> from different latent code as shown in the following figure:</p>
			<div>
				<div id="_idContainer165" class="IMG---Figure">
					<img src="image/B14538_07_08.jpg" alt="Figure 7.8 – All images are generated by our StyleGAN. The face on the right was created by mixing styles from the first two faces"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – All images are generated by our StyleGAN. The face on the right was created by mixing styles from the first two faces</p>
			<p>Knowing that StyleGAN can be difficult to train, therefore I have provided a pretrained 256x256 model that you can download. You can use the widget in the Jupyter notebook to experiment with the face generation and style mixing.</p>
			<p>This concludes our journey with StyleGAN.</p>
			<h1 id="_idParaDest-146"><a id="_idTextAnchor155"/>Summary</h1>
			<p>In this chapter, we entered the realm of high-definition image generation with ProGAN. ProGAN first trains on low-resolution images before moving on to higher-resolution images. The network training becomes more stable by growing the network progressively. This lays the foundation for high-fidelity image generation, as this coarse-to-fine training method is adopted by other GANs. For example, pix2pixHD has two generators at two different scales, where the coarse generator is pre-trained before both are trained together. We have also learned about equalized learning rates, minibatch statistics, and pixel normalization, which are also used in StyleGAN.</p>
			<p>With the use of the AdaIN layer from style transfer in the generator, not only does StyleGAN produce better-quality images, but this also allows control of features when mixing styles. By injecting different style code and noise at different scales, we can control both the global and fine details of an image. StyleGAN achieved state-of-the-art results in high-definition image generation and remains the state of the art at the time of writing. The style-based model is now the mainstream architecture. We have seen the use of this model in style transfer, image-to-image translation, and StyleGAN. </p>
			<p>In the next chapter, we will look at another popular family of GANs, which are known as attention-based models. </p>
		</div>
	</body></html>