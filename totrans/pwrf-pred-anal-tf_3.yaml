- en: Chapter 3. Clustering Your Data – Unsupervised Learning for Predictive Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this lesson, we will dig deeper into predictive analytics and find out how
    we can take advantage of it to cluster records belonging to a certain group or
    class for a dataset of unsupervised observations. We will provide some practical
    examples of unsupervised learning; in particular, clustering techniques using
    TensorFlow will be discussed with some hands-on examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this lesson:'
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised learning and clustering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using K-means for predicting neighborhood
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using K-means for clustering audio files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using unsupervised **k-nearest neighborhood** (**kNN**) for predicting nearest
    neighbors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unsupervised Learning and Clustering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will provide a brief introduction to the unsupervised **machine
    learning** (**ML**) technique. Unsupervised learning is a type of ML algorithm
    used for grouping related data objects and finding hidden patterns by inferencing
    from unlabeled datasets, that is, a training set consisting of input data without
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see a real-life example. Suppose you have a large collection of not-pirated-totally-legal
    MP3s in a crowded and massive folder on your hard drive. Now, what if you can
    build a predictive model that helps automatically group together similar songs
    and organize them into your favorite categories such as country, rap, and rock?
  prefs: []
  type: TYPE_NORMAL
- en: This is an act of assigning an item to a group so that an MP3 is added to the
    respective playlist in an unsupervised way. In [Lesson 1](ch01.html "Chapter 1. From
    Data to Decisions – Getting Started with TensorFlow"), *From Data to Decisions
    – Getting Started with TensorFlow, on classification*, we assumed that you're
    given a training dataset of correctly labeled data. Unfortunately, we don't always
    have that extravagance when we collect data in the real world. For example, suppose
    we would like to divide a huge collection of music into interesting playlists.
    How canwe possibly group together songs if we don't have direct access to their
    metadata? One possible approach is a mixture of various ML techniques, but clustering
    is often at the heart of the solution.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, the main objective of the unsupervised learning algorithms is
    to explore the unknown/hidden patterns in the input data that are unlabeled. Unsupervised
    learning, however, also comprehends other techniques to explain the key features
    of the data in an exploratory way toward finding the hidden patterns. To overcome
    this challenge, clustering techniques are used widely to group unlabeled data
    points based on certain similarity measures in an unsupervised way.
  prefs: []
  type: TYPE_NORMAL
- en: In a clustering task, an algorithm groups related features into categories by
    analyzing similarities between input examples, where similar features are clustered
    and marked using circles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clustering uses include but are not limited to the following points:'
  prefs: []
  type: TYPE_NORMAL
- en: Anomaly detection for suspicious pattern finding in an unsupervised way
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Text categorization for finding useful patterns in the tests for NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Social network analysis for finding coherent groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data center computing clusters for finding a way of putting related computers
    together
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real estate data analysis for identifying neighborhoods based on similar features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clustering analysis is about dividing data samples or data points and putting
    them into corresponding homogeneous classes or clusters. Thus, a trivial definition
    of clustering can be thought of as the process of organizing objects into groups
    whose members are similar in some way, as shown in figure 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised Learning and Clustering](img/03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A typical data pipelines for clustering raw data'
  prefs: []
  type: TYPE_NORMAL
- en: A cluster is, therefore, a collection of objects that have a similarity between
    them and are dissimilar to the objects belonging to other clusters. If a collection
    of objects is provided, clustering algorithms put these objects into groups based
    on similarity. For example, a clustering algorithm such as K-means locates the
    centroid of the groups of data points.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, to make clustering accurate and effective, the algorithm evaluates
    the distance between each point from the centroid of the cluster. Eventually,
    the goal of clustering is to determine intrinsic grouping in a set of unlabeled
    data. For example, the K-means algorithm tries to cluster related data points
    within the predefined *3* (that is *k = 3*) clusters, as shown in figure 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Unsupervised Learning and Clustering](img/03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The results of a typical clustering algorithm and a representation
    of the cluster centers'
  prefs: []
  type: TYPE_NORMAL
- en: Clustering is a process of intelligently categorizing items in your dataset.
    The overall idea is that two items in the same cluster are closer to each other
    than items that belong to separate clusters. This is a general definition, leaving
    the interpretation of closeness open. For example, perhaps cheetahs and leopards
    belong to the same cluster, whereas elephants belong to another when closeness
    is measured by the similarity of two species in the hierarchy of biological classification
    (family, genus, and species).
  prefs: []
  type: TYPE_NORMAL
- en: Using K-means for Predictive Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K-means is a clustering algorithm that tries to cluster related data points
    together. However, we should know its working principle and mathematical operations.
  prefs: []
  type: TYPE_NORMAL
- en: How K-means Works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Suppose we have *n* data points, *xi*, *i = 1...n*, that need to be partitioned
    into *k* clusters. Now that the target here is to assign a cluster to each data
    point, K-means aims to find the positions, *μi*, *i=1...k*, of the clusters that
    minimize the distance from the data points to the cluster. Mathematically, the
    K-means algorithm tries to achieve the goal by solving an equation that is an
    optimization problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How K-means Works](img/03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: In the previous equation, **ci** is a set of data points, which when assigned
    to cluster **i** and![How K-means Works](img/03_21.jpg)is the Euclidean distance
    to be calculated (we will explain why we should use this distance measurement
    shortly). Therefore, we can see that the overall clustering operation using K-means
    is not a trivial one, but a NP-hard optimization problem. This also means that
    the K-means algorithm not only tries to find the global minima but often gets
    stuck in different solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering using the K-means algorithm begins by initializing all the coordinates
    to the centroids. With every pass of the algorithm, each point is assigned to
    its nearest centroid based on some distance metric, usually the Euclidean distance
    stated earlier.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Distance calculation**: There are other ways to calculate the distance as
    well. For example, the Chebyshev distance can be used to measure the distance
    by considering only the most notable dimensions. The Hamming distance algorithm
    can identify the difference between two strings. Mahalanobis distance can be used
    to normalize the covariance matrix. The Manhattan distance is used to measure
    the distance by considering only axis-aligned directions. The Haversine distance
    is used to measure the great-circle distances between two points on a sphere from
    the location.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering these distance-measuring algorithms, it is clear that the Euclidean
    distance algorithm would be the most appropriate to solve our purpose of distance
    calculation in the K-means algorithm. The centroids are then updated to be the
    centers of all the points assigned to it in that iteration. This repeats until
    there is a minimal change in the centers. In short, the K-means algorithm is an
    iterative algorithm and works in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cluster assignment step**: K-means goes through each of the *n* data points
    in the dataset that is assigned to a cluster closest to the k centroids, then
    the least distant one is picked.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Update step**: For each cluster, a new centroid is calculated for all the
    data points in the cluster. The overall workflow of K-means can be explained using
    a flowchart, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![How K-means Works](img/03_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Flowchart of the K-means algorithm (Elbow method is an optional but
    also an advanced option)'
  prefs: []
  type: TYPE_NORMAL
- en: Using K-means for Predicting Neighborhoods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, to show an example of clustering using K-means, we will use the Saratoga
    NY Homes dataset downloaded from [http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html](http://course1.winona.edu/bdeppa/Stat%20425/Datasets.html)
    as an unsupervised learning technique. The dataset contains several features of
    the houses located in the suburb of the New York City; for example, price, lot
    size, waterfront, age, land value, new construct, central air, fuel type, heat
    type, sewer type, living area, Pct.College, bedrooms, fireplaces, bathrooms, and
    the number of rooms. However, only a few features have been shown in **Table 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using K-means for Predicting Neighborhoods](img/03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 1: A sample data from the Saratoga NY Homes dataset'
  prefs: []
  type: TYPE_NORMAL
- en: 'The target of this clustering technique is to show an exploratory analysis
    based on the features of each house in the city for finding possible neighborhoods''
    of the house located in the same area. Before performing the feature extraction,
    we need to load and parse the Saratoga NY Homes dataset. However, we will look
    at this example with step-by-step source codes for better understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading required libraries and packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need some built-in Python libraries, such as os, random, NumPy, and Pandas,
    for data manipulation; PCA for dimensionality reduction; Matplotlib for plotting;
    and of course, TensorFlow:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Loading, parsing, and preparing a training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, the first line is used to ensure the reproducibility of the result. The
    second line basically reads the dataset from your location and converts it into
    the Pandas data frame:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If you now print the data frame (using print(train)), you should find the dataframe
    containing headers and data as shown in figure 3:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using K-means for Predicting Neighborhoods](img/03_06.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 5: A snapshot of the Saratoga NY Homes dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Well, we have managed to prepare the dataset. Now, the next task is to conceptualize
    our K-means and write a function/class for it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Implementing K-means.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the source code of K-means, which is simple in a TensorFlow
    way:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The previous code contains all the steps required to develop the K-means model,
    including the distance-based centroid calculation, centroid update, and training
    parameters required.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clustering the houses.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now the previous function can be invoked with real values, for example, our
    housing dataset. Since there are many houses with their respective features, it
    would be difficult to plot the clusters along with all the properties. This is
    the **Principal Component Analysis** (**PCA**) that we discussed in the previous
    lessons:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Well, now we are all set. It would be even better to visualize the clusters
    as shown in figure 6\. For this, we will use mpl_toolkits.mplot3d for 3D projection,
    as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Using K-means for Predicting Neighborhoods](img/03_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 6: Clustering the houses with similar properties, for example, price'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we can see that most houses fall in **0** to **100,000** range. The second
    highest houses fall in the range of **100000** to **200000**. However, it's really
    difficult to separate them. Moreover, the number of predefined clusters that we
    used is 10, which might not be the most optimal one. Therefore, we need to tune
    this parameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Fine tuning and finding the optimal number of clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choosing the right number of clusters often depends on the task. For example,
    suppose you're planning an event for hundreds of people, both young and old. If
    you have a budget for only two entertainment options, then you can use the K-means
    clustering with **k = 2** to separate the guests into two age groups. Other times,
    it's not as obvious what the value of `k` should be. Automatically figuring out
    the value of `k` is a bit more complicated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As mentioned earlier, the K-means algorithm tries to minimize the sum of squares
    of the distance (that is, Euclidean distance), in terms of **Within-Cluster Sum
    of Squares** (**WCSS**).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: However, if you want to minimize the sum of squares of the distance between
    the points of each set manually or automatically, you would end up with a model
    where each cluster is its own cluster center; in this case, this measure would
    be 0, but it would hardly be a generic enough model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Therefore, once you have trained your model by specifying the parameters, you
    can evaluate the result using WCSS. Technically, it is same as the sum of distances
    of each observation in each K cluster. The beauty of clustering algorithms as
    a K-means algorithm is that it does the clustering on the data with an unlimited
    number of features. It is a great tool to use when you have raw data and would
    like to know the patterns in that data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'However, deciding the number of clusters prior to conducting the experiment
    might not be successful but sometimes may lead to an overfitting problem or an
    under-fitting one. Also, informally, determining the number of clusters is a separate
    but an optimization problem to be solved. So, based on this, we can redesign our
    K-means considering the WCSS value computation, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To fine tune the clustering performance, we can use a heuristic approach called
    Elbow method. We start from **K = 2**. Then, we run the K-means algorithm by increasing
    `K` and observe the value of the cost function (CF) using WCSS. At some point,
    we should experience a big drop with respect to CF. Nevertheless, the improvement
    then becomes marginal with an increasing value of `K`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In summary, we can pick the `K` after the last big drop of WCSS as the optimal
    one. The K-means includes various parameters such as withiness and betweenness,
    analyzing which you can find out the performance of K-means:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Betweenness**: This is the between sum of squares, also called the intra-cluster
    similarity'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Withiness**: This is the within sum of squares, also called the inter-cluster
    similarity'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Totwithiness**: This is the sum of all the withiness of all the clusters,
    also called the total intra-cluster similarity'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that a robust and accurate clustering model will have a lower value of
    withiness and a higher value of betweenness. However, these values depend on the
    number of clusters that is `K`, which is chosen before building the model. Now,
    based on this, we will train the K-means model for different `K` values that are
    a number of predefined clusters. We will start **K = 2** to `10`, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s discuss how we can take the advantage of the Elbow method for determining
    the number of clusters. We calculated the cost function WCSS as a function of
    a number of clusters for the K-means algorithm applied to home data based on all
    the features, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Using K-means for Predicting Neighborhoods](img/03_08.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 7: Number of clusters as a function of WCSS'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will try to reuse this lesson in upcoming examples using K-means too. Now,
    it can be observed that a big drop occurs when **k = 5**. Therefore, we chose
    the number of clusters to be **5** as discussed in figure 7\. Basically, this
    is the one after the last big drop. This means that the optimal number of cluster
    for our dataset that we need to set before we start training the K-means model
    is **5**.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Clustering analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From figure 8, it is clear that most houses fall in **cluster 3** (**4655 houses**)
    and then in **cluster 4** (**3356 houses**). The x-axis shows the price and the
    y-axis shows the lot size for each house. We can also observe that the **cluster
    1** has only a few houses and potentially in longer distances, but it is also
    expensive. So, it is most likely that you will not find a nearer neighborhood
    to interact with if you buy a house that falls in this cluster. However, if you
    like more human interaction and budget is a constraint, you should probably try
    buying a house from cluster 2, 3, 4, or 5:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using K-means for Predicting Neighborhoods](img/03_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 8: Clusters of neighborhoods, that is,. homogeneous houses fall in same
    clusters'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To make the analysis, we dumped the output in RStudio and generated the clusters
    shown in figure 6\. The R script can be found on my GitHub repositories at [https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics](https://github.com/rezacsedu/ScalaAndSparkForBigDataAnalytics).
    Alternatively, you can write your own script and do the visualization accordingly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Predictive Models for Clustering Audio Files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For clustering music with audio data, the data points are the feature vectors
    from the audio files. If two points are close together, it means that their audio
    features are similar. We want to discover which audio files belong to the same
    neighborhood because these clusters will probably be a good way to organize your
    music files:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading audio files with TensorFlow and Python.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some common input types in ML algorithms are audio and image files. This shouldn't
    come as a surprise because sound recordings and photographs are raw, redundant,
    ab nd often noisy representations of semantic concepts. ML is a tool to help handle
    these complications. These data files have various implementations, for example,
    an audio file can be an MP3 or WAV.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Reading files from a disk isn't exactly a ML-specific ability. You can use a
    variety of Python libraries to load files onto the memory, such as Numpy or Scipy.
    Some developers like to treat the data preprocessing step separately from the
    ML step. However, I believe that this is also a part of the whole analytics process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since this is a TensorFlow book, I will try to use something from the TensorFlow
    built-in operator to list files in a directory called `tf.train.match_filenames_once()`.
    We can then pass this information along to a `tf.train.string_input_producer()queue`
    operator. This way, we can access a filename one at a time, without loading everything
    at once. Here''s the structure of this method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This method takes two parameters: `pattern` and `name`. `pattern` signifies
    a file pattern or 1D tensor of file patterns. The `name` is used to signify the
    name of the operations. However, this parameter is optional. Once invoked, this
    method saves the list of matching patterns, so as the name implies, it is only
    computed once.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, a variable that is initialized to the list of files matching the pattern(s)
    is returned by this method. Once we have finished reading the metadata and the
    audio files, we can decode the file to retrieve usable data from the given filename.
    Now, let''s get started. First, we need to import necessary packages and Python
    modules, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we can start reading the audio files from the directory specified. First,
    we need to store filenames that match a pattern containing a particular file extension,
    for example, `.mp3`, `.wav`, and so on. Then, we need to set up a pipeline for
    retrieving filenames randomly. Now, the code natively reads a file in TensorFlow.
    Then, we run the reader to extract the file data. Use can use following code for
    this task:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Well, once we have read the data and metadata about all the audio files, the
    next and immediate tasks are to capture the audio features that will be used by
    K-means for the clustering purpose.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Extracting features and preparing feature vectors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: ML algorithms are typically designed to use feature vectors as input; however,
    sound files are a very different format. We need a way to extract features from
    sound files to create feature vectors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It helps to understand how these files are represented. If you've ever seen
    a vinyl record, you've probably noticed the representation of audio as grooves
    indented in the disk. Our ears interpret audio from a series of vibrations through
    the air. By recording the vibration properties, our algorithm can store sound
    in a data format. The real world is continuous but computers store data in discrete
    values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The sound is digitalized into a discrete representation through an **Analog
    to Digital Converter** (**ADC**). You can think about sound as a fluctuation of
    a wave over time. However, this data is too noisy and difficult to comprehend.
    An equivalent way to represent a wave is by examining the frequencies that make
    it up at each time interval. This perspective is called the frequency domain.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: It's easy to convert between time domain and frequency domain using a mathematical
    operation called a discrete Fourier transform (commonly known as the Fast Fourier
    transform). We will use this technique to extract a feature vector out of our
    sound.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A sound may produce 12 kinds of pitch. In music terminology, the 12 pitches
    are C, C#, D, D#, E, F, F#, G, G#, A, A#, and B. Figure 9 shows how to retrieve
    the contribution of each pitch in a 0.1-second interval, resulting in a matrix
    with 12 rows. The number of columns grows as the length of the audio file increases.
    Specifically, there will be **10*t** columns for a **t** second audio.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This matrix is also called a chromogram of the audio. But first, we need to
    have a placeholder for TensorFlow to hold the chromogram of the audio and the
    maximum frequency:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The next task that we can perform is that we can write a method that can extract
    these chromograms for the audio files. It can look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The workflow of the previous code is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: First, pass in the filename and use these parameters to describe 12 pitches
    every 0.1 seconds.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, represent the values of a 12-dimensional vector 10 times a second.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The chromogram output that we extract using the previous method will be a matrix,
    as visualized in figure 10\. A sound clip can be read as a chromogram, and a chromogram
    is a recipe for generating a sound clip. Now, we have a way to convert audio and
    matrices. As you have learned, most ML algorithms accept feature vectors as a
    valid form of data. That being said, the first ML algorithm we''ll look at is
    K-means clustering:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Predictive Models for Clustering Audio Files](img/03_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 9: The visualization of the chromogram matrix where the x-axis represents
    time and the y-axis represents pitch class. The green markings indicate a presence
    of that pitch at that time'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To run the ML algorithms on our chromogram, we first need to decide how we''re
    going to represent a feature vector. One idea is to simplify the audio by only
    looking at the most significant pitch class per time interval, as shown in figure
    10:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Predictive Models for Clustering Audio Files](img/03_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 10: The most influential pitch at every time interval is highlighted.
    You can think of it as the loudest pitch at each time interval'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will count the number of times each pitch shows up in the audio file.
    Figure 11 shows this data as a histogram, forming a 12-dimensional vector. If
    we normalize the vector so that all the counts add up to **1**, then we can easily
    compare audio of different lengths:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Predictive Models for Clustering Audio Files](img/03_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 11: We count the frequency of the loudest pitches heard at each interval
    to generate this histogram, which acts as our feature vector'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have the chromagram, we need to use it to extract the audio feature
    to construct a feature vector. You can use the following method for this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The workflow of the previous code is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create an operation to identify the pitch with the biggest contribution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, convert the chromogram into a feature vector.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: After this, we will construct a matrix where each row is a data item.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, if you can hear the audio clip, you can imagine and differentiate between
    the different audio files. However, this is just intuition.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Therefore, we cannot rely on this, but we should inspect them visually. So,
    we will invoke the previous method to extract the feature vector from each audio
    file and plot the feature. The whole operation should look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous code should plot the audio features of each audio file in the
    histogram as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Predictive Models for Clustering Audio Files](img/03_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 12: The ride audio files show a similar histogram'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can see some examples of audio files that we are trying to cluster based
    on their audio features. As you can see, the two on the right appear to have similar
    histograms. The two on the left also have similar sound spectrums:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Predictive Models for Clustering Audio Files](img/03_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 13: The crash cymbal audio files show a similar histogram'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, the target is to develop K-means so that it is able to group these sounds
    together accurately. We will look at the high-level view of the cough audio files,
    as shown in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Predictive Models for Clustering Audio Files](img/03_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 14: The cough audio files show a similar histogram'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we have the scream audio files that have a similar histogram and audio
    spectrum, but of course are different compared to others:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Predictive Models for Clustering Audio Files](img/03_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 15: The scream audio files show a similar histogram and audio spectrum'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we can imagine our problem. We have the features ready for training the
    K-means model. Let's start doing it.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Training K-means model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that the feature vector is ready, it's time to feed this to the K-means
    model for clustering the feature presented in figure 10\. The idea is that the
    midpoint of all the points in a cluster is called a centroid.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Depending on the audio features we choose to extract, a centroid can capture
    concepts such as loud sound, high-pitched sound, or saxophone-like sound. Therefore,
    it''s important to note that the K-means algorithm assigns non-descript labels,
    such as cluster 1, cluster 2, or cluster 3\. First, we can write a method that
    computes the initial cluster centroids as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the next task is to randomly assign the cluster number to each data point
    based on the initial cluster assignment. This time we can use another method:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous method computes the minimum distance and WCSS for the clustering
    evaluation in the later steps. Then, we need to update the centroid to check and
    make sure if there are any changes that occur in the cluster assignment:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have defined many variables, it''s time to initialize them using
    `local_variable_initializer()`, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can perform the training. Forthis, the `audioClusterin()` method
    takes the number of tentative clusters k and iterates the training up to the maximum
    iteration, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The previous method returns the cluster cost, WCSS, and also prints the cluster
    number against each audio file. So, we have been able to finish the training step.
    Now, the next task is to evaluate the K-means clustering quality.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Evaluating the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we will evaluate the clustering quality from two perspectives. First,
    we will observe the predicted cluster number. Secondly, we will also try to find
    the optimal value of `k` as a function of WCSS. So, we will iterate the training
    for **K = 2** to say **10** and observe the clustering result. However, first,
    let''s create two empty lists to hold the values of `K` and WCSS in each step:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s iterate the training using the `for` loop as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'These values signify that each audio file is clustered and the cluster number
    has been assigned (the first bracket is the cluster number, the contents in the
    second bracket is the filename). However, it is difficult to judge the accuracy
    from this output. One naïve approach would be to compare each file with figure
    12 to figure 15\. Alternatively, let''s adopt a better approach that we used in
    the first example that is the elbow method. For this, I have created a dictionary
    using two lists that are `k_list` and `wcss_list` computed previously, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The previous code produces the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the previous output, you can see a sharp drop in WCSS for **k = 4**, and
    this is generated in the third iteration. So, based on this minimum evaluation,
    we can take a decision about the following clustering assignment:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have seen two complete examples of using K-means, there is another
    example called kNN. This is typically a supervised ML algorithm. In the next section,
    we will see how we can train this algorithm in an unsupervised way for a regression
    task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using kNN for Predictive Analytics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: kNN is non-parametric and instance-based and is used in supervised learning.
    It is a robust and versatile classifier, frequently used as a benchmark for complex
    classifiers such as **Neural Networks** (**NNs**) and **Support Vector Machines**
    (**SVMs**). kNN is commonly used in economic forecasting, data compression, and
    genetics based on their expression profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Working Principles of kNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of kNN is that from a set of features **x** we try to predict the
    labels **y**. Thus, kNN falls in a supervised learning family of algorithms. Informally,
    this means that we are given a labeled dataset consisting of training observations
    (*x*, *y*). Now, the task is to model the relationship between *x* and *y* so
    that the function *f: X→Y* learns from the unseen observation *x*. The function
    *f(x)* can confidently predict the corresponding label y prediction on a point
    *z* by looking at a set of nearest neighbors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the actual method of prediction depends on whether or not we are doing
    regression (continuous) or classification (discrete). For discrete classification
    targets, the prediction may be given by a maximum voting scheme weighted by the
    distance to the prediction point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Working Principles of kNN](img/03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Here, our prediction, *f(z)*, is the maximum weighted value of all classes,
    *j*, where the weighted distance from the prediction point to the training point,
    *i*, is given by *φ (dij)*, where *d* indicates the distance between two points.
    On the other hand, *Iij* is just an indicator function if point *i* is in class
    *j*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For continuous regression targets, the prediction is given by a weighted average
    of all *k* points nearest to the prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Working Principles of kNN](img/03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'From the previous two equations, it is clear that the prediction is heavily
    dependent on the choice of the distance metric, *d*. There are many different
    specifications of distance metrics such as L1 and L2 metrics can be used for the
    textual distances:'
  prefs: []
  type: TYPE_NORMAL
- en: A straightforward way to weigh the distances is by the distance itself. Points
    that are further away from our prediction should have less impact than the nearer
    points. The most common way to weigh is the normalized inverse of the distance.
    We will implement this method in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a kNN-Based Predictive Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To illustrate how making predictions with the nearest neighbors works in TensorFlow,
    we will use the 1970s Boston housing dataset, which is available through the UCI
    ML repository at [https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data).
    The following table shows the basic description of the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Implementing a kNN-Based Predictive Model](img/03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we will predict the median neighborhood housing value that is the last
    value named MEDV as a function of several features. Since we consider the training
    set the trained model, we will find kNNs to the prediction points and do a weighted
    average of the target value. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Loading required libraries and packages.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As an entry point, we import necessary libraries and packages that will be
    needed to do predictive analytics using kNN with TensorFlow:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Resetting the default graph and disabling the TensorFlow warning.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We need to reset the default TensorFlow graph using the `reset_default_graph()`
    function from TensorFlow. You must also disable all warnings due to the absence
    of GPU on your device:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Loading and preprocessing the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we will load and parse the dataset using the `get()` function from the
    requests package as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: For more information on how the previous code works, please see the documentation
    of requests package at [http://docs.python-requests.org/en/master/user/quickstart/](http://docs.python-requests.org/en/master/user/quickstart/).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we will separate features (predictor) from the labels:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, to get some idea of the features and labels, let''s print them as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'So, the labels are okay to work with, and these are also continuous values.
    Now, let''s see the features:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Well, if you see these values, they are pretty unscaled to be fed to a predictive
    model. Thus, we need to apply the min-max scaling to get a better structure of
    the features so that an estimator scales and translates each feature individually,
    and it ensures that it is in the given range on the training set, that is, between
    zero and one. Since features are most important in predictive analytics, we should
    take special care of them. The following line of code does the min-max scaling:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let''s print them again to check to make sure what''s changed:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Preparing the training and test set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since our features are already scaled, now it''s time to split the data into
    train and test sets. Now, we split the x and y values into the train and test
    sets. We will create the training set by selecting about 75% of the rows at random
    and leave the remaining 25% for the test set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Preparing the placeholders for the tensors.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First, we will declare the batch size. Ideally, the batch size should be equal
    to the size of features in the test set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to declare the placeholders for the TensorFlow tensors, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Defining the distance metrics.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For this example, we are going to use the L1 distance. The reason is that using
    L2 did not give a better result in my case:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Implementing kNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, it''s time to implement kNN. This will predict the nearest neighbors by
    getting the minimum distance index. The `kNN()` method does the trick. There are
    several steps for doing this, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Get the minimum distance index.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the prediction function. To do this, we will use the `top_k()`, function,
    which returns the values and indices of the largest values in a tensor. Since
    we want the indices of the smallest distances, we will instead find the k-biggest
    negative distances. Since we are predicting continuous values that is regression
    task, we also declare the predictions and **Mean Squared Error** (**MSE**) of
    the target values.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the number of loops over training data.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Initialize the global variables.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate the training over the number of loops calculated in step 3.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, here''s the function of kNN. It takes the number of initial neighbors
    and starts the computation. Note that although it is a widely used convention,
    here I will make it a variable to do some tuning, as shown in the following code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Evaluating the classification/regression.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that this function does not return the optimal `mse` value, that is, the
    lowest `mse` value, but varies over different `k` values, so this is a hyperparameter
    to be tuned. One potential technique would be to iterate the method for *k = 2*
    to, say, `11` and keeping track of the optimal `k` value that forces `kNN()` to
    produce the lowest `mse` value. First, we define a method that iterates several
    times from `2` to `11` and returns two separate lists for `mse` and `k` respectively:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, it''s time to invoke the previous method and find the optimal `k` value
    for which the kNN produces the lowest `mse` value. Upon receiving the two lists,
    we create a dictionary and use the `min()`method to return the optimal `k` value,
    as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s print `the Optimal k value` for which we get the lowest `mse` value:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Running the best kNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now we have the optimal `k`, so we will entertain calculating the nearest neighbor.
    This time we will try to return the matrices for the predicted and actual labels:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'predictions = sess.run(prediction, feed_dict={x_data_train: x_vals_train, x_data_test:
    x_batch, y_target_train: y_vals_train, y_target_test: y_batch})'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Evaluating the best kNN.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will invoke the `bestKNN()` method with the optimal value of `k` that
    was calculated in the previous step, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, I would like to measure the prediction accuracy. Are you wondering why?
    I know the reason. You''re right. There is no significant reason for calculating
    the accuracy or precision since we are predicting the continuous values that is
    labels. Even so, I would like to show you whether it works or not:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The previous `getAccuracy()` method computes the accuracy, which is quite low.
    This is obvious and there is no exertion. This also implies that the previous
    method is pointless. However, if you are about to predict discrete values, this
    method will obviously help you. Try it yourself with suitable data and combinations
    of the previous code.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'But do not to be disappointed; we have another way of looking at how our predictive
    model performs. We can still plot a histogram showing the predicted versus actual
    labels that are a prediction and actual distribution:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Implementing a kNN-Based Predictive Model](img/03_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 16: Predicted versus actual median prices of the houses in $1,000s'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this lesson, we have discussed unsupervised learning from a theoretical and
    practical perspective. We have seen how we can make use of predictive analytics
    and find out how we can take advantage of it to cluster records belonging to a
    certain group or class for a dataset of unsupervised observations. We have discussed
    unsupervised learning and clustering using K-means. In addition, we have seen
    how we can fine tune the clustering using the Elbow method for better predictive
    accuracy. We have also seen how to predict neighborhoods using K-means, and then,
    we have seen another example of clustering audio clips based on their audio features.
    Finally, we have seen how we can use unsupervised kNN for predicting the nearest
    neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: In the next lesson, we will discuss the wonderful field of text analytics using
    TensorFlow. Text analytics is a wide area in **natural language processing** (**NLP**),
    and ML is useful in many use cases, such as sentiment analysis, chatbots, email
    spam detection, text mining, and natural language processing. You will learn how
    to use TensorFlow for text analytics with a focus on use cases of text classification
    from the unstructured spam prediction and movie review dataset. Based on the spam
    filtering dataset, we will develop predictive models using a LR algorithm with
    TensorFlow
  prefs: []
  type: TYPE_NORMAL
- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: kNN falls in a ______ learning family of algorithms.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether the following statement is True or False: A cluster is a collection
    of objects that have a similarity between them and are dissimilar to the objects
    belonging to other clusters. If a collection of objects is provided, clustering
    algorithms put these objects into groups based on similarity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the main objective of unsupervised learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'State whether the following statement is True or False: In a clustering task,
    an algorithm groups related features into categories by analyzing similarities
    between input examples, where similar features are clustered and marked using
    circles.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering analysis is about dividing data samples or data points and putting
    them into corresponding ______ classes or clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Heterogynous
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Linear
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Homogeneous
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Similar.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
