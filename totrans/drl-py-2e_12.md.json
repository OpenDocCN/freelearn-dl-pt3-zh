["```\nimport warnings\nwarnings.filterwarnings('ignore')\nimport tensorflow as tf\ntf.compat.v1.disable_v2_behavior() \nimport numpy as np\nimport gym \n```", "```\nenv = gym.make(\"Pendulum-v0\").unwrapped \n```", "```\nstate_shape = env.observation_space.shape[0] \n```", "```\naction_shape = env.action_space.shape[0] \n```", "```\naction_bound = [env.action_space.low, env.action_space.high] \n```", "```\ngamma = 0.9 \n```", "```\ntau = 0.001 \n```", "```\nreplay_buffer = 10000 \n```", "```\nbatch_size = 32 \n```", "```\nclass DDPG(object): \n```", "```\n def __init__(self, state_shape, action_shape, high_action_value,): \n```", "```\n self.replay_buffer = np.zeros((replay_buffer, state_shape * 2 + action_shape + 1), dtype=np.float32) \n```", "```\n self.num_transitions = 0 \n```", "```\n self.sess = tf.Session() \n```", "```\n self.noise = 3.0 \n```", "```\n self.state_shape, self.action_shape, self.high_action_value = state_shape, action_shape, high_action_value \n```", "```\n self.state = tf.placeholder(tf.float32, [None, state_shape], 'state') \n```", "```\n self.next_state = tf.placeholder(tf.float32, [None, state_shape], 'next_state') \n```", "```\n self.reward = tf.placeholder(tf.float32, [None, 1], 'reward') \n```", "```\n with tf.variable_scope('Actor'): \n```", "```\n self.actor = self.build_actor_network(self.state, scope='main', trainable=True) \n```", "```\n target_actor = self.build_actor_network(self.next_state, scope='target', trainable=False) \n```", "```\n with tf.variable_scope('Critic'): \n```", "```\n critic = self.build_critic_network(self.state, self.actor, scope='main', trainable=True) \n```", "```\n target_critic = self.build_critic_network(self.next_state, target_actor, scope='target', trainable=False) \n```", "```\n self.main_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/main') \n```", "```\n self.target_actor_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Actor/target') \n```", "```\n self.main_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/main') \n```", "```\n self.target_critic_params = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='Critic/target') \n```", "```\n self.soft_replacement = [\n            [tf.assign(phi_, tau*phi + (1-tau)*phi_), tf.assign(theta_, tau*theta + (1-tau)*theta_)]\n            for phi, phi_, theta, theta_ in zip(self.main_actor_params, self.target_actor_params, self.main_critic_params, self.target_critic_params)\n            ] \n```", "```\n y = self.reward + gamma * target_critic \n```", "```\n MSE = tf.losses.mean_squared_error(labels=y, predictions=critic) \n```", "```\n self.train_critic = tf.train.AdamOptimizer(0.01).minimize(MSE, name=\"adam-ink\", var_list = self.main_critic_params) \n```", "```\n actor_loss = -tf.reduce_mean(critic) \n```", "```\n self.train_actor = tf.train.AdamOptimizer(0.001).minimize(actor_loss, var_list=self.main_actor_params) \n```", "```\n self.sess.run(tf.global_variables_initializer()) \n```", "```\n def select_action(self, state): \n```", "```\n action = self.sess.run(self.actor, {self.state: state[np.newaxis, :]})[0] \n```", "```\n action = np.random.normal(action, self.noise) \n```", "```\n action = np.clip(action, action_bound[0],action_bound[1])\n\n        return action \n```", "```\n def train(self): \n```", "```\n self.sess.run(self.soft_replacement) \n```", "```\n indices = np.random.choice(replay_buffer, size=batch_size) \n```", "```\n batch_transition = self.replay_buffer[indices, :] \n```", "```\n batch_states = batch_transition[:, :self.state_shape]\n        batch_actions = batch_transition[:, self.state_shape: self.state_shape + self.action_shape]\n        batch_rewards = batch_transition[:, -self.state_shape - 1: -self.state_shape]\n        batch_next_state = batch_transition[:, -self.state_shape:] \n```", "```\n self.sess.run(self.train_actor, {self.state: batch_states}) \n```", "```\n self.sess.run(self.train_critic, {self.state: batch_states, self.actor: batch_actions, self.reward: batch_rewards, self.next_state: batch_next_state}) \n```", "```\n def store_transition(self, state, actor, reward, next_state): \n```", "```\n trans = np.hstack((state,actor,[reward],next_state)) \n```", "```\n index = self.num_transitions % replay_buffer \n```", "```\n self.replay_buffer[index, :] = trans \n```", "```\n self.num_transitions += 1 \n```", "```\n if self.num_transitions > replay_buffer:\n            self.noise *= 0.99995\n            self.train() \n```", "```\n def build_actor_network(self, state, scope, trainable):\n        with tf.variable_scope(scope):\n            layer_1 = tf.layers.dense(state, 30, activation = tf.nn.tanh, name = 'layer_1', trainable = trainable)\n            actor = tf.layers.dense(layer_1, self.action_shape, activation = tf.nn.tanh, name = 'actor', trainable = trainable) \n            return tf.multiply(actor, self.high_action_value, name = \"scaled_a\") \n```", "```\n def build_critic_network(self, state, actor, scope, trainable):\n        with tf.variable_scope(scope):\n            w1_s = tf.get_variable('w1_s', [self.state_shape, 30], trainable = trainable)\n            w1_a = tf.get_variable('w1_a', [self.action_shape, 30], trainable = trainable)\n            b1 = tf.get_variable('b1', [1, 30], trainable = trainable)\n            net = tf.nn.tanh( tf.matmul(state, w1_s) + tf.matmul(actor, w1_a) + b1 )\n            critic = tf.layers.dense(net, 1, trainable = trainable)\n            return critic \n```", "```\nddpg = DDPG(state_shape, action_shape, action_bound[1]) \n```", "```\nnum_episodes = 300 \n```", "```\nnum_timesteps = 500 \n```", "```\nfor i in range(num_episodes): \n```", "```\n state = env.reset() \n```", "```\n Return = 0 \n```", "```\n for t in range(num_timesteps): \n```", "```\n env.render() \n```", "```\n action = ddpg.select_action(state) \n```", "```\n next_state, reward, done, info = env.step(action) \n```", "```\n ddpg.store_transition(state, action, reward, next_state) \n```", "```\n Return += reward \n```", "```\n if done:\n            break \n```", "```\n state = next_state \n```", "```\n if i %10 ==0:\n         print(\"Episode:{}, Return: {}\".format(i,Return)) \n```"]