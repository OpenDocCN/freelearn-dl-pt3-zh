["```\ndef normalize_and_reshape_img(img):\n# Histogram normalization in v channel\nhsv = color.rgb2hsv(img)\nhsv[:, :, 2] = exposure.equalize_hist(hsv[:, :, 2])\nimg = color.hsv2rgb(hsv)\n# Crop of the centre\nmin_side = min(img.shape[:-1])\ncentre = img.shape[0] // 2, img.shape[1] // 2\nimg = img[centre[0] - min_side // 2:centre[0] + min_side // 2,\ncentre[1] - min_side // 2:centre[1] + min_side // 2,\n:]\n# Rescale to the desired size\nimg = transform.resize(img, (IMG_SIZE, IMG_SIZE))\nreturn\n img\n```", "```\ndef preprocess_and_save_data(data_type ='train'):\n'''\nPreprocesses image data and saves the image features and labels as pickle files to be used for the model\n:param data_type: data_type is 'train' or 'test'\n:return: None\n'''\nif data_type =='train':\nroot_dir = os.path.join(DATA_DIR, 'GTSRB/Final_Training/Images/')\nimgs = []\nlabels = []\nall_img_paths = glob.glob(os.path.join(root_dir, '*/*.ppm'))\nnp.random.shuffle(all_img_paths)\nfor img_path in all_img_paths:\nimg = normalize_and_reshape_img(io.imread(img_path))\nlabel = get_class(img_path)\nimgs.append(img)\nlabels.append(label)\nX_train = np.array(imgs, dtype='float32')\n# Make one hot targets\nY_train = np.array(labels, dtype = 'uint8')\ntrain_data = {\"features\": X_train, \"labels\": Y_train}\nif not os.path.exists(os.path.join(DATA_DIR,\"Preprocessed_Data\")):\nos.makedirs(os.path.join(DATA_DIR,\"Preprocessed_Data\"))\npickle.dump(train_data,open(os.path.join(DATA_DIR,\"Preprocessed_Data\",\"preprocessed_train.p\"),\"wb\"))\nreturn train_data\nelif data_type == 'test':\n# Reading the test file\ntest = pd.read_csv(os.path.join(DATA_DIR, \"GTSRB\", 'GT-final_test.csv'), sep=';')\nX_test = []\ny_test = []\ni = 0\nfor file_name, class_id in zip(list(test['Filename']), list(test['ClassId'])):\nimg_path = os.path.join(DATA_DIR, 'GTSRB/Final_Test/Images/', file_name)\nX_test.append(normalize_and_reshape_img(io.imread(img_path)))\ny_test.append(class_id)\ntest_data = {\"features\": np.array(X_test,dtype ='float32'), \"labels\": np.array(y_test,dtype = 'uint8')}\nif not os.path.exists(os.path.join(DATA_DIR,\"Preprocessed_Data\")):\nos.makedirs(os.path.join(DATA_DIR,\"Preprocessed_Data\"))\npickle.dump(test_data,open(os.path.join(DATA_DIR,\"Preprocessed_Data\",\"preprocessed_test.p\"),\"wb\"))\nreturn test_data\n```", "```\nwith tf.name_scope(\"BNN\", values=[images]):\nmodel = tf.keras.Sequential([\ntfp.layers.Convolution2DFlipout(10,\nkernel_size=5,\npadding=\"VALID\",\nactivation=tf.nn.relu),\ntf.keras.layers.MaxPooling2D(pool_size=[3, 3],\nstrides=[1, 1],\npadding=\"VALID\"),\ntfp.layers.Convolution2DFlipout(15,\nkernel_size=3,\npadding=\"VALID\",\nactivation=tf.nn.relu),\ntf.keras.layers.MaxPooling2D(pool_size=[2, 2],\nstrides=[2, 2],\npadding=\"VALID\"),\ntfp.layers.Convolution2DFlipout(30,\nkernel_size=3,\npadding=\"VALID\",\nactivation=tf.nn.relu),\ntf.keras.layers.MaxPooling2D(pool_size=[2, 2],\nstrides=[2, 2],\npadding=\"VALID\"),\ntf.keras.layers.Flatten(),\ntfp.layers.DenseFlipout(400, activation=tf.nn.relu),\ntfp.layers.DenseFlipout(120, activation = tf.nn.relu),\ntfp.layers.DenseFlipout(84, activation=tf.nn.relu),\ntfp.layers.DenseFlipout(43) ])\nlogits = model(images)\ntargets_distribution = tfd.Categorical(logits=logits)\n```", "```\n# Compute the -ELBO as the loss, averaged over the batch size.\nneg_log_likelihood = -\n  tf.reduce_mean(targets_distribution.log_prob(targets))\nkl = sum(model.losses) / X_train.shape[0]\n   elbo_loss = neg_log_likelihood + kl\n```", "```\nwith tf.name_scope(\"train\"):\noptimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE)\ntrain_op = optimizer.minimize(elbo_loss)\n```", "```\nwith tf.Session() as sess:\nsess.run(init_op)\n# Run the training loop.\ntrain_handle = sess.run(train_iterator.string_handle())\ntest_handle = sess.run(test_iterator.string_handle())\nfor step in range(EPOCHS):\n_ = sess.run([train_op, accuracy_update_op],\nfeed_dict={iter_handle: train_handle})\nif step % 5== 0:\nloss_value, accuracy_value = sess.run(\n     [elbo_loss, accuracy], feed_dict={iter_handle: train_handle})\nprint(\"Epoch: {:>3d} Loss: {:.3f} Accuracy: {:.3f}\".format(\nstep, loss_value, accuracy_value))\n```", "```\n#Sampling from the posterior and obtaining mean probability for held out dataset\nprobs = np.asarray([sess.run((targets_distribution.probs),\n        feed_dict={iter_handle: test_handle})\nfor _ in range(NUM_MONTE_CARLO)])\n```", "```\nmean_probs = np.mean(probs, axis=0)\n# Get the average accuracy\nY_pred = np.argmax(mean_probs, axis=1)\nprint(\"Overall Accuracy in predicting the test data = percent\", round((Y_pred == y_test).mean() * 100,2))\n```", "```\ntest_acc_dist = []\nfor prob in probs:\ny_test_pred = np.argmax(prob, axis=1).astype(np.float32)\naccuracy = (y_test_pred == y_test).mean() * 100\ntest_acc_dist.append(accuracy)\nplt.hist(test_acc_dist)\nplt.title(\"Histogram of prediction accuracies on test dataset\")\nplt.xlabel(\"Accuracy\")\nplt.ylabel(\"Frequency\")\nsave_dir = os.path.join(DATA_DIR, \"..\", \"Plots\")\nplt.savefig(os.path.join(save_dir, \"Test_Dataset_Prediction_Accuracy.png\"))\n```", "```\ndef plot_heldout_prediction(input_vals, probs , fname, title=\"\"):\nsave_dir = os.path.join(DATA_DIR, \"..\", \"Plots\")\nfig = figure.Figure(figsize=(1, 1))\ncanvas = backend_agg.FigureCanvasAgg(fig)\nax = fig.add_subplot(1,1,1)\nax.imshow(input_vals.reshape((IMG_SIZE,IMG_SIZE)), interpolation=\"None\")\ncanvas.print_figure(os.path.join(save_dir, fname + \"_image.png\"), format=\"png\")\nfig = figure.Figure(figsize=(10, 5))\ncanvas = backend_agg.FigureCanvasAgg(fig)\nax = fig.add_subplot(1,1,1)\n#Predictions\ny_pred_list = list(np.argmax(probs,axis=1).astype(np.int32))\nbin_range = [x for x in range(43)]\nax.hist(y_pred_list,bins = bin_range)\nax.set_xticks(bin_range)\nax.set_title(\"Histogram of predicted class: \" + title)\nax.set_xlabel(\"Class\")\nax.set_ylabel(\"Frequency\")\nfig.tight_layout()\nsave_dir = os.path.join(DATA_DIR, \"..\", \"Plots\")\ncanvas.print_figure(os.path.join(save_dir, fname + \"_predicted_class.png\"), format=\"png\")\nprint(\"saved {}\".format(fname))\n```"]