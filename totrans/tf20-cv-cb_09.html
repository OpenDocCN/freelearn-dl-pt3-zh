<html><head></head><body>
		<div id="_idContainer104">
			<h1 id="_idParaDest-251"><em class="italic"><a id="_idTextAnchor298"/>Chapter 9</em>: Localizing Elements in Images with Object Detection</h1>
			<p>Object detection is one of the most common yet challenging tasks in computer vision. It's a natural evolution of image classification, where our goal is to work out what is in an image. On the other hand, object detection is not only concerned with the content of an image but also with the location of elements of interest in a digital image.</p>
			<p>As with many other well-known tasks in computer vision, object detection has long been addressed with a wide array of techniques, ranging from naïve solutions (such as object matching) to machine learning-based ones (such as Haar Cascades). Nonetheless, the most effective detectors nowadays are powered by deep learning.</p>
			<p>Implementing <a id="_idIndexMarker853"/>state-of-the-art object detectors (such as <strong class="bold">You Only Look Once</strong> (<strong class="bold">YOLO</strong>) and <strong class="bold">Fast Region-based Convolutional Neural Network</strong> (<strong class="bold">Fast R-CNN</strong>) from scratch is a very challenging task. However, there are many<a id="_idIndexMarker854"/> pre-trained solutions we can leverage, not only to make predictions but also to train our own models from zero, as we'll discover in this chapter.</p>
			<p>Here is a list of the recipes we'll be working on in no time:</p>
			<ul>
				<li>Creating an object detector with image pyramids and sliding windows</li>
				<li>Detecting objects with YOLOv3</li>
				<li>Training<a id="_idIndexMarker855"/> your own object detector with TensorFlow's Object Detection <strong class="bold">Application Programming Interface</strong> (<strong class="bold">API</strong>)</li>
				<li>Detecting <a id="_idIndexMarker856"/>objects using <strong class="bold">TensorFlow Hub</strong> (<strong class="bold">TFHub</strong>) </li>
			</ul>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor299"/>Technical requirements</h1>
			<p>Given the complexity of object detectors, having access to a <strong class="bold">Graphics Processing Unit</strong> (<strong class="bold">GPU</strong>) is a great idea. There are many cloud providers you can use to run the recipes in this chapter, my favorite being FloydHub, but you can use whichever you like the most! Of course, do keep in mind of the fees if you don't want any surprises! In the <em class="italic">Getting ready</em> sections, you'll find the preparatory steps for each recipe. The code for this chapter is available at <a href="https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9">https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/39wInla">https://bit.ly/39wInla</a>.</p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor300"/>Creating an object detector with image pyramids and sliding windows</h1>
			<p>Traditionally, object <a id="_idIndexMarker857"/>detectors have <a id="_idIndexMarker858"/>worked following an iterative<a id="_idIndexMarker859"/> algorithm whereby a window is<a id="_idIndexMarker860"/> slid across the image, at different scales, in order to detect potential objects at every location and perspective. Although this approach is outdated due to its noticeable drawbacks (which we'll talk more about in the <em class="italic">How it works…</em> section), it has the great advantage of being agnostic about the type of image classifier we use, meaning we can use it as a framework to turn any classifier into an object detector. This is precisely what we'll do in this first recipe!</p>
			<p>Let's begin.</p>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor301"/>Getting ready</h2>
			<p>We need to install a couple of external libraries, such as <strong class="source-inline">OpenCV</strong>, <strong class="source-inline">Pillow</strong>, and <strong class="source-inline">imutils</strong>, which can easily be accomplished with this command:</p>
			<p class="source-code">$&gt; pip install opencv-contrib-python Pillow imutils</p>
			<p>We'll use a pre-trained model to power our object detector; therefore, we don't need any data for this recipe.</p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor302"/>How to do it…</h2>
			<p>Follow these<a id="_idIndexMarker861"/> steps to complete the recipe:</p>
			<ol>
				<li>Import<a id="_idIndexMarker862"/> the<a id="_idIndexMarker863"/> necessary <a id="_idIndexMarker864"/>dependencies:<p class="source-code">import cv2</p><p class="source-code">import imutils</p><p class="source-code">import numpy as np</p><p class="source-code">from tensorflow.keras.applications import imagenet_utils</p><p class="source-code">from tensorflow.keras.applications.inception_resnet_v2 \</p><p class="source-code">    import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import img_to_array</p></li>
				<li>Next, let's define our <strong class="source-inline">ObjectDetector()</strong> class, starting with the constructor:<p class="source-code">class ObjectDetector(object):</p><p class="source-code">    def __init__(self, </p><p class="source-code">                 classifier,</p><p class="source-code">                 preprocess_fn=lambda x: x,</p><p class="source-code">                 input_size=(299, 299),</p><p class="source-code">                 confidence=0.98,</p><p class="source-code">                 window_step_size=16,</p><p class="source-code">                 pyramid_scale=1.5,</p><p class="source-code">                 roi_size=(200, 150),</p><p class="source-code">                 nms_threshold=0.3):</p><p class="source-code">        self.classifier = classifier</p><p class="source-code">        self.preprocess_fn = preprocess_fn</p><p class="source-code">        self.input_size = input_size</p><p class="source-code">        self.confidence = confidence</p><p class="source-code">        self.window_step_size = window_step_size</p><p class="source-code">        self.pyramid_scale = pyramid_scale</p><p class="source-code">        self.roi_size = roi_size</p><p class="source-code">        self.nms_threshold = nms_threshold</p><p>The <strong class="source-inline">classifier</strong> is just a<a id="_idIndexMarker865"/> trained <a id="_idIndexMarker866"/>network we'll use to <a id="_idIndexMarker867"/>classify <a id="_idIndexMarker868"/>each window, while <strong class="source-inline">preprocess_fn</strong> is the function used to process each window prior to passing it to the classifier. <strong class="source-inline">confidence</strong> is the minimum probability we'll allow detections to have in order to consider them valid. The remaining parameters will be explained in the next steps.</p></li>
				<li>Now, let's define a <strong class="source-inline">sliding_window()</strong> method, which extracts portions of the input image, with dimensions equal to <strong class="source-inline">self.roi_size</strong>. It's going to be slid across the image, both horizontally and vertically, at a rate of <strong class="source-inline">self.window_step_size</strong> pixels at a time (notice the use of <strong class="source-inline">yield</strong> instead of <strong class="source-inline">return</strong>—that's because this is a generator):<p class="source-code">    def sliding_window(self, image):</p><p class="source-code">        for y in range(0,</p><p class="source-code">                       image.shape[0],</p><p class="source-code">                       self.window_step_size):</p><p class="source-code">            for x in range(0,</p><p class="source-code">                           image.shape[1],</p><p class="source-code">                           self.window_step_size):</p><p class="source-code">              y_slice = slice(y, y + self.roi_size[1], 1)</p><p class="source-code">              x_slice = slice(x, x + self.roi_size[0], 1)</p><p class="source-code">                yield x, y, image[y_slice, x_slice]</p></li>
				<li>Next, define <a id="_idIndexMarker869"/>the <strong class="source-inline">pyramid()</strong> method, which <a id="_idIndexMarker870"/>generates<a id="_idIndexMarker871"/> smaller and smaller<a id="_idIndexMarker872"/> copies of the input image, until a minimum size is met (akin to the levels of a pyramid):<p class="source-code">    def pyramid(self, image):</p><p class="source-code">        yield image</p><p class="source-code">        while True:</p><p class="source-code">            width = int(image.shape[1] / </p><p class="source-code">                     self.pyramid_scale)</p><p class="source-code">            image = imutils.resize(image, width=width)</p><p class="source-code">            if (image.shape[0] &lt; self.roi_size[1] or</p><p class="source-code">                    image.shape[1] &lt; </p><p class="source-code">                  self.roi_size[0]):</p><p class="source-code">                break</p><p class="source-code">            yield image</p></li>
				<li>Because sliding a window across the same image at different scales is very prone to producing many detections related to the same object, we need a way to keep duplicates at a minimum. That's the purpose of our next method, <strong class="source-inline">non_max_suppression()</strong>:<p class="source-code">    def non_max_suppression(self, boxes, probabilities):</p><p class="source-code">        if len(boxes) == 0:</p><p class="source-code">            return []</p><p class="source-code">        if boxes.dtype.kind == 'i':</p><p class="source-code">            boxes = boxes.astype(np.float)</p><p class="source-code">        pick = []</p><p class="source-code">        x_1 = boxes[:, 0]</p><p class="source-code">        y_1 = boxes[:, 1]</p><p class="source-code">        x_2 = boxes[:, 2]</p><p class="source-code">        y_2 = boxes[:, 3]</p><p class="source-code">        area = (x_2 - x_1 + 1) * (y_2 - y_1 + 1)</p><p class="source-code">        indexes = np.argsort(probabilities)</p></li>
				<li>We<a id="_idIndexMarker873"/> start<a id="_idIndexMarker874"/> by <a id="_idIndexMarker875"/>computing the <a id="_idIndexMarker876"/>area of all bounding boxes, and also sort them by their probability, in increasing order. Now, we'll pick the index of the bounding box with the highest probability, and add it to our final selection (<strong class="source-inline">pick</strong>) until we have <strong class="source-inline">indexes</strong> left to trim down:<p class="source-code">        while len(indexes) &gt; 0:</p><p class="source-code">            last = len(indexes) - 1</p><p class="source-code">            i = indexes[last]</p><p class="source-code">            pick.append(i)</p></li>
				<li>We <a id="_idIndexMarker877"/>compute the overlap between<a id="_idIndexMarker878"/> the picked bounding<a id="_idIndexMarker879"/> box and the other ones, and<a id="_idIndexMarker880"/> then get rid of those boxes where the overlap is higher than <strong class="source-inline">self.nms_threshold</strong>, which means that they probably refer to the same object:<p class="source-code">            xx_1 = np.maximum(x_1[i],x_1[indexes[:last]])</p><p class="source-code">            yy_1 = np.maximum(y_1[i],y_1[indexes[:last]])</p><p class="source-code">            xx_2 = np.maximum(x_2[i],x_2[indexes[:last]])</p><p class="source-code">            yy_2 = np.maximum(y_2[i],y_2[indexes[:last]])</p><p class="source-code">            width = np.maximum(0, xx_2 - xx_1 + 1)</p><p class="source-code">            height = np.maximum(0, yy_2 - yy_1 + 1)</p><p class="source-code">            overlap = (width * height) / </p><p class="source-code">                      area[indexes[:last]]</p><p class="source-code">            redundant_boxes = \</p><p class="source-code">                np.where(overlap &gt; </p><p class="source-code">                        self.nms_threshold)[0]</p><p class="source-code">            to_delete = np.concatenate(</p><p class="source-code">                ([last], redundant_boxes))</p><p class="source-code">            indexes = np.delete(indexes, to_delete)</p></li>
				<li>Return the picked bounding boxes:<p class="source-code">        return boxes[pick].astype(np.int)</p></li>
				<li>The <strong class="source-inline">detect()</strong> method ties the object detection algorithm together. We start by defining a<a id="_idIndexMarker881"/> list of <strong class="bold">regions of interest</strong> (<strong class="source-inline">rois</strong>) and their corresponding <strong class="source-inline">locations</strong> (coordinates in the original image):<p class="source-code">    def detect(self, image):</p><p class="source-code">        rois = []</p><p class="source-code">        locations = []</p></li>
				<li>Next, we'll <a id="_idIndexMarker882"/>generate different<a id="_idIndexMarker883"/> copies of the input <a id="_idIndexMarker884"/>image at several scales<a id="_idIndexMarker885"/> using the <strong class="source-inline">pyramid()</strong> generator, and at each level, we'll slide a window (with the <strong class="source-inline">sliding_windows()</strong> generator) to extract all possible ROIs:<p class="source-code">        for img in self.pyramid(image):</p><p class="source-code">            scale = image.shape[1] / </p><p class="source-code">                    float(img.shape[1])</p><p class="source-code">            for x, y, roi_original in \</p><p class="source-code">                    self.sliding_window(img):</p><p class="source-code">                x = int(x * scale)</p><p class="source-code">                y = int(y * scale)</p><p class="source-code">                w = int(self.roi_size[0] * scale)</p><p class="source-code">                h = int(self.roi_size[1] * scale)</p><p class="source-code">                roi = cv2.resize(roi_original, </p><p class="source-code">                                 self.input_size)</p><p class="source-code">                roi = img_to_array(roi)</p><p class="source-code">                roi = self.preprocess_fn(roi)</p><p class="source-code">                rois.append(roi)</p><p class="source-code">                locations.append((x, y, x + w, y + h))</p><p class="source-code">        rois = np.array(rois, dtype=np.float32)</p></li>
				<li>Pass <a id="_idIndexMarker886"/>all <a id="_idIndexMarker887"/>ROIs <a id="_idIndexMarker888"/>through <a id="_idIndexMarker889"/>the classifier at once:<p class="source-code">        predictions = self.classifier.predict(rois)</p><p class="source-code">        predictions = \</p><p class="source-code">       imagenet_utils.decode_predictions(predictions, </p><p class="source-code">                                              top=1)</p></li>
				<li>Build a <strong class="source-inline">dict</strong> to map each label produced by the classifier to all the bounding boxes and their probabilities (notice we only keep those bounding boxes with a probability of at least <strong class="source-inline">self.confidence</strong>):<p class="source-code">        labels = {}</p><p class="source-code">        for i, pred in enumerate(predictions):</p><p class="source-code">            _, label, proba = pred[0]</p><p class="source-code">            if proba &gt;= self.confidence:</p><p class="source-code">                box = locations[i]</p><p class="source-code">                label_detections = labels.get(label, [])</p><p class="source-code">                label_detections.append({'box': box,</p><p class="source-code">                                         'proba': </p><p class="source-code">                                          proba})</p><p class="source-code">                labels[label] = label_detections</p><p class="source-code">        return labels</p></li>
				<li>Instantiate an <strong class="source-inline">InceptionResnetV2</strong> network trained on ImageNet to use as our classifier and pass it to a new <strong class="source-inline">ObjectDetector</strong>. Notice that we're also passing the <strong class="source-inline">preprocess_function</strong> as input:<p class="source-code">model = InceptionResNetV2(weights='imagenet',</p><p class="source-code">                          include_top=True)</p><p class="source-code">object_detector = ObjectDetector(model, preprocess_input)</p></li>
				<li>Load<a id="_idIndexMarker890"/> the<a id="_idIndexMarker891"/> input <a id="_idIndexMarker892"/>image, resize <a id="_idIndexMarker893"/>it to a width of 600 pixels maximum (the height will be computed accordingly to preserve the aspect ratio), and run it through the object detector:<p class="source-code">image = cv2.imread('dog.jpg')</p><p class="source-code">image = imutils.resize(image, width=600)</p><p class="source-code">labels = object_detector.detect(image)</p></li>
				<li>Go over all the detections corresponding to each label, and first draw all the bounding boxes:<p class="source-code">GREEN = (0, 255, 0)</p><p class="source-code">for i, label in enumerate(labels.keys()):</p><p class="source-code">    clone = image.copy()</p><p class="source-code">    for detection in labels[label]:</p><p class="source-code">        box = detection['box']</p><p class="source-code">        probability = detection['proba']</p><p class="source-code">        x_start, y_start, x_end, y_end = box</p><p class="source-code">        cv2.rectangle(clone, (x_start, y_start),</p><p class="source-code">                      (x_end, y_end), (0, 255, 0), 2)</p><p class="source-code">    cv2.imwrite(f'Before_{i}.jpg', clone)</p><p>Then, use <strong class="bold">Non-Maximum Suppression</strong> (<strong class="bold">NMS</strong>) to <a id="_idIndexMarker894"/>get <a id="_idIndexMarker895"/>rid<a id="_idIndexMarker896"/> of <a id="_idIndexMarker897"/>duplicates <a id="_idIndexMarker898"/>and draw the surviving bounding boxes:</p><p class="source-code">    clone = image.copy()</p><p class="source-code">    boxes = np.array([d['box'] for d in </p><p class="source-code">                   labels[label]])</p><p class="source-code">    probas = np.array([d['proba'] for d in </p><p class="source-code">                    labels[label]])</p><p class="source-code">    boxes = object_detector.non_max_suppression(boxes,</p><p class="source-code">                                              probas)</p><p class="source-code">    for x_start, y_start, x_end, y_end in boxes:</p><p class="source-code">        cv2.rectangle(clone, (x_start, y_start),</p><p class="source-code">                      (x_end, y_end), GREEN, 2)</p><p class="source-code">        </p><p class="source-code">        if y_start - 10 &gt; 10:</p><p class="source-code">            y = y_start - 10</p><p class="source-code">        else:</p><p class="source-code">            y = y_start + 10</p><p class="source-code">        </p><p class="source-code">        cv2.putText(clone, label, (x_start, y),</p><p class="source-code">                    cv2.FONT_HERSHEY_SIMPLEX, .45,</p><p class="source-code">                    GREEN, 2)</p><p class="source-code">    cv2.imwrite(f'After_{i}.jpg', clone)</p><p>Here's <a id="_idIndexMarker899"/>the<a id="_idIndexMarker900"/> result <a id="_idIndexMarker901"/>without<a id="_idIndexMarker902"/> NMS: </p></li>
			</ol>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B14768_09_001.jpg" alt="Figure 9.1 – Overlapping detections of the same dog&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Overlapping detections of the same dog</p>
			<p>And here's the result after applying NMS:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B14768_09_002.jpg" alt="Figure 9.2 – With NMS, we got rid of the redundant detections&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – With NMS, we got rid of the redundant detections</p>
			<p>Although <a id="_idIndexMarker903"/>we<a id="_idIndexMarker904"/> successfully <a id="_idIndexMarker905"/>detected <a id="_idIndexMarker906"/>the dog in the previous photos, we notice that the bounding box doesn't tightly wrap the object as nicely as we might have expected. Let's talk about this and other issues regarding old-school object detection in the next section.</p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor303"/>How it works…</h2>
			<p>In this recipe, we implemented a reusable class that easily allows us to turn any image classifier into an object detector, by leveraging the iterative approach of extracting ROIs (sliding windows) at different levels of perspective (image pyramid) and passing them to such a classifier to determine where objects are in a photo, and what they are. Also, we used NMS to reduce the amount of non-informative, duplicate detections that are characteristic of this strategy.</p>
			<p>Although this a great first attempt at creating an object detector, it has its flaws:</p>
			<ul>
				<li>It's incredibly slow, which makes it unusable in real-time situations.</li>
				<li>The accuracy of the bounding boxes depends heavily on the parameter selection for the image pyramid, the sliding window, and the ROI size.</li>
				<li>The architecture is not end-to-end trainable, which means that errors in bounding-box predictions are not backpropagated through the network in order to produce better, more accurate detections in the future, by updating its weights. Instead, we're stuck with pre-trained models that limit themselves to infer but not to learn because the framework does not allow them to.</li>
			</ul>
			<p>However, don't rule out<a id="_idIndexMarker907"/> this approach yet! If you're <a id="_idIndexMarker908"/>working with images that present very little <a id="_idIndexMarker909"/>variation in size and perspective, and<a id="_idIndexMarker910"/> your application definitely doesn't operate in a real-time context, the strategy implemented in this recipe can work wonders for your project!</p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor304"/>See also</h2>
			<p>You can read more about NMS here: </p>
			<p><a href="https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c">https://towardsdatascience.com/non-maximum-suppression-nms-93ce178e177c</a></p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor305"/>Detecting objects with YOLOv3</h1>
			<p>In the <em class="italic">Creating an object detector with image pyramids and sliding windows</em> recipe, we<a id="_idIndexMarker911"/> learned how to turn any image classifier into<a id="_idIndexMarker912"/> an object detector, by embedding it in a traditional framework that relies on image pyramids and sliding windows. However, we also learned that this approach isn't ideal because it doesn't allow the network to learn from its mistakes. </p>
			<p>The reason why deep learning has conquered the field of object detection is due to its end-to-end approach. The network not only figures out how to classify an object, but also discovers how to produce the best bounding box possible to locate each element in the image. </p>
			<p>On top of this, thanks to this end-to-end strategy, a network can detect a myriad objects in a single pass! Of course, this makes such object detectors incredibly efficient! </p>
			<p>One of the seminal end-to-end object detectors is YOLO, and in this recipe, we'll learn how to detect objects with a pre-trained YOLOv3 model. </p>
			<p>Let's begin!</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor306"/>Getting ready</h2>
			<p>First, install <strong class="source-inline">tqdm</strong>, as follows:</p>
			<p class="source-code">$&gt; pip install tqdm</p>
			<p>Our implementation is heavily inspired by the amazing <strong class="source-inline">keras-yolo3</strong> repository implemented by <em class="italic">Huynh Ngoc Anh (on GitHub as experiencor)</em>, which you can consult here: </p>
			<p><a href="https://github.com/experiencor/keras-yolo3">https://github.com/experiencor/keras-yolo3</a></p>
			<p>Because we'll use a pre-trained YOLO model, we need to download the weights. They're available here: <a href="https://pjreddie.com/media/files/yolov3.weights">https://pjreddie.com/media/files/yolov3.weights</a>. For the purposes of this tutorial, we assume they're inside the <strong class="source-inline">ch9/recipe2/resources</strong> folder, in the companion repository, as <strong class="source-inline">yolov3.weights</strong>. These weights are the same ones used by the original authors of YOLO. Refer to the <em class="italic">See also</em> section to learn more about YOLO.</p>
			<p>We are good to go! </p>
			<p>How to do it…</p>
			<p>Follow these steps to complete the recipe:</p>
			<ol>
				<li value="1">Start by importing the relevant dependencies:<p class="source-code">import glob</p><p class="source-code">import json</p><p class="source-code">import struct</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tqdm</p><p class="source-code">from matplotlib.patches import Rectangle</p><p class="source-code">from tensorflow.keras.layers import *</p><p class="source-code">from tensorflow.keras.models import *</p><p class="source-code">from tensorflow.keras.preprocessing.image import *</p></li>
				<li>Define <a id="_idIndexMarker913"/>a <strong class="source-inline">WeightReader()</strong> class that automatically<a id="_idIndexMarker914"/> loads the YOLO weights in whichever format the original authors used. Notice that this is a very low-level solution, but we don't need to understand it fully in order to leverage it. Let's begin with the constructor:<p class="source-code">class WeightReader:</p><p class="source-code">    def __init__(self, weight_file):</p><p class="source-code">        with open(weight_file, 'rb') as w_f:</p><p class="source-code">            major, = struct.unpack('i', w_f.read(4))</p><p class="source-code">            minor, = struct.unpack('i', w_f.read(4))</p><p class="source-code">            revision, = struct.unpack('i', w_f.read(4))</p><p class="source-code">            if (major * 10 + minor) &gt;= 2 and \</p><p class="source-code">                    major &lt; 1000 and \</p><p class="source-code">                    minor &lt; 1000:</p><p class="source-code">                w_f.read(8)</p><p class="source-code">            else:</p><p class="source-code">                w_f.read(4)</p><p class="source-code">            binary = w_f.read()</p><p class="source-code">        self.offset = 0</p><p class="source-code">        self.all_weights = np.frombuffer(binary,</p><p class="source-code">                                     dtype='float32')</p></li>
				<li>Next, define<a id="_idIndexMarker915"/> a method to read a given number of<a id="_idIndexMarker916"/> bytes from the <strong class="source-inline">weights</strong> file:<p class="source-code">    def read_bytes(self, size):</p><p class="source-code">        self.offset = self.offset + size</p><p class="source-code">        return self.all_weights[self.offset-</p><p class="source-code">                               size:self.offset]</p></li>
				<li>The <strong class="source-inline">load_weights()</strong> method loads the weights for each of the 106 layers that comprise the YOLO architecture:<p class="source-code">    def load_weights(self, model):</p><p class="source-code">        for i in tqdm.tqdm(range(106)):</p><p class="source-code">            try:</p><p class="source-code">                conv_layer = model.get_layer(f'conv_{i}')</p><p class="source-code">                if i not in [81, 93, 105]:</p><p class="source-code">                    norm_layer = </p><p class="source-code">             model.get_layer(f'bnorm_{i}')</p><p class="source-code">                    size = np.prod(norm_layer.</p><p class="source-code">                                   </p><p class="source-code">                              get_weights()[0].shape)</p><p class="source-code">                    bias = self.read_bytes(size)</p><p class="source-code">                    scale = self.read_bytes(size)</p><p class="source-code">                    mean = self.read_bytes(size)</p><p class="source-code">                    var = self.read_bytes(size)</p><p class="source-code">                    norm_layer.set_weights([scale, </p><p class="source-code">                                            bias, mean, </p><p class="source-code">                                            var])</p></li>
				<li>Load<a id="_idIndexMarker917"/> the<a id="_idIndexMarker918"/> weights of the convolutional layers:<p class="source-code">                if len(conv_layer.get_weights()) &gt; 1:</p><p class="source-code">                    bias = self.read_bytes(np.prod(</p><p class="source-code">                   conv_layer.get_weights()[1].shape))</p><p class="source-code">                    kernel = self.read_bytes(np.prod(</p><p class="source-code">                   conv_layer.get_weights()[0].shape))</p><p class="source-code">                    kernel = </p><p class="source-code">                  kernel.reshape(list(reversed(</p><p class="source-code">                conv_layer.get_weights()[0].shape)))</p><p class="source-code">                    kernel = kernel.transpose([2, 3, </p><p class="source-code">                                               1, 0])</p><p class="source-code">                    conv_layer.set_weights([kernel, </p><p class="source-code">                                            bias])</p><p class="source-code">                else:</p><p class="source-code">                    kernel = self.read_bytes(np.prod(</p><p class="source-code">                  conv_layer.get_weights()[0].shape))</p><p class="source-code">                    kernel = </p><p class="source-code">               kernel.reshape(list(reversed(</p><p class="source-code">                        </p><p class="source-code">            conv_layer.get_weights()[0].shape)))</p><p class="source-code">                  kernel = kernel.transpose([2, 3, 1, 0])</p><p class="source-code">                    conv_layer.set_weights([kernel])</p><p class="source-code">            except ValueError:</p><p class="source-code">                pass</p></li>
				<li>Define<a id="_idIndexMarker919"/> a <a id="_idIndexMarker920"/>method to reset the offset:<p class="source-code">    def reset(self):</p><p class="source-code">        self.offset = 0</p></li>
				<li>Define a <strong class="source-inline">BoundBox()</strong> class that encapsulates the vertices of a bounding box, along with <a id="_idIndexMarker921"/>the <a id="_idIndexMarker922"/>confidence that the enclosed elements are an object (<strong class="source-inline">objness</strong>):<p class="source-code">class BoundBox(object):</p><p class="source-code">    def __init__(self, x_min, y_min, x_max, y_max,</p><p class="source-code">                 objness=None,</p><p class="source-code">                 classes=None):</p><p class="source-code">        self.xmin = x_min</p><p class="source-code">        self.ymin = y_min</p><p class="source-code">        self.xmax = x_max</p><p class="source-code">        self.ymax = y_max</p><p class="source-code">        self.objness = objness</p><p class="source-code">        self.classes = classes</p><p class="source-code">        self.label = -1</p><p class="source-code">        self.score = -1</p><p class="source-code">    def get_label(self):</p><p class="source-code">        if self.label == -1:</p><p class="source-code">            self.label = np.argmax(self.classes)</p><p class="source-code">        return self.label</p><p class="source-code">    def get_score(self):</p><p class="source-code">        if self.score == -1:</p><p class="source-code">            self.score = self.classes[self.get_label()]</p><p class="source-code">        return self.score</p></li>
				<li>Define a <strong class="source-inline">YOLO()</strong> class that encapsulates both the construction of the network and the <a id="_idIndexMarker923"/>detection logic. Let's begin with the <a id="_idIndexMarker924"/>constructor:<p class="source-code">class YOLO(object):</p><p class="source-code">    def __init__(self, weights_path,</p><p class="source-code">                 anchors_path='resources/anchors.json',</p><p class="source-code">                 labels_path='resources/coco_labels.txt',</p><p class="source-code">                 class_threshold=0.65):</p><p class="source-code">        self.weights_path = weights_path</p><p class="source-code">        self.model = self._load_yolo()</p><p class="source-code">        self.labels = []</p><p class="source-code">        with open(labels_path, 'r') as f:</p><p class="source-code">            for l in f:</p><p class="source-code">                self.labels.append(l.strip())</p><p class="source-code">        with open(anchors_path, 'r') as f:</p><p class="source-code">            self.anchors = json.load(f)</p><p class="source-code">        self.class_threshold = class_threshold</p><p>The output of YOLO is a set of encoded bounding boxes defined in the context of anchor boxes that were carefully chosen by the authors of YOLO. This is based on an analysis of the size of objects in the <strong class="source-inline">COCO</strong> dataset. That's why we store the anchors in <strong class="source-inline">self.anchors</strong>, and <strong class="source-inline">COCO</strong>'s labels in <strong class="source-inline">self.labels</strong>. Also, we rely on the <strong class="source-inline">self._load_yolo()</strong> method (defined later) to build the model.</p></li>
				<li>YOLO is comprised of a series of convolutional blocks and optional skip connections. The  <strong class="source-inline">_conv_block()</strong> helper method allows us to instantiate such<a id="_idIndexMarker925"/> blocks <a id="_idIndexMarker926"/>easily:<p class="source-code">    def _conv_block(self, input, convolutions, </p><p class="source-code">                   skip=True):</p><p class="source-code">        x = input</p><p class="source-code">        count = 0</p><p class="source-code">        for conv in convolutions:</p><p class="source-code">            if count == (len(convolutions) - 2) and </p><p class="source-code">                skip:</p><p class="source-code">                skip_connection = x</p><p class="source-code">            count += 1</p><p class="source-code">            if conv['stride'] &gt; 1:</p><p class="source-code">                x = ZeroPadding2D(((1, 0), (1, 0)))(x)</p><p class="source-code">            x = Conv2D(conv['filter'],</p><p class="source-code">                       conv['kernel'],</p><p class="source-code">                       strides=conv['stride'],</p><p class="source-code">                       padding=('valid' if </p><p class="source-code">                       conv['stride'] &gt; 1</p><p class="source-code">                                else 'same'),</p><p class="source-code">                       </p><p class="source-code">             name=f'conv_{conv["layer_idx"]}',</p><p class="source-code">                       use_bias=(False if </p><p class="source-code">                           conv['bnorm']</p><p class="source-code">                                 else True))(x)</p></li>
				<li>Check if we<a id="_idIndexMarker927"/> need to add batch normalization, leaky<a id="_idIndexMarker928"/> ReLU activations, and skip connections:<p class="source-code">            if conv['bnorm']:</p><p class="source-code">                name = f'bnorm_{conv["layer_idx"]}'</p><p class="source-code">                x = BatchNormalization(epsilon=1e-3,</p><p class="source-code">                                       name=name)(x)</p><p class="source-code">            if conv['leaky']:</p><p class="source-code">                name = f'leaky_{conv["layer_idx"]}'</p><p class="source-code">                x = LeakyReLU(alpha=0.1, name=name)(x)</p><p class="source-code">        return Add()([skip_connection, x]) if skip else x</p></li>
				<li>The <strong class="source-inline">_make_yolov3_architecture()</strong> method, defined as follows, builds the YOLO network by stacking a series of convolutional blocks, using the <strong class="source-inline">_conv_block()</strong> method defined previously:<p class="source-code">    def _make_yolov3_architecture(self):</p><p class="source-code">        input_image = Input(shape=(None, None, 3))</p><p class="source-code">        # Layer  0 =&gt; 4</p><p class="source-code">        x = self._conv_block(input_image, [</p><p class="source-code">            {'filter': 32, 'kernel': 3, 'stride': 1,</p><p class="source-code">             'bnorm': True,</p><p class="source-code">             'leaky': True, 'layer_idx': 0},</p><p class="source-code">            {'filter': 64, 'kernel': 3, 'stride': 2,</p><p class="source-code">             'bnorm': True,</p><p class="source-code">             'leaky': True, 'layer_idx': 1},</p><p class="source-code">            {'filter': 32, 'kernel': 1, 'stride': 1,</p><p class="source-code">             'bnorm': True,</p><p class="source-code">             'leaky': True, 'layer_idx': 2},</p><p class="source-code">            {'filter': 64, 'kernel': 3, 'stride': 1,</p><p class="source-code">             'bnorm': True,</p><p class="source-code">             'leaky': True, 'layer_idx': 3}])</p><p class="source-code">...</p><p>Because this <a id="_idIndexMarker929"/>method is quite large, please refer to <a id="_idIndexMarker930"/>the companion repository for the full implementation.</p></li>
				<li>The <strong class="source-inline">_load_yolo()</strong> method creates the architecture, loads the weights, and instantiates a trained YOLO model in a format TensorFlow understands:<p class="source-code">    def _load_yolo(self):</p><p class="source-code">        model = self._make_yolov3_architecture()</p><p class="source-code">        weight_reader = WeightReader(self.weights_path)</p><p class="source-code">        weight_reader.load_weights(model)</p><p class="source-code">        model.save('model.h5')</p><p class="source-code">        model = load_model('model.h5')</p><p class="source-code">        return model</p></li>
				<li>Define a static method to compute the Sigmoid value of a tensor:<p class="source-code">    @staticmethod</p><p class="source-code">    def _sigmoid(x):</p><p class="source-code">        return 1.0 / (1.0 + np.exp(-x))</p></li>
				<li>The <strong class="source-inline">_decode_net_output()</strong> method decodes the candidate bounding boxes<a id="_idIndexMarker931"/> and<a id="_idIndexMarker932"/> class predictions produced by YOLO: <p class="source-code">    def _decode_net_output(self, </p><p class="source-code">                           network_output,</p><p class="source-code">                           anchors,</p><p class="source-code">                           obj_thresh,</p><p class="source-code">                           network_height,</p><p class="source-code">                           network_width):</p><p class="source-code">      grid_height, grid_width = network_output.shape[:2]</p><p class="source-code">        nb_box = 3</p><p class="source-code">        network_output = network_output.reshape(</p><p class="source-code">            (grid_height, grid_width, nb_box, -1))</p><p class="source-code">        boxes = []</p><p class="source-code">        network_output[..., :2] = \</p><p class="source-code">            self._sigmoid(network_output[..., :2])</p><p class="source-code">        network_output[..., 4:] = \</p><p class="source-code">            self._sigmoid(network_output[..., 4:])</p><p class="source-code">        network_output[..., 5:] = \</p><p class="source-code">            (network_output[..., 4][..., np.newaxis] *</p><p class="source-code">             network_output[..., 5:])</p><p class="source-code">        network_output[..., 5:] *= \</p><p class="source-code">            network_output[..., 5:] &gt; obj_thresh</p><p class="source-code">        for i in range(grid_height * grid_width):</p><p class="source-code">            r = i / grid_width</p><p class="source-code">            c = i % grid_width</p></li>
				<li>We skip those <a id="_idIndexMarker933"/>bounding boxes that don't confidently <a id="_idIndexMarker934"/>describe an object:<p class="source-code">            for b in range(nb_box):</p><p class="source-code">                objectness = \</p><p class="source-code">                    network_output[int(r)][int(c)][b][4]</p><p class="source-code">                if objectness.all() &lt;= obj_thresh:</p><p class="source-code">                    continue</p></li>
				<li>We extract the coordinates and classes from the network output, and use them to create <strong class="source-inline">BoundBox()</strong> instances:<p class="source-code">                x, y, w, h = \</p><p class="source-code">                    network_output[int(r)][int(c)][b][:4]</p><p class="source-code">                x = (c + x) / grid_width</p><p class="source-code">                y = (r + y) / grid_height</p><p class="source-code">                w = (anchors[2 * b] * np.exp(w) /</p><p class="source-code">                     network_width)</p><p class="source-code">                h = (anchors[2 * b + 1] * np.exp(h) /</p><p class="source-code">                     network_height)</p><p class="source-code">               classes = network_output[int(r)][c][b][5:]</p><p class="source-code">                box = BoundBox(x_min=x - w / 2,</p><p class="source-code">                               y_min=y - h / 2,</p><p class="source-code">                               x_max=x + w / 2,</p><p class="source-code">                               y_max=y + h / 2,</p><p class="source-code">                               objness=objectness,</p><p class="source-code">                               classes=classes)</p><p class="source-code">                boxes.append(box)</p><p class="source-code">        return boxes</p></li>
				<li>The <strong class="source-inline">_correct_yolo_boxes()</strong> method rescales the bounding boxes to the dimensions<a id="_idIndexMarker935"/> of<a id="_idIndexMarker936"/> the original image:<p class="source-code">    @staticmethod</p><p class="source-code">    def _correct_yolo_boxes(boxes,</p><p class="source-code">                            image_height,</p><p class="source-code">                            image_width,</p><p class="source-code">                            network_height,</p><p class="source-code">                            network_width):</p><p class="source-code">        new_w, new_h = network_width, network_height</p><p class="source-code">        for i in range(len(boxes)):</p><p class="source-code">            x_offset = (network_width - new_w) / 2.0</p><p class="source-code">            x_offset /= network_width</p><p class="source-code">            x_scale = float(new_w) / network_width</p><p class="source-code">            y_offset = (network_height - new_h) / 2.0</p><p class="source-code">            y_offset /= network_height</p><p class="source-code">            y_scale = float(new_h) / network_height</p><p class="source-code">            boxes[i].xmin = int((boxes[i].xmin - x_     </p><p class="source-code">                                    offset) /</p><p class="source-code">                                x_scale * image_width)</p><p class="source-code">            boxes[i].xmax = int((boxes[i].xmax - x_</p><p class="source-code">                             offset) /x_scale * image_</p><p class="source-code">                                        width)</p><p class="source-code">            boxes[i].ymin = int((boxes[i].ymin - y_</p><p class="source-code">                                offset) /</p><p class="source-code">                                y_scale * image_height)</p><p class="source-code">            boxes[i].ymax = int((boxes[i].ymax - y_</p><p class="source-code">                                 offset) /</p><p class="source-code">                                y_scale * image_height)</p></li>
				<li>We'll perform<a id="_idIndexMarker937"/> NMS in a bit, in order to reduce the <a id="_idIndexMarker938"/>number of redundant detections. For that matter, we need a way to compute the amount of overlap between two intervals:<p class="source-code">    @staticmethod</p><p class="source-code">    def _interval_overlap(interval_a, interval_b):</p><p class="source-code">        x1, x2 = interval_a</p><p class="source-code">        x3, x4 = interval_b</p><p class="source-code">        if x3 &lt; x1:</p><p class="source-code">            if x4 &lt; x1:</p><p class="source-code">                return 0</p><p class="source-code">            else:</p><p class="source-code">                return min(x2, x4) - x1</p><p class="source-code">        else:</p><p class="source-code">            if x2 &lt; x3:</p><p class="source-code">                return 0</p><p class="source-code">            else:</p><p class="source-code">                return min(x2, x4) - x3</p></li>
				<li>Next, we <a id="_idIndexMarker939"/>can <a id="_idIndexMarker940"/>calculate the <strong class="bold">Intersection Over Union</strong> (<strong class="bold">IoU</strong>) between<a id="_idIndexMarker941"/> two bounding boxes, relying on the <strong class="source-inline">_interval_overlap()</strong> method defined before:<p class="source-code">    def _bbox_iou(self, box1, box2):</p><p class="source-code">        intersect_w = self._interval_overlap(</p><p class="source-code">            [box1.xmin, box1.xmax],</p><p class="source-code">            [box2.xmin, box2.xmax])</p><p class="source-code">        intersect_h = self._interval_overlap(</p><p class="source-code">            [box1.ymin, box1.ymax],</p><p class="source-code">            [box2.ymin, box2.ymax])</p><p class="source-code">        intersect = intersect_w * intersect_h</p><p class="source-code">        w1, h1 = box1.xmax - box1.xmin, box1.ymax - box1.ymin</p><p class="source-code">        w2, h2 = box2.xmax - box2.xmin, box2.ymax - box2.ymin</p><p class="source-code">        union = w1 * h1 + w2 * h2 - intersect</p><p class="source-code">        return float(intersect) / union</p></li>
				<li>Armed with<a id="_idIndexMarker942"/> these methods, we can apply NMS to <a id="_idIndexMarker943"/>the bounding boxes in order to keep the number of duplicate detections to a minimum:<p class="source-code">    def _non_max_suppression(self, boxes, nms_thresh):</p><p class="source-code">        if len(boxes) &gt; 0:</p><p class="source-code">            nb_class = len(boxes[0].classes)</p><p class="source-code">        else:</p><p class="source-code">            return</p><p class="source-code">        for c in range(nb_class):</p><p class="source-code">            sorted_indices = np.argsort(</p><p class="source-code">                [-box.classes[c] for box in boxes])</p><p class="source-code">            for i in range(len(sorted_indices)):</p><p class="source-code">                index_i = sorted_indices[i]</p><p class="source-code">                if boxes[index_i].classes[c] == 0:</p><p class="source-code">                    continue</p><p class="source-code">                for j in range(i + 1, </p><p class="source-code">                len(sorted_indices)):</p><p class="source-code">                    index_j = sorted_indices[j]</p><p class="source-code">                    iou = self._bbox_iou(boxes[index_i],</p><p class="source-code">                                         </p><p class="source-code">                    boxes[index_j])</p><p class="source-code">                    if iou &gt;= nms_thresh:</p><p class="source-code">                        boxes[index_j].classes[c] = 0</p></li>
				<li>The <strong class="source-inline">_get_boxes()</strong> method<a id="_idIndexMarker944"/> keeps only those<a id="_idIndexMarker945"/> boxes with a confidence score higher than the <strong class="source-inline">self.class_threshold</strong> method defined in the constructor (0.6 or 60% by default):<p class="source-code">    def _get_boxes(self, boxes):</p><p class="source-code">        v_boxes, v_labels, v_scores = [], [], []</p><p class="source-code">        for box in boxes:</p><p class="source-code">            for i in range(len(self.labels)):</p><p class="source-code">                if box.classes[i] &gt; </p><p class="source-code">               self.class_threshold:</p><p class="source-code">                    v_boxes.append(box)</p><p class="source-code">                    v_labels.append(self.labels[i])</p><p class="source-code">                    v_scores.append(box.classes[i] * </p><p class="source-code">                                      100)</p><p class="source-code">        return v_boxes, v_labels, v_scores</p></li>
				<li><strong class="source-inline">_draw_boxes()</strong> plots the most confident detections in an input image, which means that<a id="_idIndexMarker946"/> each bounding box is accompanied <a id="_idIndexMarker947"/>by its class label and its probability:<p class="source-code">    @staticmethod</p><p class="source-code">    def _draw_boxes(filename, v_boxes, v_labels, </p><p class="source-code">                    v_scores):</p><p class="source-code">        data = plt.imread(filename)</p><p class="source-code">        plt.imshow(data)</p><p class="source-code">        ax = plt.gca()</p><p class="source-code">        for i in range(len(v_boxes)):</p><p class="source-code">            box = v_boxes[i]</p><p class="source-code">            y1, x1, y2, x2 = \</p><p class="source-code">                box.ymin, box.xmin, box.ymax, box.xmax</p><p class="source-code">            width = x2 - x1</p><p class="source-code">            height = y2 - y1</p><p class="source-code">            rectangle = Rectangle((x1, y1), width, </p><p class="source-code">                                 height,</p><p class="source-code">                                  fill=False, </p><p class="source-code">                               color='white')</p><p class="source-code">            ax.add_patch(rectangle)</p><p class="source-code">            label = f'{v_labels[i]} ({v_scores[i]:.3f})'</p><p class="source-code">            plt.text(x1, y1, label, color='green')</p><p class="source-code">        plt.show()</p></li>
				<li>The only public method in the <strong class="source-inline">YOLO()</strong> class is <strong class="source-inline">detect()</strong>, which implements the end-to-end<a id="_idIndexMarker948"/> logic to detect objects in an input <a id="_idIndexMarker949"/>image. First, it passes the image through the model:<p class="source-code">    def detect(self, image, width, height):</p><p class="source-code">        image = np.expand_dims(image, axis=0)</p><p class="source-code">        preds = self.model.predict(image)</p><p class="source-code">        boxes = []</p></li>
				<li>Then, it decodes the outputs of the network:<p class="source-code">        for i in range(len(preds)):</p><p class="source-code">            boxes.extend(</p><p class="source-code">                self._decode_net_output(preds[i][0],</p><p class="source-code">                                    self.anchors[i],</p><p class="source-code">                                self.class_threshold,</p><p class="source-code">                                        416,</p><p class="source-code">                                        416))</p></li>
				<li>Next, it corrects the boxes so that they have proper proportions in relation to the input image. It also applies NMS to get rid of redundant detections:<p class="source-code">        self._correct_yolo_boxes(boxes, height, width, </p><p class="source-code">                                 416,</p><p class="source-code">                                 416)</p><p class="source-code">        self._non_max_suppression(boxes, .5)</p></li>
				<li>Lastly, it gets<a id="_idIndexMarker950"/> the valid bounding boxes and draws<a id="_idIndexMarker951"/> them in the input image:<p class="source-code">        valid_boxes, valid_labels, valid_scores = \</p><p class="source-code">            self._get_boxes(boxes)</p><p class="source-code">        for i in range(len(valid_boxes)):</p><p class="source-code">            print(valid_labels[i], valid_scores[i])</p><p class="source-code">        self._draw_boxes(image_path,</p><p class="source-code">                         valid_boxes,</p><p class="source-code">                         valid_labels,</p><p class="source-code">                         valid_scores)</p></li>
				<li>With the <strong class="source-inline">YOLO()</strong> class defined, we can instantiate it as follows:<p class="source-code">model = YOLO(weights_path='resources/yolov3.weights')</p></li>
				<li>The final step is to iterate over all test images and run the model on them:<p class="source-code">for image_path in glob.glob('test_images/*.jpg'):</p><p class="source-code">    image = load_img(image_path, target_size=(416, </p><p class="source-code">                                              416))</p><p class="source-code">    image = img_to_array(image)</p><p class="source-code">    image = image.astype('float32') / 255.0</p><p class="source-code">    original_image = load_img(image_path)</p><p class="source-code">    width, height = original_image.size</p><p class="source-code">    model.detect(image, width, height)</p><p>Here's the first example: </p></li>
			</ol>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B14768_09_003.jpg" alt="Figure 9.3 – YOLO detected the dog, with a very high confidence score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3 – YOLO detected the dog, with a very high confidence score</p>
			<p>We can observe<a id="_idIndexMarker952"/> that YOLO confidently detected my <a id="_idIndexMarker953"/>dog as such, with a confidence score of 94.5%! Awesome! Let's look at the second test image:</p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B14768_09_004.jpg" alt="Figure 9.4 – YOLO detected multiple objects at varying scales in a single pass&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – YOLO detected multiple objects at varying scales in a single pass</p>
			<p>Even though the result is crowded, a quick glance reveals the network was able to identify both cars in the foreground, as well as the people in the background. This is an<a id="_idIndexMarker954"/> interesting example because it demonstrates <a id="_idIndexMarker955"/>the incredible power of YOLO as an end-to-end object detector, which in a single pass was capable of classifying and localizing many different objects, at varying scales. Impressive, isn't it?</p>
			<p>Let's head to the <em class="italic">How it works…</em> section to connect the dots.</p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor307"/>How it works…</h2>
			<p>In this recipe, we discovered the immense power of end-to-end object detectors— particularly, one of the most famous and impressive of all: YOLO. </p>
			<p>Although YOLO was originally implemented in C++, we leveraged the fantastic Python adaptation by <em class="italic">Huynh Ngoc Anh</em> to perform object detection in our own images using a pre-trained version (specifically, version 3) of this architecture on the seminal <strong class="source-inline">COCO</strong> dataset.</p>
			<p>As you might have noticed, YOLO and many other end-to-end object detectors are very complex networks, but their advantage over traditional approaches such as image pyramids and sliding windows is evident. Not only are the results way better, but they also come through faster thanks to the ability of YOLO to look once at the input image in order to produce all the relevant detections.</p>
			<p>But what if you want to train an end-to-end object detector on your own data? Are you doomed to rely on out-of-the-box solutions? Do you need to spend hours deciphering cryptic papers in order to implement such networks? </p>
			<p>Well, that's <a id="_idIndexMarker956"/>one option, but there's another one, which we'll<a id="_idIndexMarker957"/> explore in the next recipe, and it entails the TensorFlow Object Detection API, an experimental repository of state-of-the-art architectures that will ease and boost your object detection endeavors!</p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor308"/>See also</h2>
			<p>YOLO is a milestone when it comes to deep learning and object detection, so reading the paper is a pretty smart time investment. You can find it here: </p>
			<p><a href="https://arxiv.org/abs/1506.02640">https://arxiv.org/abs/1506.02640</a> </p>
			<p>You can learn more about YOLO directly from the author's website, here: </p>
			<p><a href="https://pjreddie.com/darknet/yolo/">https://pjreddie.com/darknet/yolo/</a> </p>
			<p>If you are interested in exploring <strong class="source-inline">keras-yolo3</strong>, the tool we based our implementation on, refer to this link: </p>
			<p><a href="https://github.com/experiencor/keras-yolo3">https://github.com/experiencor/keras-yolo3</a></p>
			<h1 id="_idParaDest-262"><a id="_idTextAnchor309"/>Training your own object detector with TensorFlow's Object Detection API</h1>
			<p>It's no <a id="_idIndexMarker958"/>secret that modern <a id="_idIndexMarker959"/>object detectors rank among the most complex and challenging architectures to implement and get it right! However, that doesn't mean we can't take advantage of the most recent advancements in this domain in order to train object detectors on our own datasets. <em class="italic">How?</em>, you ask. Enter TensorFlow's Object Detection API!</p>
			<p>In this recipe, we'll install this API, prepare a custom dataset for training, tweak a couple of configuration files, and use the resulting model to localize objects on test images. This recipe is a bit different from the ones you've worked on so far, because we'll be switching back and forth between Python and the command line. </p>
			<p>Are you ready? Then let's get started.</p>
			<h2 id="_idParaDest-263"><a id="_idTextAnchor310"/>Getting ready</h2>
			<p>There are several dependencies we need to install for this recipe to work. Let's begin with the most important one: the TensorFlow Object Detection API. First, <strong class="source-inline">cd</strong> to a location of <a id="_idIndexMarker960"/>your preference and <a id="_idIndexMarker961"/>clone the <strong class="source-inline">tensorflow/models</strong> repository:</p>
			<p class="source-code">$&gt; git clone –-depth 1 https://github.com/tensorflow/models</p>
			<p>Next, install the TensorFlow Object Detection API, like this:</p>
			<p class="source-code">$&gt; sudo apt install -y protobuf-compiler</p>
			<p class="source-code">$&gt; cd models/research</p>
			<p class="source-code">$&gt; protoc object_detection/protos/*.proto –-python_out=.</p>
			<p class="source-code">$&gt; cp object_detection/packages/tf2/setup.py .</p>
			<p class="source-code">$&gt; python -m pip install -q . </p>
			<p>For the purposes of this recipe, we'll assume it's installed at the same level as the <strong class="source-inline">ch9</strong> folder (https://github.com/PacktPublishing/Tensorflow-2.0-Computer-Vision-Cookbook/tree/master/ch9). Now, we must install <strong class="source-inline">pandas</strong> and <strong class="source-inline">Pillow</strong>:</p>
			<p class="source-code">$&gt; pip install pandas Pillow</p>
			<p>The dataset we will use is <strong class="source-inline">Fruit Images for Object Detection</strong>, hosted on Kaggle, which you can access here: <a href="https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection">https://www.kaggle.com/mbkinaci/fruit-images-for-object-detection</a>. Log in or sign up and download the data to a location of your preference as <strong class="source-inline">fruits.zip</strong> (the data is available in the <strong class="source-inline">ch9/recipe3</strong> folder in the companion repository for this book). Finally, decompress it: </p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B14768_09_005.jpg" alt="Figure 9.5 – Sample images of the three classes in the dataset: apple, orange, and banana&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Sample images of the three classes in the dataset: apple, orange, and banana</p>
			<p>The labels in this dataset are in <strong class="bold">Pascal VOC</strong> format, where <strong class="bold">VOC</strong> stands for <strong class="bold">Visual Object Classes</strong>. Refer to the <em class="italic">See also…</em> section to learn more about it. </p>
			<p>Now, we're all set! Let's begin implementing.</p>
			<h2 id="_idParaDest-264"><a id="_idTextAnchor311"/>How to do it…</h2>
			<p>By the end of these steps, you'll have trained your own state-of-the-art object detector using the TensorFlow Object Detection API:</p>
			<ol>
				<li value="1">We'll work with two files in this recipe: the first one is used to prepare the data (you can find it as <strong class="source-inline">prepare.py</strong> in the repository), and the second one is used to make<a id="_idIndexMarker962"/> inferences <a id="_idIndexMarker963"/>with the object detector (<strong class="source-inline">inference.py</strong> in the repository). Open <strong class="source-inline">prepare.py</strong> and import all the needed packages:<p class="source-code">import glob</p><p class="source-code">import io</p><p class="source-code">import os</p><p class="source-code">from collections import namedtuple</p><p class="source-code">from xml.etree import ElementTree as tree</p><p class="source-code">import pandas as pd</p><p class="source-code">import tensorflow.compat.v1 as tf</p><p class="source-code">from PIL import Image</p><p class="source-code">from object_detection.utils import dataset_util</p></li>
				<li>Define the <strong class="source-inline">encode_class()</strong> function, which maps the text labels to their integer counterparts:<p class="source-code">def encode_class(row_label):</p><p class="source-code">    class_mapping = {'apple': 1, 'orange': 2, </p><p class="source-code">                     'banana': 3}</p><p class="source-code">    return class_mapping.get(row_label, None)</p></li>
				<li>Define a function to split a dataframe of labels (which we'll create later) into groups:<p class="source-code">def split(df, group):</p><p class="source-code">    Data = namedtuple('data', ['filename', 'object'])</p><p class="source-code">    groups = df.groupby(group)</p><p class="source-code">    return [Data(filename, groups.get_group(x))</p><p class="source-code">            for filename, x</p><p class="source-code">            in zip(groups.groups.keys(), </p><p class="source-code">          groups.groups)]</p></li>
				<li>The <a id="_idIndexMarker964"/>TensorFlow <a id="_idIndexMarker965"/>Object Detection API works with a data structure known as <strong class="source-inline">tf.train.Example</strong>. The next function takes the path to an image and its label (which is the set of bounding boxes and the ground-truth classes of all objects contained in it) and creates the corresponding <strong class="source-inline">tf.train.Example</strong>. First, load the image and its properties:<p class="source-code">def create_tf_example(group, path):</p><p class="source-code">    groups_path = os.path.join(path, f'{group.filename}')</p><p class="source-code">    with tf.gfile.GFile(groups_path, 'rb') as f:</p><p class="source-code">        encoded_jpg = f.read()</p><p class="source-code">    image = Image.open(io.BytesIO(encoded_jpg))</p><p class="source-code">    width, height = image.size</p><p class="source-code">    filename = group.filename.encode('utf8')</p><p class="source-code">    image_format = b'jpg'</p></li>
				<li>Now, store<a id="_idIndexMarker966"/> the<a id="_idIndexMarker967"/> dimensions of the bounding boxes, along with the classes of each object contained in the image:<p class="source-code">    xmins = []</p><p class="source-code">    xmaxs = []</p><p class="source-code">    ymins = []</p><p class="source-code">    ymaxs = []</p><p class="source-code">    classes_text = []</p><p class="source-code">    classes = []</p><p class="source-code">    for index, row in group.object.iterrows():</p><p class="source-code">        xmins.append(row['xmin'] / width)</p><p class="source-code">        xmaxs.append(row['xmax'] / width)</p><p class="source-code">        ymins.append(row['ymin'] / height)</p><p class="source-code">        ymaxs.append(row['ymax'] / height)</p><p class="source-code">        classes_text.append(row['class'].encode('utf8'))</p><p class="source-code">        classes.append(encode_class(row['class']))</p></li>
				<li>Create <a id="_idIndexMarker968"/>a <strong class="source-inline">tf.train.Features</strong> object <a id="_idIndexMarker969"/>that will contain relevant information about the image and its objects:<p class="source-code">    features = tf.train.Features(feature={</p><p class="source-code">        'image/height':</p><p class="source-code">            dataset_util.int64_feature(height),</p><p class="source-code">        'image/width':</p><p class="source-code">            dataset_util.int64_feature(width),</p><p class="source-code">        'image/filename':</p><p class="source-code">            dataset_util.bytes_feature(filename),</p><p class="source-code">        'image/source_id':</p><p class="source-code">            dataset_util.bytes_feature(filename),</p><p class="source-code">        'image/encoded':</p><p class="source-code">            dataset_util.bytes_feature(encoded_jpg),</p><p class="source-code">        'image/format':</p><p class="source-code">            dataset_util.bytes_feature(image_format),</p><p class="source-code">        'image/object/bbox/xmin':</p><p class="source-code">            dataset_util.float_list_feature(xmins),</p><p class="source-code">        'image/object/bbox/xmax':</p><p class="source-code">            dataset_util.float_list_feature(xmaxs),</p><p class="source-code">        'image/object/bbox/ymin':</p><p class="source-code">            dataset_util.float_list_feature(ymins),</p><p class="source-code">        'image/object/bbox/ymax':</p><p class="source-code">            dataset_util.float_list_feature(ymaxs),</p><p class="source-code">        'image/object/class/text':</p><p class="source-code">           dataset_util.bytes_list_feature(classes_text),</p><p class="source-code">        'image/object/class/label':</p><p class="source-code">            dataset_util.int64_list_feature(classes)</p><p class="source-code">    })</p></li>
				<li>Return a <strong class="source-inline">tf.train.Example</strong> structure initialized with the features created previously:<p class="source-code">    return tf.train.Example(features=features)</p></li>
				<li>Define a <a id="_idIndexMarker970"/>function to <a id="_idIndexMarker971"/>transform <a id="_idIndexMarker972"/>an <strong class="bold">Extensible Markup Language</strong> (<strong class="bold">XML</strong>) file—with information about the bounding boxes in an image—to an<a id="_idIndexMarker973"/> equivalent one in <strong class="bold">Comma-Separated Values</strong> (<strong class="bold">CSV</strong>) format:<p class="source-code">def bboxes_to_csv(path):</p><p class="source-code">    xml_list = []</p><p class="source-code">    bboxes_pattern = os.path.sep.join([path, '*.xml'])</p><p class="source-code">    for xml_file in glob.glob(bboxes_pattern):</p><p class="source-code">        t = tree.parse(xml_file)</p><p class="source-code">        root = t.getroot()</p><p class="source-code">        for member in root.findall('object'):</p><p class="source-code">            value = (root.find('filename').text,</p><p class="source-code">                     int(root.find('size')[0].text),</p><p class="source-code">                     int(root.find('size')[1].text),</p><p class="source-code">                     member[0].text,</p><p class="source-code">                     int(member[4][0].text),</p><p class="source-code">                     int(member[4][1].text),</p><p class="source-code">                     int(member[4][2].text),</p><p class="source-code">                     int(member[4][3].text))</p><p class="source-code">            xml_list.append(value)</p><p class="source-code">    column_names = ['filename', 'width', 'height', </p><p class="source-code">            'class','xmin', 'ymin', 'xmax', 'ymax']</p><p class="source-code">    df = pd.DataFrame(xml_list, columns=column_names)</p><p class="source-code">    return df</p></li>
				<li>Iterate<a id="_idIndexMarker974"/> over<a id="_idIndexMarker975"/> the <strong class="source-inline">test</strong> and <strong class="source-inline">train</strong> subsets in the <strong class="source-inline">fruits</strong> folder, converting the labels from CSV to XML:<p class="source-code">base = 'fruits'</p><p class="source-code">for subset in ['test', 'train']:</p><p class="source-code">    folder = os.path.sep.join([base, f'{subset}_zip', </p><p class="source-code">                               subset])</p><p class="source-code">    labels_path = os.path.sep.join([base,f'{subset}_</p><p class="source-code">                                       labels.           </p><p class="source-code">                                       csv'])</p><p class="source-code">    bboxes_df = bboxes_to_csv(folder)</p><p class="source-code">    bboxes_df.to_csv(labels_path, index=None)</p></li>
				<li>Then, use the same labels to produce the <strong class="source-inline">tf.train.Examples </strong>corresponding to the current subset of data being processed:<p class="source-code">    writer = (tf.python_io.</p><p class="source-code">            TFRecordWriter(f'resources/{subset}.record'))</p><p class="source-code">    examples = pd.read_csv(f'fruits/{subset}_labels.csv')</p><p class="source-code">    grouped = split(examples, 'filename')</p><p class="source-code">    path = os.path.join(f'fruits/{subset}_zip/{subset}')</p><p class="source-code">    for group in grouped:</p><p class="source-code">        tf_example = create_tf_example(group, path)</p><p class="source-code">        writer.write(tf_example.SerializeToString())</p><p class="source-code">    writer.close()</p></li>
				<li>After<a id="_idIndexMarker976"/> running <a id="_idIndexMarker977"/>the <strong class="source-inline">prepare.py</strong> script implemented in <em class="italic">Step 1</em> through <em class="italic">Step 10</em>, you'll have the data in the necessary shape for the TensorFlow Object Detection API to train on it. The next step is to download the weights of <strong class="source-inline">EfficientDet</strong>, a state-of-the-art <a id="_idIndexMarker978"/>architecture we'll fine-tune shortly. Download the weights from this <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>), and then decompress them into a location of your preference: <a href="http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz">http://download.tensorflow.org/models/object_detection/tf2/20200711/efficientdet_d0_coco17_tpu-32.tar.gz</a>. I placed them in my <strong class="source-inline">Desktop</strong> folder.</li>
				<li>Create a file to map the classes to integers. Name it <strong class="source-inline">label_map.txt</strong> and place it inside <strong class="source-inline">ch9/recipe3/resources</strong>:<p class="source-code">item {</p><p class="source-code">    id: 1</p><p class="source-code">    name: 'apple'</p><p class="source-code">}</p><p class="source-code">item {</p><p class="source-code">    id: 2</p><p class="source-code">    name: 'orange'</p><p class="source-code">}</p><p class="source-code">item {</p><p class="source-code">    id: 3</p><p class="source-code">    name: 'banana'</p><p class="source-code">}</p></li>
				<li>Next, we <a id="_idIndexMarker979"/>must change <a id="_idIndexMarker980"/>the configuration file for this network to adapt it to our dataset. You can either locate it in <strong class="source-inline">models/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config</strong> (assuming you installed the TensorFlow Object Detection API at the same level of the <strong class="source-inline">ch9</strong> folder in the companion repository), or download it directly from this URL: <a href="https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config">https://github.com/tensorflow/models/blob/master/research/object_detection/configs/tf2/ssd_efficientdet_d0_512x512_coco17_tpu-8.config</a>. Whichever option you choose, place a copy inside <strong class="source-inline">ch9/recipe3/resources</strong> and modify <em class="italic">line 13</em> to reflect the number of classes in our dataset:<p class="source-code">num_classes: 3</p><p>Then, modify <em class="italic">line 140</em> to point to the <strong class="source-inline">EfficientDet</strong> weights we downloaded in <em class="italic">Step 7</em>:</p><p class="source-code">fine_tune_checkpoint: "/home/jesus/Desktop/efficientdet_d0_coco17_tpu-32/checkpoint/ckpt-0"</p><p>Change <strong class="source-inline">fine_tune_checkpoint_type</strong> from <strong class="source-inline">classification</strong> to <strong class="source-inline">detection</strong> on <em class="italic">line 143</em>:</p><p class="source-code">fine_tune_checkpoint_type: "detection"</p><p>Modify <em class="italic">line 180</em> to point to the <strong class="source-inline">label_map.txt</strong> file created in <em class="italic">Step 8</em>:</p><p class="source-code">label_map_path: "/home/jesus/Desktop/tensorflow-computer-vision/ch9/recipe3/resources/label_map.txt"</p><p>Modify <em class="italic">line 182</em> to <a id="_idIndexMarker981"/>point<a id="_idIndexMarker982"/> to the <strong class="source-inline">train.record</strong> file created in <em class="italic">Step 11</em>, corresponding to the prepared training data:</p><p class="source-code">input_path: "/home/jesus/Desktop/tensorflow-computer-vision/ch9/recipe3/resources/train.record"</p><p>Modify <em class="italic">line 193</em> to point to the <strong class="source-inline">label_map.txt</strong> file created in <em class="italic">Step 12</em>:</p><p class="source-code">label_map_path: "/home/jesus/Desktop/tensorflow-computer-vision/ch9/recipe3/resources/label_map.txt"</p><p>Modify <em class="italic">line 197</em> to point to the <strong class="source-inline">test.record</strong> file created in <em class="italic">Step 11</em>, corresponding to the prepared test data:</p><p class="source-code">input_path: "/home/jesus/Desktop/tensorflow-computer-vision/ch9/recipe3/resources/test.record"</p></li>
				<li>Time to train the model! First, assuming you're at the root level of the companion repository, <strong class="source-inline">cd</strong> into the <strong class="source-inline">object_detection</strong> folder in the TensorFlow Object Detection API: <p class="source-code">$&gt; cd models/research/object_detection</p><p>Then, train the model with this command:</p><p class="source-code">$&gt; python model_main_tf2.py --pipeline_config_path=../../../ch9/recipe3/resources/ssd_efficientdet_d0_512x512_coco17_tpu-8.config --model_dir=../../../ch9/recipe3/training --num_train_steps=10000</p><p>Here, we are training the model for <strong class="source-inline">10000</strong> steps. Also, we'll save the results in the <strong class="source-inline">training</strong> folder inside <strong class="source-inline">ch9/recipe3</strong>. Finally, we're specifying the location of the configuration file with the  <strong class="source-inline">--pipeline_config_path</strong> option. This step will take several hours.</p></li>
				<li>Once the<a id="_idIndexMarker983"/> network <a id="_idIndexMarker984"/>has been fine-tuned, we must export it as a frozen graph in order to use it for inference. For that matter, <strong class="source-inline">cd</strong> once again to the <strong class="source-inline">object_detection</strong> folder in the TensorFlow Object Detection API: <p class="source-code">$&gt; cd models/research/object_detection</p><p>Now, execute the following command:</p><p class="source-code">$&gt; python exporter_main_v2.py --trained_checkpoint_dir=../../../ch9/recipe3/training/ --pipeline_config_path=../../../ch9/recipe3/resources/ssd_efficientdet_d0_512x512_coco17_tpu-8.config --output_directory=../../../ch9/recipe3/resources/inference_graph</p><p>The <strong class="source-inline">trained_checkpoint_dir</strong> parameter is used to point to the location where the trained model is, while <strong class="source-inline">pipeline_config_path</strong> points to the model's configuration file. Finally, the frozen inference graph will be saved inside the <strong class="source-inline">ch9/recipe3/resources/inference_graph</strong> folder, as stated by the <strong class="source-inline">output_directory</strong> flag.</p></li>
				<li>Open a file named <strong class="source-inline">inference.py</strong>, and import all the relevant dependencies:<p class="source-code">import glob</p><p class="source-code">import random</p><p class="source-code">from io import BytesIO</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from PIL import Image</p><p class="source-code">from object_detection.utils import ops</p><p class="source-code">from object_detection.utils import visualization_utils as viz</p><p class="source-code">from object_detection.utils.label_map_util import \</p><p class="source-code">    create_category_index_from_labelmap</p></li>
				<li>Define<a id="_idIndexMarker985"/> a function to<a id="_idIndexMarker986"/> load an image from disk as a NumPy array:<p class="source-code">def load_image(path):</p><p class="source-code">    image_data = tf.io.gfile.GFile(path, 'rb').read()</p><p class="source-code">    image = Image.open(BytesIO(image_data))</p><p class="source-code">    width, height = image.size</p><p class="source-code">    shape = (height, width, 3)</p><p class="source-code">    image = np.array(image.getdata())</p><p class="source-code">    image = image.reshape(shape).astype('uint8')</p><p class="source-code">    return image</p></li>
				<li>Define a function to run the model on a single image. First, convert the image into a tensor:<p class="source-code">def infer_image(net, image):</p><p class="source-code">    image = np.asarray(image)</p><p class="source-code">    input_tensor = tf.convert_to_tensor(image)</p><p class="source-code">    input_tensor = input_tensor[tf.newaxis, ...]</p></li>
				<li>Pass the <a id="_idIndexMarker987"/>tensor to <a id="_idIndexMarker988"/>the network, extract the number of detections, and keep as many values in the resulting dictionary as there are detections:<p class="source-code">    num_detections = int(result.pop('num_detections'))</p><p class="source-code">    result = {key: value[0, :num_detections].numpy()</p><p class="source-code">              for key, value in result.items()}</p><p class="source-code">    result['num_detections'] = num_detections</p><p class="source-code">    result['detection_classes'] = \</p><p class="source-code">        result['detection_classes'].astype('int64')</p></li>
				<li>If there are detection masks present, reframe them to image masks and return the results:<p class="source-code">    if 'detection_masks' in result:</p><p class="source-code">        detection_masks_reframed = \</p><p class="source-code">            ops.reframe_box_masks_to_image_masks(</p><p class="source-code">                result['detection_masks'],</p><p class="source-code">                result['detection_boxes'],</p><p class="source-code">                image.shape[0],</p><p class="source-code">                image.shape[1])</p><p class="source-code">        detection_masks_reframed = \</p><p class="source-code">            tf.cast(detection_masks_reframed &gt; 0.5, </p><p class="source-code">                    tf.uint8)</p><p class="source-code">        result['detection_masks_reframed'] = \</p><p class="source-code">            detection_masks_reframed.numpy()</p><p class="source-code">    return result</p></li>
				<li>Create a<a id="_idIndexMarker989"/> category<a id="_idIndexMarker990"/> index from the <strong class="source-inline">label_map.txt</strong> file we created in <em class="italic">Step 12</em>, and also load the model from the frozen inference graph produced in <em class="italic">Step 15</em>:<p class="source-code">labels_path = 'resources/label_map.txt'</p><p class="source-code">CATEGORY_IDX = \</p><p class="source-code">    create_category_index_from_labelmap(labels_path,</p><p class="source-code">                                  use_display_name=True)</p><p class="source-code">model_path = 'resources/inference_graph/saved_model'</p><p class="source-code">model = tf.saved_model.load(model_path)</p></li>
				<li>Pick three random test images:<p class="source-code">test_images = list(glob.glob('fruits/test_zip/test/*.jpg'))</p><p class="source-code">random.shuffle(test_images)</p><p class="source-code">test_images = test_images[:3]</p></li>
				<li>Run the model over the sample images, and save the resulting detections:<p class="source-code">for image_path in test_images:</p><p class="source-code">    image = load_image(image_path)</p><p class="source-code">    result = infer_image(model, image)</p><p class="source-code">    masks = result.get('detection_masks_reframed', </p><p class="source-code">                        None)</p><p class="source-code">    viz.visualize_boxes_and_labels_on_image_array(</p><p class="source-code">        image,</p><p class="source-code">        result['detection_boxes'],</p><p class="source-code">        result['detection_classes'],</p><p class="source-code">        result['detection_scores'],</p><p class="source-code">        CATEGORY_IDX,</p><p class="source-code">        instance_masks=masks,</p><p class="source-code">        use_normalized_coordinates=True,</p><p class="source-code">        line_thickness=5)</p><p class="source-code">    plt.figure(figsize=(24, 32))</p><p class="source-code">    plt.imshow(image)</p><p class="source-code">    plt.savefig(f'detections_{image_path.split("/")[-1]}')</p><p>We see <a id="_idIndexMarker991"/>the<a id="_idIndexMarker992"/> results in <em class="italic">Figure 9.6</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B14768_09_006.jpg" alt="Figure 9.6 – EfficientDet detection results on a random sample of test images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – EfficientDet detection results on a random sample of test images</p>
			<p>We can see in <em class="italic">Figure 9.6</em> that our fine-tuned network produced fairly accurate and confident detections. Considering we only concerned ourselves with data preparation and inference, and that regarding the architecture itself we just adapted a configuration file to our needs, the results are pretty impressive!</p>
			<p>Let's move on to the <em class="italic">How it works…</em> section.</p>
			<h2 id="_idParaDest-265"><a id="_idTextAnchor312"/>How it works…</h2>
			<p>In this recipe, we discovered that training an object detector is a hard and challenging feat. The good news, however, is that we have the TensorFlow Object Detection API at our disposal to train a wide range of vanguardist networks.</p>
			<p>Because the<a id="_idIndexMarker993"/> TensorFlow <a id="_idIndexMarker994"/>Object Detection API is an experimental tool, it uses different conventions than regular TensorFlow, and therefore in order to use it, we need to perform a little bit of processing work on the input data to put it into a shape that the API understands. This is done by converting the labels in the <strong class="source-inline">Fruits for Object Detection</strong> dataset (originally in XML format) to CSV and then into serialized <strong class="source-inline">tf.train.Example</strong> objects.  </p>
			<p>Then, to use the trained model, we exported it as an inference graph using the <strong class="source-inline">exporter_main_v2.py</strong> script and leveraged some of the visualization tools in the API to display the detections on the sample test images.</p>
			<p>What about the training? This is arguably the easiest part, entailing three major steps:</p>
			<ul>
				<li>Creating a mapping from text labels to integers (<em class="italic">Step 12</em>)</li>
				<li>Modifying the configuration file corresponding to the model to fine-tune it in all the relevant places (<em class="italic">Step 13</em>)</li>
				<li>Running the <strong class="source-inline">model_main_tf2.py</strong> file to train the network, passing it the proper parameters (<em class="italic">Step 14</em>)</li>
			</ul>
			<p>This recipe <a id="_idIndexMarker995"/>provides you<a id="_idIndexMarker996"/> with a template you can tweak and adapt to train virtually any modern object detector (supported by the API) on any dataset of your choosing. Pretty cool, right?</p>
			<h2 id="_idParaDest-266"><a id="_idTextAnchor313"/>See also</h2>
			<p>You can learn more<a id="_idIndexMarker997"/> about the TensorFlow Object Detection API here: </p>
			<p><a href="https://github.com/tensorflow/models/tree/master/research/object_detection">https://github.com/tensorflow/models/tree/master/research/object_detection</a> </p>
			<p>Also, I encourage you to read this great article to learn more about <strong class="source-inline">EfficientDet</strong>: </p>
			<p><a href="https://towardsdatascience.com/a-thorough-breakdown-of-efficientdet-for-object-detection-dc6a15788b73">https://towardsdatascience.com/a-thorough-breakdown-of-efficientdet-for-object-detection-dc6a15788b73</a> </p>
			<p>If you want to learn a great<a id="_idIndexMarker998"/> de<a id="_idTextAnchor314"/><a id="_idTextAnchor315"/>al about the <strong class="bold">Pascal VOC</strong> format, then you must watch this video: </p>
			<p><a href="https://www.youtube.com/watch?v=-f6TJpHcAeM">https://www.youtube.com/watch?v=-f6TJpHcAeM</a> </p>
			<h1 id="_idParaDest-267"><a id="_idTextAnchor316"/>Detecting objects using TFHub</h1>
			<p>TFHub is a<a id="_idIndexMarker999"/> cornucopia of state-of-the-art models<a id="_idIndexMarker1000"/> when it comes to object detection. As we'll discover in this recipe, using them to spot elements of interest in our images is a fairly straightforward task, especially considering they've been trained on the gigantic <strong class="source-inline">COCO</strong> dataset, which make them an excellent choice for out-of-the-box object detection.</p>
			<h2 id="_idParaDest-268"><a id="_idTextAnchor317"/>Getting ready</h2>
			<p>First, we must install <strong class="source-inline">Pillow</strong> and TFHub, as follows:</p>
			<p class="source-code">$&gt; pip install Pillow tensorflow-hub</p>
			<p>Also, because some visualization tools we'll use live in the TensorFlow Object Detection API, we must install it. First, <strong class="source-inline">cd</strong> to a location of your preference and clone the <strong class="source-inline">tensorflow/models</strong> repository:</p>
			<p class="source-code">$&gt; git clone –-depth 1 https://github.com/tensorflow/models</p>
			<p>Next, install the TensorFlow Object Detection API, like this:</p>
			<p class="source-code">$&gt; sudo apt install -y protobuf-compiler</p>
			<p class="source-code">$&gt; cd models/research</p>
			<p class="source-code">$&gt; protoc object_detection/protos/*.proto –-python_out=.</p>
			<p class="source-code">$&gt; cp object_detection/packages/tf2/setup.py .</p>
			<p class="source-code">$&gt; python -m pip install -q . </p>
			<p>That's it! Let's get started.</p>
			<h2 id="_idParaDest-269"><a id="_idTextAnchor318"/>How to do it…</h2>
			<p>Follow <a id="_idIndexMarker1001"/>these steps to learn how to use TFHub <a id="_idIndexMarker1002"/>to detect objects in your own photos:</p>
			<ol>
				<li value="1">Import the packages we'll need:<p class="source-code">import glob</p><p class="source-code">from io import BytesIO</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import tensorflow_hub as hub</p><p class="source-code">from PIL import Image</p><p class="source-code">from object_detection.utils import visualization_utils as viz</p><p class="source-code">from object_detection.utils.label_map_util import \</p><p class="source-code">    create_category_index_from_labelmap</p></li>
				<li>Define a function to load an image into a NumPy array:<p class="source-code">def load_image(path):</p><p class="source-code">    image_data = tf.io.gfile.GFile(path, 'rb').read()</p><p class="source-code">    image = Image.open(BytesIO(image_data))</p><p class="source-code">    width, height = image.size</p><p class="source-code">    shape = (1, height, width, 3)</p><p class="source-code">    image = np.array(image.getdata())</p><p class="source-code">    image = image.reshape(shape).astype('uint8')</p><p class="source-code">    return image</p></li>
				<li>Define a<a id="_idIndexMarker1003"/> function to make <a id="_idIndexMarker1004"/>predictions with a model, and save the results to disk. Start by loading the image and passing it through the model:<p class="source-code">def get_and_save_predictions(model, image_path):</p><p class="source-code">    image = load_image(image_path)</p><p class="source-code">    results = model(image)</p></li>
				<li>Convert the results to NumPy arrays:<p class="source-code">model_output = {k: v.numpy() for k, v in results.items()}</p></li>
				<li>Create a visualization of the detections with their boxes, scores, and classes:<p class="source-code">    boxes = model_output['detection_boxes'][0]</p><p class="source-code">    classes = \</p><p class="source-code">       model_output['detection_classes'][0].astype('int')</p><p class="source-code">    scores = model_output['detection_scores'][0]</p><p class="source-code">   </p><p class="source-code">    clone = image.copy()</p><p class="source-code">    viz.visualize_boxes_and_labels_on_image_array(</p><p class="source-code">        image=clone[0],</p><p class="source-code">        boxes=boxes,</p><p class="source-code">        classes=classes,</p><p class="source-code">        scores=scores,</p><p class="source-code">        category_index=CATEGORY_IDX,</p><p class="source-code">        use_normalized_coordinates=True,</p><p class="source-code">        max_boxes_to_draw=200,</p><p class="source-code">        min_score_thresh=0.30,</p><p class="source-code">        agnostic_mode=False,</p><p class="source-code">        line_thickness=5</p><p class="source-code">    )</p></li>
				<li>Save <a id="_idIndexMarker1005"/>the<a id="_idIndexMarker1006"/> result to disk:<p class="source-code">    plt.figure(figsize=(24, 32))</p><p class="source-code">    plt.imshow(image_with_mask[0])</p><p class="source-code">    plt.savefig(f'output/{image_path.split("/")[-1]}')</p></li>
				<li>Load <strong class="source-inline">COCO</strong>'s category index:<p class="source-code">labels_path = 'resources/mscoco_label_map.pbtxt'</p><p class="source-code">CATEGORY_IDX =create_category_index_from_labelmap(labels_path)</p></li>
				<li>Load Faster R-CNN from TFHub:<p class="source-code">MODEL_PATH = ('https://tfhub.dev/tensorflow/faster_rcnn/'</p><p class="source-code">              'inception_resnet_v2_1024x1024/1')</p><p class="source-code">model = hub.load(MODEL_PATH)</p></li>
				<li>Run<a id="_idIndexMarker1007"/> Faster R-CNN over all test<a id="_idIndexMarker1008"/> images: <p class="source-code">test_images_paths = glob.glob('test_images/*')</p><p class="source-code">for image_path in test_images_paths:</p><p class="source-code">    get_and_save_predictions(model, image_path)</p><p>After a while, the labeled images should be in the <strong class="source-inline">output</strong> folder. The first example showcases the power of the network, which detected with 100% confidence the two elephants in the photo:</p></li>
			</ol>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B14768_09_007.jpg" alt="Figure 9.7 – Both elephants were detected, with a perfect score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Both elephants were detected, with a perfect score</p>
			<p>However, there are instances where the model makes some mistakes, like this:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B14768_09_008.jpg" alt="Figure 9.8 – The network mistakenly detected a person in the tablecloth&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.8 – The network mistakenly detected a person in the tablecloth</p>
			<p>In this example, the network detected a person in the tablecloth, with 42% certainty, although it correctly identified my dog as a Pug, with 100% accuracy. This, and <a id="_idIndexMarker1009"/>other false positives, can<a id="_idIndexMarker1010"/> be prevented by increasing the <strong class="source-inline">min_score_thresh</strong> value passed to the <strong class="source-inline">visualize_boxes_and_labels_on_image_array()</strong> method in <em class="italic">Step 5</em>.</p>
			<p>Let's head to the next section.</p>
			<h2 id="_idParaDest-270"><a id="_idTextAnchor319"/>How it works…</h2>
			<p>In this recipe, we leveraged the ease of use of the powerful models that live in TFHub to perform out-of-the-box object detection with fairly good results. </p>
			<p>Why should we consider TFHub a viable option to satisfy our object detection needs? Well, the vast majority of the models there are really challenging to implement when starting from scratch, let alone training them to achieve decent results. On top of this, these complex architectures have been trained on <strong class="source-inline">COCO</strong>, a massive corpus of images tailored for object detection and image segmentation tasks. Nevertheless, we must keep in mind that we cannot retrain these networks and, therefore, they will work best on images containing objects that exist in <strong class="source-inline">COCO</strong>. If we need to create our own custom object detectors, the <a id="_idIndexMarker1011"/>other strategies covered in this<a id="_idIndexMarker1012"/> chapter should suffice.</p>
			<h2 id="_idParaDest-271"><a id="_idTextAnchor320"/>See also</h2>
			<p>You can access<a id="_idIndexMarker1013"/> the list of all available object detectors in TFHub here:</p>
			<p><a href="https://tfhub.dev/tensorflow/collections/object_detection/1">https://tfhub.dev/tensorflow/collections/object_detection/1</a></p>
		</div>
	</body></html>