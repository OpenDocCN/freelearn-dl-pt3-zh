- en: '*Chapter 3*: Generative Adversarial Network'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative Adversarial Network**, more commonly known as **GANs**, are currently
    the most prominent method in image and video generation. As the inventor of the
    convolutional neural network, Dr. Yann LeCun, said in 2016, *"...it is the most
    interesting idea in the last 10 years in machine learning."* The images generated
    using GANs are superior, in terms of realism, to other competing technologies
    and things have advanced tremendously since their invention in 2014 by then graduate
    student Ian Goodfellow.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first learn about the fundamentals of GANs and build
    a DCGAN to generate Fashion MNIST. We'll learn about the challenges in training
    GANs. Finally, we will learn how to build a WGAN and its variant, WGAN-GP, to
    resolve many of the challenges involved in generating faces.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the fundamentals of GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Deep Convolutional GAN (DCGAN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in training GANs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a Wasserstein GAN (WGAN)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Jupyter notebooks and code can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter03](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter03)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebooks used in the chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ch3_dcgan.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ch3_mode_collapse`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ch3_wgan_fashion_mnist.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ch3_wgan_gp_fashion_mnist.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ch3_wgan_gp_celeb_a.ipynb`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the fundamentals of GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of generative models is to learn a data distribution and to sample
    from it to generate new data. With the models that we looked at in the previous
    chapters, namely PixelCNN and VAE, their generative part gets to look at the image
    distribution during training. Thus, they are known as **explicit density models**.
    In contrast, the generative part in a GAN never gets to look at the images directly;
    rather, it is only told whether the generated images look real or fake. For this
    reason, GANs are categorized as **implicit density models**.
  prefs: []
  type: TYPE_NORMAL
- en: We could use an analogy to compare the explicit and implicit models. Let's say
    an art student, G, was given a collection of Picasso paintings and asked to learn
    how to draw fake Picasso paintings. The student can look at the collections as
    they learn to paint, so that is an explicit model. In a different scenario, we
    ask student G to forge Picasso paintings, but we don't show them any paintings
    and they don't know what a Picasso painting looks like. The only way they learn
    is from the feedback they get from student D, who is learning to spot fake Picasso
    paintings. The feedback is simple – the painting is either *fake* or *real*. That
    is our implicit density GAN model.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps one day they painted a twisted face by chance and learned from the feedback
    that it looked like a real Picasso painting. Then they start to draw in that style
    to fool student D. Students G and D are the two networks in a GAN, known as the
    **generator** and **discriminator**. This is the biggest difference in the network
    architecture compared with other generative models.
  prefs: []
  type: TYPE_NORMAL
- en: We will start this chapter by learning about the GAN building blocks, followed
    by the losses. The original GAN does not have reconstruction loss, which is another
    thing that sets it apart from other algorithms. Then, we will create custom training
    steps for a GAN, and we'll be ready to train our first GAN.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of a GAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The word *adversarial* in Generative Adversarial Network means *involving opposition
    or disagreement* according to the dictionary definition. There are two networks,
    known as the generator and discriminator, that compete with each other. The generator,
    as the name implies, generates *fake* images; while the discriminator will look
    at the generated images to decide whether they are real or fake. Each network
    is trying to win the game – the discriminator wants to correctly identify every
    real and fake image and the generator wants to fool the discriminator into believing
    the fake images generated by it are real. The following diagram shows the architecture
    of a GAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Architecture of a GAN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.1 – Architecture of a GAN
  prefs: []
  type: TYPE_NORMAL
- en: 'The GAN architecture bears some resemblance to a VAE (see [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039),
    *Variational Autoencoder*). In fact, you could rearrange the blocks in a VAE block
    diagram and add some lines and switches to produce this GAN block diagram. If
    a VAE was made up of two separate networks, we could think of:'
  prefs: []
  type: TYPE_NORMAL
- en: The GAN's generator as the VAE's decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The GAN's discriminator as the VAE's encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generator converts low-dimensional and simple distributions into high-dimensional
    images with a complex distribution, just like a decoder does. In fact, they are
    identical; we could simply copy and paste the decoder code and rename it as the
    generator, and vice versa, and it would just work. The input to the generator
    is usually samples from a normal distribution, despite some using uniform distribution.
  prefs: []
  type: TYPE_NORMAL
- en: We send real and fake images to the discriminator in different minibatches.
    Real images are those from the dataset while fake images are generated by the
    generator. The discriminator outputs a single value probability of whether the
    input is real or fake. It is a binary classifier and we could implement it using
    a CNN. Technically, the discriminator serves a different purpose than the encoder
    but they both reduce the dimensionality of their inputs.
  prefs: []
  type: TYPE_NORMAL
- en: Well, it turns out having two networks in a model is not that scary after all.
    The generator and discriminator are our old friends in disguise and under new
    names. We already know how to build those models, therefore let's not worry about
    the details of constructing them now. In fact, the original GAN paper used only
    a multilayer perceptron, which is made up of some basic dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: Value functions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The value function captures the fundamentals of how a GAN works. The equation
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*D* stands for discriminator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*G* is the generator.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* is input data and *z* is a latent variable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will also use the same notation in the code. This is the function that the
    generator tries to minimize while the discriminator wants to maximize it.
  prefs: []
  type: TYPE_NORMAL
- en: When you understand it, the code implementation will be a lot easier and will
    make a lot of sense. Furthermore, much of our later discussion about the challenges
    of GANs and improvements to it revolves around the loss function. Therefore, it
    is well worth your time studying it. The GAN loss function is also known as **adversarial
    loss** in some literature. It looks rather complex now, but I'll break it down
    and show you step by step how it can be converted into simple loss functions that
    we can implement.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first right-hand term of the value function is the value to classify a
    real image correctly. From the left-hand term, we know the discriminator wants
    to maximize it. **Expectation** is a mathematical term that is the sum of the
    weighted average of every sample of a random variable. In this equation, the weight
    is the probability of data, and the variable is the log of the discriminator output
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'In a minibatch of size *N*, *p(x)* is *1/N*. This is because *x* is a single
    image. Instead of trying to maximize it, we can change the sign to minus and try
    to minimize it instead. This can be done with the help of the following equation,
    called the **log loss**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y*i is the label, which is *1* for real images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p(y*i*)* is the probability of the sample being real.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The second right-hand term of the value function is about fake images; *z*
    is random noise and *G(z)* is generated fake images. *D(G(z))* is the discriminator''s
    confidence score of how likely the image is to be real. If we use a label of *0*
    for fake images, we can use the same method to cast it into the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, putting everything together, we have our discriminator loss function,
    which is binary cross-entropy loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The following code shows how to implement the discriminator loss. You can find
    the code in Jupyter notebook `ch3_dcgan.ipynb`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In our training, we do a forward pass on real and fake images separately using
    the same minibatch size. Therefore, we compute the binary cross-entropy loss for
    them separately and take the average as the loss.
  prefs: []
  type: TYPE_NORMAL
- en: Generator loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The generator is only involved when the model is evaluating fake images, thus
    we only need to look at the second right-hand term of the value function and simplify
    it to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'At the beginning of the training, the generator is not good at generating images,
    therefore the discriminator is confident in classifying it as *0* all the time,
    making *D(G(z))* always *0*, and so is *log (1 – 0)*. When the error in the model
    output is always *0*, then there is no gradient to backpropagate. As a result,
    the generator''s weights are not updated, and the generator is not learning. This
    phenomenon is known as **saturating gradient** due to there being almost no gradient
    in the discriminator''s sigmoid output. To avoid this problem, the equation is
    cast from minimizing *1-D(G(z))* to maximizing *D(G(z))* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: GANs that use this function are also known as **Non-Saturating GANs (NS-GANs)**.
    In fact, almost every implementation of **Vanilla GAN** uses this value function
    rather than the original GAN function.
  prefs: []
  type: TYPE_NORMAL
- en: Vanilla GAN
  prefs: []
  type: TYPE_NORMAL
- en: The interest of researchers in GANs exploded soon after their invention and
    many researchers gave their GAN a name. Some tried to keep track of all the named
    GANs over the years, but the list got too long. Vanilla GAN is the name used to
    loosely refer to the first basic GAN without fancy flavors. Vanilla GAN is usually
    implemented with two or three hidden dense layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can derive the generator loss using the same mathematical steps for the
    discriminator, which will eventually lead to the same discriminator loss function
    except that labels of one is used for real images. It can be confusing to beginners
    as to why to use real labels for fake images. It will be clear if we derive the
    equation, or we can also understand it as we want to fool the discriminator into
    assuming that those generated images are real, thus we use the real labels. The
    code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Congratulations, you have turned the most complex equation in a GAN into simple
    binary cross-entropy loss and implemented it in a few lines of code! Now let's
    look at the GAN training pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: GAN training steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To train a conventional neural network in TensorFlow or any other high-level
    machine learning framework, we specify the model, loss function, optimizer, and
    then call `model.fit()`. TensorFlow will do all the work for us – we just sit
    there and wait for the loss to drop. Unfortunately, we cannot chain the generator
    and discriminator to be a single model, like we did for the VAE, and call `model.fit()`
    to train the GAN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before delving into the GAN problem, let''s take a pause and refresh ourselves
    on what happens underneath the hood when doing a single training step:'
  prefs: []
  type: TYPE_NORMAL
- en: Perform a forward pass to compute the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From the loss, backpropagate the gradients backward with respect to the variables
    (weights and biases).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, it's the variables update step. The optimizer will scale the gradients
    and add them to the variables, which completes one training step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are the generic training steps in a deep neural network. The various optimizers
    differ only in how they calculate the scaling factors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now come back to the GAN and look at the flow of gradients. When we train with
    real images, only the discriminator is involved – the network input is a real
    image and the output is a label of *1*. The generator plays no role here and therefore
    we can''t use `model.fit()`. However, we could still fit the model using the discriminator
    only, that is, `D.fit()` so that it is not the blocking issue. The problem arises
    when we use fake images and the gradients backpropagate to the generator via the
    discriminator. So, what is the problem? Let''s take the generator loss and discriminator
    loss for the fake image and put them side by side:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If you try to spot the difference between them, then you'll find that their
    labels are opposite signs! This means, using generator loss to train the entire
    model will make the discriminator move in the opposite direction and not learn
    to discriminate. This is counterproductive and we don't want to have an untrained
    discriminator that will discourage the generator from learning. For this reason,
    we must train the generator and discriminator separately. We will freeze the discriminator's
    variables when training the generator.
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to design a GAN training pipeline. One is to use the high-level
    Keras model, which needs less code and therefore looks more elegant. We'll only
    need to define the model once, and call `train_on_batch()` to perform all the
    steps, including the forward pass, backpropagation, and weights update. However,
    it is less flexible when it comes to implementing more complex loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other method is to use low-level code so we can control every step. For
    our first GAN, we will use a low-level custom training step function from the
    official TensorFlow GAN tutorial ([https://www.tensorflow.org/tutorials/generative/dcgan](https://www.tensorflow.org/tutorials/generative/dcgan)),
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`tf.GradientTape()` is used to record the gradients of a single pass. You may
    have seen another API, `tf.Gradient()`, that has a similar function, but the latter
    does not work in TensorFlow eager execution. We will see how the three procedural
    steps mentioned previously get implemented in `train_step()`. The preceding code
    snippet shows the first step to carry out a forward pass to calculate the losses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The second step is to calculate the gradient of the generator and discriminator
    from their respective losses using a tape gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The third and final step is to use the optimizer to apply the gradients to
    the variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: You have now learned everything you need to train a GAN. What is left to be
    done is to set up the input pipeline, generator, and discriminator, and we will
    go over that in the coming section.
  prefs: []
  type: TYPE_NORMAL
- en: Custom model fit
  prefs: []
  type: TYPE_NORMAL
- en: After TensorFlow 2.2, it is now possible to create a custom `train_step()` for
    a Keras model without re-writing the entire training pipeline. Then, we can use
    `model.fit()` in the usual way. This will also enable the use of multiple GPUs
    for training. Unfortunately, this new feature was not released in time to make
    it into the code in this book. However, do check out the TensorFlow tutorial at
    [https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit)
    and feel free to modify the GAN's code to use a custom model fit.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Deep Convolutional GAN (DCGAN)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although Vanilla GAN has proven itself as a generative model, it suffers from
    a few training problems. One of them is the difficulty in scaling networks to
    make them deeper in order to increase their capacities. The authors of **DCGAN**
    incorporated a few recent advancements in CNNs at that time to make networks deeper
    and stabilize the training. These include the removal of the **maxpool** layer,
    replacing it with strided convolutions for downsampling, and the removal of fully
    connected layers. This has since become the standard way of designing a new CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture guidelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DCGAN is not strictly a fixed neural network that has layers pre-defined with
    a fixed set of parameters such as kernel size and the number of layers. Instead,
    it is more like architecture design guidelines. The use of batch normalization,
    activation, and upsampling in DCGAN has influenced the development of GANs. We
    will therefore look into them more, which should provide guidance in designing
    our own GAN.
  prefs: []
  type: TYPE_NORMAL
- en: Batch normalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Batch normalization** is informally called **batchnorm** within the machine
    learning community. In the early days of deep neural network training, a layer
    updated its weights after backpropagation to produce outputs that are closer to
    the targets. However, the weights of the subsequent layers have also changed,
    so it is like a moving goal, and this makes the training of deep networks difficult.
    Batchnorm solves this by normalizing the input to every layer to have zero mean
    and unity variance, hence stabilizing the training. These are operations that
    happen within batchnorm:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate the mean *µ* and standard deviation *σ* of tensor *x* in a minibatch
    for every channel (hence the name *batch* normalization).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Normalize the tensor: *x'' = (x – µ) / σ*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Perform an affine transformation: *y = α * x'' + β*, where *α* and *β* are
    trainable variables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a DCGAN, batchnorm is added to both the generator and discriminator, except
    for the first layer of the discriminator and the last layer of the generator.
    One thing to note is that newer researches show that batchnorm is not the best
    normalization technique to use for image generation as it removes some of the
    important information. We will look at other normalization techniques in later
    chapters, but we will keep using batchnorm in our GAN until then. One thing that
    we should know is that in order to use batchnorm, we will have to use a large
    minibatch, otherwise, the batch statistics can vary greatly from batch to batch
    and make the training unstable.
  prefs: []
  type: TYPE_NORMAL
- en: Activations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following figure shows the activations that we will use in DCGAN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – ReLU and leaky ReLU are used in intermediate layers of the generator
    and discriminator'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.2 – ReLU and leaky ReLU are used in intermediate layers of the generator
    and discriminator
  prefs: []
  type: TYPE_NORMAL
- en: As the discriminator's job is to be a binary classifier, we use sigmoid to squeeze
    the output to within the range of *0* (fake) and *1* (real). On the other hand,
    the generator's output uses *tanh*, which bounds the images between *-1* and *+1*.
    Therefore, we will need to scale our images to this range in the preprocessing
    step.
  prefs: []
  type: TYPE_NORMAL
- en: For intermediate layers, the generator uses ReLU in all layers, but the discriminator
    uses leaky ReLU instead. In standard ReLU, the activation increases linearly with
    positive input but is zero for all negative input values. This limits the gradient
    flow when it is negative and thus the generator does not receive gradients in
    order to update its weights and learn. Leaky ReLU alleviates that problem by allowing
    small gradients to flow when the activation is negative.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the preceding figure, for input above and equal to *0*, it
    is identical to ReLU where the output equals input with a slope of *1*. For input
    below *0*, the output is scaled to *0.2* of the input. The default slope of leaky
    ReLU in TensorFlow is *0.3* while the DCGAN uses *0.2*. It is just a hyperparameter
    and you are free to try any other values.
  prefs: []
  type: TYPE_NORMAL
- en: Upsampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In DCGAN, upsampling in the generator is performed using the transpose convolutional
    layer. However, it has been shown that this will produce a checkerboard pattern
    in the generated image, especially in images with strong colors. As a result,
    we replace it with `UpSampling2D`, which performs conventional image resizing
    methods by using bilinear interpolation.
  prefs: []
  type: TYPE_NORMAL
- en: Building a DCGAN for Fashion-MNIST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Jupyter notebook for this exercise is `ch3_dcgan.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST has been used in many introductory machine learning tutorials and we are
    all familiar with it. With recent advancements in machine learning, this dataset
    began to look a bit trivial for deep learning. As a result, a new dataset, Fashion-MNIST,
    has been created as a direct drop-in replacement for the MNIST dataset. It has
    exactly the same number of training and test examples, 28x28 grayscale images
    of 10 classes. This is what we will train our DCGAN with.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Examples of images from the Fashion-MNIST dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.3 – Examples of images from the Fashion-MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: Generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The design of the generator can be broken into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the 1D latent vector into a 3D activation map.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Double the activation map's spatial resolution until it matches the target image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first thing to do is to work out the number of upsampling stages. As the
    images have a shape of 28x28, we can use two upsampling stages to increase the
    dimension from 7->14->28\.
  prefs: []
  type: TYPE_NORMAL
- en: For simple data, we can use one convolution layer per upsampling stage, but
    we could also use more layers. This method is similar to CNNs in that you have
    several convolution layers working on the same spatial resolution before downsampling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will decide the channel numbers of the first convolutional layer.
    Let''s say we use [512, 256, 128, 1], where the last channel number is the image
    channel number. With this information, we know the neuron numbers in first dense
    layer to be `7 x 7 x 512`. The `7x7` is the spatial resolution we worked out and
    `512` is the filter number in the first convolutional layer. After the dense layer,
    we reshape it to `(7,7,512)` so it can be fed into a convolutional layer. Then,
    we only need to define the filter number of convolutional layers and add the batchnorm
    and ReLU, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The model summary for the generator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – DCGAN generator model summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.4 – DCGAN generator model summary
  prefs: []
  type: TYPE_NORMAL
- en: The generator's model summary shows the activation map shapes that are doubling
    in spatial resolution `(7×7 to 14×14 to 28×28)` while halving in the channel numbers
    `(512 to 256 to 128)`.
  prefs: []
  type: TYPE_NORMAL
- en: Discriminator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The design of the discriminator is straightforward, just like a simple classifier
    CNN but with leaky ReLU as activation. As a matter of fact, the discriminator
    architecture was not even mentioned in the DCGAN paper. As a rule of thumb, the
    discriminator should have fewer or an equal number of layers as the generator,
    so it doesn''t overpower the generator to stop the latter from learning. The following
    is the code to create the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The model summary of the discriminator, which is a simple CNN classifier, is
    shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – DCGAN discriminator model summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.5 – DCGAN discriminator model summary
  prefs: []
  type: TYPE_NORMAL
- en: Training our DCGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now we can start training our first GAN. The following diagram shows the samples
    generated during different steps in the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Generated images during DCGAN training'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.6 – Generated images during DCGAN training
  prefs: []
  type: TYPE_NORMAL
- en: The first row of samples is generated right after network weight initialization
    and before any training steps. As we can see, they are just some random noise.
    As training progresses, the generated images become better. However, the generator
    loss is higher than when it was only generating random noise.
  prefs: []
  type: TYPE_NORMAL
- en: The loss is not an absolute measurement of generated image quality; it merely
    provides relative terms to compare the performance of the generator relative to
    the discriminator and vice versa. The generator loss was low simply because the
    discriminator had not learned to do its job well. This is one of the challenges
    of a GAN where the loss does not give sufficient information about the model's
    quality.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following graphs show the discriminator loss and generator loss during
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Discriminator and generator training losses'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.7 – Discriminator and generator training losses
  prefs: []
  type: TYPE_NORMAL
- en: We can see that the equilibrium achieved in the first 1,000 steps and the loss
    remain roughly stable after that. However, the loss isn't definitive in gauging
    when to stop training. For now, we can save the weights every few epochs and eyeball
    to select the one that generates the best-looking images!
  prefs: []
  type: TYPE_NORMAL
- en: In theory, the global optimal for the discriminator is achieved when *pdiscriminator
    = pdata*. In other words, if *pdata = 0.5* as half of the data is real and half
    is fake, then *pdiscriminator = 0.5* will mean it can no longer distinguish between
    the two classes and the prediction is no better than flipping a coin.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in training GANs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GANs are notoriously difficult to train. We'll discuss some of the main challenges
    in training a GAN.
  prefs: []
  type: TYPE_NORMAL
- en: Uninformative loss and metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training a CNN for classification or detection tasks, we can look at the
    shape of the loss plots to tell whether the network has converged or is overfitting
    and we'll know when to stop training. Then the metrics will correlate with the
    loss. For example, classification accuracy is normally the highest when the loss
    is the lowest. However, we can't do the same with GAN loss, as it doesn't have
    a minimum but fluctuates around some constant values after training for a while.
    We also could not correlate the generated image quality with the loss. A few metrics
    were invented to address this in the early days of GANs and one of them is the
    **inception score.**
  prefs: []
  type: TYPE_NORMAL
- en: A classification CNN known as `ImageNet` dataset. If high confidence is recorded
    for a class, it is more likely to be a real image. There is another metric known
    as the **Fréchet inception distance**, which measures the variety of generated
    images. These metrics are normally used only in academic papers to make a comparison
    with other models (so they can claim their models are superior), so we will not
    cover them in detail in this book. Human visual inspection is still the most reliable
    way of assessing the quality of generated images.
  prefs: []
  type: TYPE_NORMAL
- en: Instability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GANs are extremely sensitive to any change in hyperparameters, including learning
    rate and filter kernel size. Even after a lot of hyperparameter tuning, and the
    correct architecture, there are instances while retraining the model where the
    following can occur:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Generator stuck in local minima'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.8 – Generator stuck in local minima
  prefs: []
  type: TYPE_NORMAL
- en: If the network weights are unfortunately randomly initialized to some bad values,
    the generator could get stuck in some bad local minima and may never recover,
    while the discriminator keeps improving. As a result, the generator gives up and
    produces only nonsensical images. This is also known as **convergence failure**,
    where the losses fail to converge. We'll need to stop the training, re-initialize
    the network, and restart the training. This is also the reason why I haven't chosen
    a more complex dataset such as CelebA to introduce GANs, but don't worry, we'll
    get there before the chapter ends.
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing gradient
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One reason for instability is the vanishing gradient of the generator. As we''ve
    already mentioned, when we train the generator, the gradient will flow through
    the discriminator. If the discriminator is confident that the images are fake,
    then there will be little or even zero gradient to backpropagate to the generator.
    The following points are some of the methods of mitigation:'
  prefs: []
  type: TYPE_NORMAL
- en: Reformulating the value function from minimizing log *(1-D(G(z))* to maximizing
    log *D(G(z))*, which we already did. In practice, this alone is still not enough.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using activation functions that allow more gradients to flow, such as leaky
    ReLU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Balancing between the generator and the discriminator by reducing the discriminator's
    network capacity or increasing the training steps for the generator.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using *one-sided label smoothing,* where the label of the real image is decreased
    from *1* to, say, *0.9* to reduce the discriminator's confidence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mode collapse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Mode collapse** happens when the generator is producing images that look
    like each other. This is not to be confused with convergence failure where the
    GAN produces only garbage images.'
  prefs: []
  type: TYPE_NORMAL
- en: Mode collapse can happen even when the generated images look great but are limited
    to small subsets of classes (inter-class mode collapse) or a few of the same images
    within the class (intra-class mode collapse). We can demonstrate mode collapse
    by training a Vanilla GAN on a mixture of two Gaussian distributions, which you
    could run in the `ch3_mode_collapse` notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the shape of the generated samples during training
    taking the form of two Gaussian blobs. One sample is round and the other is elliptical:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – The top figure is the real samples. The bottom figures show
    generated samples in two different epochs during training](img/B14538_03_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – The top figure is the real samples. The bottom figures show generated
    samples in two different epochs during training
  prefs: []
  type: TYPE_NORMAL
- en: As the Vanilla GAN trains, the generated samples can look like one of two modes
    in a minibatch but never two modes at the same time. For Fashion-MNIST, it may
    be that the generator is producing shoes that look the same every time, regardless.
    After all, the objective of the generator is to produce realistic-looking images,
    and it is not penalized for showing the same shoes every time as long as the discriminator
    deems the images to be real. As proven in the original GAN paper mathematically,
    after the discriminator achieved optimality, the generator will work toward optimizing
    for **Jensen-Shannon divergence** (**JSD**).
  prefs: []
  type: TYPE_NORMAL
- en: 'For our purposes, we only need to know that JSD is a symmetrical version of
    **Kullback-Leibler divergence** (**KLD**) with an upper bound of *log(2)* rather
    than an infinite upper bound. Unfortunately, JSD is also the cause of mode collapse,
    as can be illustrated in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – A standard Gaussian distribution fit on data drawn from a mixture
    of Gaussians by minimizing KLD, MMD, and JSD (Source: L. Theis et al, 2016, "A
    Note On The Evaluation of Generative Models," https://arxiv.org/abs/1511.01844)](img/B14538_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.10 – A standard Gaussian distribution fit on data drawn from a mixture
    of Gaussians by minimizing KLD, MMD, and JSD (Source: L. Theis et al, 2016, "A
    Note On The Evaluation of Generative Models," https://arxiv.org/abs/1511.01844)'
  prefs: []
  type: TYPE_NORMAL
- en: We will not talk about **maximum mean discrepancy** (**MMD**), which is not
    used in a GAN. The data is two Gaussian distributions where one has more mass
    density than the other. A single Gaussian is fitted on the data. In other words,
    we try to estimate one best mean and standard deviation to describe the two type
    of Gaussian distribution. With KLD, we see that although the fitted Gaussian leans
    toward the bigger Gaussian blob, it still provides some coverage to the smaller
    Gaussian blob. This is not the case for JSD, where it is fitted to only the most
    prominent Gaussian blob. This explains mode collapse in a GAN – when the probability
    of some particular generated images is high, when these few modes are *locked*
    by the optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Building a Wasserstein GAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Many have attempted to solve the instability of GAN training by using heuristic
    approaches such as trying different network architectures, hyperparameters, and
    optimizers. One major breakthrough happened in 2016 with the introduction of **Wasserstein
    GAN (WGAN)**.
  prefs: []
  type: TYPE_NORMAL
- en: WGAN alleviates or even eliminates many of the GAN challenges we've discussed
    altogether. It no longer requires careful design of network architecture nor careful
    balancing of the discriminator and the generator. The mode collapse problem is
    also reduced drastically.
  prefs: []
  type: TYPE_NORMAL
- en: The biggest fundamental improvement from the original GAN is the change of the
    loss function. The theory is that if the two distributions are disjointed, JSD
    will no longer be continuous, hence not differentiable, resulting in a zero gradient.
    WGAN solves this by using a new loss function that is continuous and differentiable
    everywhere!
  prefs: []
  type: TYPE_NORMAL
- en: The notebook for this exercise is `ch3_wgan_fashion_mnist.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: Tips
  prefs: []
  type: TYPE_NORMAL
- en: It is alright to not learn how to implement the code in this section, particularly
    WGAN-GP, which is more complex. Although theoretically superior, we could still
    train GANs stably using a simpler loss function with carefully designed model
    architecture and hyperparameters. However, you should try to understand the term
    Lipschitz constraint as it was used in the development of several advanced techniques,
    which we will cover in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Wasserstein loss
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s remind ourselves of the non-saturating value function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'WGAN uses a new loss function known as the **Earth mover''s distance** or just
    Wasserstein distance. It measures the distance or the effort needed to transform
    one distribution into another. Mathematically, it is the minimum distance for
    every joint distribution between real and generated images, which is intractable,
    with some mathematical assumptions that are outside the scope of this book, and
    the value function becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s compare the preceding equation with NS loss and use that to derive
    the loss function. The most prominent change is that the *log()* is gone, and
    another is the sign of the fake image term changes. The loss function of the first
    term is therefore:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the average of the discriminator output, multiplied by *-1*. We can
    also generalize it by using *y*i as labels where *+1 is for real images*, and
    *-1 is for fake images*. Thus, we can implement Wasserstein loss as a TensorFlow
    Keras custom loss function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: As this loss function is no longer binary cross-entropy, the discriminator's
    objective is no longer classifying or discriminating between real and fake images.
    Instead, it aims to maximize the score for real images with respect to fake images.
    For this reason, in WGAN, the discriminator is given a new name of **critic**.
  prefs: []
  type: TYPE_NORMAL
- en: The generator and discriminator architecture stays the same. The only change
    is that the sigmoid is removed from the discriminator's output. Therefore, the
    critic's prediction is unbounded and can be very large positive and negative values.
    This is put in check by implementing the **1-Lipschitz** constraint.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the 1-Lipschitz constraint
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mathematical assumption mentioned in Wasserstein loss is the **1-Lipschitz
    function**. We say the critic *D(x)* is *1-Lipschitz* if it satisfies the following
    inequality:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: For two images, *x*1 and *x*2, their absolute critic's output difference must
    be smaller or equal to their average pixel-wise absolute difference. In other
    words, the critic's outputs should not differ too much for different images –
    be it real images or fakes. When WGAN was invented, the authors could not think
    of a proper implementation to enforce inequality. Therefore, they came up with
    a hack, which is to clip the critic's weights to some small values. By doing that,
    the layers' outputs and eventually the critics' outputs are capped to some small
    values. In the WGAN paper, the weights are clipped to the range of *[-0.01, 0.01]*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Weight clipping can be implemented in two ways. One way is to write a custom
    constraint function and use that in instantiating a new layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then pass the function to layers that accept constraint functions as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'However, adding the constraint code in every layer creation can make the code
    look bloated. As we don''t need to cherry-pick which layer to clip, we can use
    a loop to read the weights and clips and write them back as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: This is the method we use in the code example.
  prefs: []
  type: TYPE_NORMAL
- en: Restructuring training steps
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the original GAN theory, the discriminator is supposed to be trained optimally
    before the generator. That was not possible in practice due to the vanishing gradient
    of the generator as the discriminator gets better. Now, with the Wasserstein loss
    function, the gradient is derivable everywhere and we don't have to worry about
    the critic being too good to compare with the generator.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, in WGAN, the critic is trained for five steps for every one training
    step for the generator. In order to do this, we will split the critic training
    step into a separate function, which we can then loop through multiple times:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need to rework the generator training step. In our DCGAN code,
    we use two models – the generator and discriminator. To train the generator, we
    also use gradient tape to update the weights. All these are rather cumbersome.
    There is another way of implementing the training step for the generator by merging
    the two models into one as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we freeze the critic layers by setting `trainable=False`,
    and we chain that to the generator to create a new model and compile it. After
    that, we can set the critic to be trainable again, which will not affect the model
    that we have already compiled.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use the `train_on_batch()` API to perform a single training step that will
    automatically do the forward pass, loss calculation, backpropagation, and weights
    update:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For this exercise, we resize the image shape to 32x32 so we can use deeper
    layers in the generator to upscale the image. The WGAN generator and discriminator
    architecture are shown in the following model summaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Model summary of the WGAN''s generator'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.11 – Model summary of the WGAN's generator
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator architecture follows the usual design with decreasing channel
    numbers as the feature map''s size doubles. The following is the model summary
    of the WGAN''s critic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Model summary of the WGAN''s critic'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.12 – Model summary of the WGAN's critic
  prefs: []
  type: TYPE_NORMAL
- en: Despite the improvement over DCGAN, I found it difficult to train a WGAN and
    the image quality produced is no more superior than DCGAN. We'll now implement
    a WGAN variant that trains faster and produces sharper images.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing gradient penalty (WGAN-GP)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Weight clipping is not an ideal way to enforce a Lipschitz constraint, as acknowledged
    by the WGAN authors. There are two drawbacks: capacity underuse and exploding/vanishing
    gradients. As we limit the weights, we also limit the critic''s ability to learn.
    It was found that weight clipping forces the network to learn only simple functions.
    Therefore, the neural network''s capacity becomes underused.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Secondly, the clipping values require careful tuning. If set too high, the
    gradients will explode, hence violating the Lipschitz constraint. If set too low,
    gradients will vanish as we move the network back.  Also, the weight clipping
    will push the gradients to the two limits, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Left: Weight clipping pushes weights toward two values. Right:
    Gradients produced by gradient penalty. Source: I. Gulrajani et al, 2017, Improved
    Training of Wasserstein GANs](img/B14538_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.13 – Left: Weight clipping pushes weights toward two values. Right:
    Gradients produced by gradient penalty. Source: I. Gulrajani et al, 2017, Improved
    Training of Wasserstein GANs'
  prefs: []
  type: TYPE_NORMAL
- en: 'As a result, **gradient penalty** (**GP**) is proposed to replace weight clipping
    to enforce the Lipschitz constraint as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: We will look at each of the variables in the equation and implement them in
    the code. The Jupyter notebook for this exercise is `ch3_wgan_gp_fashion_mnist.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We normally use *x* to denote a real image, but there is now an ![](img/Formula_03_013.png)
    in the equation. This ![](img/Formula_03_014.png) is pointwise interpolation between
    a real image and a fake image. The ratio of the images, or the epsilon, is drawn
    from a uniform distribution of *[0,1]*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There is mathematical proof that the *"optimal critic contains straight lines
    with gradient norm 1 connecting coupled points from Pr and Pg",* as quoted from
    the WGAN-GP paper *Improved Training of Wasserstein GANs* ([https://arxiv.org/pdf/1704.00028.pdf](https://arxiv.org/pdf/1704.00028.pdf)).
    For our purposes, we can understand it as the gradient comes from the mixture
    of both real and fake images and we don't need to calculate the penalty for real
    and fake images separately.
  prefs: []
  type: TYPE_NORMAL
- en: 'The term ![](img/Formula_03_015.png) is the gradient of the critic''s output
    with respect to the interpolation. We can again use gradient tape to get the gradient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to calculate the L2-norm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_03_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'We square every value, add them together, then do a square root as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'When doing `tf.reduce_sum()`, we exclude the first dimension in the axis as
    that dimension is the batch size. The penalty aims to bring the gradient norm
    close to `1`, and this is the last step to calculate the gradient loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The lambda in the equation is the ratio of the gradient penalty to other critic
    losses and is set to 10 in the paper. Now we add all the critic losses and gradient
    penalty to backpropagate and update weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'That is everything you''ll need to add to the WGAN to make it WGAN-GP. There
    are two things to remove though:'
  prefs: []
  type: TYPE_NORMAL
- en: Weight clipping
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization in the critic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The gradient penalty is to penalize the norm of the critic's gradient with respect
    to each input independently. However, batch normalization changes the gradients
    with the batch statistics. To avoid this problem, batch normalization was removed
    from the critic and it was found that it still works well. This has since become
    a common practice in GANs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The critic architecture is the same as WGAN, less the batch normalization:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Model summary of WGAN-GP'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.14 – Model summary of WGAN-GP
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the samples generated by a trained WGAN-GP:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Samples generated by WGAN-GP'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.15 – Samples generated by WGAN-GP
  prefs: []
  type: TYPE_NORMAL
- en: They look sharp and pretty, much like samples from the Fashion-MNIST dataset.
    The training was very stable and converged quickly! Next, we will put WGAN-GP
    to the test by training it on CelebA!
  prefs: []
  type: TYPE_NORMAL
- en: Tweaking WGAN-GP for CelebA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will make some small tweaks to WGAN-GP to train on the CelebA dataset. First,
    as we will use a larger image size of 64 compared to 32 previously, we will need
    to add another stage of upsampling. Then we replace the batch normalization with
    **layer normalization** as suggested by the WGAN-GP authors. The following figure
    shows different types of normalization for tensors with a dimension of **(N, H,
    W, C)** where the notations stand for batch size, height, width, and channel respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Different types of normalizations used in deep learning. (Source:
    Y. Wu, K. He, 2018, Group Normalization)](img/B14538_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3.16 – Different types of normalizations used in deep learning. (Source:
    Y. Wu, K. He, 2018, Group Normalization)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch normalization calculates statistics across **(N, H, W)** to produce one
    statistic for each channel. In contrast, layer normalization calculates statistics
    across all tensors within one sample, that is, **(H,W,C)** and therefore does
    not correlate between samples and hence works better for image generation. It
    is a drop-in replacement for batch normalization where we replace the word *Batch*
    with *Layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The Jupyter notebook for this exercise is `ch3_wgan_gp_celeb_a.ipynb`. The
    following are the images generated by our WGAN-GP. Although the training time
    of WGAN-GP is longer due to the additional step to do gradient penalty, the training
    is able to converge faster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Celebrity faces generated by WGAN-GP'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_03_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3.17 – Celebrity faces generated by WGAN-GP
  prefs: []
  type: TYPE_NORMAL
- en: They don't look quite perfect compared to the VAE, partly because there wasn't
    reconstruction loss to make sure the facial features stay in the places they belong.
    Nonetheless, this encourages the GAN to be more imaginative and, as a result,
    more varieties of faces were generated. I also did not notice mode collapse. WGAN-GP
    is a milestone to achieve the training stability of a GAN. Many subsequent GANs
    use Wasserstein loss and gradient penalty, and that includes the Progressive GAN,
    to generate high-resolution images, which we will talk about in detail in [*Chapter
    7*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)*, High Fidelity Face Generation*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have definitely learned a lot in this chapter. We started by learning about
    the theory and loss functions of GANs, and how to translate the mathematical value
    function into the code implementation of binary cross-entropy loss. We implemented
    DCGAN with convolutional layers, batch normalization layers, and leaky ReLU to
    make the networks go deeper. However, there are still challenges in training GANs,
    which include instability and being prone to mode collapse due to Jensen-Shannon
    divergence.
  prefs: []
  type: TYPE_NORMAL
- en: Many of these problems were solved by WGAN with Wasserstein distance, weight
    clipping, and the removal of the sigmoid at the critic's output. Finally, WGAN-GP
    introduces gradient penalty to properly enforce the 1-Lipztschitz constraint and
    give us a framework for stable GAN training. We then replaced batch normalization
    with layer normalization to train on the CelebA dataset successfully to generate
    a good variety of faces.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes part 1 of the book. Well done to you for making it this far!
    By now, you have learned about using different families of generative models to
    generate images. That includes autoregressive models like PixelCNN in [*Chapter
    1*](B14538_01_Final_JM_ePub.xhtml#_idTextAnchor017)*, Getting Started with Image
    Generation Using TensorFlow*, in [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*,
    Variational Autoencoder* and GANs in this chapter. You are now familiar with the
    concept of distribution, loss functions, and how to construct neural networks
    for image generation.
  prefs: []
  type: TYPE_NORMAL
- en: With this solid foundation, we will explore some interesting applications in
    part 2 of the book, where we will also get to learn about some advanced techniques
    and cool applications. In the next chapter, we will learn how to perform image-to-image
    translation with GANs.
  prefs: []
  type: TYPE_NORMAL
