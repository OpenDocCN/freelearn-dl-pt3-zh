<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer008">
<h1 class="mainHeading" id="_idParaDest-5">Preface</h1>
<p class="normal"><em class="italic">Deep Learning with TensorFlow and Keras, Third Edition</em>, is a concise yet thorough introduction to modern neural networks, artificial intelligence, and deep learning technologies designed especially for software engineers and data scientists. The book is the natural follow-up of the books <em class="italic">Deep Learning with Keras</em> [1] and <em class="italic">TensorFlow 1.x Deep Learning Cookbook</em> [2] previously written by the same authors.</p>
<p class="normal">This book provides a very detailed panorama of the evolution of learning technologies over the past six years. The book presents dozens of working deep neural networks coded in Python using TensorFlow 2.x, a modular network library based on Keras-like APIs [1].</p>
<p class="normal"><strong class="keyWord">Artificial Intelligence</strong> (<strong class="keyWord">AI</strong>) lays the ground for everything this book discusses. <strong class="keyWord">Machine Learning</strong> (<strong class="keyWord">ML</strong>) is a branch of AI, and <strong class="keyWord">Deep Learning</strong> (<strong class="keyWord">DL</strong>) is in turn a subset of ML. This section will briefly discuss these three concepts, which you will regularly encounter throughout the rest of this book.</p>
<p class="normal">AI denotes any activity where machines mimic intelligent behaviors typically shown by humans. More formally, it is a research field in which machines aim to replicate cognitive capabilities such as learning behaviors, proactive interaction with the environment, inference and deduction, computer vision, speech recognition, problem-solving, knowledge representation, and perception. AI builds on elements of computer science, mathematics, and statistics, as well as psychology and other sciences studying human behaviors. There are multiple strategies for building AI. During the 1970s and 1980s, “expert” systems became extremely popular. The goal of these systems was to solve complex problems by representing the knowledge with a large number of manually defined if-then rules. This approach worked for small problems on very specific domains, but it was not able to scale up for larger problems and multiple domains. Later, AI focused more and more on methods based on statistical methods that are part of ML.</p>
<p class="normal">ML is a subdiscipline of AI that focuses on teaching computers how to learn without the need to be programmed for specific tasks. The key idea behind ML is that it is possible to create algorithms that learn from, and make predictions on, data. There are three different broad categories of ML:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Supervised learning</strong>, in which the machine is presented with input data and the desired output, and the goal is to learn from those training examples in such a way that meaningful predictions can be made for data that the machine has never observed before.</li>
<li class="bulletList"><strong class="keyWord">Unsupervised learning</strong>, in which the machine is presented with input data only, and the machine has to subsequently find some meaningful structure by itself, with no external supervision or input.</li>
<li class="bulletList"><strong class="keyWord">Reinforcement learning</strong>, in which the machine acts as an agent, interacting with the environment. The machine is provided with “rewards” for behaving in a desired manner, and “penalties” for behaving in an undesired manner. The machine attempts to maximize rewards by learning to develop its behavior accordingly.</li>
</ul>
<p class="normal">DL took the world by storm in 2012. During that year, the ImageNet 2012 challenge was launched with the goal of predicting the content of photographs using a subset of a large hand-labeled dataset. A deep learning model named AlexNet achieved a top-5 error rate of 15.3%, a significant improvement with respect to previous state-of-the-art results. According to the Economist, <em class="italic">Suddenly people started to pay attention, not just within the AI community but across the technology industry as a whole</em>.</p>
<p class="normal">That was only the beginning. Today, DL techniques are successfully applied in heterogeneous domains including, but not limited to, healthcare, environment, green energy, computer vision, text analysis, multimedia, finance, retail, gaming, simulation, industry, robotics, and self-driving cars. In each of these domains, DL techniques can solve problems with a level of accuracy that was not possible using previous methods.</p>
<p class="normal">Looking back at the past eight years, it is fascinating and exciting to see the extent of the contributions that DL has made to science and industry. There is no reason to believe that the next eight years will see any less contribution; indeed, as the field of DL continues to advance, we anticipate that we’ll see even more exciting and fascinating contributions provided by DL.</p>
<p class="normal">This book introduces you to the magic of deep learning. We will start with simple models and progressively will introduce increasingly sophisticated models. The approach will always be hands-on, with a healthy dose of code to work with.</p>
<h1 class="heading-1" id="_idParaDest-6">Who this book is for</h1>
<p class="normal">If you are a data scientist with experience in ML or an AI programmer with some exposure to neural networks, you will find this book a useful entry point to DL with TensorFlow. If you are a software engineer with a growing interest in the DL tsunami, you will find this book a foundational platform to broaden your knowledge on the topic. Basic knowledge of Python is required for this book.</p>
<h1 class="heading-1" id="_idParaDest-7">What this book covers</h1>
<p class="normal"><em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, is where we learn the basics of TensorFlow, an open-source library developed by Google for machine learning and deep learning. In addition, we introduce the basics of neural networks and deep learning, two areas of machine learning that had incredible growth during the last few years. The idea behind this chapter is to provide all the tools needed to do basic but fully hands-on deep learning.</p>
<p class="normal"><em class="chapterRef">Chapter 2</em>, <em class="italic">Regression and Classification</em>, focuses on the fundamental tasks in ML techniques: regression and classification. We will learn how to use TensorFlow to build simple, multiple, and multivariate regression models. We will use logistic regression to solve a multi-class classification problem.</p>
<p class="normal"><em class="chapterRef">Chapter 3</em>, <em class="italic">Convolutional Neural Networks</em>, covers how to use deep learning ConvNets for recognizing MNIST handwritten characters with high accuracy. We use the CIFAR 10 dataset to build a deep learning classifier with 10 categories, and the ImageNet dataset to build an accurate classifier with 1,000 categories. In addition, we investigate how to use large deep learning networks such as VGG16 and very deep networks such as InceptionV3. We will conclude with a discussion on transfer learning</p>
<p class="normal"><em class="chapterRef">Chapter 4</em>,<em class="italic"> Word Embeddings</em>, is where we describe the origins of and theory behind distributed representations and word embeddings and chart the progress of word embeddings from static word-based embeddings more dynamic and expressive embeddings based on sentences and paragraphs. We also explore how the idea of word embeddings can be extended to include non-word sequences as well, such as nodes in a graph or user sessions in a web application. The chapter also contains multiple examples of using word embeddings of various kinds.</p>
<p class="normal"><em class="chapterRef">Chapter 5</em>, <em class="italic">Recurrent Neural Networks</em>, describes an important architectural subclass of neural networks that are optimized for handling sequence data such as natural language or time series. We describe the important architectures in this genre, such as <strong class="keyWord">LSTM</strong> (<strong class="keyWord">Long Short-Term Memory</strong>) and <strong class="keyWord">GRU</strong> (<strong class="keyWord">Gated Recurrent Unit</strong>) and show how they can be extended to handle bidirectional states and states across batches. We also provide examples of using RNNs with various topologies for specific tasks, such as generating text, sentiment analysis, and part-of-speech tagging. We also describe the popular seq2seq architecture, which uses a pair of RNNs in an encoder-decoder pipeline to solve a variety of NLP tasks.</p>
<p class="normal"><em class="chapterRef">Chapter 6</em>, <em class="italic">Transformers</em>, covers transformers, a deep learning architecture that has revolutionized the traditional natural language processing field. We start by reviewing the key intuitions behind the architecture and various categories of transformers, together with a deep dive into the most popular models. Then, we focus on implementations both based on the vanilla architecture and on popular libraries, such as Hugging Face and TensorFlow Hub. After that, we briefly discuss evaluation, optimization, and some of the best practices commonly adopted when using transformers. The last section is devoted to reviewing how transformers can be used to perform computer vision tasks, a totally different domain from NLP. That requires a careful definition of the attention mechanism. In the end, attention is all you need! And at the core of attention, there is nothing more than the cosine similarity between vectors.</p>
<p class="normal"><em class="chapterRef">Chapter 7</em>, <em class="italic">Unsupervised Learning</em>, delves into unsupervised learning models. It will cover techniques required for clustering and dimensionality reduction like PCA, k-means, and self-organized maps. It will go into the details of Boltzmann machines and their implementation using TensorFlow. The concepts covered will be extended to build <strong class="keyWord">Restricted Boltzmann Machines</strong> (<strong class="keyWord">RBMs</strong>).</p>
<p class="normal"><em class="chapterRef">Chapter 8</em>, <em class="italic">Autoencoders</em>, describes autoencoders, a class of neural networks that attempt to recreate the input as its target. It will cover different varieties of autoencoders like sparse autoencoders, convolutional autoencoders, and denoising autoencoders. The chapter will train a denoising autoencoder to remove noise from input images. It will demonstrate how autoencoders can be used to create MNIST digits. It will also cover the steps involved in building an LSTM autoencoder to generate sentence vectors. Finally, we will learn how to build a variational autoencoder to generate images.</p>
<p class="normal"><em class="chapterRef">Chapter 9</em>, <em class="italic">Generative Models</em>, focuses on <strong class="keyWord">Generative Adversarial Networks</strong> (<strong class="keyWord">GANs</strong>). We start with the first proposed GAN model and use it to forge MNIST characters. The chapter shows you how to use deep convolutional GANs to create celebrity images. The chapter discusses the various GAN architectures like SRGAN, InfoGAN, and CycleGAN. The chapter covers a range of cool GAN applications. Finally, the chapter concludes with a TensorFlow implementation of CycleGAN to convert winter-summer images.</p>
<p class="normal"><em class="chapterRef">Chapter 10</em>, <em class="italic">Sel</em><em class="italic">f-Supervised Learning</em>, provides an overview of various strategies used for self-supervised learning in computer vision, audio, and natural language processing. It covers self-prediction through strategies such as autoregressive generation, masked generation, relationship prediction, and hybrids of these approaches. It also covers contrastive learning, a popular technique for self-supervised learning, and its application to various pretext tasks in various application domains.</p>
<p class="normal"><em class="chapterRef">Chapter 11</em>, <em class="italic">Reinforcement Learning</em>, focuses on reinforcement learning, covering the Q-learning algorithm and the Bellman equation. The chapter covers discounted rewards, exploration and exploitation, and discount factors. It explains policy-based and model-based reinforcement learning. We will build a <strong class="keyWord">Deep Q-learning Network</strong> (<strong class="keyWord">DQN</strong>) to play an Atari game. And finally, we learn how to train agents using the policy gradient algorithm.</p>
<p class="normal"><em class="chapterRef">Chapter 12</em>, <em class="italic">Probabilistic TensorFlow</em>, introduces TensorFlow Probability, the library built over TensorFlow to perform probabilistic reasoning and statistical analysis. The chapter demonstrates how to use TensorFlow Probability to generate synthetic data. We will build Bayesian networks and perform inference. The chapter also introduces the concept of uncertainty, aleatory and epistemic, and how to calculate the uncertainty of your trained models.</p>
<p class="normal"><em class="chapterRef">Chapter 13</em>, <em class="italic">An Introduction to AutoML</em>, introduces AutoML, whose goal is to enable domain experts who are unfamiliar with machine learning technologies to use ML techniques easily. We will go through a practical exercise using Google Cloud Platform and do quite a bit of hands-on work after briefly discussing the fundamentals. The chapter covers automatic data preparation, automatic feature engineering, and automatic model generation. Then, we introduce AutoKeras and Google Cloud AutoML with its multiple solutions for table, vision, text, translation, and video processing.</p>
<p class="normal"><em class="chapterRef">Chapter 14</em>, <em class="italic">The Math Behind Deep Learning</em>, covers the math behind deep learning. This topic is quite advanced and not necessarily required for practitioners. However, it is recommended reading to understand what is going on “under the hood” when we play with neural networks. We start with a historical introduction, and then we will review the high school concept of derivatives and gradients and introduce the gradient descent and backpropagation algorithms commonly used to optimize deep learning networks.</p>
<p class="normal"><em class="chapterRef">Chapter 15</em>, <em class="italic">Tensor Processing Unit</em>, discusses TPUs. TPUs are very special ASIC chips developed at Google for executing neural network mathematical operations in an ultra-fast manner. The core of the computation is a systolic multiplier that computes multiple dot products (row * column) in parallel, thus accelerating the computation of basic deep learning operations. Think of a TPU as a special-purpose co-processor for deep learning that is focused on matrix or tensor operations. We will review the four generations of TPUs so far, plus an additional Edge TPU for IoT.</p>
<p class="normal"><em class="chapterRef">Chapter 16</em>, <em class="italic">Other Useful Deep Learning Libraries</em>, introduces other deep learning frameworks. We will explore Hugging Face, OpenAI’s GPT3, and DALL-E 2. The chapter introduces another very popular deep learning framework, PyTorch. We also cover H2O.ai and its AutoML module. The chapter also briefly discusses the ONNX open-source format for deep learning models.</p>
<p class="normal"><em class="chapterRef">Chapter 17</em>, <em class="italic">Graph Neura</em><em class="italic">l Networks</em>, introduces graphs and graph machine learning, with particular emphasis on graph neural networks and the popular <strong class="keyWord">Deep Graph Library</strong> (<strong class="keyWord">DGL</strong>). We describe the theory behind various commonly used graph layers used in GNNs (and available in DGL) and provide examples of GNNs used for node classification, link prediction, and graph classification. We also show how to work with your own graph dataset and customize graph layers to create novel GNN architectures. We then cover more cutting-edge advances in the field of Graph ML, such as heterogeneous graphs and temporal graphs.</p>
<p class="normal"><em class="chapterRef">Chapter 18</em>, <em class="italic">Machine Learning Best Practices</em>, focuses on strategies and practices to follow to get the best model in training and production. The chapter discusses the best practices from two different perspectives: the best practices for the data and the best practices with respect to models.</p>
<p class="normal"><em class="chapterRef">Chapter 19</em>, <em class="italic">TensorFlow 2 Ecosystem</em>, lays out the different components of the TensorFlow ecosystem. We introduce TensorFlow Hub, a repository for pretrained deep learning models. The chapter talks about TensorFlow Datasets – a collection of ready-to-use datasets. We will also talk about TensorFlow Lite and TensorFlow JS – the framework for mobile and embedded systems and the web. Finally, the chapter talks about federated learning, a decentralized machine learning framework.</p>
<p class="normal"><em class="chapterRef">Chapter 20</em>, <em class="italic">Advanced Convolutional Neural Networks</em>, shows more advanced uses for <strong class="keyWord">convolutional neural networks</strong> (<strong class="keyWord">CNNs</strong>). We will explore how CNNs can be applied within the areas of computer vision, video, textual documents, audio, and music. We’ll conclude with a section summarizing convolution operations.</p>
<h2 class="heading-2" id="_idParaDest-8">Download the example code files</h2>
<p class="normal">The code bundle for the book is hosted on GitHub at <a href="https://packt.link/dltf"><span class="url">https://packt.link/dltf</span></a>. We also have other code bundles from our rich catalog of books and videos available at <a href="https://github.com/PacktPublishing/"><span class="url">https://github.com/PacktPublishing/</span></a>. Check them out!</p>
<h2 class="heading-2" id="_idParaDest-9">Download the color images</h2>
<p class="normal">We also provide a PDF file that has color images of the screenshots/diagrams used in this book. You can download it here: <a href="https://static.packt-cdn.com/downloads/9781803232911_ColorImages.pdf"><span class="url">https://static.packt-cdn.com/downloads/9781803232911_ColorImages.pdf</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-10">Conventions used</h2>
<p class="normal">There are a number of text conventions used throughout this book.</p>
<p class="normal"><code class="inlineCode">CodeInText</code>: Indicates code words in the text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles. For example: “Each neuron can be initialized with specific weights via the <code class="inlineCode">'kernel_initializer'</code> parameter.”</p>
<p class="normal">A block of code is set as follows:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Build the model.</span>
model = tf.keras.models.Sequential()
model.add(keras.layers.Dense(NB_CLASSES,
            input_shape=(RESHAPED,),
            name=<span class="hljs-string">'dense_layer'</span>, 
            activation=<span class="hljs-string">'softmax'</span>))
</code></pre>
<p class="normal">When we wish to draw your attention to a particular part of a code block, the relevant lines or items are highlighted:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Build the model.</span>
model = tf.keras.models.Sequential()
model.add(keras.layers.Dense(NB_CLASSES,
            input_shape=(RESHAPED,),
            <span class="code-highlight"><strong class="hljs-slc">name=</strong><strong class="hljs-string-slc">'</strong><strong class="hljs-string-slc">dense_layer'</strong><strong class="hljs-slc">, </strong></span>
            <span class="code-highlight"><strong class="hljs-slc">activation=</strong><strong class="hljs-string-slc">'softmax'</strong><strong class="hljs-slc">))</strong></span>
</code></pre>
<p class="normal">Any command-line input or output is written as follows:</p>
<pre class="programlisting con"><code class="hljs-con">pip install gym
</code></pre>
<p class="normal"><strong class="keyWord">Bold</strong>: Indicates a new term, an important word, or words that you see on the screen. For instance, words in menus or dialog boxes appear in the text like this. For example: “A <strong class="keyWord">Deep Convolutional Neural Network</strong> (<strong class="keyWord">DCNN</strong>) consists of many neural network layers.”</p>
<div class="note">
<p class="normal">Warnings or important notes appear like this.</p>
</div>
<div class="packt_tip">
<p class="normal">Tips and tricks appear like this.</p>
</div>
<h1 class="heading-1" id="_idParaDest-11">Get in touch</h1>
<p class="normal">Feedback from our readers is always welcome.</p>
<p class="normal"><strong class="keyWord">General feedback</strong>: Email <code class="inlineCode">feedback@packtpub.com</code> and mention the book’s title in the subject of your message. If you have questions about any aspect of this book, please email us at <code class="inlineCode">questions@packtpub.com</code>.</p>
<p class="normal"><strong class="keyWord">Errata</strong>: Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you have found a mistake in this book, we would be grateful if you reported this to us. Please visit <a href="http://www.packtpub.com/submit-errata"><span class="url">http://www.packtpub.com/submit-errata</span></a>, click <strong class="keyWord">Submit Errata</strong>, and fill in the form.</p>
<p class="normal"><strong class="keyWord">Piracy</strong>: If you come across any illegal copies of our works in any form on the internet, we would be grateful if you would provide us with the location address or website name. Please contact us at <code class="inlineCode">copyright@packtpub.com</code> with a link to the material.</p>
<p class="normal"><strong class="keyWord">If you are interested in becoming an author</strong>: If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, please visit <a href="http://authors.packtpub.com"><span class="url">http://authors.packtpub.com</span></a>.</p>
<h1 class="heading-1" id="_idParaDest-12">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1"><em class="italic">Deep Learning with Keras: Implementing deep learning models and neural networks with the power of Python</em>, Paperback – 26 Apr 2017, Antonio Gulli, Sujit Pal</li>
<li class="numberedList"><em class="italic">TensorFlow 1.x Deep Learning Cookbook</em>: <em class="italic">Over 90 unique recipes to solve artificial-intelligence driven problems with Python</em>, Antonio Gulli, Amita Kapoor</li>
</ol>
</div>
<div class="Basic-Text-Frame" id="_idContainer009">
<h1 class="heading-1" id="_idParaDest-13">Share your thoughts</h1>
<p class="normal">Once you’ve read <em class="italic">Deep Learning with TensorFlow and Keras, Third Edition</em>, we’d love to hear your thoughts! Please <a href="https://packt.link/r/1803232919"><span class="url">click here to go straight to the Amazon review page</span></a> for this book and share your feedback.</p>
<p class="normal">Your review is important to us and the tech community and will help us make sure we’re delivering excellent quality content.</p>
</div>
</div>
</body></html>