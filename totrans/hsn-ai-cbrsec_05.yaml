- en: Ham or Spam? Detecting Email Cybersecurity Threats with AI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most security threats use email as an attack vector. Since the amount of traffic
    conveyed in this way is particularly large, it is necessary to use automated detection
    procedures that exploit **machine learning** (**ML**) algorithms. In this chapter,
    different detection strategies ranging from linear classifiers and Bayesian filters
    to more sophisticated solutions such as decision trees, logistic regression, and **natural
    language processing** (**NLP**) will be illustrated.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to detect spam with Perceptrons
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image spam detection with **support vector machines** (**SVMs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Phishing detection with logistic regression and decision trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam detection with Naive Bayes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spam detection adopting NLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting spam with Perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the first concrete and successful applications of AI in the field of
    cybersecurity was spam detection, and one of the most famous open source tools
    is **SpamAssassin**.
  prefs: []
  type: TYPE_NORMAL
- en: The strategies that can be implemented for effective spam detection are different,
    as we will see in the course of the chapter, but the most common and simpler one
    uses **Neural Networks** (**NNs**) in the most basic form; that is, the Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Spam detection also provides us with the opportunity to introduce theoretical
    concepts related to NNs in a gradual and accessible way, starting with the Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Meet NNs at their purest – the Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The peculiar characteristic that unites all NNs (regardless of their implementation
    complexity) is that they conceptually mimic the behavior of the human brain. The
    most basic structure we encounter when we analyze the behavior of the brain, is
    undoubtedly the neuron.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Perceptron is one of the first successful implementations of a neuron in
    the field of **Artificial Intelligence** (**AI**). Just like a neuron in the human
    brain, it is characterized by a layered structure, aimed at associating a result
    in output to certain input levels, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8ed6924f-b57b-4920-916d-6bf7aca65538.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In the same way, the artificial representation of the neuron implemented through
    the Perceptron model is structured in such a way as to associate a given output
    value to one or more levels of input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce0d6325-06b4-42af-bf86-736180df18a3.png)'
  prefs: []
  type: TYPE_IMG
- en: The mechanism that transforms the input data into an output value is implemented
    by making use of an appropriate weighing of the values of an input, which are
    synthesized and forwarded to an activation function, which, when exceeding a certain
    threshold, produces a value of output that is forwarded to the remaining components
    of the NN.
  prefs: []
  type: TYPE_NORMAL
- en: It's all about finding the right weight!
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the differences in the approach between the statistical models and the
    AI algorithms is that the algorithms implement an optimization strategy based
    on the iteration. At each iteration, in fact, the algorithm tries to adjust its
    own estimate of the values, attributing to them a greater or lesser weight depending
    on the cost function that we must minimize. One of the aims of the algorithm is
    to identify precisely an optimal weight vector to be applied to the estimated
    values in order to obtain reliable future predictions on unknown future data.
  prefs: []
  type: TYPE_NORMAL
- en: To fully understand the power of AI algorithms applied to spam detection, we
    must first clarify the ideas on which tasks we should perform a spam filter.
  prefs: []
  type: TYPE_NORMAL
- en: Spam filters in a nutshell
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To understand the tasks performed by a spam filter, let's look at an example.
    Imagine separating the emails we receive, categorizing them based on the presence
    or the absence of particular keywords occurring within the text of the emails
    with a certain frequency. To this end, we could list all the messages we receive
    in our inbox within a table. But how will we proceed with classifying our messages
    as ham or spam?
  prefs: []
  type: TYPE_NORMAL
- en: As we said, we will look for the number of occurrences of the suspicious keywords
    within the text of the email messages. We will then assign a score to the individual
    messages identified as spam, based on the number of occurrences of identified
    keywords. This score will also provide us with a reference to classify subsequent
    email messages.
  prefs: []
  type: TYPE_NORMAL
- en: We will identify a threshold value that allows us to separate spam messages.
    If the calculated score exceeds the threshold value, the email will automatically
    be classified as spam; otherwise, it will be accepted as a legitimate message,
    and thus classified as ham. This threshold value (as well as the assigned scores)
    will be constantly redetermined to take into account the new series of spam messages
    that we will meet in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even from the abstract description of our spam detection algorithm, we notice
    some important features that must be kept in mind: we must proceed to identify
    a certain number of suspicious keywords that allow us to classify the messages
    as potential spam emails, assigning to each email a score based on the number
    of occurrences of identified keywords.'
  prefs: []
  type: TYPE_NORMAL
- en: We need to set a threshold value for the score assigned to the individual emails
    above which the emails will automatically be classified as spam. We must also
    correctly weigh the significance of the keywords present in the text of the emails
    in order to adequately represent the degree of probability that the message that
    contains them represents spam (the keywords, in fact, taken individually, could
    even be harmless, but put together, they are more likely to represent junk mail).
  prefs: []
  type: TYPE_NORMAL
- en: We must consider that the spammers are well aware of our attempt to filter unwanted
    messages, and therefore they'll try their best to adopt new strategies to deceive
    us and our spam filters. This translates into a process of continuous and iterative
    learning, which lends itself well to being implemented using an AI algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: From what we have said, it is clear that it is no coincidence that spam detection
    represents a first test in the adoption of AI in the cybersecurity field. The
    first spam detection solution, in fact, made use of static rules, using regular
    expressions to identify predefined patterns of suspicious words in the email text.
  prefs: []
  type: TYPE_NORMAL
- en: These static rules quickly proved to be ineffective as a result of the ever-new
    deception strategies implemented by spammers to deceive the anti-spam filters.
    It was therefore necessary to adopt a dynamic approach, which allowed the spam
    filter to learn based on the continuous innovations introduced by spammers, also
    taking advantage of the decisions made by the user in classifying their emails.
    This way, it was possible to effectively manage the explosive spread of the spam
    phenomenon.
  prefs: []
  type: TYPE_NORMAL
- en: Spam filters in action
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'How does an anti-spam algorithm actually behave in the classification of emails?
    First of all, let''s classify the emails based on suspicious keywords. Let''s
    imagine, for the sake of simplicity, that the list of the most representative
    suspicious keywords is thus reduced to only two words: buy and sex.'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we will classify the email messages within a table, showing
    the number of occurrences of the individual keywords identified within the text
    of the emails, indicating the messages as spam or ham:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d2b3161-314c-406a-9866-ed9e0b87d485.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, we will assign a score to every single email message.
  prefs: []
  type: TYPE_NORMAL
- en: This score will be calculated using a scoring function that takes into account
    the number of occurrences of suspicious keywords contained within the text.
  prefs: []
  type: TYPE_NORMAL
- en: A possible scoring function could be the sum of the occurrences of our two keywords,
    represented in this case by the *B* variable instead of the word buy, and the
    *S* variable instead of the word sex.
  prefs: []
  type: TYPE_NORMAL
- en: 'The scoring function therefore becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: <q>![](img/7fe320e9-9583-4ddf-b21b-50309382d3ff.png)</q>
  prefs: []
  type: TYPE_NORMAL
- en: We can also attribute different weights to the representative variables of the
    respective keywords, based on the fact that, for example, the keyword sex contained
    within the message is indicative of a greater probability of spam than the word
    buy.
  prefs: []
  type: TYPE_NORMAL
- en: It is clear that if both words are present in the text of the email, the probability
    of it being spam increases. Therefore, we will attribute a lower weight of *2* to
    the *B* variable and a greater weight of *3* to the *S* variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our scoring function, corrected with the relative weights assigned to the variables/keywords,
    therefore becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1080b19-8eb9-4a55-8151-2e18a348d0c4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now let''s try to reclassify our emails, calculating the relative scores with
    our scoring function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7a298cdb-ef25-4c2f-97f8-f51ebf195e7b.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, we must try to identify a threshold value that effectively separates
    spam from ham. Indeed, a threshold value between **4** and **5** allows us to
    properly separate the spam from the ham. In other words, in the event that a new
    email message scores a value equal to or greater than **4**, we would most likely
    be faced with spam rather than ham.
  prefs: []
  type: TYPE_NORMAL
- en: How can we effectively translate the concepts we have just seen into mathematical
    formulas that can be used in our algorithms?
  prefs: []
  type: TYPE_NORMAL
- en: To this end, linear algebra (as we mentioned in [Chapter 2](fbb9686a-7359-4659-bcdf-d0bf5e4e6af8.xhtml), *Setting
    Your AI for Cybersecurity Arsenal*, when we talked about the matrix implementation
    offered by the `numpy` library) comes to our aid.
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss further the implementation of Perceptrons, but first, we will
    introduce the concept of a linear classifier, useful for mathematically representing
    the task performed by a common spam detection algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting spam with linear classifiers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As known from linear algebra, the equation that represents the function used
    to determine the score to be associated with every single email message is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ebf6f6ad-2011-4d6c-b4b0-be0c9587e690.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This identifies a straight line in the Cartesian plane; therefore, the classifier
    used by our spam filter to classify emails is called a **linear classifier**.
    Using the known mathematical formalization commonly adopted in statistics, it
    is possible to redefine the previous equation in a more compact form by introducing
    the sum operator ![](img/e17c9d51-ab4d-4c35-a5ac-dc51b490b4fc.png), substituting
    in place of the *B* and *S* variables a matrix of indexed values ![](img/18e041cb-9379-4929-995d-7ad8488cbe71.png),
    and a vector of weights ![](img/cabcde63-c1a9-4098-bfca-ce82f3b97179.png) associated
    with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2c5ccfa-dd01-462f-86b1-74ba10310214.png)'
  prefs: []
  type: TYPE_IMG
- en: 'With the index *i,* which takes the values from *1* to *n, *this formalization
    is nothing more than the compact form of the previous summation between the variables
    and our relative weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/014da7e8-db01-451b-bf0f-d46ed7176999.png)'
  prefs: []
  type: TYPE_IMG
- en: This way, we have generalized our linear classifier to an unspecified number
    of variables, *n*, rather than limiting ourselves to *2* as in the previous case.
    This compact representation is also useful for exploiting linear algebra formulas
    in the implementation of our algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In fact, our function translates into a sum of products (between individual
    weights and variables) that can easily be represented as a product of matrices
    and vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f0458cf-0f6c-4264-8ad9-20a99cf8e4c6.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *wT* stands for the transposed weights carrier, necessary calculating
    the product of the matrices and vectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen, to adequately classify email messages, we need to identify
    an appropriate threshold value that correctly splits spam messages from ham messages:
    if the score associated with a single email message is equal to or higher than
    the threshold value, the message email will be classified as spam (and we will
    assign it the value `+1`); otherwise, it will be classified as ham (to which we
    will assign the value `-1`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In formal terms, we represent this condition as follows (where *θ* represents
    the threshold value):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/443b7012-87c0-4c7e-abec-3367a3abb7be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The preceding conditions are nothing but the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a3e4c31-ab06-452a-8774-8f2f7845186b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'It is a consolidated habit to further generalize this formalization by shifting
    the *θ* threshold value on the left side of the equation, associating it with
    the *x[0]* variable (thus introducing the *i = 0* positional index of the summation)
    to which we attribute the conventional value of 1, and a weight *w[0]* equal to
    *-θ* (that is, the threshold value taken with the negative sign, following the
    displacement of *θ* on the left side of the equation). Therefore, with *θ* we
    replace the product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/091ecebf-33c3-4e1a-b7fc-56971aec261b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This way, our compact formulation of the linear classifier takes its definitive
    form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed16fec0-b6bc-4fa0-b2f3-fa888f826544.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the index ![](img/85b1f65e-49c9-4197-96c2-1e0fba296044.png) now assumes
    the values from *0* to *n*.
  prefs: []
  type: TYPE_NORMAL
- en: How the Perceptron learns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The approach followed by Rosenblatt Perceptron model, which we have described
    so far in this chapter, is based on a simplified description of the neuron of
    the human brain. Just as the brain's neurons activate in the case of a positive
    signal, and remain inert otherwise, the Perceptron uses the threshold value via
    an activation function, which assigns a `+1` value (in case of excitement of the
    Perceptron, which indicates the pre-established threshold value has been exceeded),
    or a `-1` value (in other words, indicating a failure to exceed the threshold
    value).
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking up the previous mathematical expression that determines the conditions
    of activation of the Perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8db10016-290d-4dd5-8186-11841ebeac4f.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that it is the product of the ![](img/ab8d4619-eaa0-4f07-aaa6-634ba22ef1e2.png) values
    (that is, the input data for the corresponding weights) that has to overcome the
    *θ* threshold to determine the activation of the Perceptron. Since the *x[i]*
    input data is by definition prefixed, it is the value of the corresponding weights
    that helps to determine if the Perceptron has to activate itself or not.
  prefs: []
  type: TYPE_NORMAL
- en: But how are weights updated in practice, thus determining the Perceptron learning
    process?
  prefs: []
  type: TYPE_NORMAL
- en: 'The Perceptron learning process can be synthesized in the following three phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the weights to a predefined value (usually equal to *0*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculating the output value, ![](img/6bf7d5f2-776c-4897-87d9-093052675e4f.png), for
    each corresponding training sample, ![](img/d41005e9-d267-4216-bfb0-8794327e9c79.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating the weights on the basis of the distance between the expected output
    value (that is, the ![](img/19a2dde2-3988-4d4e-8097-65ce1b3bad83.png) value associated
    with the original class label of the corresponding input data, ![](img/caa3cac9-acc6-44e0-9e39-83ce74d8ad8b.png))
    and the predicted value (the ![](img/f30fa8fa-3141-4c99-9118-a3fb1ef950a5.png)value estimated
    by the Perceptron)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, the individual weights are updated according to the following
    formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ffa93f5-2b2a-48d6-a6ef-bc9dda3c258d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, the ![](img/b5c79404-c080-4be4-ae11-a978b4912c51.png) value represents
    the deviation between the expected (***y***) value and the predicted value (![](img/70392b2e-fb5a-4bd5-b2fb-f404b267f785.png)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53423643-515b-4686-b32f-bb24a7e4bae1.png)'
  prefs: []
  type: TYPE_IMG
- en: As is evident from the preceding formula, the deviation between the expected
    ***y*** value and predicted ![](img/3a2ca1aa-1975-4a19-bbfe-ac56a659697f.png)
    value is multiplied by the value of input ![](img/d338e333-09e9-49cb-a2d2-824e58873414.png),
    and by the ![](img/15dc2b1d-f3df-4f4d-a0ac-cd4218c3e8c7.png) constant, which represents
    the learning rate assigned to the Perceptron. The ![](img/c2622b38-1fd9-402e-afc9-12e4c79d7e84.png)
    constant usually assumes a value between *0.0* and *1.0*, a value that is assigned
    at the Perceptron initialization phase.
  prefs: []
  type: TYPE_NORMAL
- en: As we will see, the value of the learning rate is crucial for the learning of
    the Perceptron, and it is therefore necessary to carefully evaluate (even by trial
    and error) the value to be attributed to the ![](img/64f04147-11a8-43bf-9f96-79cff0f45531.png)
    constant to optimize the results returned from the Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: A simple Perceptron-based spam filter
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now see a concrete example of the use of the Perceptron. We will use
    the `scikit-learn` library to create a simple spam filter based on the Perceptron.
    The dataset we will use to test our spam filter is based on the sms spam messages
    collection, available at [https://archive.ics.uci.edu/ml/datasets/sms+spam+collection](https://archive.ics.uci.edu/ml/datasets/sms+spam+collection)
  prefs: []
  type: TYPE_NORMAL
- en: The original dataset can be downloaded in CSV format; we proceeded to process
    the data contained in the CSV file, transforming it into numerical values to make
    it manageable by the Perceptron. Moreover, we have selected only the messages
    containing the buy and sex keywords (according to our previous description), counting
    for each message (be it spam or ham) the number of occurrences of the keywords
    present in the text of the message.
  prefs: []
  type: TYPE_NORMAL
- en: The result of our preprocessing is available in the `sms_spam_perceptron.csv`
    file (attached to the source code repository that comes with this book).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then proceed with the loading of data from the `sms_spam_perceptron.csv` file,
    through the `pandas` library, extracting from the `DataFrame` of pandas the respective
    values, referenced through the `iloc()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We have, therefore, assigned the class labels `ham` and `spam` (present in the
    `.csv` file in the first column of the `DataFrame`) to the `y` variable (which
    represents the vector of the expected values) using the `iloc()` method. Moreover,
    we have converted the previously mentioned class labels into the numerical values
    of `-1` (in the case of spam) and `+1` (in the case of ham) using the `where()`
    method of NumPy, to allow us to manage the class labels with the Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: In the same way, we assigned to the `X` matrix the values corresponding to the
    `sex` and `buy` columns of the `DataFrame`, containing the number of occurrences
    corresponding to the two keywords within the message text. These values are also
    in numerical format, so it is possible to feed them to our Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before proceeding with the creation of the Perceptron, we divide the input
    data between training data and test data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Using the `train_test_split()` method applied to the `X` and `y` variables,
    we split the dataset into two subsets, assigning a percentage of 30% of the original
    dataset (using the parameter `test_size = 0.3`) to the test values, and the remaining
    70% to the training values.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we can define our Perceptron by instantiating the `Perceptron`
    class of the `sklearn.linear_model` package:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: During the initialization phase of the `p` Perceptron, we assigned a maximum
    number of iterations equal to `40` (with the `max_iter = 40parameter` initialization)
    and a learning rate equal to `0.1` (`eta0 = 0.1`). Finally, we invoked the `fit()`
    method of the Perceptron, training the `p`object with the training data.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now proceed to estimate the values on the test data, invoking the `predict()`
    method of the Perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: As a consequence of the training phase on the sample data (which accounts for
    70% of the original dataset), the Perceptron should now be able to correctly estimate
    the expected values of the test data subset (equal to the remaining 30% of the
    original dataset).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the accuracy of the estimated values returned by the Perceptron
    using the `sklearn.metrics` package of `scikit-learn` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: By comparing the test data (`y_test`) with the predicted values (`y_pred`),
    and summing up the overall number of mismatches, we are now able to evaluate the
    accuracy of the predictions provided by the Perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, the percentage of accuracy is quite good (90%), since the total
    number of cases of incorrect classifications amounts to only three.
  prefs: []
  type: TYPE_NORMAL
- en: Pros and cons of Perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite the relative simplicity of the implementation of the Perceptron (simplicity
    here constitutes the strength of the algorithm, if compared to the accuracy of
    the predictions provided), it suffers from some important limitations. Being essentially
    a binary linear classifier, the Perceptron is able to offer accurate results only
    if the analyzed data can be linearly separable; that is, it is possible to identify
    a straight line (or a hyperplane, in case of multidimensional data) that completely
    bisects the data in the Cartesian plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77014850-7866-4ff8-a0a2-2bad99558bef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If instead (and this is so in the majority of real cases) the analyzed data
    was not linearly separable, the Perceptron learning algorithm would oscillate
    indefinitely around the data, looking for a possible vector of weights that can
    linearly separate the data (without, however, being able to find it):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8cfba7dd-ad73-4b1a-b4e9-20d90b614b91.png)'
  prefs: []
  type: TYPE_IMG
- en: Therefore, the convergence of the Perceptron is only possible in the presence
    of linearly separable data, and in the case of a small learning rate. If the classes
    of data are not linearly separable, it is of great importance to set a maximum
    number of iterations (corresponding to the `max_iter` parameter) in order to prevent
    the algorithm from oscillating indefinitely in search of an (nonexistent) optimal
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: One way to overcome the Perceptron's practical limitations is to accept a **wider
    margin** of data separation between them. This is the strategy followed by SVMs,
    a topic we'll encounter in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Spam detection with SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SVMs are an example of *supervised* algorithms (as well as the Perceptron),
    whose task is to identify the hyperplane that best separates classes of data that
    can be represented in a **multidimensional space**. It is possible, however, to
    identify different hyperplanes that correctly separate the data from each other;
    in this case, the choice falls on the hyperplane that **optimizes the prefixed
    margin**, that is, the distance between the hyperplane and the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the advantages of the SVM is that the identified hyperplane is **not
    limited** to the linear model (unlike the Perceptron), as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/57633dad-9b33-4d6a-97ed-408eda75ecae.png)'
  prefs: []
  type: TYPE_IMG
- en: The SVM can be considered as an extension of the Perceptron, however. While
    in the case of the Perceptron, our goal was to **minimize** classification errors,
    in the case of SVM, our goal instead is to **maximize** the margin, that is, the
    **distance** between the hyperplane and the training data *closest* to the hyperplane
    (the nearest training data is thus known as a **support vector**).
  prefs: []
  type: TYPE_NORMAL
- en: SVM optimization strategy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why choose the hyperplane that maximizes the margin in the first place? The
    reason lies in the fact that **wider margins** correspond to fewer classification
    errors, while with **narrower margins** we risk incurring the phenomenon known
    as **overfitting** (a real disaster that we may incur when dealing with *iterative*
    algorithms, as we will see when we will discuss verification and optimization
    strategies for our AI solutions).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can translate the SVM optimization strategy in mathematical terms, similar
    to what we have done in the case of the Perceptron (which remains our starting
    point). We define the condition that must be met to assure that the SVM correctly
    identifies the best hyperplane that separates the classes of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ef752721-9f69-4f15-a213-e9a48c28e73f.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the *β* constant represents the *bias*, while *µ* represents our *margin*
    (which assumes the maximum possible positive value in order to obtain the best
    separation between the classes of values).
  prefs: []
  type: TYPE_NORMAL
- en: In practice, to the algebraic multiplication (represented by ![](img/d31c5f8b-b344-4e90-b3dd-54e4eeece6a4.png))
    we add the value of the *β* bias, which allows us to obtain a value greater than
    or equal to zero, in the presence of values ​​that fall in the same **class label**
    (remember that *y* can only assume the values of ​​`-1` or `+1` to distinguish
    between the corresponding classes to which the samples belong, as we have already
    seen in the case of the Perceptron).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the value calculated in this way is compared with the ![](img/0b961c0e-f1c9-427f-82ce-c80f17a8b870.png) margin in
    order to ensure that the distance between each sample and the separating hyperplane
    we identified (thus constituting our decision boundary) is greater or at most
    equal to our margin (which, as we have seen, is identified as the maximum possible
    positive value, in order to obtain the *best* separation between the classes of
    values).
  prefs: []
  type: TYPE_NORMAL
- en: SVM spam filter example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's go back to our sample spam filter, and replace the Perceptron with an
    SVM, as we have seen in the identification of the hyperplane that we are not limited
    to using linear classifier models only (being able to choose between classifiers
    characterized by greater complexity).
  prefs: []
  type: TYPE_NORMAL
- en: However, to compare the results obtained previously with the Perceptron, which
    represents a strictly linear classifier, we will also choose a linear classifier
    in the case of the SVM.
  prefs: []
  type: TYPE_NORMAL
- en: This time, however, our dataset (stored in the `sms_spam_svm.csv` file, and
    derived from the collection of SMS spam messages we found earlier in the chapter,
    in which the total occurrences of the various suspicious keywords were extracted
    and compared to the total number of harmless words appearing within the messages)
    is not strictly linearly separable.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same way as in the case of the Perceptron, we will proceed to load the
    data with `pandas`, associating the class labels with the corresponding ​​`-1` values
    (in the case of spam) and `1` (in the case of ham):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data has been loaded, we proceed to split the original dataset into
    30% test data and 70% training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can thus proceed to instantiate our SVM, importing the `SVC`
    class (which stands for **support vector classifier**) from the `sklearn.svm` package,
    choosing the linear classifier (`kernel = ''linear''`), then proceeding to the
    model training by invoking the `fit()` method, and finally estimating the test
    data by invoking the `predict()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now evaluate the accuracy of the predictions returned by the SVM algorithm,
    making use of the `sklearn.metrics` package as we did with the Perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Even in the presence of non-linearly separable data, we see how well the SVM
    algorithm behaves, since the level of accuracy of the predictions accounts to
    84%, with the number of incorrect classifications accounting to only 7 cases.
  prefs: []
  type: TYPE_NORMAL
- en: Image spam detection with SVMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The versatility of the SVM algorithm allows us to deal with even more complex
    real-world classification cases, such as in the case of spam messages represented
    by images, instead of simple text.
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, spammers are well aware of our detection attempts, and therefore
    try to adopt all possible solutions to deceive our filters. One of the evasion
    strategies is to use images as a vehicle for spreading spam, instead of simple
    text.
  prefs: []
  type: TYPE_NORMAL
- en: 'For some time, however, viable image-based spam detection solutions have been
    available. Among these, we can distinguish detection strategies based on the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Content-based filtering**: The approach consists of trying to identify the
    suspect keywords that are most commonly used in textual spam messages even within
    images; to this end, pattern recognition techniques leveraging optical character
    recognition (**OCR**) technology are implemented in order to extract text from
    images (this is the solution that SpamAssassin adopts).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non content-based filtering**: In this case, we try to identify specific
    features of spam images (such as color features and so on), on the grounds that
    spam images, being computer-generated, show different characteristics compared
    to natural images; for the extraction of the features, we make use of advanced
    recognition techniques based on NNs and **deep learning** (**DL**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How did SVM come into existence?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the salient features of the images have been extracted, and the corresponding
    samples have been classified within their respective classes (spam or ham), it
    is possible to exploit an SVM to perform model training on these features.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most recent projects on this subject is *Image Spam Analysis* by
    Annapurna Sowmya Annadatha ([http://scholarworks.sjsu.edu/etd_projects/486](http://scholarworks.sjsu.edu/etd_projects/486)),
    which is characterized by the innovative approach adopted, based on the assumption
    that the features that characterize a spam image, being computer generated, are
    different to those associated with an image generated by a camera; and the selective
    use of SVM, which leads to high accuracy of results compared to a reduced cost
    in computational terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'The approach consists of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the classifier using the linear SVM and the feature set
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the SVM weights for all the features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the first one with the largest weights
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a model based on the subset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For further information, refer to the project reference mentioned in the previous
    paragraph.
  prefs: []
  type: TYPE_NORMAL
- en: Phishing detection with logistic regression and decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After having analyzed the Perceptron and the SVM, we now deal with alternative
    classification strategies that make use of logistic regression and decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: But before continuing, we will discover the distinctive features of these algorithms
    and their use for spam detection and phishing, starting with regression models.
  prefs: []
  type: TYPE_NORMAL
- en: Regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression models are undoubtedly the most used of all learning algorithms.
    Developed from statistical analysis, regression models have quickly spread in
    ML and in AI in general. The most known and used regression model is linear regression,
    thanks to the simplicity of its implementation and the good predictive capacity
    that it allows us to achieve in many practical cases (such as estimating the level
    of house prices in relation to changes in interest rates).
  prefs: []
  type: TYPE_NORMAL
- en: Alongside the linear model, there is also the logistic regression model, especially
    useful in the most complex cases, where the linear model proves to be too rigid
    for the data to be treated. Both models, therefore, represent the tools of choice
    for analysts and algorithm developers.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will analyze the characteristics and advantages of regression
    models, and their possible uses in the field of spam detection. Let's start our
    analysis with the simplest model, the linear regression model, which will help
    us make comparisons with the logistic regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing linear regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The linear regression model is characterized by the fact that the data is represented
    as sums of features, leading to a straight line in the Cartesian plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'In formal terms, linear regression can be described by the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/61f944ad-cec4-4a1f-8dbd-4f4c901079b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y* represents the predicted values, which are the result of the linear
    combination of the single features (represented by the *X* matrix) to which a
    weight vector is applied (represented by the *w vector*), and by the addition
    of a constant (*β*), which represents the default predicted value when all features
    assume the value of zero (or simply are missing).
  prefs: []
  type: TYPE_NORMAL
- en: The *β* constant can also be interpreted as the systematic distortion of the
    model, and corresponds graphically with the intercept value on the vertical axis
    of the Cartesian plane (that is to say, the point where the regression line meets
    the vertical axis).
  prefs: []
  type: TYPE_NORMAL
- en: 'Obviously, the linear model can be extended to cases in which there is more
    than just one feature. In this case, the mathematical formalization assumes the
    following aspect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1707e517-6e40-432d-8e07-1b998b2aa02d.png)'
  prefs: []
  type: TYPE_IMG
- en: The geometric representation of the previous formula will correspond to a hyperplane
    in the *n*-dimensional space, rather than a straight line in the Cartesian plane.
    We have mentioned the importance of the ![](img/ee6a30be-f904-4e97-8f92-ca8517a68fa8.png)
    constant as the default predictive value of the model in the case in which the
    features assume a value equal to zero.
  prefs: []
  type: TYPE_NORMAL
- en: The individual ​​![](img/6dffb83c-d6d6-4145-9f31-e6ec4acba0a6.png) values within
    the vector of the weights, ![](img/aa889ef4-0ff3-4e3f-9b5b-05c300af8b09.png),
    can be interpreted as a measure of the intensity of the corresponding features,
    ![](img/1cd520fc-8894-40a4-b160-ca820d1ac691.png)[.]
  prefs: []
  type: TYPE_NORMAL
- en: In practice, if the value of the ![](img/5c0cd86a-07c6-4abd-b14a-4a395f6a092f.png)
    weight is close to zero, the corresponding ![](img/37084f2a-9bfa-4559-b9b5-38177aebd8c1.png)
    feature assumes a minimum importance (or none at all) in the determination of
    predicted values. If, instead, the ![](img/d5561866-a060-4c5f-984e-3f1a36a87a7b.png)
    weight assumes positive values, it will amplify the final value returned by the
    regression model.
  prefs: []
  type: TYPE_NORMAL
- en: If, on the other hand, ![](img/89ba77ce-4e92-4de7-a30e-42d0f1230e70.png) assumes
    negative values, it will help to reverse the direction of the model's predictions,
    as the value of the *![](img/f4aae63f-6229-4f1f-a898-807f7044571d.png)* feature increases,
    it will correspond to a decrease in the value estimated by the regression. Hence,
    it is important to consider the impacts of the weights on the *![](img/9c016c68-d6c7-4054-9d11-5616f3db9da9.png)* features, as
    they are determinant in the correctness of the predictions that we can derive
    from the regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression with scikit-learn
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the following code snippet, we will see how to implement a simple predictive
    model based on linear regression, using the `linear_model` module of `scikit-learn`,
    which we will feed with one of the previously used spam message datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: To verify the accuracy of the predictions provided by the linear regression
    model, we can use the `score()` method, which gives us the measurement of the
    coefficient of the R² determination.
  prefs: []
  type: TYPE_NORMAL
- en: This coefficient varies between `0` and `1`, and measures how much better the
    predictions returned by the linear model are, when compared to the simple mean.
  prefs: []
  type: TYPE_NORMAL
- en: Linear regression – pros and cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we have seen, the simplicity of implementation represents an undoubted advantage
    of the linear regression model. However, the limitations of the model are rather
    important.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, the linear regression model can only be used to manage quantitative
    data, whereas in the case where the predictive analysis used categorical data,
    we have to resort to the logistic regression model. Furthermore, the major limitation
    of linear regression is that the model assumes that features are mostly unrelated;
    that is, they do not influence each other. This assumption legitimizes the representation
    of the products between the features and their respective weights as sums of independent
    terms.
  prefs: []
  type: TYPE_NORMAL
- en: There are, however, real cases in which this assumption is unrealistic (for
    example, the possible relationship between variables such as the age and the weight
    of a person, which are related to each other, as weight varies according to age).
    The negative side effect of this assumption consists in the fact that we risk
    adding the same information several times, failing to correctly predict the effect
    of the combination of the variables on the final result.
  prefs: []
  type: TYPE_NORMAL
- en: In technical terms, the linear regression model is characterized by a greater
    bias in the predictions, instead of greater variance (we will have the opportunity
    to face the trade-off between bias and variance later on).
  prefs: []
  type: TYPE_NORMAL
- en: In other words, when the data being analyzed exhibits complex relationships,
    the linear regression model leads us to systematically distorted predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have seen that one of the limits of linear regression is that it cannot
    be used to solve classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, in case we wanted to use linear regression to classify the samples
    within two classes (as is the case in spam detection) whose labels are represented
    by numerical values ​​(for example, `-1` for **spam**, and `+1` for **ham**),
    the linear regression model will try to identify the result that is closest to
    the target value (that is, linear regression has the purpose of minimizing forecasting
    errors). The negative side effect of this behavior is that it leads to greater
    classification errors. With respect to the Perceptron, linear regression does
    not give us good results in terms of classification accuracy, precisely because
    linear regression works better with continuous intervals of values, rather than
    with classes of discrete values ​​(as is the case in classification).
  prefs: []
  type: TYPE_NORMAL
- en: An alternative strategy, most useful for the purposes of classification, consists
    of estimating the probability of the samples belonging to individual classes.
    This is the strategy adopted by logistic regression (which, in spite of the name,
    constitutes a classification algorithm, rather than a regression model).
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical formulation of logistic regression is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a127c2e4-f02b-4988-abb2-688c11c6e204.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![](img/dc3f4c7d-604d-4827-977a-748627caf92e.png). ![](img/2760b21d-2534-48ee-bdb2-9e6f36a3a9d7.png)
    therefore measures the conditional probability that a given sample falls into
    the ![](img/00e35927-6dee-4570-ab02-1fd300f2007b.png) class, given the ![](img/28a16344-441e-46fb-9f56-43417e767bb3.png)
    features.
  prefs: []
  type: TYPE_NORMAL
- en: A phishing detector with logistic regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can then use logistic regression to implement a phishing detector, exploiting
    the fact that logistic regression is particularly useful for solving classification
    problems. Like spam detection, phishing detection is nothing more than a sample
    classification task.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, we will use the dataset available on the UCI machine learning
    repository website ([https://archive.ics.uci.edu/ml/datasets/Phishing+Websites](https://archive.ics.uci.edu/ml/datasets/Phishing+Websites)).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset has been converted into CSV format starting from the original `.arff`
    format, using the data wrangling technique known as **one-hot encoding** ([https://en.wikipedia.org/wiki/One-hot](https://en.wikipedia.org/wiki/One-hot)),
    and consists of records containing 30 features that characterize phishing websites.
  prefs: []
  type: TYPE_NORMAL
- en: 'Find the source code of our detector in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the level of accuracy of the logistic regression classifier is
    quite good, as the model is able to correctly detect over 90% of URLs.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression pros and cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The advantages of adopting logistic regression can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The model can be trained very efficiently
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used effectively even in the presence of a large number of features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm has a high degree of scalability, due to the simplicity of its
    scoring function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the same time, however, logistic regression suffers from some important limitations,
    deriving from the basic assumptions that characterize it, such as the need for
    the features to be linearly independent (a rule that translates in technical terms
    as the absence of multicollinearity), as well as requiring more training samples
    on average than other competing algorithms, as the maximum likelihood criterion
    adopted in logistic regression is known to be less powerful than, say, the least
    squares method used in linear regression to minimize prediction errors.
  prefs: []
  type: TYPE_NORMAL
- en: Making decisions with trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous paragraphs, when we have to choose which algorithm
    to use to perform a given task, we must consider the type of features that characterize
    our data. The features can in fact be made up of quantitative values ​​or qualitative
    data.
  prefs: []
  type: TYPE_NORMAL
- en: ML algorithms are obviously more at ease when dealing with quantitative values;
    however, most of the real cases involve the use of data expressed in a qualitative
    form (such as descriptions, labels, words, and so on) that imply information expressed
    in non-numerical form.
  prefs: []
  type: TYPE_NORMAL
- en: As in the case of spam detection, we have seen how the translation in numerical
    form (a practice known as **numeric encoding**) of qualitative features (such
    as the spam and ham labels, to which we assigned the numerical values of ​​`-1`
    and `+1`, respectively) only partially solve the classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: It is not by chance that the paper entitled *Induction of Decision Trees* ([http://dl.acm.org/citation.cfm?id=637969](http://dl.acm.org/citation.cfm?id=637969)),
    in which John Ross Quinlan described the decision trees algorithm, takes into
    consideration information conveyed in qualitative form. The object of the paper
    by Quinlan (whose contribution was significant for the development of decision
    trees) is in fact the choice of whether to play tennis outside, based on features
    such as outlook (sunny, overcast, or rain), temperatures (cool, mild, or hot),
    humidity (high or normal), windy (true or false).
  prefs: []
  type: TYPE_NORMAL
- en: How can we instruct a machine to process information presented both in quantitative
    and qualitative forms?
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees rationales
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees use binary trees to analyze and process data, thus succeeding
    in formulating predictions concerning both values ​​expressed in numerical and
    categorical form, accepting both numerical values ​​and qualitative information
    as input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To intuitively understand the strategy adopted by decision trees, let''s see
    the typical steps involving their implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: The first step consists in subdividing the original dataset into two child subsets,
    after having verified a binary condition, following the first subdivision, we
    will have two child subsets as a result, in which the binary condition is verified
    or falsified.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The child subsets will be further subdivided on the basis of further conditions;
    at each step, the condition that provides the best bipartition of the original
    subset is chosen (for this purpose, appropriate metrics are used to measure the
    quality of the subdivision).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The division proceeds in a recursive manner. It is therefore necessary to define
    a stopping condition (such as the achievement of a maximum depth).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each iteration, the algorithm generates a tree structure in which the child
    nodes represent the choices taken at each step, with each leaf contributing to
    the overall classification of the input data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Take a look at the following diagram, which depicts the decision tree for the
    Iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/003c40cf-dfa5-4005-9e8a-ac54f87b6aa1.png)'
  prefs: []
  type: TYPE_IMG
- en: Decision trees are also very efficient in the elaboration of large datasets.
    In fact, the characteristics of the tree data structures allow us to limit the
    complexity of the algorithm to an order of magnitude equal to *0* (*log n*).
  prefs: []
  type: TYPE_NORMAL
- en: Phishing detection with decision trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now see the use of decision trees in the task of phishing detection.
    As we said in the previous paragraphs, phishing detection (as well as spam filtering)
    basically involves the classification of input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can see how the decision tree classifier further enhances the already excellent
    performance obtained previously with logistic regression.
  prefs: []
  type: TYPE_NORMAL
- en: Decision trees – pros and cons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the advantages already described, we must remember the possible
    disadvantages related to decision trees; these are essentially associated with
    the phenomenon of overfitting, which is due to the complexity of the tree data
    structures (it is in fact necessary to proceed in a systematic manner with the
    pruning of the tree, in order to reduce its overall complexity).
  prefs: []
  type: TYPE_NORMAL
- en: One of the undesirable consequences of the complexity is the high sensitivity
    of the algorithm to even the smallest changes in the training dataset, which can
    lead to sensible impacts on the prediction model. Therefore, decision trees are
    not the best fit for incremental learning.
  prefs: []
  type: TYPE_NORMAL
- en: Spam detection with Naive Bayes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the advantages associated with the use of Naive Bayes is the fact that
    it requires little starting data to begin classifying input data; moreover, the
    information that progressively adds up contributes to dynamically updating the
    previous estimates, incrementally improving the forecasting model (unlike, as
    we saw in the previous paragraph, the algorithm based on decision trees).
  prefs: []
  type: TYPE_NORMAL
- en: Advantages of Naive Bayes for spam detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The aforementioned features are well suited to the task of spam detection. Without
    the need to resort to large datasets, in fact, the spam detection algorithms based
    on Naive Bayes can exploit the emails already present in the inbox, constantly
    updating the probability estimations on the base of new email messages that are
    progressively added to those already existing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The constant process of updating the probability estimates is based on the
    well-known Bayes rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6283b99d-def2-4213-8f43-85cd788579df.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding equation describes the relationship between the probability of
    the occurrence of an event, *A,* conditioned to the evidence, *E*.
  prefs: []
  type: TYPE_NORMAL
- en: This relationship depends on the probability of *A* (prior probability) and
    the likelihood, ![](img/834b7330-4413-44d0-a70a-be766c67857f.png), of evidence, *E*,
    which determines the probability estimate, ![](img/cf3e7f1e-cb2f-4e29-b54b-85a51147b8b1.png)
    (the posterior probability).
  prefs: []
  type: TYPE_NORMAL
- en: An important feature of the Bayes rule probability update is that the probability
    ![](img/fb696743-a298-4c7f-9d4f-1f72d190f1eb.png) (the posterior probability),
    as a result of the updating process, becomes the new prior probability, thus contributing
    to dynamically updating the existing probability estimates.
  prefs: []
  type: TYPE_NORMAL
- en: Why Naive Bayes?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the basic assumptions of the Bayes rule is that it postulates the independence
    of events. This assumption is not always realistic.
  prefs: []
  type: TYPE_NORMAL
- en: However, in most cases, it is a reasonable condition that leads to good forecasts,
    while at the same time simplifying the application of the Bayes rule, especially
    in the presence of several competing events, thus reducing the calculations to
    a simple multiplication of the probabilities associated with each event.
  prefs: []
  type: TYPE_NORMAL
- en: Before seeing Naive Bayes in action, applying the algorithm in spam detection,
    we need to analyze the text analysis techniques to allow Naive Bayes to dynamically
    recognize the suspect keywords used by spammers (rather than choosing them in
    a fixed fashion, as we did in the previous examples).
  prefs: []
  type: TYPE_NORMAL
- en: NLP to the rescue
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most exciting areas of AI is certainly NLP, which consists of the
    analysis and automated understanding of human language.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of NLP is to try to extract sensible information from unstructured
    data (such as email messages, tweets, and Facebook posts).
  prefs: []
  type: TYPE_NORMAL
- en: The fields of application of NLP are huge, and vary from simultaneous translations
    to sentiment analysis speech recognition.
  prefs: []
  type: TYPE_NORMAL
- en: NLP steps
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The phases that characterize NPL are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Identification of the words (tokens) constituting the language
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analysis of the structure of the text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Identification of the relationships between words (in paragraphs, sentences,
    and so on)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Semantic analysis of the text
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One of the best known Python libraries for NLP is the **Natural Language Toolkit**
    (**NLTK**), often used for spam detection.
  prefs: []
  type: TYPE_NORMAL
- en: In the following example, we will see how to take advantage of NLTK combined
    with Naive Bayes to create a spam detector.
  prefs: []
  type: TYPE_NORMAL
- en: A Bayesian spam detector with NLTK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As a concluding example, we will show the use of a classifier based on Naive
    Bayes, using `MultinomialNB` from the `sklearn.naive_bayes` module. As usual,
    we will divide the original dataset consisting of the spam message archive in
    CSV format, assigning a percentage equal to 30% to the test data subset, and the
    remaining 70% to the training data subset.
  prefs: []
  type: TYPE_NORMAL
- en: The data will be treated with the **bag of words** (**BoW**) technique, which
    assigns a number to each identified word in the text using `CountVectorizer` of `sklearn`,
    to which we will pass the `get_lemmas()` method, which returns the individual
    tokens extracted from the text of the messages.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will proceed to normalize and weigh the data using `TfidfTransformer`,
    which transforms a count matrix to a normalized `tf` or `tf-idf` representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the scikit-learn documentation for `TfidfTransformer` ([https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)),
    we can find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Tf means term frequency, while tf-idf means term-frequency times inverse document
    frequency. This is a common term weighting scheme in information retrieval that
    has also found good use in document classification. The goal of using tf-idf instead
    of the raw frequencies of occurrence of a token in a given document is to scale
    down the impact of tokens that occur very frequently in a given corpus and that
    are hence empirically less informative than features that occur in a small fraction
    of the training corpus<q>."</q>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get into the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can check if `spam_detector` works well by trying to run a prediction on
    a random message (in our example, we chose the 26^(th) message from the dataset),
    and checking that the detector correctly classifies the type of message (spam
    or ham) by comparing the predicted value with the corresponding `type` label associated
    with the message:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, once the correct functioning has been verified, we proceed to
    the prediction on the whole dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding commands generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1782eb55-8ee7-4a9c-b80d-23c1110c6e34.png)'
  prefs: []
  type: TYPE_IMG
- en: As can be seen in the preceding screenshot, the level of accuracy of Naive Bayes
    is already quite high (equal to 80%) with the advantage, unlike the other algorithms,
    that this accuracy can improve further still as the number of messages analyzed
    increases.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, several supervised learning algorithms have been explained,
    and we have seen their concrete application in solving common tasks in the field
    of cybersecurity, such as spam detection and phishing detection.
  prefs: []
  type: TYPE_NORMAL
- en: The knowledge acquired in this chapter contributes to forming the right mindset
    to face increasingly complex tasks, such as those we will face in the next chapters,
    leading to a greater awareness of the advantages and disadvantages associated
    with each AI algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about malware analysis and advanced malware
    detection with DL.
  prefs: []
  type: TYPE_NORMAL
