- en: 7\. Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how **convolutional neural networks** (**CNNs**)
    process image data. You will also learn how to correctly use a CNN on image data.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will be able to create your own CNN for classification
    and object identification on any image dataset using TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covers CNNs. CNNs use convolutional layers that are well-suited
    to extracting features from images. They use learning filters that correlate with
    the task at hand. Simply put, they are very good at finding patterns in images.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, you explored regularization and hyperparameter tuning.
    You used L1 and L2 regularization and added dropout to a classification model
    to prevent overfitting on the `connect-4` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: You will now be shifting gears quite a bit as you dive into deep learning with
    CNNs. In this chapter, you will learn the fundamentals of how CNNs process image
    data and how to apply those concepts to your own image classification problem.
    This is truly where TensorFlow shines.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'CNNs share many common components with the ANNs you have built so far. The
    key difference is the inclusion of one or more convolutional layers within the
    network. Convolutional layers apply convolutions of input data with filters, also
    known as kernels. Think of a **convolution** as an **image transformer**. You
    have an input image, which goes through the CNN and gives you an output label.
    Each layer has a unique function or special ability to detect patterns such as
    curves or edges in an image. CNNs combine the power of deep neural networks and
    kernel convolutions to transform images and make these image edges or curves easy
    for the model to see. There are three key components in a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input image**: The raw image data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filter/kernel**: The image transformation mechanism'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output label**: The image classification'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following figure is an example of a CNN in which the image is input into
    the network on the left-hand side and the output is generated on the right-hand
    side. The image components are identified throughout the hidden layers with more
    basic components, such as edges, identified in earlier hidden layers. Image components
    combine in the hidden layers to form recognizable features from the dataset. For
    example, in a CNN to classify images into planes or cars, the recognizable features
    may be filters that resemble a wheel or propellor. Combinations of these features
    will be instrumental in determining whether the image is a plane or a car.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the output layer is a dense layer used to determine the specific output
    of the model. For a binary classification model, this may be a dense layer with
    one unit with a sigmoid activation function. For a more complex multi-class classification,
    it may be a dense layer with many units, determined by the number of classes,
    and a softmax activation function to determine one output label for each image
    presented to the model.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.1: CNN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.1: CNN'
  prefs: []
  type: TYPE_NORMAL
- en: A common CNN configuration includes a convolutional layer followed by a pooling
    layer. These layers are often used together in this order, as pairs (convolution
    and pooling). We'll get into the reason for this later in the chapter, but for
    now, think of these pooling layers as decreasing the size of input images by summarizing
    the filter results.
  prefs: []
  type: TYPE_NORMAL
- en: Before you move deeper into convolutional layers, you first need to understand
    what the data looks like from the computer's perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Image Representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, consider how a computer processes an image. To a computer, images are
    numbers. To be able to work with images for classification or object identification,
    you need to understand how a model transforms an image input into data. A **pixel**
    in an image file is just a piece of data.
  prefs: []
  type: TYPE_NORMAL
- en: In the following figure, you can see an example of pixel values for a grayscale
    image of the number eight. For the `28x28`-pixel image, there are a total of `784`
    pixels. Each pixel has a value between `0` and `255` identifying how light or
    dark the pixel is. On the right side, there is one large column vector with each
    pixel value listed. This is used by the model to identify the image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.2: Pixel values'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.2: Pixel values'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know what the input data looks like, it's time to get a closer
    look at the convolutional process—more specifically, the convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: The Convolutional Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Think of a convolution as nothing more than an image transformer with three
    key elements. First, there is an input image, then a filter, and finally, a feature
    map.
  prefs: []
  type: TYPE_NORMAL
- en: This section will cover each of these in turn to give you a solid idea of how
    images are filtered in a convolutional layer. The convolution is the process of
    passing a filter window over the input data, which will result in a map of activations
    known as a `3x3` for two-dimensional data, in which the specific values of the
    filter are learned during the training process. The filter passes across the input
    data with a window size equal to the size of the filter, then, the scalar product
    of the filter and section of the input data is applied, producing what's known
    as an **activation**. As this process continues across the entire input data using
    the same filter, the map of activations is produced, also known as the **feature
    map**.
  prefs: []
  type: TYPE_NORMAL
- en: 'This concept is illustrated in the following figure, which has two convolutional
    layers, producing two sets of feature maps. After the feature maps are produced
    from the first convolutional layer, they are passed into the second convolutional
    layer. The feature map of the second convolutional layer is passed into a classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.3: Convolution for classification'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.3: Convolution for classification'
  prefs: []
  type: TYPE_NORMAL
- en: The distance, or number of steps, the filter moves with each operation is known
    as the `0`. This is known as **valid padding**.
  prefs: []
  type: TYPE_NORMAL
- en: Let's recap some keywords. There's a `2x2` kernel. There's **stride**, which
    is the number of pixels that you move the kernel by. Lastly, there's **padding
    with zeros** around the image, whether or not you add pixels. This ensures that
    the output is the same size as the input.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From the very first chapter, you encountered different types of dimensional
    tensors. One important thing to note is that you will only be working with `Conv2D`.
    The layer name `Conv2D` refers only to the movement of a **filter** or **kernel**.
    So, if you recall the description of what the convolutional process is doing,
    it's simply sliding a kernel across a 2D space. So, for a flat, square image,
    the kernel only slides in two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you implement `Conv2D`, you need to pass in certain parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: The first parameter is `filter`. The filters are the dimensionality of the output space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Specify `strides`, which is how many pixels will move the kernel across.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, specify `padding`, which is usually `valid` or `same` depending on whether
    you want an output that is of the same dimension as the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, you can also have `activation`. Here, you will specify what sort of
    activation you would like to apply to the outputs. If you don't specify an activation,
    it's simply a linear activation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Before you continue, recall from *Chapter 4*, *Regression and Classification
    Models*, that a dense layer is one in which every neuron is connected to every
    neuron in the previous layer. As you can see in the following code, you can easily
    add a dense layer with `model.add(Dense(32))`. `32` is the number of neurons,
    followed by the input shape. **AlexNet** is an example of a CNN with multiple
    convolution kernels that extracts interesting information from an image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.4: AlexNet consists of five convolution layers and three connected
    layers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.4: AlexNet consists of five convolution layers and three connected
    layers'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: AlexNet is the name of a CNN designed by Alex Krizhevsky.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sequential model can be used to build a CNN. Different methods can be used
    to add a layer; here, we will use the framework of sequentially adding layers
    to the model using the model''s `add` method or passing in a list of all layers
    when the model is instantiated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is a code block showing the code that you''ll be using later
    in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Use the `Conv2D` layer when working with data that you want to convolve in two
    dimensions, such as images. For parameters, set the number of filters to `32`,
    followed by the kernel size of `3x3` pixels (`(3, 3)` in the example). In the
    first layer, you will always need to specify the `input_shape` dimensions, the
    height, width, and depth. `input_shape` is the size of the images you will be
    using. You can also select the activation function to be applied at the end of
    the layer.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how to build a CNN layer in your model, you will practice
    doing so in your first exercise. In this exercise, you will build the first constructs
    of a CNN, initialize the model, and add a single convolutional layer to the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.01: Creating the First Layer to Build a CNN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a TensorFlow freelancer, you've been asked to show your potential employer
    a few lines of code that demonstrate how you might build the first layer in a
    CNN. They ask that you keep it simple but provide the first few steps to create
    a CNN layer. In this exercise, you will complete the first step in creating a
    CNN—that is, adding the first convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to complete this exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: Open a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the TensorFlow library and the `models` and `layers` classes from `tensorflow.keras`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the TensorFlow version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, use `models.Sequential` to create your model. The first layer (`Conv2D`)
    will require the number of nodes (`filters`), the filter size (`3,3`), and the
    shape of the input. `input_shape` for your first layer will determine the shape
    of your input images. Add a ReLU activation layer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Simple enough. You have just taken the first steps in creating your first CNN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You will now move on to the type of layer that usually follows a convolutional
    layer—the pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling Layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pooling is an operation that is commonly added to a CNN to reduce the dimensionality
    of an image by reducing the number of pixels in the output from the convolutional
    layer it follows. **Pooling layers** shrink the input image to increase computational
    efficiency and reduce the number of parameters to limit the risk of **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A **pooling layer** immediately follows a convolution layer and is considered
    another important part of the CNN structure. This section will focus on two types
    of pooling:'
  prefs: []
  type: TYPE_NORMAL
- en: Max pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With max pooling, a filter or kernel only retains the largest pixel value from
    an input matrix. To get a clearer idea of what is happening, consider the following
    example. Say you have a `4x4` input. This first step in max pooling would be to
    divide the `4x4` matrix into four quadrants. Each quadrant will be of the size
    `2x2`. Apply a filter of size `2`. This means that your filter will look exactly
    like a `2x2` matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Begin by placing the filter on top of your input. For max pooling, this filter
    will look at all values within the `2x2` area that it covers. It will find the
    largest value, send that value to your output, and store it there in the upper-left
    corner of the feature map.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.5: Max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.5: Max pooling'
  prefs: []
  type: TYPE_NORMAL
- en: Then, the filter will move over to the right and repeat the same process, storing
    the value in the upper-right corner of the `2x2` matrix. Once this operation is
    complete, the filter will slide down and start at the far left, again repeating
    the same process, looking for the largest (or maximum) value, and then storing
    it in the correct place on the `2x2` matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the sliding movement is referred to as `2`. This process is repeated
    until the maximum values in each of the four quadrants are `8`, `5`, `7`, and
    `5`, respectively. Again, to get these numbers, you used a filter of `2x2` and
    filtered for the largest number within that `2x2` matrix.
  prefs: []
  type: TYPE_NORMAL
- en: So, in this case, you had a stride of two because you moved two pixels. These
    are the `filter` and `stride` are `2`. *Figure 7.6* shows what an implementation
    of max pooling might look like with a filter size of 3 x 3 and a `stride` of `1`.
  prefs: []
  type: TYPE_NORMAL
- en: There are two steps shown in *Figure 7.6*. Start at the upper left of the feature
    map. With the `3x3` filter, you would look at the following numbers, `2`, `8`,
    `2`, `5`, `4`, `9`, `8`, `4`, and `6`, and choose the largest value, `9`. The
    `9` would be placed in the upper-left box of our pooled feature map. With a stride
    of `1`, you would slide the filter one place to the right, as shown in gray.
  prefs: []
  type: TYPE_NORMAL
- en: Now, look for the largest values from `8`, `2`, `1`, `4`, `9`, `6`, `4`, `6`,
    and `4`. Again, `9` is the largest value, so add a `9` to the middle place in
    the top row of the pooled feature map (shown in gray).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.6: Pooled feature map'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.6: Pooled feature map'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding pool size is `(2, 2)`. It specifies factors that you will downscale
    with. Here''s a more detailed look at what you could do to implement `MaxPool2D`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '`MaxPool2D` instance. The code snippet initializes a max pooling layer with
    a pool size of `2x2` and the `stride` value is not specified, so it will default
    to the pool size value. The `padding` parameter is set to `valid`, meaning there
    is no padding added. The following code snippet demonstrates its use within a
    CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, a sequential model is created with two convolutional
    layers, after each layer is a ReLU activation function, and after the activation
    function of the first convolutional layer is a max pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have explored max pooling, let''s look at the other type of pooling:
    average pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: Average Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Average pooling** operates in a similar way to max pooling, but instead of
    extracting the largest weight value within the filter, it calculates the average.
    It then passes along that value to the feature map. *Figure 7.7* highlights the
    difference between max pooling and average pooling.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.7*, consider the `4x4` matrix on the left. The average of the
    numbers in the upper-left quadrant is `13`. This would be the average pooling
    value. The same upper-left quadrant would output `20` to its feature map if it
    were max pooled because `20` is the largest value within the filter frame. This
    is a comparison between max pooling and average pooling with hyperparameters,
    with the `filter` and `stride` parameters both set to `2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.7: Max versus average pooling'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.7: Max versus average pooling'
  prefs: []
  type: TYPE_NORMAL
- en: For average pooling, you would use `AveragePooling2D` in place of `MaxPool2D`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement the average pooling code, you could use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '`AveragePooling2D` layer. In a similar manner to max pooling, the `pool_size`,
    `strides`, and `padding` parameters can be modified. The following code snippet
    demonstrates its use within a CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It's a good idea to keep in mind the benefits of using pooling layers. One of
    these benefits is that if you down-sample the image, the *image shrinks*. This
    means that you have *less data to process* and fewer multiplications to do, which,
    of course, speeds things up.
  prefs: []
  type: TYPE_NORMAL
- en: Up to this point, you've created your first CNN layer and learned how to use
    pooling layers. Now you'll use what you've learned so far to build a pooling layer
    for the CNN in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.02: Creating a Pooling Layer for a CNN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You receive an email from your potential employer for the TensorFlow freelancing
    job that you applied for in *Exercise 7.01*, *Creating the First Layer to Build
    a CNN*. The email asks whether you can show how you would code a pooling layer
    for a CNN. In this exercise, you will build your base model by adding a pooling
    layer, as requested by your potential employer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open a new Jupyter notebook and import the TensorFlow library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create your model using `models.Sequential`. The first layer, `Conv2D`, will
    require the number of nodes, the filter size, and the shape of the tensor, as
    in the previous exercise. It will be followed by an activation layer, a node at
    the end of the neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, add a `MaxPool2D` layer by using the model''s `add` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this model, you have created a CNN with a convolutional layer, followed by
    a ReLU activation function then a max pooling layer. The models take images of
    size `300x300` with three color channels.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that you have successfully added a `MaxPool2D` layer to your CNN, the next
    step is to add a **flattening layer** so that your model can use all the data.
  prefs: []
  type: TYPE_NORMAL
- en: Flattening Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adding a flattening layer is an important step as you will need to provide
    the neural network with data in a form that it can process. Remember that after
    you perform the convolution operation, it will still be multi-dimensional. So,
    to change your data back into one-dimensional form, you will use a flattening
    layer. To achieve this, you take the pooled feature map and flatten it into a
    column, as shown in the following figure. In *Figure 7.8*, you can see that you
    start with the input matrix on the left side of the diagram, use a final pooled
    feature map, and stretch it out into a single column vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8: Flattening layer'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.8: Flattening layer'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an implemented flattening layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Here, a flatten layer is added as the final layer to this model. Now that you've
    created your first CNN and pooling layers, you will put all the pieces together
    and build a CNN in the upcoming exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.03: Building a CNN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You were hired as a freelancer from your work in *Exercise 7.01*, *Creating
    the First Layer to Build a CNN*, and *Exercise 7.02*, *Creating a Pooling Layer
    for a CNN*. Now that you've got the job, your first assignment is to help your
    start-up company build its prototype product to show to investors and raise capital.
    The company is trying to develop a horse or human classifier app, and they want
    you to get started right away. They tell you that they just need the classifier
    to work for now and that there will be room for improvements on it soon.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you will build a convolutional base layer for your model using
    the `horses_or_humans` dataset. In this dataset, the images aren't centered. The
    target images are displayed at all angles and at different positions in the frame.
    You will continue to build on this foundation throughout the chapter, adding to
    it piece by piece.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The dataset can be downloaded using the `tensorflow_datasets` package.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import all the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: First, you need to import the TensorFlow library. You will use `tensorflow_datasets`
    to load your dataset, `tensorflow.keras.models` to build a sequential TensorFlow
    model, `tensorflow.keras.layers` to add layers to your CNN model, `RMSprop` as
    your optimizer, and `matplotlib.pyplot` and `matplotlib.image` for some quick
    visualizations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load your dataset from the `tensorflow_datasets` package:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, you used the `tensorflow_datasets` package imported as `tfds`. You used
    the `tfds.load()` function to load the `horses_or_humans` dataset. It is a binary
    image classification dataset with two classes: horses and humans.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: More information on the dataset can be found at [https://laurencemoroney.com/datasets.html](https://laurencemoroney.com/datasets.html).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: More information on the `tensorflow_datasets` package can be found at [https://www.tensorflow.org/datasets](https://www.tensorflow.org/datasets).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `split = ['train', 'test']` argument specifies which split of the data you
    want to load. In this example, you are loading the train and test splits into
    `our_train_dataset` and `our_test_dataset`, respectively. Specify `with_info =
    True` to load the metadata about the dataset into the `dataset_info` variable.
    After loading, use `assert` to make sure that the loaded dataset is an instance
    of the `tf.data.Dataset` object class.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'View information about the dataset using the loaded metadata in `dataset_info`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.9: horses_or_humans dataset information'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.9: horses_or_humans dataset information'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, view the number of images in the dataset and its distribution of classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.10: horses_or_humans dataset distribution'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.10: horses_or_humans dataset distribution'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, view some sample images in the training dataset, using the `tfds.show_examples()`
    function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This function is for interactive use, and it displays and returns a plot of
    images from the training dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Your output should be something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.11: Sample training images'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.11: Sample training images'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'View some sample images in the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.12: Sample test images'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.12: Sample test images'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, create your model with `our_model = models.Sequential`. Set up the
    first `Conv2D` layer and set `filters` to `16`. The kernel is `3x3`. Use ReLU
    activation. Because this is the first convolutional layer, you also need to set
    `input_shape` to `image_shape`, the dimensions of the color images you''re working
    with. Now, add the `MaxPool2D` pooling layer. Then, add another `Conv2D` and `MaxPool2D`
    pair for more model depth, followed by the flatten and dense layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile the model with `RMSProp` for `optimizer` set to the recommended default
    of `0.001`, `loss` as `binary_crossentropy`, and `metrics` set to `acc` for accuracy.
    Print the model summary using the `summary()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will print the model summary with details on the layer type, output shape,
    and parameters:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.13: Model summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.13: Model summary'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding screenshot, you can see that there are layers and types listed
    on the left side. The layers are listed in order from first to last, top to bottom.
    The output shape is shown in the middle. There are several parameters for each
    layer listed alongside the assigned layer. At the bottom, you'll see a count of
    the total parameters, trainable parameters, and non-trainable parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ve been able to explore the convolutional layer and pooling layers quite
    a bit. Let''s now dive into another important component when using image data:
    image augmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: Image Augmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Augmentation is defined as making something better by making it greater in
    size or amount. This is exactly what data or image augmentation does. You use
    augmentation to provide the model with more versions of your image training data.
    Remember that the more data you have, the better the model''s performance will
    be. By *augmenting* your data, you can transform your images in a way that makes
    the model generalize better on real data. To do this, you *transform* the images
    that you have at your disposal so that you can use your augmented images alongside
    your original image dataset to train with a greater variation and variety than
    you would have otherwise. This improves results and prevents overfitting. Take
    a look at the following three images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.14: Augmented leopard images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.14: Augmented leopard images'
  prefs: []
  type: TYPE_NORMAL
- en: It's clear that this is the same leopard in all three images. They're just in
    different positions. Neural networks can still make sense of this due to convolution.
    However, with the use of image augmentation, you can improve the model's ability
    to learn **translational invariance**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike most other types of data with images, you can shift, rotate, and move
    the images around to make variations of the original image. This creates more
    data, and with CNNs, more data and data variation will create a better-performing
    model. To be able to create these image augmentations, take a look at how you
    would do this in TensorFlow with the loaded `tf.data.Dataset` object. You will
    use the `dataset.map()` function to map preprocessing image augmentation functions
    to your dataset, that is, `our_train_dataset`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You will use the `tensorflow.image` and `tensorflow.keras.preprocessing.image`
    packages for this purpose. These packages have a lot of image manipulation functions
    that can be used for image data augmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Additional functions include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kimage.random_rotation`: This function allows you to rotate an image randomly
    between specified degrees.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kimage.random_brightness`: This function randomly adjusts the brightness level.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kimage.random_shear`: This function applies shear transformations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kimage.random_zoom`: This function randomly zooms images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tfimage.random_flip_left_right`: This function randomly flips images horizontally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tfimage.random_flip_up_down`: This function randomly flips images vertically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next step, you will pass in the data that you want to augment with the
    `tf.data.Dataset.map()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, with `fit()`, you just need to pass the generator
    that you have already created. You need to pass in the `epochs` value. If you
    don't do this, the generator will never stop. The `fit()` function returns the
    history (plots loss per iteration and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'You need some more functions to add to `our_train_dataset` before you can train
    the model on it. With `batch()` function, you specify how many images per batch
    you will train. With `cache()` function, you fit your dataset in memory to improve
    performance. With `shuffle()` function, you set the shuffle buffer of your dataset
    to the entire length of the dataset, for true randomness. `prefetch()` function
    is also used for good performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Now that you've seen how you would implement augmentation in your training model,
    take a closer look at what some of those transformations are doing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of `random_rotation`, `random_shift`, and `random_brightnes`
    implementation. Use the following code to randomly rotate an image up to an assigned
    value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: In *Figure 7.15*, you can see the outcome of `random_rotation`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15: Rotation range'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.15: Rotation range'
  prefs: []
  type: TYPE_NORMAL
- en: The images were randomly rotated up to 135 degrees.
  prefs: []
  type: TYPE_NORMAL
- en: '`random_shift` is used to randomly shift the pixels width-wise. Notice the
    `.15` in the following code, which means the image can be randomly shifted up
    to 15 pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the random adjustment of an image''s width by up
    to 15 pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16: Width shift range'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.16: Width shift range'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, `random_shift` is used here, which randomly adjusts the height by 15
    pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '*Figure 7.17* shows the random adjustment of an image''s height by up to 15
    pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17: Height shift range'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.17: Height shift range'
  prefs: []
  type: TYPE_NORMAL
- en: 'For random brightness levels using `random_brightness`, you will use a float
    value range to lighten or darken the image by percentage. Anything below `1.0`
    will darken the image. So, in this example, the images are being darkened randomly
    between 10% and 90%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'In the following figure, you''ve adjusted the brightness with `random_brightness`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18: Brightness range'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.18: Brightness range'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you've been exposed to some of the image augmentation options, take
    a look at how you can use batch normalization to drive performance improvement
    in models.
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In 2015, **batch normalization**, also called **batch norm**, was introduced
    by *Christian Szegedy* and *Sergey Ioffe*. Batch norm is a technique that reduces
    the number of training epochs to improve performance. Batch norm standardizes
    the inputs for a mini-batch and "normalizes" the input layer. It is most commonly
    used following a convolutional layer, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19: Batch norm'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.19: Batch norm'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows one common way that batch normalization is implemented.
    In the following example, you can see that you have a batch norm layer following
    a convolutional layer three times. Then you have a flattening layer, followed
    by two dense layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20: Layer sequences'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16341_07_20.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7.20: Layer sequences'
  prefs: []
  type: TYPE_NORMAL
- en: Batch norm helps the model generalize better. With each batch that batch norm
    trains, the model has a different mean and standard deviation. Because the batch
    means and standard deviations each vary slightly from the true overall mean and
    standard deviation, these changes act as noise that you are training with, making
    the model perform better overall.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example of `BatchNormalization` implementation. You can
    simply add a batch norm layer, followed by an activation layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: So far, you've created a CNN model and learned how to utilize image augmentation.
    Now you will bring everything together and build a CNN with some additional convolutional
    layers in the following exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.04: Building a CNN with Additional Convolutional Layers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Your new employers were happy with what you were able to make in *Exercise 7.03*,
    *Building a CNN*. Now that the **Minimal Viable Product** (**MVP**), or prototype,
    is complete, it's time to build a better model.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, you will add additional ANN layers to your model. You will
    be adding additional layers to your convolutional base layer that you created
    earlier. You will be using the `horses_or_humans` dataset again.
  prefs: []
  type: TYPE_NORMAL
- en: Let's get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'Because you''re expanding on *Exercise 7.03*, *Building a CNN*, and using the
    same data, begin from where you left off with the last step in the previous exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a function to rescale the images then apply the function to the train
    and test datasets using the `map` method. Continue building your train and test
    dataset pipelines using the `cache`, `shuffle`, `batch`, and `prefetch` methods
    of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the model. Specify the values of `epochs` and `validation_steps` and set
    `verbose` equal to `1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.21: Model fitting process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.21: Model fitting process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Take a batch from the test dataset and plot the first image from the batch.
    Convert the image to an array, then use the model to predict what the image shows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will have the following details:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.22: Output of image test with its metadata'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.22: Output of image test with its metadata'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: For prediction, you have a picture of a person from the test set to see what
    the classification would be.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Take a look at what''s happening with each successive layer. Do this by creating
    a list containing all names of the layers within the CNN and another list containing
    predictions on a random sample from each of the layers in the list created previously.
    Next, iterate through the list of names of the layers and their respective predictions
    and plot the features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should get something like the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.23: Transformation at different layers'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.23: Transformation at different layers'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have created your own CNN model and used it to determine whether
    an image was a horse or a human, you're now going to focus on how you can classify
    whether an image is or isn't a specific class.
  prefs: []
  type: TYPE_NORMAL
- en: Binary Image Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Binary classification is the simplest approach for classification models as
    it classifies images into just two categories. In this chapter, we started with
    the convolutional operation and discussed how you use it as an image transformer.
    Then, you learned what a pooling layer does and the differences between max and
    average pooling. Next, we also looked at how a flattening layer converts a pooled
    feature map into a single column. Then, you learned how and why to use image augmentation,
    and how to use batch normalization. These are the key components that differentiate
    CNNs from other ANNs.
  prefs: []
  type: TYPE_NORMAL
- en: After convolutional base layers, pooling, and normalization layers, CNNs are
    often structured like many ANNs you've built thus far, with a series of one or
    more dense layers. Much like other binary classifiers, binary image classifiers
    terminate with a dense layer with one unit and a sigmoid activation function.
    To provide more utility, image classifiers can be outfitted to classify more than
    two objects. Such classifiers are known generally as object classifiers, which
    you will learn about in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Object Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will learn about object detection and classification.
    The next step involves image classification for a dataset with more than two classes.
    The three different types of models for object classification we will cover are
    **image classification**, **classification with localization**, and **detection**:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image classification**: This involves training with a set number of classes
    and then trying to determine which of those classes is shown in the image. Think
    of the MNIST handwriting dataset. For these problems, you''ll use a traditional
    CNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification with localization**: With this type, the model tries to predict
    where the object is in the image space. For these models, you use a simplified
    **You Only Look Once** (**YOLO**) or R-CNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Detection**: The last type is detection. This is where your model can detect
    several different objects and where they are located. For this, you use YOLO or
    an R-CNN:![Figure 7.24: Object classification types'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '](img/B16341_07_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.24: Object classification types'
  prefs: []
  type: TYPE_NORMAL
- en: Now, you'll take a brief look at image classification with the `Fashion-MNIST`
    dataset. `Fashion-MNIST` was compiled from a dataset of Zalando's article images.
    Zalando is a fashion-focused e-commerce company based in Berlin, Germany. The
    dataset consists of 10 classes with a training set of 60,000 `28x28` grayscale
    images and 10,000 test images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Import TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, make some additional imports, such as for NumPy, Matplotlib, and of course,
    layers and models. You''ll notice here that you will be using additional dropout
    layers. If you recall, dropout layers help prevent overfitting:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `Fashion-MNIST` dataset using `tdfs` in any one of the datasets that
    they have decided to include. Others include `CIFAR-10` and `CIFAR-100`, just
    to name a couple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the data for its properties:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give you the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.25: Details of properties for data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.25: Details of properties for data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, print the total examples of the train and test data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give you the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.26: Details of train and test datasets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.26: Details of train and test datasets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build your model with the functional API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile and fit your model. With `compile()` method, use `adam` as your optimizer,
    set the loss to `sparse_categorical_crossentropy`, and set the `accuracy` metric.
    Then, call `model.fit()` on your training and validation sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give the following as output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.27: Function returning history'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.27: Function returning history'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use `matplotlib.pyplot` to plot the loss and accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give the following plot as output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.28: Accuracy plot using matplotlib.pyplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.28: Accuracy plot using matplotlib.pyplot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the validation loss and training loss. Use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give the following plot as output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.29: Validation loss and training loss'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.29: Validation loss and training loss'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the accuracy and loss curves as a function of epochs, the
    accuracy increases, and loss decreases. On the validation set, both begin to plateau,
    which is a good signal to stop training to prevent overfitting to the training
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, you will build a CNN to classify images into 10 distinct
    classes from the `CIFAR-10` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 7.05: Building a CNN'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The start-up now wants to expand its capabilities and to work with more classes
    and larger image datasets. Your challenge is to accurately predict the class of
    an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset you will be using is the `CIFAR-10` dataset, a dataset containing
    60,000 `32x32` color images across 10 classes: airplanes, automobiles, birds,
    cats, deer, dogs, frogs, horses, ships, and trucks. Each class has 6,000 images
    and the entire dataset contains 50,000 training images and 10,000 test images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More info on the dataset can be found at *Learning Multiple Layers of Features
    from Tiny Images* ([http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](http://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)),
    *Alex Krizhevsky*, *2009*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start a new Jupyter notebook and import the TensorFlow library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import the other additional libraries that are needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the `CIFAR-10` dataset directly from `tfds` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the properties of your dataset using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give the following output with the properties and the number of classes:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.30: Number of classes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.30: Number of classes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build the train and test data pipelines, as shown in *Exercise 7.03*, *Building
    a CNN*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the model using the functional API. Set the shape, layer types, strides,
    and activation functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compile and fit your model. Be sure to use your GPU for this, if possible,
    as it will speed up the process quite a bit. If you decide not to use the GPU
    and your machine has difficulty in terms of computation, you can decrease the
    number of epochs accordingly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The function will return the following history:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.31: Fitting the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.31: Fitting the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get a visual representation of the model''s performance by plotting your loss
    and accuracy per epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will produce the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.32: Loss plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.32: Loss plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, get an accuracy plot by using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give the following plot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.33: Accuracy plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.33: Accuracy plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the confusion matrix without normalization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will give the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.34: Confusion matrix without normalization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.34: Confusion matrix without normalization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the following code to plot the confusion matrix with normalization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.35: Confusion matrix with normalization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.35: Confusion matrix with normalization'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Take a look at one of the images that the model got wrong. Plot one of the
    incorrect predictions with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 7.36: True versus predicted results'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16341_07_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.36: True versus predicted results'
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll notice it says `True label: bird` and `Predicted label: cat`. This
    means that the model predicted that this image was a cat, but it was a bird. The
    image is blurry since the resolution is only `32x32`; however, the results are
    not bad. It would be fair to say that it is difficult for a human to identify
    whether the image was a dog or a cat.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have completed this chapter, it's time to put everything that you've
    learned to the test with *Activity 7.01*, *Building a CNN with More ANN Layers*,
    where you'll be building a CNN with additional ANN layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 7.01: Building a CNN with More ANN Layers'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The start-up that you've been working for has loved your work so far. They have
    tasked you with creating a new model that is capable of classifying images from
    100 different classes.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, you'll be putting everything that you've learned to use as
    you build your own classifier with `CIFAR-100`. `CIFAR-100` is a more advanced
    version of the `CIFAR-10` dataset, with 100 classes, and is commonly used for
    benchmarking performance in machine learning research.
  prefs: []
  type: TYPE_NORMAL
- en: Start a new Jupyter notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the TensorFlow library.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the additional libraries that you will need, including NumPy, Matplotlib,
    Input, Conv2D, Dense, Flatten, Dropout, GlobalMaxPooling2D, Activation, Model,
    confusion_matrix, and itertools.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Load the `CIFAR-100` dataset directly from `tensorflow_datasets` and view its
    properties from the metadata, and build a train and test data pipeline:![Figure
    7.37: Properties of the CIFAR-100 dataset'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16341_07_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.37: Properties of the CIFAR-100 dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Create a function to rescale images. Then, build a test and train data pipeline
    by rescaling, caching, shuffling, batching, and prefetching the images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build the model using the functional API using `Conv2D` and `Flatten`, among others.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compile and fit the model using `model.compile` and `model.fit`:![Figure 7.38:
    Model fitting'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16341_07_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.38: Model fitting'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the loss with `plt.plot`. Remember to use the history collected during
    the `model.fit()` procedure:![Figure 7.39: Loss versus epochs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16341_07_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.39: Loss versus epochs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Plot the accuracy with `plt.plot`:![Figure 7.40: Accuracy versus epochs'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16341_07_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.40: Accuracy versus epochs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Specify the labels for the different classes in your dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Display a misclassified example with `plt.imshow`:![Figure 7.41: Wrong classification
    example'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16341_07_41.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 7.41: Wrong classification example'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this activity can be found via [this link](B16341_Solution_ePub.xhtml#_idTextAnchor272).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered CNNs. We reviewed core concepts such as neurons, layers,
    model architecture, and tensors to understand how to create effective CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: You learned about the convolution operation and explored kernels and feature
    maps. We analyzed how to assemble a CNN, and then explored the different types
    of pooling layers and when to apply them.
  prefs: []
  type: TYPE_NORMAL
- en: You then learned about the stride operation and how padding is used to create
    extra space around images if needed. Then, we delved into the flattening layer
    and how it is able to convert data into a 1D array for the next layer. You put
    everything that you learned to the test in the final activity, as you were presented
    with several classification problems, including `CIFAR-10` and even `CIFAR-100`.
  prefs: []
  type: TYPE_NORMAL
- en: In completing this chapter, you are now well on your way to being able to implement
    CNNs to confront image classification problems head-on and with confidence.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you'll learn about pre-trained models and how to utilize
    them for your own applications by adding ANN layers on top of the pre-trained
    model and fine-tuning the weights given your own training data.
  prefs: []
  type: TYPE_NORMAL
