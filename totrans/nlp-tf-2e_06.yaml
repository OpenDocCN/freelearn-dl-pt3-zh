- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent Neural Networks** (**RNNs**) are a special family of neural networks
    that are designed to cope with sequential data (that is, time-series data), such
    as stock market prices or a sequence of texts (for example, variable-length sentences).
    RNNs maintain a state variable that captures the various patterns present in sequential
    data; therefore, they are able to model sequential data. In comparison, conventional
    feed-forward neural networks do not have this ability unless the data is represented
    with a feature representation that captures the important patterns present in
    the sequence. However, coming up with such feature representations is extremely
    difficult. Another alternative for feed-forward models to model sequential data
    is to have a separate set of parameters for each position in time/sequence so
    that the set of parameters assigned to a certain position learns about the patterns
    that occur at that position. This will greatly increase the memory requirement
    for your model.'
  prefs: []
  type: TYPE_NORMAL
- en: However, as opposed to having a separate set of parameters for each position
    like feed-forward networks, RNNs share the same set of parameters over time. Sharing
    parameters over time is an important part of RNNs and in fact is one of the main
    enablers for learning temporal patterns. Then the state variable is updated over
    time for each input we observe in the sequence. These parameters shared over time,
    combined with the state vector, are able to predict the next value of a sequence,
    given the previously observed values of the sequence. Furthermore, since we process
    a single element of a sequence at a time (for example, one word in a document
    at a time), RNNs can process data of arbitrary lengths without padding data with
    special tokens.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will dive into the details of RNNs. First, we will discuss
    how an RNN can be formed by starting with a simple feed-forward model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After this we will discuss the basic functionality of an RNN. We will also
    delve into the underlying equations, such as output calculation and parameter
    update rules of RNNs, and discuss several variants of applications of RNNs: one-to-one,
    one-to-many, and many-to-many RNNs. We will walk through an example of using RNNs
    to identify named entities (e.g. person names, organization, etc.), which has
    valuable downstream use cases like building knowledge bases. We will discuss a
    more complex RNN model that can read text both forward and backward, and uses
    convolutional layers to increase the model accuracy. This chapter will cover this
    through the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Backpropagation Through Time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named Entity Recognition (NER) with RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NER with character and token embeddings
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss what an RNN is by starting with a gentle introduction,
    and then move on to more in-depth technical details. We mentioned earlier that
    RNNs maintain a state variable that evolves over time as the RNN sees more data,
    thus giving it the power to model sequential data. In particular, this state variable
    is updated over time by a set of recurrent connections. The existence of recurrent
    connections is the main structural difference between an RNN and a feed-forward
    network. The recurrent connections can be understood as links between a series
    of memories that the RNN learned in the past, connecting to the current state
    variable of the RNN. In other words, the recurrent connections update the current
    state variable with respect to the past memory the RNN has, enabling the RNN to
    make a prediction based on the current input as well as the previous inputs.
  prefs: []
  type: TYPE_NORMAL
- en: The term RNN is sometimes used to refer to the family of recurrent models, which
    has many different models. In other words, it is sometimes used as a generalization
    of a specific RNN variant. Here, we are using the term RNN to refer to one of
    the earliest implementations of an RNN model known as the Elman network.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming section, we will discuss the following topics. First, we will
    discuss how we can start by representing a feed-forward network as a computational
    graph.
  prefs: []
  type: TYPE_NORMAL
- en: Then we will see through an example why a feed-forward network might fail at
    a sequential task. Then we will adapt that feed-forward graph to model sequential
    data, which will give us the basic computational graph of an RNN. We will also
    discuss the technical details (for example, update rules) of an RNN. Finally,
    we will discuss the details of how we can train RNN models.
  prefs: []
  type: TYPE_NORMAL
- en: The problem with feed-forward neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand the limits of feed-forward neural networks and how RNNs address
    them, let’s imagine a sequence of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, let’s assume that, in the real world, *x* and *y* are linked in the following
    relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_002.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_06_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *g*[1] and *g*[2] are transformations (e.g. multiplying with a weight
    matrix followed by a non-linear transformation). This means that the current output
    *y*[t] depends on the current state *h*[t,] where *h*[t] is calculated with the
    current input *x*[t] and previous state *h*[t-1]. The state encodes information
    about previous inputs observed historically by the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s imagine a simple feed-forward neural network, which we will represent
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *y*[t] is the predicted output for some input *x*[t].
  prefs: []
  type: TYPE_NORMAL
- en: If we use a feed-forward neural network to solve this task, the network will
    have to produce ![](img/B14070_06_005.png) one at a time, by taking ![](img/B14070_06_006.png)
    as inputs, one at a time. Now, let’s consider the problem we face in this solution
    for a time-series problem.
  prefs: []
  type: TYPE_NORMAL
- en: The predicted output *y*[t] at time *t* of a feed-forward neural network depends
    only on the current input *x*[t]. In other words, it does not have any knowledge
    about the inputs that led to *x*[t] (that is, ![](img/B14070_06_007.png)). For
    this reason, a feed-forward neural network will fail at a task where the current
    output not only depends on the current input but also on the previous inputs.
    Let’s understand this through an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we need to train a neural network to fill in missing words. We have the
    following phrase, and we would like to predict the next word:'
  prefs: []
  type: TYPE_NORMAL
- en: '*James has a cat and it likes to drink ____.*'
  prefs: []
  type: TYPE_NORMAL
- en: If we are to process one word at a time and use a feed-forward neural network,
    we will only have the input *drink* and this is not enough at all to understand
    the phrase or even to understand the context (the word *drink* can appear in many
    different contexts). One can argue that we can achieve good results by processing
    the full sentence in a single go. Even though this is true, such an approach has
    limitations such as processing very long sentences. However, there is a new family
    of models known as Transformers that are processing the full sequences of data
    with fully-connected layers, and have been surpassing the performance of sequential
    models. We will have a separate chapter on these models later.
  prefs: []
  type: TYPE_NORMAL
- en: Modeling with RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On the other hand, we can use an RNN to find a solution to this problem. We
    will start with the data we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Assume that we have the following relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_009.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_06_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let’s replace *g*[1] with a function approximator ![](img/B14070_06_011.png)
    parametrized by ![](img/B14070_06_012.png) that takes the current input *x*[t]
    and the previous state of the system *h*[t-1] as the input and produces the current
    state *h*[t]. Then, we will replace *g*[2] with ![](img/B14070_06_013.png), which
    takes the current state of the system *h*[t] to produce *y*[t]. This gives us
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_014.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_06_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can think of ![](img/B14070_06_016.png) as an approximation of the true
    model that generates *x* and *y*. To understand this more clearly, let’s now expand
    the equation as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, we can represent *y*[4] as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Also, by expansion we get the following (omitting ![](img/B14070_06_012.png)
    and ![](img/B14070_06_020.png) for clarity):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can be illustrated in a graph, as shown in *Figure 6.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling with Recurrent Neural Networks](img/B14070_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: The relationship between x[t] and y[t] expanded'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can generally summarize the diagram, for any given time step *t*, as shown
    in *Figure 6.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling with Recurrent Neural Networks](img/B14070_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: A single-step calculation of an RNN structure'
  prefs: []
  type: TYPE_NORMAL
- en: However, it should be understood that *h*[t-1] in fact is what *h*[t] was before
    receiving *x*[t]. In other words, *h*[t-1] is *h*[t] before one time step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we can represent the calculation of *h*[t] with a recurrent connection,
    as shown in *Figure 6.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Modeling with Recurrent Neural Networks](img/B14070_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: A single-step calculation of an RNN with the recurrent connection'
  prefs: []
  type: TYPE_NORMAL
- en: The ability to summarize a chain of equations mapping ![](img/B14070_06_022.png)
    to ![](img/B14070_06_023.png) as in *Figure 6.3* allows us to write any *y*[t]
    in terms of *x*[t], *h*[t-1], and *h*[t]. This is the key idea behind an RNN.
  prefs: []
  type: TYPE_NORMAL
- en: Technical description of an RNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s now have an even closer look at what makes an RNN and define the mathematical
    equations for the calculations taking place within an RNN. Let’s start with the
    two functions we derived as function approximators for learning *y*[t] from *x*[t]:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_024.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B14070_06_025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we have seen, a neural network is composed of a set of weights and biases
    and some nonlinear activation function. Therefore, we can write the preceding
    relation as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_026.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, tanh is the tanh activation function, and *U* is a weight matrix of size
    ![](img/B14070_06_027.png), where *m* is the number of hidden units and *d* is
    the dimensionality of the input. Also, *W* is a weight matrix of size ![](img/B14070_06_028.png)
    that creates the recurrent link from *h*[t-1] to *h*[t]. The *y*[t] relation is
    given by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_029.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *V* is a weight matrix of size ![](img/B14070_06_030.png) and *c* is
    the dimensionality of the output (this can be the number of output classes). In
    *Figure 6.4*, we illustrate how these weights form an RNN. The arrows represent
    the direction that the data flows in the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Technical description of a Recurrent Neural Network](img/B14070_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: The structure of an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen how we can represent an RNN with a graph of computational
    nodes, with edges denoting computations. Also, we looked at the actual mathematics
    behind an RNN. Let’s now look at how to optimize (or train) the weights of an
    RNN to learn from sequential data.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation Through Time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For training RNNs, a special form of **backpropagation**, known as **Backpropagation
    Through Time** (**BPTT**), is used. To understand BPTT, however, first we need
    to understand how **BP** works. Then we will discuss why BP cannot be directly
    applied to RNNs, but how BP can be adapted for RNNs, resulting in BPTT. Finally,
    we will discuss two major problems present in BPTT.
  prefs: []
  type: TYPE_NORMAL
- en: How backpropagation works
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Backpropagation is the technique that is used to train a feed-forward neural
    network. In backpropagation, you do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Calculate a prediction for a given input
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate an error, *E*, of the prediction by comparing it to the actual label
    of the input (for example, mean squared error and cross-entropy loss)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the weights of the feed-forward network to minimize the loss calculated
    in *step 2*, by taking a small step in the opposite direction of the gradient
    ![](img/B14070_06_031.png) for all *w*[ij], where *w*[ij] is the *j*^(th) weight
    of the *i*^(th) layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To understand the above computations more clearly, consider the feed-forward
    network depicted in *Figure 6.5*. This has two single weights, *w*[1] and *w*[2],
    and calculates two outputs, *h* and *y*, as shown in the following figure. We
    assume no nonlinearities in the model for simplicity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![How backpropagation works](img/B14070_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Computations of a feed-forward network'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate ![](img/B14070_06_032.png) using the chain rule as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_033.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This simplifies to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_034.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *l* is the correct label for the data point *x*. Also, we are assuming
    the mean squared error as the loss function. Everything here is defined, and it
    is quite straightforward to calculate ![](img/B14070_06_035.png).
  prefs: []
  type: TYPE_NORMAL
- en: Why we cannot use BP directly for RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let’s try the same for the RNN in *Figure 6.6*. Now we have an additional
    recurrent weight *w*[3]. We have omitted the time components of inputs and outputs
    for the clarity of the problem we are trying to emphasize:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why we cannot use BP directly for RNNs](img/B14070_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Computations of an RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see what happens if we apply the chain rule to calculate ![](img/B14070_06_036.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_038.png)'
  prefs: []
  type: TYPE_IMG
- en: The term ![](img/B14070_06_039.png) here creates problems because it is a recursive
    term. You end up with an infinite number of derivative terms, as *h* is recursive
    (that is, calculating *h* includes *h* itself) and *h* is not a constant and dependent
    on *w*[3]. This is solved by unrolling the input sequence *x* over time, creating
    a copy of the RNN for each input *x*[t] and calculating derivatives for each copy
    separately, and collapsing those updates into one, by summing up the gradients,
    to calculate the weight update. We will discuss the details of this process next.
  prefs: []
  type: TYPE_NORMAL
- en: Backpropagation Through Time – training RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The trick to calculating backpropagation for RNNs is to consider not a single
    input, but the full input sequence. Then, if we calculate ![](img/B14070_06_040.png)
    at time step 4, we will get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_041.png)'
  prefs: []
  type: TYPE_IMG
- en: This means that we need to calculate the sum of gradients for all the time steps
    up to the fourth time step. In other words, we will first unroll the sequence
    so that we can calculate ![](img/B14070_06_042.png) and ![](img/B14070_06_043.png)
    for each time step *j*. This is done by creating four copies of the RNN. So, to
    calculate ![](img/B14070_06_044.png), we need *t-j+1* copies of the RNN. Then
    we will roll up the copies to a single RNN by summing up gradients with respect
    to all previous time steps to get the gradient, and update the RNN with the gradient
    ![](img/B14070_06_045.png).
  prefs: []
  type: TYPE_NORMAL
- en: However, this becomes costly as the number of time steps increases. For more
    computational efficiency, we can use **Truncated Backpropagation Through Time**
    (**TBPTT**) to optimize recurrent models, which is an approximation of BPTT.
  prefs: []
  type: TYPE_NORMAL
- en: Truncated BPTT – training RNNs efficiently
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In TBPTT, we only calculate the gradients for a fixed number of *T* time steps
    (in contrast to calculating it up to the very beginning of the sequence as in
    BPTT). More specifically, when calculating ![](img/B14070_06_040.png), for time
    step *t*, we only calculate derivatives down to *t-T* (that is, we do not compute
    derivatives up to the very beginning):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_047.png)'
  prefs: []
  type: TYPE_IMG
- en: This is much more computationally efficient than standard BPTT. In standard
    BPTT, for each time step *t*, we calculate derivatives up to the very beginning
    of the sequence. But this gets computationally infeasible as the sequence length
    becomes larger and larger (for example, this could occur when processing a long
    text document word by word). However, in truncated BPTT, we only calculate the
    derivatives for a fixed number of steps backward, and as you can imagine, the
    computational cost does not change as the sequence becomes larger.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of BPTT – vanishing and exploding gradients
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having a way to calculate gradients for recurrent weights and having a computationally
    efficient approximation such as TBPTT does not enable us to train RNNs without
    trouble. Something else can go wrong with the calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see why, let’s expand a single term in ![](img/B14070_06_048.png), which
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_049.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since we know that the issues of backpropagation arise from the recurrent connections,
    let’s ignore the *w*[1]*x* terms and consider the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_050.png)'
  prefs: []
  type: TYPE_IMG
- en: 'By simply expanding *h*[3] and doing simple arithmetic operations we can show
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_051.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that for just four time steps we have a term ![](img/B14070_06_052.png).
    So at the *n*^(th) time step, it would become ![](img/B14070_06_053.png). Say
    we initialized *w*[3] to be very small (say 0.00001) at *n*=*100* time step; the
    gradient would be infinitesimally small (of scale 10^(-500)). Also, since computers
    have limited precision in representing a number, this update would be ignored
    (that is, arithmetic underflow). This is called the **vanishing gradient**.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the vanishing gradient is not very straightforward. There are no easy
    ways of rescaling the gradients so that they will properly propagate through time.
    A few techniques used in practice to solve the problem of vanishing gradients
    are to use careful initialization of weights (for example, the Xavier initialization),
    or to use momentum-based optimization methods (that is, in addition to the current
    gradient update, we add an additional term, which is the accumulation of all the
    past gradients known as the **velocity term**). However, more principled approaches
    to solving the vanishing gradient problem, such as different structural modifications
    to the standard RNN, have been introduced, as we will see in *Chapter 7, Understanding
    Long* *Short-Term Memory Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, say that we initialized *w*[3] to be very large (say 1000.00).
    Then at the *n*=*100* time step, the gradients would be massive (of scale 10^(300)).
  prefs: []
  type: TYPE_NORMAL
- en: This leads to numerical instabilities and you will get values such as `Inf`
    or `NaN` (that is, not a number) in Python. This is called the **exploding gradient**.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient explosion can also take place due to the complexity of the loss surface
    of a problem. Complex nonconvex loss surfaces are very common in deep neural networks
    due to both the dimensionality of inputs as well as the large number of parameters
    (weights) present in the models.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure* *6.7* illustrates the loss surface of an RNN and highlights the presence
    of walls with very high curvature. If the optimization method comes in contact
    with such a wall, then the gradients will explode or overshoot, as shown by the
    solid line in the image. This can either lead to very poor loss minimization,
    numerical instabilities, or both. A simple solution to avoid gradient explosion
    in such situations is to clip the gradients to a reasonably small value when it
    is larger than some threshold. The dashed line in the figure shows what happens
    when we clip the gradient at some small value. (Gradient clipping is covered in
    the paper *On the difficulty of training recurrent neural networks*, *Pascanu*,
    *Mikolov*, *and* *Bengio*, *International Conference on Machine Learning (2013):
    1310-1318*.)'
  prefs: []
  type: TYPE_NORMAL
- en: '![Limitations of BPTT – vanishing and exploding gradients](img/B14070_06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: The gradient explosion phenomenon. Source: This figure is from
    the paper ‘On the difficulty of training recurrent neural networks’ by Pascanu,
    Mikolov, and Bengio'
  prefs: []
  type: TYPE_NORMAL
- en: Here we conclude our discussion about BPTT, which adapts backpropagation for
    RNNs. Next we will discuss various ways that RNNs can be used to solve applications.
    These applications include sentence classification, image captioning, and machine
    translation. We will categorize the RNNs into several different categories such
    as one-to-one, one-to-many, many-to-one, and many-to-many.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have only talked about one-to-one-mapped RNNs, where the current
    output depends on the current input as well as the previously observed history
    of inputs. This means that there exists an output for the sequence of previously
    observed inputs and the current input. However, in the real word, there can be
    situations where there is only one output for a sequence of inputs, a sequence
    of outputs for a single input, and a sequence of outputs for a sequence of inputs
    where the sequence sizes are different. In this section, we will look at several
    different settings of RNN models and the applications they would be used in.
  prefs: []
  type: TYPE_NORMAL
- en: One-to-one RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In one-to-one RNNs, the current input depends on the previously observed inputs
    (see *Figure 6.8*). Such RNNs are appropriate for problems where each input has
    an output, but the output depends both on the current input and the history of
    inputs that led to the current input. An example of such a task is stock market
    prediction, where we output a value for the current input, and this output also
    depends on how the previous inputs have behaved. Another example would be scene
    classification, where each pixel in an image is labeled (for example, labels such
    as car, road, and person). Sometimes *x*[t+1] can be the same as *y*[t] for some
    problems. For example, in text generation problems, the previously predicted word
    becomes an input to predict the next word. The following figure depicts a one-to-one
    RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B14070_06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: One-to-one RNNs having temporal dependencies'
  prefs: []
  type: TYPE_NORMAL
- en: One-to-many RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A one-to-many RNN would take a single input and output a sequence (see *Figure
    6.9*). Here, we assume the inputs to be independent of each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is, we do not need information about previous inputs to make a prediction
    about the current input. However, the recurrent connections are needed because,
    although we process a single input, the output is a sequence of values that depends
    on the previous output values. An example task where such an RNN would be used
    is an image captioning task. For example, for a given input image, the text caption
    can consist of five or ten words. In other words, the RNN will keep predicting
    words until it outputs a meaningful phrase describing the image. The following
    figure depicts a one-to-many RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![One-to-many RNNs](img/B14070_06_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: A one-to-many RNN'
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-one RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many-to-one RNNs take an input of arbitrary length and produce a single output
    for the sequence of inputs (see *Figure 6.10*). Sentence classification is one
    such task that can benefit from a many-to-one RNN. A sentence is represented to
    the model as a sequence of words of arbitrary length. The model takes it as the
    input and produces an output, classifying the sentence into one of a set of predefined
    classes. Some specific examples of sentence classification are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying movie reviews as positive or negative statements (that is, sentiment
    analysis)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying a sentence depending on what the sentence describes (for example,
    person, object, or location)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another application of many-to-one RNNs is classifying large-scale images by
    processing only a patch of images at a time and moving the window over the whole
    image.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a many-to-one RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Many-to-one RNNs](img/B14070_06_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: A many-to-one RNN'
  prefs: []
  type: TYPE_NORMAL
- en: Many-to-many RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Many-to-many RNNs (or Sequences-to-Sequence, seq2seq for short) often produce
    arbitrary-length outputs from arbitrary-length inputs (see *Figure 6.11*). In
    other words, inputs and outputs do not have to be of the same length. This is
    particularly useful in machine translation, where we translate a sentence from
    one language to another. As you can imagine, one sentence in a certain language
    does not always align with a sentence from another language. Another such example
    is chatbots, where the chatbot reads a sequence of words (that is, a user request)
    and outputs a sequence of words (that is, the answer). The following figure depicts
    a many-to-many RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Many-to-many RNNs](img/B14070_06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: A many-to-many RNN'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize the different types of applications of feed-forward networks
    and RNNs as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Algorithm** | **Description** | **Applications** |'
  prefs: []
  type: TYPE_TB
- en: '| One-to-one RNNs | These take a single input and give a single output. Current
    input depends on the previously observed input(s). | Stock market prediction,
    scene classification, and text generation |'
  prefs: []
  type: TYPE_TB
- en: '| One-to-many RNNs | These take a single input and give an output consisting
    of an arbitrary number of elements | Image captioning |'
  prefs: []
  type: TYPE_TB
- en: '| Many-to-one RNNs | These take a sequence of inputs and give a single output.
    | Sentence classification (considering a single word as a single input) |'
  prefs: []
  type: TYPE_TB
- en: '| Many-to-many RNNs | These take a sequence of arbitrary length as inputs and
    output a sequence of arbitrary length. | Machine translation, chatbots |'
  prefs: []
  type: TYPE_TB
- en: Next, we will learn how to use RNNs to identify various entities mentioned in
    a text corpus.
  prefs: []
  type: TYPE_NORMAL
- en: Named Entity Recognition with RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now let’s look at our first task: using an RNN to identify named entities in
    a text corpus. This task is known as **Named Entity Recognition** (**NER**). We
    will be using a modified version of the well-known **CoNLL 2003** (which stands
    for **Conference on Computational Natural Language Learning - 2003**) dataset
    for NER.'
  prefs: []
  type: TYPE_NORMAL
- en: CoNLL 2003 is available for multiple languages, and the English data was generated
    from a Reuters Corpus that contains news stories published between August 1996
    and August 1997\. The database we’ll be using is found at [https://github.com/ZihanWangKi/CrossWeigh](https://github.com/ZihanWangKi/CrossWeigh)
    and is called **CoNLLPP**. It is a more closely curated version than the original
    CoNLL, which contains errors in the dataset induced by incorrectly understanding
    the context of a word. For example, in the phrase *“Chicago won …”* Chicago was
    identified as a location, whereas it is in fact an organization. This exercise
    is available in `ch06_rnns_for_named_entity_recognition.ipynb` in the `Ch06-Recurrent-Neural-Networks`
    folder.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have defined a function called `download_data()`, which can be used to download
    the data. We will not go into the details of it as it simply downloads several
    files and places them in a data folder. Once the download finishes, you’ll have
    three files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`data\conllpp_train.txt` – Training set, contains 14041 sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data\conllpp_dev.txt` – Validation set, contains 3250 sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data\conllpp_test.txt` – Test set, contains 3452 sentences'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next up, we will read the data and convert it into a specific format that suits
    our model. But before that, we need to see what our data looks like originally:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the document has a single word in each line along with the
    associated tags of that word. These tags are in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: The Part-of-speech (POS) tag (e.g. noun - `NN`, verb - `VB`, determinant - `DT`,
    etc.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Chunk tag – A chunk is a segment of text made of one or more tokens (for example,
    `NP` represents a noun phrase such as “The European Commission”)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Named entity tag (e.g. Location, Organization, Person, etc.)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Both chunk tags and named entity tags have a `B-` and `I-` prefix (e.g. `B-ORG`
    or `I-ORG`). These prefixes are there to differentiate the starting token of an
    entity/chunk from the continuing token of an entity/chunk.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also five types of entities in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Location-based entities (`LOC`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Person-based entities (`PER`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Organization-based entities (`ORG`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miscellaneous entities (`MISC`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-entities (`O`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, there’s an empty line between separate sentences.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now let’s look at the code that loads the data we downloaded into memory, so
    that we can start using it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we will store all the sentences (as a list of strings in `sentences`)
    and all the labels associated with each token in the sentences (as a list of lists
    in `ner_labels`). We will read the file line by line. We will maintain a Boolean
    called `is_sos` that indicates whether we are at the start of a sentence. We will
    also have two temporary lists (`sentence_tokens` and `sentence_labels`) that will
    accumulate the tokens and the NER labels of the current sentence. When we are
    at the start of a sentence, we reset these temporary lists. Otherwise, we keep
    writing each token and NER label we see in the file to these temporary lists.
    We can now run this function on the train, validation, and test corpora we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will print a few samples and see what we have with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the unique characteristics of NER tasks is the class imbalance. That
    is, not all classes will have a roughly equal number of samples. As you can probably
    guess, in a corpus, there are more non-named entities than named entities. This
    leads to a significant class imbalance among labels. Therefore, let’s have a look
    at the distribution of samples among different classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'To analyze the data, we will first convert the NER labels into a pandas `Series`
    object. This can be done by simply calling the `pd.Series()` construct on `train_labels`,
    `valid_labels`, and `test_labels`. But remember that these were lists of lists,
    where each inner list represents the NER tags for all the tokens in a sentence.
    To create a flat list, we can use the `chain()` function from the built-in Python
    library `itertools`. It will chain several lists together to form a single list.
    After that, we call the `value_counts()` function on that pandas `Series`. This
    will return a new list, where the indices are unique labels found in the original
    `Series` and the values are the counts of occurrences of each label. This gives
    us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, O labels are several magnitudes higher than the volume of other
    labels. We need to keep this in mind when training the model. Subsequently, we
    will analyze the sequence length (i.e. number of tokens) of each sentence. We
    need this information later to pad our sentences to a fixed length.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we create a pandas `Series`, where each item has the length of a sentence
    after splitting each sentence into a list of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we will look at the 5% and 95% percentiles of those lengths. This produces:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We can see that 95% of our sentences have 37 tokens or less.
  prefs: []
  type: TYPE_NORMAL
- en: Processing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now it’s time to process the data. We will keep the sentences in the same format,
    i.e. a list of strings where each string represents a sentence. This is because
    we will integrate text processing right into our model (as opposed to doing it
    externally). For labels, we have to do several changes. Remember labels are a
    list of lists, where the inner lists represent labels for all the tokens in each
    sentence. Specifically we will do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Convert the class labels to class IDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pad the sequences of labels to a specified maximum length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate a mask that indicates the padded labels, so that we can use this information
    to disregard the padded labels during model training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First let’s write a function to get a class label to class ID mapping. This
    function leverages pandas’ `unique()` function to get the unique labels in the
    training set and generate a mapping of integers to unique labels found.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then you will get:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We write a function called `get_padded_int_labels()` that will take sequences
    of class labels and return sequences of padded class IDs, with the option to return
    a mask indicating padded labels. This function takes the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`labels` (`List[List[str]]`) – A list of lists of strings, where each string
    is a class label of the string type'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labels_map` (`Dict[str, int]`) – A dictionary mapping a string label to a
    class ID of type integer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_seq_length` (`int`) – A maximum length to be padded to (longer sequences
    will be truncated at this length)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_mask` (`bool`) – Whether to return the mask showing padded labels or
    not'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s now look at the code that performs the aforementioned operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: You can see the first step in the function converts all the string labels in
    `labels` to integer labels using the `labels_map`. Next we get the padded sequences
    with the `tf.keras.preprocessing.sequence.pad_sequences()` function. We discussed
    this function in detail in the previous chapter. Essentially, it will pad (with
    a specified value) and truncate arbitrary-length sequences, to return fixed-length
    sequences. We are instructing the function to do both padding and truncating at
    the end of sequences, and to pad with a special value of `-1`. Then we can simply
    generate the mask as a boolean filter where `padded_labels` is not equal to `-1`.
    Thus, the positions where original labels exist will have a value of `1` and the
    rest will have `0`. However, we have to convert the `-1` values to a class ID
    found in the `labels_map`. We will give them the class ID of the label `O` (i.e.
    others).
  prefs: []
  type: TYPE_NORMAL
- en: 'From our findings in the previous chapter, we will set the maximum sequence
    length to `40`. Remember that the 95% percentile fell at the length of 37 words:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'And now we will generate processed labels and masks for all of the training,
    validation, and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will print the processed labels and masks of the first two sequences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Which returns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You can see that the mask is indicating the true labels and padded ones clearly.
    Next, we will define some hyperparameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now let’s define several hyperparameters needed for our RNN, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_seq_length` – Denotes the maximum length for a sequence. We infer this
    from our training data during data exploration. It is important to have a reasonable
    length for sequences, as otherwise, memory can explode, due to the unrolling of
    the RNN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`emedding_size` – The dimensionality of token embeddings. Since we have a small
    corpus, a value < 100 will suffice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`rnn_hidden_size` – The dimensionality of hidden layers in the RNN. Increasing
    dimensionality of the hidden layer usually leads to better performance. However,
    note that increasing the size of the hidden layer causes all three sets of internal
    weights (that is, *U*, *W*, and *V*) to increase as well, thus resulting in a
    high computational footprint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_classes` – Number of unique output classes present.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` – The batch size for training data, validation data, and test
    data. A higher batch size often leads to better results as we are seeing more
    data during each optimization step, but just like unrolling, this causes a higher
    memory requirement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs` – The number of epochs to train the model for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are defined below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Now we will define the model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will define the model here. Our model will have an embedding layer, followed
    by a simple RNN layer, and finally a dense prediction layer. One thing to note
    in the work we have done so far is that, unlike in previous chapters, we haven’t
    yet defined a `Tokenizer` object. Although the `Tokenizer` has been an important
    part of our NLP pipeline to convert each token (or word) into an ID, there’s a
    big downside to using an external tokenizer. After training the model, if you
    forget to save the tokenizer along with the model, your machine learning model
    becomes useless: to combat this, during inference, you would need to map each
    word to the exact ID it was mapped to during training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a significant risk the tokenizer poses. In this chapter, we will seek
    an alternative, where we will integrate the tokenization mechanism right into
    our model, so that we don’t need to worry about it later. *Figure 6.12* depicts
    the overall architecture of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Overall architecture of the model. The text vectorization layer
    tokenizes the text and converts it into word IDs. Next, each token is fed as an
    input at each timestep of the RNN. Finally, the RNN predicts a label for each
    token at every time step'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the TextVectorization layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `TextVectorization` layer can be thought of as a modernized tokenizer that
    can be plugged into the model. Here, we will play around just with the `TextVectorization`
    layer, without the overhead of the complexity from the rest of the model. First,
    we will import the `TextVectorization` layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we will define a simple text corpus:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can instantiate a text vectorization layer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'After instantiating, you need to fit this layer on some data. This way, just
    like the tokenizer we used previously, it can learn a word-to-numerical ID mapping.
    For this, we invoke the `adapt()` method of the layer, by passing the corpus of
    text as an input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We can generate the tokenized output as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Which will have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also see the vocabulary the layer has learned:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the layer has done some pre-processing (e.g. turned words to
    lowercase and removed punctuation). Next let’s see how we can limit the size of
    the vocabulary. We can do this with the `max_tokens` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If you convert the `toy_corpus` to word IDs, you will see:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The vocabulary will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now see that there are only five elements in the vocabulary, just like
    we specified. Now if you need to skip the text pre-processing that happens within
    the layer, you can do so by setting the `standardize` argument to `None` in the
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The vocabulary will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we can also control the padding/truncation of sequences with the `output_sequence_length`
    command. For example, the following command will pad/truncate sequences at length
    `4:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Here the vocabulary is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Now you have a good understanding of the arguments and what they do in the `TextVectorization`
    layer. Let’s now discuss the model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the rest of the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First we will import the necessary modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define an input layer that has a single column (i.e. each sentence
    represented as a single unit) and has `dtype=tf.string`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will define a function that takes a corpus, a maximum sequence length,
    and a vocabulary size, and returns the trained `TextVectorization` layer and the
    vocabulary size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The function does what we have already described. However, pay attention to
    the various arguments we have set for the vectorization layer. We are passing
    the vocabulary size as `max_tokens`; we are setting the `standardize` to `None`.
    This is an important setting. When performing NER, keeping the case of characters
    is very important. Typically, an entity starts with an uppercase letter (e.g.
    the name of a person or organization). Therefore, we should preserve the case
    in the text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we also set the `output_sequence_length` to the sequence length we
    found during the analysis. With that, we create the text vectorization layer as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Then pass the `word_input` to the `vectorize_layer` and get the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output from the `vectorize_layer` (i.e `vectorized_out`) will be sent to
    an embedding layer. This embedding layer is a randomly initialized embedding layer,
    which will have an output dimensionality of `embedding_size`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Until now, we dealt with feed-forward networks. Outputs of feed-forward networks
    did not have a time dimension. But if you look at the output from the `TextVectorization`
    layer, it will be a `[batch size, sequence length]` - sized output. When this
    output goes through an embedding layer, the output would be a `[batch size, sequence
    length, embedding size]`-shaped tensor. In other words, there is an additional
    time dimension included in the output of the embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: Another difference is the introduction of the `mask_true` argument. Masking
    is used to mask uninformative words added to sequences (e.g. the padding token
    added to make sentences a fixed length), as they do not contribute to the final
    outcome. Masking is a commonly used technique in sequence learning. To learn more
    about masking, please read the information box below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Masking in sequence learning**'
  prefs: []
  type: TYPE_NORMAL
- en: Naturally, text has arbitrary lengths. For example, sentences in a corpus would
    have a wide variety of token lengths. But deep networks process tensors with fixed
    dimensions. To bring arbitrary-length sentences to constant length, we pad these
    sequences with some special value (e.g. 0). However, these padded values are synthetic,
    and only serve as a way to ensure the correct input shape. They should not contribute
    to the final loss or evaluation metrics. To ignore them during loss calculation
    and evaluation, “masking” is used. The idea is to multiply the loss resulting
    from padded timesteps with a zero, essentially cutting them off from the final
    loss.
  prefs: []
  type: TYPE_NORMAL
- en: It would be cumbersome to manually perform masking when training a model. But
    in TensorFlow, most layers support masking. For example, in the embedding layer,
    to ignore padded values (which will be zeros), all you need to do is set `mask_true=True`.
  prefs: []
  type: TYPE_NORMAL
- en: When you enable masking in a layer, it will propagate the mask to the downstream
    layers, flowing down until the loss computations. In other words, you only need
    to enable masking at the start of the model (as we have done at the embedding
    layer) and the rest is taken care of by TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this, we will define the core layer of our model, the RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'You can implement a vanilla RNN by simply calling `tf.keras.layers.SimpleRNN`.
    Here we pass two important arguments. There are other useful arguments besides
    the two discussed here, however, they will be covered in later chapters with more
    complex variants of RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: '`units` (`int`) – This defines the hidden output size of the RNN model. The
    larger this is, the more representational power the model will have.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`return_sequences` (`bool`) – Whether to return outputs from all the timesteps,
    or to return only the last output. For NER tasks, we need to label every single
    token. Therefore we need to return outputs for all the time steps.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `rnn_layer` takes a `[batch size, sequence length, embedding size]`-sized
    tensor and returns a `[batch size, sequence length, rnn hidden size]`-sized tensor.
    Finally, the time-distributed output from the RNN will go to a Dense layer with
    `n_classes` output nodes and a `softmax` activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can define the final model as follows. It takes a batch of string
    sentences as the input, and returns a batch of sequences of labels as the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: We have now finished building the model. Next, we will discuss the loss function
    and the evaluation metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation metrics and the loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'During our previous discussion, we alluded to the fact that NER tasks carry
    a high class imbalance. It is quite normal for text to have more non-entity-related
    tokens than entity-related tokens. This leads to large amounts of other (`O`)
    type labels and fewer of the remaining types. We need to take this into consideration
    when training the model and evaluating the model. We will address the class imbalance
    in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: We will create a new evaluation metric that is resilient to class imbalance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use sample weights to penalize more frequent classes and boost the importance
    of rare classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we will only address the former. The latter will be addressed
    in the next section. We will define a modified version of the accuracy. This is
    called a macro-averaged accuracy. In macro averaging, we compute accuracies for
    each class separately, and then average it. Therefore, the class imbalance is
    ignored when computing the accuracy. When computing standard metrics like accuracy
    precision or recall, there are different types of averaging available. To learn
    more about these, read the information box below.
  prefs: []
  type: TYPE_NORMAL
- en: '**Different types of metric averaging**'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are different types of averaging available for metrics. You can read
    one such example of these averaging available in scikit-learn explained at [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html
    ). Consider a simple binary classification example with the following confusion
    matrix results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Example confusion matrix results'
  prefs: []
  type: TYPE_NORMAL
- en: '**micro** – Computes a global metric, ignoring the differences in class distribution.
    e.g. 35/65 = ~54%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**macro** – Computes the metric for each class separately and computes the
    mean. e.g. (35/40 + 0/25)/2 = ~43.7%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**weighted** – Computes the metric for each class separately and weighs it
    by support (i.e. number of true labels for each class). e.g. (35/40)* 40 + (0/25)
    * 25 / 65 = ~54%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here you can see the micro and weighted return the same result. This is because
    the denominator of the accuracy computation is the same as the support. Therefore,
    they cancel out in the weighted averaging. However for other metrics such as precision
    and recall you will get different values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Below we define the function to compute macro accuracy using a batch of true
    targets (`y_true`) and predictions (`y_pred`). `y_true` will have the shape `[batch_size,
    sequence length]` and `y_pred` will have the shape `[batch size, sequence length,
    n_classes]`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: It is important to note that we have to write this function using TensorFlow
    operations, so that they are executed as a graph. Even though TensorFlow 2 has
    migrated toward more imperative style execution operations, there still are remnants
    of the declarative style introduced by TensorFlow 1.
  prefs: []
  type: TYPE_NORMAL
- en: First we flatten `y_true` so that it’s a vector. Next we get the predicted label
    from `y_pred` using the `tf.argmax()` function and flatten the predicted labels
    to a vector. The two flattened structures will have the same number of elements.
    Then we sort `y_true`, so that same-labeled elements are close together.
  prefs: []
  type: TYPE_NORMAL
- en: We take the indices of the original data after sorting and then use the `tf.gather()`
    function to order `y_pred` in the same order as `y_true`. In other words, `sorted_y_true`
    and `sorted_y_pred` still have the same correspondence with each other. The `tf.gather()`
    function takes a tensor and a set of indices and orders the passed tensor in the
    order of the indices. For more information about `tf.gather()` refer to [https://www.tensorflow.org/api_docs/python/tf/gather](https://www.tensorflow.org/api_docs/python/tf/gather).
  prefs: []
  type: TYPE_NORMAL
- en: 'Then we compute `sorted_correct`, which is a simple indicator function that
    switches on if the corresponding element in `sorted_y_true` and `sorted_y_pred`
    are the same, and if not stays off. Then we use the `tf.math.segment_sum()` function
    to compute a segmented sum of correctly predicted samples. Samples belonging to
    each class are considered a single segment (`correct_for_each_label`). The `segment_sum()`
    function takes two arguments: `data` and `segment_ids`. For example, if the `data`
    is `[0, 1, 2, 3, 4, 5, 6, 7]` and `segment_ids` are `[0, 0, 0, 1, 1, 2, 3, 3]`,
    then the segment sum would be `[0+1+2, 3+4, 5, 6+7] = [3, 7, 5, 13]`.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we do the same for a vector of 1s. In this case, we get the number of true
    samples present for each class in the batch of data (`all_for_each_label`). Note
    that we are adding a 1 at the end. This is to avoid division by 0 in the next
    step. Finally, we divide `correct_for_each_label` by `all_for_each_label`, which
    gives us a vector containing the accuracy of each class. With that we compute
    the mean accuracy, which is the macro-averaged accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally we wrap this function in a `MeanMetricWrapper` that will produce a
    `tf.keras.metrics.Metric` object that we can pass to the `model.compile()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Compile the model by calling:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will train the model with the data prepared.
  prefs: []
  type: TYPE_NORMAL
- en: Training and evaluating RNN on NER task
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s train our model on the data we have prepared. But first, we need to define
    a function to tackle the class imbalance in our dataset. We will pass sample weights
    to the `model.fit()` function. To compute sample weights, we will first define
    a function called `get_class_weights()` that computes `class_weights` for each
    class. Next we will pass the class weights to another function, `get_sample_weights_from_class_weights()`,
    which will generate sample weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The first function, `get_class_weights()`, takes a `train_labels` (a list of
    list of class IDs). Then we create a pandas `Series` object with `train_labels`.
    Note that we are using a function called `chain` from the built-in `itertools`
    library, which will flatten `train_labels` to a list of class IDs. The `Series`
    object contains frequency counts of each class label that appears in the train
    dataset. Next to compute weights, we divide the minimum frequency element-wise
    from other frequencies. In other words, if the frequency for class label ![](img/B14070_06_054.png)
    is denoted by ![](img/B14070_06_055.png), and the total label set is denoted by
    ![](img/B14070_06_056.png), the weight for class ![](img/B14070_06_054.png) is
    computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14070_06_058.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, the output is converted into a dictionary that has class IDs as keys
    and class weights as values. Next we need to convert the `class_weights` to `sample_weights`.
    We simply perform a dictionary lookup element-wise on each label to generate a
    sample weight from `class_weights`. The `sample_weights` will be the same shape
    as the `train_labels` as there’s one weight for each sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use NumPy’s `np.vectorize()` function to achieve this. `np.vectorize()`
    takes in a function (e.g. `class_weights.get()` is the key lookup function provided
    by Python) and applies that on all elements, which gives us the sample weights.
    Call the functions we defined above to generate the actual weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'After we have the sample weights at our disposal, we can train our model. You
    can view the `class_weights` by printing them out. This will give:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see the class `Other` has the lowest weight (because it’s the most
    frequent), and the class `I-MISC` has the highest as it’s the least frequent.
    Now we will train our model using the prepared data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'You should get an accuracy of around 78-79% without any special performance
    optimization tricks. Next you can evaluate the model on test data with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This will give a test accuracy of around 77%. Since the validation accuracy
    and test accuracy are on par, we can say that the model has generalized well.
    But to make sure, let’s visually inspect a few samples from the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Visually analyzing outputs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To analyze the output, we will use the first five sentences in the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Next predict using the model and convert those predictions to predicted class
    IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'We will create a reversed `labels_map` that has a mapping from label ID to
    label string:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will print out the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: It can be seen that our model is doing a decent job. It is good at identifying
    locations but is struggling at identifying the names of people. Here we end our
    discussion about the basic RNN solution that performs NER. In the next section,
    we will make the model more complex, giving it the ability to understand text
    better by providing more fine-grained details. Let’s understand how we can improve
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: NER with character and token embeddings
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nowadays, recurrent models used to solve the NER task are much more sophisticated
    than having just a single embedding layer and an RNN model. They involve using
    more advanced recurrent models like **Long Short-Term Memory** (**LSTM**), **Gated
    Recurrent Units** (**GRUs**), etc. We will set aside the discussion about these
    advanced models for several upcoming chapters. Here we will focus our discussion
    on a technique that provides the model embeddings at multiple scales, enabling
    it to understand language better. That is, instead of relying only on token embeddings,
    also use character embeddings. Then a token embedding is generated with the character
    embeddings by shifting a convolutional window over the characters in the token.
    Don’t worry if you don’t understand the details yet. The following sections will
    go into specific details of the solution. This exercise is available in `ch06_rnns_for_named_entity_recognition.ipynb`
    in the `Ch06-Recurrent-Neural-Networks` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Using convolution to generate token embeddings
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A combination of character embeddings and a convolutional kernel can be used
    to generate token embeddings (*Figure 6.14*). The method will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Pad each token (e.g. word) to a predefined length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Look up the character embeddings for the characters in the token from an embedding
    layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shift a convolutional kernel over the sequence of character embeddings to generate
    a token embedding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B14070_06_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: How token embeddings are generated using character embeddings
    and the convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The very first thing we need to do is analyze the statistics around how many
    characters there are for a token in our corpus. Similar to how we did it previously,
    we can do this with pandas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: In computing `vocab_ser`, the first part (i.e. `pd.Series(train_sentences).str.split()`)
    will result in a pandas `Series`, whose elements are a list of tokens (each token
    in the sentence is an item of that list). Next, `explode()` will convert the `Series`
    of a list of tokens into a `Series` of tokens, by converting each token into a
    separate item in the `Series`. Finally we take only the unique tokens in that
    `Series`. Here we end up with a pandas `Series` where each item is a unique token.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now use the `str.len()` function to get the length of each token (i.e.
    the number of characters) and look at the 95% percentile in that. We will get
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see around 95% of our words have less than or equal to 12 characters.
    Next, we will write a function to pad shorter tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The function takes a set of tokenized sentences (i.e. each sentence as a list
    of tokens, not a string) and a maximum sequence length. Note that this is the
    maximum sequence length we used previously, not the new token length we discussed.
    This function would then do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For longer sentences, only return the `max_seq_length` tokens
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For shorter sentences, append ‘‘ as a token until `max_seq_length` is reached
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s run this function on a small toy dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now define a new `TextVectorization` layer that can cope with the changes
    we introduced to the data. Instead of tokenizing on the token level, the new `TextVectorization`
    layer must tokenize on the character level. For this we need to make a few changes.
    We will again write a function to contain this vectorization layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'We first define a function called `_split_char()` that takes a token (as a
    `tf.Tensor`) and returns a char-tokenized tensor. For example, `_split_char(tf.constant([''abcd'']))`
    would return `<tf.RaggedTensor [[b''a'', b''b'', b''c'', b''d'']]>`. Then we define
    a `TextVectorization` layer that will use this newly defined function as the way
    to split the data it gets. We will also define `output_sequence_length` as `max_token_length`.
    Then we create `tokenized_sentences`, a list of list of strings, and pad it using
    the `prepare_corpus_for_char_embeddings()` function we defined earlier. Finally
    we use the `TextVectorization` layer’s `adapt()` function to fit it with the data
    we prepared. Two key differences between the previous token-based text vectorizer
    and this char-based text vectorizer are in the input dimensions and the final
    output dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: Token-based vectorizer – Takes in a `[batch size, 1]`-sized input and produces
    a `[batch size, sequence length]`-sized output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Char-based vectorizer – Takes in a `[batch size, sequence length, 1]`-sized
    input and produces a `[batch size, sequence length, token length]`-sized output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we are equipped with the ingredients to implement our new and improved NER
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the new NER model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With a good conceptual understanding of the model, let’s implement the new NER
    model. We will first define some hyperparameters, followed by defining a text
    vectorizer as before. However, our `TextVectorization` will be more complex in
    this section, as we have several different levels of tokenization taking place
    (e.g. char-level and token-level). Finally we define the RNN-based model that
    produces the output.
  prefs: []
  type: TYPE_NORMAL
- en: Defining hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, we will define the two hyperparameters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: Defining the input layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We then define an input layer with the data type `tf.strings` as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: The inputs to this layer would be a batch of sentences, where each sentence
    is a string.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the token-based TextVectorization layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Then we define the token-level `TextVectorization` layer just like we did above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Defining the character-based TextVectorization layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the character-level vectorization layer we will employ the `get_fitted_char_vectorization_layer()`
    function we defined above:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will discuss the inputs for this layer.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the inputs for the char_vectorize_layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use the same `word_input` for this new vectorization layer as well.
    However, using the same input means we need to introduce some interim pre-processing
    to get the input to the correct format intended for this layer. Remember that
    the input to this layer needs to be a `[batch size, sequence length, 1]`-sized
    tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'This means the sentences need to be tokenized to a list of tokens. For that
    we will use the `tf.keras.layers.Lambda()` layer and the `tf.strings.split()`
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: The `Lambda` layer is used as a way to create a layer from a custom TensorFlow/Keras
    function, which may not be available as a standard layer in Keras. Here we are
    using a `Lambda` layer to define a layer that will tokenize a passed input to
    a list of tokens. Furthermore, the `tf.strings.split()` function returns a ragged
    tensor. In a typical tensor, all the dimensions need to have a constant size.
    A ragged tensor is a special tensor whose dimensions are not fixed. For example,
    since a list of sentences is highly unlikely to have the same number of tokens,
    this results in a ragged tensor. But TensorFlow will complain if you try to go
    forward with a `tf.RaggedTensor` as most layers do not support these tensors.
    Therefore, we need to convert this to a standard tensor using the `to_tensor()`
    function. We can pass a shape to this function and it will make sure the shape
    of the resulting tensor will be the defined shape (by means of padding and truncations).
  prefs: []
  type: TYPE_NORMAL
- en: A key thing to pay attention to is how the shapes of the input-output tensors
    are transformed at each layer. For example, we started off with a `[batch size,
    1]`-sized tensor that went into the `Lambda` layer to be transformed to a `[batch
    size, sequence length, 1]`-sized layer. Finally, the `char_vectorize_layer` transforms
    this into a `[batch size, sequence length, token length]`-sized tensor.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will then define an embedding layer, with which we will look up embeddings
    for the resulting char IDs coming from the `char_vectorize_layer`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: This layer produces a `[batch size, sequence length, token length, 32]`-sized
    tensor, with a char embedding vector for each character in the tensor. Now it’s
    time to perform convolution on top of this output.
  prefs: []
  type: TYPE_NORMAL
- en: Performing convolution on the character embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will define a 1D convolution layer with a kernel size of 5 (i.e. convolutional
    window size), a stride of 1, `''same''` padding, and a ReLU activation. We then
    feed the output from the previous section to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: This layer typically takes a `[batch size, width, in channels]`-sized tensor.
    However, in our case, we have a four-dimensional input. This means, our Conv1D
    layer is going to behave in a time-distributed fashion. Put in another way, it
    will take an input with a temporal dimension (i.e. sequence length dimension)
    and produce an output with that dimension intact. In other words, it takes our
    input of shape `[batch size, sequence length, token length, 32 (in channels)]`
    and produces a `[batch size, sequence length, token length, 1 (out channels)]`-sized
    output. You can see that the convolution only operates on the last two dimensions,
    while keeping the first two as they are.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to think about this is, ignore the batch and sequence dimensions
    and visualize how convolution would work on the width and in channel dimensions.
    Then apply the same operation element-wise to other dimensions, while considering
    the operation on 2D `[width, in channel]` tensors as a single unit of computation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that we have a `[batch size, sequence length, token length, 1]`-sized
    output. This has an extra dimension of 1 at the end. We will write a simple `Lambda`
    layer to get rid of this dimension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'To get the final output embedding (i.e. a combination of token- and character-based
    embeddings), we concatenate the two embeddings on the last axis. This would result
    in a 48 element-long vector (i.e. 32 element-long token embedding + 12 element-long
    char-based token embedding):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'The rest of the model, we will keep it the same. First define an RNN layer
    and pass the `concat_embedding_out` as an input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Remember that we have set `return_sequences=True`, which means it will produce
    an output at each time step, as opposed to only at the last time step. Next, we
    define the final Dense layer, which has `n_classes` output nodes (i.e. 9) and
    a `softmax` activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the model and compile it like before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: This is our final model. The key difference in this model compared to the previous
    solution is that it used two different embedding types. A standard token-based
    embedding layer and a complex, char-based embedding that was leveraged to generate
    token embeddings using the convolution operation. Now let’s train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Model training and evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Model training is identical to the training we did for the standard RNN model,
    so we will not discuss it further:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: You should get around a ~2% validation accuracy and a ~1% test accuracy boost
    after these modifications.
  prefs: []
  type: TYPE_NORMAL
- en: Other improvements you can make
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we will discuss several improvements you can make to uplift the model performance
    even further.
  prefs: []
  type: TYPE_NORMAL
- en: '**More RNN layers** – Adding more stacked RNN layers. By adding more hidden
    RNN layers, we can allow the model to learn more refined latent representations,
    leading to better performance. An example usage is shown below:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Make the RNN layer bidirectional** – The RNN models we discussed so far are
    uni-directional, i.e. looks at the sequence of text from forward to backward.
    However a different variant known as bi-directional RNNs looks at the sequence
    in both directions, i.e. forward to backward and backward to forward. This leads
    to better language understanding in models and inevitably better performance.
    We will discuss this variant in more detail in the upcoming chapters. An example
    usage is shown below:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Incorporate regularization techniques** – You can leverage L2 regularization
    and dropout techniques to avoid overfitting and improve generalization of the
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use early stopping and learning rate reduction to reduce overfitting** –
    During model training, use early stopping (i.e. training the model only until
    the validation accuracy is improving) and learning rate reduction (i.e. gradually
    reducing the learning rate over the epochs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We recommend experimenting with some of these techniques yourself to see how
    they can maximize the performance of your RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at RNNs, which are different from conventional feed-forward
    neural networks and more powerful in terms of solving temporal tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, we discussed how to arrive at an RNN from a feed-forward neural
    network type structure.
  prefs: []
  type: TYPE_NORMAL
- en: We assumed a sequence of inputs and outputs, and designed a computational graph
    that can represent the sequence of inputs and outputs.
  prefs: []
  type: TYPE_NORMAL
- en: This computational graph resulted in a series of copies of functions that we
    applied to each individual input-output tuple in the sequence. Then, by generalizing
    this model to any given single time step *t* in the sequence, we were able to
    arrive at the basic computational graph of an RNN. We discussed the exact equations
    and update rules used to calculate the hidden state and the output.
  prefs: []
  type: TYPE_NORMAL
- en: Next we discussed how RNNs are trained with data using BPTT. We examined how
    we can arrive at BPTT with standard backpropagation as well as why we can’t use
    standard backpropagation for RNNs. We also discussed two important practical issues
    that arise with BPTT—vanishing gradient and exploding gradient—and how these can
    be solved on the surface level.
  prefs: []
  type: TYPE_NORMAL
- en: Then we moved on to the practical applications of RNNs. We discussed four main
    categories of RNNs. One-to-one architectures are used for tasks such as text generation,
    scene classification, and video frame labeling. Many-to-one architectures are
    used for sentiment analysis, where we process the sentences/phrases word by word
    (compared to processing a full sentence in a single go, as we saw in the previous
    chapter). One-to-many architectures are common in image captioning tasks, where
    we map a single image to an arbitrarily long sentence phrase describing the image.
    Many-to-many architectures are leveraged for machine translation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: We solved the task of NER with RNNs. In NER, the problem is to, given a sequence
    of tokens, predict a label for each token. The label represents an entity (e.g.
    organization, location, person, etc.). For this we used embeddings as well as
    an RNN to process each token while considering the sequence of tokens as a time-series
    input. We also used a text vectorization layer to convert tokens into word IDs.
    A key benefit of the text vectorization layer is that it is built as a part of
    the model, unlike the tokenizer we used before.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at how we can adopt character embeddings and the convolution
    operation to generate token embeddings. We used these new token embeddings along
    with standard word embeddings to improve model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss a more powerful RNN model known as **Long
    Short-Term Memory** (**LSTM**) networks that further reduces the adverse effect
    of the vanishing gradient, and thus produces much better results.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the code files for this book, visit our GitHub page at: [https://packt.link/nlpgithub](https://packt.link/nlpgithub)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 1000 members at: [https://packt.link/nlp](https://packt.link/nlp)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code5143653472357468031.png)'
  prefs: []
  type: TYPE_IMG
