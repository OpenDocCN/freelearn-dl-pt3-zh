<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer912">
<h1 class="chapterNumber">20</h1>
<h1 class="chapterTitle" id="_idParaDest-500">Advanced Convolutional Neural Networks</h1>
<p class="normal">In this chapter, we will see some more advanced uses for <strong class="keyWord">Convolutional Neural Networks</strong> (<strong class="keyWord">CNNs</strong>). We will explore:</p>
<ul>
<li class="bulletList">How CNNs can be applied within the areas of computer vision, video, textual documents, audio, and music</li>
<li class="bulletList">How to use CNNs for text processing</li>
<li class="bulletList">What capsule networks are</li>
<li class="bulletList">Computer vision</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp20"><span class="url">https://packt.link/dltfchp20</span></a>.</p>
</div>
<p class="normal">Let’s start by using CNNs for complex tasks.</p>
<h1 class="heading-1" id="_idParaDest-501">Composing CNNs for complex tasks</h1>
<p class="normal">We have discussed <a id="_idIndexMarker1755"/>CNNs quite extensively in <em class="chapterRef">Chapter 3</em>, <em class="italic">Convolutional Neural Networks</em>, and at this point, you are probably convinced about the effectiveness of the CNN architecture for image classification tasks. What you may find surprising, however, is that the basic CNN architecture can be composed and extended in various ways to solve a variety of more complex tasks. In this section, we will look at the computer vision tasks mentioned in <em class="italic">Figure 20.1</em> and show how they<a id="_idIndexMarker1756"/> can be solved by turning CNNs into larger and more complex architectures.</p>
<figure class="mediaobject"><img alt="" height="393" src="../Images/B18331_20_01.png" width="880"/></figure>
<p class="packt_figref">Figure 20.1: Different Computer Vision Tasks – source: Introduction to Artificial Intelligence and Computer Vision Revolution (https://www.slideshare.net/darian_f/introduction-to-the-artificial-intelligence-and-computer-vision-revolution) </p>
<h2 class="heading-2" id="_idParaDest-502">Classification and localization</h2>
<p class="normal">In the classification and localization task, not only do you have to report the class of object found in the image, but also the <a id="_idIndexMarker1757"/>coordinates of the bounding box where the object appears in the image. This type of task assumes that there is only one instance of the object in an image.</p>
<p class="normal">This can be achieved by attaching a “regression head” in addition to the “classification head” in a typical classification network. Recall that in a classification network, the final output of convolution and pooling operations, called the feature map, is fed into a fully connected network that produces a vector of class probabilities. This fully connected network is called the classification head, and it is tuned using a categorical loss function (<em class="italic">L</em><sub class="italic">c</sub>) such as categorical cross-entropy.</p>
<p class="normal">Similarly, a regression head is another fully connected network that takes the feature map and produces a vector (<em class="italic">x</em>, <em class="italic">y</em>, <em class="italic">w</em>, <em class="italic">h</em>) representing the top left <em class="italic">x</em> and <em class="italic">y</em> coordinates, and the width and height of the bounding box. It is tuned using a continuous loss function (<em class="italic">L</em><sub class="italic">R</sub>) such as mean squared error. The entire network is tuned using a linear combination of the two losses, i.e.,</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_20_001.png" style="height: 1.25em !important;" width="350"/></p>
<p class="normal">Here, <img alt="" height="42" src="../Images/B18331_11_021.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/> is a hyperparameter and can take a value between 0 and 1. Unless the value is determined by some domain<a id="_idIndexMarker1758"/> knowledge about the problem, it can be set to 0.5.</p>
<p class="normal"><em class="italic">Figure 20.2</em> shows a typical classification and localization network architecture:</p>
<figure class="mediaobject"><img alt="" height="340" src="../Images/B18331_20_02.png" width="879"/></figure>
<p class="packt_figref">Figure 20.2: Network architecture for image classification and localization</p>
<p class="normal">As you can see, the only difference with respect to a typical CNN classification network is the additional regression head at the top right-hand side.</p>
<h2 class="heading-2" id="_idParaDest-503">Semantic segmentation</h2>
<p class="normal">Another class of problem that builds on<a id="_idIndexMarker1759"/> the basic classification idea is “semantic segmentation.” Here the aim is to classify every single pixel on the image as belonging to a single class.</p>
<p class="normal">An initial method of implementation could be to build a classifier network for each pixel, where the input is a small neighborhood around each pixel. In practice, this approach is not very performant, so an improvement over this implementation might be to run the image through convolutions that will increase the feature depth, while keeping the image width and height constant. Each pixel then has a feature map that can be sent through a fully connected network that predicts the class of the pixel. However, in practice, this is also quite expensive and is not normally used.</p>
<p class="normal">A third approach is to use a<a id="_idIndexMarker1760"/> CNN encoder-decoder network, where the encoder decreases the width and height of the image but increases its depth (number of features), while the decoder uses transposed convolution operations to increase its size and decrease its depth. Transposed convolution (or upsampling) is the process of going in the opposite direction of a normal convolution. Input to this network is the image and the output is the segmentation map. A popular implementation of this encoder-decoder architecture is the U-Net (a good<a id="_idIndexMarker1761"/> implementation is available at <a href="https://github.com/jakeret/tf_unet"><span class="url">https://github.com/jakeret/tf_unet</span></a>), originally developed for biomedical image segmentation, which has additional skip connections between corresponding layers of the encoder and decoder. </p>
<p class="normal"><em class="italic">Figure 20.3</em> shows the U-Net architecture:</p>
<figure class="mediaobject"><img alt="Chart  Description automatically generated" height="588" src="../Images/B18331_20_03.png" width="882"/></figure>
<p class="packt_figref">Figure 20.3: U-Net architecture</p>
<h2 class="heading-2" id="_idParaDest-504">Object detection</h2>
<p class="normal">The object detection task is similar to<a id="_idIndexMarker1762"/> the classification and localization task. The big difference is that now there are multiple objects in the image, and for each one of them, we need to find the class and the bounding box coordinates. In addition, neither the number of objects nor their size is known in advance. As you can imagine, this is a difficult problem, and a fair amount of research has gone into it.</p>
<p class="normal">A first approach to the problem might be to create many random crops of the input image and, for each crop, apply the classification and localization network we described earlier. However, such an approach is very wasteful in terms of computing and unlikely to be very successful.</p>
<p class="normal">A more practical approach would be to use a tool such as Selective Search (<em class="italic">Selective Search for Object Recognition</em>, by Uijlings et al., <a href="http://www.huppelen.nl/publications/selectiveSearchDraft.pdf"><span class="url">http://www.huppelen.nl/publications/selectiveSearchDraft.pdf</span></a>), which uses traditional computer vision techniques to find areas in the image that might contain objects. These regions are called “region proposals,” and the network to detect them is called <strong class="keyWord">Region-based CNN</strong>, or <strong class="keyWord">R-CNN</strong>. In the<a id="_idIndexMarker1763"/> original R-CNN, the regions were resized and fed into a network to yield image vectors. These vectors were then<a id="_idIndexMarker1764"/> classified with an SVM-based classifier (see <a href="https://en.wikipedia.org/wiki/Support-vector_machine"><span class="url">https://en.wikipedia.org/wiki/Support-vector_machine</span></a>), and the bounding boxes proposed by the external tool were corrected using a linear regression network over the image vectors. An R-CNN network can be represented conceptually as shown in <em class="italic">Figure 20.4</em>:</p>
<figure class="mediaobject"><img alt="Chart, diagram  Description automatically generated" height="419" src="../Images/B18331_20_04.png" width="879"/></figure>
<p class="packt_figref">Figure 20.4: R-CNN network</p>
<p class="normal">The next iteration of the R-CNN network is called the Fast R-CNN. The Fast R-CNN still gets its region proposals from an external tool, but instead of feeding each region proposal through the CNN, the <a id="_idIndexMarker1765"/>entire image is fed through the CNN and the region proposals are projected onto the resulting feature map. Each region of interest is fed<a id="_idIndexMarker1766"/> through a <strong class="keyWord">Region Of Interest</strong> (<strong class="keyWord">ROI</strong>) pooling layer and then to a fully connected network, which produces a feature vector for the ROI.</p>
<p class="normal">ROI pooling is a widely used operation in object detection tasks using CNNs. The ROI pooling layer uses max pooling to convert the features inside any valid region of interest into a small feature map with a fixed spatial extent of <em class="italic">H</em> x <em class="italic">W</em> (where <em class="italic">H</em> and <em class="italic">W</em> are two hyperparameters). The feature vector is then fed into two fully connected networks, one to predict the class of the ROI and the other to correct the bounding box coordinates for the proposal. This is illustrated in <em class="italic">Figure 20.5</em>:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="557" src="../Images/B18331_20_05.png" width="879"/></figure>
<p class="packt_figref">Figure 20.5: Fast R-CNN network architecture</p>
<p class="normal">The Fast R-CNN is about 25x faster than the R-CNN. The next improvement, called the Faster R-CNN (an implementation is at <a href="https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN"><span class="url">https://github.com/tensorpack/tensorpack/tree/master/examples/FasterRCNN</span></a>), removes the external region proposal mechanism and replaces it with a trainable component, called <a id="_idIndexMarker1767"/>the <strong class="keyWord">Region Proposal Network</strong> (<strong class="keyWord">RPN</strong>), within the network itself. The output of this network is<a id="_idIndexMarker1768"/> combined with the feature map and passed in through a similar pipeline to the Fast R-CNN network, as shown in <em class="italic">Figure 20.6</em>. </p>
<p class="normal">The Faster R-CNN network is about 10x faster than the Fast R-CNN network, making it approximately 250x faster than an R-CNN network:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="242" src="../Images/B18331_20_06.png" width="876"/></figure>
<p class="packt_figref">Figure 20.6: Faster R-CNN network architecture</p>
<p class="normal">Another somewhat different<a id="_idIndexMarker1769"/> class of object detection networks are <strong class="keyWord">Single Shot Detectors</strong> (<strong class="keyWord">SSD</strong>) such as <strong class="keyWord">YOLO</strong> (<strong class="keyWord">You Only Look Once</strong>). In these cases, each image is split into a predefined <a id="_idIndexMarker1770"/>number of parts using a grid. In the case of YOLO, a 7 x 7 grid is used, resulting in 49 sub-images. A predetermined set of crops with different aspect ratios are applied to each sub-image. Given <em class="italic">B</em> bounding boxes and <em class="italic">C</em> object classes, the output for each image is a vector of size <img alt="" height="50" src="../Images/B18331_20_003.png" style="height: 1.25em !important; vertical-align: -0.30em !important;" width="304"/>. Each bounding box has a confidence and coordinates (<em class="italic">x</em>, <em class="italic">y</em>, <em class="italic">w</em>, <em class="italic">h</em>), and each grid has prediction probabilities for the different objects detected within them.</p>
<p class="normal">The YOLO network is a CNN, which <a id="_idIndexMarker1771"/>does this transformation. The final predictions and bounding boxes are found by aggregating the findings from this vector. In YOLO, a single convolutional network predicts the bounding boxes and the related class probabilities. YOLO is the faster solution for <a id="_idIndexMarker1772"/>object detection. An implementation is at <a href="https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow"><span class="url">https://www.kaggle.com/aruchomu/yolo-v3-object-detection-in-tensorflow</span></a>.</p>
<h2 class="heading-2" id="_idParaDest-505">Instance segmentation</h2>
<p class="normal">Instance segmentation is similar to <a id="_idIndexMarker1773"/>semantic segmentation – the process of associating each pixel of an image with a class label – with a few important distinctions. First, it needs to distinguish between different instances of the same class in an image. Second, it is not required to label every single pixel in the image. In some respects, instance segmentation is also similar to object detection, except that instead of bounding boxes, we want to find a binary mask that covers each object.</p>
<p class="normal">The second definition leads to the intuition behind the Mask R-CNN network. The Mask R-CNN is a Faster R-CNN with an additional CNN in front of its regression head, which takes as input the bounding box coordinates reported for each ROI and converts it to a binary mask [11]:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="329" src="../Images/B18331_20_07.png" width="879"/></figure>
<p class="packt_figref">Figure 20.7: Mask R-CNN architecture</p>
<p class="normal">In April 2019, Google released <a id="_idIndexMarker1774"/>Mask R-CNN in open source, pretrained with TPUs. This is available at </p>
<p class="normal"><a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb"><span class="url">https://colab.research.google.com/github/tensorflow/tpu/blob/master/models/official/mask_rcnn/mask_rcnn_demo.ipynb</span></a>.</p>
<p class="normal">I suggest playing with the Colab notebook to see what the results are. In <em class="italic">Figure 20.8</em>, we see an example of image segmentation:</p>
<figure class="mediaobject"><img alt="" height="494" src="../Images/B18331_20_08.png" width="740"/></figure>
<p class="packt_figref">Figure 20.8: An example of image segmentation</p>
<p class="normal">Google also released another <a id="_idIndexMarker1775"/>model trained on TPUs called DeepLab, and you can see an image (<em class="italic">Figure 20.9</em>) from the demo. This is available at</p>
<p class="normal"><a href="https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr"><span class="url">https://colab.research.google.com/github/tensorflow/models/blob/master/research/deeplab/deeplab_demo.ipynb#scrollTo=edGukUHXyymr</span></a>:</p>
<figure class="mediaobject"><img alt="" height="174" src="../Images/B18331_20_09.png" width="881"/></figure>
<p class="packt_figref">Figure 20.9: An example of image segmentation</p>
<p class="normal">In this section, we have covered, at a somewhat high level, various network architectures that are popular in computer vision. Note that all of them are composed by the same basic CNN and fully connected architectures. This composability is one of the most powerful features of<a id="_idIndexMarker1776"/> deep learning. Hopefully, this has given you some ideas for networks that could be adapted for your own computer vision use cases.</p>
<h1 class="heading-1" id="_idParaDest-506">Application zoos with tf.Keras and TensorFlow Hub</h1>
<p class="normal">One of the nice things about transfer learning is that it is possible to reuse pretrained networks to save time and resources. There are many collections of ready-to-use networks out there, but the following two are the most used.</p>
<h2 class="heading-2" id="_idParaDest-507">Keras Applications</h2>
<p class="normal">Keras Applications (Keras Applications are <a id="_idIndexMarker1777"/>available at <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/applications</span></a>) includes models for image<a id="_idIndexMarker1778"/> classification with weights trained on ImageNet (Xception, VGG16, VGG19, ResNet, ResNetV2, ResNeXt, InceptionV3, InceptionResNetV2, MobileNet, MobileNetV2, DenseNet, and NASNet). In addition, there are a few other reference implementations from the community for object detection and segmentation, sequence learning, reinforcement learning (see <em class="chapterRef">Chapter 11</em>), and GANs (see <em class="chapterRef">Chapter 9</em>).</p>
<h2 class="heading-2" id="_idParaDest-508">TensorFlow Hub</h2>
<p class="normal">TensorFlow Hub (available at <a href="https://www.tensorflow.org/hub"><span class="url">https://www.tensorflow.org/hub</span></a>) is an alternative collection of<a id="_idIndexMarker1779"/> pretrained models. TensorFlow Hub<a id="_idIndexMarker1780"/> includes modules for text classification, sentence encoding (see <em class="chapterRef">Chapter 4</em>), image classification, feature extraction, image generation with GANs, and video classification. Currently, both Google and DeepMind contribute to TensorFlow Hub.</p>
<p class="normal">Let’s look at an example of using <code class="inlineCode">TF.Hub</code>. In this case, we have a simple image classifier using MobileNetv2:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pylab <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow_hub <span class="hljs-keyword">as</span> hub
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> PIL.Image <span class="hljs-keyword">as</span> Image
classifier_url =<span class="hljs-string">"https://tfhub.dev/google/tf2-preview/mobilenet_v2/classification/2"</span> <span class="hljs-comment">#@param {type:"string"}</span>
IMAGE_SHAPE = (<span class="hljs-number">224</span>, <span class="hljs-number">224</span>)
<span class="hljs-comment"># wrap the hub to work with tf.keras</span>
classifier = tf.keras.Sequential([
    hub.KerasLayer(classifier_url, input_shape=IMAGE_SHAPE+(<span class="hljs-number">3</span>,))
])
grace_hopper = tf.keras.utils.get_file(<span class="hljs-string">'image.jpg'</span>,<span class="hljs-string">'https://storage.googleapis.com/download.tensorflow.org/example_images/grace_hopper.jpg'</span>)
grace_hopper = Image.<span class="hljs-built_in">open</span>(grace_hopper).resize(IMAGE_SHAPE)
grace_hopper = np.array(grace_hopper)/<span class="hljs-number">255.0</span>
result = classifier.predict(grace_hopper[np.newaxis, ...])
predicted_class = np.argmax(result[<span class="hljs-number">0</span>], axis=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span> (predicted_class)
</code></pre>
<p class="normal">Pretty simple indeed. Just remember to use <code class="inlineCode">hub.KerasLayer()</code> for wrapping any Hub layer. In this section, we have discussed how to use TensorFlow Hub.</p>
<p class="normal">Next, we will focus on other CNN architectures.</p>
<h1 class="heading-1" id="_idParaDest-509">Answering questions about images (visual Q&amp;A)</h1>
<p class="normal">One of the nice things about <a id="_idIndexMarker1781"/>neural networks is that different<a id="_idIndexMarker1782"/> media types can be combined together to provide a unified interpretation. For instance, <strong class="keyWord">Visual Question Answering</strong> (<strong class="keyWord">VQA</strong>) combines image recognition and text natural language<a id="_idIndexMarker1783"/> processing. Training can use VQA (VQA is available at <a href="https://visualqa.org/"><span class="url">https://visualqa.org/</span></a>), a dataset containing open-ended questions about images. These questions require an understanding of vision, language, and common knowledge to be answered. The following images are taken from a demo available at <a href="https://visualqa.org/"><span class="url">https://visualqa.org/</span></a>.</p>
<p class="normal">Note the question at the top of the image, and the subsequent answers:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="949" src="../Images/B18331_20_10.png" width="879"/></figure>
<p class="packt_figref">Figure 20.10: Examples of visual question and answers </p>
<p class="normal">If you want to start playing<a id="_idIndexMarker1784"/> with VQA, the first thing is to get appropriate training datasets such as the VQA dataset, the <a id="_idIndexMarker1785"/>CLEVR dataset (available at <a href="https://cs.stanford.edu/people/jcjohns/clevr/"><span class="url">https://cs.stanford.edu/people/jcjohns/clevr/</span></a>), or the <a id="_idIndexMarker1786"/>FigureQA dataset (available at <a href="https://datasets.maluuba.com/FigureQA"><span class="url">https://datasets.maluuba.com/FigureQA</span></a>); alternatively, you can participate in a Kaggle VQA <a id="_idIndexMarker1787"/>challenge (available at <a href="https://www.kaggle.com/c/visual-question-answering"><span class="url">https://www.kaggle.com/c/visual-question-answering</span></a>). Then you can build a model that is the combination of a CNN and an RNN and start experimenting. For instance, a CNN can be something like this code fragment, which takes an image with three channels (224 x 224) as input and produces a feature vector for the image:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> layers, models
<span class="hljs-comment"># IMAGE</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment"># Define CNN for visual processing</span>
cnn_model = models.Sequential()
cnn_model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'</span><span class="hljs-string">relu'</span>, padding=<span class="hljs-string">'same'</span>, 
        input_shape=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>)))
cnn_model.add(layers.Conv2D(<span class="hljs-number">64</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
cnn_model.add(layers.MaxPooling2D(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
cnn_model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>))
cnn_model.add(layers.Conv2D(<span class="hljs-number">128</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
cnn_model.add(layers.MaxPooling2D(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
cnn_model.add(layers.Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>, padding=<span class="hljs-string">'same'</span>))
cnn_model.add(layers.Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'</span><span class="hljs-string">relu'</span>))
cnn_model.add(layers.Conv2D(<span class="hljs-number">256</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), activation=<span class="hljs-string">'relu'</span>))
cnn_model.add(layers.MaxPooling2D(<span class="hljs-number">2</span>, <span class="hljs-number">2</span>))
cnn_model.add(layers.Flatten())
cnn_model.summary()
<span class="hljs-comment">#define the visual_model with proper input</span>
image_input = layers.Input(shape=(<span class="hljs-number">224</span>, <span class="hljs-number">224</span>, <span class="hljs-number">3</span>))
visual_model = cnn_model(image_input)
</code></pre>
<p class="normal">Text can be encoded with<a id="_idIndexMarker1788"/> an RNN; for now, think of it as a black box taking a text fragment (the question) in input and producing a feature vector for the text:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># TEXT</span>
<span class="hljs-comment">#</span>
<span class="hljs-comment">#define the RNN model for text processing</span>
question_input = layers.Input(shape=(<span class="hljs-number">100</span>,), dtype=<span class="hljs-string">'int32'</span>)
emdedding = layers.Embedding(input_dim=<span class="hljs-number">10000</span>, output_dim=<span class="hljs-number">256</span>, 
    input_length=<span class="hljs-number">100</span>)(question_input)
encoded_question = layers.LSTM(<span class="hljs-number">256</span>)(emdedding)
</code></pre>
<p class="normal">Then the two feature vectors (one for the image, and one for the text) are combined into one joint vector, which is provided as input to a dense network to produce the combined network:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># combine the encoded question and visual model</span>
merged = layers.concatenate([encoded_question, visual_model])
<span class="hljs-comment">#attach a dense network at the end</span>
output = layers.Dense(<span class="hljs-number">1000</span>, activation=<span class="hljs-string">'softmax'</span>)(merged)
<span class="hljs-comment">#get the combined model</span>
vqa_model = models.Model(inputs=[image_input, question_input], outputs=output)
vqa_model.summary()
</code></pre>
<p class="normal">For instance, if we have a set of labeled images, then we can learn what the best questions and answers are for describing an image. The number of options is enormous! If you want to know more, I suggest that you investigate Maluuba, a start-up providing the FigureQA dataset with 100,000 figure images and 1,327,368 question-answer pairs in the training set. Maluuba has<a id="_idIndexMarker1789"/> been recently acquired by Microsoft, and the lab is advised by Yoshua Bengio, one of the fathers of deep learning.</p>
<p class="normal">In this section, we have discussed how to implement visual Q&amp;A. The next section is about style transfer, a deep learning technique used for training neural networks to create art.</p>
<h1 class="heading-1" id="_idParaDest-510">Creating a DeepDream network</h1>
<p class="normal">Another interesting application <a id="_idIndexMarker1790"/>of CNNs is DeepDream, a computer vision program created by Google [8] that uses a CNN to find and enhance patterns in images. The result is a dream-like hallucinogenic effect. Similar to the previous example, we are going to use a pretrained network to extract features. However, in this case, we want to “enhance” patterns in images, meaning that we need to maximize some functions. This tells us that we need to use a gradient ascent and not a descent. First, let’s see an example from Google gallery (available at <a href="https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb"><span class="url">https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/generative/deepdream.ipynb</span></a>) where the classic Seattle landscape is “incepted” with hallucinogenic dreams such as birds, cards, and strange flying objects. </p>
<p class="normal">Google released the DeepDream code as open source (available at <a href="https://github.com/google/deepdream"><span class="url">https://github.com/google/deepdream</span></a>), but we will use a simplified example made by a random forest (available at <a href="https://www.tensorflow.org/tutorials/generative/deepdream"><span class="url">https://www.tensorflow.org/tutorials/generative/deepdream</span></a>):</p>
<figure class="mediaobject"><img alt="" height="509" src="../Images/B18331_20_11.png" width="764"/></figure>
<p class="packt_figref">Figure 20.11: DeepDreaming Seattle</p>
<p class="normal">Let’s start with some image<a id="_idIndexMarker1791"/> preprocessing:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Download an image and read it into a NumPy array, </span>
<span class="hljs-keyword">def</span> <span class="hljs-title">download</span>(<span class="hljs-params">url</span>):
  name = url.split(<span class="hljs-string">"/"</span>)[-<span class="hljs-number">1</span>]
  image_path = tf.keras.utils.get_file(name, origin=url)
  img = image.load_img(image_path)
  <span class="hljs-keyword">return</span> image.img_to_array(img)
<span class="hljs-comment"># Scale pixels to between (-1.0 and 1.0)</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">preprocess</span>(<span class="hljs-params">img</span>):
  <span class="hljs-keyword">return</span> (img / <span class="hljs-number">127.5</span>) - <span class="hljs-number">1</span>
  
<span class="hljs-comment"># Undo the preprocessing above</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">deprocess</span>(<span class="hljs-params">img</span>):
  img = img.copy()
  img /= <span class="hljs-number">2.</span>
  img += <span class="hljs-number">0.5</span>
  img *= <span class="hljs-number">255.</span>
  <span class="hljs-keyword">return</span> np.clip(img, <span class="hljs-number">0</span>, <span class="hljs-number">255</span>).astype(<span class="hljs-string">'uint8'</span>)
<span class="hljs-comment"># Display an image</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">show</span>(<span class="hljs-params">img</span>):
  plt.figure(figsize=(<span class="hljs-number">12</span>,<span class="hljs-number">12</span>))
  plt.grid(<span class="hljs-literal">False</span>)
  plt.axis(<span class="hljs-string">'off'</span>)
  plt.imshow(img)
<span class="hljs-comment"># https://commons.wikimedia.org/wiki/File:Flickr_-_Nicholas_T_-_Big_Sky_(1).jpg</span>
url = <span class="hljs-string">'https://upload.wikimedia.org/wikipedia/commons/thumb/d/d0/Flickr_-_Nicholas_T_-_Big_Sky_%281%29.jpg/747px-Flickr_-_Nicholas_T_-_Big_Sky_%281%29.jpg'</span>
img = preprocess(download(url))
show(deprocess(img))
</code></pre>
<p class="normal">Now let’s use the Inception pretrained network to extract features. We use several layers, and the goal is to<a id="_idIndexMarker1792"/> maximize their activations. The <code class="inlineCode">tf.keras</code> functional API is our friend here:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># We'll maximize the activations of these layers</span>
names = [<span class="hljs-string">'mixed2'</span>, <span class="hljs-string">'mixed3'</span>, <span class="hljs-string">'mixed4'</span>, <span class="hljs-string">'mixed5'</span>]
layers = [inception_v3.get_layer(name).output <span class="hljs-keyword">for</span> name <span class="hljs-keyword">in</span> names]
<span class="hljs-comment"># Create our feature extraction model</span>
feat_extraction_model = tf.keras.Model(inputs=inception_v3.<span class="hljs-built_in">input</span>, outputs=layers)
<span class="hljs-keyword">def</span> <span class="hljs-title">forward</span>(<span class="hljs-params">img</span>):
  
  <span class="hljs-comment"># Create a batch</span>
  img_batch = tf.expand_dims(img, axis=<span class="hljs-number">0</span>)
  
  <span class="hljs-comment"># Forward the image through Inception, extract activations</span>
  <span class="hljs-comment"># for the layers we selected above</span>
  <span class="hljs-keyword">return</span> feat_extraction_model(img_batch)
</code></pre>
<p class="normal">The loss function is the mean of all the activation layers considered, normalized by the number of units in the layer itself:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">calc_loss</span>(<span class="hljs-params">layer_activations</span>):
  
  total_loss = <span class="hljs-number">0</span>
  
  <span class="hljs-keyword">for</span> act <span class="hljs-keyword">in</span> layer_activations:
    
    <span class="hljs-comment"># In gradient ascent, we'll want to maximize this value</span>
    <span class="hljs-comment"># so our image increasingly "excites" the layer</span>
    loss = tf.math.reduce_mean(act)
    <span class="hljs-comment"># Normalize by the number of units in the layer</span>
    loss /= np.prod(act.shape)
    total_loss += loss
  <span class="hljs-keyword">return</span> total_loss
</code></pre>
<p class="normal">Now let’s run the gradient <a id="_idIndexMarker1793"/>ascent:</p>
<pre class="programlisting code"><code class="hljs-code">img = tf.Variable(img)
steps = <span class="hljs-number">400</span>
<span class="hljs-keyword">for</span> step <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(steps):
  
  <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
    activations = forward(img)
    loss = calc_loss(activations)
    
  gradients = tape.gradient(loss, img)
  <span class="hljs-comment"># Normalize the gradients</span>
  gradients /= gradients.numpy().std() + <span class="hljs-number">1e-8</span> 
  
  <span class="hljs-comment"># Update our image by directly adding the gradients</span>
  img.assign_add(gradients)
  
  <span class="hljs-keyword">if</span> step % <span class="hljs-number">50</span> == <span class="hljs-number">0</span>:
    clear_output()
    <span class="hljs-built_in">print</span> (<span class="hljs-string">"Step %d, loss %f"</span> % (step, loss))
    show(deprocess(img.numpy()))
    plt.show()
<span class="hljs-comment"># Let's see the result</span>
clear_output()
show(deprocess(img.numpy()))
</code></pre>
<p class="normal">This transforms the image <a id="_idIndexMarker1794"/>on the left into the psychedelic image on the right:</p>
<figure class="mediaobject"><img alt="" height="272" src="../Images/B18331_20_12.png" width="678"/></figure>
<p class="packt_figref">Figure 20.12: DeepDreaming of a green field with clouds</p>
<h1 class="heading-1" id="_idParaDest-511">Inspecting what a network has learned</h1>
<p class="normal">A particularly interesting<a id="_idIndexMarker1795"/> research effort is being devoted to understand what neural networks are actually learning in order to be able to recognize images so well. This is called neural network “interpretability.” Activation atlases is a promising recent technique that aims to show the feature visualizations of averaged activation functions. In this way, activation atlases produce a global map seen through the eyes of the network. Let’s look at a demo available at <a href="https://distill.pub/2019/activation-atlas/"><span class="url">https://distill.pub/2019/activation-atlas/</span></a>:</p>
<figure class="mediaobject"><img alt="" height="391" src="../Images/B18331_20_13.png" width="880"/></figure>
<p class="packt_figref">Figure 20.13: Examples of inspections</p>
<p class="normal">In this image, an InceptionV1<a id="_idIndexMarker1796"/> network used for vision classification reveals many fully realized features, such as electronics, screens, a Polaroid camera, buildings, food, animal ears, plants, and watery backgrounds. Note that grid cells are labeled with the classification they give the most support for. Grid cells are also sized according to the number of activations that are averaged within. This representation is very powerful because it allows us to inspect the different layers of a network and how the activation functions fire in response to the input.</p>
<p class="normal">In this section, we have seen many techniques to process images with CNNs. Next, we’ll move on to video processing.</p>
<h1 class="heading-1" id="_idParaDest-512">Video</h1>
<p class="normal">In this section, we are<a id="_idIndexMarker1797"/> going to discuss how to use CNNs with videos and the different techniques that we can use.</p>
<h2 class="heading-2" id="_idParaDest-513">Classifying videos with pretrained nets in six different ways</h2>
<p class="normal">Classifying videos is an area of <a id="_idIndexMarker1798"/>active research because of the large amount of data needed for processing this type of media. Memory requirements are frequently reaching the limits of modern GPUs and a distributed form of training on multiple machines might be required. Researchers are currently exploring different directions of investigation, with increasing levels of complexity from the first approach to the sixth, as described below. Let’s review them:</p>
<ul>
<li class="bulletList">The <strong class="keyWord">first approach</strong> consists of classifying one video frame at a time by considering each one of them as a separate image processed with a 2D CNN. This approach simply reduces the video classification problem to an image classification problem. Each video frame “emits” a classification output, and the video is classified by taking into account the more frequently chosen category for each frame.</li>
<li class="bulletList">The <strong class="keyWord">second approach</strong> consists of creating one single network where a 2D CNN is combined with an RNN (see <em class="chapterRef">Chapter 9</em>, <em class="italic">Generative Models</em>). The idea is that the CNN will take into account the image components and the RNN will take into account the sequence information for each video. This type of network can be very difficult to train because of the very high number of parameters to optimize.</li>
<li class="bulletList">The <strong class="keyWord">third approach</strong> is to use a 3D ConvNet, where 3D ConvNets are an extension of 2D ConvNets operating on a 3D tensor (time, image width, and image height). This approach is another natural extension of image classification. Again, 3D ConvNets can be hard to train.</li>
<li class="bulletList">The <strong class="keyWord">fourth approach</strong> is based on a clever idea: instead of using CNNs directly for classification, they can be used for storing offline features for each frame in the video. The idea is that feature extraction can be made very efficient with transfer<a id="_idIndexMarker1799"/> learning, as shown in a previous recipe. After all the features are extracted, they can be passed as a set of inputs into an RNN, which will learn sequences across multiple frames and emit the final classification.</li>
<li class="bulletList">The <strong class="keyWord">fifth approach</strong> is a simple variant of the fourth, where the final layer is an MLP instead of an RNN. In certain situations, this approach can be simpler and less expensive in terms of computational requirements.</li>
<li class="bulletList">The <strong class="keyWord">sixth approach</strong> is a variant of the fourth, where the phase of feature extraction is realized with a 3D CNN that extracts spatial and visual features. These features are then passed into either an RNN or an MLP.</li>
</ul>
<p class="normal">Deciding upon the best approach is strictly dependent on your specific application, and there is no definitive answer. The first three approaches are generally more computationally expensive and less clever, while the last three approaches are less expensive, and they frequently achieve better performance.</p>
<p class="normal">So far, we have explored how CNNs can be used for image and video applications. In the next section, we will apply these ideas within a text-based context.</p>
<h1 class="heading-1" id="_idParaDest-514">Text documents</h1>
<p class="normal">What do text and images have in common? At first glance, very little. However, if we represent a sentence or a <a id="_idIndexMarker1800"/>document as a matrix, then this matrix is not much different from an image matrix where each cell is a pixel. So, the next question is, how can we represent a piece of text as a matrix? </p>
<p class="normal">Well, it is pretty simple: each row of<a id="_idIndexMarker1801"/> a matrix is a vector that represents a basic unit for the text. Of course, now we need to define what a basic unit is. A simple choice could be to say that the basic unit is a character. Another choice would be to say that a basic unit is a word; yet another choice is to aggregate similar words together and then denote each aggregation (sometimes called cluster or embedding) with a representative symbol.</p>
<p class="normal">Note that regardless of the specific choice adopted for our basic units, we need to have a 1:1 mapping from basic units into integer IDs so that the text can be seen as a matrix. For instance, if we have a document with 10 lines of text and each line is a 100-dimensional embedding, then we will represent our text with a matrix of 10 x 100. In this very particular “image,” a “pixel” is turned on if that sentence, <em class="italic">X</em>, contains the embedding, represented by position <em class="italic">Y</em>. You might also notice that a text is not really a matrix but more a vector because two words located in adjacent rows of text have very little in common. Indeed, this is a major difference when compared with images, where two pixels located in adjacent columns are likely to have some degree of correlation.</p>
<p class="normal">Now you might wonder: <em class="italic">I understand that we represent the text as a vector but, in doing so, we lose the position of the words. This position should be important, shouldn’t it?</em> Well, it turns out that in many real applications, knowing whether a sentence contains a particular basic unit (a char, a word, or an aggregate) or not is pretty useful information even if we don’t keep track of where exactly in the sentence this basic unit is located.</p>
<p class="normal">For instance, CNNs achieve pretty good<a id="_idIndexMarker1802"/> results for <strong class="keyWord">sentiment analysis</strong>, where we need to understand if a piece of text has a positive or a negative sentiment; for <strong class="keyWord">spam detection</strong>, where we need to <a id="_idIndexMarker1803"/>understand if a piece of text is useful information or spam; and <a id="_idIndexMarker1804"/>for <strong class="keyWord">topic categorization</strong>, where we need to understand what a piece of text is all about. However, CNNs are not well suited for a <strong class="keyWord">Part of Speech</strong> (<strong class="keyWord">POS</strong>) analysis, where the goal is to understand what the logical role of<a id="_idIndexMarker1805"/> every single word is (for example, a verb, an adverb, a subject, and so on). CNNs are also not well suited for <strong class="keyWord">entity extraction</strong>, where we <a id="_idIndexMarker1806"/>need to understand where relevant entities are located in sentences. </p>
<p class="normal">Indeed, it turns out that a position is pretty useful information for the last two use cases. 1D ConvNets are very similar to 2D ConvNets. However, the former operates on a single vector, while the latter operates on matrices.</p>
<h2 class="heading-2" id="_idParaDest-515">Using a CNN for sentiment analysis</h2>
<p class="normal">Let’s have a look<a id="_idIndexMarker1807"/> at the code. First of all, we load the dataset with <code class="inlineCode">tensorflow_datasets</code>. In this case we use IMDB, a collection of movie reviews:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models, preprocessing
<span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
max_len = <span class="hljs-number">200</span>
n_words = <span class="hljs-number">10000</span>
dim_embedding = <span class="hljs-number">256</span>
EPOCHS = <span class="hljs-number">20</span>
BATCH_SIZE =<span class="hljs-number">500</span>
<span class="hljs-keyword">def</span> <span class="hljs-title">load_data</span>():
    <span class="hljs-comment">#load data</span>
    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)
    <span class="hljs-comment"># Pad sequences with max_len</span>
    X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)
    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)
    <span class="hljs-keyword">return</span> (X_train, y_train), (X_test, y_test)
</code></pre>
<p class="normal">Then we build a suitable CNN model. We use embeddings (see <em class="chapterRef">Chapter 4</em>, <em class="italic">Word Embeddings</em>) to map the sparse vocabulary typically observed in documents into a dense feature space of dimensions <code class="inlineCode">dim_embedding</code>. Then we use <code class="inlineCode">Conv1D</code>, followed by a <code class="inlineCode">GlobalMaxPooling1D</code> for averaging, and two <code class="inlineCode">Dense</code> layers – the last one has only one neuron firing binary choices (positive or negative reviews):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">build_model</span>():
    model = models.Sequential()
    <span class="hljs-comment">#Input - Embedding Layer</span>
    <span class="hljs-comment"># the model will take as input an integer matrix of size (batch, input_length)</span>
    <span class="hljs-comment"># the model will output dimension (input_length, dim_embedding)</span>
    <span class="hljs-comment"># the largest integer in the input should be no larger</span>
    <span class="hljs-comment"># than n_words (vocabulary size).</span>
    model.add(layers.Embedding(n_words,
        dim_embedding, input_length=max_len))
    model.add(layers.Dropout(<span class="hljs-number">0.3</span>))
    model.add(layers.Conv1D(<span class="hljs-number">256</span>, <span class="hljs-number">3</span>, padding=<span class="hljs-string">'valid'</span>, 
        activation=<span class="hljs-string">'relu'</span>))
    <span class="hljs-comment">#takes the maximum value of either feature vector from each of the n_words features</span>
    model.add(layers.GlobalMaxPooling1D())
    model.add(layers.Dense(<span class="hljs-number">128</span>, activation=<span class="hljs-string">'relu'</span>))
    model.add(layers.Dropout(<span class="hljs-number">0.5</span>))
    model.add(layers.Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-string">'sigmoid'</span>))
    <span class="hljs-keyword">return</span> model
(X_train, y_train), (X_test, y_test) = load_data()
model=build_model()
model.summary()
</code></pre>
<p class="normal">The model has more <a id="_idIndexMarker1808"/>than 2,700,000 parameters, and it is summarized as follows:</p>
<pre class="programlisting con"><code class="hljs-con">_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 embedding (Embedding)       (None, 200, 256)          2560000   
                                                                 
 dropout (Dropout)           (None, 200, 256)          0         
                                                                 
 conv1d (Conv1D)             (None, 198, 256)          196864    
                                                                 
 global_max_pooling1d (Globa  (None, 256)              0         
 lMaxPooling1D)                                                  
                                                                 
 dense (Dense)               (None, 128)               32896     
                                                                 
 dropout_1 (Dropout)         (None, 128)               0         
                                                                 
 dense_1 (Dense)             (None, 1)                 129       
                                                                 
=================================================================
Total params: 2,789,889
Trainable params: 2,789,889
Non-trainable params: 0
</code></pre>
<p class="normal">Then we compile and <a id="_idIndexMarker1809"/>fit the model with the Adam optimizer and binary cross-entropy loss:</p>
<pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(optimizer = <span class="hljs-string">"adam"</span>, loss = <span class="hljs-string">"binary_crossentropy"</span>,
  metrics = [<span class="hljs-string">"accuracy"</span>]
)
score = model.fit(X_train, y_train,
  epochs= EPOCHS,
  batch_size = BATCH_SIZE,
  validation_data = (X_test, y_test)
)
score = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"\nTest score:"</span>, score[<span class="hljs-number">0</span>])
<span class="hljs-built_in">print</span>(<span class="hljs-string">'Test accuracy:'</span>, score[<span class="hljs-number">1</span>])
</code></pre>
<p class="normal">The final accuracy is 88.21%, showing that it is possible to successfully use CNNs for textual processing:</p>
<pre class="programlisting con"><code class="hljs-con">Epoch 19/20
25000/25000 [==============================] - 135s 5ms/sample - loss: 7.5276e-04 - accuracy: 1.0000 - val_loss: 0.5753 - val_accuracy: 0.8818
Epoch 20/20
25000/25000 [==============================] - 129s 5ms/sample - loss: 6.7755e-04 - accuracy: 0.9999 - val_loss: 0.5802 - val_accuracy: 0.8821
25000/25000 [==============================] - 23s 916us/sample - loss: 0.5802 - accuracy: 0.8821
Test score: 0.5801781857013703
Test accuracy: 0.88212
</code></pre>
<p class="normal">Note that many other non-image applications can also be converted to an image and classified using CNNs (see, for instance, <a href="https://becominghuman.ai/sound-classification-using-images-68d4770df426"><span class="url">https://becominghuman.ai/sound-classification-using-images-68d4770df426</span></a>). </p>
<h1 class="heading-1" id="_idParaDest-516">Audio and music</h1>
<p class="normal">We have used CNNs for images, videos, and texts. Now let’s have a look at how variants of CNNs can be used for audio.</p>
<p class="normal">So, you might <a id="_idIndexMarker1810"/>wonder why learning to synthesize audio is so difficult. Well, each digital sound we hear is based on 16,000 samples per second (sometimes 48K or more), and building a predictive model where we learn to reproduce a sample based on all the previous ones is a very difficult challenge.</p>
<h2 class="heading-2" id="_idParaDest-517">Dilated ConvNets, WaveNet, and NSynth</h2>
<p class="normal">WaveNet is a deep generative model for producing raw audio waveforms. This breakthrough technology was introduced (available at <a href="https://deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/"><span class="url">https://deepmind.com/blog/wavenet-a-generative-model-for-raw-audio/</span></a>) by Google DeepMind for teaching computers how to speak. The results are truly impressive, and online you can find examples of synthetic voices where the computer learns how to talk with the voice of celebrities such as Matt Damon. There are experiments showing that WaveNet improved the current state-of-the-art <strong class="keyWord">Text-to-Speech</strong> (<strong class="keyWord">TTS</strong>) systems, reducing the difference <a id="_idIndexMarker1811"/>with respect to human voices by 50% for both US English and Mandarin Chinese. The metric used for comparison is called <strong class="keyWord">Mean Opinion Score</strong> (<strong class="keyWord">MOS</strong>), a subjective<a id="_idIndexMarker1812"/> paired comparison test. In the MOS tests, after listening to each sound stimulus, the subjects were asked to rate the naturalness of the stimulus on a five-point scale from “Bad” (1) to “Excellent” (5).</p>
<p class="normal">What is even cooler is that DeepMind demonstrated that WaveNet can be also used to teach computers how to generate the sound of musical instruments such as piano music.</p>
<p class="normal">Now some definitions. TTS systems are typically divided into two different classes: concatenative and parametric.</p>
<p class="normal">Concatenative TTS is where single<a id="_idIndexMarker1813"/> speech voice fragments are first memorized and then recombined when the voice has to be reproduced. However, this approach does not scale because it is possible to reproduce only the memorized voice fragments, and it is not possible to reproduce new speakers or different types of audio without memorizing the fragments from the beginning.</p>
<p class="normal">Parametric TTS is where a<a id="_idIndexMarker1814"/> model is created to store all the characteristic features of the audio to be synthesized. Before WaveNet, the audio generated with parametric TTS was less natural than concatenative TTS. WaveNet enabled significant improvement by modeling directly the production of audio sounds, instead of using intermediate signal processing algorithms as in the past.</p>
<p class="normal">In principle, WaveNet can be seen as a stack of 1D convolutional layers with a constant stride of one and with no pooling layers. Note that the input and the output have by construction the same dimension, so ConvNets are well suited to modeling sequential data such as audio sounds. However, it has been shown that in order to reach a large size for the receptive field in the output neuron, it is necessary to either use a massive number of large filters or increase the network depth prohibitively. Remember that the receptive field of a neuron in a layer is the cross-section of the previous layer from which neurons provide inputs. For this reason, pure ConvNets are not so effective in learning how to synthesize audio.</p>
<p class="normal">The key intuition behind WaveNet is the <a id="_idIndexMarker1815"/>so-called <strong class="keyWord">Dilated Causal Convolutions</strong> [5] (sometimes called <strong class="keyWord">atrous convolution</strong>), which <a id="_idIndexMarker1816"/>simply means that some input values are skipped when the filter of a convolutional layer is applied. “Atrous” is a “bastardization” of the French expression “à trous,” meaning “with holes.” So an atrous convolution is a convolution with holes. As an example, in one dimension, a filter <em class="italic">w</em> of size 3 with a dilation of 1 would compute the following sum: <em class="italic">w</em>[0] <em class="italic">x</em>[0] + <em class="italic">w</em>[1] <em class="italic">x</em>[2] + <em class="italic">w</em>[3] <em class="italic">x</em>[4].</p>
<p class="normal">In short, in D-dilated convolution, usually <a id="_idIndexMarker1817"/>the stride is 1, but nothing prevents you from using other strides. An example is given in <em class="italic">Figure 20.14</em> with increased dilatation (hole) sizes = 0, 1, 2:</p>
<figure class="mediaobject"><img alt="" height="220" src="../Images/B18331_20_14.png" width="771"/></figure>
<p class="packt_figref">Figure 20.14: Dilatation with increased sizes</p>
<p class="normal">Thanks to this simple idea of introducing <em class="italic">holes</em>, it is possible to stack multiple dilated convolutional layers with exponentially increasing filters and learn long-range input dependencies without having an excessively deep network.</p>
<p class="normal">A WaveNet is therefore a ConvNet <a id="_idIndexMarker1818"/>where the convolutional layers have various dilation factors, allowing the receptive field to grow exponentially with depth and therefore efficiently cover thousands of audio timesteps.</p>
<p class="normal">When we train, the inputs are sounds recorded from human speakers. The waveforms are quantized to a fixed integer range. A WaveNet defines an initial convolutional layer accessing only the current and previous input. Then, there<a id="_idIndexMarker1819"/> is a stack of dilated ConvNet layers, still accessing only current and previous inputs. At the end, there is a series of dense layers combining previous results, followed by a softmax activation function for categorical outputs.</p>
<p class="normal">At each step, a value is predicted from the network and fed back into the input. At the same time, a new prediction for the next step is computed. The loss function is the cross-entropy between the output for the current step and the input at the next step. <em class="italic">Figure 20.15</em> shows the visualization of a WaveNet stack and its receptive field as introduced in Aaron van den Oord [9]. Note that generation can be slow because the waveform has to be synthesized in a sequential fashion, as <em class="italic">x</em><sub class="italic">t</sub> must be sampled first in order to obtain <img alt="" height="50" src="../Images/B18331_20_004.png" style="height: 1.25em !important; vertical-align: -0.30em !important;" width="54"/> where <em class="italic">x</em> is the input: </p>
<figure class="mediaobject"><img alt="A picture containing diagram  Description automatically generated" height="273" src="../Images/B18331_20_15.png" width="826"/></figure>
<p class="packt_figref">Figure 20.15: WaveNet internal connections</p>
<p class="normal">A method for performing a sampling in parallel has been proposed in Parallel WaveNet [10], which achieves a three orders-of-magnitude speedup. This uses two networks as a WaveNet teacher network, which is slow but ensures a correct result, and a WaveNet student network, which tries to mimic the behavior of the teacher; this can prove to be less accurate but is faster. This approach is similar to the one used for GANs (see <em class="chapterRef">Chapter 9</em>, <em class="italic">Generative Models</em>) but the student does not try to fool the teacher, as typically happens in GANs. In fact, the model is not just quicker but also of higher fidelity, capable of creating waveforms with 24,000 samples per second:</p>
<figure class="mediaobject"><img alt="Diagram, shape  Description automatically generated" height="461" src="../Images/B18331_20_16.png" width="774"/></figure>
<p class="packt_figref">Figure 20.16: Examples of WaveNet Student and Teacher</p>
<p class="normal">This model has been deployed in production at Google, and is currently being used to serve Google Assistant queries in real time to millions of users. At the annual I/O developer conference in May 2018, it was announced that new Google Assistant voices were available thanks to WaveNet.</p>
<p class="normal">Two implementations of WaveNet models for TensorFlow are <a id="_idIndexMarker1820"/>currently available. One is the original implementation of DeepMind’s WaveNet, and the other is called Magenta NSynth. The <a id="_idIndexMarker1821"/>original WaveNet <a id="_idIndexMarker1822"/>version is available at <a href="https://github.com/ibab/tensorflow-wavenet"><span class="url">https://github.com/ibab/tensorflow-wavenet</span></a>. NSynth is an<a id="_idIndexMarker1823"/> evolution of WaveNet recently released by the Google Brain group, which, instead of being causal, aims at seeing the entire context of the input chunk. Magenta is available at <a href="https://magenta.tensorflow.org/nsynth"><span class="url">https://magenta.tensorflow.org/nsynth</span></a>. </p>
<p class="normal">The neural network<a id="_idIndexMarker1824"/> is truly complex, as depicted in the image below, but for the sake of this introductory discussion, it is sufficient to know that the network learns how to reproduce its input by using an approach based on reducing the error during the encoding/decoding phases:</p>
<figure class="mediaobject"><img alt="NSynth_blog_figs_WaveNetAE_diagram.png" height="331" src="../Images/B18331_20_17.png" width="879"/></figure>
<p class="packt_figref">Figure 20.17: Magenta internal architecture</p>
<p class="normal">If you are interested in understanding more, I would suggest having a look at the online Colab notebook where you can play with <a id="_idIndexMarker1825"/>models generated with NSynth. NSynth Colab is available at <a href="https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb"><span class="url">https://colab.research.google.com/notebooks/magenta/nsynth/nsynth.ipynb</span></a>.</p>
<p class="normal">MuseNet is a very recent and impressive cool audio generation tool developed by OpenAI. MuseNet uses a sparse transformer to train a 72-layer network with 24 attention heads. MuseNet is<a id="_idIndexMarker1826"/> available at <a href="https://openai.com/blog/musenet/"><span class="url">https://openai.com/blog/musenet/</span></a>. Transformers, discussed in <em class="chapterRef">Chapter 6</em>, are very good at predicting what comes next in a sequence – whether text, images, or sound.</p>
<p class="normal">In transformers, every output element is connected to every input element, and the weightings between them are dynamically calculated according to a process called attention. MuseNet can produce up to 4-minute musical compositions with 10 different instruments, and can combine styles from country, to Mozart, to the Beatles. For instance, I generated a remake of Beethoven’s “Für Elise” in the style of Lady Gaga with piano, drums, guitar, and bass. You can try this for yourself at the link provided under the section <strong class="screenText">Try MuseNet</strong>:</p>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="519" src="../Images/B18331_20_18.png" width="821"/></figure>
<p class="packt_figref">Figure 20.18: An example of using MuseNet</p>
<h1 class="heading-1" id="_idParaDest-518">A summary of convolution operations</h1>
<p class="normal">In this section, we present a summary of different convolution operations. A convolutional layer has <em class="italic">I</em> input channels<a id="_idIndexMarker1827"/> and produces <em class="italic">O</em> output channels. <em class="italic">I</em> x <em class="italic">O</em> x <em class="italic">K</em> parameters are used, where <em class="italic">K </em>is the number of values in the kernel. </p>
<h2 class="heading-2" id="_idParaDest-519">Basic CNNs</h2>
<p class="normal">Let’s remind ourselves briefly what a CNN is. CNNs take in an input image (two dimensions), text (two dimensions), or<a id="_idIndexMarker1828"/> video (three dimensions) and apply multiple filters to the input. Each filter is like a flashlight sliding across the areas of the input, and the areas that it is shining over are called the receptive field. Each filter is a tensor of the same depth of the input (for instance, if the image has a depth of three, then the filter must also have a depth of three).</p>
<p class="normal">When the filter is sliding, or convolving, around the input image, the values in the filter are multiplied by the values of the input. The multiplications are then summarized into one single value. This process is repeated for each location, producing an activation map (a.k.a. a feature map). Of course, it is possible to use multiple filters where each filter will act as a feature identifier. For instance, for images, the filter can identify edges, colors, lines, and curves. The key<a id="_idIndexMarker1829"/> intuition is to treat the filter values as weights and fine-tune them during training via backpropagation.</p>
<p class="normal">A convolution layer can be configured by using the following config parameters:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Kernel size</strong>: This is the field of<a id="_idIndexMarker1830"/> view of the convolution.</li>
<li class="bulletList"><strong class="keyWord">Stride</strong>: This is the step size<a id="_idIndexMarker1831"/> of the kernel when we traverse the image.</li>
<li class="bulletList"><strong class="keyWord">Padding</strong>: Defines how the border<a id="_idIndexMarker1832"/> of our sample is handled.</li>
</ul>
<h2 class="heading-2" id="_idParaDest-520">Dilated convolution</h2>
<p class="normal">Dilated convolutions (or atrous convolutions) introduce<a id="_idIndexMarker1833"/> another config parameter:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Dilation rate</strong>: This is the spacing<a id="_idIndexMarker1834"/> between the values in a kernel.</li>
</ul>
<p class="normal">Dilated convolutions are used in many contexts including audio processing with WaveNet.</p>
<h2 class="heading-2" id="_idParaDest-521">Transposed convolution</h2>
<p class="normal">Transposed convolution is a<a id="_idIndexMarker1835"/> transformation going in the opposite direction of a normal convolution. For instance, this can be <a id="_idIndexMarker1836"/>useful to project feature maps into a higher dimensional space or for building convolutional autoencoders (see <em class="chapterRef">Chapter 8</em>, <em class="italic">Autoencoders</em>). One way to think about transposed convolution is to compute the output shape of a normal CNN for a given input shape first. Then we invert input and output shapes with the transposed convolution. TensorFlow 2.0 supports transposed convolutions with Conv2DTranspose layers, which can be used, for instance, in GANs (see <em class="chapterRef">Chapter 9</em>, <em class="italic">Generative Models</em>) for generating images.</p>
<h2 class="heading-2" id="_idParaDest-522">Separable convolution</h2>
<p class="normal">Separable convolution aims at separating<a id="_idIndexMarker1837"/> the kernel in multiple steps. Let the convolution be <em class="italic">y</em> = <em class="italic">conv</em>(<em class="italic">x</em>, <em class="italic">k</em>) where <em class="italic">y</em> is the output, <em class="italic">x</em> is the input, and <em class="italic">k</em> is the<a id="_idIndexMarker1838"/> kernel. Let’s assume the kernel is separable, <em class="italic">k</em> = <em class="italic">k</em>1.<em class="italic">k</em>2 where . is the dot product – in this case, instead of doing a 2-dimension convolution with <em class="italic">k</em>, we can get to the same result by doing two 1-dimension convolutions with <em class="italic">k</em>1 and <em class="italic">k</em>2. Separable convolutions are frequently used to save on computation resources.</p>
<h2 class="heading-2" id="_idParaDest-523">Depthwise convolution</h2>
<p class="normal">Let’s consider an image with multiple channels. In the normal 2D convolution, the filter is as deep as the input, and it allows us to<a id="_idIndexMarker1839"/> mix channels for generating each element of the output. In depthwise convolutions, each channel is kept<a id="_idIndexMarker1840"/> separate, the filter is split into channels, each convolution is applied separately, and the results are stacked back together into one tensor.</p>
<h2 class="heading-2" id="_idParaDest-524">Depthwise separable convolution</h2>
<p class="normal">This convolution should not be <a id="_idIndexMarker1841"/>confused with the separable convolution. After completing the depthwise convolution, an additional step is performed: a 1x1 convolution across channels. Depthwise separable convolutions are <a id="_idIndexMarker1842"/>used in Xception. They are also used in MobileNet, a model particularly useful for mobile and embedded vision applications because of its reduced model size and complexity.</p>
<p class="normal">In this section, we have discussed all the major forms of convolution. The next section will discuss capsule networks, a new form of learning introduced in 2017.</p>
<h1 class="heading-1" id="_idParaDest-525">Capsule networks</h1>
<p class="normal">Capsule networks (or CapsNets) are a very recent <a id="_idIndexMarker1843"/>and innovative type of deep learning network. This technique was introduced at the end of October 2017 in a seminal paper titled <em class="italic">Dynamic Routing Between Capsules</em> by Sara Sabour, Nicholas Frost, and Geoffrey Hinton (<a href="https://arxiv.org/abs/1710.09829"><span class="url">https://arxiv.org/abs/1710.09829</span></a>) [14]. Hinton is the father of deep learning and, therefore, the whole deep learning community is excited to see the progress<a id="_idIndexMarker1844"/> made with Capsules. Indeed, CapsNets are already beating the best CNN on MNIST classification, which is... well, impressive!!</p>
<h2 class="heading-2" id="_idParaDest-526">What is the problem with CNNs?</h2>
<p class="normal">In CNNs, each layer “understands” an<a id="_idIndexMarker1845"/> image at a progressive level of granularity. As we discussed in multiple sections, the first layer will most likely recognize straight lines or simple curves and edges, while subsequent layers will start to understand more complex shapes such as rectangles up to complex forms such as human faces. </p>
<p class="normal">Now, one critical operation used for CNNs is pooling. Pooling aims at creating positional invariance and it is used after each CNN layer to make any problem computationally tractable. However, pooling introduces a significant problem because it forces us to lose all the positional data. This is not good. Think about a face: it consists of two eyes, a mouth, and a nose, and what is important is that there is a spatial relationship between these parts (for example, the mouth is below the nose, which is typically below the eyes). Indeed, Hinton said: <em class="italic">The pooling operation used in convolutional neural networks is a big mistake and the fact that it works so well is a disaster</em>. Technically, we do not need positional invariance but instead we need equivariance. Equivariance is a fancy term for indicating that we want to understand the rotation or proportion change in an image, and we want to adapt the network accordingly. In this way, the spatial positioning among the different components in an image is not lost.</p>
<h2 class="heading-2" id="_idParaDest-527">What is new with capsule networks?</h2>
<p class="normal">According to Hinton et al., our brain has modules called “capsules,” and each capsule is specialized in handling a particular type <a id="_idIndexMarker1846"/>of information. In particular, there are capsules that work well for “understanding” the concept of position, the concept of size, the concept of orientation, the concept of deformation, textures, and so on. In addition to that, the authors suggest that our brain has particularly efficient mechanisms for dynamically routing each piece of information to the capsule that is considered best suited for handling a particular type of information.</p>
<p class="normal">So, the main difference between CNN and CapsNets is that with a CNN, we keep adding layers for creating a deep network, while with CapsNet, we nest a neural layer inside another. A capsule is a group of neurons that introduces more structure to a network, and it produces a vector to signal the existence of an entity in an image. In particular, Hinton uses the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. When multiple predictions agree, a higher-level capsule becomes active. For each possible parent, the capsule produces an additional prediction vector.</p>
<p class="normal">Now a second innovation comes in place: we will use dynamic routing across capsules and will no longer use the <a id="_idIndexMarker1847"/>raw idea of pooling. A lower-level capsule prefers to send its output to higher-level capsules for which the activity vectors have a big scalar product, with the prediction coming from the lower-level capsule. The parent with the largest scalar prediction vector product increases the capsule bond. All the other parents decrease their bond. In other words, the idea is that if a higher-level capsule agrees with a lower-level one, then it will ask to send more information of that type. If there is no agreement, it will ask to send fewer of them. This dynamic routing by the agreement method is superior to the current mechanism like max pooling and, according to Hinton, routing is ultimately a way to parse the image. Indeed, max pooling is ignoring anything but the largest value, while dynamic routing selectively propagates information according to the agreement between lower layers and upper layers.</p>
<p class="normal">A third difference is that a new nonlinear activation function has been introduced. Instead of adding a squashing function to each layer as in CNN, CapsNet adds a squashing function to a nested set of layers. The nonlinear activation function is represented in <em class="italic">Equation 1</em>, and it is called the squashing function:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_20_005.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="746"/></p>
<p class="normal">where v<sub class="italic">j</sub> is the vector output of capsule <em class="italic">j</em> and s<sub class="italic">j</sub> is its total input.</p>
<p class="normal">Moreover, Hinton and others show that a discriminatively trained, multi-layer capsule system achieves state-of-the-art performances on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits.</p>
<p class="normal">Based on the paper <em class="italic">Dynamic Routing Between Capsules</em>, a simple CapsNet architecture looks as follows:</p>
<figure class="mediaobject"><img alt="Screen Shot 2017-11-03 at 7.22.09 PM.png" height="265" src="../Images/B18331_20_19_new.png" width="880"/></figure>
<p class="packt_figref">Figure 20.19: An example of CapsNet</p>
<p class="normal">The architecture is shallow with only two convolutional layers and one fully connected layer. Conv1 has 256 9 x 9 convolution kernels with a stride of 1 and ReLU activation. The role of this layer is to convert pixel intensities to the activities of local feature detectors that are then used as inputs to the PrimaryCapsules layer. PrimaryCapsules is a convolutional capsule layer<a id="_idIndexMarker1848"/> with 32 channels; each primary capsule contains 8 convolutional units with a 9 x 9 kernel and a stride of 2. In total, PrimaryCapsules has [32, 6, 6] capsule outputs (each output is an 8D vector) and each capsule in the [6, 6] grid shares its weights with each other. The final layer (DigitCaps) has one 16D capsule per digit class and each one of these capsules receives input from all the other capsules in the layer below. Routing happens only between two consecutive capsule layers (for example, PrimaryCapsules and DigitCaps).</p>
<h1 class="heading-1" id="_idParaDest-528">Summary</h1>
<p class="normal">In this chapter, we have seen many applications of CNNs across very different domains, from traditional image processing and computer vision to close-enough video processing, not-so-close audio processing, and text processing. In just a few years, CNNs have taken machine learning by storm.</p>
<p class="normal">Nowadays, it is not uncommon to see multimodal processing, where text, images, audio, and videos are considered together to achieve better performance, frequently by means of combining CNNs together with a bunch of other techniques such as RNNs and reinforcement learning. Of course, there is much more to consider, and CNNs have recently been applied to many other domains such as genetic inference [13], which are, at least at first glance, far away from the original scope of their design.</p>
<h1 class="heading-1" id="_idParaDest-529">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Yosinski, J. and Clune, Y. B. J. <em class="italic">How transferable are features in deep neural networks</em>. Advances in Neural Information Processing Systems 27, pp. 3320–3328.</li>
<li class="numberedList">Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. (2016). <em class="italic">Rethinking the Inception Architecture for Computer Vision</em>. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818–2826.</li>
<li class="numberedList">Sandler, M., Howard, A., Zhu, M., Zhmonginov, A., and Chen, L. C. (2019). <em class="italic">MobileNetV2: Inverted Residuals and Linear Bottlenecks</em>. Google Inc.</li>
<li class="numberedList">Krizhevsky, A., Sutskever, I., Hinton, G. E., (2012). <em class="italic">ImageNet classification with deep convolutional neural networks</em>.</li>
<li class="numberedList">Huang, G., Liu, Z., van der Maaten, L., and Weinberger, K. Q. (28 Jan 2018). <em class="italic">Densely Connected Convolutional Networks</em>. <a href="http://arxiv.org/abs/1608.06993"><span class="url">http://arxiv.org/abs/1608.06993</span></a></li>
<li class="numberedList">Chollet, F. (2017). <em class="italic">Xception: Deep Learning with Depthwise Separable Convolutions</em>. <a href="https://arxiv.org/abs/1610.02357"><span class="url">https://arxiv.org/abs/1610.02357</span></a></li>
<li class="numberedList">Gatys, L. A., Ecker, A. S., and Bethge, M. (2016). <em class="italic">A Neural Algorithm of Artistic Style</em>. <a href="https://arxiv.org/abs/1508.06576"><span class="url">https://arxiv.org/abs/1508.06576</span></a></li>
<li class="numberedList">Mordvintsev, A., Olah, C., and Tyka, M. ( 2015). <em class="italic">DeepDream - a code example for visualizing Neural Networks</em>. Google Research.</li>
<li class="numberedList">van den Oord, A., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. (2016). <em class="italic">WaveNet: A generative model for raw audio</em>. arXiv preprint.</li>
<li class="numberedList">van den Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., van den Driessche, G., Lockhart, E., Cobo, L. C., Stimberg, F., Casagrande, N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H., Graves, A., King, H., Walters, T., Belov, D., and Hassabis, D. (2017). <em class="italic">Parallel WaveNet: Fast High-Fidelity Speech Synthesis</em>.</li>
<li class="numberedList">He, K., Gkioxari, G., Dollár, P., and Girshick, R. (2018). <em class="italic">Mask R-CNN</em>.</li>
<li class="numberedList">Chen, L-C., Zhu, Y., Papandreou, G., Schroff, F., and Adam, H. (2018). <em class="italic">Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation</em>.</li>
<li class="numberedList">Flagel, L., Brandvain, Y., and Schrider, D.R. (2018). <em class="italic">The Unreasonable Effectiveness of Convolutional Neural Networks in Population Genetic Inference</em>.</li>
<li class="numberedList">Sabour, S., Frosst, N., and Hinton, G. E. (2017). <em class="italic">Dynamic Routing Between Capsules</em> <a href="https://arxiv.org/abs/1710.09829"><span class="url">https://arxiv.org/abs/1710.09829</span></a></li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>