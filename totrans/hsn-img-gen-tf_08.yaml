- en: '*Chapter 6*: AI Painter'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we are going to look at two **generative adversarial networks**
    (**GANs**) that could be used to generate and edit images interactively; they
    are iGAN and GauGAN . The **iGAN** (**interactive GAN**) was the first network
    to demonstrate how to use GANs for interactive image editing and transformation,
    back in 2016\. As GANs were still in fancy at that time, the generated image quality
    was not impressive as that of today's networks, but the door was opened to the
    incorporation of GANs into mainstream image editing.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will be introduced to the concepts behind iGANs and some
    websites that feature video demonstrations of them. There won't be any code in
    that section. Then, we will go over a more recent award-winning application called
    **GauGAN**, produced by Nvidia in 2019, that gives impressive results in converting
    semantic segmentation masks into real landscape photos.
  prefs: []
  type: TYPE_NORMAL
- en: We will implement GauGAN from scratch, starting with a new normalization technique
    known as **spatially adaptive normalization**. We will also learn about a new
    loss known as **hinge loss** and will go on to build a full-size GauGAN. The quality
    of the image generated by GauGAN is far superior to that of the general-purpose
    image-to-image translation networks that we covered in previous chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to iGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Segmentation map-to-image translation with GauGAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The relevant Jupyter notebooks and code can be found here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Image-Generation-with-TensorFlow-2.0/tree/master/Chapter06)'
  prefs: []
  type: TYPE_NORMAL
- en: The notebook used in this chapter is `ch6_gaugan.ipynb`.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to iGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We are now familiar with using generative models such as pix2pix (see [*Chapter
    4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*, Image-to-Image Translation*)to
    generate images from sketch or segmentation masks. However, as most of us are
    not skilled artists, we are only able to draw simple sketches, and as a result,
    our generated images also have simple shapes. What if we could use a real image
    as input and use sketches to change the appearance of the real image?
  prefs: []
  type: TYPE_NORMAL
- en: 'In the early days of GANs, a paper titled *Generative Visual Manipulation on
    the Natural Image Manifold* by J-Y. Zhu (inventor of CycleGAN) et al. was published
    that explored how to use a learned latent representation to perform image editing
    and morphing. The authors made a website, [http://efrosgans.eecs.berkeley.edu/iGAN/](http://efrosgans.eecs.berkeley.edu/iGAN/),
    that contains videos that demonstrate a few of the following use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interactive image generation**: This involves generating images from sketches
    in real time, as shown here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Interactive image generation, where an image is generated only
    from simple brush strokes (Source: J-Y. Zhu et al., 2016, "Generative Visual Manipulation
    on the Natural Image Manifold", https://arxiv.org/abs/1609.03552)](img/B14538_06_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1 – Interactive image generation, where an image is generated only
    from simple brush strokes (Source: J-Y. Zhu et al., 2016, "Generative Visual Manipulation
    on the Natural Image Manifold", [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552))'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interactive image editing**: A picture is imported and we perform image editing
    using a GAN. Early GANs generated images uses only noise as input. Even BicycleGAN
    (which was invented a few years after the iGAN) could only change the appearance
    of generated images randomly without direct manipulation. iGANs allow us to specify
    changes in color and texture, which is impressive.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Interactive image transformation** (**morphing**): Given two images, an iGAN
    can create sequences of images that show a morphing process from one image to
    the other, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Interactive image transformation (morphing). Given two images,
    sequences of intermediates images can be generated'
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: J-Y. Zhu et al., 2016, "Generative Visual Manipulation on the Natural
    Image Manifold", https://arxiv.org/abs/1609.03552)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_06_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2 – Interactive image transformation (morphing). Given two images,
    sequences of intermediates images can be generated (Source: J-Y. Zhu et al., 2016,
    "Generative Visual Manipulation on the Natural Image Manifold", [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552))'
  prefs: []
  type: TYPE_NORMAL
- en: The term **manifold** appears in the paper a lot. It also appears in other machine
    learning literature, so let's spend some time understanding it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding manifold
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can understand manifold from the perspective of a natural image. A color
    pixel can be represented by 8-bit or 256-bit number; a single RGB pixel alone
    can have 256x256x256 = 1.6 million different possible combinations! Using the
    same logic, the total possibilities for all pixels in an image is astronomically
    high!
  prefs: []
  type: TYPE_NORMAL
- en: However, we know that the pixels are not independent of each other; for example,
    the pixels of grassland are confined to the green color range. Thus, the high
    dimensionality of an image is not as daunting as it seems. In other words, the
    dimension space is a lot smaller than we might think at first. Thus, we can say
    that a high-dimensional image space is supported by a low-dimensional manifold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Manifold is a term in physics and mathematics that''s used to describe smooth
    geometric surfaces. They can exist in any dimension. One-dimensional manifolds
    include lines and circles; two-dimensional manifolds are called **surfaces**.
    A *sphere* is a three-dimensional manifold that is smooth everywhere. In contrast,
    *cubes* are not manifolds as they are not smooth at the vertices. In fact, we
    saw in [*Chapter 2*](B14538_02_Final_JM_ePub.xhtml#_idTextAnchor039)*, Variational
    Autoencoder*, that a latent space of an autoencoder with a latent dimension of
    2 was a 2D manifold of the MNIST digits projected. The following diagram shows
    a 2D latent space of digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Illustration of a 2D manifold of digits.'
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: https://scikit-learn.org/stable/modules/manifold.html)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_06_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3 – Illustration of a 2D manifold of digits. (Source: https://scikit-learn.org/stable/modules/manifold.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'A good resource to visualize manifolds in GAN is the interactive tool at [https://poloclub.github.io/ganlab/](https://poloclub.github.io/ganlab/).
    In the following example, a GAN is trained to map uniformly distributed 2D samples
    into 2D samples that have a circular distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – The generator''s data transformation is visualized as a manifold,
    which turns input noise (on the left) into fake samples (on the right).'
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: M. Kahng, 2019, "GAN Lab: Understanding Complex Deep Generative Models
    using Interactive Visual Experimentation," IEEE Transactions on Visualization
    and Computer Graphics, 25(1) (VAST 2018) https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_06_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.4 – The generator''s data transformation is visualized as a manifold,
    which turns input noise (on the left) into fake samples (on the right).(Source:
    M. Kahng, 2019, "GAN Lab: Understanding Complex Deep Generative Models using Interactive
    Visual Experimentation," IEEE Transactions on Visualization and Computer Graphics,
    25(1) (VAST 2018) [https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf](https://minsuk.com/research/papers/kahng-ganlab-vast2018.pdf))'
  prefs: []
  type: TYPE_NORMAL
- en: We can visualize this mapping using a manifold, where the input is represented
    as a uniform square grid. The generator wraps the high-dimensional input grid
    into a warped version with fewer dimensions. The output shown at the top right
    of the figure is the manifold approximated by the generator. The generator output,
    or the fake image (bottom right in the figure), is the samples sampled from the
    manifold, where an area with smaller grid blocks means a higher sampling probability.
  prefs: []
  type: TYPE_NORMAL
- en: The assumption of the paper is that a GAN's output sampled from random noise
    **z**, **G(z)**, lies in on a smooth manifold. Therefore, given two images on
    the manifold, **G(z**0**)** and **G(z**N**)**, we could get a sequence of *N*
    + 1 images *[G(z*0*) , G(z*0*), ..., G(z*N*)]* with a smooth transition by interpolation
    in the latent space. This approximation of the manifold of natural images is used
    for performing *realistic image editing*.
  prefs: []
  type: TYPE_NORMAL
- en: Image editing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know what a manifold is, let's see how to use that knowledge to
    perform image editing. The first step in image editing is to project an image
    onto the manifold.
  prefs: []
  type: TYPE_NORMAL
- en: Projecting an image onto a manifold
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What projecting an image onto a manifold means is using a pre-trained GAN to
    generate an image that is close to the given image. In this book, we will use
    a pre-trained DCGAN where the input to the generator is a 100-dimension latent
    vector. Therefore, we will need to find a latent vector that generates an image
    manifold that is as close as possible to the original image. One way to do this
    is to use optimization such as **style transfer**, a topic we covered in detail
    in [*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*, Style Transfer*.
  prefs: []
  type: TYPE_NORMAL
- en: We first extract the features of the original image using a pretrained **convolutional
    neural network** (**CNN**), such as the output of the *block5_conv1* layer in
    VGG (see [*Chapter 5*](B14538_05_Final_JM_ePub.xhtml#_idTextAnchor104)*, Style
    Transfer*), and use it as a target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we use the pre-trained DCGAN's generator with frozen weights and optimize
    on the input latent vector by minimizing the L2 loss between the features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we have learned regarding style transfer, optimization can be slow to run
    and hence not responsive when it comes to interactive drawing.
  prefs: []
  type: TYPE_NORMAL
- en: Another method is to train a feedforward network to predict the latent vector
    from the image, which is a lot faster. If the GAN is to translate a segmentation
    mask into an image, then we can use a network such as U-Net to predict the segmentation
    mask from the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Manifold projection using a feedforward network looks similar to using an autoencoder.
    The encoder encodes (predicts) the latent variable from the original image, then
    the decoder (the generator, in the case of a GAN) projects the latent variable
    onto the image manifold. However, this method is not always perfect. This is where
    the *hybrid* method comes in. We use the feedforward network to predict the latent
    variables, which we then fine-tune using optimization. The following figure shows
    the images generated using different techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Projecting real photos onto an image manifold using a GAN. (Source:
    J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold,"
    https://arxiv.org/abs/1609.03552)](img/B14538_06_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5 – Projecting real photos onto an image manifold using a GAN. (Source:
    J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold,"
    [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552))'
  prefs: []
  type: TYPE_NORMAL
- en: As we have now obtained the latent vector, we will use it to edit the manifold.
  prefs: []
  type: TYPE_NORMAL
- en: Editing the manifold using latent vector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we have obtained the latent variable *z*0 and the image manifold *x*0
    *= G(z*0*)*, the next step is to manipulate *z*0 to modify the image. Now, let's
    say the image is a red shoe and we want to change the color to black – how can
    we do that? The simplest and crudest method is to open an image editing software
    package to select all the red pixels in the picture and change them to black.
    The resulting picture is likely to not look very natural as some details may be
    lost. Traditional image editing tools' algorithms tend to not work that well on
    natural images with complex shapes and fine texture details.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, we know we probably could change the latent vector and feed
    that to the generator and change the color. In practice, we do not know how to
    modify the latent variables to get the results we want.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, instead of changing the latent vector directly, we could attack the
    problem from a different direction. We can edit the manifold, for example, by
    drawing a black stripe on the shoes, then use that to optimize the latent variables,
    and then project that to generate another image on the manifold.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, we are performing optimization as previously described for manifold
    projection but with different loss functions. We want to find an image manifold
    *x* that minimizes the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s start with the second loss term *S(x, x*0*)* for manifold smoothness.
    It is L2 loss, used to encourage the new manifold to not deviate too much from
    the original manifold. This loss term keeps the global appearance of the image
    in check. The first loss term is the data term, which sums up all the editing
    operation loss. This is best described using the following images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Projecting real photos onto the image manifold using GAN. (Source:
    J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold,"
    https://arxiv.org/abs/1609.03552)](img/B14538_06_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6 – Projecting real photos onto the image manifold using GAN. (Source:
    J-Y. Zhu et al, 2016, "Generative Visual Manipulation on the Natural Image Manifold,"
    [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552))'
  prefs: []
  type: TYPE_NORMAL
- en: This example uses a color brush to change the color of the shoe. The color change
    may not be obvious in the grayscale print of this book and you are encouraged
    to check out the color version of the paper, which you can download from [https://arxiv.org/abs/1609.03552](https://arxiv.org/abs/1609.03552).
    The top row of the preceding figure shows the brush stroke as the constraint *v*g
    and *f*g as the editing operation. We want every manifold pixel in the brush stroke
    *f*g*(x)* to be as close to *v*g as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In other words, if we put a black stroke on the shoe, we want that part in
    the image manifold to be black. That is the intention, but to execute it, we will
    need to do the optimization of the latent variables. Thus, we reformulate the
    preceding equation from pixel space into latent space. The equation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The last term ![](img/Formula_06_003.png) is the *GAN's adversarial loss*. This
    is used for making the manifold look real and improve the visual quality slightly.
    By default, this term is not used for increasing the frame rate. With all the
    loss terms defined, we can use a TensorFlow optimizer such as Adam to run the
    optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Edit transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Edit transfer** is the last step of image editing. Now we have two manifolds,
    *G(z*0*)* and *G(z*1*)*, and we can generate sequences of intermediate images
    using linear interpolation in latent space between *z*0 and *z*1\. Due to the
    capacity limitations of the DCGAN, the generated manifolds can appear blurry and
    may not look as realistic as we hoped them to be.'
  prefs: []
  type: TYPE_NORMAL
- en: The way the authors of the previously mentioned paper address this problem is
    to not use the manifold as the final image but instead to estimate the color and
    geometric changes between the manifolds and apply the changes to the original
    image. The estimation of color and motion flow is performed using optical flow;
    this is a traditional computer vision technique that is beyond the scope of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: Using the preceding shoe example, if all we are interested in is the color change,
    we estimate the color change of the pixel between manifolds, then transfer the
    color changes on the pixels in the original image. Similarly, if the transformation
    involves warping, that is, a change in shape, we measure the motion of pixels
    and apply them to original image to perform morphing. The demonstration video
    on the website was created using both motion and color flow.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, we have now learned that an iGAN is not a GAN but a method of using
    a GAN to perform image editing. This is done by first projecting a real image
    onto a manifold using either optimization or a feedforward network. Next, we use
    brush strokes as constraints to modify the manifold generated by the latent vector.
    Finally, we transfer the color and motion flow of the interpolated manifolds onto
    the real image to complete the image editing.
  prefs: []
  type: TYPE_NORMAL
- en: As there aren't any new GAN architectures, we will not be implementing an iGAN.
    Instead, we are going to implement GauGAN, which includes some new innovations
    that are exciting for code implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation map-to-image translation with GauGAN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**GauGAN** (named after 19th-century painter Paul Gauguin) is a GAN from **Nvidia**.
    Speaking of Nvidia, it is one of the handful of companies that has invested heavily
    in GANs. They have achieved several breakthroughs in this space, including **ProgressiveGAN**
    (we''ll cover that in [*Chapter 7*](B14538_07_Final_JM_ePub.xhtml#_idTextAnchor136)*,
    High Fidelity Face Generation*), to generate high-resolution images, and **StyleGAN**
    for high-fidelity faces.'
  prefs: []
  type: TYPE_NORMAL
- en: Their main business is in making graphics chips rather than AI software. Therefore,
    unlike some other companies, who keep their code and trained models as closely
    guarded secrets, Nvidia tends to open source their software code to the general
    public. They have built a web page ([http://nvidia-research-mingyuliu.com/gaugan/](http://nvidia-research-mingyuliu.com/gaugan/))
    to showcase GauGAN, which can generate photorealistic landscape photos from segmentation
    maps. The following screenshot is taken from their web page.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feel free to pause this chapter for a bit and have a play with the application
    to see how good it is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 –From brush stroke to photo with GauGAN'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_06_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 –From brush stroke to photo with GauGAN
  prefs: []
  type: TYPE_NORMAL
- en: We will now learn about pix2pixHD.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to pix2pixHD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'GauGAN uses **pix2pixHD** as a base and adds new features to it. pix2pixHD
    is an upgraded version of pix2pix that can generate **high-definition** (**HD**)
    images. As we haven''t covered pix2pixHD in this book and we won''t be using an
    HD dataset, we will build our GauGAN base on pix2pix''s architecture and the code
    base that we are already familiar with. Nevertheless, it is good to know the high-level
    architecture of pix2pixHD, and I''ll walk you through some of the high-level concepts.
    The following diagram shows the architecture of pix2pixHD''s generators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – The network architecture of the pix2pixHD generator.'
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: T-C. W et al., 2018, "High-Resolution Image Synthesis and Semantic
    Manipulation with Conditional GANs," https://arxiv.org/abs/1711.11585)](img/B14538_06_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.8 – The network architecture of the pix2pixHD generator. (Source:
    T-C. W et al., 2018, "High-Resolution Image Synthesis and Semantic Manipulation
    with Conditional GANs," [https://arxiv.org/abs/1711.11585](https://arxiv.org/abs/1711.11585))'
  prefs: []
  type: TYPE_NORMAL
- en: In order to generate high-resolution images, pix2pixHD uses two generators at
    different image resolutions at coarse- and fine-scale. The coarse generator **G1**
    works at half the image resolution; that is, the input and target images are downsampled
    into half the resolution. When that is trained, we start training the coarse generator
    **G**1 together with the fine generator **G2**, which works on the full-image
    scale. From the preceding architecture diagram, we can see that **G**1's encoder
    output concatenates with **G**1's features and feeds into the decoder part of
    **G**2 to generate high-resolution images. This setting is also known as a **coarse-to-fine
    generator**.
  prefs: []
  type: TYPE_NORMAL
- en: pix2pixHD uses three PatchGAN discriminators that operate at different image
    scales. A new loss, known as feature matching loss, is used to match the layer
    features between the real and fake images. This is used in style transfer, where
    we use a pre-trained VGG for feature extraction and optimize on the style features.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've had a quick introduction to pix2pixHD, we can move on to GauGAN.
    But first, we will implement a normalization technique that demonstrates GauGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Spatial Adaptive Normalization (SPADE)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main innovation in GauGAN is a layer normalization method for segmentation
    map known as **Spatial-Adaptive Normalization** (**SPADE**). That's right, another
    entry into the already-long list of normalization techniques in the GAN's toolbox.
    We will dive deep into SPADE, but before that, we should learn about the format
    of the network input – the **semantic segmentation map**.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot encoded segmentation masks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We will use the `facades` dataset to train our GauGAN. In previous experiments,
    the segmentation map was encoded as different colors in an RGB image; for example,
    a wall was represented by a purple mask and a door was green. This representation
    is visually easy for us to understand but it is not that helpful for the neural
    network to learn. This is because the colors do not have semantic meaning.
  prefs: []
  type: TYPE_NORMAL
- en: Colors being closer in color space does not mean they are close in semantic
    meaning. We could use light green to represent grass and dark green to represent
    an airplane, and their semantic meanings would not be related even though the
    segmentation maps would be close in color shade.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this reason, instead of labeling pixels using colors, we should use class
    labels. However, this still does not solve the problem as the class labels are
    numbers assigned randomly and they also do not have semantic meaning. Therefore,
    a better way is to use a **segmentation mask** with a label of 1 when an object
    is present in that pixel and a label of 0 otherwise. In other words, we one-hot
    encode the labels in a segmentation map into a segmentation mask with the shape
    (H, W, number of classes). The following figure shows an example of semantic segmentation
    masks for a building image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – On the left is a segmentation map encoded in RGB. On the right,
    the segmentation maps are separated into individual classes of window, façade,
    and pillar ](img/B14538_06_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – On the left is a segmentation map encoded in RGB. On the right,
    the segmentation maps are separated into individual classes of window, façade,
    and pillar
  prefs: []
  type: TYPE_NORMAL
- en: 'The data in the `facades` dataset that we used in the previous chapters was
    encoded as JPEG, so we cannot use that to train GauGAN. In JPEG encoding, some
    visual information that is less important to the visuals is removed in the compression
    process. The resulting pixels may have different values even if they should belong
    to the same class and appear to be the same color. As a result, we cannot map
    the colors in a JPEG image to classes. To tackle this problem, I got the raw dataset
    from the original source and created a new dataset that includes three different
    image file types for each sample as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: JPEG – real photo
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PNG – segmentation map using RGB color
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BMP – segmentation map using class labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BMP is uncompressed. We can think of a BMP image as the image in RGB format
    in the preceding figure, except that the pixel values are 1-channel class labels
    rather than 3-channel RGB colors. In image loading and pre-processing, we will
    load all three files and convert them from BMP into one-hot encoded segmentation
    masks.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, TensorFlow's basic image pre-processing APIs are not able to do some
    of the more complex tasks, so we need to resort to using other Python libraries.
    Luckily, `tf.py_function` allows us to run a generic Python function within a
    TensorFlow training pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this file-loading function, as shown in the following code, we use `.numpy()`
    to convert TensorFlow tensors into Python objects. The function name is a bit
    misleading as it applies not only to numerical values but also string values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Now that we understand the format of one-hot encoded semantic segmentation masks,
    we will look at how SPADE can help us generate better images from segmentation
    masks.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing SPADE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Instance normalization has become popular in image generation, but it tends
    to wash away the semantic meaning of segmentation masks. What does that mean?
    Let's assume an input image consists of only one single segmentation label; for
    example, say the entire image is of the sky. As the input has uniform values,
    the output, after passing through the convolution layer, will also have uniform
    values.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that instance normalization calculates the mean across dimensions (H,
    W) for each channel. Therefore, the mean for that channel will be the same uniform
    value, and the normalized activation after subtraction with mean will become zero.
    Obviously, the semantic meaning is lost and the sky has just vanished into thin
    air. This is an extreme example, but using the same logic, we can see that a segmentation
    mask loses its semantic meaning as its area grows larger.
  prefs: []
  type: TYPE_NORMAL
- en: 'To solve this problem, SPADE normalizes on local areas confined by the segmentation
    mask rather than on the entire mask. The following diagram shows the high-level
    architecture of SPADE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – High-level SPADE architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '(Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)](img/B14538_06_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.10 – High-level SPADE architecture. (Redrawn from: T. Park et al.,
    2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  prefs: []
  type: TYPE_NORMAL
- en: 'In batch normalization, we calculate the means and standard deviations of channels
    across dimensions (N, H, W). This is the same for SPADE, as shown in the preceding
    figure. The difference is that gamma and beta for each channel are no longer scalar
    values (or vectors of C channels) but two-dimensional (H, W). In other words,
    there is a gamma and a beta for every activation that is learned from the semantic
    segmentation map. So, normalization is applied differently to different segmentation
    areas. These two parameters are learned by using two convolutional layers, as
    shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – SPADE design diagram, where k denotes the number of convolutional
    filters'
  prefs: []
  type: TYPE_NORMAL
- en: '(Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)](img/B14538_06_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.11 – SPADE design diagram, where k denotes the number of convolutional
    filters (Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  prefs: []
  type: TYPE_NORMAL
- en: SPADE is used not only at the network input stage but also in the internal layers.
    The resize layer is to resize the segmentation map to match the dimensions of
    the layer's activation. We can now implement a TensorFlow custom layer for SPADE.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will first define the convolutional layers in the `__init__` constructor
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will get the activation map dimensions to be used in resizing later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we will connect the layers and operations together in `call()` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This is a straightforward implementation based on the SPADE design diagram.
    Next, we will look at how to make use of SPADE.
  prefs: []
  type: TYPE_NORMAL
- en: Inserting SPADE into residual blocks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GauGAN uses **residual blocks** in the generator. We will now look at how to
    insert SPADE into residual blocks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – SPADE residual blocks'
  prefs: []
  type: TYPE_NORMAL
- en: '(Redrawn from: T. Park et. al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)](img/B14538_06_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.12 – SPADE residual blocks (Redrawn from: T. Park et. al., 2019, "Semantic
    Image Synthesis with Spatially-Adaptive Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  prefs: []
  type: TYPE_NORMAL
- en: The basic building block within the SPADE residual block is the **SPADE-ReLU-Conv
    layer**. Each SPADE takes two inputs – the activation from the previous layer
    and the semantic segmentation map.
  prefs: []
  type: TYPE_NORMAL
- en: 'As with the standard residual block, there are two convolution-ReLU layers
    and a skip path. Whenever there is a change in the number of channels before and
    after the residual block, the skip connection is learned via the sub-block in
    the dashed-line box shown in the preceding diagram. When this happens, the activation
    maps at the inputs of the two SPADEs in forward path will have different dimensions.
    That is alright as we have built-in resizing within SPADE block. The following
    is the code for the SPADE residual block to build the needed layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we connect the layers up in `call()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: In the original GauGAN implementation, spectral normalization is applied after
    the convolutional layer. It is yet another normalization that we will cover in
    [*Chapter 8*](B14538_08_Final_JM_ePub.xhtml#_idTextAnchor156)*, Self-Attention
    for Image Generation*, when we talk about self-attention GANs. Therefore, we will
    skip over that and put the residual blocks together to implement GauGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing GauGAN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will first build the generator, followed by the discriminator. Finally, we
    will implement the loss functions and start training GauGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Building the GauGAN generator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before diving into the GauGAN generator, let's revise what we know about some
    of its predecessors. In pix2pix, the generator takes in only one input – the semantic
    segmentation map. As there is no randomness in the network, given the same input,
    it will always generate building facades with the same color and texture. The
    naive way of concatenating the input with random noise doesn't work.
  prefs: []
  type: TYPE_NORMAL
- en: One of the two methods used by **BicycleGAN** ([*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation*) to address this problem is using an encoder to encode
    the target image (the real photo) into latent vectors, which are then used to
    sample random noise for the generator input. This **cVAE-GAN** structure is used
    in the GauGAN generator. There are two inputs to the generator – the semantic
    segmentation mask and the real photo.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the GauGAN web application, we can select a photo (the generated image will
    resemble the style of the photo). This is made possible by using the encoder to
    encode style information into latent variables. The code for the encoder is the
    same as that which we used in the previous chapters, so we will move on to look
    at the generator architecture. Feel free to revisit [*Chapter 4*](B14538_04_Final_JM_ePub.xhtml#_idTextAnchor084)*,
    Image-to-Image Translation*, to refresh the encoder implementation. In the following
    diagram, we can see the GauGAN generator architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – GauGAN generator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '(Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_06_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.13 – GauGAN generator architecture (Redrawn from: T. Park et al.,
    2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  prefs: []
  type: TYPE_NORMAL
- en: The generator is a decoder-like architecture. The main difference is that the
    segmentation mask goes into every residual block via SPADE. The latent variable
    dimension chosen for GauGAN is 256\.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The encoder is not an integral part of the generator; we can choose not to use
    any style image and sample from a standard multivariate Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the code to build the generator using the residual block that
    we wrote previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: You now know about everything that makes GauGAN work – SPADE and the generator.
    The rest of the network architecture are ideas borrowed from other GANs that we
    have previously learned. Next, we will look at how to build the discriminator.
  prefs: []
  type: TYPE_NORMAL
- en: Building the discriminator
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The discriminator is PatchGAN, where the input is a concatenation of the segmentation
    map and the generated image. The segmentation map has to have the same number
    of channels as the generated RGB image; therefore, we will use the RGB segmentation
    map instead of a one-hot encoded segmentation mask. The architecture of the GauGAN
    discriminator is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – GauGAN discriminator architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '(Redrawn from: T. Park et al., 2019, "Semantic Image Synthesis with Spatially-Adaptive
    Normalization," https://arxiv.org/abs/1903.07291)](img/B14538_06_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.14 – GauGAN discriminator architecture (Redrawn from: T. Park et al.,
    2019, "Semantic Image Synthesis with Spatially-Adaptive Normalization," [https://arxiv.org/abs/1903.07291](https://arxiv.org/abs/1903.07291))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Except for the last layer, the discriminator layers consist of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional layer with a kernel size of 4x4 and a stride of 2 for downsampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instance normalization (except for the first layer)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leaky ReLU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GauGAN uses multiple discriminators at different scales. Since our dataset image
    has a low resolution of 256x256, one discriminator is sufficient. If we were to
    use multiple discriminators, all we would need to do is downsample the input size
    by half for the next discriminator and calculate the average loss from all the
    discriminators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code implementation for a single PatchGAN is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This is identical to pix2pix, except that the discriminator returns all downsampling
    blocks' output. Why do we need that? Well, this brings us to a discussion about
    loss functions.
  prefs: []
  type: TYPE_NORMAL
- en: Feature matching loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Feature matching loss** has been successfully used in style transfer. The
    content and style features are extracted using a pre-trained VGG and the losses
    are calculated between the target image and the generated image. The content features
    are simply the outputs from multiple convolutional blocks in VGG. GauGAN employs
    content loss to replace L1 reconstruction loss, which was common among GANs. The
    reason is that reconstruction loss makes comparisons pixel by pixel, and the loss
    can be large if the image shifts in location despite still looking the same to
    human eyes.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the content features of convolutional layers are spatially
    invariant. As a result, when using the content loss to train on the `facades`
    dataset, our generated buildings will look a lot less blurry and the lines will
    look straighter. The content loss in style transfer literature is sometimes known
    as **VGG loss** in code, as people love to use VGG for feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: Why do people still love using the old VGG?
  prefs: []
  type: TYPE_NORMAL
- en: Newer CNN architectures such as ResNet have long surpassed VGG's performance
    and achieve much higher accuracy in image classification. So, why do people still
    use VGG for feature extraction? Some have tried Inception and ResNet for neural
    style transfer but have found that the results generated using VGG were more visually
    pleasant. This is likely due to the hierarchy of VGG's architecture, with its
    monotonically increasing channel numbers across layers. This allows feature extraction
    to happen smoothly from low-level to high-level representation.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast, the residual blocks of ResNet have a bottleneck design that squeezes
    the input activation channels (say, 256) to a lower number (say, 64), before restoring
    it back to a higher number (256 again). Residual blocks also have a skip connection
    that could *smuggle* information for the classification task and bypass feature
    extraction in the convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to calculate the VGG feature loss is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: When calculating VGG loss, we first convert the images from `[-1, +1]` to `[0,
    255]` and from `RGB` to `BGR`, which is the image format expected by Keras' VGG
    `preprocess` function. GauGAN gives more weights to higher layers to emphasize
    structural accuracy. This is to align the generated image with the segmentation
    mask. Anyway, this is not set in stone and you are welcome to try different weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature matching is also used on the discriminator, where we extract the discriminator
    layer outputs of real and fake images. The following code is used to calculate
    the L1 feature matching loss in the discriminator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Apart from this, we will also have **KL divergence loss** for the encoder. The
    last loss is **hinge loss** as the new adversarial loss.
  prefs: []
  type: TYPE_NORMAL
- en: Hinge loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hinge loss may be a newcomer in the GAN world, but it has long been used in
    **support vector machines** (**SVMs**) for classification. It maximizes the margin
    of the decision boundary. The following plots show the hinge loss for positive
    (real) and negative (fake) labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Hinge loss for a discriminator'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_06_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.15 – Hinge loss for a discriminator
  prefs: []
  type: TYPE_NORMAL
- en: 'On the left is the hinge loss for the discriminator when the image is real.
    When we use hinge loss for the discriminator,the loss is bounded to 0 when prediction
    is over 1\. If it is anything under 1, the loss increases to penalize for not
    predicting the image as real. It''s similar for fake images but in the opposite
    direction: the hinge loss is 0 when the prediction of fake image is under -1 and
    it increases linearly once above that threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement the hinge loss using basic mathematic operations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Another way of doing this is by using TensorFlow''s hinge loss API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The loss for the generator isn''t really hinge loss; it is simply a negative
    mean of prediction. This is unbounded, so the loss is lower when the prediction
    score is higher:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We now have everything we need to train GauGAN using a training framework as
    we did in the previous chapter. The following figure shows the images generated
    using the segmentation mask:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Example of images generated by our GauGAN implementation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14538_06_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 – Example of images generated by our GauGAN implementation
  prefs: []
  type: TYPE_NORMAL
- en: They look a lot better than the pix2pix and CycleGAN results! If we encode the
    ground truth image's style into random noise, the generated images will be almost
    indistinguishable from the ground truth. It is really impressive to look at on
    the computer!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using AI in image editing is already prevalent now, and all this started at
    around the time that the iGAN was introduced. We learned about the key principle
    of the iGAN being to first project an image onto a manifold and then directly
    perform editing on the manifold. We then optimize this on the latent variables
    and generate an edited image that is natural-looking. This is in contrast with
    previous methods that could only change generated images indirectly by manipulating
    latent variables.
  prefs: []
  type: TYPE_NORMAL
- en: GauGAN incorporates many advanced techniques to generate crisp images from semantic
    segmentation masks. This includes the use of hinge loss and feature matching loss.
    However, the key ingredient is SPADE, which provides superior performance when
    using a segmentation mask as input. SPADE performs normalization on a local segmentation
    map to preserve its semantic meaning, which helps us to produce high-quality images.
    So far, we have been using images with up to 256x256 resolution to train our networks.
    We now have techniques that are mature enough to generate high-resolution images,
    as we briefly discussed when introducing pix2pixHD.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will move to the realm of high-resolution images with
    advanced models such as ProgressiveGAN and StyleGAN.
  prefs: []
  type: TYPE_NORMAL
