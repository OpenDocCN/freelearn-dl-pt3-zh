- en: Double DQN, Dueling Architectures, and Rainbow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We discussed the **Deep Q-Network** (**DQN**) algorithm in the previous chapter,
    coded it in Python and TensorFlow, and trained it to play Atari Breakout. In DQN,
    the same Q-network was used to select and evaluate an action. This, unfortunately,
    is known to overestimate the Q values, which results in over-optimistic estimates
    for the values. To mitigate this, DeepMind released another paper where it proposed
    the decoupling of the action selection and action evaluation. This is the crux
    of the **Double DQN** (**DDQN**) architectures, which we will investigate in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Even later, DeepMind released another paper where they proposed the Q-network
    architecture with two output values, one representing the value, *V(s)*, and the
    other the advantage of taking an action at the given state, *A(s,a)*. DeepMind
    then combined these two to compute the *Q(s,a)* action-value, instead of directly
    determining it as done in DQN and DDQN. These Q-network architectures are referred
    to as the **dueling** network architectures, as the neural network now has dual
    output values, *V(s)* and *A(s,a)*, which are later combined to obtain *Q(s,a)*.
    We will also see these dueling networks in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Another extension we will also consider in this chapter are **Rainbow networks**,
    which are a blend of several different ideas fused into one algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that will be covered in this chapter are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning the theory behind DDQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding DDQN and training it to play Atari Breakout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance of DDQN on Atari Breakout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding dueling network architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coding dueling network architecture and training it to play Atari Breakout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the performance of dueling architectures on Atari Breakout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Rainbow networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running a Rainbow network on Dopamine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To successfully complete this chapter, knowledge of the following will help
    significantly:'
  prefs: []
  type: TYPE_NORMAL
- en: Python (2 or 3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorFlow (version 1.4 or higher)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dopamine (we will discuss this in more detail later)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding Double DQN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'DDQN is an extension to DQN, where we use the target network in the Bellman
    update. Specifically, in DDQN, we evaluate the target network''s Q function using
    the action that would be greedy maximization of the primary network''s Q function.
    First, we will use the vanilla DQN target for the Bellman equation update step,
    then, we will extend to DDQN for the same Bellman equation update step; this is
    the crux of the DDQN algorithm. We will then code DDQN in TensorFlow to play Atari
    Breakout. Finally, we will compare and contrast the two algorithms: DQN and DDQN.'
  prefs: []
  type: TYPE_NORMAL
- en: Updating the Bellman equation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In vanilla DQN, the target for the Bellman update is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c173c74-a2f2-4790-aec1-5e9aaca83a53.png)'
  prefs: []
  type: TYPE_IMG
- en: '*θ[t]* represents the model parameters of the target network. This is known
    to over-predict *Q,* and so the change made in DDQN is to replace this target
    value, *y[t]*, with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b33dc1c6-5089-4349-b253-78997a943795.png)'
  prefs: []
  type: TYPE_IMG
- en: We must distinguish between the Q-network parameters, *θ*, and the target network
    model parameters, *θ[t]*.
  prefs: []
  type: TYPE_NORMAL
- en: Coding DDQN and training to play Atari Breakout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now code DDQN in TensorFlow to play Atari Breakout. As before, we have
    three Python files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`funcs.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`model.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ddqn.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`funcs.py` and `model.py` are the same as used before for DQN in [Chapter 3](1bf60e32-a842-4f5c-b7be-c1dc50a43c1e.xhtml),
    *Deep Q-Network (DQN)*. The `ddqn.py` file is the only code where we need to make
    changes to implement DDQN. We will use the same `dqn.py` file from the previous
    chapter and make changes to it to code DDQN. So, let''s first copy the `dqn.py` file
    from before and rename it `ddqn.py`.'
  prefs: []
  type: TYPE_NORMAL
- en: We will summarize the changes we will make to `ddqn.py`, which are actually
    quite minimal. We will still not delete the DQN-related lines of code in the file,
    and instead, use `if` loops to choose between the two algorithms. This helps to
    use one code for both algorithms, which is a better way to code.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we create a variable called `ALGO`, which will store one of two strings:
    `DQN` or `DDQN`, which is where we specify which of the two algorithms to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Then, in the lines of code where we evaluate the targets for the mini-batch,
    we use `if` loops to decide whether the algorithm to use is DQN or DDQN and accordingly
    compute the targets as follows. Note that, in DQN, the `greedy_q` variable stores
    the Q value corresponding to the greedy action taking, that is, the largest Q
    value in the target network, which is computed using `np.amax()` and then used
    to compute the target variable, `targets_batch`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In DDQN, on the other hand, we compute the action corresponding to the maximum
    Q in the primary Q-network, which we store in `greedy_q` and evaluate using `np.argmax()`.
    Then, we use `greedy_q` (which represents an action now) in the target network
    Q values. Note that, for Terminal time steps, that is, `done = True`, we should
    not consider the next state and likewise, for non-Terminal steps, `done = False`,
    and here we consider the next step. This is easily accomplished using `np.invert().astype(np.float32)`
    on `done_batch`. The following lines of code show DDQN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: That's it for `ddqn.py`. We will now evaluate it on Atari Breakout.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of DDQN on Atari Breakout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now evaluate the performance of DDQN on Atari Breakout. Here, we will
    plot the performance of our DDQN algorithm on Atari Breakout using the `performance.txt`
    file that we wrote in the code. We will use `matplotlib` to plot two graphs as
    explained in the following.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we present the number of time steps per episode
    on Atari Breakout using DDQN and its exponentially weighted moving average. As
    evident, the peak number of time steps is ~2,000 for many episodes toward the
    end of the training, with one episode where it exceeded even 3,000 time steps!
    The moving average is approximately 1,500 time steps toward the end of the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/29cdcfd8-86bb-4826-9dfe-3da46f587773.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Number of time steps per episode for Atari Breakout using DDQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we show the total rewards received per episode
    versus the time number of the global time step. The peak episode reward is over
    350, with the moving average near 150\. Interestingly, the moving average (in
    orange) is still increasing toward the end, which means you can run the training
    even longer to see further gains. This is left to the interested reader:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad3e8213-a62c-4ff0-a258-57ad53d8cbb3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Total episode reward versus time step for Atari Breakout using DDQN'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, due to RAM constraints (16 GB), we used a replay buffer size of 300,000
    only. If the user has access to more RAM power, a bigger replay buffer size can
    be used—for example, 500,000 to 1,000,000, which can result in even better scores.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the DDQN agent is learning to play Atari Breakout well. The moving
    average of the episode rewards is constantly going up, which means you can train
    longer to obtain even higher rewards. This upward trend in the episode reward
    demonstrates the efficacy of the DDQN algorithm for such problems.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding dueling network architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now understand the use of dueling network architectures. In DQN and
    DDQN, and other DQN variants in the literature, the focus was primarily on algorithms,
    that is, how to efficiently and stably update the value function neural networks.
    While this is crucial for developing robust RL algorithms, a parallel but complementary
    direction to advance the field is to also innovate and develop novel neural network
    architectures that are well suited for model-free RL. This is precisely the concept
    behind dueling network architectures, another contribution from DeepMind.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in dueling architectures are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Dueling network architecture figure; compare with standard DQN
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Computing *Q(s,a)*
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Subtracting the average of the advantage from the `advantage` function
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As we saw in the previous chapter, the output of the Q-network in DQN is *Q(s,a)*,
    the action-value function. In dueling networks, the Q-network instead has two
    output values: the `state value` function, *V(s)*, and the `advantage` function, *A(s,a)*.
    You can then combine them to compute the `state-action value` function, *Q(s,a)*.
    This has the advantage that the network need not learn the `value` function for
    every action at every state. This is particularly useful in states where the actions
    do not affect the environment.'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if the agent is a car driving on a straight road with no traffic,
    no action is necessary and so *V(s)* alone will suffice in these states. On the
    other hand, if the road suddenly curves or other cars come into the vicinity of
    the agent, then the agent needs to take actions and so, in these states, the `advantage`
    function comes into play to find the incremental returns a given action can provide
    over the `state value` function. This is the intuition behind separating the estimation
    of *V(s)* and *A(s,a)* in the same network by using two different branches, and
    later combining them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Refer to the following diagram for a schematic showing a comparison of the
    standard DQN network and the dueling network architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e364fe22-6475-4485-981c-8b25a530f2ec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Schematic of the standard DQN network (top) and the dueling network
    architecture (bottom)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can compute the `action-value` function *Q(s,a)* as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8c8ad6f8-0b44-43a8-a37f-8a2edbd99f2d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, this is not unique in that you can have an amount, *δ*, over-predicted
    in *V(s)* and the same amount, *δ*, under-predicted in *A(s,a)*. This makes the
    neural network predictions unidentifiable. To circumvent this problem, the authors
    of the dueling network paper recommend the following way to combine *V(s)* and
    *A(s,a)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/773c5473-e05f-4c28-ab5f-412c4e10a57a.png)'
  prefs: []
  type: TYPE_IMG
- en: '*|A|* represents the number of actions and *θ* is the neural network parameters
    that are shared between the *V(s)* and *A(s,a)* streams; in addition, *α* and *β*
    are used to denote the neural network parameters in the two different streams,
    that is, in the *A(s,a)* and *V(s)* streams, respectively. Essentially, in the
    preceding equation, we subtract the average `advantage` function from the `advantage`
    function and sum it to the `state value` function to obtain *Q(s,a)*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the link to the dueling network architectures paper: [https://arxiv.org/abs/1511.06581](https://arxiv.org/abs/1511.06581).'
  prefs: []
  type: TYPE_NORMAL
- en: Coding dueling network architecture and training it to play Atari Breakout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now code the dueling network architecture and train it to learn to
    play Atari Breakout. For the dueling network architecture, we require the following
    codes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`funcs.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`dueling.py`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will use `funcs.py`, which was used earlier for DDQN, so we reuse it. The
    `dueling.py` code is also identical to `ddqn.py` (which was used earlier, so we
    just rename and reuse it). The only changes to be made are in `model.py`. We copy
    the same `model.py` file from DDQN and summarize here the changes to be made for
    the dueling network architecture. The steps involved are the following.
  prefs: []
  type: TYPE_NORMAL
- en: 'We first create a Boolean variable called `DUELING` in `model.py` and assign
    it to `True` if using dueling network architecture; otherwise, it is assigned
    to `False`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We will write the code with an `if` loop so that the `DUELING` variable, if
    `False`, will use the earlier code we used in DDQN, and if `True`, we will use
    the dueling network. We will use the `flattened` object that is the flattened
    version of the output of the convolutional layers to create two sub-neural network
    streams. We send `flattened` separately into two different fully connected layers
    with `512` neurons, using the `relu` activation function and the `winit` weights
    initializer defined earlier; the output values of these fully connected layers
    are called `valuestream` and `advantagestream`, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Combining V and A to obtain Q
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `advantagestream` object is passed into a fully connected layer with a
    number of neurons equal to the number of actions, that is, `len(self.VALID_ACTIONS)`.
    Likewise, the `valuestream` object is passed into a fully connected layer with
    one neuron. Note that we do not use an activation function for computing the `advantage`
    and `state value` functions, as they can be positive or negative (`relu` will
    set all negative values to zero!). Finally, we combine the advantage and value
    streams using `tf.subtract()` to subtract the advantage and the mean of the `advantage`
    function. The mean is computed using `tf.reduce_mean()` on the `advantage` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: That's it for coding dueling network architectures. We will train an agent with
    the dueling network architecture and evaluate its performance on Atari Breakout.
    Note that we can use the dueling architecture in conjunction with either DQN or
    DDQN. That is to say that we only changed the neural network architecture, not
    the actual Bellman update, and so the dueling architecture works with both DQN
    and DDQN.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the performance of dueling architectures on Atari Breakout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will now evaluate the performance of dueling architectures on Atari Breakout.
    Here, we will plot the performance of our dueling network architecture with DDQN
    on Atari Breakout using the `performance.txt` file that we wrote during the training
    of the agent. We will use `matplotlib` to plot two graphs as explained in the
    following.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we present the number of time steps per episode
    on Atari Breakout using DDQN (in blue) and its exponentially weighted moving average
    (in orange). As evident, the peak number of time steps is ~2,000 for many episodes
    toward the end of the training, with a few episodes even exceeding 4,000 time
    steps! The moving average is approximately 1,500 time steps toward the end of
    the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/21bc1175-c1e7-4a36-a128-aed5bff0c893.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Number of time steps per episode on Atari Breakout using dueling
    network architecture and DDQN'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we show the total rewards received per episode
    versus the time number of the global time step. The peak episode reward is over
    400, with the moving average near 220\. We also note that the moving average (in
    orange) is still increasing toward the end, which means you can run the training
    even longer to obtain further gains. Overall, the average rewards are higher with
    the dueling network architecture vis-a-vis the non-dueling counterparts, and so
    it is strongly recommended to use these dueling architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/447de67c-abb0-4983-8c94-fac145c6a658.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Total episode reward received versus global time step number for
    Atari Breakout using dueling network architecture and DDQN'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, due to RAM constraints (16 GB), we used a replay buffer size of only
    300,000\. If the user has access to more RAM power, a bigger replay buffer size
    can be used—for example, 500,000 to 1,000,000, which can result in even better
    scores.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Rainbow networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will now move on to **Rainbow networks**, which is a confluence of several
    different DQN improvements. Since the original DQN paper, several different improvements
    were proposed with notable success. This motivated DeepMind to combine several
    different improvements into an integrated agent, which they refer to as the **Rainbow
    DQN**. Specifically, six different DQN improvements are combined into one integrated
    Rainbow DQN agent. These six improvements are summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: DDQN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dueling network architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritized experience replay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-step learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributional RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Noisy nets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DQN improvements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen DDQN and dueling network architectures and have coded them
    in TensorFlow. The rest of the improvements are described in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Prioritized experience replay
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We used a replay buffer where all of the samples have an equal probability
    of being sampled. This, however, is not very efficient, as some samples are more
    important than others. This is the motivation behind prioritized experience replay,
    where samples that have a higher **Temporal Difference** (**TD**) error are sampled
    with a higher probability than others. The first time a sample is added to the
    replay buffer, it is set a maximum priority value so as to ensure that all samples
    in the buffer are sampled at least once. Thereafter, the TD error is used to determine
    the probability of the experience to be sampled, which we compute as this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/180e299e-e33a-4a27-b859-2b8afcd62782.png)'
  prefs: []
  type: TYPE_IMG
- en: Whereas the previous *r* is the reward, *θ* is the primary Q-network model parameters,
    and *θ^t* is the target network parameters. *ω* is a positive hyper-parameter
    that determines the shape of the distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-step learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In Q-learning, we accumulate a single reward and use the greedy action at the
    next step. Alternatively, you can also use multi-step targets and compute an *n*-step
    return from a single state:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91f444c4-3846-43ab-b7ac-bf3bf367131e.png)'
  prefs: []
  type: TYPE_IMG
- en: Then, the *n*-step return, *r[t]^((n))* , is used in the Bellman update and
    is known to lead to faster learning.
  prefs: []
  type: TYPE_NORMAL
- en: Distributional RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In **distributional RL**, we learn to approximate the distribution of returns
    instead of the expected return. This is mathematically complicated, is beyond
    the scope of this book, and is not discussed further.
  prefs: []
  type: TYPE_NORMAL
- en: Noisy nets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In some games (such as Montezuma''s revenge), ε-greedy does not work well,
    as many actions need to be executed before the first reward is received. Under
    this setting, the use of a noisy linear layer that combined a deterministic and
    a noisy stream is recommended, shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/041b0d30-df4e-44e6-9b32-a81183b3f968.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *x* is the input, *y* is the output, and *b* and *W* are the biases and
    weights in the deterministic stream; *b^(noisy)* and *W^(noisy)* are the biases
    and weights, respectively, in the noisy stream; and *ε^b* and *ε^W* are random
    variables and are applied as element-wise product to the biases and weights, respectively,
    in the noisy stream. The network may choose to ignore the noisy stream in some
    regions of the state space and may use them otherwise, as required. This allows
    for a state-determined exploration strategy.
  prefs: []
  type: TYPE_NORMAL
- en: We will not be coding the full Rainbow DQN, as it is exhaustive. Instead, we
    will use an open source framework called Dopamine to train a Rainbow DQN agent,
    which will be discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Running a Rainbow network on Dopamine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In 2018, some engineers at Google released an open source, lightweight, TensorFlow-based
    framework for training RL agents, called **Dopamine**. Dopamine, as you may already
    know, is the name of an organic chemical that plays an important role in the brain.
    We will use Dopamine to run Rainbow.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Dopamine framework is based on four design principles:'
  prefs: []
  type: TYPE_NORMAL
- en: Easy experimentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flexible development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compact and reliable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reproducible
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To download Dopamine from GitHub, type the following command in a Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test whether Dopamine was successfully installed by typing the following
    commands into a Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this will look something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You should see `OK` at the end to confirm that everything went well with the
    download.
  prefs: []
  type: TYPE_NORMAL
- en: Rainbow using Dopamine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run Rainbow DQN, type the following command into a Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s it. Dopamine will start training Rainbow DQN and print out training
    statistics on the screen, as well as save checkpoint files. The configuration
    file is stored in the following path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: It looks like the following code. `game_name` is set to `Pong` as default; feel
    free to try other Atari games. The number of agent steps for training is set in
    `training_steps`, and for evaluation in `evaluation_steps`. In addition, it introduces
    stochasticity to the training by using the concept of sticky actions, where the
    most recent action is repeated multiple times with a probability of 0.25\. That
    is, if a uniform random number (computed using NumPy's `np.random.rand()`) is
    < 0.25, the most recent action is repeated; otherwise, a new action is taken from
    the policy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The sticky action is a new method of introducing stochasticity to the learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Feel free to experiment with the hyperparameters and see how the learning is
    affected. This is a very nice way to ascertain the sensitivity of the different
    hyperparameters on the learning of the RL agent.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we were introduced to DDQN, dueling network architectures,
    and the Rainbow DQN. We extended our previous DQN code to DDQN and dueling architectures
    and tried it out on Atari Breakout. We can clearly see that the average episode
    rewards are higher with these improvements, and so these improvements are a natural
    choice to use. Next, we also saw Google's Dopamine and used it to train a Rainbow
    DQN agent. Dopamine has several other RL algorithms, and the user is encouraged
    to dig deeper and try out these other RL algorithms as well.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter was a good deep dive into the DQN variants, and we really covered
    a lot of mileage as far as coding of RL algorithms is involved. In the next chapter,
    we will learn about our next RL algorithm called **Deep Deterministic Policy Gradient**
    (**DDPG**), which is our first Actor-Critic RL algorithm and our first continuous
    action space RL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Why does DDQN perform better than DQN?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the dueling network architecture help in the training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why does prioritized experience replay speed up the training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do sticky actions help in the training?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The DDQN paper, *Deep Reinforcement Learning with Double Q-learning*, by Hado
    van Hasselt, Arthur Guez*,* and David Silver can be obtained from the following
    link, and the interested reader is recommended to read it: [https://arxiv.org/abs/1509.06461](https://arxiv.org/abs/1509.06461)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Rainbow: Combining Improvements in Deep Reinforcement Learning*, Matteo Hessel,
    Joseph Modayil, Hado van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan
    Horgan, Bilal Piot, Mohammad Azar, and David Silver, arXiv:1710.02298 (the Rainbow
    DQN): [https://arxiv.org/abs/1710.02298](https://arxiv.org/abs/1710.02298)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prioritized Experience Replay*, Tom Schaul, John Quan, Ioannis Antonoglou,
    David Silver, arXiv:1511.05952: [https://arxiv.org/abs/1511.05952](https://arxiv.org/abs/1511.05952)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Multi-Step Reinforcement Learning: A Unifying Algorithm*, Kristopher de Asis,
    J Fernando Hernandez-Garcia, G Zacharias Holland, Richard S Sutton: [https://arxiv.org/pdf/1703.01327.pdf](https://arxiv.org/pdf/1703.01327.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Noisy Networks for Exploration,* by Meire Fortunato, Mohammad Gheshlaghi Azar,
    Bilal Piot, Jacob Menick, Ian Osband, Alex Graves, Vlad Mnih, Remi Munos, Demis
    Hassabis, Olivier Pietquin, Charles Blundell, and Shane Legg, arXiv:1706.10295:
    [https://arxiv.org/abs/1706.10295](https://arxiv.org/abs/1706.10295)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
