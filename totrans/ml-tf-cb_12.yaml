- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Taking TensorFlow to Production
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, we have seen that TensorFlow is capable of implementing
    many models, but there is more that TensorFlow can do. This chapter will show
    you a few of those things. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing graphs in TensorBoard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Managing hyperparameter tuning with TensorBoard's HParams
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing unit tests using tf.test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple executors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelizing TensorFlow using tf.distribute.strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving and restoring a TensorFlow model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using TensorFlow Serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll start by showing how to use the various aspects of TensorBoard, a capability
    that comes with TensorFlow. This tool allows us to visualize summary metrics,
    graphs, and images even while our model is training. Next, we will show you how
    to write code that is ready for production use with a focus on unit tests, training
    distribution across multiple processing units, and efficient model saving and
    loading. Finally, we will address a machine learning serving solution by hosting
    a model as REST endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Graphs in TensorBoard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring and troubleshooting machine learning algorithms can be a daunting
    task, especially if you have to wait a long time for the training to complete
    before you know the results. To work around this, TensorFlow includes a computational
    graph visualization tool called **TensorBoard**. With TensorBoard, we can visualize
    graphs and important values (loss, accuracy, batch training time, and so on) even
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate the various ways we can use TensorBoard, we will reimplement the
    MNIST model from *The Introductory CNN Model* recipe in *Chapter 8*, *Convolutional
    Neural Networks*. Then, we'll add the TensorBoard callback and fit the model.
    We will show how to monitor numerical values, histograms of sets of values, how
    to create an image in TensorBoard, and how to visualize TensorFlow models.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we''ll load the libraries necessary for the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll now reimplement the MNIST model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will compile the model with the sparse categorical cross-entropy loss
    and the Adam optimizer. Then, we''ll display the summary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will create a timestamped subdirectory for each run. The summary writer
    will write the `TensorBoard` logs to this folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will instantiate a `TensorBoard` callback and pass it to the `fit`
    method. All logs during the training phase will be stored in this directory and
    can be viewed instantly in `TensorBoard`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then start the `TensorBoard` application by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then we navigate in our browser to the following link: `http://127.0.0.0:6006`.
    We can specify a different port if needed by passing, for example, a `--port 6007`
    command (for running on port 6007). We can also start TensorBoard within the notebook
    through the `%tensorboard --logdir="logs"` command line. Remember that TensorBoard
    will be viewable as your program is running.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can quickly and easily visualize and compare metrics of several experiments
    during the model training through TensorBoard's scalars view. By default, TensorBoard
    writes the metrics and losses every epoch. We can update this frequency by batch
    using the following argument: `update_freq='batch'`. We can also visualize model
    weights as images with the argument `write_images=True` or display bias and weights
    with histograms (computing every epoch) using `histogram_freq=1.`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here is a screenshot of the scalars view:![](img/B16254_12_01.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 12.1: Training and test loss decrease over time while the training and
    test accuracy increase'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Here, we show how to visualize weights and bias with a histogram summary. With
    this dashboard, we can plot many histogram visualizations of all the values of
    a non-scalar tensor (such as weights and bias) at different points in time. So,
    we can see how the values have changed over time:![](img/B16254_12_02.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 12.2: The Histograms view to visualize weights and bias in TensorBoard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will visualize the TensorFlow model through TensorFlow's Graphs dashboard,
    which shows the model using different views. This dashboard allows visualizing
    the op-level graph but also the conceptual graph. The op-level displays the Keras
    model with extra edges to other computation nodes, whereas the conceptual graph
    displays only the Keras model. These views allow quickly examining and comparing
    our intended design and understanding the TensorFlow model structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we show how to visualize the op-level graph:![](img/B16254_12_03.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 12.3: The op-level graph in TensorBoard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'By adding the TensorBoard callback, we can visualize the loss, the metrics,
    model weights as images, and so on. But we can also use the `tf.summary` module
    for writing summary data that can be visualized in TensorFlow. First, we have
    to create a `FileWriter` and then, we can write histogram, scalar, text, audio,
    or image summaries. Here, we''ll write images using the Image Summary API and
    visualize them in TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/B16254_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: Visualize images in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: Be careful of writing image summaries too often to TensorBoard. For example,
    if we were to write an image summary every generation for 10,000 generations,
    that would generate 10,000 images worth of summary data. This tends to eat up
    disk space very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we implemented a CNN model on the MNIST dataset. We added a
    TensorBoard callback and fitted the model. Then, we used TensorFlow's visualization
    tool, which enables you to monitor numerical values and histograms of sets of
    values, to visualize the model graph, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that we can launch TensorBoard through a command line as in the recipe
    but we can also launch it within a notebook by using the `%tensorboard` magic
    line.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the TensorBoard API, visit the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official TensorBoard guide: [https://www.tensorflow.org/tensorboard/get_started](https://www.tensorflow.org/tensorboard/get_started)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The TensorFlow summary API: [https://www.tensorflow.org/api_docs/python/tf/summary](https://www.tensorflow.org/api_docs/python/tf/summary)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard.dev is a free managed service provided by Google. The aim is to
    easily host, track, and share machine learning experiments with anyone. After
    we launch our experiments, we just have to upload our TensorBoard logs to the
    TensorBoard server. Then, we share the link and anyone who has the link can view
    our experiments. Note not to upload sensitive data because uploaded TensorBoard
    datasets are public and visible to everyone.
  prefs: []
  type: TYPE_NORMAL
- en: Managing Hyperparameter tuning with TensorBoard's HParams
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tuning hyperparameters in a machine learning project can be a real pain. The
    process is iterative and can take a long time to test all the hyperparameter combinations.
    But fortunately, HParams, a TensorBoard plugin, comes to the rescue. It allows
    testing to find the best combination of hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To illustrate how the HParams plugin works, we will use a sequential model implementation
    on the MNIST dataset. We'll configure HParams and compare several hyperparameter
    combinations in order to find the best hyperparameter optimization.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we''ll load the libraries necessary for the script:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll load and prepare the MNIST dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, for each hyperparameter, we''ll define the list or the interval of values
    to test. In this section, we''ll go over three hyperparameters: the number of
    units per layer, the dropout rate, and the optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model will be a sequential model with five layers: a flatten layer, followed
    by a dense layer, a dropout layer, another dense layer, and the output layer with
    10 units. The train function takes as an argument the HParams dictionary that
    contains a combination of hyperparameters. As we use a Keras model, we add an
    HParams Keras callback on the fit method to monitor each experiment. For each
    experiment, the plugin will log the hyperparameter combinations, losses, and metrics.
    We can add a summary File Writer if we want to monitor other information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll iterate on all the hyperparameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then start the TensorBoard application by running this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we can quickly and easily visualize the results (hyperparameters and metrics)
    in the HParams table view. Filters and sorting can be applied on the left pane
    if needed:![](img/B16254_12_05.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 12.5: The HParams table view visualized in TensorBoard'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the parallel coordinates view, each axis represents a hyperparameter or a
    metric and each run is represented by a line. This visualization allows the quick
    identification of the best hyperparameter combination:![](img/B16254_12_06.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 12.6: The HParams parallel coordinates view visualized in TensorBoard'
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard HParams is a simple and insightful way to identify the best
    hyperparameters and also to manage your experiments with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a reference on the HParams TensorBoard plugin, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official TensorBoard guide: [https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams](https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing unit tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Testing code results in faster prototyping, more efficient debugging, faster
    changing, and makes it easier to share code. TensorFlow 2.0 provides the `tf.test`
    module and we will cover it in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When programming a TensorFlow model, it helps to have unit tests to check the
    functionality of the program. This helps us because when we want to make changes
    to a program unit, tests will make sure those changes do not break the model in
    unknown ways. In Python, the main test framework is `unittest` but TensorFlow
    provides its own test framework. In this recipe, we will create a custom layer
    class. We will implement a unit test to illustrate how to write it in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we need to load the necessary libraries as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we need to declare our custom gate that applies the function `f(x) =
    a1 * x + b1`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we create our unit test class that inherits from the `tf.test.TestCase`
    class. The `setup` method is a `hook` method that is called before every `test`
    method. The `assertAllEqual` method checks that the expected and the computed
    outputs have the same values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we need a `main()` function in our script, to run all unit tests:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the terminal, run the following command. We should get the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We implemented one test and it passed. Don't worry about the two `test_session`
    tests – they are phantom tests.
  prefs: []
  type: TYPE_NORMAL
- en: Note that many assertions tailored to TensorFlow are available in the `tf.test`
    API.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we implemented a TensorFlow unit test using the `tf.test` API
    that is very similar to the Python unit test. Remember that unit testing helps
    assure us that code will function as expected, provides confidence in sharing
    code, and makes reproducibility more accessible.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For a reference on the `tf.test` module, visit the following website:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official TensorFlow test API: [https://www.tensorflow.org/api_docs/python/tf/test](https://www.tensorflow.org/api_docs/python/tf/test)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using multiple executors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You will be aware that there are many features of TensorFlow, including computational
    graphs that lend themselves naturally to being computed in parallel. Computational
    graphs can be split over different processors as well as in processing different
    batches. We will address how to access different processors on the same machine
    in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will show you how to access multiple devices on the same
    system and train on them. A device is a CPU or an accelerator unit (GPUs, TPUs)
    where TensorFlow can run operations. This is a very common occurrence: along with
    a CPU, a machine may have one or more GPUs that can share the computational load.
    If TensorFlow can access these devices, it will automatically distribute the computations
    to multiple devices via a greedy process. However, TensorFlow also allows the
    program to specify which operations will be on which device via a name scope placement.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will show you different commands that will allow you to access
    various devices on your system; we'll also demonstrate how to find out which devices
    TensorFlow is using. Remember that some functions are still experimental and are
    subject to change.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to find out which devices TensorFlow is using for which operations,
    we will activate the logs for device placement by setting `tf.debugging.set_log_device_placement` to `True`.
    If a TensorFlow operation is implemented for CPU and GPU devices, the operation
    will be executed by default on a GPU device if a GPU is available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also use the tensor device attribute that returns the name of the device
    on which this tensor will be assigned:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'By default, TensorFlow automatically decides how to distribute computations
    across computing devices (CPUs and GPUs) and sometimes we need to select the device
    to use by creating a device context with the `tf.device` function. Each operation
    executed in this context will use the selected device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we move the `matmul` operation out of the context, this operation will be
    executed on a GPU device if it''s available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When using GPUs, TensorFlow automatically takes up a large portion of the GPU
    memory. While this is usually desired, we can take steps to be more careful with
    GPU memory allocation. While TensorFlow never releases GPU memory, we can slowly
    grow its allocation to the maximum limit (only when needed) by setting a GPU memory
    growth option. Note that physical devices cannot be modified after being initialized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we want to put a hard limit on the GPU memory used by TensorFlow, we can
    also create a virtual GPU device and set the maximum memory limit (in MB) to allocate
    on this virtual GPU. Note that virtual devices cannot be modified after being
    initialized:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also simulate virtual GPU devices with a single physical GPU. This is
    done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Sometimes we may need to write robust code that can determine whether it is
    running with the GPU available or not. TensorFlow has a built-in function that
    can test whether the GPU is available. This is helpful when we want to write code
    that takes advantage of the GPU when it is available and assign specific operations
    to it. This is done with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we need to assign specific operations, say, to the GPU, we input the following
    code. This will perform simple calculations and assign operations to the main
    CPU and the two auxiliary GPUs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the first two operations have been performed on the main CPU,
    the next two on the first auxiliary GPU, and the last two on the second auxiliary
    GPU.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we want to set specific devices on our machine for TensorFlow operations,
    we need to know how TensorFlow refers to such devices. Device names in TensorFlow
    follow the following conventions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Device | Device name |'
  prefs: []
  type: TYPE_TB
- en: '| Main CPU | `/device:CPU:0` |'
  prefs: []
  type: TYPE_TB
- en: '| Main GPU | `/GPU:0` |'
  prefs: []
  type: TYPE_TB
- en: '| Second GPU | `/job:localhost/replica:0/task:0/device:GPU:1` |'
  prefs: []
  type: TYPE_TB
- en: '| Third GPU | `/job:localhost/replica:0/task:0/device:GPU:2` |'
  prefs: []
  type: TYPE_TB
- en: Remember that TensorFlow considers a CPU as a unique processor even if the processor
    is a multi-core processor. All cores are wrapped in `/device:CPU:0`, that is to
    say, TensorFlow does indeed use multiple CPU cores by default.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Fortunately, running TensorFlow in the cloud is now easier than ever. Many cloud
    computation service providers offer GPU instances that have a main CPU and a powerful
    GPU alongside it. Note that an easy way to have a GPU is to run the code in Google
    Colab and set the GPU as the hardware accelerator in the notebook settings.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelizing TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a model can be very time-consuming. Fortunately, TensorFlow offers
    several distributed strategies to speed up the training, whether for a very large
    model or a very large dataset. This recipe will show us how to use the TensorFlow
    distributed API.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The TensorFlow distributed API allows us to distribute the training by replicating
    the model into different nodes and training on different subsets of data. Each
    strategy supports a hardware platform (multiple GPUs, multiple machines, or TPUs)
    and uses either a synchronous or asynchronous training strategy. In synchronous
    training, each worker trains over different batches of data and aggregates their
    gradients at each step. While in the asynchronous mode, each worker is independently
    training over the data and the variables are updated asynchronously. Note that
    for the moment, TensorFlow only supports data parallelism described above and
    according to the roadmap, it will soon support model parallelism. This paradigm
    is used when the model is too large to fit on a single device and needs to be
    distributed over many devices. In this recipe, we will go over the mirrored strategy
    provided by this API.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, we''ll load the libraries necessary for this recipe as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will create two virtual GPUs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will load the MNIST dataset via the `tensorflow_datasets` API as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will prepare the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to apply a mirrored strategy. The goal of this strategy is
    to replicate the model across all GPUs on the same machine. Each model is trained
    on different batches of data and a synchronous training strategy is applied:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we check that we have two devices corresponding to the two virtual GPUs
    created at the beginning of this recipe as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we''ll define the value of the batch size. The batch size given to the
    dataset is the global batch size. The global batch size is the sum of all batch
    sizes of every replica. So, we had to compute the global batch size using the
    number of replicas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll define and compile our model using the mirrored strategy scope.
    Note that all variables created inside the scope are mirrored across all replicas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the compilation is over, we can fit the previous model as we would normally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using a strategy scope is the only thing you have to do to distribute your training.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the distributed TensorFlow API is quite easy. All you have to do is to
    assign the scope. Then, operations can be manually or automatically assigned to
    workers. Note that we can easily switch between strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a brief overview of some distributed strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: The TPU strategy is like the mirrored strategy but it runs on TPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Multiworker Mirrored strategy is very similar to the mirrored strategy but
    the model is trained across several machines, potentially with multiple GPUs.
    We have to specify the cross-device communication.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Central Storage strategy uses a synchronous mode on one machine with multiple
    GPUs. Variables aren't mirrored but placed on the CPU and operations are replicated
    into all local GPUs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Parameter Server strategy is implemented on a cluster of machines. Some
    machines have a worker role and others have a parameter server role. The workers
    compute and the parameter servers store the variable of the model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on the `tf.distribute.Strategy` module, visit the following
    websites:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed training with TensorFlow: [https://www.tensorflow.org/guide/distributed_training](https://www.tensorflow.org/guide/distributed_training)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `tf.distribute` API: [https://www.tensorflow.org/api_docs/python/tf/distribute](https://www.tensorflow.org/api_docs/python/tf/distribute)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we've just gotten over the mirrored strategy and we've executed
    our program eagerly with the Keras API. Note that the TensorFlow distributed API
    works better when used in graph mode than in eager mode.
  prefs: []
  type: TYPE_NORMAL
- en: This API moves quickly so feel free to consult the official documentation to
    know which distributed strategies are supported in which scenarios (the Keras
    API, a custom training loop, or the Estimator API).
  prefs: []
  type: TYPE_NORMAL
- en: Saving and restoring a TensorFlow model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we want to use our machine learning model in production or reuse our trained
    model for a transfer learning task, we have to store our model. In this section,
    we will outline some methods for storing and restoring the weights or the whole
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we want to summarize various ways to store a TensorFlow model.
    We will cover the best way to save and restore an entire model, only the weights,
    and model checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We start by loading the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we''ll build an MNIST model using the Keras Sequential API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will use the recommended format to save an entire model on disk named
    the SavedModel format. This format saves the model graph and variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'A directory named `SavedModel` is created on disk. It contains a TensorFlow
    program,the `saved_model.pb` file; the `variables` directory, which contains the
    exact value of all parameters; and the `assets` directory, which contains files
    used by the TensorFlow graph:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the `save()` operation also takes other parameters. Extra directories
    can be created based on the model complexity and the signatures and options passed
    to the s`ave` method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we''ll restore our saved model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If we prefer to save the model in the H5 format, we can either pass a filename
    that ends in `.h5` or add the `save_format="h5"` argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also use a `ModelCheckpoint` callback in order to save an entire model
    or just the weights into a checkpoint structure at some intervals. This callback
    is added to the `callback` argument in the `fit` method. In the configuration
    below, the model weights will be stored at each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can load the entire model or only the weights later in order to continue
    the training. Here, we will reload the weights:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, you're ready to save and restore an entire model, only the weights, or
    model checkpoints.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provided several ways to store and restore an entire model
    or only the weights. That allows you to put a model into production or avoids
    retraining a full model from scratch. We have also seen how to store a model during
    the training process and after it.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For some references on this topic, visit the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The official training checkpoints guide: [https://www.tensorflow.org/guide/checkpoint](https://www.tensorflow.org/guide/checkpoint)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official SavedModel format guide: [https://www.tensorflow.org/guide/saved_model](https://www.tensorflow.org/guide/saved_model)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `tf.saved_model` API: [https://www.tensorflow.org/api_docs/python/tf/saved_model/save](https://www.tensorflow.org/api_docs/python/tf/saved_model/save)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Keras Model Checkpoint API: [https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using TensorFlow Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will show you how to serve machine learning models in production.
    We will use the TensorFlow Serving components of the **TensorFlow Extended** (**TFX**)
    platform. TFX is an MLOps tool that builds complete, end-to-end machine learning
    pipelines for scalable and high-performance model tasks. A TFX pipeline is composed
    of a sequence of components for data validation, data transformation, model analysis,
    and model serving. In this recipe, we will focus on the last component, which
    can support model versioning, multiple models, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We'll start this section by encouraging you to read through the official documentation
    and the short tutorials on the TFX site, available at [https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx).
  prefs: []
  type: TYPE_NORMAL
- en: For this example, we will build an MNIST model, save it, download the TensorFlow
    Serving Docker image, run it, and send POST requests to the REST server in order
    to get some image predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will start in the same way as before, by loading the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We''ll build an MNIST model using the Keras Sequential API:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Then, we will save our model as the SavedModel format and create a directory
    for each version of our model. TensorFlow Serving wants a specific tree structure
    and models saved into SavedModel format. Each model version should be exported
    to a different subdirectory under a given path. So, we can easily specify the
    version of a model we want to use when we call the server to do predictions:![](img/B16254_12_07.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 12.7: A screenshot of the directory structure that TensorFlow Serving
    expects'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The preceding screenshot shows the desired directory structure. In it, we have
    our defined data directory, `my_mnist_model`, followed by our model-version number, `1`.
    In the version number directory, we save our protobuf model and a `variables` folder
    that contains the desired variables to save.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We should be aware that inside our data directory, TensorFlow Serving will look
    for integer folders. TensorFlow Serving will automatically boot up and grab the
    model under the largest integer number. This means that to deploy a new model,
    we need to label it version 2 and stick it under a new folder that is also labeled 2\.
    TensorFlow Serving will then automatically pick up the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Then, we'll install TensorFlow Serving by using Docker. We encourage readers
    to visit the official Docker documentation to get Docker installation instructions
    if needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first step is to pull the latest TensorFlow Serving Docker image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we''ll start a Docker container: publish the REST API port 8501 to our
    host''s port 8501, take the previously created model, `my_mnist_model`, bind it
    to the model base path, `/models/my_mnist_model`, and fill in the environment
    variable `MODEL_NAME` with `my_mnist_model`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will display the images to predict:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![](img/B16254_12_08.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: We can now submit binary data to the `<host>:8501` and get back the JSON response
    showing the results. We can do this via any machine and with any programming language.
    It is very useful to not have to rely on the client to have a local copy of TensorFlow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here, we will send POST predict requests to our server and pass the images.
    The server will return 10 probabilities for each image corresponding to the probability
    for each digit between `0` and `9`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we will display the prediction results for our images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let''s look at a visual representation of 16 predictions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B16254_12_09.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning teams focus on creating machine learning models and operations
    teams focus on deploying models. MLOps applies DevOps principles to machine learning.
    It brings the best practices of software development (commenting, documentation,
    versioning, testing, and so on) to data science. MLOps is about removing the barriers
    between the machine learning teams that produce models and the operations teams
    that deploy models.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we only focus on serving models using the TFX Serving component
    but TFX is an MLOps tool that builds complete, end-to-end machine learning pipelines.
    We can only encourage the reader to explore this platform.
  prefs: []
  type: TYPE_NORMAL
- en: There are also many other solutions available that may be used to serve a model,
    such as Kubeflow, Django/Flask, or managed cloud services such as AWS SageMaker,
    GCP AI Platform, or Azure ML.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Links to tools and resources for architectures not covered in this chapter
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Using TensorFlow Serving** **with Docker**: [https://www.tensorflow.org/serving/docker](https://www.tensorflow.org/serving/docker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Using TensorFlow Serving** **with Kubernetes**: [https://www.tensorflow.org/tfx/serving/serving_kubernetes](https://www.tensorflow.org/tfx/serving/serving_kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Installing** **TensorFlow Serving**: [https://www.tensorflow.org/tfx/tutorials/serving/rest_simple](https://www.tensorflow.org/tfx/tutorials/serving/rest_simple)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow** **extended**: [https://www.tensorflow.org/tfx](https://www.tensorflow.org/tfx)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubeflow – The machine** **learning toolkit for Kubernetes**: [https://www.kubeflow.org/](https://www.kubeflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GCP AI** **Platform**: [https://cloud.google.com/ai-platform](https://cloud.google.com/ai-platform)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AWS** **SageMaker**: [https://aws.amazon.com/fr/sagemaker/](https://aws.amazon.com/fr/sagemaker/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure** **ML**: [https://azure.microsoft.com/services/machine-learning/](https://azure.microsoft.com/services/machine-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Share your experience**Thank you for taking the time to read this book.
    If you enjoyed this book, help others to find it. Leave a review at [https://www.amazon.com/dp/1800208863](https://www.amazon.com/dp/1800208863)
    |'
  prefs: []
  type: TYPE_TB
