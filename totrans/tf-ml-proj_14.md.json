["```\npip3 install gym\n```", "```\nall_env = list(gym.envs.registry.all())\nprint('Total Environments in Gym version {} : {}'\n    .format(gym.__version__,len(all_env)))\n\n```", "```\nTotal Environments in Gym version 0.10.5 : 797\n```", "```\nfor e in list(all_env):\n    print(e)\n```", "```\nEnvSpec(Copy-v0) EnvSpec(RepeatCopy-v0) EnvSpec(ReversedAddition-v0) EnvSpec(ReversedAddition3-v0) EnvSpec(DuplicatedInput-v0) EnvSpec(Reverse-v0) EnvSpec(CartPole-v0) EnvSpec(CartPole-v1) EnvSpec(MountainCar-v0) EnvSpec(MountainCarContinuous-v0) EnvSpec(Pendulum-v0)\n```", "```\nenv=gym.make('MsPacman-v0')\n```", "```\nprint(env.action_space)\n```", "```\nDiscrete(9)\n```", "```\nprint(env.observation_space)\n```", "```\nBox(210, 160, 3)\n```", "```\nimport time\n\nframe_time = 1.0 / 15 # seconds\nn_episodes = 1\n\nfor i_episode in range(n_episodes):\n    t=0\n    score=0\n    then = 0\n    done = False\n    env.reset()\n    while not done:\n        now = time.time()\n        if frame_time < now - then:\n            action = env.action_space.sample()\n            observation, reward, done, info = env.step(action)\n            score += reward\n            env.render()\n            then = now\n            t=t+1\n    print('Episode {} finished at t {} with score {}'.format(i_episode,\n                                                             t,score))\n```", "```\nEpisode 0 finished at t 551 with score 100.0\n```", "```\nimport time\nimport numpy as np\n\nframe_time = 1.0 / 15 # seconds\nn_episodes = 500\n\nscores = []\nfor i_episode in range(n_episodes):\n    t=0\n    score=0\n    then = 0\n    done = False\n    env.reset()\n    while not done:\n        now = time.time()\n        if frame_time < now - then:\n            action = env.action_space.sample()\n            observation, reward, done, info = env.step(action)\n            score += reward\n            env.render()\n            then = now\n            t=t+1\n    scores.append(score)\n    #print(\"Episode {} finished at t {} with score {}\".format(i_episode,t,score))\nprint('Average score {}, max {}, min {}'.format(np.mean(scores),\n                                          np.max(scores),\n                                          np.min(scores)\n                                         ))\n```", "```\nAverage 219.46, max 1070.0, min 70.0\n```", "```\ndef policy_q_nn(obs, env):\n    # Exploration strategy - Select a random action\n    if np.random.random() < explore_rate:\n        action = env.action_space.sample()\n    # Exploitation strategy - Select the action with the highest q\n    else:\n        action = np.argmax(q_nn.predict(np.array([obs])))\n    return action\n```", "```\ndef episode(env, policy, r_max=0, t_max=0):\n\n    # create the empty list to contain game memory\n    #memory = deque(maxlen=1000)\n\n    # observe initial state\n    obs = env.reset()\n    state_prev = obs\n    #state_prev = np.ravel(obs) # replaced with keras reshape[-1]\n\n    # initialize the variables\n    episode_reward = 0\n    done = False\n    t = 0\n\n    while not done:\n\n        action = policy(state_prev, env)\n        obs, reward, done, info = env.step(action)\n        state_next = obs\n        #state_next = np.ravel(obs) # replaced with keras reshape[-1]\n\n        # add the state_prev, action, reward, state_new, done to memory\n        memory.append([state_prev,action,reward,state_next,done])\n\n        # Generate and update the q_values with \n        # maximum future rewards using bellman function:\n        states = np.array([x[0] for x in memory])\n        states_next = np.array([np.zeros(n_shape) if x[4] else x[3] for x in memory])\n\n        q_values = q_nn.predict(states)\n        q_values_next = q_nn.predict(states_next)\n\n        for i in range(len(memory)):\n            state_prev,action,reward,state_next,done = memory[i]\n            if done:\n                q_values[i,action] = reward\n            else:\n                best_q = np.amax(q_values_next[i])\n                bellman_q = reward + discount_rate * best_q\n                q_values[i,action] = bellman_q\n\n        # train the q_nn with states and q_values, same as updating the q_table\n        q_nn.fit(states,q_values,epochs=1,batch_size=50,verbose=0)\n\n        state_prev = state_next\n\n        episode_reward += reward\n        if r_max > 0 and episode_reward > r_max:\n            break\n        t+=1\n        if t_max > 0 and t == t_max:\n            break\n    return episode_reward\n```", "```\n# experiment collect observations and rewards for each episode\ndef experiment(env, policy, n_episodes,r_max=0, t_max=0):\n\n    rewards=np.empty(shape=[n_episodes])\n    for i in range(n_episodes):\n        val = episode(env, policy, r_max, t_max)\n        #print('episode:{}, reward {}'.format(i,val))\n        rewards[i]=val\n\n    print('Policy:{}, Min reward:{}, Max reward:{}, Average reward:{}'\n        .format(policy.__name__,\n              np.min(rewards),\n              np.max(rewards),\n              np.mean(rewards)))\n```", "```\nfrom collections import deque \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\n\n# build the Q-Network\nmodel = Sequential()\nmodel.add(Flatten(input_shape = n_shape))\nmodel.add(Dense(512, activation='relu',name='hidden1'))\nmodel.add(Dense(9, activation='softmax', name='output'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam')\nmodel.summary()\nq_nn = model\n```", "```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nflatten_3 (Flatten)          (None, 100800)            0         \n_________________________________________________________________\nhidden1 (Dense)              (None, 8)                 806408    \n_________________________________________________________________\noutput (Dense)               (None, 9)                 81        \n=================================================================\nTotal params: 806,489\nTrainable params: 806,489\nNon-trainable params: 0\n_________________________________________________________________\n```", "```\n# Hyperparameters\n\ndiscount_rate = 0.9\nexplore_rate = 0.2\nn_episodes = 1\n\n# create the empty list to contain game memory\nmemory = deque(maxlen=1000)\n\nexperiment(env, policy_q_nn, n_episodes)\n```", "```\nPolicy:policy_q_nn, Min reward:490.0, Max reward:490.0, Average reward:490.0\n```", "```\n# Hyperparameters\n\ndiscount_rate = 0.9\nexplore_rate = 0.2\nn_episodes = 100\n\n# create the empty list to contain game memory\nmemory = deque(maxlen=1000)\n\nexperiment(env, policy_q_nn, n_episodes)\n```", "```\nPolicy:policy_q_nn, Min reward:70.0, Max reward:580.0, Average reward:270.5\n```", "```\nfrom collections import deque \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\n\n# build the CNN Q-Network\nmodel = Sequential()\nmodel.add(Conv2D(16, kernel_size=(5, 5), \n                 strides=(1, 1),\n                 activation='relu',\n                 input_shape=n_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu',name='hidden1'))\nmodel.add(Dense(9, activation='softmax', name='output'))\nmodel.compile(loss='categorical_crossentropy',optimizer='adam')\nmodel.summary()\nq_nn = model\n\n```", "```\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d_4 (Conv2D)            (None, 206, 156, 16)      1216      \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 103, 78, 16)       0         \n_________________________________________________________________\nflatten_8 (Flatten)          (None, 128544)            0         \n_________________________________________________________________\nhidden1 (Dense)              (None, 512)               65815040  \n_________________________________________________________________\noutput (Dense)               (None, 9)                 4617      \n=================================================================\nTotal params: 65,820,873\nTrainable params: 65,820,873\nNon-trainable params: 0\n```"]