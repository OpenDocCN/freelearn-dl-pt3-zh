- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression and Classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Regression and classification are two fundamental tasks ubiquitously present
    in almost all machine learning applications. They find application in varied fields
    ranging from engineering, physical science, biology, and the financial market,
    to the social sciences. They are the fundamental tools in the hands of statisticians
    and data scientists. In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difference between classification and regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Different types of linear regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification using the TensorFlow Keras API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying linear regression to estimate the price of a house
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying logistic regression to identify handwritten digits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the code files for this chapter can be found at [https://packt.link/dltfchp2](https://packt.link/dltfchp2)
  prefs: []
  type: TYPE_NORMAL
- en: Let us first start with understanding what regression really is.
  prefs: []
  type: TYPE_NORMAL
- en: What is regression?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regression is normally the first algorithm that people in machine learning work
    with. It allows us to make predictions from data by learning about the relationship
    between a given set of dependent and independent variables. It has its use in
    almost every field; anywhere that has an interest in drawing relationships between
    two or more things will find a use for regression.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the case of house price estimation. There are many factors that can
    have an impact on the house price: the number of rooms, the floor area, the locality,
    the availability of amenities, the parking space, and so on. Regression analysis
    can help us in finding the mathematical relationship between these factors and
    the house price.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us imagine a simpler world where only the area of the house determines
    its price. Using regression, we could determine the relationship between the area
    of the house (**independent variable**: these are the variables that do not depend
    upon any other variables) and its price (**dependent variable**: these variables
    depend upon one or more independent variables). Later, we could use this relationship
    to predict the price of any house, given its area. To learn more about dependent
    and independent variables and how to identify them, you can refer to this post:
    [https://medium.com/deeplearning-concepts-and-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db](https://medium.com/deeplearning-concepts-and-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db).
    In machine learning, the independent variables are normally input into the model
    and the dependent variables are output from our model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending upon the number of independent variables, the number of dependent
    variables, and the relationship type, we have many different types of regression.
    There are two important components of regression: the *relationship* between independent
    and dependent variables, and the *strength of impact* of different independent
    variables on dependent variables. In the following section, we will learn in detail
    about the widely used linear regression technique.'
  prefs: []
  type: TYPE_NORMAL
- en: Prediction using linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Linear regression** is one of the most widely known modeling techniques.
    Existing for more than 200 years, it has been explored from almost all possible
    angles. Linear regression assumes a linear relationship between the input variable
    (*X*) and the output variable (*Y*). The basic idea of linear regression is building
    a model, using training data that can predict the output given the input, such
    that the predicted output ![](img/B18331_02_001.png) is as near the observed training
    output *Y* for the input *X*. It involves finding a linear equation for the predicted
    value ![](img/B18331_02_001.png) of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_003.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where ![](img/B18331_02_004.png) are the *n* input variables, and ![](img/B18331_02_005.png)
    are the linear coefficients, with *b* as the bias term. We can also expand the
    preceding equation to:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_006.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The bias term allows our regression model to provide an output even in the
    absence of any input; it provides us with an option to shift our data for a better
    fit. The error between the observed values (*Y*) and predicted values (![](img/B18331_02_001.png))
    for an input sample *i* is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_008.png)'
  prefs: []
  type: TYPE_IMG
- en: The goal is to find the best estimates for the coefficients *W* and bias *b*,
    such that the error between the observed values *Y* and the predicted values ![](img/B18331_02_001.png)
    is minimized. Let’s go through some examples to better understand this.
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we consider only one independent variable and one dependent variable, what
    we get is a simple linear regression. Consider the case of house price prediction,
    defined in the preceding section; the area of the house (*A*) is the independent
    variable, and the price (*Y*) of the house is the dependent variable. We want
    to find a linear relationship between predicted price ![](img/B18331_02_001.png)
    and *A*, of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where *b* is the bias term. Thus, we need to determine *W* and *b*, such that
    the error between the price *Y* and the predicted price ![](img/B18331_02_001.png)
    is minimized. The standard method used to estimate *W* and *b* is called the method
    of least squares, that is, we try to minimize the sum of the square of errors
    (*S*). For the preceding case, the expression becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We want to estimate the regression coefficients, *W* and *b*, such that *S*
    is minimized. We use the fact that the derivative of a function is 0 at its minima
    to get these two equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_014.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/B18331_02_015.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These two equations can be solved to find the two unknowns. To do so, we first
    expand the summation in the second equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_016.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Take a look at the last term on the left-hand side; it just sums up a constant
    *N* time. Thus, we can rewrite it as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_017.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reordering the terms, we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_018.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The two terms on the right-hand side can be replaced by ![](img/B18331_02_019.png),
    the average price (output), and ![](img/B18331_02_020.png), the average area (input),
    respectively, and thus we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_021.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In a similar fashion, we expand the partial differential equation of *S* with
    respect to weight *W*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Substitute the expression for the bias term *b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_023.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Reordering the equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_024.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Playing around with the mean definition, we can get from this the value of
    weight *W* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_025.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where ![](img/B18331_02_019.png) and ![](img/B18331_02_020.png) are the average
    price and area, respectively. Let us try this on some simple sample data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We import the necessary modules. It is a simple example, so we’ll be using
    only NumPy, pandas, and Matplotlib:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we generate random data with a linear relationship. To make it more realistic,
    we also add a random noise element. You can see the two variables (the cause,
    `area`, and the effect, `price`) follow a positive linear dependence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Scatter plot between the area of the house and its price'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we calculate the two regression coefficients using the equations we defined.
    You can see the result is very much near the linear relationship we have simulated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us now try predicting the new prices using the obtained weight and bias
    values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we plot the predicted prices along with the actual price. You can see
    that predicted prices follow a linear relationship with the area:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![A close up of a map  Description automatically generated](img/B18331_02_02.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 2.2: Predicted values vs the actual price'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: From *Figure 2.2*, we can see that the predicted values follow the same trend
    as the actual house prices.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The preceding example was simple, but that is rarely the case. In most problems,
    the dependent variables depend upon multiple independent variables. Multiple linear
    regression finds a linear relationship between the many independent input variables
    (*X*) and the dependent output variable (*Y*), such that they satisfy the predicted
    *Y* value of the form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_003.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B18331_02_029.png) are the *n* independent input variables, and
    ![](img/B18331_02_005.png)are the linear coefficients, with *b* as the bias term.
  prefs: []
  type: TYPE_NORMAL
- en: 'As before, the linear coefficients *W*[s] are estimated using the method of
    least squares, that is, minimizing the sum of squared differences between predicted
    values (![](img/B18331_02_001.png)) and observed values (*Y*). Thus, we try to
    minimize the loss function (also called squared error, and if we divide by *n*,
    it is the mean squared error):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_032.png)'
  prefs: []
  type: TYPE_IMG
- en: where the sum is over all the training samples.
  prefs: []
  type: TYPE_NORMAL
- en: As you might have guessed, now, instead of two, we will have *n+1* equations,
    which we will need to simultaneously solve. An easier alternative will be to use
    the TensorFlow Keras API. We will learn shortly how to use the TensorFlow Keras
    API to perform the task of regression.
  prefs: []
  type: TYPE_NORMAL
- en: Multivariate linear regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There can be cases where the independent variables affect more than one dependent
    variable. For example, consider the case where we want to predict a rocket’s speed
    and its carbon dioxide emission – these two will now be our dependent variables,
    and both will be affected by the sensors reading the fuel amount, engine type,
    rocket body, and so on. This is a case of multivariate linear regression. Mathematically,
    a multivariate regression model can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_033.png)'
  prefs: []
  type: TYPE_IMG
- en: where ![](img/B18331_02_034.png) and ![](img/B18331_02_035.png). The term ![](img/B18331_02_036.png)
    represents the *j*^(th) predicted output value corresponding to the *i*^(th) input
    sample, *w* represents the regression coefficients, and *x*[ik] is the *k*^(th)
    feature of the *i*^(th) input sample. The number of equations needed to solve
    in this case will now be *n x m*. While we can solve these equations using matrices,
    the process will be computationally expensive as it will involve calculating the
    inverse and determinants. An easier way would be to use the gradient descent with
    the sum of least square error as the loss function and to use one of the many
    optimizers that the TensorFlow API includes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will delve deeper into the TensorFlow Keras API, a versatile
    higher-level API to develop your model with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Neural networks for linear regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding sections, we used mathematical expressions for calculating
    the coefficients of a linear regression equation. In this section, we will see
    how we can use the neural networks to perform the task of regression and build
    a neural network model using the TensorFlow Keras API.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before performing regression using neural networks, let us first review what
    a neural network is. Simply speaking, a neural network is a network of many artificial
    neurons. From *Chapter 1*, *Neural Network Foundations with TF*, we know that
    the simplest neural network, the (simple) perceptron, can be mathematically represented
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_037.png)'
  prefs: []
  type: TYPE_IMG
- en: where *f* is the activation function. Consider, if we have *f* as a linear function,
    then the above expression is similar to the expression of linear regression that
    we learned in the previous section. In other words, we can say that a neural network,
    which is also called a function approximator, is a generalized regressor. Let
    us try to build a neural network simple regressor next using the TensorFlow Keras
    API.
  prefs: []
  type: TYPE_NORMAL
- en: Simple linear regression using TensorFlow Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the first chapter, we learned about how to build a model in TensorFlow Keras.
    Here, we will use the same `Sequential` API to build a single-layered perceptron
    (fully connected neural network) using the `Dense` class. We will continue with
    the same problem, that is, predicting the price of a house given its area:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start with importing the packages we will need. Notice the addition of the
    `Keras` module and the `Dense` layer in importing packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we generate the data, as in the previous case:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The input to neural networks should be normalized; this is because input gets
    multiplied with weights, and if we have very large numbers, the result of multiplication
    will be large, and soon our metrics may cross infinity (the largest number your
    computer can handle):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us now build the model; since it is a simple linear regressor, we use a
    `Dense` layer with only one unit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To train a model, we will need to define the loss function and optimizer. The
    loss function defines the quantity that our model tries to minimize, and the optimizer
    decides the minimization algorithm we are using. Additionally, we can also define
    metrics, which is the quantity we want to log as the model is trained. We define
    the loss function, `optimizer` (see *Chapter 1*, *Neural Network Foundations with
    TF*), and metrics using the `compile` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that model is defined, we just need to train it using the `fit` function.
    Observe that we are using a `batch_size` of 32 and splitting the data into training
    and validation datasets using the `validation_spilt` argument of the `fit` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Well, you have successfully trained a neural network to perform the task of
    linear regression. The mean squared error after training for 100 epochs is 0.0815
    on training data and 0.074 on validation data. We can get the predicted value
    for a given input using the `predict` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we plot a graph of the predicted and the actual data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 2.3* shows the plot between the predicted data and the actual data.
    You can see that, just like the linear regressor, we have got a nice linear fit:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Predicted price vs actual price'
  prefs: []
  type: TYPE_NORMAL
- en: 'In case you are interested in knowing the coefficients `W` and `b`, we can
    do it by printing the weights of the model using `model.weights`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see from the result above that our coefficients are `W= 0.69` and bias
    `b= 0.127`. Thus, using linear regression, we can find a linear relationship between
    the house price and its area. In the next section, we explore multiple and multivariate
    linear regression using the TensorFlow Keras API.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple and multivariate linear regression using the TensorFlow Keras API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The example in the previous section had only one independent variable, the *area*
    of the house, and one dependent variable, the *price* of the house. However, problems
    in real life are not that simple; we may have more than one independent variable,
    and we may need to predict more than one dependent variable. As you must have
    realized from the discussion on multiple and multivariate regression, they involve
    solving multiple equations. We can make our tasks easier by using the Keras API
    for both tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we can have more than one neural network layer, that is, we can
    build a **deep neural network**. A deep neural network is like applying multiple
    function approximators:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_038.png)'
  prefs: []
  type: TYPE_IMG
- en: 'with ![](img/B18331_02_039.png) being the function at layer *L*. From the expression
    above, we can see that if *f* was a linear function, adding multiple layers of
    a neural network was not useful; however, using a non-linear activation function
    (see *Chapter 1*, *Neural Network Foundations with TF*, for more details) allows
    us to apply neural networks to the regression problems where dependent and independent
    variables are related in some non-linear fashion. In this section, we will use
    a deep neural network, built using TensorFlow Keras, to predict the fuel efficiency
    of a car, given its number of cylinders, displacement, acceleration, and so on.
    The data we use is available from the UCI ML repository (Blake, C., & Merz, C.
    (1998), the UCI repository of machine learning databases ([http://www.ics.uci.edu/~mlearn/MLRepository.xhtml](http://www.ics.uci.edu/~mlearn/MLRepository.xhtml)):'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by importing the modules that we will need. In the previous example,
    we normalized our data using the DataFrame operations. In this example, we will
    make use of the Keras `Normalization` layer. The `Normalization` layer shifts
    the data to a zero mean and one standard deviation. Also, since we have more than
    one independent variable, we will use Seaborn to visualize the relationship between
    different variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let us first download the data from the UCI ML repo.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The data consists of eight features: mpg, cylinders, displacement, horsepower,
    weight, acceleration, model year, and origin. Though the origin of the vehicle
    can also affect the fuel efficiency “mpg” (*miles per gallon*), we use only seven
    features to predict the mpg value. Also, we drop any rows with NaN values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We divide the dataset into training and test datasets. Here, we are keeping
    80% of the 392 datapoints as training data and 20% as test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we use Seaborn’s `pairplot` to visualize the relationship between the
    different variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see that mpg (fuel efficiency) has dependencies on all the other variables,
    and the dependency relationship is non-linear, as none of the curves are linear:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A picture containing text, electronics, display  Description automatically
    generated](img/B18331_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Relationship among different variables of auto-mpg data'
  prefs: []
  type: TYPE_NORMAL
- en: 'For convenience, we also separate the variables into input variables and the
    label that we want to predict:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we use the Normalization layer of Keras to normalize our data. Note that
    while we normalized our inputs to a value with mean 0 and standard deviation 1,
    the output prediction `''mpg''` remains as it is:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We build our model. The model has two hidden layers, with 64 and 32 neurons,
    respectively. For the hidden layers, we have used **Rectified Linear Unit** (**ReLU**)
    as our activation function; this should help in approximating the non-linear relation
    between fuel efficiency and the rest of the variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Earlier, we used stochastic gradient as the optimizer; this time, we try the
    Adam optimizer (see *Chapter 1*, *Neural Network Foundations with TF*, for more
    details). The loss function for the regression we chose is the mean squared error
    again:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we train the model for 100 epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Cool, now that the model is trained, we can check if our model is overfitted,
    underfitted, or properly fitted by plotting the loss curve. Both validation loss
    and training loss are near each other as we increase the training epochs; this
    suggests that our model is properly trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Chart, line chart  Description automatically generated](img/B18331_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: Model error'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us finally compare the predicted fuel efficiency and the true fuel efficiency
    on the test dataset. Remember that the model has not seen a test dataset ever,
    thus this prediction is from the model’s ability to generalize the relationship
    between inputs and fuel efficiency. If the model has learned the relationship
    well, the two should form a linear relationship:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Chart, scatter chart  Description automatically generated](img/B18331_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: Plot between predicted fuel efficiency and actual values'
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we can also plot the error between the predicted and true fuel
    efficiency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Chart, histogram  Description automatically generated](img/B18331_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Prediction error'
  prefs: []
  type: TYPE_NORMAL
- en: In case we want to make more than one prediction, that is, dealing with a multivariate
    regression problem, the only change would be that instead of one unit in the last
    dense layer, we will have as many units as the number of variables to be predicted.
    Consider, for example, we want to build a model which takes into account a student’s
    SAT score, attendance, and some family parameters, and wants to predict the GPA
    score for all four undergraduate years; then we will have the output layer with
    four units. Now that you are familiar with regression, let us move toward the
    classification tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Classification tasks and decision boundaries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Till now, the focus of the chapter was on regression. In this section, we will
    talk about another important task: the task of classification. Let us first understand
    the difference between regression (also sometimes referred to as prediction) and
    classification:'
  prefs: []
  type: TYPE_NORMAL
- en: In classification, the data is grouped into classes/categories, while in regression,
    the aim is to get a continuous numerical value for given data. For example, identifying
    the number of handwritten digits is a classification task; all handwritten digits
    will belong to one of the ten numbers lying between 0-9\. The task of predicting
    the price of the house depending upon different input variables is a regression
    task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a classification task, the model finds the decision boundaries separating
    one class from another. In the regression task, the model approximates a function
    that fits the input-output relationship.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification is a subset of regression; here, we are predicting classes. Regression
    is much more general.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2.8* shows how classification and regression tasks differ. In classification,
    we need to find a line (or a plane or hyperplane in multidimensional space) separating
    the classes. In regression, the aim is to find a line (or plane or hyperplane)
    that fits the given input points:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: Classification vs regression'
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will explain logistic regression, which is a very
    common and useful classification technique.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Logistic regression is used to determine the probability of an event. Conventionally,
    the event is represented as a categorical dependent variable. The probability
    of the event is expressed using the sigmoid (or “logit”) function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_040.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The goal now is to estimate weights ![](img/B18331_02_005.png) and bias term
    *b*. In logistic regression, the coefficients are estimated using either the maximum
    likelihood estimator or stochastic gradient descent. If *p* is the total number
    of input data points, the loss is conventionally defined as a cross-entropy term
    given by:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_042.png)'
  prefs: []
  type: TYPE_IMG
- en: Logistic regression is used in classification problems. For example, when looking
    at medical data, we can use logistic regression to classify whether a person has
    cancer or not. If the output categorical variable has two or more levels, we can
    use multinomial logistic regression. Another common technique used for two or
    more output variables is one versus all.
  prefs: []
  type: TYPE_NORMAL
- en: 'For multiclass logistic regression, the cross-entropy loss function is modified
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18331_02_043.png)'
  prefs: []
  type: TYPE_IMG
- en: where *K* is the total number of classes. You can read more about logistic regression
    at [https://en.wikipedia.org/wiki/Logistic_regression](https://en.wikipedia.org/wiki/Logistic_regression).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have some idea about logistic regression, let us see how we can
    apply it to any dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Logistic regression on the MNIST dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, we will use TensorFlow Keras to classify handwritten digits using logistic
    regression. We will be using the **MNIST** (**Modified National Institute of Standards
    and Technology**) dataset. For those working in the field of deep learning, MNIST
    is not new, it is like the ABC of machine learning. It contains images of handwritten
    digits and a label for each image, indicating which digit it is. The label contains
    a value lying between 0-9 depending on the handwritten digit. Thus, it is a multiclass
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: To implement the logistic regression, we will make a model with only one dense
    layer. Each class will be represented by a unit in the output, so since we have
    10 classes, the number of units in the output would be 10\. The probability function
    used in the logistic regression is similar to the sigmoid activation function;
    therefore, we use sigmoid activation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us build our model:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is, as always, importing the modules needed. Notice that here
    we are using another useful layer from the Keras API, the `Flatten` layer. The
    `Flatten` layer helps us to resize the 28 x 28 two-dimensional input images of
    the MNIST dataset into a 784 flattened array:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We take the input data of MNIST from the `tensorflow.keras` dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we preprocess the data. We normalize the images; the MNIST dataset images
    are black and white images with the intensity value of each pixel lying between
    0-255\. We divide it by 255, so that now the values lie between 0-1:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we define a very simple model; it has only one `Dense` layer with `10`
    units, and it takes an input of size 784\. You can see from the output of the
    model summary that only the `Dense` layer has trainable parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since the test labels are integral values, we will use `SparseCategoricalCrossentropy`
    loss with `logits` set to `True`. The optimizer selected is Adam. Additionally,
    we also define accuracy as metrics to be logged as the model is trained. We train
    our model for 50 epochs, with a train-validation split of 80:20:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us see how our simple model has fared by plotting the loss plot. You can
    see that since the validation loss and training loss are diverging, as the training
    loss is decreasing, the validation loss increases, thus the model is overfitting.
    You can improve the model performance by adding hidden layers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Chart, line chart  Description automatically generated](img/B18331_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: Loss plot'
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand the result, we build two utility functions; these functions
    help us in visualizing the handwritten digits and the probability of the 10 units
    in the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using these utility functions, we plot the predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The plot on the left is the image of the handwritten digit, with the predicted
    label, the confidence in the prediction, and the true label. The image on the
    right shows the probability (logistic) output of the 10 units; we can see that
    the unit which represents the number 4 has the highest probability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![A picture containing logo  Description automatically generated](img/B18331_02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: Predicted digit and confidence value of the prediction'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this code, to stay true to logistic regression, we used a sigmoid activation
    function and only one `Dense` layer. For better performance, adding dense layers
    and using softmax as the final activation function will be helpful. For example,
    the following model gives 97% accuracy on the validation dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can experiment by adding more layers, or by changing the number of neurons
    in each layer, and even changing the optimizer. This will give you a better understanding
    of how these parameters influence the model performance.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter dealt with different types of regression algorithms. We started
    with linear regression and used it to predict house prices for a simple one-input
    variable case. We built simple and multiple linear regression models using the
    TensorFlow Keras API. The chapter then moved toward logistic regression, which
    is a very important and useful technique for classifying tasks. The chapter explained
    the TensorFlow Keras API and used it to implement both linear and logistic regression
    for some classical datasets. The next chapter will introduce you to convolutional
    neural networks, the most commercially successful neural network models for image
    data.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are some good resources if you are interested in knowing more about the
    concepts we’ve covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow website: [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Exploring bivariate numerical data*: [https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data](https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Murphy, K. P. (2022). *Probabilistic Machine Learning: An introduction*, MIT
    Press.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Blake, C., & Merz, C. (1998). UCI repository of machine learning databases:
    [http://www.ics.uci.edu/~mlearn/MLRepository.xhtml](http://www.ics.uci.edu/~mlearn/MLRepository.xhtml%20)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 2000 members at: [https://packt.link/keras](https://packt.link/keras)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code18312172242788196871.png)'
  prefs: []
  type: TYPE_IMG
