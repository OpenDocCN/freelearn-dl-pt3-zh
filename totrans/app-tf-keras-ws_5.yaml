- en: Appendix
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Chapter 01: Introduction to Neural Networks and Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 1.01: Training a Neural Network with Different Hyperparameters'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your Terminal, navigate to the directory cloned from [https://packt.live/2ZVyf0C](https://packt.live/2ZVyf0C)
    and execute the following command to start TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.15: A screenshot of a Terminal after starting a TensorBoard instance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_01_15.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.15: A screenshot of a Terminal after starting a TensorBoard instance'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, open the URL provided by TensorBoard in your browser. You should be able
    to see the TensorBoard `SCALARS` page:![Figure 1.16: A screenshot of the TensorBoard
    SCALARS page'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_01_16.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.16: A screenshot of the TensorBoard SCALARS page'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the TensorBoard page, click on the `SCALARS` page and enlarge the `epoch_accuracy`
    graph. Now, move the smoothing slider to `0.6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The accuracy graph measures how accurately the network was able to guess the
    labels of a test set. At first, the network guesses those labels completely incorrectly.
    This happens because we have initialized the weights and biases of our network
    with random values, so its first attempts are a guess. The network will then change
    the weights and biases of its layers on a second run; the network will continue
    to invest in the nodes that give positive results by altering their weights and
    biases and will penalize those that don't by gradually reducing their impact on
    the network (eventually reaching `0`). As you can see, this is an efficient technique
    that quickly yields great results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To train another model by changing various hyperparameters, open a Terminal
    in `Chapter01/Activity1.01`. Activate the environment. Change the following lines
    in `mnist.py`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `mnist.py` file will look as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.17: A screenshot of the mnist.py file and the hyperparameters to
    change'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_01_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.17: A screenshot of the mnist.py file and the hyperparameters to change'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now repeat *steps 1-3* for this newly trained model. Start TensorFlow, open
    the `Scalar` page with the URL seen on TensorBoard, and view the `epoch_accuracy`
    graph on the `Scalar` page. You will see the difference compared to the earlier
    graphs:![Figure 1.18: A screenshot from TensorBoard showing the parameters specified
    in step 4'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_01_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.18: A screenshot from TensorBoard showing the parameters specified
    in step 4'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now repeat *step 4*. Open a Terminal in `Chapter01/Activity1.01`. Activate
    the environment. Change the following lines in `mnist.py` to the following values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Visualize the results. You will get graphs like these:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 1.19: A screenshot of the TensorBoard graphs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_01_19.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 1.19: A screenshot of the TensorBoard graphs'
  prefs: []
  type: TYPE_NORMAL
- en: Now try running the model with any of your custom values and see how the graph
    changes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Use the `mnist.py` file for your reference at [https://packt.live/2ZVyf0C](https://packt.live/2ZVyf0C).
  prefs: []
  type: TYPE_NORMAL
- en: There are many other parameters that you can modify in your neural network.
    For now, experiment with the epochs and the learning rate of your network. You
    will notice that those two on their own can greatly change the output of your
    networkâ€”but only by so much. Experiment to see if you can train this network faster
    with the current architecture just by altering those two parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3eiFdC3](https://packt.live/3eiFdC3).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
- en: Verify how your network is training using TensorBoard. Alter those parameters
    a few more times by multiplying the starting values by 10 until you notice that
    the network is improving. This process of tuning the network and finding improved
    accuracy is essentially what is used in industry applications today to improve
    existing neural network models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 02: Real-World Deep Learning: Predicting the Price of Bitcoin'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 2.01: Assembling a Deep Learning System'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will continue to use Jupyter Notebooks and the data prepared in previous
    exercises of chapter 2 (`data/train_dataset.csv`), as well as the model that we
    stored locally (`bitcoin_ lstm_v0.h5`):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the libraries required to load and train the deep learning model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `close_point_relative_normalization` variable will be used to train our
    LSTM model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We will start by loading the dataset we prepared during our previous activities.
    We'll use pandas to load that dataset into memory.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the training dataset into memory using pandas, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, quickly inspect the dataset by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As explained in this chapter, LSTM networks require tensors with three dimensions.
    These dimensions are period length, the number of periods, and the number of features.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, proceed to create weekly groups, and then rearrange the resulting array
    to match those dimensions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Feel free to use the `create_groups()` function provided to perform this operation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The default values for that function are `7` days.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, make sure you split the data into two sets: training and validation. We
    do this by assigning the last week from the Bitcoin prices dataset to the evaluation
    set. We then train the network to evaluate that last week. Separate the last week
    of the training data and reshape it using `numpy.reshape()`. Reshaping it is important,
    as the LSTM model only accepts data organized in this way:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our data is now ready to be used in training. Now, we load our previously saved
    model and train it with a given number of epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Navigate to the `Load Our Model` header and load our previously trained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'And now, train that model with our training data, `X_train` and `Y_validation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice that we store the logs of the model in a variable called `history`.
    The model logs are useful for exploring specific variations in its training accuracy
    and observing how well the loss function is performing:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.27: Section of the Jupyter notebook where we load our earlier model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: and train it with new data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.27: Section of the Jupyter Notebook where we load our earlier model
    and train it with new data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, let''s make a prediction with our trained model. Using the same data,
    `X_train`, call the following method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The model immediately returns a list of normalized values with the prediction
    for the next 7 days. Use the `denormalize()` function to turn the data into US
    dollar values. Use the latest values available as a reference for scaling the
    predicted results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.28: Projection of Bitcoin prices for 7 days in the future'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: using the LSTM model we just built
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_02_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.28: Projection of Bitcoin prices for 7 days in the future using the
    LSTM model we just built'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We combine both time series in this graph: the real data (before the vertical
    line) and the predicted data (after the vertical line). The model shows a variance
    similar to the patterns seen before and it suggests a price increase during the
    following 7-day period.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When you are done experimenting, save your model with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We will save this trained network for future reference and compare its performance
    with other models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The network may have learned patterns from our data, but how can it do that
    with such a simple architecture and so little data? LSTMs are powerful tools for
    learning patterns from data. However, we will learn in our next sessions that
    they can also suffer from overfitting, a phenomenon common in neural networks,
    in which they learn patterns from the training data that are useless when predicting
    real-world patterns. We will learn how to deal with that and how to improve our
    network to make useful predictions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZWfqub](https://packt.live/2ZWfqub).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3gIhhcT](https://packt.live/3gIhhcT).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 3: Real-World Deep Learning: Evaluating the Bitcoin Model'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 3.01: Optimizing a Deep Learning Model'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your Terminal, start a TensorBoard instance by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see the `SCALARS` page once TensorBoard opens in the browser:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.20: Screenshot of a TensorBoard showing SCALARS page'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_03_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.20: Screenshot of a TensorBoard showing SCALARS page'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Open the URL that appears on screen and leave that browser tab open as well.
    Also, start a Jupyter Notebook instance with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here''s the screenshot showing the Jupyter Notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.21: Jupyter Notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_03_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.21: Jupyter Notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Open the URL that appears in a different browser window.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, open the Jupyter Notebook called `Activity3.01_Optimizing_a_deep_learning_model.ipynb`
    and navigate to the title of the Notebook. Run the cell to, import all the required
    libraries.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set the seed to avoid randomness:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will load the train and test data like we did in the previous activities.
    We will also split it into train and test groups using the `split_lstm_input()`
    utility function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.22: Screenshot showing results of loading datasets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_03_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.22: Screenshot showing results of loading datasets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In each section of this Notebook, we will implement new optimization techniques
    in our model. Each time we do so, we'll train a fresh model and store its trained
    instance in a variable that describes the model version. For instance, our first
    model, `bitcoin_lstm_v0`, is called `model_v0` in this Notebook. At the very end
    of the Notebook, we'll evaluate all the models using MSE, RMSE, and MAPE.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To get these models up and running, execute the cells under the **Reference
    Model** section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, in the open Jupyter Notebook, navigate to the `Adding Layers and Nodes`
    header. You will recognize our first model in the next cell. This is the basic
    LSTM network that we built in *Chapter 2*, *Real-World Deep Learning with TensorFlow
    and Keras: Predicting the Price of Bitcoin*. Now, we have to add a new LSTM layer
    to this network:![Figure 3.23: Jupyter Notebook with code for adding new LSTM
    layer'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_03_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.23: Jupyter Notebook with code for adding new LSTM layer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using your knowledge from this chapter, go ahead and add a new LSTM layer and
    then compile and train the model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'While training your models, remember to frequently visit the running TensorBoard
    instance. You will be able to see each model run and compare the results of their
    loss functions there:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.24: Output of the loss function for different models'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15911_03_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.24: Output of the loss function for different models'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The TensorBoard instance displays many different model runs. TensorBoard is
    really useful for tracking model training in real time.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In this section, we are interested in exploring different magnitudes of epochs.
    Use the `train_model()` utility function to name different model versions and
    runs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Train the model with a few different epoch parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, you are interested in making sure the model doesn't overfit the
    training data. You want to avoid this, because if it does, it will not be able
    to predict patterns that are represented in the training data but have different
    representations in the test data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After you are done experimenting with epochs, move to the next optimization
    technique: activation functions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, navigate to the `Activation Functions` header in the Notebook. In this
    section, you only need to change the following variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have used the `tanh` function in this section, but feel free to try other
    activation functions. Review the list available at [https://keras.io/activations/](https://keras.io/activations/)
    and try other possibilities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Our final option is to try different regularization strategies. This is notably
    more complex and may take a few iterations for us to notice any gainsâ€”especially
    with so little data. Also, adding regularization strategies typically increases
    the training time of your network.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, navigate to the `Regularization Strategies` header in the Notebook. In
    this section, you need to implement the `Dropout()` regularization strategy. Find
    the right place to put that step and implement it in our model:![Figure 3.25:
    Jupyter Notebook showing code for regularization strategies'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_03_25.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3.25: Jupyter Notebook showing code for regularization strategies'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also try L2 regularization here (or combine both). Do the same as you
    did with `Dropout()`, but now using `ActivityRegularization(l2=0.0001)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, let's evaluate our models using RMSE and MAPE.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, navigate to the `Evaluate Models` header in the Notebook. In this section,
    we will evaluate the model predictions for the next 19 weeks of data in the test
    set. Then, we will compute the RMSE and MAPE of the predicted series versus the
    test series.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'First plot looks as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 3.26: Prediction series versus test series #1'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.26: Prediction series versus test series #1'
  prefs: []
  type: TYPE_NORMAL
- en: 'Second plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.27: Prediction series versus test series #2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.27: Prediction series versus test series #2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Third plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.28: Prediction series versus test series #3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.28: Prediction series versus test series #3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fourth plot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.29: Prediction series versus test series #4'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.29: Prediction series versus test series #4'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fifth plot will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.30: Prediction series versus test series #5'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15911_03_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3.30: Prediction series versus test series #5'
  prefs: []
  type: TYPE_NORMAL
- en: We have implemented the same evaluation techniques from *Exercise 2.01,* *Exploring
    Bitcoin Dataset*, [https://packt.live/3ehbgCi](https://packt.live/3ehbgCi), all
    wrapped in utility functions. Simply run all the cells from this section until
    the end of the Notebook to see the results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZgAo87](https://packt.live/2ZgAo87).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3ft5Wgk](https://packt.live/3ft5Wgk).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we used different evaluation techniques to get more accurate
    results. We tried to train for more epochs, changed the activation function, added
    regularization, and compared results in different scenarios. Take this opportunity
    to tweak the values for the preceding optimization techniques and attempt to beat
    the performance of that model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Chapter 4: Productization'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Activity 4.01: Deploying a Deep Learning Application'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Solution:'
  prefs: []
  type: TYPE_NORMAL
- en: In this activity, we deploy our model as a web application locally. This allows
    us to connect to the web application using a browser or to use another application
    through the application's HTTP API. You can find the code for this activity at
    [https://packt.live/2Zdor2S](https://packt.live/2Zdor2S).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using your Terminal, navigate to the `cryptonic` directory and build the Docker
    images for all the required components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Those two commands build the two images that we will use in this application:
    `cryptonic` (containing the Flask application) and `cryptonic-cache` (containing
    the Redis cache).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After building the images, identify the `docker-compose.yml` file and open
    it in a text editor. Change the `BITCOIN_START_DATE` parameter to a date other
    than `2017-01-01`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As a final step, deploy your web application locally using `docker-compose
    up`, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see a log of activity on your Terminal, including the training epochs
    completed by your model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'After the model has been trained, you can visit your application at `http://
    localhost:5000` and make predictions at `http://localhost:5000/predict`:![Figure
    4.7: Screenshot of the Cryptonic application deployed locally'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B15911_04_07.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 4.7: Screenshot of the Cryptonic application deployed locally'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Zg0wjd](https://packt.live/2Zg0wjd).
  prefs: []
  type: TYPE_NORMAL
- en: This section does not currently have an online interactive example, and will
    need to be run locally.
  prefs: []
  type: TYPE_NORMAL
