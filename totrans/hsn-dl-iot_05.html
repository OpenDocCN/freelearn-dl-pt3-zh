<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Image Recognition in IoT</h1>
                </header>
            
            <article>
                
<p>Many IoT applications, including smart homes, smart cities, and smart healthcare, will extensively use image recognition-based <span>decision-making </span><span>(such as facial recognition for a smart door or lock) in the future. <strong>Machine learning</strong> (<strong>ML</strong>) and <strong>deep learning</strong> (<strong>DL</strong>) algorithms are useful for image recognition and decision-making. Consequently, they are very promising for IoT applications. This chapter will cover hands-on DL-based image data processing for IoT applications.</span></p>
<p>The first part of this chapter will briefly describe different IoT applications and their image detection-based decision-making. Furthermore, it will briefly discuss an IoT application and its image detection-based implementation in a real-world scenario. In the second part of the chapter, we shall present a hands-on image detection implementation of an application using a DL algorithm. In this chapter, we will cover the following topics:</p>
<ul>
<li>IoT applications and image recognition</li>
<li>Use case one: Image-based road fault detection</li>
<li><span>Use case two: Image-based smart solid waste separation</span></li>
<li>Implementing the use cases</li>
<li>Transfer learning for image recognition in IoT</li>
<li>CNNs for image recognition in IoT applications</li>
<li>Collecting data</li>
<li>Data pre-processing</li>
<li>Model training</li>
<li>Evaluating the models</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">IoT applications and image recognition</h1>
                </header>
            
            <article>
                
<p>The image recognition landscape in IoT applications is rapidly changing. Significant advances in mobile processing power, edge computing, and machine learning are paving the way for the widespread use of image recognition in many IoT applications. For example, omnipresent mobile devices (which are a key components in many IoT applications) equipped with high-resolution cameras facilitate the generation of images and videos by everyone, everywhere.</p>
<p>Moreover, intelligent video cameras, such as IP cameras and Raspberry Pis with cameras, are used in many places, such as smart homes, campuses, and factories, for different applications. Many IoT applications—including smart cities, smart homes, smart health, smart education, smart factories, and smart agriculture—make decisions using image recognition/classification. As shown in the <span><span>following diagram, </span></span>these applications use one or more of the following image recognition services:</p>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-1085 image-border" src="assets/c5027b9b-10a4-4fb5-9d03-75004e180697.png" style="width:50.33em;height:20.67em;"/></p>
<p class="mce-root"/>
<p>Let's us discuss the previous image in detail:</p>
<ul>
<li><strong>People Identification</strong>: Generally, secure and friendly access to home, office, and any other premises is a challenging task. The use of smart devices, including IoT solutions, can offer secure and friendly access to many premises. Let's consider the example of office or home access. We use one or more keys access to our homes or offices. If we lose these keys, this could not only inconvenience us but put our security at risk if somebody else finds them. In this context, image recognition-based people identification can be used as a keyless access method for a smart home or office.</li>
<li><strong>Object Identification</strong>: IoT-based automated object identification is highly desirable in many domains, including driverless cars, smart cities, and smart factories. For example, smart city applications, such as smart vehicle license plate recognition and vehicle detection, as well as city-wide public asset monitoring, can use image recognition-based object detection services. Similarly, a smart factory can use the object detection service for inventory management.</li>
<li><strong>Facial Recognition</strong>: The image processing-based facial detection and recognition landscape is changing so rapidly that it will be a commodity soon. Smartphones with biometrics will then be the norm. Smartphones and IoT-based facial recognition can be used in many applications, such as safety and security, and smart education. For example, in a smart class (education), a face recognition system can be used to identify the response to a lecture.</li>
<li><strong>Event Detection</strong>: Symptoms of many human diseases (such as  hand foot mouth), animal diseases (such as, foot and mouth, and poultry diseases), and plant diseases are explicit and visible. These diseases can be digitally detected using IoT solutions integrated with DL-based image classification.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case one – image-based automated fault detection</h1>
                </header>
            
            <article>
                
<p>Public assets (such as roads, public buildings, and tourist places) in a city are heterogeneous and distributed within the city. Most cities in the world face challenges in monitoring, fault detection, and reporting these assets. For example, in many UK cities, citizens often report faults, but the accuracy and efficiency of the reporting is an issue in many cases. In a smart city, these assets can be monitored, and their faults can be detected and reported through an IoT application. For example, a vehicle (such as a city council vehicle) attached with one or more sensors (such as a camera or a mic) can be used for the road fault monitoring and detection.</p>
<p class="mce-root"/>
<p>Roads are important assets in a city, and they have many faults. Potholes, bumps, and road roughness are some of the most frustrating hazards and anomalies experienced by commuters and vehicles. Importantly, vehicles may frequently face suspension problems, steering misalignment, and punctures, which could also lead to accidents. The cost of road-fault-related damages is significant. For example, pothole-related damage alone cost UK drivers £1.7 billion a year. An IoT application with the support of an appropriate DL algorithm can be used to automatically detect these faults and report them appropriately. This reduces the number of road-fault-related damage in a cost-effective way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing use case one</h1>
                </header>
            
            <article>
                
<p>As shown in <span><span>the following diagram</span></span>, the implementation of the use case consists of three main elements:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c7204812-461d-4169-8ae5-af23821b9b21.png" style="width:35.75em;height:29.08em;"/></p>
<p>Let us learn about the components in detail:</p>
<ul>
<li><strong>Sensors and data gathering</strong>:<strong> </strong>The selection of sensors for data gathering depends on the assets and fault types. If we use a smartphone as the edge-computing device, its camera can be used for sensing and data gathering about road faults. On the contrary, if we use Raspberry Pi as the edge-computing device, we need to use an external camera, as there is no built-in camera within the Raspberry Pi. The preceding diagram shows the Raspberry Pi and camera used for the use case implementation. We used a Raspberry Pi 3 model B+ with 1 GB RAM and a 5-megapixel sensor with an Omnivision OV5647 sensor in a fixed-focus lens. The sampling or photographic rate of the camera will depend on the vehicle's speed and the availability of a road's faults. For example, if the smartphone camera or the camera installed on the vehicle can capture one picture a second, the phone or  Raspberry Pi will be able to detect the faults within two seconds if the speed of the vehicle is 40 km/h or less. Once the image is sensed and captured, it will be sent to the detection method.</li>
<li><strong>Fault detection and reporting</strong>:<strong> </strong>In this phase, the edge-computing device will be installed with one app. The installed app in a smartphone or Raspberry Pi will be loaded with pre-trained fault detection and a classification model. Once the vehicle’s smartphone or Raspberry Pi camera takes a picture (following a sampling rate), these models will detect and classify a potential fault and report to the application server (local council).</li>
<li><strong>Council's server and Fault Detection Model</strong>:<strong> </strong>The council's server is responsible for the following:
<ul>
<li>Learning the model for fault detection and classification using reference datasets</li>
<li>Disseminating and updating the models for the edge-computing device</li>
<li>Receiving and storing the fault data</li>
</ul>
</li>
</ul>
<div>
<p>Image-based model learning and validation of road's fault detection is at the heart of the implementation. The second part (covered in the sections starting from <em>Transfer learning for image recognition in IoT</em>) of the chapter will describe the implementation of the DL-based anomaly detection of the previous use case. All the necessary codes are available in the chapter's code folder.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Use case two – image-based smart solid waste separation</h1>
                </header>
            
            <article>
                
<p>Solid waste is a global challenge. The management of solid waste is expensive, and improper waste management is <span>also </span><span>seriously impacting the global economy, public health, and the environment. Generally, solid waste, such as plastic, glass bottles, and paper, are recyclable, and they need an effective recycling method to become economically and environmentally beneficial. However, in most countries, the existing recycling processes are done manually. In addition, citizens or consumers often become confused about the recycling method.</span></p>
<p>In this context, IoT with the support of machine learning and deep learning, especially image-based object recognition, can <span><span>identify</span></span> the type of waste and help sort it accordingly without any human intervention.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Implementing use case two </h1>
                </header>
            
            <article>
                
<p>The implementation of image-based smart solid waste separation includes two key components:</p>
<ul>
<li>A bin with an individual chamber with a controllable lid for each type of solid waste</li>
<li>An IoT infrastructure with a DL model for image recognition</li>
</ul>
<p>The first component of the implementation is not within the scope of this book, and we are considering the component as available for this implementation. As shown in the following diagram, the IoT implementation of the use case consists of two main elements:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/58f00fef-94d2-4c5c-beff-e05f95948f48.png" style="width:34.25em;height:15.58em;"/></p>
<ul>
<li><strong>Sensors and data gathering</strong>: Selection of sensors for data gathering depends on the types of solid waste and their features. For example, many glass and plastic bottles are very similar in color and appearance. However, their weights are generally distinctly different. For the use case, we are considering two sensors:
<ul>
<li>One or more cameras to capture an image of trash when it enters into a bin through the entry point</li>
<li>A weight sensor to get the weight of the trash</li>
</ul>
</li>
</ul>
<p style="padding-left: 60px">We use Raspberry Pi as the computing platform. The use case was tested using a Raspberry Pi 3 model B+ with 1 GB RAM and a 5 megapixel sensor with an Omnivision OV5647 sensor in a fixed-focus lens. Once the image and weight are sensed and captured, they are sent to the sorting method.</p>
<ul>
<li><strong>Trash detection and sorting</strong>: This is the key element of the implementation. The Raspberry Pi will be loaded with a pretrained trash detection and sorting model using DL. Once the detection algorithm detects trash and sorts it, it will actuate the control system to open the appropriate lid and move it into the bin.</li>
</ul>
<p>The use case scenario is focusing on waste management in urban public areas, including parks, tourist attractions, landscaping, and other recreational areas. Generally, citizens and/or visitors in these areas individually dispose of their waste. Importantly, they dispose of items in small numbers, from single items to just a few items.</p>
<p><span>All of the following sections will describe the implementation of the DL-based image recognition needed for the aforementioned use cases. All the necessary codes are available in the chapter's code folder.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Transfer learning for image recognition in IoT</h1>
                </header>
            
            <article>
                
<p><span>Generally, transfer learning means transferring pre-trained machine learning model representations to another problem. In recent years, this is becoming a popular means of applying DL models to a problem, especially in image processing and recognition, as it enables training a DL model with comparatively little data.</span></p>
<p>The following <span><span>diagram </span></span>shows two models:</p>
<ul>
<li><span>An architecture for a standard DL model (a)</span></li>
<li><span>An architecture for a transfer-learning DL model (b):</span></li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/adc41d97-4ee5-461a-8bca-ef6bbabcea35.png"/></p>
<p>As <span>shown in the figure of an architecture for a standard DL model</span>, a fully trained neural net takes input values in an initial layer and then sequentially feeds this information forward with necessary transformation until the second-to-last layer (which is also known as the <strong>bottleneck layer</strong>) has constructed a high-level representation of the input that can more easily be transformed into a final output. The complete training of the model involves the optimization of weight and bias terms used in each connection (labeled in blue). In<span> large and heterogeneous datasets, the number</span> of these weight and bias terms could be in the millions.</p>
<p>In transfer learning, we can use the early and middle layers and only re-train the latter layers. One popular approach to transfer learning is to reuse the pre-trained weights for the whole network other than the last layer and relearn the weights of the last layer or classification part by retraining the network using the new dataset. As shown <span>in the diagram of an architecture for a transfer-learning DL model</span>, we reused the orange connections and retrained the network using the new dataset to learn the last layer’s green connections.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Many pre-trained DL models, including the Inception-v3 and MobileNets models, are available to be used for transfer learning. The Inception-v3 model, which was trained for the ImageNet <em>Large Visual Recognition Challenge</em>, classifies images into 1,000 classes, such as <em>Zebra</em>, <em>Dalmatian</em>, and <em>Dishwasher</em>. Inception-v3 consists of two parts:</p>
<ul>
<li>A feature extraction part with a convolutional neural network, which extracts features from the input</li>
<li>A classification part with fully connected and softmax layers, which classifies the input data based on the features identified in part one</li>
</ul>
<p>If we want to use Inception-v3, we can reuse the feature extraction part and re-train the classification part with our dataset.</p>
<p>Transfer learning offers two benefits:</p>
<ul>
<li>Training on new data is faster.</li>
<li>The ability to solve a problem with less training data rather than learning from scratch.</li>
</ul>
<p>These features of transfer learning are especially useful for the implementation of DL models in IoT's resource-constrained edge devices, as we do not need to train the resource-hungry feature extraction part. Thus, the model can be trained using less computational resources and time. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">CNNs for image recognition in IoT applications</h1>
                </header>
            
            <article>
                
<p><span>A <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>) has different implementations. <strong>AlexNet</strong> is one such implementation, and it won the ImageNet Challenge: ILSVRC 2012. Since then, CNNs have become omnipresent in computer vision and image detection and classification. Until April 2017, the general trend was to make deeper and more complicated networks to achieve higher accuracy. However, these deeper and complex networks offered improved accuracy but did not always make the networks more efficient, particularly in terms of size and speed. In many real-world applications, especially in IoT applications, such as a self-driving car and patient monitoring, recognition tasks need to be accomplished in a timely fashion on a resource-constrained (processing, memory) platform.</span></p>
<p class="mce-root"/>
<p><span>In this context, MobileNet V1 was introduced in April 2017. This version of Mobilenet was an improvement on its second version (MobileNetV2) in April 2018. <strong>Mobilenets</strong> and their variants are the efficient CNN DL model's IoT applications, especially for image recognition-based IoT applications. In the following paragraphs, we present a brief overview of MobileNets.</span></p>
<p>MobileNets are the implementations of most popular and widely used DL models, namely CNNs. They are especially designed for resource-constrained mobile devices to support classification, detection, and prediction. Personal mobile devices, including smartphones, wearable devices, and smartwatches, installed with DL models improve user experience, offering any time, anywhere access, with the additional benefits of security, privacy, and energy consumption. Importantly, new emerging applications in mobile devices will need ever-more efficient neural networks to interact with the real world in real time.</p>
<p><span>The following diagram shows how the standard convolutional filters (figure a) are replaced by two layers in Mobilenet V1. It uses a depthwise convolution (figure b) and a pointwise convolution (figure c) to build a depthwise separable filter:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2294acd6-1260-41c1-80fc-d142b3d6cb0a.jpg"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>The main motivation of MobileNet V1 is that convolutional layers are expensive to compute, and they can be replaced by so-called <strong>depthwise separable convolutions</strong>. In MobileNet V1, the depthwise convolution process uses a single filter to every input channel, and the pointwise convolution then uses a 1 x 1 convolution process to the outputs of the earlier depthwise convolution. <span>As </span><span>shown in the diagram of a standard convolution filter</span>, a standard convolution both filters and combines inputs into a new set of outputs in one step. Unlike standard CNNs, the depthwise separable convolution (factorized) in MobileNets splits this into two layers (as shown in the diagram of Mobilenet V1): a layer for filtering and a separate layer for combining. </p>
<p>The following diagram presents the factorized architecture of Mobilenet V1. This factorization drastically reduces computation and model size as the model needs to calculate a significantly smaller number of parameters. For example, MobileNet V1 needs to calculate 4.2 million parameters, whereas a full convolution network needs to calculate 29.3 million parameters:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/6aac716a-4376-421f-b2ee-b1bf9f081ed6.png"/></div>
<p>MobileNet V2 is an updated and significantly improved version of MobileNet V1. It has greatly improved and pushes existing mobile visual recognition, including classification, detection, and semantic segmentation. Like <span>MobileNet V1, the</span> MobileNet V2 was released as part of the TensorFlow-Slim Image Classification Library. If needed, you can explore this in Google's Colaboratory. In addition, MobileNet V2 is available as modules on TF-Hub, and pre-trained checkpoints or saved models can be found at <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank">https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet</a> <a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank">and can be used as transfer learning.</a></p>
<p class="mce-root"/>
<p><a href="https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet" target="_blank"/></p>
<p>The following diagram presents a simple architecture of MobileNet V2.  <span>MobileNet V2  has been developed as an extension of MobileNet V1. It uses depth-wise separable convolution as efficient building blocks. In addition, MobileNet V2 includes two new features in the architecture. One is the linear bottlenecks between the layers, and the other one is shortcut connections between the bottlenecks:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b80688e0-0d3f-465f-b2ff-976064b84f48.png" style="width:47.25em;height:25.33em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting data for use case one</h1>
                </header>
            
            <article>
                
<p>We can collect data using a smartphone camera or a Raspberry Pi camera and prepare the dataset by ourselves, or download existing images from the internet (that is, via Google, Bing, and so on) and prepare the dataset. Alternatively, we can use an existing open source dataset. For use case one, we have used a combination of both. We have downloaded an existing dataset on pothole images (one of the most common road faults) from and updated the dataset with more images from Google images. The open source dataset (<kbd>PotDataset</kbd>) for pothole recognition was published by Cranfield University, UK. The dataset includes images of pothole objects and non-pothole objects, including manholes, pavements, road markings<span>, and shadows. The images were manually annotated and organized into the following folders:</span></p>
<ul>
<li>Manhole</li>
<li>Pavement</li>
<li>Pothole</li>
<li>Road markings</li>
<li>Shadow</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring the dataset from use case one</h1>
                </header>
            
            <article>
                
<p>It is essential to explore the dataset before applying DL algorithms to the data. For the exploration, we can run <kbd>image_explorer.py</kbd> on the dataset as follows:</p>
<pre>python image_explorer.py datset_original</pre>
<p class="CDPAlignLeft CDPAlign">The following diagram presents a snapshot of the data exploration process:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/07f715ce-8c41-4106-95fe-475b550a1361.jpg" style="width:49.67em;height:39.08em;"/></p>
<p>As shown in the diagram of data exploration, the differences between pothole and non-pothole objects<span> are not always obvious if we are using only the smartphone camera. A combination of an IR and smartphone camera can improve the situation. In addition, we found that the pothole images we have used here might not be enough to cover a wide range of potholes such as the following:</span></p>
<ul>
<li><span>Many</span> images in the used dataset show that the potholes are already maintained/fixed. </li>
<li>There are a few images of a large-sized pothole in the used dataset.</li>
</ul>
<p>In this context, we decided to update the pothole images dataset by collecting more images from the internet. Next, we briefly discuss the data collection process:</p>
<ol>
<li><strong>Search</strong>:<strong> </strong>Use any browser (we used Chrome), go to Google, and search for <em>pothole images</em> in Google Images. Your search window will look like the following screenshot:</li>
</ol>
<div class="packt_infobox"><span>You can select copyright-free images by clicking on <em>Tools</em> and changing the usage rights to <em>Labeled for reuse with modification.</em> </span></div>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-753 image-border" src="assets/e95a92bb-cdea-48e0-a81d-acbe3b9b806b.png" style="width:55.08em;height:37.00em;"/></p>
<ol start="2">
<li class="CDPAlignLeft CDPAlign"><strong>Gathering Images URLs</strong>: This step is to use a few lines of JavaScript code to gather the image URLs. The gathered URLs can be used in Python to download the images. As shown in the following screenshot, select the JavaScript console (assuming you use the Chrome web browser, but you can use Firefox as well) by clicking <strong>View</strong> | <strong>Developer </strong>| <strong>JavaScript Console</strong> (in macOS) and customize and control <strong>Google Chrome</strong> | <strong>More tools</strong> | <strong>Developer tools</strong> (in Windows OS):</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-754 image-border" src="assets/8dc8d211-0d45-458c-a81c-fa9da19a824d.png" style="width:56.42em;height:39.58em;"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">Once you have selected the JavaScript console, you will see a browser window such as the following screenshot, and this will enable you to execute JavaScript in a REPL-like manner:</p>
<div class="CDPAlignCenter CDPAlign"><img class="aligncenter size-full wp-image-755 image-border" src="assets/8dd135fa-09e2-46a8-9a84-986e8a490d14.png" style="width:57.42em;height:38.58em;"/></div>
<ol start="3">
<li>Now do the following in order:
<ul>
<li>Scroll the page and go down until you have found all useful images (note: please use images that are not subject to copyright) for your dataset. After that, you need to collect the URLs for the selected images.</li>
<li>Now move to the JavaScript console and then copy and paste the following JavaScript codes into the console:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 120px">// Get the jquery into the JavaScript console<br/>var scriptJs = document.createElement('scriptJs');<br/>scriptJs.src = "https://ajax.googleapis.com/ajax/libs/jquery/2.2.0/jquery.min.js";<br/>document.getElementsByTagName('head')[0].appendChild(scriptJs)</pre>
<ol start="4">
<li style="list-style-type: none">
<ul>
<li>The preceding line of code will <span>pull </span>the jQuery JavaScript library.  Now you can use a CSS selector to collect a list of URLs using the following lines of code:</li>
</ul>
</li>
</ol>
<pre style="padding-left: 120px">// Collect the selected URLs<br/>var urls_images = $('.rg_di .rg_meta').map(function() { return JSON.parse($(this).text()).ou; });</pre>
<ol start="4">
<li>Finally, write the URLs to a file (one per line) using the following lines of code:</li>
</ol>
<pre style="padding-left: 60px">// write the URls to a file <br/>var text_url_Save = urls_images.toArray().join('\n');<br/>var hiddenComponents = document.createElement('a');<br/>hiddenComponents.href = 'data:attachment/text,' + encodeURI(text_url_Save);<br/>hiddenComponents.target = '_blank';<br/>hiddenComponents.download = 'imageurls.txt';<br/>hiddenComponents.click();</pre>
<p style="padding-left: 60px">Once you execute the preceding lines of code, you will have a file named <kbd>imageurls.txt</kbd> in your default download directory.  If you want to download them into a specific folder, then write <kbd>hiddenComponents.download = 'your fooler/imageurls.txt</kbd> instead of <kbd>hiddenComponents.download = 'imageurls.txt'</kbd> in the preceding code.</p>
<ol start="5">
<li><strong>Downloading the images</strong>:<span> </span>Now you are ready to download the running the images <kbd>download_images.py</kbd> <span>(available in code folder of the chapter) in the previously downloaded</span> <kbd>imageurls.txt</kbd><span>:</span></li>
</ol>
<pre style="padding-left: 60px">python <span>download_images.py  imageurls.txt</span></pre>
<ol start="6">
<li><strong>Exploration</strong><span>: Once we have downloaded the images, we need to explore them in order delete the irrelevant images. We can do this through a bit of manual inspection. After this, we need to resize and convert them into grayscale images to match the previously downloaded dataset: </span></li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/46a249b1-8e27-4169-a8c4-50d62acc6e8d.png" style="width:19.75em;height:25.75em;"/></p>
<p>The preceding screenshot shows the folder structure of the pothole and non-pothole images datasets.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collecting data for use case two </h1>
                </header>
            
            <article>
                
<p>As is the case with use case one, we can collect data through digital cameras or use an existing open source or a combination of both. We are using an existing and open source dataset for the implementation of the sorting algorithm. The dataset was collected from urban environments of the USA . As solid waste types may vary by country, it is better to update the dataset based on the country the use case will be used for. The dataset consists of six types of solid wastes: glass, paper, cardboard, plastic, metal, and trash. The dataset consists of 2,527 images, and they were annotated and organized into the following folders, as shown in the  following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/688c0162-ec3e-4ecb-97e1-60c59f450432.png" style="width:23.00em;height:28.83em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data exploration of use case two </h1>
                </header>
            
            <article>
                
<p>The following presents a snapshot of the data exploration for use case two. As we can see, glass and plastic images could be confusing to the sorting algorithm. In this context, weight sensor data can be useful for fixing this issue:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1efe9aad-74f3-4cfe-879a-da94f0bf12da.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Data pre-processing </h1>
                </header>
            
            <article>
                
<p>This is an essential step for a DL pipeline. The existing datasets on pothole images and the solid waste images used in the <span><span>use cases are </span></span>pre-processed and are ready to be used for training, validation, and testing. As shown in the following diagram, both the original and modified (additional images downloaded for the pothole class) are organized as sub-folders, each named after one of the five categories and containing only images from that category. There are a few issues to be noted during the training image set preparation:</p>
<ul>
<li><strong>Data size</strong>:<em> </em>We need to collect at least a hundred images for each class to train a model that works well. The more we can gather, the better the accuracy of the trained model is likely to be. Each of the five categories in the used dataset has more than 1,000 sample images. We also made sure that the images are a good representation of what our application will actually face in real implementation.</li>
<li><strong>Data heterogeneity</strong>:<em> </em>Data collected for training should be heterogeneous. For example, images about potholes need to be taken in as wide a variety of situations as we can, at different times, and with different devices.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Models training </h1>
                </header>
            
            <article>
                
<p>As we mentioned earlier, we are using transfer learning that does not require training from scratch; retraining of the models with a new dataset will sufficiently work in many cases. We retrained two popular architectures or models of CNN, namely Incentive V3 and Mobilenet V1, on a desktop computer, which is replicating the city council’s server. In both models, it took less than an hour to retrain the models, which is an advantage of the transfer learning approach. We need to understand the list of key arguments before running the <kbd>retrain.py</kbd>file, which is in the code folder. If we type in our Terminal (in Linux or macOS) or Command Prompt (Windows) <kbd>python retrain.py -h</kbd>, we shall see a window like the following screenshot with additional information (that is, an overview of each argument). The compulsory argument is the image directory, and it is one of the dataset directories shown in the preceding figures on the folder view of the datasets: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/716599bf-9f65-4cd9-82e6-b46defa25ad4.png"/></p>
<p>In the following, we are presenting two examples of command: one to retrain the Incentive model V3 and the other to retain Mobilenet V1 on the modified dataset (dataset-modified). To retrain Incentive V3, we did not pass the architecture argument value as it is the default architecture included in <kbd>retrain.py</kbd>. For the rest of the arguments, including data split ratio among training, validation, and test, we used the default values. In this use case, we are using the split rule of the data that put 80% of the images into the main training set, keeping 10% separate for validation during training, and the final 10% of the data as a testing set. The testing set is to test the real-world classification performance of the classifier:</p>
<pre>python retrain.py \<br/>--output_graph=trained_model_incentive-modified-dataset/retrained_graph.pb \<br/>--output_labels=trained_model_incentive-modified-dataset/retrained_labels.txt \<br/>--image_dir=dataset-modified</pre>
<p>To run the training and validation of the Mobilenet V1 model, use the following command:</p>
<pre>python retrain.py \<br/>--output_graph=trained_model_mobilenetv1-modified-dataset/retrained_graph.pb \<br/>--output_labels=trained_model_mobilenetv1-modified-dataset/retrained_labels.txt \<br/>--architecture mobilenet_1.0_224 \<br/>--image_dir=dataset-modified</pre>
<p>Once we run the preceding commands, it will generate the retrain models (<kbd>retrained_graph.pb</kbd>), labels text (<kbd>retrained_labels.txt</kbd>) in the given directory and summary directory consists of train and validation summary information of the models. The summary information <kbd>(--summaries_dir</kbd>  argument with default value <kbd>retrain_logs</kbd>) can be used by TensorBoard to visualize different aspects of the models, including the networks, and their performance graphs. If we type the following command in the terminal or Command Prompt, it will run TensorBoard:</p>
<pre>tensorboard --logdir retrain_logs<br/> </pre>
<p>Once TensorBoard is running, navigate your web browser to <kbd>localhost:6006</kbd> to view the TensorBoard and view the network of the corresponding model. The following diagrams <strong>(a)</strong> and <strong>(b)</strong> show the network for Incentive V3 and Mobilenet V1 respectively. The diagram demonstrates the complexity of Incentive V3 compared to Mobilenet V1:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/22d6ea4b-2faa-4ca9-8936-08c84bc6711f.png" style="width:86.67em;height:62.25em;"/></p>
<p><span>In the second use case, we have retrained only the </span><span>Mobilenet V1 on the solid waste dataset. </span><span>You can retrain the model as mentioned by only providing an image or dataset directory as follows:</span></p>
<pre>--image_dir=dataset-solidwaste</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating models</h1>
                </header>
            
            <article>
                
<p class="CDPAlignLeft CDPAlign">Firstly, we have identified the size of the retrain models. As shown in the following screenshot, Mobilenet V1 requires only 17.1 MB (for both use cases), which is than one-fifth of Incentive V3 (92.3 MB), and this model can be easily deployed in resource-constrained IoT devices, including Raspberry Pi or smartphones. Secondly, we have evaluated the performance of the models. Two levels of performance evaluation have been done for the use cases: (i) dataset-wide evaluation or testing has been done during the retraining phase on the desktop PC platform/server, and (ii) an individual image or sample (real-life image) was tested or evaluated in the Raspberry Pi 3 environment:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/89912302-26f7-4677-b93e-6f01168445dd.png" style="width:40.75em;height:24.67em;"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance (use case one) </h1>
                </header>
            
            <article>
                
<p>All the evaluation performances of use case one are presented in the following screenshots. The following six screenshots present the training, validation, and testing performances of Incentive V3 and Mobilenet V1 models on the two sets of data. The first three screenshots present the results generated in the terminal after retraining the models, and the last three screenshots are generated from the TensorBoard.</p>
<p><span>The following screenshot presents the evaluation results of Incentive V3 on the original dataset:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f968d524-a60a-4c52-8817-65c4cf04c4f5.png" style="width:51.25em;height:10.08em;"/></p>
<p><span>The following screenshot presents the evaluation results of Incentive V3 on the modified dataset: </span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/5eaee1f3-7570-44ed-bd87-1d28aa93f7f7.png" style="width:52.92em;height:10.00em;"/></div>
<p><span>The following screenshot presents the evaluation results of Mobilenet V1 on the original dataset:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cfb61326-9ce7-4ded-97a7-8b0b1c20e223.png" style="width:52.50em;height:11.00em;"/></div>
<p class="mce-root">The following screenshot presents the evaluation results of  Mobilenet V1 on the modified dataset:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="assets/eaa8056d-ed33-4e1e-9b9e-e57e812d82f1.png"/></p>
<p><span>The following screenshot presents the evaluation results of</span><span> </span><span>Incentive V3 </span><span>on the original dataset generated by TensorBoard:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/943f8ef3-d31f-4d57-87e3-a52137a240c8.png" style="width:69.08em;height:33.42em;"/></div>
<p><span>The following screenshot presents the evaluation results of</span><span> </span><span> </span><span>Mobilenet V1 on the original dataset generated by TensorBoard:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/2469605e-54d7-4ff8-8437-fa1e9a518650.png" style="width:66.75em;height:31.25em;"/></div>
<p class="CDPAlignLeft CDPAlign"><span>From all the previous model performance screenshots, it is clear that both training and validation accuracies</span><span> are well above 90%, which is enough for fault detection. </span></p>
<p>The following diagrams show the classification or object detection performances on individual samples. For these, we have used two different sets of classification code (available in the chapter's code folder).</p>
<p>The first screenshot is showing the snapshot of running the classifier for Mobilenet V1 on two samples. As we can see from all results, test or evaluation accuracy is well above 94%, and with such accuracy, the DL models (CNNs) have the potential to detect objects, including potholes, manholes, and other objects on the road. However, object detection time on the Pi 3 was in the range of three to five seconds, which needs to be improved if we want to use them in real-time detection and actuation. In addition, results show that models trained on the modified dataset have a good chance to provide high detection or testing accuracy in a real environment (shown in the preceding screenshots), especially in detecting potholes, as this class of data has been improved by adding diverse images from the googled images:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8256fc92-b359-45b0-8b38-4f76fc35cd19.png"/></p>
<p><span>The following screenshot presents the evaluation results of</span><span> </span><span>pothole detection with the Incentive V3 model trained on the original dataset (Pi 3 B+):</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/4e62eb2e-fec0-4466-bfb5-c1296717e078.png" style="width:34.42em;height:14.50em;"/></div>
<p><span>The following diagram presents the evaluation results of</span><span> manhole detection with the Incentive V3 model trained on the original dataset (Pi 3 B+):</span></p>
<div class="packt_figref">
<p class="CDPAlignCenter CDPAlign"><img src="assets/09716d1d-c32b-400c-a862-160475bb473c.png" style="font-size: 1em;width:36.83em;height:15.25em;"/></p>
</div>
<div class="CDPAlignCenter CDPAlign packt_figref"/>
<p><span>The following diagram presents the evaluation results of</span><span> </span><span> pothole detection with the Mobilenet V1 model trained on the original dataset (Pi 3 B+):</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/072508d6-ea2a-4eef-9d33-a287aaec601d.png" style="width:36.75em;height:15.17em;"/></p>
<p><span>The following diagram presents the evaluation results of</span><span> </span><span> manhole detection with the Mobilenet V1 model trained on the original dataset (Pi 3 B+):</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d0460b70-2622-48f5-aa3e-8fee182a53f4.png" style="width:39.08em;height:15.92em;"/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model performance (use case two) </h1>
                </header>
            
            <article>
                
<p>All the evaluation performances of use case two are presented in the following screenshots. For this use case, we are presenting only the results for <span>Mobilenet V1 .</span>The following diagrams present the training, validation, and testing performances of the Mobilenet V1 models on the two datasets. As we can see from the following screenshot, the test accuracy is not that high (77.5%) but good enough for solid waste detection and sorting: </p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/81309be7-f043-4486-ae9f-2243d26318db.png"/></p>
<p><span>The following screenshot presents the evaluation results of</span><span> </span><span>Mobilenet V1 on the dataset generated by TensorBoard:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3a8271f6-78a8-4302-8e08-c7c4f63c25ad.png" style="width:70.00em;height:32.42em;"/></p>
<p><span>The following three screenshots show the classification or object (solid waste) detection performances on individual samples.</span> <span>The first screenshot presents the evaluation results of </span><span>glass detection:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/784a2d8a-4dcb-4d74-88d4-725e03dee6ae.png"/></p>
<p><span>The following screenshot presents the evaluation results of </span><span>plastic detection:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img src="assets/d037aa38-1eb6-40be-b709-4377b1bb1ee7.png"/></span></p>
<p><span>The following screenshot presents the evaluation results of</span> <span>metal detection using Mobilenet V1:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/0a1d55f8-9114-4ba5-a4c8-39bd2a1cff87.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary </h1>
                </header>
            
            <article>
                
<p><span>In the first part of this chapter, we briefly described different IoT applications and their image detection-based decision-making. In addition, we briefly discussed two use cases: image detection-based road fault detection, and image detection-based solid waste sorting. The first application can detect potholes on the road using a smartphone camera or a Raspberry Pi camera. The second application detects different types of solid waste and sorts them according to smart recycling.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>In the second part of the chapter, we briefly discussed transfer learning with a few example networks, and examined its usefulness in resource-constrained IoT applications. In addition, we discussed the rationale behind selecting a CNN, including two popular implementations, namely Inception V3 and Mobilenet V1. The rest of the chapter described all the necessary components of the DL pipeline for the Inception V3 and Mobilenet V1 models.</span></p>
<p>In many IoT applications, image recognition alone may not be enough for object and/or subject detection. In this context, sometimes, audio/speech/voice recognition can be useful. <a href="ff7fc37c-f5d6-4e2f-8d3b-3f64c47c4c2e.xhtml">Chapter 3</a>, Audio/Speech/Voice Recognition in IoT, will present DL-based speech/voice data analysis and recognition in IoT applications.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">References</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Smart patrolling: An efficient road surface monitoring using smartphone sensors and crowdsourcing</em>, <span>Gurdit Singh, Divya Bansal, Sanjeev Sofat, Naveen Aggarwal, </span><em>Pervasive and Mobile Computing</em>, volume 40, 2017, pages 71-88</li>
<li><em>Road Damage Detection Using Deep Neural Networks with Images Captured Through a Smartphone</em>, <span>Hiroya Maeda, Yoshihide Sekimoto, Toshikazu Seto, Takehiro Kashiyama, Hiroshi Omata, </span>arXiv:1801.09454</li>
<li><em>Potholes cost UK drivers £1.7 billion a year: Here's how to claim if you car is damaged</em>, <span>Luke John Smith:</span> <a href="https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK">https://www.express.co.uk/life-style/cars/938333/pothole-damage-cost-how-to-claim-UK</a></li>
<li><em>What a Waste: A Global Review of Solid Waste Management</em>, <span>D Hoornweg and P Bhada-Tata, </span>World Bank, Washington, DC, USA, 2012 </li>
<li><em>Efficient Convolutional Neural Networks for Mobile Vision Applica</em><em>tions</em>, <span>Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, Hartwig Adam, </span><em>MobileNets:<span> </span></em>arXiv:1704.04861</li>
<li><em>Imagenet classification with deep convolutional neural networks</em>, <span>A Krizhevsky, I Sutskever, G E Hinton, i</span>n <em>Advances in Neural Information Processing Systems</em>, pages 1,097–1,105, 2012. 1, 6.</li>
<li><em>MobileNetV2: Inverted Residuals and Linear Bottlenecks</em>, <span>Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen, </span>arXiv:1801.04381.</li>
<li>Pothole dataset: <a href="https://cord.cranfield.ac.uk/articles/PotDataset/5999699">https://cord.cranfield.ac.uk/articles/PotDataset/5999699 </a></li>
<li>Trashnet: <a href="https://github.com/garythung/trashnet">https://github.com/garythung/trashnet</a></li>
</ul>


            </article>

            
        </section>
    </body></html>