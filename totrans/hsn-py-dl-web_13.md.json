["```\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import WordPunctTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Comment below line if you already have stopwords installed\nnltk.download('stopwords')\n```", "```\ndf = pd.read_csv('Reviews.csv', encoding = \"ISO-8859-1\")\ndf = df.head(10000)\n```", "```\nimport string\nimport re\n\nstopwordSet = set(stopwords.words(\"english\"))\n\ndef cleanText(line):\n    global stopwordSet\n\n    line = line.translate(string.punctuation)\n    line = line.lower().split()\n\n    line = [word for word in line if not word in stopwordSet and len(word) >= 3]\n    line = \" \".join(line)\n\n    return re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", line) \n```", "```\ndata = df[['ProductId', 'UserId', 'Score', 'Text']]\n```", "```\n%%time\ndata['Text'] = data['Text'].apply(cleanText)\n```", "```\nX_train, X_valid, y_train, y_valid = train_test_split(data['Text'], df['ProductId'], test_size = 0.2) \n```", "```\nuser_df = data[['UserId','Text']]\nproduct_df = data[['ProductId', 'Text']]\nuser_df = user_df.groupby('UserId').agg({'Text': ' '.join})\nproduct_df = product_df.groupby('ProductId').agg({'Text': ' '.join})\n```", "```\nuser_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=1000)\nuser_vectors = user_vectorizer.fit_transform(user_df['Text'])\nuser_vectors.shape\n```", "```\nproduct_vectorizer = TfidfVectorizer(tokenizer = WordPunctTokenizer().tokenize, max_features=1000)\nproduct_vectors = product_vectorizer.fit_transform(product_df['Text'])\nproduct_vectors.shape\n```", "```\nuserRatings = pd.pivot_table(data, values='Score', index=['UserId'], columns=['ProductId'])\nuserRatings.shape\n```", "```\nP = pd.DataFrame(user_vectors.toarray(), index=user_df.index, columns=user_vectorizer.get_feature_names())\nQ = pd.DataFrame(product_vectors.toarray(), index=product_df.index, columns=product_vectorizer.get_feature_names())\n```", "```\ndef matrix_factorization(R, P, Q, steps=1, gamma=0.001,lamda=0.02):\n    for step in range(steps):\n        for i in R.index:\n            for j in R.columns:\n                if R.loc[i,j]>0:\n                    eij=R.loc[i,j]-np.dot(P.loc[i],Q.loc[j])\n                    P.loc[i]=P.loc[i]+gamma*(eij*Q.loc[j]-lamda*P.loc[i])\n                    Q.loc[j]=Q.loc[j]+gamma*(eij*P.loc[i]-lamda*Q.loc[j])\n        e=0\n        for i in R.index:\n            for j in R.columns:\n                if R.loc[i,j]>0:\n                    e= e + pow(R.loc[i,j]-np.dot(P.loc[i],Q.loc[j]),2)+lamda*(pow(np.linalg.norm(P.loc[i]),2)+pow(np.linalg.norm(Q.loc[j]),2))\n        if e<0.001:\n            break\n\n    return P,Q\n```", "```\n%%time\nP, Q = matrix_factorization(userRatings, P, Q, steps=1, gamma=0.001,lamda=0.02)\n```", "```\nimport pickle\noutput = open('api/model.pkl', 'wb')\npickle.dump(P,output)\npickle.dump(Q,output)\npickle.dump(user_vectorizer,output)\noutput.close()\n```", "```\nimport numpy as np\nimport pandas as pd\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import WordPunctTokenizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom flask import Flask, request, render_template, make_response\nfrom flask_wtf import FlaskForm\nfrom wtforms import StringField, validators\nimport io\nfrom flask_restful import Resource, Api\nimport string\nimport re\nimport pickle\nfrom flask_jsonpify import jsonpify\n```", "```\nDEBUG = True\napp = Flask(__name__)\napp.config['SECRET_KEY'] = 'abcdefgh'\napi = Api(app)\n```", "```\nclass TextFieldForm(FlaskForm):\n    text = StringField('Document Content', validators=[validators.data_required()])\n```", "```\nclass Flask_Work(Resource):\n    def __init__(self):\n        self.stopwordSet = set(stopwords.words(\"english\"))\n        pass\n```", "```\n    def cleanText(self, line): \n        line = line.translate(string.punctuation)\n        line = line.lower().split()\n\n        line = [word for word in line if not word in self.stopwordSet and len(word) >= 3]\n        line = \" \".join(line)\n\n        return re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", line) \n```", "```\n    def get(self):\n        headers = {'Content-Type': 'text/html'}\n        return make_response(render_template('index.html'), 200, headers)\n```", "```\n    def post(self):\n        f = open('model.pkl', 'rb')\n        P, Q, userid_vectorizer = pickle.load(f), pickle.load(f), pickle.load(f)\n        sentence = request.form['search']\n        test_data = pd.DataFrame([sentence], columns=['Text'])\n        test_data['Text'] = test_data['Text'].apply(self.cleanText)\n        test_vectors = userid_vectorizer.transform(test_data['Text'])\n        test_v_df = pd.DataFrame(test_vectors.toarray(), index=test_data.index,\n                                 columns=userid_vectorizer.get_feature_names())\n\n        predicted_ratings = pd.DataFrame(np.dot(test_v_df.loc[0], Q.T), index=Q.index, columns=['Rating'])\n        predictions = pd.DataFrame.sort_values(predicted_ratings, ['Rating'], ascending=[0])[:10]\n\n        JSONP_data = jsonpify(predictions.to_json())\n        return JSONP_data\n```", "```\napi.add_resource(Flask_Work, '/')\n\nif __name__ == '__main__':\n    app.run(host='127.0.0.1', port=4000, debug=True)\n```", "```\npython main.py\n```"]