- en: Introduction to Object Detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Detecting and classifying objects in images is a challenging problem. So far,
    we have treated the issue of image classification on a simple level; in a real-life
    scenario, we are unlikely to have pictures containing just one object. In industrial
    environments, it is possible to set up cameras and mechanical supports to capture
    images of single objects. However, even in constrained environments, such as an
    industrial one, it is not always possible to have such a strict setup. Smartphone
    applications, automated guided vehicles, and, more generally, any real-life application
    that uses images captured in a non-controlled environment require the simultaneous
    localization and classification of several objects in the input images. Object
    detection is the process of localizing an object into an image by predicting the
    coordinates of a bounding box that contains it, while at the same time correctly
    classifying it.
  prefs: []
  type: TYPE_NORMAL
- en: State-of-the-art methods to tackle object detection problems are based on convolutional
    neural networks that, as we will see in this chapter, can be used not only to
    extract meaningful classification features but also to regress the coordinates
    of the bounding box. Being a challenging problem, it is better to start with the
    foundations. Detecting and classifying more than one object at the same time requires
    the convolutional architecture to be designed and trained in a more complicated
    way than the one needed to solve the same problem with a single object. The tasks
    of regressing the bounding box coordinates of a single object and classifying
    the content are called **localization and classification**. Solving this task
    is the starting point to develop more complicated architectures that address the
    object detection task.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will look at both problems; we start from the foundations,
    developing a regression network completely, and then extending it to perform both
    regression and classification. The chapter ends with an introduction to anchor-based
    detectors, since a complete implementation of an object detection network goes
    beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset used throughout the chapter is the PASCAL Visual Object Classes
    Challenge 2007.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classification and localization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting the data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection is a supervised learning problem that requires a considerable
    amount of data to reach good performance. The process of carefully annotating
    images by drawing bounding boxes around the objects and assigning them the correct
    labels is a time-consuming process that requires several hours of repetitive work.
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, there are already several datasets for object detection that are
    ready to use. The most famous is the ImageNet dataset, immediately followed by
    the PASCAL VOC 2007 dataset. To be able to use ImageNet, dedicated hardware is
    required since its size and number of labeled objects per image makes the object
    detection task hard to tackle.
  prefs: []
  type: TYPE_NORMAL
- en: 'PASCAL VOC 2007, instead, consists of only 9,963 images in total, each of them
    with a different number of labeled objects belonging to the 20 selected object
    classes. The twenty object classes are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Person**: Person'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Animal**: Bird, cat, cow, dog, horse, sheep'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vehicle**: Airplane, bicycle, boat, bus, car, motorbike, train'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Indoor**: Bottle, chair, dining table, potted plant, sofa, tv/monitor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As described in the official dataset page ([http://host.robots.ox.ac.uk/pascal/VOC/voc2007/](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/)),
    the dataset already comes with three splits (train, validation, and test) ready
    to use. The data has been split into 50% for training/validation and 50% for testing.
    The distributions of images and objects by class are approximately equal across
    the training/validation and test sets. In total there are 9,963 images, containing
    24,640 annotated objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow datasets allow us to download the whole dataset with a single line
    of code (approximately 869 MiB) and to obtain the `tf.data.Dataset` object of
    every split:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As usual, TensorFlow datasets give us a lot of useful information about the
    dataset format. The output that follows is the result of `print(info)`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: For every image, there is a `SequenceDict` object that contains the information
    of every labeled object present. Something handy when working with any data related
    project is to visualize the data. In this case in particular, since we are trying
    to solve a computer vision problem, visualizing the images and the bounding boxes
    can help us to have a better understanding of the difficulties the network should
    face during the training.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the labeled images, we use `matplotlib.pyplot` together with the
    usage of the `tf.image` package; the former is used to display the images, and
    the latter is used to draw the bounding boxes and to convert them to `tf.float32`
    (thus scaling the values in the [0,1] range). Moreover, how to use the `tfds.ClassLabel.int2str` method
    is shown; this method is convenient since it allows us to get the text representation
    of a label from its numerical representation:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'From the training set, take five images, draw the bounding box, and then print
    the class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, plot the image using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following images are a collage of the five images produced by the code
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0987746-b915-42a3-80cc-3e059f3178d9.png)'
  prefs: []
  type: TYPE_IMG
- en: Please note that since TensorFlow dataset shuffles the data when it creates
    the TFRecords, it is unlikely that the same execution on a different machine will
    produce the same sequence of images.
  prefs: []
  type: TYPE_NORMAL
- en: It is also worth noting that partial objects are annotated as full objects;
    for instance, the human hand on the bottom-left image is labeled as a person,
    and the rear wheel of a motorbike present in the bottom right of the picture is
    marked as a motorbike .
  prefs: []
  type: TYPE_NORMAL
- en: 'The object detection task is challenging by its nature, but looking at the
    data, we can see that the data itself is hard to use. In fact, the labels printed
    to the standard output for the bottom-right image are:'
  prefs: []
  type: TYPE_NORMAL
- en: Person
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bird
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, the dataset contains full objects annotated and labeled (bird) and also
    partial objects annotated and labeled as the whole object (for example, the human
    hand is labeled as a person). This small example shows how difficult object detection
    is: the network should be able to classify and localize, for example, people from
    their attributes (a hand) or from their complete shape while dealing with the
    problem of occlusion.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the data gives us a better idea of how challenging the problem is.
    However, before facing the challenge of object detection, it is better to start
    from the foundations by tackling the problem of localization and classification.
    Therefore, we have to filter the dataset objects to extract only the images that
    contain a single labeled object. For this reason, a simple function that accepts
    a `tf.data.Dataset` object as input and applies a filter on it can be defined
    and used. Create a subset of the dataset by filtering the elements: we are interested in
    creating a dataset for object detection and classification, that is, a dataset
    of images with a single object annotated:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using the same snippet as earlier, we can visualize some of the images to check
    if everything goes as we expect:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e52f0ef-e349-41e4-9cc1-69633671986f.png)'
  prefs: []
  type: TYPE_IMG
- en: We can see images that contain only a single object, sampled from the training
    set, drawn using the previous code snippet after applying the `filter` function.
    The `filter` function returns a new dataset that contains only the elements of
    the input dataset that contain a single bounding box, hence the perfect candidates
    to train a single network for classification and localization.
  prefs: []
  type: TYPE_NORMAL
- en: Object localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convolutional neural networks (CNNs) are extremely flexible objects—so far,
    we have used them to solve classification problems, making them learn to extract
    features specific to the task. As shown in [Chapter 6](699c6d94-72fa-4636-8e23-6da8928847b6.xhtml), *Image
    Classification Using TensorFlow Hub*, the standard architecture of CNNs designed
    to classify images is made of two parts—the feature extractor, which produces
    a feature vector, and a set of fully connected layers that classifies the feature
    vector in the (hopefully) correct class:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/78335c1a-7738-41d9-8705-3b16ad17e97e.png)'
  prefs: []
  type: TYPE_IMG
- en: The classifier placed on top of the feature vector can also be seen as the head
    of the network
  prefs: []
  type: TYPE_NORMAL
- en: The fact that, so far, CNNs have only been used to solve classification problems
    should not mislead us. These types of networks are extremely powerful, and, especially
    in their multilayer setting, they can be used to solve many different kinds of
    problems, extracting information from the visual input.
  prefs: []
  type: TYPE_NORMAL
- en: For this reason, solving the localization and classification problem is just
    a matter of adding a new head to the network, the localization head.
  prefs: []
  type: TYPE_NORMAL
- en: The input data is an image that contains a single object together with the four
    coordinates of the bounding box. So the idea is to use this information to solve
    the classification and localization problem at the same time by treating the localization
    as a regression problem.
  prefs: []
  type: TYPE_NORMAL
- en: Localization as a regression problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ignoring for a moment the classification problem and focusing only on the localization
    part, we can think about the localization as the problem of regressing the four
    coordinates of the bounding box that contains the subject of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, there is not much difference between training a CNN to solve a
    classification task or a regression task: the architecture of the feature extractor
    remains the same, while the classification head changes and becomes a regression
    head. In the very end, this only means to change the number of output neurons
    from the number of classes to 4, one neuron per coordinate of the bounding box.'
  prefs: []
  type: TYPE_NORMAL
- en: The idea is that the regression head should learn to output the correct coordinates
    when certain input features are present.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a0293f3f-3b42-4702-bb64-3599c35c2d81.png)'
  prefs: []
  type: TYPE_IMG
- en: The AlexNet architecture used as a feature extractor and the classification
    head replaced with a regression head with four output neurons
  prefs: []
  type: TYPE_NORMAL
- en: To make the network learn to regress the coordinates of the bounding box of
    an object, we have to express the input/output relationship between the neurons
    and the labels (that is, the four coordinates of the bounding box, present in
    the dataset) using a loss function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The L2 distance can be effectively used as the loss function: the goal is to
    regress correctly all the four coordinates and thus minimize the distance between
    the predicted values and the real one, making it tend to zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36609293-b950-431f-818c-ffc25791c50b.png)'
  prefs: []
  type: TYPE_IMG
- en: Where the first tuple [![](img/e8fd1f5e-9e42-4b2c-8952-cec3d47769c6.png)] is
    the regression head output, and the second tuple [![](img/be5cedad-3e23-4a0e-8dd2-acd229159641.png)] represents
    the ground truth bounding box coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a regression network in TensorFlow 2.0 is straightforward. As shown
    in [Chapter 6](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=31&action=edit#post_30), *Image
    Classification Using TensorFlow Hub*, TensorFlow Hub can be used to speed up the
    training phase by using it to download and embed a pretrained feature extractor
    in our model.
  prefs: []
  type: TYPE_NORMAL
- en: A detail that is worth pointing out is the format that TensorFlow uses to represent
    the bounding box coordinates (and the coordinates in general)—the format used
    is `[ymin, xmin, ymax, xmax]` , and the coordinates are normalized in the [0,1]
    range, in order not to depend on the original image resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorFlow 2.0 and TensorFlow Hub, we can define and train a coordinate
    regression network on the PASCAL VOC 2007 dataset in a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the Inception v3 network from TensorFlow Hub as the backbone of the coordinate
    regression network, defining the regression model is straightforward. Although
    the network has a sequential structure, we define it using the functional API
    since this will allow us to extend the model easily without the need to rewrite
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Moreover, since we decided to use the inception network that needs a 299 x
    299 input image resolution with values in the [0,1] range, we have to add an additional
    step to the input pipeline to prepare the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As previously introduced, the loss function to use is the standard L2 loss,
    which already comes implemented in TensorFlow as a Keras loss that can be found
    in the `tf.losses` package. However, instead of using `tf.losses.MeanSquaredError`,
    it is worth defining the loss function ourselves since there is a detail to highlight.
  prefs: []
  type: TYPE_NORMAL
- en: If we decide to use the implemented **Mean Squared Error** (**MSE**) function,
    we have to take into account that under the hood, the `tf.subtract` operation
    is used. This operation simply computes the subtraction of the left-hand operation
    with the right-hand operand. This behavior is what we are looking for, of course,
    but the subtraction operation in TensorFlow follows the NumPy broadcasting semantic
    (as almost any mathematical operation). This particular semantic broadcasts the
    value of the left-side tensor to the right-side tensor, and if the right-side
    tensor has a dimension of 1 where the value of the left-side tensor is copied.
  prefs: []
  type: TYPE_NORMAL
- en: Since we selected the images with only one object inside, we have a single bounding
    box present in the `"bbox"` attribute. Hence, if we pick a batch size of 32, the
    tensor that contains the bounding box will have a shape of `(32, 1, 4)`. The 1
    in the second position can cause problems in the loss computation and preventing
    the model from converging .
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we have two options:'
  prefs: []
  type: TYPE_NORMAL
- en: Define the loss function using Keras, removing the unary dimension by using `tf.squeeze`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the loss function manually
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, defining the loss function manually allows us to place the `tf.print`
    statements in the body function, which can be used for a raw debugging process
    and, more importantly, to define the training loop in the standard way, making
    the loss function itself taking care of squeezing the unary dimension where needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The training loop is straightforward, and it can be implemented in two different
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Writing a custom training loop (thus using the `tf.GradientTape` object)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the `compile` and `fit` methods of the Keras model, since this is a standard
    training loop that Keras can build for us
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, since we are interested in extending this solution in the next sections,
    it is better to start using the custom training loop, as it gives more freedom
    for customization. Moreover, we are interested in visualizing the ground truth
    and the predicted bounding box, by logging them on TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, before defining the training loop, it is worth defining a `draw`
    function that takes a dataset, the model, and the current step, and using them
    to draw both the ground truth and the predicted boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The training loop for our coordinate regressor (that can also be thought of
    as a region proposal, since it is now aware of the label of the object it''s detecting
    in the images), which also logs on TensorBoard the training loss value and the
    prediction on three images sampled from the training and validation set (using
    the `draw` function), can be easily defined:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the `global_step` variable, used to keep track of the training iterations,
    followed by the definition of the file writers, used to log the train and validation
    summaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Following the TensorFlow 2.0 best practice, we can define the training step
    as a function and convert it to its graph representation using `tf.function`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the training loop over the batches and invoke the `train_step` function
    on every iteration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Although the Inception network is used as a fixed feature extractor, the training
    process can take a few hours on a CPU and almost half an hour on a GPU.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the visible trend of the loss function during
    the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/065c98e0-6122-4e3a-a8f1-0ae70395309a.png)'
  prefs: []
  type: TYPE_IMG
- en: We see that from the early training steps, the loss value is close to zero,
    although oscillations are present during the whole training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'During the training process, in the images tab of TensorBoard, we can visualize
    the images with the regressed and ground truth bounding boxes drawn. Since we
    created two different summary writers (one for the training logs and the other
    for the validation logs), TensorFlow visualizes for us the images logged for the
    two different splits:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf1356bb-95db-44f3-af09-41f9a77c51fd.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding images are samples from the training (first row) and validation
    (second row) set, with the ground truth and regressed bounding boxes. The regressed
    bounding boxes in the training set are close to the ground truth boxes, while
    the regressed boxes on the validation images are different.
  prefs: []
  type: TYPE_NORMAL
- en: 'The training loop previously defined has various problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The only measured metric is the L2 loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The validation set is never used to measure any numerical score
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No check for overfitting is present
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a complete lack of a metric that measures how good the regression of
    the bounding box is, measured on both the training and the validation set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The training loop can, therefore, be improved by measuring an object detection
    metric; measuring the metric also reduces the training time since we can stop
    the training earlier. Moreover, from the visualization of the results, it is pretty
    clear that the model is overfitting the training set, and a regularization layer,
    such as dropout, can be added to address this problem. The problem of regressing
    a bounding box can be treated as a binary classification problem. In fact, there
    are only two possible outcomes: ground truth bounding box matched or not matched.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, having a perfect match is not an easy task; for this reason, a function
    that measures how good the detected bounding box is with a numerical score (with
    respect to the ground truth) is needed. The most widely used function to measure
    the goodness of localization is the **Intersection over Union** (**IoU**), which
    we will explore in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Intersection over Union
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Intersection over Union (IoU) is defined as the ratio between the area of overlap
    and the area of union. The following image is a graphical representation of IoU:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d078024a-428b-4e25-a723-f3d0c50838d3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Credits: Jonathan Hui ([https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173))'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, the IoU measures *how much* the predicted bounding box overlaps
    with the ground truth. Since IoU is a metric that uses the areas of the objects,
    it can be easily expressed treating the ground truth and the detected area like
    sets. Let A be the set of the proposed object pixels and B the set of true object
    pixels; then IoU is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79a3faa7-f117-4f73-b9e1-878c13ebcc7e.png)'
  prefs: []
  type: TYPE_IMG
- en: The IoU value is in the [0,1] range, where 0 is a no-match (no overlap), and
    1 is the perfect match. The IoU value is used as an overlap criterion; usually,
    an IoU value greater than 0.5 is considered as a true positive (match), while
    any other value is regarded as a false positive. There are no true negatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing the IoU formula in TensorFlow is straightforward. The only subtlety
    to take into account is that de-normalizing the coordinates is needed since the
    area should be computed in pixels. The conversion in pixel coordinates together
    with the coordinate swapping in a more friendly representation is implemented
    in the `_swap` closure:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Transpose the coordinates from `y_min`, `x_min`, `y_max`, and `x_max` in absolute
    coordinates to `x_min`, `y_min`, `x_max`, and `y_max` in pixel coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, compute the width and height of the bounding box:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Average precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A value of IoU greater than a specified threshold (usually 0.5) allows us to
    treat the bounding box regressed as a match.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of single class prediction, having the number of **true positives**
    (**TP**) and **false positives** (**FP**) measured on the dataset allows us to
    compute the average precision as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/377c65ae-3390-47f8-8dbb-9235a35a05bd.png)'
  prefs: []
  type: TYPE_IMG
- en: In the object detection challenges, the **Average Precision** (**AP**) is often
    measured for different values of IoU. The minimum requirement is to measure the
    AP for an IoU value of 0.5, but achieving good results with a half overlap is
    not sufficient in most real-life scenarios. Usually, in fact, a bounding box prediction
    is required to match at least a value of IoU of 0.75 or 0.85 to be useful.
  prefs: []
  type: TYPE_NORMAL
- en: So far we have treated the AP for the single-class case, but it is worth it
    to treat the more general multi-class object detection scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Mean Average Precision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the case of a multi-class detection, where every bounding box regressed can
    contain one of the available classes, the standard metric used to evaluate the
    performance of the object detector is the **Mean Average Precision** (**mAP**).
  prefs: []
  type: TYPE_NORMAL
- en: 'Computing it is straightforward—the mAP is the average precision for each class
    in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4fe09f9c-4399-4297-a2ed-385872534493.png)'
  prefs: []
  type: TYPE_IMG
- en: Knowing the metrics to use for object detection, we can improve the training
    script by adding this measurement on the validation set at the end of every training
    epoch and measuring it on a batch of training data every ten steps. Since the
    model defined so far is just a coordinate regressor without classes, the measured
    metric will be the AP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing the mAP in TensorFlow is trivial since in the `tf.metrics` package
    there is an implementation ready to use. The first parameter of the `update_state`
    method is the true labels; the second parameter is the predicted labels. For instance,
    for a binary classification problem, a possible scenario can be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: It should also be noted that the average precision and the IoU are not object-detection-specific
    metrics, but they can be used whenever a localization task is performed (the IoU)
    and the precision of the detection is measured (the mAP).
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](51f4dcda-add6-4e58-a660-75f34a7e5593.xhtml), *Semantic Segmentation
    and Custom Dataset Builder*, dedicated to the semantic segmentation task, the
    same metrics are used to measure the segmentation model performance. The only
    difference is that the IoU is measured at the pixel level and not using a bounding
    box. The training loop can be improved; in the next section, a draft of an improved
    training script is presented, but the real improvement is left as an exercise.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the training script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring the mean average precision (over a single class) requires you to fix
    a threshold for the IoU measurement and to define the `tf.metrics.Precision` object
    that computes the mean average precision over the batches.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order not to change the whole code structure, the `draw` function is used
    not only to draw the ground truth and regressed boxes, but also to measure the
    IoU and log the mean average precision summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As an exercise (see the *Exercises* section), you can use this code as a baseline
    and restructure them, in order to improve the code organization. After improving
    the code organization, you are also invited to retrain the model and analyze the
    precision plot.
  prefs: []
  type: TYPE_NORMAL
- en: Object localization alone, without the information about the class of the object
    being localized, has a limited utility, but, in practice, it is the basis of any
    object detection algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Classification and localization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An architecture like the one defined so far that has no information about the
    class of the object it's localizing is called a** region proposal**.
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to perform object detection and localization using a single neural
    network. In fact, there is nothing stopping us adding a second head on top of
    the feature extractor and training it to classify the image and at the same time
    training the regression head to regress the bounding box coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: Solving multiple tasks at the same time is the goal of multitask learning.
  prefs: []
  type: TYPE_NORMAL
- en: Multitask learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rich Caruna defines multi-task learning in his paper *Multi-task learning*
    (1997):'
  prefs: []
  type: TYPE_NORMAL
- en: '"Multitask Learning is an approach to inductive transfer that improves generalization
    by using the domain information contained in the training signals of related tasks
    as an inductive bias. It does this by learning tasks in parallel while using a
    shared representation; what is learned for each task can help other tasks be learned
    better."'
  prefs: []
  type: TYPE_NORMAL
- en: In practice, multi-task learning is a machine learning subfield with the explicit
    goal of solving multiple different tasks, exploiting commonalities and differences
    across tasks. It has been empirically shown that using the same network to solve
    multiple tasks usually results in improved learning efficiency and prediction
    accuracy compared to the performance achieved by the same network trained to solve
    the same tasks separately.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task learning also helps to fight the overfitting problem since the neural
    network is less likely to adapt its parameters to solve a specific task, so it
    has to learn how to extract meaningful features that can be useful to solve different
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Double-headed network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the past few years, several architectures for object detection and classification
    have been developed using a two-step process. The first process was to use a region
    proposal to get regions of the input image that are likely to contain an object.
    The second step was to use a simple classifier on the proposed regions to classify
    the content.
  prefs: []
  type: TYPE_NORMAL
- en: Using a double-headed neural network allows us to have faster inference time,
    since only a single forward pass of a single model is needed to achieve better
    performance overall.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the architectural side, supposing for simplicity that our feature extractor
    is AlexNet (when, instead, it is the more complex network Inception V3), adding
    a new head to the network changes the model architecture as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01503375-3d15-4fef-a4ff-4e12d4403912.png)'
  prefs: []
  type: TYPE_IMG
- en: The preceding screenshot is a representation of what a classification and localization
    network looks like. The feature extract part should be able to extract features
    general enough to make the two heads solve the two different tasks, using the
    same shared features.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the code side, as we used the Keras functional style model definition,
    adding the additional output to the model is straightforward. In fact, it is only
    a matter of adding the desired numbers of layers that compose the new head and
    adding the final layer to the outputs list of the Keras model definition. As may
    be obvious at this point in the book, this second head must end with a number
    of neurons equal to the number of classes the model will be trained to classify.
    In our case, the PASCAL VOC 2007 dataset contains 20 different classes. Therefore,
    we only have to define the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`(tf2)`'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, start with the input layer definition:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, using TensorFlow Hub, we define the fixed (non-trainable) feature extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we define the regression head, which is just a stack of fully connected
    layers that end with four linear neurons (one per bounding box coordinate):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define the classification head, which is just a stack of fully connected
    layers that is trained to classify the features extracted by the fixed (non-trainable)
    feature extractor:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can define the Keras model that will perform classification and
    localization. Please note that the model has a single input and two outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Using TensorFlow datasets, we have all the information needed to perform both
    the classification and localization easily since every row is a dictionary that
    contains the label for every bounding box present in the image. Moreover, since
    we filtered the dataset in order to have only images with a single object inside,
    we can treat the training of the classification head exactly as the training of
    the classification model, as shown in [Chapter 6](https://cdp.packtpub.com/hands_on_applied_neural_networks_with_tensorflow_2_x/wp-admin/post.php?post=31&action=edit#post_30), *Image
    Classification Using TensorFlow Hub*.
  prefs: []
  type: TYPE_NORMAL
- en: The implementation of the training script is left as an exercise for you (see
    the *Exercises* section). The only peculiarity of the training process is the
    loss function to use. In order to effectively train the network to perform different
    tasks at the same time, the loss function should contain different terms for each
    different task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Usually, the weighted sum of different terms is used as a loss function. In
    our case, one term is the classification loss that can easily be the sparse categorical
    cross-entropy loss, and the other is the regression loss (the L2 loss previously
    defined):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9395cba6-22ff-4db6-8f8c-3a9d5d3e36d1.png)'
  prefs: []
  type: TYPE_IMG
- en: The multiplicative factors [![](img/0006eaf3-b30f-43d7-b932-d3fdf746e9e1.png)] are
    hyperparameters used to give more or less *importance* (strength of the gradient
    update) to the different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying images with a single object inside and regressing the coordinate
    of the only bounding box present can be applied only in limited real-life scenarios.
    More often, instead, given an input image, it is required to localize and classify
    multiple objects at the same time (the real object detection problem).
  prefs: []
  type: TYPE_NORMAL
- en: Over the years, several models for object detection have been proposed, and
    the ones that recently outperformed all the others are all based on the concept
    of anchor. We will explore anchor-based detectors in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Anchor-based detectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anchor-based detectors rely upon the concept of anchor boxes to detect objects
    in images in a single pass, using a single architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The intuitive idea of the anchor-based detectors is to split the input image
    into several regions of interests (the anchor boxes) and apply a localization
    and regression network to each of them. The idea is to make the network learn
    not only to regress the coordinates of a bounding box and classify its content,
    but also to use the same network to look at different regions of the image in
    a single forward pass.
  prefs: []
  type: TYPE_NORMAL
- en: To train these models, it is required not only to have a dataset with the annotated
    ground truth boxes, but also to add to every input image a new collection of boxes
    that overlap (with the desired amount of IoU) the ground truth boxes.
  prefs: []
  type: TYPE_NORMAL
- en: Anchor-boxes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Anchor-boxes are a discretization of the input image in different regions, also called
    **anchors** or **bounding boxes prior**. The idea behind the concept of anchor-boxes
    is that the input can be discretized in different regions, each of them with a
    different appearance. An input image could contain big and small objects, and
    therefore the discretization should be made at different scales in order to detect
    the same time objects at different resolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'When discretizing the input in anchor boxes, the important parameters are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The grid size:** How the input is evenly divided'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The box scale levels**: Given the parent box, how to resize the current box'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The aspect ratio levels**: For every box, the ratio between width and height'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The input image can be divided into a grid with cells of equal dimensions,
    for instance, a 4 x 4 grid. Each cell of this grid can then be resized with different
    scales (0.5, 1, 2, ...) and each of them with different levels of aspect ratios
    (0.5, 1, 2, ...). As an example, the following image shows how an image can be
    "covered" by anchor boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7922f964-ec85-43f2-90f6-def4ea0f519f.png)'
  prefs: []
  type: TYPE_IMG
- en: The generation of the anchor boxes influences the network performance—the dimension
    of the smaller box represents the dimension of the smaller objects the network
    is able to detect. The same reasoning applies to the larger box as well.
  prefs: []
  type: TYPE_NORMAL
- en: In the last few years, anchor-based detectors have demonstrated they are capable
    of reaching astonishing detection performance, being not only accurate, but also
    faster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most famous anchor-based detector is **You Only Look Once** (**YOLO**),
    followed by **Single Shot MultiBox Detector** (**SSD**). The following YOLO image detects
    multiple objects in the image, at different scales, with a single forward pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79700b05-c3c0-4dde-beeb-f8472c0d7817.png)'
  prefs: []
  type: TYPE_IMG
- en: The implementation of an anchor-based detector goes beyond the scope of this
    book due to the theory needed to understand the concepts and the complexity of
    these models. Therefore, only an intuitive idea of what happens when these models
    are used has been presented.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, the problem of object detection was introduced and some basic
    solutions were proposed. We first focused on the data required and used TensorFlow
    datasets to get the PASCAL VOC 2007 dataset ready to use in a few lines of code.
    Then, the problem of using a neural network to regress the coordinate of a bounding
    box was looked at, showing how a convolutional neural network can be easily used
    to produce the four coordinates of a bounding box, starting from the image representation.
    In this way, we build a region proposal, that is, a network able to suggest where
    in the input image a single object can be detected, without producing other information
    about the detected object.
  prefs: []
  type: TYPE_NORMAL
- en: After that, the concept of multi-task learning was introduced and how to add
    a classification head next to the regression head was shown by using the Keras
    functional API. Then, we covered a brief introduction about the anchor-based detectors.
    These detectors are used to solve the problem of object detection (the detection
    and classification of multiple objects in a single image) by discretizing the
    input in thousands of regions (anchors).
  prefs: []
  type: TYPE_NORMAL
- en: We used TensorFlow 2.0 and TensorFlow Hub together, allowing us to speed up
    the training process by using the Inception v3 model as a fixed feature extractor.
    Moreover, thanks to the quick execution, mixing pure Python and TensorFlow code
    simplified the way of defining the whole training process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about semantic segmentation and dataset builder.
  prefs: []
  type: TYPE_NORMAL
- en: Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can answer all the theoretical questions and, perhaps more importantly,
    struggle to solve all the code challenges that each exercise contains:'
  prefs: []
  type: TYPE_NORMAL
- en: In the *Getting the data* section, a filtering function was applied to the PASCAL
    VOC 2007 dataset to select only the images with a single object inside. The filtering
    process, however, doesn't take into account the class balancement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a function that, given the three filtered datasets, merges them first
    and then creates three balanced splits (with a tolerable class imbalance, if it
    is not possible to have them perfectly balanced).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Use the splits created in the previous point to retrain the network for localization
    and classification defined in the chapter. How and why do the performances change?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What measures the Intersection over Union metric?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does an IoU value of 0.4 represent? A good or a bad match?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the Mean Average Precision? Explain the concept and write the formula.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is multi-task learning?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does multi-task learning improve or worsen the model performance on single tasks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In the domain of object detection, what is an anchor?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describe how an anchor-based detector looks at the input image during training
    and inference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Are mAP and IoU object detection metrics only?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To improve the code of the object detection and localization network, add the
    support saving the model at the end of every epoch into a checkpoint and to restore
    the model (and the global step variable) status to continue the training process.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code of the localization and regression networks explicitly use a `draw` function
    that not only draws the bounding boxes but also measures the mAP. Improve the
    code quality by creating different functions for each different feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The code that measures network performance only uses three samples. This is
    wrong, can you explain the reason? Change the code in order to use a single training
    batch during the training and the whole validation set at the end of every epoch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Define a training script for the model defined in the "Multi-Headed Network
    and Multi-Task learning": train the regression and classification head at the
    same time and measure the training and validation accuracy at the end of every
    training epoch.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Filter the PASCAL VOC train, validation, and test datasets to produce only images
    with at least a person inside (there can be other labeled objects present in the
    picture).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Replace the regression and classification head of the trained localization and
    classification network with two new heads. The classification head should now
    have a single neuron that represents the probability of the image to contain a
    person. The regression head should regress the coordinates of the objects labeled
    as a person.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply transfer learning to train the previously defined network. Stop the training
    process when the mAP on the person class stops increasing (with a tolerance of
    +/- 0.2) for 50 steps.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a Python script that generates anchor boxes at different resolutions
    and scales.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
