["```\nimport tensorflow as tf\n```", "```\nA = tf.constant([2])\n```", "```\nB = tf.constant([3])\n```", "```\nC = tf.add(A,B)\n```", "```\n#C = A + B is also a way to define the sum of the terms\n```", "```\nsession = tf.Session()\n```", "```\nresult = session.run(C)\nprint(result)\n```", "```\nOutput:\n[5]\n```", "```\nsalar_var = tf.constant([4])\nvector_var = tf.constant([5,4,2])\nmatrix_var = tf.constant([[1,2,3],[2,2,4],[3,5,5]])\ntensor = tf.constant( [ [[1,2,3],[2,3,4],[3,4,5]] , [[4,5,6],[5,6,7],[6,7,8]] , [[7,8,9],[8,9,10],[9,10,11]] ] )\nwith tf.Session() as session:\n    result = session.run(salar_var)\n    print \"Scalar (1 entry):\\n %s \\n\" % result\n    result = session.run(vector_var)\n    print \"Vector (3 entries) :\\n %s \\n\" % result\n    result = session.run(matrix_var)\n    print \"Matrix (3x3 entries):\\n %s \\n\" % result\n    result = session.run(tensor)\n    print \"Tensor (3x3x3 entries) :\\n %s \\n\" % result\n```", "```\nOutput:\nScalar (1 entry):\n [2] \n\nVector (3 entries) :\n [5 6 2] \n\nMatrix (3x3 entries):\n [[1 2 3]\n [2 3 4]\n [3 4 5]] \n\nTensor (3x3x3 entries) :\n [[[ 1  2  3]\n  [ 2  3  4]\n  [ 3  4  5]]\n\n [[ 4  5  6]\n  [ 5  6  7]\n  [ 6  7  8]]\n\n [[ 7  8  9]\n  [ 8  9 10]\n  [ 9 10 11]]]\n```", "```\nMatrix_one = tf.constant([[1,2,3],[2,3,4],[3,4,5]])\nMatrix_two = tf.constant([[2,2,2],[2,2,2],[2,2,2]])\nfirst_operation = tf.add(Matrix_one, Matrix_two)\nsecond_operation = Matrix_one + Matrix_two\nwith tf.Session() as session:\n    result = session.run(first_operation)\n    print \"Defined using tensorflow function :\"\n    print(result)\n    result = session.run(second_operation)\n    print \"Defined using normal expressions :\"\n    print(result)\n```", "```\nOutput:\nDefined using tensorflow function :\n[[3 4 5]\n [4 5 6]\n [5 6 7]]\nDefined using normal expressions :\n[[3 4 5]\n [4 5 6]\n [5 6 7]]\n```", "```\nMatrix_one = tf.constant([[2,3],[3,4]])\nMatrix_two = tf.constant([[2,3],[3,4]])\nfirst_operation = tf.matmul(Matrix_one, Matrix_two)\nwith tf.Session() as session:\n    result = session.run(first_operation)\n    print \"Defined using tensorflow function :\"\n    print(result)\n```", "```\nOutput:\nDefined using tensorflow function :\n[[13 18]\n [18 25]]\n```", "```\nstate = tf.Variable(0)\n```", "```\none = tf.constant(1)\nnew_value = tf.add(state, one)\nupdate = tf.assign(state, new_value)\n```", "```\ninit_op = tf.global_variables_initializer()\n```", "```\nwith tf.Session() as session:\n session.run(init_op)\n print(session.run(state))\n for _ in range(3):\n    session.run(update)\n    print(session.run(state))\n```", "```\nOutput:\n0\n1\n2\n3\n```", "```\na=tf.placeholder(tf.float32)\n```", "```\nb=a*2\n```", "```\nwith tf.Session() as sess:\n    result = sess.run(b,feed_dict={a:3.5})\n    print result\n```", "```\nOutput:\n7.0\n```", "```\ndictionary={a: [ [ [1,2,3],[4,5,6],[7,8,9],[10,11,12] ] , [ [13,14,15],[16,17,18],[19,20,21],[22,23,24] ] ] }\nwith tf.Session() as sess:\n    result = sess.run(b,feed_dict=dictionary)\n    print result\n```", "```\nOutput:\n[[[  2\\.   4\\.   6.]\n  [  8\\.  10\\.  12.]\n  [ 14\\.  16\\.  18.]\n  [ 20\\.  22\\.  24.]]\n\n [[ 26\\.  28\\.  30.]\n  [ 32\\.  34\\.  36.]\n  [ 38\\.  40\\.  42.]\n  [ 44\\.  46\\.  48.]]]\n```", "```\na = tf.constant([5])\nb = tf.constant([2])\nc = tf.add(a,b)\nd = tf.subtract(a,b)\nwith tf.Session() as session:\n    result = session.run(c)\n    print 'c =: %s' % result\n    result = session.run(d)\n    print 'd =: %s' % result\n```", "```\nOutput:\nc =: [7]\nd =: [3]\n```", "```\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (10, 6)\n```", "```\ninput_values = np.arange(0.0, 5.0, 0.1)\ninput_values\n```", "```\nOutput:\narray([ 0\\. ,  0.1,  0.2,  0.3,  0.4,  0.5,  0.6,  0.7,  0.8,  0.9,  1\\. ,\n        1.1,  1.2,  1.3,  1.4,  1.5,  1.6,  1.7,  1.8,  1.9,  2\\. ,  2.1,\n        2.2,  2.3,  2.4,  2.5,  2.6,  2.7,  2.8,  2.9,  3\\. ,  3.1,  3.2,\n        3.3,  3.4,  3.5,  3.6,  3.7,  3.8,  3.9,  4\\. ,  4.1,  4.2,  4.3,\n        4.4,  4.5,  4.6,  4.7,  4.8,  4.9])\n```", "```\n##You can adjust the slope and intercept to verify the changes in the graph\nweight=1\nbias=0\noutput = weight*input_values + bias\nplt.plot(input_values,output)\nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\nOutput:\n```", "```\ninput_values = np.random.rand(100).astype(np.float32)\n```", "```\noutput_values = input_values * 2 + 3\noutput_values = np.vectorize(lambda y: y + np.random.normal(loc=0.0, scale=0.1))(output_values)\n```", "```\nlist(zip(input_values,output_values))[5:10]\n```", "```\nOutput:\n[(0.25240293, 3.474361759429548), \n(0.946697, 4.980617375175061), \n(0.37582186, 3.650345806087635), \n(0.64025956, 4.271037640404975), \n(0.62555283, 4.37001850440196)]\n```", "```\nweight = tf.Variable(1.0)\nbias = tf.Variable(0.2)\npredicted_vals = weight * input_values + bias\n```", "```\nmodel_loss = tf.reduce_mean(tf.square(predicted_vals - output_values))\n```", "```\nmodel_optimizer = tf.train.GradientDescentOptimizer(0.5)\ntrain = model_optimizer.minimize(model_loss)\n```", "```\ninit = tf.global_variables_initializer()\nsess = tf.Session()\nsess.run(init)\n```", "```\ntrain_data = []\nfor step in range(100):\n    evals = sess.run([train,weight,bias])[1:]\n    if step % 5 == 0:\n       print(step, evals)\n       train_data.append(evals)\n```", "```\nOutput:\n(0, [2.5176678, 2.9857566])\n(5, [2.4192538, 2.3015416])\n(10, [2.5731843, 2.221911])\n(15, [2.6890132, 2.1613526])\n(20, [2.7763696, 2.1156814])\n(25, [2.8422525, 2.0812368])\n(30, [2.8919399, 2.0552595])\n(35, [2.9294133, 2.0356679])\n(40, [2.957675, 2.0208921])\n(45, [2.9789894, 2.0097487])\n(50, [2.9950645, 2.0013444])\n(55, [3.0071881, 1.995006])\n(60, [3.0163314, 1.9902257])\n(65, [3.0232272, 1.9866205])\n(70, [3.0284278, 1.9839015])\n(75, [3.0323503, 1.9818509])\n(80, [3.0353084, 1.9803041])\n(85, [3.0375392, 1.9791379])\n(90, [3.039222, 1.9782581])\n(95, [3.0404909, 1.9775947])\n```", "```\nprint('Plotting the data points with their corresponding fitted line...')\nconverter = plt.colors\ncr, cg, cb = (1.0, 1.0, 0.0)\n\nfor f in train_data:\n\n    cb += 1.0 / len(train_data)\n    cg -= 1.0 / len(train_data)\n\n    if cb > 1.0: cb = 1.0\n\n    if cg < 0.0: cg = 0.0\n\n    [a, b] = f\n    f_y = np.vectorize(lambda x: a*x + b)(input_values)\n    line = plt.plot(input_values, f_y)\n    plt.setp(line, color=(cr,cg,cb))\n\nplt.plot(input_values, output_values, 'ro')\ngreen_line = mpatches.Patch(color='red', label='Data Points')\nplt.legend(handles=[green_line])\nplt.show()\n```", "```\nOutput:\n```", "```\nimport tensorflow as tf\n\nimport pandas as pd\n\nimport numpy as np\nimport time\nfrom sklearn.datasets import load_iris\nfrom sklearn.cross_validation import train_test_split\nimport matplotlib.pyplot as plt\n```", "```\niris_dataset = load_iris()\niris_input_values, iris_output_values = iris_dataset.data[:-1,:], iris_dataset.target[:-1]\niris_output_values= pd.get_dummies(iris_output_values).values\ntrain_input_values, test_input_values, train_target_values, test_target_values = train_test_split(iris_input_values, iris_output_values, test_size=0.33, random_state=42)\n```", "```\n# numFeatures is the number of features in our input data.\n# In the iris dataset, this number is '4'.\nnum_explanatory_features = train_input_values.shape[1]\n\n# numLabels is the number of classes our data points can be in.\n# In the iris dataset, this number is '3'.\nnum_target_values = train_target_values.shape[1]\n\n# Placeholders\n# 'None' means TensorFlow shouldn't expect a fixed number in that dimension\ninput_values = tf.placeholder(tf.float32, [None, num_explanatory_features]) # Iris has 4 features, so X is a tensor to hold our data.\noutput_values = tf.placeholder(tf.float32, [None, num_target_values]) # This will be our correct answers matrix for 3 classes.\n\n```", "```\n#Randomly sample from a normal distribution with standard deviation .01\n\nweights = tf.Variable(tf.random_normal([num_explanatory_features,num_target_values],\n                                      mean=0,\n                                      stddev=0.01,\n                                      name=\"weights\"))\n\nbiases = tf.Variable(tf.random_normal([1,num_target_values],\n                                   mean=0,\n                                   stddev=0.01,\n                                   name=\"biases\"))\n```", "```\n# Three-component breakdown of the Logistic Regression equation.\n# Note that these feed into each other.\napply_weights = tf.matmul(input_values, weights, name=\"apply_weights\")\nadd_bias = tf.add(apply_weights, biases, name=\"add_bias\")\nactivation_output = tf.nn.sigmoid(add_bias, name=\"activation\")\n```", "```\n#Number of training epochs\nnum_epochs = 700\n# Defining our learning rate iterations (decay)\nlearning_rate = tf.train.exponential_decay(learning_rate=0.0008,\n                                          global_step=1,\n                                          decay_steps=train_input_values.shape[0],\n                                          decay_rate=0.95,\n                                          staircase=True)\n\n# Defining our cost function - Squared Mean Error\nmodel_cost = tf.nn.l2_loss(activation_output - output_values, name=\"squared_error_cost\")\n# Defining our Gradient Descent\nmodel_train = tf.train.GradientDescentOptimizer(learning_rate).minimize(model_cost)\n```", "```\n# tensorflow session\nsess = tf.Session()\n\n# Initialize our variables.\ninit = tf.global_variables_initializer()\nsess.run(init)\n\n#We also want some additional operations to keep track of our model's efficiency over time. We can do this like so:\n# argmax(activation_output, 1) returns the label with the most probability\n# argmax(output_values, 1) is the correct label\ncorrect_predictions = tf.equal(tf.argmax(activation_output,1),tf.argmax(output_values,1))\n\n# If every false prediction is 0 and every true prediction is 1, the average returns us the accuracy\nmodel_accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))\n\n# Summary op for regression output\nactivation_summary = tf.summary.histogram(\"output\", activation_output)\n\n# Summary op for accuracy\naccuracy_summary = tf.summary.scalar(\"accuracy\", model_accuracy)\n\n# Summary op for cost\ncost_summary = tf.summary.scalar(\"cost\", model_cost)\n\n# Summary ops to check how variables weights and biases are updating after each iteration to be visualized in TensorBoard\nweight_summary = tf.summary.histogram(\"weights\", weights.eval(session=sess))\nbias_summary = tf.summary.histogram(\"biases\", biases.eval(session=sess))\n\nmerged = tf.summary.merge([activation_summary, accuracy_summary, cost_summary, weight_summary, bias_summary])\nwriter = tf.summary.FileWriter(\"summary_logs\", sess.graph)\n\n#Now we can define and run the actual training loop, like this:\n# Initialize reporting variables\n\ninital_cost = 0\ndiff = 1\nepoch_vals = []\naccuracy_vals = []\ncosts = []\n\n# Training epochs\nfor i in range(num_epochs):\n    if i > 1 and diff < .0001:\n       print(\"change in cost %g; convergence.\"%diff)\n       break\n\n    else:\n       # Run training step\n       step = sess.run(model_train, feed_dict={input_values: train_input_values, output_values: train_target_values})\n\n       # Report some stats evert 10 epochs\n       if i % 10 == 0:\n           # Add epoch to epoch_values\n           epoch_vals.append(i)\n\n           # Generate the accuracy stats of the model\n           train_accuracy, new_cost = sess.run([model_accuracy, model_cost], feed_dict={input_values: train_input_values, output_values: train_target_values})\n\n           # Add accuracy to live graphing variable\n           accuracy_vals.append(train_accuracy)\n\n           # Add cost to live graphing variable\n           costs.append(new_cost)\n>\n           # Re-assign values for variables\n           diff = abs(new_cost - inital_cost)\n           cost = new_cost\n\n           print(\"Training step %d, accuracy %g, cost %g, cost change %g\"%(i, train_accuracy, new_cost, diff))\n```", "```\nOutput:\nTraining step 0, accuracy 0.343434, cost 34.6022, cost change 34.6022\nTraining step 10, accuracy 0.434343, cost 30.3272, cost change 30.3272\nTraining step 20, accuracy 0.646465, cost 28.3478, cost change 28.3478\nTraining step 30, accuracy 0.646465, cost 26.6752, cost change 26.6752\nTraining step 40, accuracy 0.646465, cost 25.2844, cost change 25.2844\nTraining step 50, accuracy 0.646465, cost 24.1349, cost change 24.1349\nTraining step 60, accuracy 0.646465, cost 23.1835, cost change 23.1835\nTraining step 70, accuracy 0.646465, cost 22.3911, cost change 22.3911\nTraining step 80, accuracy 0.646465, cost 21.7254, cost change 21.7254\nTraining step 90, accuracy 0.646465, cost 21.1607, cost change 21.1607\nTraining step 100, accuracy 0.666667, cost 20.677, cost change 20.677\nTraining step 110, accuracy 0.666667, cost 20.2583, cost change 20.2583\nTraining step 120, accuracy 0.666667, cost 19.8927, cost change 19.8927\nTraining step 130, accuracy 0.666667, cost 19.5705, cost change 19.5705\nTraining step 140, accuracy 0.666667, cost 19.2842, cost change 19.2842\nTraining step 150, accuracy 0.666667, cost 19.0278, cost change 19.0278\nTraining step 160, accuracy 0.676768, cost 18.7966, cost change 18.7966\nTraining step 170, accuracy 0.69697, cost 18.5867, cost change 18.5867\nTraining step 180, accuracy 0.69697, cost 18.3951, cost change 18.3951\nTraining step 190, accuracy 0.717172, cost 18.2191, cost change 18.2191\nTraining step 200, accuracy 0.717172, cost 18.0567, cost change 18.0567\nTraining step 210, accuracy 0.737374, cost 17.906, cost change 17.906\nTraining step 220, accuracy 0.747475, cost 17.7657, cost change 17.7657\nTraining step 230, accuracy 0.747475, cost 17.6345, cost change 17.6345\nTraining step 240, accuracy 0.757576, cost 17.5113, cost change 17.5113\nTraining step 250, accuracy 0.787879, cost 17.3954, cost change 17.3954\nTraining step 260, accuracy 0.787879, cost 17.2858, cost change 17.2858\nTraining step 270, accuracy 0.787879, cost 17.182, cost change 17.182\nTraining step 280, accuracy 0.787879, cost 17.0834, cost change 17.0834\nTraining step 290, accuracy 0.787879, cost 16.9895, cost change 16.9895\nTraining step 300, accuracy 0.79798, cost 16.8999, cost change 16.8999\nTraining step 310, accuracy 0.79798, cost 16.8141, cost change 16.8141\nTraining step 320, accuracy 0.79798, cost 16.732, cost change 16.732\nTraining step 330, accuracy 0.79798, cost 16.6531, cost change 16.6531\nTraining step 340, accuracy 0.808081, cost 16.5772, cost change 16.5772\nTraining step 350, accuracy 0.818182, cost 16.5041, cost change 16.5041\nTraining step 360, accuracy 0.838384, cost 16.4336, cost change 16.4336\nTraining step 370, accuracy 0.838384, cost 16.3655, cost change 16.3655\nTraining step 380, accuracy 0.838384, cost 16.2997, cost change 16.2997\nTraining step 390, accuracy 0.838384, cost 16.2359, cost change 16.2359\nTraining step 400, accuracy 0.848485, cost 16.1741, cost change 16.1741\nTraining step 410, accuracy 0.848485, cost 16.1141, cost change 16.1141\nTraining step 420, accuracy 0.848485, cost 16.0558, cost change 16.0558\nTraining step 430, accuracy 0.858586, cost 15.9991, cost change 15.9991\nTraining step 440, accuracy 0.858586, cost 15.944, cost change 15.944\nTraining step 450, accuracy 0.858586, cost 15.8903, cost change 15.8903\nTraining step 460, accuracy 0.868687, cost 15.8379, cost change 15.8379\nTraining step 470, accuracy 0.878788, cost 15.7869, cost change 15.7869\nTraining step 480, accuracy 0.878788, cost 15.7371, cost change 15.7371\nTraining step 490, accuracy 0.878788, cost 15.6884, cost change 15.6884\nTraining step 500, accuracy 0.878788, cost 15.6409, cost change 15.6409\nTraining step 510, accuracy 0.878788, cost 15.5944, cost change 15.5944\nTraining step 520, accuracy 0.878788, cost 15.549, cost change 15.549\nTraining step 530, accuracy 0.888889, cost 15.5045, cost change 15.5045\nTraining step 540, accuracy 0.888889, cost 15.4609, cost change 15.4609\nTraining step 550, accuracy 0.89899, cost 15.4182, cost change 15.4182\nTraining step 560, accuracy 0.89899, cost 15.3764, cost change 15.3764\nTraining step 570, accuracy 0.89899, cost 15.3354, cost change 15.3354\nTraining step 580, accuracy 0.89899, cost 15.2952, cost change 15.2952\nTraining step 590, accuracy 0.909091, cost 15.2558, cost change 15.2558\nTraining step 600, accuracy 0.909091, cost 15.217, cost change 15.217\nTraining step 610, accuracy 0.909091, cost 15.179, cost change 15.179\nTraining step 620, accuracy 0.909091, cost 15.1417, cost change 15.1417\nTraining step 630, accuracy 0.909091, cost 15.105, cost change 15.105\nTraining step 640, accuracy 0.909091, cost 15.0689, cost change 15.0689\nTraining step 650, accuracy 0.909091, cost 15.0335, cost change 15.0335\nTraining step 660, accuracy 0.909091, cost 14.9987, cost change 14.9987\nTraining step 670, accuracy 0.909091, cost 14.9644, cost change 14.9644\nTraining step 680, accuracy 0.909091, cost 14.9307, cost change 14.9307\nTraining step 690, accuracy 0.909091, cost 14.8975, cost change 14.8975\n\n```", "```\n# test the model against the test set\nprint(\"final accuracy on test set: %s\" %str(sess.run(model_accuracy,\n                                                    feed_dict={input_values: test_input_values,\n                                                               output_values: test_target_values}))\n```", "```\nOutput:\nfinal accuracy on test set: 0.9\n```"]