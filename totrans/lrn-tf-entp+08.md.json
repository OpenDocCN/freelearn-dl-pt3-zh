["```\ngit clone **https://github.com/PacktPublishing/learn-tensorflow-enterprise.git**\n```", "```\ngcloud --help\n```", "```\n    curl -H 'Authorization: Bearer $(gcloud auth print-access-token)'  \\  https://ml.googleapis.com/v1/projects/<your-project-id>:getConfig\n    ```", "```\n    curl -H 'Authorization: Bearer $(gcloud auth print-access-token)'  \\  -H 'Content-Type: application/json' -d '{}'  \\  https://serviceusage.googleapis.com/v1beta1/projects/<serviceAccountProject>/services/tpu.googleapis.com:generateServiceIdentity\n    ```", "```\n    setup.py. In install_requires, you will see a Python list that contains TensorFlow datasets or tensorflow_hub. This is where dependencies are added to the runtime in Google Cloud AI Platform.\n    ```", "```\n    gcloud ai-platform jobs submit training cloudtpu \\\n    --staging-bucket=gs://ai-tpu-experiment \\\n    --package-path=python \\\n    --module-name=python.ScriptProject.traincloudtpu_resnet_cache \\\n    --runtime-version=2.2 \\\n    --python-version=3.7 \\\n    --scale-tier=BASIC_TPU \\\n    --region=us-central1 \\\n    -- \\\n    --distribution_strategy=tpu \\\n    --model_dir=gs://ai-tpu-experiment/traincloudtpu_tfkd_resnet_cache \\\n    --train_epochs=10 \\\n    --data_dir=gs://ai-tpu-experiment/tfrecord-flowers\n    ```", "```\ngcloud ai-platform jobs submit training cloudtpu \\\n```", "```\n--staging-bucket=gs://ai-tpu-experiment \\\n```", "```\n--package-path=python \\\n```", "```\n--module-name=python.ScriptProject.traincloudtpu_resnet_cache \\\n```", "```\n--runtime-version=2.1 \\\n```", "```\n--python-version=3.7 \\\n```", "```\n--scale-tier=BASIC_TPU \\\n```", "```\n--region=us-central1 \\\n```", "```\nmodule-name=python.ScriptProject.traincloudtpu_resnet_cache\n```", "```\n--distribution_strategy=tpu \\\n```", "```\n--model_dir=gs://ai-tpu-experiment/traincloudtpu_tfkd_resnet_cache \\\n```", "```\n--train_epochs=10 \\--data_dir=gs://ai-tpu-experiment/tfrecord-flowers\n```", "```\ndef run( input parameters ):\n```", "```\n\t# specify distribute strategy (https://cloud.google.com/\n```", "```\n\tai-platform/training/docs/using-tpus)\n```", "```\n\timport tensorflow as tf\n```", "```\n\tif distribution_strategy==TPU: \n```", "```\n\t\tresolver = tf.distribute.cluster_resolver.\t\t\t\tTPUClusterResolver()\n```", "```\n\ttf.config.experimental_connect_to_cluster(resolver)\n```", "```\n\ttf.tpu.experimental.initialize_tpu_system(resolver)\n```", "```\n\tstrategy = tf.distribute.experimental.TPUStrategy(resolver)\n```", "```\n\t# build data streaming pipeline with tf.io and tf.data.TFRecordDataset\n```", "```\n\t# build model\n```", "```\n\t# train model\n```", "```\n\t# save results\n```", "```\ndef main():\n```", "```\nrun(input parameters)\n```", "```\nif __name__ == '__main__'\n```", "```\napp.run(main)\n```", "```\n    root_dir = flags_obj.data_dir # this is gs://<bucket>/folder where tfrecord are stored\n    train_file_pattern = '{}/image_classification_builder-train*.tfrecord*'.format(root_dir)\n    val_file_pattern = '{}/image_classification_builder-validation*.tfrecord*'.format(root_dir)\n    train_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(train_file_pattern))\n    val_all_files = tf.data.Dataset.list_files( tf.io.gfile.glob(val_file_pattern))\n    train_all_ds = tf.data.TFRecordDataset(train_all_files,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    val_all_ds = tf.data.TFRecordDataset(val_all_files,num_parallel_reads=tf.data.experimental.AUTOTUNE)\n    ```", "```\n        def decode_and_resize(serialized_example):\n            # resized image should be [224, 224, 3] and have \t          value range [0, 255] \n            # label is integer index of class.\n\n            parsed_features = tf.io.parse_single_example(\n            serialized_example,\n            features = {\n            'image/channels' :  tf.io.FixedLenFeature([],    \t                                           tf.int64),\n            'image/class/label' :  tf.io.FixedLenFeature([], \t                                              tf.int64),\n            'image/class/text' : tf.io.FixedLenFeature([], \t                                           tf.string),\n            'image/colorspace' : tf.io.FixedLenFeature([], \t                                           tf.string),\n            'image/encoded' : tf.io.FixedLenFeature([], \t                                                                                      \t                                        tf.string),\n            'image/filename' : tf.io.FixedLenFeature([], \t                                            \t                                         tf.string),\n            'image/format' : tf.io.FixedLenFeature([], \t                                                                                    \t                                       tf.string),\n            'image/height' : tf.io.FixedLenFeature([],  \t                                         \t                                        tf.int64),\n            'image/width' : tf.io.FixedLenFeature([], \n                                            tf.int64)\n            })\n            image = tf.io.decode_jpeg(parsed_features[\n                                'image/encoded'], channels=3)\n            label = tf.cast(parsed_features[\n                              'image/class/label'], tf.int32)\n            label_txt = tf.cast(parsed_features[\n                              'image/class/text'], tf.string)\n            label_one_hot = tf.one_hot(label, depth = 5)\n            resized_image = tf.image.resize(image, \n                                [224, 224], method='nearest')\n            return resized_image, label_one_hot\n    ```", "```\n      return resized_image, label_one_hot, label_txt, label\n    ```", "```\n        dataset = train_all_ds.map(decode_and_resize)\n    val_dataset = val_all_ds.map(decode_and_resize)\n    ```", "```\n    def normalize(image, label):\n            #Convert `image` from [0, 255] -> [0, 1.0] floats \n            image = tf.cast(image, tf.float32) / 255\\. + 0.5\n            return image, label\n    ```", "```\n    def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n            if cache:\n                if isinstance(cache, str):\n                    ds = ds.cache(cache)\n                else:\n                    ds = ds.cache()\n            ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n            ds = ds.repeat()\n            ds = ds.batch(BATCH_SIZE)\n            AUTOTUNE = tf.data.experimental.AUTOTUNE\n            ds = ds.prefetch(buffer_size=AUTOTUNE)\n            return ds\n    ```", "```\n    AUTOTUNE = tf.data.experimental.AUTOTUNE\n    BATCH_SIZE = flags_obj.train_batch_size\n    VALIDATION_BATCH_SIZE = flags_obj.validation_batch_size\n    train_dataset = train_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\n    val_dataset = val_dataset.map(normalize, num_parallel_calls=AUTOTUNE)\n    val_ds = val_dataset.batch(VALIDATION_BATCH_SIZE)   \n    train_ds = prepare_for_training(train_dataset)\n    ```", "```\n    with strategy.scope():\n      base_model = tf.keras.applications.ResNet50(\n          input_shape=(224,224,3), include_top=False, \t   \t      weights='imagenet')\n      model = tf.keras.Sequential(\n          [base_model,\n           tf.keras.layers.GlobalAveragePooling2D(),\n           tf.keras.layers.Dense(5, \n                                 activation='softmax', \n                                 name = 'custom_class')\n           ])\n      lr_schedule = \\\n      tf.keras.optimizers.schedules.ExponentialDecay(\n          0.05, decay_steps=100000, decay_rate=0.96)\n      optimizer = tf.keras.optimizers.SGD(\n          learning_rate=lr_schedule)\n    model.compile(optimizer=optimizer, \n      loss=tf.keras.losses.CategoricalCrossentropy(\n          from_logits=True, label_smoothing=0.1),\n          metrics=['accuracy'])\n    ```", "```\n    checkpoint_prefix = os.path.join(flags_obj.model_dir, \t                                          'ckpt_{epoch}')\n        callbacks = [\n        tf.keras.callbacks.ModelCheckpoint\n    \t\t(filepath=checkpoint_prefix,                                    \n    \t\tsave_weights_only=True)]\n    ```", "```\n    train_sample_size=0\n        for raw_record in train_all_ds:\n            train_sample_size += 1\n        print('TRAIN_SAMPLE_SIZE = ', train_sample_size)\n        validation_sample_size=0\n        for raw_record in val_all_ds:\n            validation_sample_size += 1\n        print('VALIDATION_SAMPLE_SIZE = ', \n               validation_sample_size)\n        steps_per_epoch = train_sample_size // BATCH_SIZE\n        validation_steps = validation_sample_size \n                                     // VALIDATION_BATCH_SIZE\n    ```", "```\n    hist = model.fit(\n            train_ds,\n            epochs=flags_obj.train_epochs, \n                             steps_per_epoch=steps_per_epoch,\n            validation_data=val_ds,\n            validation_steps=validation_steps,\n            callbacks=callbacks)\n        model_save_dir = os.path.join(flags_obj.model_dir,  \t                                            \n    'save_model')\n        model.save(model_save_dir)\n    ```", "```\nvs_code % gcloud ai-platform jobs submit training traincloudtpu_tfk_resnet50 \\\n```", "```\n--staging-bucket=gs://ai-tpu-experiment \\\n```", "```\n--package-path=python \\\n```", "```\n--module-name=python.ScriptProject.trainer \\\n```", "```\n--runtime-version=2.2 \\\n```", "```\n--python-version=3.7 \\\n```", "```\n--scale-tier=BASIC_TPU \\\n```", "```\n--region=us-central1 \\\n```", "```\n-- \\\n```", "```\n--distribution_strategy=tpu \\\n```", "```\n--model_dir=gs://ai-tpu-experiment/traincloudtpu_tfk_resnet50 \\\n```", "```\n--train_epochs=10 \\\n```", "```\n--data_dir=gs://ai-tpu-experiment/tfrecord-flowers\n```", "```\nJob [traincloudtpu_tfk_resnet50] submitted successfully.\n```", "```\n  $ gcloud ai-platform jobs describe traincloudtpu_tfk_resnet50\n```", "```\n  $ gcloud ai-platform jobs stream-logs traincloudtpu_tfk_resnet50\n```", "```\njobId: traincloudtpu_tfk_resnet50\n```", "```\nstate: QUEUED\n```", "```\nJob name: traincloudtpu_tfk_resnet50\n```", "```\nStaging bucket is gs://ai-tpu-experiment\n```", "```\nBucket to save the model is gs://ai-tpu-experiment/traincloudtpu_tfk_resnet50\n```", "```\nTraining data is in gs://tfrecord-dataset/flowers\n```", "```\nvs_code % gcloud ai-platform jobs describe traincloudtpu_tfk_resnet50\n```", "```\ncreateTime: ‚2020-08-09T20:59:16Z'\n```", "```\netag: QMhh5Jz_KMU=\n```", "```\njobId: traincloudtpu_tfk_resnet50\n```", "```\nstate: PREPARING\n```", "```\ntrainingInput:\n```", "```\n  args:\n```", "```\n  - --distribution_strategy=tpu\n```", "```\n  - --model_dir=gs://ai-tpu-experiment/traincloudtpu_tfk_resnet50\n```", "```\n  - --train_epochs=10\n```", "```\n  - --data_dir=gs://ai-tpu-experiment/tfrecord-flowers\n```", "```\n  packageUris:\n```", "```\n  - gs://ai-tpu-experiment/traincloudtpu_tfk_resnet50/XXXXXX/official-0.0.0.tar.gz\n```", "```\n  pythonModule: python.ScriptProject.trainer\n```", "```\n  pythonVersion: '3.7'\n```", "```\n  region: us-central1\n```", "```\n  runtimeVersion: '2.2'\n```", "```\n  scaleTier: BASIC_TPU\n```", "```\ntrainingOutput: {}\n```", "```\nm = tf.keras.Sequential([\n```", "```\n    hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4', trainable=False),  \n```", "```\n    tf.keras.layers.Dense(num_classes, activation='softmax')\n```", "```\n])\n```", "```\nm.build([None, 224, 224, 3])  # Batch input shape.\n```", "```\nos.environ['TFHUB_CACHE_DIR'] = 'gs://ai-tpu-experiment/model-cache-dir/imagenet_resnet_v2_50_feature_vector_4'\n```", "```\nwith strategy.scope():\n```", "```\n  model = tf.keras.Sequential([\n```", "```\n     tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n```", "```\n     hub.KerasLayer('https://tfhub.dev/google/imagenet/resnet_v2_50/feature_vector/4',\n```", "```\n                   trainable=flags_obj.fine_tuning_choice),\n```", "```\n            tf.keras.layers.Dense(5, activation='softmax', \n```", "```\n                                         name = 'custom_class')\n```", "```\n        ])\n```", "```\nvs_code % gcloud ai-platform jobs submit training traincloudtpu_tfhub_resnet50 \\\n```", "```\n--staging-bucket=gs://ai-tpu-experiment \\\n```", "```\n--package-path=python \\\n```", "```\n--module-name=python.ScriptProject.trainer_hub \\\n```", "```\n--runtime-version=2.2 \\\n```", "```\n--python-version=3.7 \\\n```", "```\n--scale-tier=BASIC_TPU \\\n```", "```\n--region=us-central1 \\\n```", "```\n-- \\\n```", "```\n--distribution_strategy=tpu \\\n```", "```\n--model_dir=gs://ai-tpu-experiment/traincloudtpu_tfhub_resnet50 \\\n```", "```\n--train_epochs=10 \\\n```", "```\n--data_dir=gs://ai-tpu-experiment/tfrecord-flowers\n```", "```\nJob [traincloudtpu_tfhub_resnet50] submitted successfully.\n```", "```\n  $ gcloud ai-platform jobs describe traincloudtpu_tfhub_resnet50\n```", "```\n  $ gcloud ai-platform jobs stream-logs traincloudtpu_tfhub_resnet50\n```", "```\njobId: traincloudtpu_tfhub_resnet50\n```", "```\nstate: QUEUED\n```", "```\n        if flags_obj.distribution_strategy == 'tpu':\n            resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n            tf.config.experimental_connect_to_cluster(resolver)\n            tf.tpu.experimental.initialize_tpu_system(resolver)\n            strategy = tf.distribute.experimental.TPUStrategy(resolver)\n            strategy_scope = strategy.scope()\n            print('All devices: ', tf.config.list_logical_devices('TPU'))\n        elif flags_obj.distribution_strategy == 'gpu': \n            devices = ['device:GPU:%d' % i for i in range(flags_obj.num_gpus)]\n            strategy = tf.distribute.MirroredStrategy(device=devices)\n            strategy_scope = strategy.scope()\n    ```", "```\n    vs_code % gcloud ai-platform jobs submit training traincloudgpu_tfhub_resnet_gpu_1 \\\n    --staging-bucket=gs://ai-tpu-experiment \\\n    --package-path=python \\\n    --module-name=python.ScriptProject.trainer_hub \\\n    --runtime-version=2.2 \\\n    --python-version=3.7 \\\n    --scale-tier=BASIC_GPU \\\n    --region=us-central1 \\\n    -- \\\n    --distribution_strategy=gpu \\\n    --model_dir=gs://ai-tpu-experiment/traincloudgpu_tfhub_resnet_gpu_1 \\\n    --train_epochs=10 \\\n    --data_dir=gs://ai-tpu-experiment/tfrecord-flowers\n    Job [traincloudtpu_tfhub_resnet_gpu_1] submitted successfully.\n    ```", "```\n  $ gcloud ai-platform jobs describe traincloudgpu_tfhub_resnet_gpu_1\n```", "```\n  $ gcloud ai-platform jobs stream-logs traincloudtpu_tfhub_resnet_gpu_1\n```", "```\njobId: traincloudgpu_tfhub_resnet_gpu_1\n```", "```\nstate: QUEUED\n```"]