- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multi-Modal Networks and Image Captioning with ResNets and Transformer Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '"A picture is worth a thousand words" is a famous adage. In this chapter, we''ll
    put this adage to the test and generate captions for an image. In doing so, we''ll
    work with **multi-modal** networks. Thus far, we have operated on text as input.
    Humans can handle multiple sensory inputs together to make sense of the environment
    around them. We can watch a video with subtitles and combine the information provided
    to understand the scene. We can use facial expressions and lip movement along
    with sounds to understand speech. We can recognize text in an image, and we can
    answer natural language questions about images. In other words, we have the ability
    to process information from different modalities at the same time, and then put
    them together to understand the world around us. The future of artificial intelligence
    and deep learning is in building multi-modal networks as they closely mimic human
    cognitive functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Recent advances in image, speech, and text processing lay a solid foundation
    for multi-modal networks. This chapter transitions you from the world of NLP to
    the world of multi-modal learning, where we will combine visual and textual features
    using the familiar Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of multi-modal deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vision and language tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detailed overview of the Image Captioning task and the MS-COCO dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architecture of a residual network, specifically ResNet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting features from images using pre-trained ResNet50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a full Transformer model from scratch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideas for improving the performance of image captioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our journey starts with an overview of the various tasks in the visual understanding
    domain, with a focus on tasks that combine language and images.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-modal deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dictionary definition of "modality" states that it is "a particular mode
    in which something exists or is experienced or expressed." Sensory modalities,
    like touch, taste, smell, vision, and sound, allow humans to experience the world
    around them. Suppose you are out at the farm picking strawberries, and your friend
    tells you to pick ripe and red strawberries. The instruction, *ripe and red strawberries*,
    is processed and converted into a visual and haptic criterion. As you see strawberries
    and feel them, you know instinctively if they match the criteria of *ripe and
    red*. This task is an example of multiple modalities working together for a task.
    As you can imagine, these capabilities are essential for robotics.
  prefs: []
  type: TYPE_NORMAL
- en: As a direct application of the preceding example, consider a harvesting robot
    that needs to pick ripe and ready fruit. In December 1976, Harry McGurk and John
    MacDonald published a piece of research titled *Hearing lips and seeing voices*
    ([https://www.nature.com/articles/264746a0](https://www.nature.com/articles/264746a0))
    in the reputed journal, Nature. They recorded a video of a young woman talking,
    where utterances of the syllable *ba* had been dubbed onto the lip movement of
    the syllable *ga*. When this video was played back to adults, people repeated
    hearing the syllable *da*. When the audio track was played without the video,
    the right syllable was reported. This research paper highlighted the role of vision
    in speech recognition. Speech recognition models using lip-reading information
    were developed in the field of **Audio-Visual Speech Recognition** (**AVSR**).
    There are several exciting applications of multi-modal deep learning models in
    medical devices and diagnosis, learning technology, and other **Artificial Intelligence**
    (**AI**) areas.
  prefs: []
  type: TYPE_NORMAL
- en: Let's drill down into the specific interaction of vision and language and the
    various tasks we can perform.
  prefs: []
  type: TYPE_NORMAL
- en: Vision and language tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A combination of **Computer Vision** (**CV**) and **Natural Language Processing**
    (**NLP**) allows us to build smart AI systems that can see and talk. CV and NLP
    together produce interesting tasks for model development. Taking an image and
    generating a caption for it is a well-known task. A practical application of this
    task is generating alt-text tags for images on web pages. Visually impaired readers
    use screen readers, which can read these tags while reading the page, improving
    the accessibility of web pages. Other topics in this area include video captioning
    and storytelling – composing a story from a sequence of images. The following
    image shows some examples of images and captions. Our primary focus in this chapter
    is on image captioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing photo, room, bunch, many  Description automatically
    generated](img/B16252_07_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.1: Example images with captions'
  prefs: []
  type: TYPE_NORMAL
- en: '**Visual Question Answering** (**VQA**) is the challenging task of answering
    questions about objects in the image. The following image shows some examples
    from the VQA dataset. Compared to image captioning, where prominent objects are
    reflected in the caption, VQA is a more complex task. Answering the question may
    also require some reasoning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the bottom-right panel in the following image. Answering the question,
    "Does this person have 20/20 vision?" requires reasoning. Datasets for VQA are
    available at [visualqa.org](http://visualqa.org):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person posing for a photo  Description automatically generated](img/B16252_07_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2: Examples from the VQA Dataset (Source: VQA: Visual Question Answering
    by Agrawal et al.)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Reasoning leads to another challenging but fascinating task – **Visual Commonsense
    Reasoning** (**VCR**). When we look at an image, we can guess emotions, actions,
    and frame a hypothesis of what is happening. Such a task is quite easy for people
    and may even happen without conscious effort. The aim of the VCR task is to build
    models that can perform such a task. These models should also be able to explain
    or choose an appropriate reason for the logical inference that''s been made. The
    following image shows an example from the VCR dataset. More details on the VCR
    dataset can be found at [visualcommonsense.com](http://visualcommonsense.com):'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a social media post  Description automatically generated](img/B16252_07_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.3: VCR example (Source: From Recognition to Cognition: Visual Commonsense
    Reasoning by Zellers et al.)'
  prefs: []
  type: TYPE_NORMAL
- en: Thus far, we have gone from images to text. The reverse is also possible and
    is an active area of research. In this task, images or videos are generated from
    text using GANs and other generative architectures. Imagine being able to generate
    an illustrative comic book from the text of a story! This particular task is at
    the forefront of research currently.
  prefs: []
  type: TYPE_NORMAL
- en: A critical concept in this area is **visual grounding**. Grounding enables tying
    concepts in language to the real world. Simply put, it matches words to objects
    in a picture. By combining vision and language, we can ground concepts from languages
    to parts of an image. For example, mapping the word "basketball" to something
    that looks like one in an image is called visual grounding. There can be more
    abstract concepts that can be grounded. For example, a short elephant and a short
    person have different measurements. Grounding provides us with a way to see what
    models are learning and helps us guide them in the right direction.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a proper perspective on vision and language tasks, let's dive
    deep into an image captioning task.
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Image captioning is all about describing the contents of an image in a sentence.
    Captions can help in content-based image retrieval and visual search. We already
    discussed how captions could improve the accessibility of websites by making it
    easier for screen readers to summarize the content of an image. A caption can
    be considered a summary of the image. Once we frame the problem as an image summarization
    problem, we can adapt the seq2seq model from the previous chapter to solve this
    problem. In text summarization, the input is a sequence of the long-form article,
    and the output is a short sequence summarizing the content. In image captioning,
    the output is similar in format to summarization. However, it may not be obvious
    how to structure an image that consists of pixels as a sequence of embeddings
    to be fed into the Encoder.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, the summarization architecture used **Bi-directional Long Short-Term
    Memory networks** (**BiLSTMs**), with the underlying principle that words that
    are closer together to each other are similar to each other in meaning. BiLSTMs
    exploited this property by looking at the input sequence from both sides and generated
    encoded representations. Generating a representation for an image that works for
    the Encoder requires some thought.
  prefs: []
  type: TYPE_NORMAL
- en: 'A naïve solution for representing images as a sequence could be expressing
    them as a list of pixels. So, an image of size 28x28 pixels becomes a sequence
    of 784 tokens. When the tokens represent text, an Embedding layer learns the representation
    of each token. If this Embedding layer had a dimension of 64, then each token
    would be represented by a 64-dimensional vector. This embedding vector was learned
    during training. Extending our analogy of using a pixel as a token, a straightforward
    solution is to use the value of the Red/Green/Blue channels of the pixel in an
    image to generate a three-dimensional embedding. However, training these three
    dimensions does not sound like a logical approach. More importantly, pixels are
    laid out in a 2D representation, while the text is laid out in a 1D representation.
    This concept is illustrated in the following image. Words are related to words
    next to each other. When pixels are laid out in a sequence, the **data locality**
    of these pixels is broken since the content of a pixel is related to the pixels
    all around it, not just to the left and right of it. This idea is shown by the
    following super zoomed in image of a tulip:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_07_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.4: Data locality in text versus images'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data locality and translation invariance are two critical properties of images.
    Translation invariance is the idea that an object can appear in various spots
    in an image. In a fully connected model, the model would try to learn the position
    of the object, which would prevent the model from generalizing. The specialized
    architecture of **Convolutional Neural Networks** (**CNNs**) can be used to exploit
    these properties and extract signals from the image. At a high level, we use CNNs,
    specifically the **ResNet50** architecture, to convert the image into a tensor
    that can be fed to a seq2seq architecture. Our model will combine the best of
    CNNs and RNNs to handle the image and text parts under the seq2seq model. The
    following diagram shows our architecture at a very high level:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_07_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.5: High-level image captioning model architecture'
  prefs: []
  type: TYPE_NORMAL
- en: While a comprehensive explanation of CNNs is beyond the scope of this book,
    we will review the key concepts in short. Since we will be using a pre-trained
    CNN model, we won't have to go into much depth about CNNs. *Python Machine Learning,
    Third Edition*, published by Packt, is an excellent resource for reading up on
    CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter on text summarization, we built a seq2seq model with
    attention. In this chapter, we will build a Transformer model. Transformer models
    are currently state of the art in NLP. The Encoder part of the Transformer is
    the core of the **Bidirectional Encoder Representations from Transformers** (**BERT**)
    architecture. The Decoder part of the Transformer is the core of the **Generative
    Pre-trained Transformer** (**GPT**) family of architectures. There is a specific
    advantage of the Transformer architecture that is relevant to the image captioning
    problem. In the seq2seq architecture, we used BiLSTMS, which tries to learn relationships
    via co-occurrence. In the Transformer architecture, there is no recurrence. Instead,
    positional encodings and self-attention model relationships are made between inputs.
    This change enables us to feed in processed image patches as input and hope that
    the relationships between the image patches will be learned.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the image captioning model requires a large amount of code as we
    will implement several pieces, like pre-processing images, with ResNet50 and a
    complete implementation of Transformer architecture from scratch. This chapter
    contains much more code than the other chapters. We will rely on code fragments
    to highlight the most important aspects of the code rather than going over every
    line of code in detail, as we have been doing so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main steps of building our model are summarized here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Downloading the data**: Given the large size of the dataset, this is a time-consuming
    activity.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Pre-processing captions**: Since the captions are in JSON format, they are
    flattened into a CSV for easier processing.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Feature extraction**: We pass the image files through ResNet50 to extract
    features and save them to speed up training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Transformer training**: A full Transformer model with positional encoding,
    multi-head attention, an Encoder, and a Decoder is trained on the processed data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Inference**: Use the trained model to caption some images!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Evaluating performance**: **Bilingual Evaluation Understudy** (**BLEU**)
    scores are used to compare the trained models with ground truth data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's start with the dataset first.
  prefs: []
  type: TYPE_NORMAL
- en: MS-COCO dataset for image captioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microsoft published the **Common Objects in Context** or **COCO** dataset in
    2014\. All the versions of the dataset can be found at [cocodataset.org](http://cocodataset.org).
    The COCO dataset is a big dataset that's used for object detection, segmentation,
    and captioning, among other annotations. Our focus will be on the 2014 training
    and validation images, where five captions per image are available. There are
    roughly 83K images in the training set and 41K images in the validation set. The
    training and validation images and captions need to be downloaded from the COCO
    website.
  prefs: []
  type: TYPE_NORMAL
- en: '**Large download warning**: The training image dataset is approximately 13
    GB, while the validation dataset is over 6 GB. The annotations for the image files,
    which include captions, are about 214 MB in size. Please be careful of your internet
    bandwidth usage and potential costs as you download this dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Google has also published a new Conceptual Captions dataset at [https://ai.google.com/research/ConceptualCaptions](https://ai.google.com/research/ConceptualCaptions).
    It contains over 3M images. Having a large dataset allows deep models to train
    better. There is a corresponding competition where you can submit your models
    and see how they compete with others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that these are large downloads, you may wish to use the download that''s
    the most comfortable to you. If `wget` is available on your environment, you could
    use it to download the files, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the annotations for the training and validation sets are in one compressed
    archive. Once the files have been downloaded, they need to be unzipped. Each of
    these compressed files creates its own folder and puts the contents in there.
    We will create a folder called `data` and move all the expanded contents inside
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'All the images are either in the `train2014` or `val2014` folder. The code
    for the initial pre-processing of the data is in the `data-download-preprocess.py`
    file. Captions for the training and validation images can be found in the `captions_train2014.json`
    or `captions_val2014.json` JSON file inside the `annotations` subfolder. Both
    of these files are in a similar format. The files have four main keys – info,
    image, license, and annotation. The image key contains a record per image, along
    with information about the size, URL, name, and a unique ID that is used to refer
    to that image in the dataset. Captions are stored as a tuple of the image ID and
    caption text, along with a unique ID for the caption. We use the Python `json`
    module to read and process these files:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our objective is to produce a single simple file with two columns – one for
    the image file name and another containing the caption for that file. Note that
    the validation set contains half the number of images of the training set. In
    a seminal paper on captioning titled *Deep Visual-Semantic Alignment for Generating
    Image Descriptions*, Andrej Karpathy and Fei-Fei Li proposed training on all the
    training and validation images after reserving 5,000 images from the validation
    set for testing. We will follow this approach by processing the image names and
    IDs into a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Since each image has five captions, the validation set cannot be split based
    on captions. Otherwise, there will be leakage of data from the training set into
    the validation/test set. In the preceding code, we reserved the last 5K images
    for the validation set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s go over the captions for the training and validation images and
    create a combined list. We will create empty lists to store the tuples of image
    paths and captions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will process all the training captions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'For the validation captions, the logic is similar, but we need to ensure that
    no captions are included for the images that have been reserved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Hopefully, there should not be any errors. If you encounter errors, this could
    be due to corrupted downloads or errors while unzipping the files. The training
    dataset is shuffled to aid in training. Finally, two CSV files are persisted with
    the training and testing data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: At this point, the data download and pre-processing phases are complete. The
    next step is to pre-process all the images using ResNet50 to extract features.
    Before we write the code for that, we will take a short detour and look at CNNs
    and the ResNet architecture. If you are already comfortable with CNNs, you may
    skip ahead to the code part.
  prefs: []
  type: TYPE_NORMAL
- en: Image processing with CNNs and ResNet50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of deep learning, specific architectures have been developed to
    handle specific modalities. CNNs have been incredibly successful in processing
    images and are the standard architecture for CV tasks. A good mental model for
    using a pre-trained model for extracting features from images is that of using
    pre-trained word embeddings like GloVe for text. In this particular case, we use
    a specific architecture called ResNet50\. While a comprehensive treatment of CNNs
    is outside the scope of this book, a brief overview of CNNs and ResNet will be
    provided in this section. If you are already comfortable with these concepts,
    you may skip ahead to the section titled *Image feature extraction with ResNet50*.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'CNNs are an architecture designed to learn from the following key properties,
    which are relevant to image recognition:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data locality**: The pixels in an image are highly correlated to the pixels
    around them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Translation invariance**: An object of interest, for example, a bird, may
    appear at different places in an image. The model should be able to identify the
    object, irrespective of the object''s position in the image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scale invariance**: An object of interest may have a smaller or large size,
    depending on the zoom. Ideally, the model should be able to identify objects of
    interest in an image, irrespective of their size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution and pooling layers are key components that aid CNNs in extracting
    features from images.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A convolution is a mathematical operation that is performed on patches taken
    from an image with a filter. A filter is a matrix, usually square and with 3x3,
    5x5, and 7x7 as common dimensions. The following image shows an example of a 3x3
    convolution matrix applied to a 5x5 image. The image patches are taken from left
    to right and then top to bottom. The number of pixels this patch shifts by every
    step is called the **stride length**. A stride length of 1 in a horizontal and
    vertical direction reduces a 5x5 image to a 3x3 image, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a green screen  Description automatically generated](img/B16252_07_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.6: Example of a convolution operation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The specific filter that was applied here is an edge detection filter. Prior
    to CNNs, CV relied heavily on handcrafted filters. Sobel filters are an example
    of a special filter for the purpose of edge detection. The `convolution-example.ipynb`
    notebook provides an example of detecting edges using the Sobel filter. The code
    is quite straightforward. After the imports, the image file is loaded and converted
    into a grayscale image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we define and apply the Sobel filters to the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The original image, along with the intermediate versions, are shown in the
    following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screen shot of a computer  Description automatically generated](img/B16252_07_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7: Edge detection using Sobel filters'
  prefs: []
  type: TYPE_NORMAL
- en: Constructing such filters is very tedious. However, CNNs can learn many such
    filters by treating the filter matrices as learnable parameters. CNNs often pass
    an image through hundreds or thousands of such filters, referred to as channels,
    and stack them together. You can think of each filter as detecting some features,
    like vertical lines, horizontal lines, arcs, circles, trapezoids, and so on. However,
    the magic happens when multiple such layers are put together. Stacking multiple
    layers leads to learning hierarchical representations. An easy way to understand
    this concept is by imagining that earlier layers are learning simple shapes like
    lines and arcs, middle layers are learning shapes like circles and hexagons, and
    the top layers are learning complex objects like stop signs and steering wheels.
    The convolution operation is the key innovation that exploits data locality and
    extracts features that enable translation invariance.
  prefs: []
  type: TYPE_NORMAL
- en: A consequence of this layering is the amount of data flowing through the model
    increasing. Pooling is an operation that helps reduce the dimensions of the data
    flowing through and further highlights these features.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once the values from the convolution operation have been computed, a pooling
    operation can be applied to patches to further concentrate the signal in the image.
    The most common form of pooling is called **Max pooling** and is demonstrated
    in the following diagram. It is as simple as taking the maximum value in a patch.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows max pooling on non-overlapping 2x2 patches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a colorful background  Description automatically generated](img/B16252_07_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.8: Max pooling operation'
  prefs: []
  type: TYPE_NORMAL
- en: Another way to pool is by averaging the values. While pooling reduces the complexity
    and computation load, it also helps modestly with scale invariance. However, there
    is a chance that such a model overfits and does not generalize well. Dropout is
    a technique that helps with regularization and enables such models to generalize
    better.
  prefs: []
  type: TYPE_NORMAL
- en: Regularization with dropout
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may recall that we used dropout settings in previous chapters with the
    LSTM and BiLSTM settings. The core idea behind dropout is shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a logo  Description automatically generated](img/B16252_07_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Dropout'
  prefs: []
  type: TYPE_NORMAL
- en: Rather than connecting every unit from a lower layer to every unit in the next
    higher layer of the model, some of the connections are randomly dropped during
    training time. Inputs are dropped only during training time. Since dropping inputs
    reduces the total input reaching a node compared to test/inference time, inputs
    are upscaled in the proportion of dropout to ensure the relative magnitudes are
    preserved. Dropping some of the inputs during training forces the model to learn
    more from each of the inputs. This is because it cannot rely on the presence of
    a specific input. This helps the network build resilience to missing inputs and
    consequently helps generalize the models.
  prefs: []
  type: TYPE_NORMAL
- en: A combination of these techniques helped build deeper and deeper networks. A challenge
    that showed up as networks got deeper was that the signal from the inputs became
    quite small in the higher layers. Residual connections is a technique that helps
    deal with this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Residual connections and ResNets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intuition suggests that adding more layers should make performance better. A deeper
    network has more model capacity, so it should be able to model more complex distributions
    compared to shallower networks. As deeper and deeper models were built, a degradation
    in accuracy was observed. Since the reduction happened even on the training data,
    overfitting can be ruled out as a probable cause. As inputs pass through more
    and more layers, the optimizers have a harder time adjusting the gradients to
    the point where learning is impaired in the model. Kaiming He and his collaborators
    published the ResNet architecture in their seminal paper titled *Deep Residual
    Learning for Image Recognition*.
  prefs: []
  type: TYPE_NORMAL
- en: We must understand residual connections before understanding ResNets. The core
    concept of the residual connection is shown in the following diagram. In a regular
    dense layer, the input is first multiplied by the weights. Then, biases are added
    in, which is a linear operation. The output is passed through an activation function,
    like ReLU, which introduces non-linearity in the layer. The output from the activation
    function is the final output of the layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, residual connections introduce a summation in-between the linear computation
    and the activation function, as shown on the right-hand side of the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_07_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: A conceptual residual connection'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the preceding diagram is only for illustrating the core concept behind
    residual connections. In ResNets, the residual connection is made between multiple
    blocks. The following diagram shows the basic building blocks of ResNet50, also
    referred to as the bottleneck design. This design is called the bottleneck design
    because the 1x1 convolution blocks reduce the dimensions of the inputs before
    passing them to the 3x3 convolution. The last 1x1 block scales the inputs out
    again for the next layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a keyboard  Description automatically generated](img/B16252_07_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: ResNet50 bottleneck building block'
  prefs: []
  type: TYPE_NORMAL
- en: 'ResNet50 is composed of several such blocks stacked on top of each other. There
    are four groups, each consisting of three to seven such blocks. **BatchNorm**
    or batch normalization was proposed by Sergey Ioffe and Christian Szegedy in their
    paper titled *Batch Normalization: Accelerating Deep Network Training By Reducing
    Internal Covariate Shift* in 2015\. Batch normalization aims to reduce the variance
    of the outputs coming from one layer being fed into the next layer. By reducing
    this variance, BatchNorm acts like L2 regularization, which attempts to do the
    same thing by adding the penalties of the magnitude of the weights to the cost
    function. The main motivation of BatchNorm is to efficiently backpropagate gradient
    updates through a large number of layers, while minimizing the risk that this
    update could result in divergence. In stochastic gradient descent, gradients are
    used to update the weights of all the layers at the same time, assuming that the
    output of one layer doesn''t impact any other layers. However, this is not a completely
    valid assumption. For an *n*-layer network, computing this would need *n*th order
    gradients, which is intractable. Instead, batch-norm is used, which works on one
    mini-batch at a time and the constraints of the updates to reduce this unwanted
    shift in the distribution of weights. It does this by normalizing the outputs
    before they are fed into the next layer.'
  prefs: []
  type: TYPE_NORMAL
- en: The last two layers of ResNet50 are dense layers that classify the outputs from
    the last block into an object category. Covering ResNets comprehensively is a
    tough ask, but hopefully, this crash course on CNNs and ResNets has given you
    enough background on how they work. You are encouraged to read the referenced
    papers and *Deep Learning with TensorFlow 2 and Keras, Second Edition*, published
    by Packt, for a detailed treatment of this topic. Fortunately for us, TensorFlow
    provides a pre-trained ResNet50 model that is ready for use. In the next section,
    we'll use this pre-trained ResNet50 model for extracting image features.
  prefs: []
  type: TYPE_NORMAL
- en: Image feature extraction with ResNet50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ResNet50 models are trained on the ImageNet dataset. This dataset contains millions
    of images in over 20,000 categories. The large-scale visual recognition challenge,
    ILSVRC, focuses on the top 1,000 categories for models to compete on recognizing
    images. Consequently, the top layers of the ResNet50 that perform classification
    have a dimension of 1,000\. The idea behind using a pre-trained ResNet50 model
    is that it is already able to parse out objects that may be useful in image captioning.
  prefs: []
  type: TYPE_NORMAL
- en: The `tensorflow.keras.applications` package provides pre-trained models like
    ResNet50\. At the time of writing, all the pre-trained models provided are related
    to CV. Loading up the pre-trained model is quite easy. All the code for this section
    is in the `feature-extraction.py` file in this chapter's folder on GitHub. The
    main reason for using a separate file is that it gives us the ability to run feature
    extraction as a script.
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that we will be processing over 100,000 images, this process may take
    a while. CNNs benefit greatly from a GPU in computation. Let''s get into the code
    now. First, we must set up the paths for the CSV file we created from the JSON
    annotations in the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'ResNet50 expects each image to be 224x224 pixels with three channels. The input
    images from the COCO set have different sizes. Hence, we must convert the input
    files into the standard that ResNet was trained on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The highlighted code shows a special pre-processing function provided by the
    ResNet50 package. The pixels in the input image are loaded into an array via the
    `decode_jpeg()` function. Each pixel has a value between 0 and 255 for each color
    channel. The `preprocess_input()` function normalizes the pixel values so that
    their mean is 0\. Since each input image has five captions, we should only process
    the unique images in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we must convert the dataset into a `tf.dat.Dataset`, which makes it easier
    to batch and process the input files using the convenience function defined previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'For efficiently processing and generating features, we must process 16 image
    files at a time. The next step is loading a pre-trained ResNet50 model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output has been abbreviated for brevity. The model contains over
    23 million trainable parameters. We don't need the top classification layer as
    we are using the model for feature extraction. We defined a new model with the
    input and output layer. Here, we took the output from the last layer. We could
    take output from different parts of ResNet by changing the definition of the `hidden_layer`
    variable. In fact, this variable can be a list of layers, in which case the output
    of the `features_extract` model will be the output from each of the layers in
    the list.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, a directory must be set up to store the extracted features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The feature extraction model can work on batches of images and predict the
    output. The output is 2,048 patches of 7x7 pixels for each image. If a batch of
    16 images is supplied, then the output from the model will be a tensor of dimensions
    [16, 7, 7, 2048]. We store the features of each image file as a separate file
    while flattening the dimensions to [49, 2048]. Each image has now been converted
    into a sequence of 49 pixels, with an embedding size of 2,048\. The following
    code performs this action:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This could be a time-consuming operation, depending on your computing environment.
    On my Ubuntu Linux box with an RTX 2070 GPU, this took ~23 minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step in data pre-processing is to train the Subword Encoder. This
    part should be quite familiar to you as it is identical to what we''ve done in
    previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Note that we included two special tokens to signal the start and end of the
    sequences. You may recall this technique from *Chapter 5*, *Generating Text with
    RNNs and GPT-2*. Here, we used a slightly different way of accomplishing the same
    technique to show how you can accomplish the same objective in different ways.
  prefs: []
  type: TYPE_NORMAL
- en: With that, pre-processing and feature extraction is complete. The next step
    is defining the Transformer model. Then, we will be ready to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Transformer model was discussed in *Chapter 4*, *Transfer Learning with
    BERT*. It was inspired by the seq2seq model and has an Encoder and a Decoder part.
    Since the Transformer model does not rely on RNNs, input sequences need to be
    annotated with positional encodings, which allow the model to learn about the
    relationships between inputs. Removing recurrence improves the speed of the model
    vastly while reducing the memory footprint. This innovation of the Transformer
    model has made very large-sized models such as BERT and GPT-3 possible. The Encoder
    part of the Transformer model was shown in the aforementioned chapter. The full
    Transformer model was shown in *Chapter 5*, *Generating Text with RNNs and GPT-2*.
    We will start with a modified version of the full Transformer. Specifically, we
    will modify the Encoder part of the Transformer to create a visual Encoder, which
    takes image data as input instead of text sequences. There are some other small
    modifications to be made to accommodate images as input to the Encoder. The Transformer
    model we are going to build is shown in the following diagram. The main difference
    here is how the input sequence is encoded. In the case of text, we will tokenize
    the text using a Subword Encoder and pass it through an Embedding layer, which
    is trainable.
  prefs: []
  type: TYPE_NORMAL
- en: 'As training proceeds, the embeddings of the tokens are also learned. In the
    case of image captioning, we will pre-process the images into a sequence of 49
    pixels, each with an "embedding" size of 2,048\. This actually simplifies padding
    the inputs. All the images are pre-processed so that they''re the same length.
    Consequently, padding and masking the inputs is not required:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_07_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Transformer model with a visual Encoder'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following pieces of code need to be implemented to build the Transformer
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding of the inputs, along with input and output masks. Our inputs
    are of a fixed length, but the output and captions are of a variable length.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaled dot-product attention and multi-head attention to enable the Encoders
    and Decoders to focus on specific aspects of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Encoder that consists of multiple repeating blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Decoder that uses the outputs from the Encoder through its repeating blocks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for the Transformer has been taken from the TensorFlow tutorial titled
    *Transformer model for language understanding*. We will be using this code as
    the base and adapting it for the image captioning use case. One of the beautiful
    things about the Transformer architecture is that if we can cast a problem as
    a sequence-to-sequence problem, then we can apply the Transformer model. As we
    describe the implementation, the main points of the code will be highlighted.
    Note that the code for this section is in the `visual_transformer.py` file.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the full Transformer model does take a little bit of code. If you
    are already familiar with the Transformer model or want to only know where our
    model differs from the standard Transformer model, please focus on the next section
    and the *VisualEncoder* section. You can read the rest of the sections at your
    leisure.
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding and masks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformer models don't use RNNs. This allows them to compute all the outputs
    in one step, leading to significant improvements in speed and also the ability
    to learn dependencies across long inputs. However, it comes at the cost of the
    model not knowing anything about the relationship between neighboring words or
    tokens. A positional encoding vector, with values for the odd and even positions
    of the tokens to help the model learn relationships between the positions of inputs,
    helps compensate for the lack of information about the ordering of tokens.
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings help place tokens that are similar in meaning close to each other
    in the embedding space. Positional encodings put tokens closer to each other based
    on their position in the sentence. Put together, the two are quite powerful.
  prefs: []
  type: TYPE_NORMAL
- en: 'In image captioning, this is important for captions. Technically, we don''t
    need to provide these positional encodings for the image inputs as ResNet50 should
    have produced appropriate patches. Positional encoding can, however, still be
    used for the inputs as well. Positional encoding uses a *sin* function for even
    positions and a *cos* function for odd positions. The formula for computing the
    encodings for a position is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_07_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w*[i] is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_07_002.png)'
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, *pos* refers to the position of a given token, *d*[model]
    refers to the dimensions of the embeddings, and *i* is the specific dimension
    being computed. The positional encoding process produces a vector with the same
    dimensions as the embedding for each token. You may be wondering why this complex
    formulation is used for computing these positional encodings. Wouldn't numbering
    the tokens from one side to the other suffice? It turns out that the positional
    encoding algorithm must have a few characteristics. First, the values must generalize
    easily to sequences of a variable length. Using a straight-up numbering scheme
    would prevent inputs that have sequences longer than those in the training data.
    The output should be unique for each token's position. Furthermore, the distance
    between any two positions should be consistent across different lengths of input
    sequences. This formulation is relatively simple to implement. The code for this
    is in the Positional Encoder section of the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must compute the *angle*, as shown in the preceding *w*[i] formula,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must compute the vector of positional encodings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to compute the masks for input and output. Let''s focus on
    the Decoder for a second. Since we are not using an RNN, the entire output is
    fed to the Decoder at once. However, we don''t want the Decoder to look at data
    from future timesteps. So, the outputs must be masked. In terms of the Encoder,
    masks are needed if the input is padded to a fixed length. However, in our case,
    the inputs are always exactly a length of 49\. So, the mask is a fixed vector
    of ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The first method is used to mask inputs if they are padded. This method has
    been included for the sake of completeness, but you will see later that we pass
    it a sequence of ones. So, all this method does is reshape the masks. The second
    mask function is used for masking Decoder inputs so that it can only see the positions
    it has generated.
  prefs: []
  type: TYPE_NORMAL
- en: The layers of the transfer Encoder and Decoder use a specific form of attention.
    This is a fundamental building block of the architecture and will be implemented
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Scaled dot-product and multi-head attention
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of the attention function is to match a query to a set of key-value
    pairs. The output is a sum of the values, weighted by the correspondence between
    the query and the key. multi-head attention learns multiple ways to compute the
    scaled dot-product attention and combines it.
  prefs: []
  type: TYPE_NORMAL
- en: Scaled dot-product attention is computed by multiplying the query vector by
    the key vector. This product is scaled by the square root of the dimensions of
    the query and key. Note that this formulation assumes that the key and query vectors
    have the same dimensions. Practically, the dimensions of the query, key, and value
    vectors are all set to the size of the embedding.
  prefs: []
  type: TYPE_NORMAL
- en: 'This was referred to as *d*[model] in the position encoding. After computing
    the scaled product of the key and query vector, a softmax is applied, and the
    result of the softmax is multiplied by the value vector. A mask is used to mask
    the product of the query and keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Multi-ahead attention concatenates outputs from multiple scaled dot-product
    attention units and passes them through a linear layer. The dimensions of the
    embedding inputs are divided by the number of heads to compute the dimensions
    of the key and value vectors. Multi-head attention is implemented as a custom
    layer. First, we must create the constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the `assert` statement that is highlighted. When the Transformer model
    is instantiated, it is vital to choose some parameters so that the number of heads
    divides the model size or embedding dimensions completely. The main computation
    of this layer is in the `call()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The three highlighted rows show splitting the vectors into multiple heads.
    `split_heads()` is defined like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This completes the multi-head attention implementation. This is the key part
    of the Transformer model. There is a small detail surrounding a Dense layer, which
    is used to aggregate the outputs from multi-head attention. It is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Thus far, we have looked at the following parameters for specifying a Transformer
    mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '*d*[model] is used for the size of the embeddings and primary flow of inputs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*d*[ff] is the size of the output from the intermediate Dense layer in the
    FeedForward part'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h* specifies the number of heads for multi-head attention'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will implement a visual Encoder, which has been modified to accommodate
    images as input.
  prefs: []
  type: TYPE_NORMAL
- en: VisualEncoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The diagram shown in the *The Transformer model* section shows the Encoder''s
    structure. The Encoder processes the inputs with positional encodings and masks,
    and then passes them through stacks of multi-head attention and feed-forward blocks.
    The implementation deviates from the TensorFlow tutorial as the input in the tutorial
    is text. In our case, we are passing 49x2,048 vectors that were generated by passing
    images through ResNet50\. The main difference is in how the inputs are handled.
    `VisualEncoder` is built as a layer to allow composition into the eventual Transform
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The constructor is shown next. A new parameter that states the number of layers
    is introduced. The original paper used 6 layers, 512 as *d*[model], 8 multi-attention
    heads, and 2,048 as the size of the intermediate feed-forward output. Note the
    highlighted lines in the preceding code. The dimensions of the pre-processed images
    can vary depending on the layer of ResNet50 from which output is pulled. We pass
    the input through a dense layer, `fc`, to the size inputs according to the model.
    This allows us to experiment with different models to pre-process images such
    as VGG19 or Inception without changing the architecture. Also, note that the maximum
    position encoding is hardcoded to 49, since that is the dimension of the output
    of the ResNet50 model. Lastly, we add a flag that can switch positional encoding
    on or off in the Visual Encoder. You should experiment with training models with
    and without positional encodings in the input to see if this helps or hinders
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: '`VisualEncoder` is composed of multiple multi-head attention and feed-forward
    blocks. We can utilize a convenience class, `EncoderLayer`, to define one such
    block. A stack of these blocks is created based on the input parameters. We will
    examine the internals of `EncoderLayer` momentarily. First, let''s see how inputs
    pass through `VisualEncoder`. The `call()` function is used to produce the outputs
    for the given inputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is fairly simple due to the abstractions defined previously. Note
    the use of the training flag to turn dropout on or off. Now, let''s see how `EncoderLayer`
    is defined. Each Encoder building is composed of two sub-blocks. The first sub-block
    passes inputs through multi-head attention, while the second sub-block passes
    the output of the first sub-block through the 2-layer feed-forward layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Each layer first computes the output from multi-head attention and passes it
    through dropout. A residual connection passes the sum of the output and input
    through LayerNorm. The second part of this block passes the output of the first
    LayerNorm through the feed-forward layer and another dropout layer.
  prefs: []
  type: TYPE_NORMAL
- en: Again, a residual connection combines the output and input to the feed-forward
    part before passing it through LayerNorm. Note the use of dropout and residual
    connections, which were developed for CV in the Transformer architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '**Layer normalization or LayerNorm**'
  prefs: []
  type: TYPE_NORMAL
- en: LayerNorm was proposed in 2016 in a paper by the same name as an alternative
    to BatchNorm for RNNs. BatchNorm, as described in the *CNNs* section, normalizes
    the outputs across the entire batch. But sequences can be of variable length in
    the case of RNNs. A different formulation is required for normalization that can
    handle variable sequence lengths. LayerNorm normalizes across all the hidden units
    in a given layer. It is independent of the batch size, and the normalization is
    the same for all the units in a given layer. LayerNorm results in a significant
    speedup of training and convergence of seq2seq style models.
  prefs: []
  type: TYPE_NORMAL
- en: With `VisualEncoder` in place, we are ready to implement the Decoder before
    we put this all together into the full Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Decoder is also composed of blocks, just like the Encoder. Each block of
    the Decoder, however, contains three sub-blocks, as shown in the diagram in the
    *The Transformer model* section. There is a masked multi-head attention sub-block,
    followed by a multi-head attention block, and finally a feed-forward sub-block.
    The feed-forward sub-block is identical to the Encoder sub-block. We must define
    a Decoder layer that can be stacked to construct the Decoder. The constructor
    for this is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Three sub-blocks should be quite evident based on the preceding variables.
    Input passes through this layer and is converted into output, as defined by the
    computations in the `call()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The first sub-block, also referred to as the masked multi-head attention block,
    uses the output tokens, masked to the current position being generated. The outputs,
    in our case, are the tokens that make up the caption. The look-ahead mask masks
    tokens that haven't been generated yet.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this sub-block does not use the output of the Encoder. It is trying
    to predict the relationship of the next token to the previous token that was generated.
    The second sub-block uses the output of the Encoder, along with the output of
    the previous sub-block, to generate the outputs. Finally, the feed-forward network
    generates the final output by operating on the output of the second sub-block.
    Both the multi-head attention sub-blocks have their own attention weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the Decoder as a custom layer that is composed of multiple `DecoderLayer`
    blocks. The structure of the Transformer is symmetrical. The number of Encoder
    and Decoder blocks is the same. The constructor is defined first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the Decoder is computed by the `call()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Whew, that was a fair amount of code. The structure of the Transformer model
    is so elegant. The beauty of the model allows us to stack more Encoder and Decoder
    layers to create more powerful models, as demonstrated by GPT-3 recently. Let's
    put the Encoder and Decoder together to create a full Transformer.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Transformer is composed of the Encoder, the Decoder, and the final Dense
    layer for generating output token distributions across the subword vocabulary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: That was a whirlwind tour of the full Transformer code. Ideally, Keras in TensorFlow
    will provide a higher-level API for defining a Transformer model without you having
    to write the code out. If this was too much to absorb, then focus on the masks
    and VisualEncoder as they are the only deviations from the standard Transformer
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to train the model. We'll take a very similar approach to the
    one we adopted in the previous chapter, by setting up learning rate annealing
    and checkpointing.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Transformer model with VisualEncoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training the Transformer model can take hours as we want to train for around
    20 epochs. It is best to put the training code into a file so that it can be run
    from the command line. Note that the model will be able to show some results even
    after 4 epochs of training. The training code is in the `caption-training.py`
    file. At a high level, the following steps need to be performed before starting
    training. First, the CSV file with captions and image names is loaded in, and
    the corresponding paths for the files with extracted image features are appended.
    The Subword Encoder is also loaded in. A `tf.data.Dataset` is created with the
    encoded captions and image features for easy batching and feeding them into the
    model for training. A loss function, an optimizer with a learning rate schedule,
    is created for use in training. A custom training loop is used to train the Transformer
    model. Let's go over these steps in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Loading training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following code loads the CSV file we generated in the pre-processing step:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The captions in the data are tokenized using the Subword Encoder we generated
    and persisted to disk earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The maximum length of the captions is generated to accommodate 99% of the caption
    lengths. All the captions are truncated or padded to this maximum length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Image features are persisted to disk. When training begins, those features
    need to be read from the disk and fed in, along with the encoded captions. The
    name of the file containing the image features is then added to the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'A `tf.data.Dataset` is created and a map function that reads image features
    while enumerating batches is set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Now that the dataset has been prepared, we are ready to instantiate the Transformer
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating the Transformer model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will instantiate a small model in terms of the number of layers, attention
    heads, embedding dimensions, and feed-forward units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'For comparison, the BERT base model contains the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'These settings are available in the file but commented out. Using these settings
    slows down training and requires a large amount of GPU memory. A couple of other
    parameters need to be set up and the Transformer instantiated:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'This model contains over 4 million trainable parameters. It is a smaller model
    than we have seen previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: However, the model summary is not available since the input dimensions have
    not yet been supplied. The summary will be available once we've run a training
    example through the model.
  prefs: []
  type: TYPE_NORMAL
- en: A custom learning rate schedule is created for training the model. A custom
    learning rate schedule anneals or reduces the learning rate as the model improves
    its accuracy, resulting in better accuracy. This process is called learning rate
    decay or learning rate annealing and was discussed in detail in *Chapter 5*, *Generating
    Text with RNNs and GPT-2*.
  prefs: []
  type: TYPE_NORMAL
- en: Custom learning rate schedule
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This rate schedule is identical to the one proposed in the *Attention Is All
    You Need* paper:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the learning schedule:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a person  Description automatically generated](img/B16252_07_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: Custom learning rate schedule'
  prefs: []
  type: TYPE_NORMAL
- en: When training starts, a higher learning rate is used as the loss is high. As
    the model learns more and more, the loss starts decreasing, which requires a lower
    learning rate. Using the preceding learning rate schedule significantly speeds
    up training and convergence. We also need a loss function to optimize.
  prefs: []
  type: TYPE_NORMAL
- en: Loss and metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The loss function is based on categorical cross-entropy. It is a common loss
    function that we have used in previous chapters. In addition to the loss, an accuracy
    metric is also defined to track how the model is doing on the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: This formulation has been used in previous chapters as well. We are almost ready
    to start training. There are two more steps we must follow before we get into
    the custom training function. We need to set up checkpoints to save progress in
    case of failures, and we also need to mask inputs for the Encoder and Decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Checkpoints and masks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to specify a checkpoint directory for TensorFlow to save progress.
    We will use a `CheckpointManager` here, which automatically manages the checkpoints
    and stores a limited number of them. A checkpoint can be quite large. Five checkpoints
    for the small model would take up approximately 243 MB of space. Larger models
    would take up more space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, a method that will create masks for the input images and captions must
    be defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Inputs are always a constant length, so the input sequence is set as ones. Only
    the captions, which are used by the Decoder, are masked. There are two types of
    masks for the Decoder. The first mask is the padding mask. Since the captions
    are set to the maximum length to handle 99% of the captions, which works out at
    about 22 tokens, any captions that are smaller than this number of tokens have
    padding appended to the end of them. The padding mask helps separate caption tokens
    from padding tokens. The second mask is the look-ahead mask. It prevents the Decoder
    from seeing tokens from the future or tokens it has not generated yet. Now, we
    are ready to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: Custom training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the summarization model, teacher forcing will be used for training.
    Consequently, a custom training function will be used. First, we must define a function
    that will train on one batch of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'This method is very similar to the summarization training code. All we need
    to do now is define the number of epochs and batch size and start training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Training can be started from the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This training may take some time. An epoch of training takes about 11 minutes
    on my GPU-enabled machine. If you contrast this to the summarization model, this
    model is training extremely fast. Compared to the summarization model, which contains
    13 million parameters, it is much smaller and trains very fast. This speed boost
    is due to the lack of recurrence.
  prefs: []
  type: TYPE_NORMAL
- en: The state-of-the-art summarization models use the Transformer architecture along
    with subword encoding. Given that you have all the pieces of the Transformer,
    a good exercise to test your understanding would be editing the VisualEncoder
    to process text and rebuild the summarization model as a Transformer. You will
    then be able to experience these speedup and accuracy improvements.
  prefs: []
  type: TYPE_NORMAL
- en: A longer training time allows the model to learn better. However, this model
    can give reasonable results in as few as 5-10 epochs of training. Once training
    is complete, we can try the model on some images.
  prefs: []
  type: TYPE_NORMAL
- en: Generating captions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, you need to be congratulated! You made it through a whirlwind implementation
    of the Transformer. I am sure you must have noticed a number of common building
    blocks that were used in previous chapters. Since the Transformer model is complex,
    we left it for this chapter to look at other techniques like Bahdanau attention,
    custom layers, custom rate schedules, custom training using teacher forcing, and
    checkpointing so that we could cover a lot of ground quickly in this chapter.
    You should consider all these building blocks an important part of your toolkit
    when you try and solve an NLP problem.
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let's try and caption some images. Again, we will use a
    Jupyter notebook for inference so that we can quickly try out different images.
    All the code for inference is in the `image-captioning-inference.ipynb` file.
  prefs: []
  type: TYPE_NORMAL
- en: The inference code needs to load the Subword Encoder, set up masking, instantiate
    a ResNet50 model to extract features from test images, and generate captions a
    token at a time until the end of the sequence or a maximum sequence length is
    reached. Let's go over these steps one at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''ve done the appropriate imports and optionally initialized the GPU,
    we can load the Subword Encoder that was saved when we pre-processed the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We must now instantiate the Transformer model. This is an important step to
    ensure the parameters are the same as the checkpoint ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Restoring the model from the checkpoint requires the optimizer, even though
    we are not training the model. So, we will reuse the custom scheduler from the
    training code. As this code was provided previously, it has been omitted here.
    For the checkpoint, I used a model that was trained for 40 epochs, but without
    positional encoding in the Encoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we must set up the masking function for the generated captions. Note
    that the look ahead masks don''t really help during inference as future tokens
    have not been generated yet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The main code for inference is in an `evaluate()` function. This method takes
    in the image features generated by ResNet50 as input and seeds the output caption
    sequence with the start token. Then, it runs in a loop to generate a token at
    a time while updating the masks, until an end of sequence token is encountered
    or the maximum length of the caption is reached:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'A wrapper method is used to call the evaluation method and print out the caption:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'The only thing remaining now is instantiating a ResNet50 model to extract features
    from image files on the fly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s the moment of truth, finally! Let''s try out the model on an image. We
    will load the image, pre-process it for ResNet50, and extract the features from
    it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the example image and its caption:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A person riding a wave on a surfboard in the ocean  Description automatically
    generated](img/B16252_07_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Generated caption - A man is riding a surfboard on a wave'
  prefs: []
  type: TYPE_NORMAL
- en: This looks like an amazing caption for the given image! However, the overall
    accuracy of the model is in the low 30s. There is a lot of scope for improvement
    in the model. The next section talks about the state-of-the-art techniques for
    image captioning and also proposes some simpler ideas that you can try and play around
    with.
  prefs: []
  type: TYPE_NORMAL
- en: Note that you may see slightly different results. The reviewer for this book
    got the result *A man in a black shirt is riding a surfboard* while running this
    code. This is expected as slight differences in the probabilities and the exact
    place where the model stops training in the loss surface is not exact. We are
    operating in the probabilistic realm here, so there may be slight differences.
    You may have experienced similar differences in the text generation and summarization
    code in the previous chapters as well.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows some more examples of images and their captions.
    The notebook contains several good, as well as some atrocious, examples of the
    generated labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing photo, different, various, group  Description automatically
    generated](img/B16252_07_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: Examples of images and their generated captions'
  prefs: []
  type: TYPE_NORMAL
- en: None of these images were in the training set. The caption quality goes down
    from top to bottom. Our model understands close up, cake, groups of people, sandy
    beaches, streets, and luggage, among other things. However, the bottom two examples
    are concerning. They hint at some **bias** in the model. In both of the bottom
    two images, the model is misinterpreting gender.
  prefs: []
  type: TYPE_NORMAL
- en: The images were deliberately chosen to show a woman in a business suit and women
    playing basketball. In both cases, the model proposes men in the captions. When
    the model was tried with a female tennis player's image, it guessed the right
    gender, but it changed genders in an image from a women's soccer game. Bias in
    models is a very important concern. In cases such as image captioning, this bias
    is immediately apparent. In fact, over 600,000 images were removed from the ImageNet
    database ([https://bit.ly/3qk4FgN](https://bit.ly/3qk4FgN)) in 2019 after bias
    was found in how it classifies and tags people in its pictures. ResNet50 is pre-trained
    on ImageNet. However, in other models, the bias may be harder to detect. Building
    fair deep learning models and reducing bias in models are active areas of research
    in the ML community.
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that we skipped running the model on an evaluation set
    and on the test set. This was done for brevity, and also because those techniques
    were covered previously.
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick note on metrics for evaluating the quality of captions. We saw ROUGE
    metrics in the previous chapters. ROUGE-L is still applicable in the case of image
    captioning. You can use a mental model of the caption as a summary of an image,
    as opposed to the summary of a paragraph in text summarization. There can be more than
    one way to express the summary, and ROUGE-L tries to capture the intent. There
    are two other commonly reported metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**BLEU**: This stands for **Bilingual Evaluation Understudy** and is the most
    popular metric in machine translation. We can cast the image captioning problem
    as a machine translation problem as well. It relies on n-grams for computing the
    overlap of the predicted text with a number of reference texts and combines the
    results into one score.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CIDEr**: This stands for **Consensus-Based Image Description Evaluation**
    and was proposed in a paper by the same name in 2015\. It tries to deal with the
    difficulty of automatic evaluation when multiple captions could be reasonable
    by combining TF-IDF and n-grams. The metric tries to compare the captions generated
    by the model against multiple captions by human annotators and tries to score
    them based on consensus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before wrapping up this chapter, let's spend a little time discussing ways to
    improve performance and state-of-the-art models.
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance and state-of-the-art models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first talk through some simple experiments you can try to improve performance
    before talking about the latest models. Recall our discussion on positional encodings
    for inputs in the Encoder. Adding or removing positional encodings helps or hinders
    performance. In the previous chapter, we implemented the beam search algorithm
    for generating summaries. You can adapt the beam search code and see an improvement
    in the results with beam search. Another avenue of exploration is the ResNet50\.
    We used a pre-trained network and did not fine-tune it further. It is possible
    to build an architecture where ResNet is part of the architecture and not a pre-processing
    step. Image files are loaded in, and features are extracted from ResNet50 as part
    of the VisualEncoder. ResNet50 layers can be trained from the get-go, or only
    in the last few iterations. This idea is implemented in the `resnet-finetuning.py`
    file for you to try. Another line of thinking is using a different object detection
    model than ResNet50 or using the output from a different layer. You can try a
    more complex version of ResNet like ResNet152, or a different object detection
    model like Detectron from Facebook or other models. It should be quite easy to
    use a different model in our code as it is quite modular.
  prefs: []
  type: TYPE_NORMAL
- en: When you use a different model for extracting image features, the key will be
    to make sure tensor dimensions are flowing properly through the Encoder. The Decoder
    should not require any changes. Depending on the complexity of the model, you
    can either pre-process and store the image features or compute them on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that we just used the pixels from the image directly. This was based
    on a paper published recently at CVPR titled *Pixel-BERT*. Most models use region
    proposals extracted from images instead of the pixels directly. Object detection
    in an image involves drawing a boundary around that object in the image. Another
    way to perform the same task is to classify each pixel into an object or background.
    These region proposals can be in the form of bounding boxes in an image. State-of-the-art
    models use bounding boxes or region proposals as input.
  prefs: []
  type: TYPE_NORMAL
- en: The second-biggest gain in image captioning comes from pre-training. Recall
    that BERT and GPT are pre-trained on specific pre-training objectives. Models
    differ based on whether the Encoder is pre-trained or both the Encoder and Decoder
    are pre-trained. A common pre-training objective is a version of the BERT MLM
    task. Recall that BERT inputs are structured as `[CLS] I1 I2 … In [SEP] J1 J2
    … Jk [SEP]`, where some of the tokens from the input sequence are masked. This
    is adapted for image captioning, where the image features and caption tokens in
    the input are concatenated. Caption tokens are masked similar to how they are
    in the BERT model, and the pre-training objective is for the model to predict
    the masked token. After pre-training, the output of the CLS token can be used
    for classification or fed to the Decoder to generate the caption. Care must be
    exercised to not pre-train on the same dataset, like that for evaluation. An example
    of the setup could be using the Visual Genome and Flickr30k datasets for pre-training
    and COCO for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning is an active area of research. The research is just getting
    started on multi-modal networks in general. Now, let's recap everything we've
    learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of deep learning, specific architectures have been developed to
    handle specific modalities. **Convolutional Neural Networks** (**CNNs**) have
    been incredibly effective in processing images and is the standard architecture
    for CV tasks. However, the world of research is moving toward the world of multi-modal
    networks, which can take multiple types of inputs, like sounds, images, text,
    and so on and perform cognition like humans. After reviewing multi-modal networks,
    we dived into vision and language tasks as a specific focus. There are a number
    of problems in this particular area, including image captioning, visual question
    answering, VCR, and text-to-image, among others.
  prefs: []
  type: TYPE_NORMAL
- en: Building on our learnings from previous chapters on seq2seq architectures, custom
    TensorFlow layers and models, custom learning schedules, and custom training loops,
    we implemented a Transformer model from scratch. Transformers are state of the
    art at the time of writing. We took a quick look at the basic concepts of CNNs
    to help with the image side of things. We were able to build a model that may
    not be able to generate a thousand words for a picture but is definitely able
    to generate a human-readable caption. Its performance still needs improvement,
    and we discussed a number of possibilities so that we can try to do so, including
    the latest techniques.
  prefs: []
  type: TYPE_NORMAL
- en: It is apparent that deep models perform very well when they contain a lot of
    data. The BERT and GPT models have shown the value of pre-training on massive
    amounts of data. It is still very hard to get good quality labeled data for use
    in pre-training or fine-tuning. In the world of NLP, we have a lot of text data,
    but not enough labeled data. The next chapter focuses on weak supervision to build
    classification models that can label data for pre-training or even fine-tuning
    tasks.
  prefs: []
  type: TYPE_NORMAL
