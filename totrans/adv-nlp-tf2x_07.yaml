- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Multi-Modal Networks and Image Captioning with ResNets and Transformer Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态网络与图像描述生成，使用 ResNet 和 Transformer 网络
- en: '"A picture is worth a thousand words" is a famous adage. In this chapter, we''ll
    put this adage to the test and generate captions for an image. In doing so, we''ll
    work with **multi-modal** networks. Thus far, we have operated on text as input.
    Humans can handle multiple sensory inputs together to make sense of the environment
    around them. We can watch a video with subtitles and combine the information provided
    to understand the scene. We can use facial expressions and lip movement along
    with sounds to understand speech. We can recognize text in an image, and we can
    answer natural language questions about images. In other words, we have the ability
    to process information from different modalities at the same time, and then put
    them together to understand the world around us. The future of artificial intelligence
    and deep learning is in building multi-modal networks as they closely mimic human
    cognitive functions.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: “一图胜千言”是一句著名的谚语。在本章中，我们将验证这句谚语，并为图像生成描述。在此过程中，我们将使用**多模态**网络。到目前为止，我们的输入是文本。人类可以将多种感官输入结合起来，理解周围的环境。我们可以带字幕观看视频并结合所提供的信息来理解场景。我们可以通过面部表情和唇部动作与声音一起理解语言。我们可以在图像中识别文本，并能回答有关图像的自然语言问题。换句话说，我们能够同时处理来自不同模态的信息，并将它们整合在一起理解我们周围的世界。人工智能和深度学习的未来在于构建多模态网络，因为它们能
    closely 模拟人类的认知功能。
- en: Recent advances in image, speech, and text processing lay a solid foundation
    for multi-modal networks. This chapter transitions you from the world of NLP to
    the world of multi-modal learning, where we will combine visual and textual features
    using the familiar Transformer architecture.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 图像、语音和文本处理的最新进展为多模态网络奠定了坚实的基础。本章将引导你从自然语言处理（NLP）领域过渡到多模态学习领域，我们将使用熟悉的 Transformer
    架构结合视觉和文本特征。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Overview of multi-modal deep learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多模态深度学习概述
- en: Vision and language tasks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视觉与语言任务
- en: Detailed overview of the Image Captioning task and the MS-COCO dataset
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像描述任务和 MS-COCO 数据集的详细概述
- en: Architecture of a residual network, specifically ResNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差网络架构，特别是 ResNet
- en: Extracting features from images using pre-trained ResNet50
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的 ResNet50 提取图像特征
- en: Building a full Transformer model from scratch
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从零构建完整的 Transformer 模型
- en: Ideas for improving the performance of image captioning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升图像描述生成性能的思路
- en: Our journey starts with an overview of the various tasks in the visual understanding
    domain, with a focus on tasks that combine language and images.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程从视觉理解领域的各种任务概述开始，重点介绍结合语言和图像的任务。
- en: Multi-modal deep learning
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多模态深度学习
- en: The dictionary definition of "modality" states that it is "a particular mode
    in which something exists or is experienced or expressed." Sensory modalities,
    like touch, taste, smell, vision, and sound, allow humans to experience the world
    around them. Suppose you are out at the farm picking strawberries, and your friend
    tells you to pick ripe and red strawberries. The instruction, *ripe and red strawberries*,
    is processed and converted into a visual and haptic criterion. As you see strawberries
    and feel them, you know instinctively if they match the criteria of *ripe and
    red*. This task is an example of multiple modalities working together for a task.
    As you can imagine, these capabilities are essential for robotics.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “模态”一词的词典定义是“某物存在、体验或表达的特定方式。”感官模态，如触觉、味觉、嗅觉、视觉和听觉，使人类能够体验周围的世界。假设你在农场采摘草莓，朋友告诉你挑选成熟且红的草莓。指令
    *成熟且红的草莓* 会被处理并转化为视觉和触觉标准。当你看到草莓并触摸它们时，你会直觉地知道它们是否符合 *成熟且红的* 标准。这项任务就是多个模态协同工作来完成一个任务的例子。正如你能想象的，这些能力对机器人学至关重要。
- en: As a direct application of the preceding example, consider a harvesting robot
    that needs to pick ripe and ready fruit. In December 1976, Harry McGurk and John
    MacDonald published a piece of research titled *Hearing lips and seeing voices*
    ([https://www.nature.com/articles/264746a0](https://www.nature.com/articles/264746a0))
    in the reputed journal, Nature. They recorded a video of a young woman talking,
    where utterances of the syllable *ba* had been dubbed onto the lip movement of
    the syllable *ga*. When this video was played back to adults, people repeated
    hearing the syllable *da*. When the audio track was played without the video,
    the right syllable was reported. This research paper highlighted the role of vision
    in speech recognition. Speech recognition models using lip-reading information
    were developed in the field of **Audio-Visual Speech Recognition** (**AVSR**).
    There are several exciting applications of multi-modal deep learning models in
    medical devices and diagnosis, learning technology, and other **Artificial Intelligence**
    (**AI**) areas.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为前面示例的直接应用，考虑一个需要采摘成熟果实的收割机器人。1976年12月，Harry McGurk和John MacDonald在著名期刊《自然》上发表了一篇题为*听嘴唇，看声音*的研究论文（[https://www.nature.com/articles/264746a0](https://www.nature.com/articles/264746a0)）。他们录制了一段年轻女性说话的视频，其中*ba*音节的发音被配上了*ga*音节的口型。当这个视频播放给成年人时，人们听到的音节是*da*。而当没有视频只播放音频时，正确的音节被报告了出来。这篇研究论文强调了视觉在语音识别中的作用。使用唇读信息的语音识别模型在**视听语音识别**（**AVSR**）领域得到了开发。多模态深度学习模型在医疗设备和诊断、学习技术及其他**人工智能**（**AI**）领域中有许多令人兴奋的应用。
- en: Let's drill down into the specific interaction of vision and language and the
    various tasks we can perform.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨视觉与语言的具体互动以及我们可以执行的各种任务。
- en: Vision and language tasks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉与语言任务
- en: 'A combination of **Computer Vision** (**CV**) and **Natural Language Processing**
    (**NLP**) allows us to build smart AI systems that can see and talk. CV and NLP
    together produce interesting tasks for model development. Taking an image and
    generating a caption for it is a well-known task. A practical application of this
    task is generating alt-text tags for images on web pages. Visually impaired readers
    use screen readers, which can read these tags while reading the page, improving
    the accessibility of web pages. Other topics in this area include video captioning
    and storytelling – composing a story from a sequence of images. The following
    image shows some examples of images and captions. Our primary focus in this chapter
    is on image captioning:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算机视觉**（**CV**）和**自然语言处理**（**NLP**）的结合使我们能够构建能够“看”和“说”的智能AI系统。CV和NLP的结合为模型开发提供了有趣的任务。给定一张图像并为其生成描述是一个广为人知的任务。该任务的一个实际应用是为网页上的图像生成替代文本标签。视觉障碍读者使用屏幕阅读器来读取这些标签，从而在浏览网页时提高网页的可访问性。该领域的其他话题包括视频描述和讲故事——从一系列图像中编写故事。下图展示了图像和描述的一些示例。本章的主要关注点是图像描述：'
- en: '![A picture containing photo, room, bunch, many  Description automatically
    generated](img/B16252_07_01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含照片、房间、大量物品的图片  描述自动生成](img/B16252_07_01.png)'
- en: 'Figure 7.1: Example images with captions'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：带有描述的示例图像
- en: '**Visual Question Answering** (**VQA**) is the challenging task of answering
    questions about objects in the image. The following image shows some examples
    from the VQA dataset. Compared to image captioning, where prominent objects are
    reflected in the caption, VQA is a more complex task. Answering the question may
    also require some reasoning.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**视觉问答**（**VQA**）是一个具有挑战性的任务，旨在回答关于图像中物体的问题。下图展示了来自VQA数据集的一些示例。与图像描述不同，图像描述会在描述中体现显著物体，而VQA是一个更为复杂的任务。回答问题可能还需要一定的推理。'
- en: 'Consider the bottom-right panel in the following image. Answering the question,
    "Does this person have 20/20 vision?" requires reasoning. Datasets for VQA are
    available at [visualqa.org](http://visualqa.org):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请看下图右下方的面板。回答问题“这个人视力是20/20吗？”需要推理。VQA的数据集可以在[visualqa.org](http://visualqa.org)获取：
- en: '![A person posing for a photo  Description automatically generated](img/B16252_07_02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![一人摆姿势拍照  描述自动生成](img/B16252_07_02.png)'
- en: 'Figure 7.2: Examples from the VQA Dataset (Source: VQA: Visual Question Answering
    by Agrawal et al.)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：来自VQA数据集的示例（来源：VQA：视觉问答，Agrawal等人）
- en: 'Reasoning leads to another challenging but fascinating task – **Visual Commonsense
    Reasoning** (**VCR**). When we look at an image, we can guess emotions, actions,
    and frame a hypothesis of what is happening. Such a task is quite easy for people
    and may even happen without conscious effort. The aim of the VCR task is to build
    models that can perform such a task. These models should also be able to explain
    or choose an appropriate reason for the logical inference that''s been made. The
    following image shows an example from the VCR dataset. More details on the VCR
    dataset can be found at [visualcommonsense.com](http://visualcommonsense.com):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 推理引出了另一个具有挑战性但又令人着迷的任务——**视觉常识推理**（**VCR**）。当我们查看一张图像时，我们可以猜测情绪、动作，并推测正在发生的事情。这个任务对人类来说相当简单，甚至可能不需要有意识地努力。VCR
    任务的目标是构建能够执行此类任务的模型。这些模型还应能够解释或选择一个适当的理由，来说明已作出的逻辑推理。以下图像展示了 VCR 数据集中的一个示例。有关
    VCR 数据集的更多细节，请访问 [visualcommonsense.com](http://visualcommonsense.com)：
- en: '![A screenshot of a social media post  Description automatically generated](img/B16252_07_03.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![社交媒体帖子截图 说明自动生成](img/B16252_07_03.png)'
- en: 'Figure 7.3: VCR example (Source: From Recognition to Cognition: Visual Commonsense
    Reasoning by Zellers et al.)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：VCR 示例（来源：《从识别到认知：视觉常识推理》，Zellers 等人著）
- en: Thus far, we have gone from images to text. The reverse is also possible and
    is an active area of research. In this task, images or videos are generated from
    text using GANs and other generative architectures. Imagine being able to generate
    an illustrative comic book from the text of a story! This particular task is at
    the forefront of research currently.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经从图像转向了文本。反过来也可以实现，并且是一个活跃的研究领域。在这个任务中，图像或视频是通过使用生成对抗网络（GANs）和其他生成架构从文本生成的。想象一下，能够根据故事的文本生成一本插画漫画书！这个特定任务目前处于研究的前沿。
- en: A critical concept in this area is **visual grounding**. Grounding enables tying
    concepts in language to the real world. Simply put, it matches words to objects
    in a picture. By combining vision and language, we can ground concepts from languages
    to parts of an image. For example, mapping the word "basketball" to something
    that looks like one in an image is called visual grounding. There can be more
    abstract concepts that can be grounded. For example, a short elephant and a short
    person have different measurements. Grounding provides us with a way to see what
    models are learning and helps us guide them in the right direction.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 该领域的一个关键概念是**视觉基础**。基础使得将语言中的概念与现实世界相连接。简单来说，就是将词语与图片中的物体相匹配。通过结合视觉和语言，我们可以将语言中的概念与图像中的部分进行对接。例如，将“篮球”这个词与图像中看起来像篮球的物体匹配，这就是视觉基础。也可以有更抽象的概念进行基础化。例如，一只矮小的大象和一个矮小的人具有不同的测量值。基础为我们提供了一种方式来查看模型正在学习的内容，并帮助我们引导它们朝着正确的方向前进。
- en: Now that we have a proper perspective on vision and language tasks, let's dive
    deep into an image captioning task.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对视觉和语言任务有了一个正确的视角，让我们深入探讨图像描述任务。
- en: Image captioning
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像描述
- en: Image captioning is all about describing the contents of an image in a sentence.
    Captions can help in content-based image retrieval and visual search. We already
    discussed how captions could improve the accessibility of websites by making it
    easier for screen readers to summarize the content of an image. A caption can
    be considered a summary of the image. Once we frame the problem as an image summarization
    problem, we can adapt the seq2seq model from the previous chapter to solve this
    problem. In text summarization, the input is a sequence of the long-form article,
    and the output is a short sequence summarizing the content. In image captioning,
    the output is similar in format to summarization. However, it may not be obvious
    how to structure an image that consists of pixels as a sequence of embeddings
    to be fed into the Encoder.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图像描述就是用一句话描述图像的内容。描述有助于基于内容的图像检索和视觉搜索。我们已经讨论过，描述如何通过使屏幕阅读器更容易总结图像内容来提高网站的可访问性。描述可以视为图像的总结。一旦我们将问题框定为图像摘要问题，我们可以借用上一章中的
    seq2seq 模型来解决这个问题。在文本摘要中，输入是长篇文章的序列，输出是总结内容的简短序列。在图像描述中，输出格式与摘要类似。然而，如何将由像素组成的图像结构化为一系列嵌入，以便输入到编码器中，这可能并不显而易见。
- en: Secondly, the summarization architecture used **Bi-directional Long Short-Term
    Memory networks** (**BiLSTMs**), with the underlying principle that words that
    are closer together to each other are similar to each other in meaning. BiLSTMs
    exploited this property by looking at the input sequence from both sides and generated
    encoded representations. Generating a representation for an image that works for
    the Encoder requires some thought.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，摘要架构使用了**双向长短期记忆网络**（**BiLSTMs**），其基本原理是相互之间靠得更近的单词在意义上也较为相似。BiLSTMs通过从两侧查看输入序列并生成编码表示来利用这一特性。为图像生成适合编码器的表示需要一些思考。
- en: 'A naïve solution for representing images as a sequence could be expressing
    them as a list of pixels. So, an image of size 28x28 pixels becomes a sequence
    of 784 tokens. When the tokens represent text, an Embedding layer learns the representation
    of each token. If this Embedding layer had a dimension of 64, then each token
    would be represented by a 64-dimensional vector. This embedding vector was learned
    during training. Extending our analogy of using a pixel as a token, a straightforward
    solution is to use the value of the Red/Green/Blue channels of the pixel in an
    image to generate a three-dimensional embedding. However, training these three
    dimensions does not sound like a logical approach. More importantly, pixels are
    laid out in a 2D representation, while the text is laid out in a 1D representation.
    This concept is illustrated in the following image. Words are related to words
    next to each other. When pixels are laid out in a sequence, the **data locality**
    of these pixels is broken since the content of a pixel is related to the pixels
    all around it, not just to the left and right of it. This idea is shown by the
    following super zoomed in image of a tulip:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个表示图像为序列的简单解决方案是将其表示为像素列表。因此，一个28x28像素的图像就变成了784个标记的序列。当这些标记代表文本时，嵌入层学习每个标记的表示。如果这个嵌入层的维度为64，那么每个标记将通过一个64维的向量来表示。这个嵌入向量是在训练过程中学习到的。继续延伸我们使用像素作为标记的类比，一种直接的解决方案是使用图像中每个像素的红/绿/蓝通道值来生成三维嵌入。然而，训练这三个维度似乎并不是一种合乎逻辑的方法。更重要的是，像素在2D表示中排布，而文本则是在1D表示中排布。这个概念在以下图像中得到了说明。单词与其旁边的单词相关。当像素以序列的形式排列时，这些像素的**数据局部性**被打破，因为像素的内容与其周围的所有像素相关，而不仅仅是与其左右相邻的像素相关。这个想法通过下面的图像展示出来：
- en: '![](img/B16252_07_04.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_07_04.png)'
- en: 'Figure 7.4: Data locality in text versus images'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：文本与图像中的数据局部性
- en: 'Data locality and translation invariance are two critical properties of images.
    Translation invariance is the idea that an object can appear in various spots
    in an image. In a fully connected model, the model would try to learn the position
    of the object, which would prevent the model from generalizing. The specialized
    architecture of **Convolutional Neural Networks** (**CNNs**) can be used to exploit
    these properties and extract signals from the image. At a high level, we use CNNs,
    specifically the **ResNet50** architecture, to convert the image into a tensor
    that can be fed to a seq2seq architecture. Our model will combine the best of
    CNNs and RNNs to handle the image and text parts under the seq2seq model. The
    following diagram shows our architecture at a very high level:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据局部性和平移不变性是图像的两个关键特性。平移不变性是指一个物体可以出现在图像的不同位置。在一个全连接模型中，模型会试图学习物体的位置，这会阻止模型的泛化。**卷积神经网络**（**CNNs**）的专门架构可以用来利用这些特性并从图像中提取信号。总的来说，我们使用CNNs，特别是**ResNet50**架构，将图像转换为可以输入到seq2seq架构的张量。我们的模型将在seq2seq模型下结合CNNs和RNNs的优势来处理图像和文本部分。以下图示展示了我们架构的高层概述：
- en: '![](img/B16252_07_05.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_07_05.png)'
- en: 'Figure 7.5: High-level image captioning model architecture'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：高级图像标注模型架构
- en: While a comprehensive explanation of CNNs is beyond the scope of this book,
    we will review the key concepts in short. Since we will be using a pre-trained
    CNN model, we won't have to go into much depth about CNNs. *Python Machine Learning,
    Third Edition*, published by Packt, is an excellent resource for reading up on
    CNNs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对CNNs的全面解释超出了本书的范围，但我们将简要回顾关键概念。由于我们将使用一个预训练的CNN模型，因此无需深入探讨CNNs的细节。*Python机器学习（第三版）*（由Packt出版）是一本阅读CNNs的优秀资源。
- en: In the previous chapter on text summarization, we built a seq2seq model with
    attention. In this chapter, we will build a Transformer model. Transformer models
    are currently state of the art in NLP. The Encoder part of the Transformer is
    the core of the **Bidirectional Encoder Representations from Transformers** (**BERT**)
    architecture. The Decoder part of the Transformer is the core of the **Generative
    Pre-trained Transformer** (**GPT**) family of architectures. There is a specific
    advantage of the Transformer architecture that is relevant to the image captioning
    problem. In the seq2seq architecture, we used BiLSTMS, which tries to learn relationships
    via co-occurrence. In the Transformer architecture, there is no recurrence. Instead,
    positional encodings and self-attention model relationships are made between inputs.
    This change enables us to feed in processed image patches as input and hope that
    the relationships between the image patches will be learned.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章的文本摘要中，我们构建了一个带有注意力机制的seq2seq模型。在这一章，我们将构建一个Transformer模型。Transformer模型目前是自然语言处理领域的最前沿技术。Transformer的编码器部分是**双向编码器表示（Bidirectional
    Encoder Representations from Transformers）**（**BERT**）架构的核心。Transformer的解码器部分是**生成式预训练Transformer**（**GPT**）系列架构的核心。Transformer架构有一个在图像字幕生成问题中尤为重要的特定优势。在seq2seq架构中，我们使用了BiLSTM，它尝试通过共现来学习关系。在Transformer架构中，没有递归。相反，使用位置编码和自注意力机制来建模输入之间的关系。这一变化使我们能够将处理后的图像补丁作为输入，并希望学习到图像补丁之间的关系。
- en: Implementing the image captioning model requires a large amount of code as we
    will implement several pieces, like pre-processing images, with ResNet50 and a
    complete implementation of Transformer architecture from scratch. This chapter
    contains much more code than the other chapters. We will rely on code fragments
    to highlight the most important aspects of the code rather than going over every
    line of code in detail, as we have been doing so far.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 实现图像字幕生成模型需要大量代码，因为我们将实现多个部分，例如使用ResNet50进行图像预处理，并从零开始完整实现Transformer架构。本章的代码量远远超过其他章节。我们将依赖代码片段来突出代码中的最重要部分，而不是像以前那样逐行详细讲解代码。
- en: 'The main steps of building our model are summarized here:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 构建模型的主要步骤总结如下：
- en: '**Downloading the data**: Given the large size of the dataset, this is a time-consuming
    activity.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**下载数据**：由于数据集的庞大体积，这是一个耗时的活动。'
- en: '**Pre-processing captions**: Since the captions are in JSON format, they are
    flattened into a CSV for easier processing.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预处理字幕**：由于字幕是JSON格式的，它们被平坦化为CSV格式，以便更轻松地处理。'
- en: '**Feature extraction**: We pass the image files through ResNet50 to extract
    features and save them to speed up training.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征提取**：我们通过ResNet50将图像文件传递来提取特征，并将其保存，以加速训练。'
- en: '**Transformer training**: A full Transformer model with positional encoding,
    multi-head attention, an Encoder, and a Decoder is trained on the processed data.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Transformer训练**：一个完整的Transformer模型，包括位置编码、多头注意力机制、编码器和解码器，在处理后的数据上进行训练。'
- en: '**Inference**: Use the trained model to caption some images!'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**推理**：使用训练好的模型为一些图像生成字幕！'
- en: '**Evaluating performance**: **Bilingual Evaluation Understudy** (**BLEU**)
    scores are used to compare the trained models with ground truth data.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估性能**：使用**双语评估替代法**（**Bilingual Evaluation Understudy**，简称**BLEU**）分数来比较训练模型与真实数据。'
- en: Let's start with the dataset first.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先从数据集开始。
- en: MS-COCO dataset for image captioning
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MS-COCO数据集用于图像字幕生成
- en: Microsoft published the **Common Objects in Context** or **COCO** dataset in
    2014\. All the versions of the dataset can be found at [cocodataset.org](http://cocodataset.org).
    The COCO dataset is a big dataset that's used for object detection, segmentation,
    and captioning, among other annotations. Our focus will be on the 2014 training
    and validation images, where five captions per image are available. There are
    roughly 83K images in the training set and 41K images in the validation set. The
    training and validation images and captions need to be downloaded from the COCO
    website.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微软在2014年发布了**上下文中的常见物体**（**Common Objects in Context**，简称**COCO**）数据集。所有版本的数据集可以在[cocodataset.org](http://cocodataset.org)上找到。COCO数据集是一个大型数据集，广泛用于物体检测、分割和字幕生成等任务。我们的重点将放在2014年的训练和验证图像上，每个图像都有五个字幕。训练集大约有83K张图像，验证集有41K张图像。训练和验证图像及字幕需要从COCO网站下载。
- en: '**Large download warning**: The training image dataset is approximately 13
    GB, while the validation dataset is over 6 GB. The annotations for the image files,
    which include captions, are about 214 MB in size. Please be careful of your internet
    bandwidth usage and potential costs as you download this dataset.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**大文件下载警告**：训练集图像数据集大约为 13 GB，而验证集数据集超过 6 GB。图像文件的注释，包括标题，大小约为 214 MB。下载该数据集时，请小心你的网络带宽使用和潜在费用。'
- en: Google has also published a new Conceptual Captions dataset at [https://ai.google.com/research/ConceptualCaptions](https://ai.google.com/research/ConceptualCaptions).
    It contains over 3M images. Having a large dataset allows deep models to train
    better. There is a corresponding competition where you can submit your models
    and see how they compete with others.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Google 还发布了一个新的 Conceptual Captions 数据集，地址为 [https://ai.google.com/research/ConceptualCaptions](https://ai.google.com/research/ConceptualCaptions)。它包含超过
    300 万张图像。拥有一个大型数据集可以让深度模型更好地训练。还有一个相应的比赛，你可以提交你的模型，看看它与其他模型的表现如何。
- en: 'Given that these are large downloads, you may wish to use the download that''s
    the most comfortable to you. If `wget` is available on your environment, you could
    use it to download the files, like so:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这些是大文件下载，你可能希望使用最适合你的下载方式。如果你的环境中有 `wget`，你可以使用它来下载文件，方法如下：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Note that the annotations for the training and validation sets are in one compressed
    archive. Once the files have been downloaded, they need to be unzipped. Each of
    these compressed files creates its own folder and puts the contents in there.
    We will create a folder called `data` and move all the expanded contents inside
    it:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，训练集和验证集的注释文件是一个压缩包。下载文件后，需要解压。每个压缩文件都会创建一个文件夹，并将内容放入其中。我们将创建一个名为 `data`
    的文件夹，并将所有解压后的内容移动到其中：
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'All the images are either in the `train2014` or `val2014` folder. The code
    for the initial pre-processing of the data is in the `data-download-preprocess.py`
    file. Captions for the training and validation images can be found in the `captions_train2014.json`
    or `captions_val2014.json` JSON file inside the `annotations` subfolder. Both
    of these files are in a similar format. The files have four main keys – info,
    image, license, and annotation. The image key contains a record per image, along
    with information about the size, URL, name, and a unique ID that is used to refer
    to that image in the dataset. Captions are stored as a tuple of the image ID and
    caption text, along with a unique ID for the caption. We use the Python `json`
    module to read and process these files:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有图像都在 `train2014` 或 `val2014` 文件夹中。数据的初步预处理代码位于 `data-download-preprocess.py`
    文件中。训练和验证图像的标题可以在 `annotations` 子文件夹中的 `captions_train2014.json` 或 `captions_val2014.json`
    JSON 文件中找到。这两个文件的格式相似。文件中有四个主要键——info、image、license 和 annotation。image 键包含每个图像的记录，以及关于图像的大小、URL、名称和用于引用该图像的唯一
    ID。标题以图像 ID 和标题文本的元组形式存储，并带有一个用于标题的唯一 ID。我们使用 Python 的 `json` 模块来读取和处理这些文件：
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our objective is to produce a single simple file with two columns – one for
    the image file name and another containing the caption for that file. Note that
    the validation set contains half the number of images of the training set. In
    a seminal paper on captioning titled *Deep Visual-Semantic Alignment for Generating
    Image Descriptions*, Andrej Karpathy and Fei-Fei Li proposed training on all the
    training and validation images after reserving 5,000 images from the validation
    set for testing. We will follow this approach by processing the image names and
    IDs into a dictionary:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是生成一个包含两列的简单文件——一列是图像文件名，另一列是该文件的标题。请注意，验证集包含的图像数量是训练集的一半。在一篇关于图像标题生成的开创性论文《*深度视觉-语义对齐用于生成图像描述*》中，Andrej
    Karpathy 和 Fei-Fei Li 提出了在保留 5,000 张验证集图像用于测试后，训练所有的训练集和验证集图像。我们将通过将图像名称和 ID 处理成字典来遵循这种方法：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Since each image has five captions, the validation set cannot be split based
    on captions. Otherwise, there will be leakage of data from the training set into
    the validation/test set. In the preceding code, we reserved the last 5K images
    for the validation set.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每个图像都有五个标题，验证集不能根据标题进行拆分。否则，会出现训练集数据泄漏到验证集/测试集的情况。在前面的代码中，我们保留了最后 5K 张图像用于验证集。
- en: 'Now, let''s go over the captions for the training and validation images and
    create a combined list. We will create empty lists to store the tuples of image
    paths and captions:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看训练集和验证集图像的标题，并创建一个合并的列表。我们将创建空列表来存储图像路径和标题的元组：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will process all the training captions:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将处理所有的训练标签：
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For the validation captions, the logic is similar, but we need to ensure that
    no captions are included for the images that have been reserved:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于验证标签，逻辑类似，但我们需要确保不为已预留的图像添加标签：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Hopefully, there should not be any errors. If you encounter errors, this could
    be due to corrupted downloads or errors while unzipping the files. The training
    dataset is shuffled to aid in training. Finally, two CSV files are persisted with
    the training and testing data:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 希望没有任何错误。如果遇到错误，可能是由于下载文件损坏或解压时出错。训练数据集会进行洗牌，以帮助训练。最后，会持久化保存两个CSV文件，分别包含训练数据和测试数据：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: At this point, the data download and pre-processing phases are complete. The
    next step is to pre-process all the images using ResNet50 to extract features.
    Before we write the code for that, we will take a short detour and look at CNNs
    and the ResNet architecture. If you are already comfortable with CNNs, you may
    skip ahead to the code part.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，数据下载和预处理阶段已经完成。下一步是使用ResNet50对所有图像进行预处理，以提取特征。在我们编写相关代码之前，我们将稍作绕行，了解一下CNN和ResNet架构。如果你已经熟悉CNN，可以跳过这部分，直接进入代码部分。
- en: Image processing with CNNs and ResNet50
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN和ResNet50进行图像处理
- en: In the world of deep learning, specific architectures have been developed to
    handle specific modalities. CNNs have been incredibly successful in processing
    images and are the standard architecture for CV tasks. A good mental model for
    using a pre-trained model for extracting features from images is that of using
    pre-trained word embeddings like GloVe for text. In this particular case, we use
    a specific architecture called ResNet50\. While a comprehensive treatment of CNNs
    is outside the scope of this book, a brief overview of CNNs and ResNet will be
    provided in this section. If you are already comfortable with these concepts,
    you may skip ahead to the section titled *Image feature extraction with ResNet50*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的世界里，已经开发出特定的架构来处理特定的模态。CNN在处理图像方面取得了巨大的成功，并且是计算机视觉任务的标准架构。使用预训练模型提取图像特征的一个很好的思维模型是，将其类比为使用预训练的词向量（如GloVe）来处理文本。在这个特定的案例中，我们使用一种叫做ResNet50的架构。虽然本书并不深入讲解CNN的所有细节，但这一节将简要概述CNN和ResNet。如果你已经熟悉这些概念，可以跳到*使用ResNet50进行图像特征提取*这一部分。
- en: CNNs
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN（卷积神经网络）
- en: 'CNNs are an architecture designed to learn from the following key properties,
    which are relevant to image recognition:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: CNN（卷积神经网络）是一种旨在从以下关键特性中学习的架构，这些特性与图像识别相关：
- en: '**Data locality**: The pixels in an image are highly correlated to the pixels
    around them.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据局部性**：图像中的像素与周围像素高度相关。'
- en: '**Translation invariance**: An object of interest, for example, a bird, may
    appear at different places in an image. The model should be able to identify the
    object, irrespective of the object''s position in the image.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平移不变性**：一个感兴趣的物体，例如鸟，可能出现在图像的不同位置。模型应该能够识别该物体，而不管它在图像中的位置如何。'
- en: '**Scale invariance**: An object of interest may have a smaller or large size,
    depending on the zoom. Ideally, the model should be able to identify objects of
    interest in an image, irrespective of their size.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尺度不变性**：感兴趣的物体可能会根据缩放显示为较小或较大的尺寸。理想情况下，模型应该能够识别图像中的物体，而不管它们的尺寸如何。'
- en: Convolution and pooling layers are key components that aid CNNs in extracting
    features from images.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层和池化层是帮助CNN从图像中提取特征的关键组件。
- en: Convolutions
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积操作
- en: 'A convolution is a mathematical operation that is performed on patches taken
    from an image with a filter. A filter is a matrix, usually square and with 3x3,
    5x5, and 7x7 as common dimensions. The following image shows an example of a 3x3
    convolution matrix applied to a 5x5 image. The image patches are taken from left
    to right and then top to bottom. The number of pixels this patch shifts by every
    step is called the **stride length**. A stride length of 1 in a horizontal and
    vertical direction reduces a 5x5 image to a 3x3 image, as shown here:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积是一种数学运算，它在从图像中提取的区域上执行，使用的是过滤器。过滤器是一个矩阵，通常是方形的，常见的尺寸为3x3、5x5和7x7。下图展示了一个3x3卷积矩阵应用于5x5图像的例子。图像区域从左到右、从上到下提取。每次步进的像素数被称为**步幅**。在水平和垂直方向上，步幅为1时，会将一个5x5的图像缩减为3x3的图像，如下所示：
- en: '![A close up of a green screen  Description automatically generated](img/B16252_07_06.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![一张绿色屏幕的特写图像 自动生成的描述](img/B16252_07_06.png)'
- en: 'Figure 7.6: Example of a convolution operation'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：卷积操作示例
- en: 'The specific filter that was applied here is an edge detection filter. Prior
    to CNNs, CV relied heavily on handcrafted filters. Sobel filters are an example
    of a special filter for the purpose of edge detection. The `convolution-example.ipynb`
    notebook provides an example of detecting edges using the Sobel filter. The code
    is quite straightforward. After the imports, the image file is loaded and converted
    into a grayscale image:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里应用的特定滤波器是边缘检测滤波器。在CNN出现之前，计算机视觉（CV）在很大程度上依赖于手工制作的滤波器。Sobel滤波器是用于边缘检测的特殊滤波器之一。`convolution-example.ipynb`笔记本提供了使用Sobel滤波器检测边缘的示例。代码非常简单。在导入模块后，图像文件被加载并转换为灰度图像：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we define and apply the Sobel filters to the image:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义并将Sobel滤波器应用到图像中：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The original image, along with the intermediate versions, are shown in the
    following image:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像及其中间版本如下图所示：
- en: '![A screen shot of a computer  Description automatically generated](img/B16252_07_07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图 描述自动生成](img/B16252_07_07.png)'
- en: 'Figure 7.7: Edge detection using Sobel filters'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：使用Sobel滤波器进行边缘检测
- en: Constructing such filters is very tedious. However, CNNs can learn many such
    filters by treating the filter matrices as learnable parameters. CNNs often pass
    an image through hundreds or thousands of such filters, referred to as channels,
    and stack them together. You can think of each filter as detecting some features,
    like vertical lines, horizontal lines, arcs, circles, trapezoids, and so on. However,
    the magic happens when multiple such layers are put together. Stacking multiple
    layers leads to learning hierarchical representations. An easy way to understand
    this concept is by imagining that earlier layers are learning simple shapes like
    lines and arcs, middle layers are learning shapes like circles and hexagons, and
    the top layers are learning complex objects like stop signs and steering wheels.
    The convolution operation is the key innovation that exploits data locality and
    extracts features that enable translation invariance.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 构建这样的滤波器是非常繁琐的。然而，CNN（卷积神经网络）可以通过将滤波器矩阵视为可学习的参数来学习许多这样的滤波器。CNN通常会通过数百或数千个这样的滤波器（称为通道）处理一张图像，并将它们堆叠在一起。你可以将每个滤波器视为检测某些特征，如竖直线、水平线、弧形、圆形、梯形等。然而，当多个这样的层组合在一起时，魔法就发生了。堆叠多个层导致了分层表示的学习。理解这一概念的一个简单方法是，想象早期的层学习的是简单的形状，如线条和弧形；中间层学习的是圆形和六边形等形状；顶层学习的是复杂的物体，如停车标志和方向盘。卷积操作是关键创新，它利用数据的局部性并提取出特征，从而实现平移不变性。
- en: A consequence of this layering is the amount of data flowing through the model
    increasing. Pooling is an operation that helps reduce the dimensions of the data
    flowing through and further highlights these features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分层的结果是模型中流动的数据量增加。池化是一种帮助减少通过数据的维度并进一步突出这些特征的操作。
- en: Pooling
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 池化
- en: Once the values from the convolution operation have been computed, a pooling
    operation can be applied to patches to further concentrate the signal in the image.
    The most common form of pooling is called **Max pooling** and is demonstrated
    in the following diagram. It is as simple as taking the maximum value in a patch.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦卷积操作的值被计算出来，就可以对图像中的补丁应用池化操作，以进一步集中图像中的信号。最常见的池化形式称为**最大池化**，并在以下图示中展示。这就像是在一个补丁中取最大值一样简单。
- en: 'The following diagram shows max pooling on non-overlapping 2x2 patches:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示显示了在不重叠的2x2补丁上进行最大池化：
- en: '![A close up of a colorful background  Description automatically generated](img/B16252_07_08.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![色彩丰富的背景特写 描述自动生成](img/B16252_07_08.png)'
- en: 'Figure 7.8: Max pooling operation'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：最大池化操作
- en: Another way to pool is by averaging the values. While pooling reduces the complexity
    and computation load, it also helps modestly with scale invariance. However, there
    is a chance that such a model overfits and does not generalize well. Dropout is
    a technique that helps with regularization and enables such models to generalize
    better.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种池化方法是通过对值进行平均。虽然池化降低了复杂性和计算负担，但它也在一定程度上帮助了尺度不变性。然而，这样的模型有可能会出现过拟合，并且无法很好地泛化。Dropout（丢弃法）是一种有助于正则化的技术，它使得此类模型能够更好地泛化。
- en: Regularization with dropout
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用dropout进行正则化
- en: 'You may recall that we used dropout settings in previous chapters with the
    LSTM and BiLSTM settings. The core idea behind dropout is shown in the following
    diagram:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，在之前的章节中，我们在LSTM和BiLSTM设置中使用了dropout设置。dropout的核心思想如下图所示：
- en: '![A close up of a logo  Description automatically generated](img/B16252_07_09.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.9: Dropout'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Rather than connecting every unit from a lower layer to every unit in the next
    higher layer of the model, some of the connections are randomly dropped during
    training time. Inputs are dropped only during training time. Since dropping inputs
    reduces the total input reaching a node compared to test/inference time, inputs
    are upscaled in the proportion of dropout to ensure the relative magnitudes are
    preserved. Dropping some of the inputs during training forces the model to learn
    more from each of the inputs. This is because it cannot rely on the presence of
    a specific input. This helps the network build resilience to missing inputs and
    consequently helps generalize the models.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: A combination of these techniques helped build deeper and deeper networks. A challenge
    that showed up as networks got deeper was that the signal from the inputs became
    quite small in the higher layers. Residual connections is a technique that helps
    deal with this problem.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Residual connections and ResNets
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Intuition suggests that adding more layers should make performance better. A deeper
    network has more model capacity, so it should be able to model more complex distributions
    compared to shallower networks. As deeper and deeper models were built, a degradation
    in accuracy was observed. Since the reduction happened even on the training data,
    overfitting can be ruled out as a probable cause. As inputs pass through more
    and more layers, the optimizers have a harder time adjusting the gradients to
    the point where learning is impaired in the model. Kaiming He and his collaborators
    published the ResNet architecture in their seminal paper titled *Deep Residual
    Learning for Image Recognition*.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: We must understand residual connections before understanding ResNets. The core
    concept of the residual connection is shown in the following diagram. In a regular
    dense layer, the input is first multiplied by the weights. Then, biases are added
    in, which is a linear operation. The output is passed through an activation function,
    like ReLU, which introduces non-linearity in the layer. The output from the activation
    function is the final output of the layer.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'However, residual connections introduce a summation in-between the linear computation
    and the activation function, as shown on the right-hand side of the following
    diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_07_10.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.10: A conceptual residual connection'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the preceding diagram is only for illustrating the core concept behind
    residual connections. In ResNets, the residual connection is made between multiple
    blocks. The following diagram shows the basic building blocks of ResNet50, also
    referred to as the bottleneck design. This design is called the bottleneck design
    because the 1x1 convolution blocks reduce the dimensions of the inputs before
    passing them to the 3x3 convolution. The last 1x1 block scales the inputs out
    again for the next layer:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a keyboard  Description automatically generated](img/B16252_07_11.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.11: ResNet50 bottleneck building block'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 'ResNet50 is composed of several such blocks stacked on top of each other. There
    are four groups, each consisting of three to seven such blocks. **BatchNorm**
    or batch normalization was proposed by Sergey Ioffe and Christian Szegedy in their
    paper titled *Batch Normalization: Accelerating Deep Network Training By Reducing
    Internal Covariate Shift* in 2015\. Batch normalization aims to reduce the variance
    of the outputs coming from one layer being fed into the next layer. By reducing
    this variance, BatchNorm acts like L2 regularization, which attempts to do the
    same thing by adding the penalties of the magnitude of the weights to the cost
    function. The main motivation of BatchNorm is to efficiently backpropagate gradient
    updates through a large number of layers, while minimizing the risk that this
    update could result in divergence. In stochastic gradient descent, gradients are
    used to update the weights of all the layers at the same time, assuming that the
    output of one layer doesn''t impact any other layers. However, this is not a completely
    valid assumption. For an *n*-layer network, computing this would need *n*th order
    gradients, which is intractable. Instead, batch-norm is used, which works on one
    mini-batch at a time and the constraints of the updates to reduce this unwanted
    shift in the distribution of weights. It does this by normalizing the outputs
    before they are fed into the next layer.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: The last two layers of ResNet50 are dense layers that classify the outputs from
    the last block into an object category. Covering ResNets comprehensively is a
    tough ask, but hopefully, this crash course on CNNs and ResNets has given you
    enough background on how they work. You are encouraged to read the referenced
    papers and *Deep Learning with TensorFlow 2 and Keras, Second Edition*, published
    by Packt, for a detailed treatment of this topic. Fortunately for us, TensorFlow
    provides a pre-trained ResNet50 model that is ready for use. In the next section,
    we'll use this pre-trained ResNet50 model for extracting image features.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Image feature extraction with ResNet50
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ResNet50 models are trained on the ImageNet dataset. This dataset contains millions
    of images in over 20,000 categories. The large-scale visual recognition challenge,
    ILSVRC, focuses on the top 1,000 categories for models to compete on recognizing
    images. Consequently, the top layers of the ResNet50 that perform classification
    have a dimension of 1,000\. The idea behind using a pre-trained ResNet50 model
    is that it is already able to parse out objects that may be useful in image captioning.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The `tensorflow.keras.applications` package provides pre-trained models like
    ResNet50\. At the time of writing, all the pre-trained models provided are related
    to CV. Loading up the pre-trained model is quite easy. All the code for this section
    is in the `feature-extraction.py` file in this chapter's folder on GitHub. The
    main reason for using a separate file is that it gives us the ability to run feature
    extraction as a script.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'Given that we will be processing over 100,000 images, this process may take
    a while. CNNs benefit greatly from a GPU in computation. Let''s get into the code
    now. First, we must set up the paths for the CSV file we created from the JSON
    annotations in the previous chapter:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'ResNet50 expects each image to be 224x224 pixels with three channels. The input
    images from the COCO set have different sizes. Hence, we must convert the input
    files into the standard that ResNet was trained on:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The highlighted code shows a special pre-processing function provided by the
    ResNet50 package. The pixels in the input image are loaded into an array via the
    `decode_jpeg()` function. Each pixel has a value between 0 and 255 for each color
    channel. The `preprocess_input()` function normalizes the pixel values so that
    their mean is 0\. Since each input image has five captions, we should only process
    the unique images in the dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we must convert the dataset into a `tf.dat.Dataset`, which makes it easier
    to batch and process the input files using the convenience function defined previously:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For efficiently processing and generating features, we must process 16 image
    files at a time. The next step is loading a pre-trained ResNet50 model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The preceding output has been abbreviated for brevity. The model contains over
    23 million trainable parameters. We don't need the top classification layer as
    we are using the model for feature extraction. We defined a new model with the
    input and output layer. Here, we took the output from the last layer. We could
    take output from different parts of ResNet by changing the definition of the `hidden_layer`
    variable. In fact, this variable can be a list of layers, in which case the output
    of the `features_extract` model will be the output from each of the layers in
    the list.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, a directory must be set up to store the extracted features:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The feature extraction model can work on batches of images and predict the
    output. The output is 2,048 patches of 7x7 pixels for each image. If a batch of
    16 images is supplied, then the output from the model will be a tensor of dimensions
    [16, 7, 7, 2048]. We store the features of each image file as a separate file
    while flattening the dimensions to [49, 2048]. Each image has now been converted
    into a sequence of 49 pixels, with an embedding size of 2,048\. The following
    code performs this action:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This could be a time-consuming operation, depending on your computing environment.
    On my Ubuntu Linux box with an RTX 2070 GPU, this took ~23 minutes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step in data pre-processing is to train the Subword Encoder. This
    part should be quite familiar to you as it is identical to what we''ve done in
    previous chapters:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note that we included two special tokens to signal the start and end of the
    sequences. You may recall this technique from *Chapter 5*, *Generating Text with
    RNNs and GPT-2*. Here, we used a slightly different way of accomplishing the same
    technique to show how you can accomplish the same objective in different ways.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: With that, pre-processing and feature extraction is complete. The next step
    is defining the Transformer model. Then, we will be ready to train the model.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: The Transformer model
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Transformer model was discussed in *Chapter 4*, *Transfer Learning with
    BERT*. It was inspired by the seq2seq model and has an Encoder and a Decoder part.
    Since the Transformer model does not rely on RNNs, input sequences need to be
    annotated with positional encodings, which allow the model to learn about the
    relationships between inputs. Removing recurrence improves the speed of the model
    vastly while reducing the memory footprint. This innovation of the Transformer
    model has made very large-sized models such as BERT and GPT-3 possible. The Encoder
    part of the Transformer model was shown in the aforementioned chapter. The full
    Transformer model was shown in *Chapter 5*, *Generating Text with RNNs and GPT-2*.
    We will start with a modified version of the full Transformer. Specifically, we
    will modify the Encoder part of the Transformer to create a visual Encoder, which
    takes image data as input instead of text sequences. There are some other small
    modifications to be made to accommodate images as input to the Encoder. The Transformer
    model we are going to build is shown in the following diagram. The main difference
    here is how the input sequence is encoded. In the case of text, we will tokenize
    the text using a Subword Encoder and pass it through an Embedding layer, which
    is trainable.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'As training proceeds, the embeddings of the tokens are also learned. In the
    case of image captioning, we will pre-process the images into a sequence of 49
    pixels, each with an "embedding" size of 2,048\. This actually simplifies padding
    the inputs. All the images are pre-processed so that they''re the same length.
    Consequently, padding and masking the inputs is not required:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_07_12.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.12: Transformer model with a visual Encoder'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'The following pieces of code need to be implemented to build the Transformer
    model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding of the inputs, along with input and output masks. Our inputs
    are of a fixed length, but the output and captions are of a variable length.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaled dot-product attention and multi-head attention to enable the Encoders
    and Decoders to focus on specific aspects of the data.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Encoder that consists of multiple repeating blocks.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Decoder that uses the outputs from the Encoder through its repeating blocks.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for the Transformer has been taken from the TensorFlow tutorial titled
    *Transformer model for language understanding*. We will be using this code as
    the base and adapting it for the image captioning use case. One of the beautiful
    things about the Transformer architecture is that if we can cast a problem as
    a sequence-to-sequence problem, then we can apply the Transformer model. As we
    describe the implementation, the main points of the code will be highlighted.
    Note that the code for this section is in the `visual_transformer.py` file.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the full Transformer model does take a little bit of code. If you
    are already familiar with the Transformer model or want to only know where our
    model differs from the standard Transformer model, please focus on the next section
    and the *VisualEncoder* section. You can read the rest of the sections at your
    leisure.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: Positional encoding and masks
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformer models don't use RNNs. This allows them to compute all the outputs
    in one step, leading to significant improvements in speed and also the ability
    to learn dependencies across long inputs. However, it comes at the cost of the
    model not knowing anything about the relationship between neighboring words or
    tokens. A positional encoding vector, with values for the odd and even positions
    of the tokens to help the model learn relationships between the positions of inputs,
    helps compensate for the lack of information about the ordering of tokens.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Embeddings help place tokens that are similar in meaning close to each other
    in the embedding space. Positional encodings put tokens closer to each other based
    on their position in the sentence. Put together, the two are quite powerful.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'In image captioning, this is important for captions. Technically, we don''t
    need to provide these positional encodings for the image inputs as ResNet50 should
    have produced appropriate patches. Positional encoding can, however, still be
    used for the inputs as well. Positional encoding uses a *sin* function for even
    positions and a *cos* function for odd positions. The formula for computing the
    encodings for a position is:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_07_001.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
- en: 'Here, *w*[i] is defined as:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_07_002.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: In the preceding formula, *pos* refers to the position of a given token, *d*[model]
    refers to the dimensions of the embeddings, and *i* is the specific dimension
    being computed. The positional encoding process produces a vector with the same
    dimensions as the embedding for each token. You may be wondering why this complex
    formulation is used for computing these positional encodings. Wouldn't numbering
    the tokens from one side to the other suffice? It turns out that the positional
    encoding algorithm must have a few characteristics. First, the values must generalize
    easily to sequences of a variable length. Using a straight-up numbering scheme
    would prevent inputs that have sequences longer than those in the training data.
    The output should be unique for each token's position. Furthermore, the distance
    between any two positions should be consistent across different lengths of input
    sequences. This formulation is relatively simple to implement. The code for this
    is in the Positional Encoder section of the file.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must compute the *angle*, as shown in the preceding *w*[i] formula,
    like so:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we must compute the vector of positional encodings:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next step is to compute the masks for input and output. Let''s focus on
    the Decoder for a second. Since we are not using an RNN, the entire output is
    fed to the Decoder at once. However, we don''t want the Decoder to look at data
    from future timesteps. So, the outputs must be masked. In terms of the Encoder,
    masks are needed if the input is padded to a fixed length. However, in our case,
    the inputs are always exactly a length of 49\. So, the mask is a fixed vector
    of ones:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The first method is used to mask inputs if they are padded. This method has
    been included for the sake of completeness, but you will see later that we pass
    it a sequence of ones. So, all this method does is reshape the masks. The second
    mask function is used for masking Decoder inputs so that it can only see the positions
    it has generated.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: The layers of the transfer Encoder and Decoder use a specific form of attention.
    This is a fundamental building block of the architecture and will be implemented
    next.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Scaled dot-product and multi-head attention
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The purpose of the attention function is to match a query to a set of key-value
    pairs. The output is a sum of the values, weighted by the correspondence between
    the query and the key. multi-head attention learns multiple ways to compute the
    scaled dot-product attention and combines it.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Scaled dot-product attention is computed by multiplying the query vector by
    the key vector. This product is scaled by the square root of the dimensions of
    the query and key. Note that this formulation assumes that the key and query vectors
    have the same dimensions. Practically, the dimensions of the query, key, and value
    vectors are all set to the size of the embedding.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'This was referred to as *d*[model] in the position encoding. After computing
    the scaled product of the key and query vector, a softmax is applied, and the
    result of the softmax is multiplied by the value vector. A mask is used to mask
    the product of the query and keys:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Multi-ahead attention concatenates outputs from multiple scaled dot-product
    attention units and passes them through a linear layer. The dimensions of the
    embedding inputs are divided by the number of heads to compute the dimensions
    of the key and value vectors. Multi-head attention is implemented as a custom
    layer. First, we must create the constructor:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note the `assert` statement that is highlighted. When the Transformer model
    is instantiated, it is vital to choose some parameters so that the number of heads
    divides the model size or embedding dimensions completely. The main computation
    of this layer is in the `call()` function:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The three highlighted rows show splitting the vectors into multiple heads.
    `split_heads()` is defined like so:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This completes the multi-head attention implementation. This is the key part
    of the Transformer model. There is a small detail surrounding a Dense layer, which
    is used to aggregate the outputs from multi-head attention. It is quite simple:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Thus far, we have looked at the following parameters for specifying a Transformer
    mode:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '*d*[model] is used for the size of the embeddings and primary flow of inputs'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*d*[ff] is the size of the output from the intermediate Dense layer in the
    FeedForward part'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h* specifies the number of heads for multi-head attention'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will implement a visual Encoder, which has been modified to accommodate
    images as input.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: VisualEncoder
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The diagram shown in the *The Transformer model* section shows the Encoder''s
    structure. The Encoder processes the inputs with positional encodings and masks,
    and then passes them through stacks of multi-head attention and feed-forward blocks.
    The implementation deviates from the TensorFlow tutorial as the input in the tutorial
    is text. In our case, we are passing 49x2,048 vectors that were generated by passing
    images through ResNet50\. The main difference is in how the inputs are handled.
    `VisualEncoder` is built as a layer to allow composition into the eventual Transform
    model:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The constructor is shown next. A new parameter that states the number of layers
    is introduced. The original paper used 6 layers, 512 as *d*[model], 8 multi-attention
    heads, and 2,048 as the size of the intermediate feed-forward output. Note the
    highlighted lines in the preceding code. The dimensions of the pre-processed images
    can vary depending on the layer of ResNet50 from which output is pulled. We pass
    the input through a dense layer, `fc`, to the size inputs according to the model.
    This allows us to experiment with different models to pre-process images such
    as VGG19 or Inception without changing the architecture. Also, note that the maximum
    position encoding is hardcoded to 49, since that is the dimension of the output
    of the ResNet50 model. Lastly, we add a flag that can switch positional encoding
    on or off in the Visual Encoder. You should experiment with training models with
    and without positional encodings in the input to see if this helps or hinders
    learning.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '`VisualEncoder` is composed of multiple multi-head attention and feed-forward
    blocks. We can utilize a convenience class, `EncoderLayer`, to define one such
    block. A stack of these blocks is created based on the input parameters. We will
    examine the internals of `EncoderLayer` momentarily. First, let''s see how inputs
    pass through `VisualEncoder`. The `call()` function is used to produce the outputs
    for the given inputs:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This code is fairly simple due to the abstractions defined previously. Note
    the use of the training flag to turn dropout on or off. Now, let''s see how `EncoderLayer`
    is defined. Each Encoder building is composed of two sub-blocks. The first sub-block
    passes inputs through multi-head attention, while the second sub-block passes
    the output of the first sub-block through the 2-layer feed-forward layer:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Each layer first computes the output from multi-head attention and passes it
    through dropout. A residual connection passes the sum of the output and input
    through LayerNorm. The second part of this block passes the output of the first
    LayerNorm through the feed-forward layer and another dropout layer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: Again, a residual connection combines the output and input to the feed-forward
    part before passing it through LayerNorm. Note the use of dropout and residual
    connections, which were developed for CV in the Transformer architecture.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '**Layer normalization or LayerNorm**'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: LayerNorm was proposed in 2016 in a paper by the same name as an alternative
    to BatchNorm for RNNs. BatchNorm, as described in the *CNNs* section, normalizes
    the outputs across the entire batch. But sequences can be of variable length in
    the case of RNNs. A different formulation is required for normalization that can
    handle variable sequence lengths. LayerNorm normalizes across all the hidden units
    in a given layer. It is independent of the batch size, and the normalization is
    the same for all the units in a given layer. LayerNorm results in a significant
    speedup of training and convergence of seq2seq style models.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: With `VisualEncoder` in place, we are ready to implement the Decoder before
    we put this all together into the full Transformer.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Decoder
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Decoder is also composed of blocks, just like the Encoder. Each block of
    the Decoder, however, contains three sub-blocks, as shown in the diagram in the
    *The Transformer model* section. There is a masked multi-head attention sub-block,
    followed by a multi-head attention block, and finally a feed-forward sub-block.
    The feed-forward sub-block is identical to the Encoder sub-block. We must define
    a Decoder layer that can be stacked to construct the Decoder. The constructor
    for this is shown here:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Three sub-blocks should be quite evident based on the preceding variables.
    Input passes through this layer and is converted into output, as defined by the
    computations in the `call()` function:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The first sub-block, also referred to as the masked multi-head attention block,
    uses the output tokens, masked to the current position being generated. The outputs,
    in our case, are the tokens that make up the caption. The look-ahead mask masks
    tokens that haven't been generated yet.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Note that this sub-block does not use the output of the Encoder. It is trying
    to predict the relationship of the next token to the previous token that was generated.
    The second sub-block uses the output of the Encoder, along with the output of
    the previous sub-block, to generate the outputs. Finally, the feed-forward network
    generates the final output by operating on the output of the second sub-block.
    Both the multi-head attention sub-blocks have their own attention weights.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'We define the Decoder as a custom layer that is composed of multiple `DecoderLayer`
    blocks. The structure of the Transformer is symmetrical. The number of Encoder
    and Decoder blocks is the same. The constructor is defined first:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output of the Decoder is computed by the `call()` function:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Whew, that was a fair amount of code. The structure of the Transformer model
    is so elegant. The beauty of the model allows us to stack more Encoder and Decoder
    layers to create more powerful models, as demonstrated by GPT-3 recently. Let's
    put the Encoder and Decoder together to create a full Transformer.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: Transformer
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Transformer is composed of the Encoder, the Decoder, and the final Dense
    layer for generating output token distributions across the subword vocabulary:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: That was a whirlwind tour of the full Transformer code. Ideally, Keras in TensorFlow
    will provide a higher-level API for defining a Transformer model without you having
    to write the code out. If this was too much to absorb, then focus on the masks
    and VisualEncoder as they are the only deviations from the standard Transformer
    architecture.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: We are now ready to train the model. We'll take a very similar approach to the
    one we adopted in the previous chapter, by setting up learning rate annealing
    and checkpointing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: Training the Transformer model with VisualEncoder
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training the Transformer model can take hours as we want to train for around
    20 epochs. It is best to put the training code into a file so that it can be run
    from the command line. Note that the model will be able to show some results even
    after 4 epochs of training. The training code is in the `caption-training.py`
    file. At a high level, the following steps need to be performed before starting
    training. First, the CSV file with captions and image names is loaded in, and
    the corresponding paths for the files with extracted image features are appended.
    The Subword Encoder is also loaded in. A `tf.data.Dataset` is created with the
    encoded captions and image features for easy batching and feeding them into the
    model for training. A loss function, an optimizer with a learning rate schedule,
    is created for use in training. A custom training loop is used to train the Transformer
    model. Let's go over these steps in detail.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Loading training data
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following code loads the CSV file we generated in the pre-processing step:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The captions in the data are tokenized using the Subword Encoder we generated
    and persisted to disk earlier:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The maximum length of the captions is generated to accommodate 99% of the caption
    lengths. All the captions are truncated or padded to this maximum length:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Image features are persisted to disk. When training begins, those features
    need to be read from the disk and fed in, along with the encoded captions. The
    name of the file containing the image features is then added to the dataset:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'A `tf.data.Dataset` is created and a map function that reads image features
    while enumerating batches is set up:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Now that the dataset has been prepared, we are ready to instantiate the Transformer
    model.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Instantiating the Transformer model
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will instantiate a small model in terms of the number of layers, attention
    heads, embedding dimensions, and feed-forward units:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'For comparison, the BERT base model contains the following parameters:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'These settings are available in the file but commented out. Using these settings
    slows down training and requires a large amount of GPU memory. A couple of other
    parameters need to be set up and the Transformer instantiated:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This model contains over 4 million trainable parameters. It is a smaller model
    than we have seen previously:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: However, the model summary is not available since the input dimensions have
    not yet been supplied. The summary will be available once we've run a training
    example through the model.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: A custom learning rate schedule is created for training the model. A custom
    learning rate schedule anneals or reduces the learning rate as the model improves
    its accuracy, resulting in better accuracy. This process is called learning rate
    decay or learning rate annealing and was discussed in detail in *Chapter 5*, *Generating
    Text with RNNs and GPT-2*.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Custom learning rate schedule
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This rate schedule is identical to the one proposed in the *Attention Is All
    You Need* paper:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The following graph shows the learning schedule:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '![A close up of a person  Description automatically generated](img/B16252_07_13.png)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.13: Custom learning rate schedule'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: When training starts, a higher learning rate is used as the loss is high. As
    the model learns more and more, the loss starts decreasing, which requires a lower
    learning rate. Using the preceding learning rate schedule significantly speeds
    up training and convergence. We also need a loss function to optimize.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Loss and metrics
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The loss function is based on categorical cross-entropy. It is a common loss
    function that we have used in previous chapters. In addition to the loss, an accuracy
    metric is also defined to track how the model is doing on the training set:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: This formulation has been used in previous chapters as well. We are almost ready
    to start training. There are two more steps we must follow before we get into
    the custom training function. We need to set up checkpoints to save progress in
    case of failures, and we also need to mask inputs for the Encoder and Decoder.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: Checkpoints and masks
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We need to specify a checkpoint directory for TensorFlow to save progress.
    We will use a `CheckpointManager` here, which automatically manages the checkpoints
    and stores a limited number of them. A checkpoint can be quite large. Five checkpoints
    for the small model would take up approximately 243 MB of space. Larger models
    would take up more space:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Next, a method that will create masks for the input images and captions must
    be defined:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Inputs are always a constant length, so the input sequence is set as ones. Only
    the captions, which are used by the Decoder, are masked. There are two types of
    masks for the Decoder. The first mask is the padding mask. Since the captions
    are set to the maximum length to handle 99% of the captions, which works out at
    about 22 tokens, any captions that are smaller than this number of tokens have
    padding appended to the end of them. The padding mask helps separate caption tokens
    from padding tokens. The second mask is the look-ahead mask. It prevents the Decoder
    from seeing tokens from the future or tokens it has not generated yet. Now, we
    are ready to train the model.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Custom training
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Similar to the summarization model, teacher forcing will be used for training.
    Consequently, a custom training function will be used. First, we must define a function
    that will train on one batch of data:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This method is very similar to the summarization training code. All we need
    to do now is define the number of epochs and batch size and start training:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Training can be started from the command line:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This training may take some time. An epoch of training takes about 11 minutes
    on my GPU-enabled machine. If you contrast this to the summarization model, this
    model is training extremely fast. Compared to the summarization model, which contains
    13 million parameters, it is much smaller and trains very fast. This speed boost
    is due to the lack of recurrence.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: The state-of-the-art summarization models use the Transformer architecture along
    with subword encoding. Given that you have all the pieces of the Transformer,
    a good exercise to test your understanding would be editing the VisualEncoder
    to process text and rebuild the summarization model as a Transformer. You will
    then be able to experience these speedup and accuracy improvements.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: A longer training time allows the model to learn better. However, this model
    can give reasonable results in as few as 5-10 epochs of training. Once training
    is complete, we can try the model on some images.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: Generating captions
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, you need to be congratulated! You made it through a whirlwind implementation
    of the Transformer. I am sure you must have noticed a number of common building
    blocks that were used in previous chapters. Since the Transformer model is complex,
    we left it for this chapter to look at other techniques like Bahdanau attention,
    custom layers, custom rate schedules, custom training using teacher forcing, and
    checkpointing so that we could cover a lot of ground quickly in this chapter.
    You should consider all these building blocks an important part of your toolkit
    when you try and solve an NLP problem.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Without further ado, let's try and caption some images. Again, we will use a
    Jupyter notebook for inference so that we can quickly try out different images.
    All the code for inference is in the `image-captioning-inference.ipynb` file.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: The inference code needs to load the Subword Encoder, set up masking, instantiate
    a ResNet50 model to extract features from test images, and generate captions a
    token at a time until the end of the sequence or a maximum sequence length is
    reached. Let's go over these steps one at a time.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we''ve done the appropriate imports and optionally initialized the GPU,
    we can load the Subword Encoder that was saved when we pre-processed the data:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We must now instantiate the Transformer model. This is an important step to
    ensure the parameters are the same as the checkpoint ones:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Restoring the model from the checkpoint requires the optimizer, even though
    we are not training the model. So, we will reuse the custom scheduler from the
    training code. As this code was provided previously, it has been omitted here.
    For the checkpoint, I used a model that was trained for 40 epochs, but without
    positional encoding in the Encoder:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Finally, we must set up the masking function for the generated captions. Note
    that the look ahead masks don''t really help during inference as future tokens
    have not been generated yet:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The main code for inference is in an `evaluate()` function. This method takes
    in the image features generated by ResNet50 as input and seeds the output caption
    sequence with the start token. Then, it runs in a loop to generate a token at
    a time while updating the masks, until an end of sequence token is encountered
    or the maximum length of the caption is reached:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'A wrapper method is used to call the evaluation method and print out the caption:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The only thing remaining now is instantiating a ResNet50 model to extract features
    from image files on the fly:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'It''s the moment of truth, finally! Let''s try out the model on an image. We
    will load the image, pre-process it for ResNet50, and extract the features from
    it:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The following is the example image and its caption:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '![A person riding a wave on a surfboard in the ocean  Description automatically
    generated](img/B16252_07_14.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.14: Generated caption - A man is riding a surfboard on a wave'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: This looks like an amazing caption for the given image! However, the overall
    accuracy of the model is in the low 30s. There is a lot of scope for improvement
    in the model. The next section talks about the state-of-the-art techniques for
    image captioning and also proposes some simpler ideas that you can try and play around
    with.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: Note that you may see slightly different results. The reviewer for this book
    got the result *A man in a black shirt is riding a surfboard* while running this
    code. This is expected as slight differences in the probabilities and the exact
    place where the model stops training in the loss surface is not exact. We are
    operating in the probabilistic realm here, so there may be slight differences.
    You may have experienced similar differences in the text generation and summarization
    code in the previous chapters as well.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows some more examples of images and their captions.
    The notebook contains several good, as well as some atrocious, examples of the
    generated labels:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing photo, different, various, group  Description automatically
    generated](img/B16252_07_15.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.15: Examples of images and their generated captions'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: None of these images were in the training set. The caption quality goes down
    from top to bottom. Our model understands close up, cake, groups of people, sandy
    beaches, streets, and luggage, among other things. However, the bottom two examples
    are concerning. They hint at some **bias** in the model. In both of the bottom
    two images, the model is misinterpreting gender.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: The images were deliberately chosen to show a woman in a business suit and women
    playing basketball. In both cases, the model proposes men in the captions. When
    the model was tried with a female tennis player's image, it guessed the right
    gender, but it changed genders in an image from a women's soccer game. Bias in
    models is a very important concern. In cases such as image captioning, this bias
    is immediately apparent. In fact, over 600,000 images were removed from the ImageNet
    database ([https://bit.ly/3qk4FgN](https://bit.ly/3qk4FgN)) in 2019 after bias
    was found in how it classifies and tags people in its pictures. ResNet50 is pre-trained
    on ImageNet. However, in other models, the bias may be harder to detect. Building
    fair deep learning models and reducing bias in models are active areas of research
    in the ML community.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: You may have noticed that we skipped running the model on an evaluation set
    and on the test set. This was done for brevity, and also because those techniques
    were covered previously.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: 'A quick note on metrics for evaluating the quality of captions. We saw ROUGE
    metrics in the previous chapters. ROUGE-L is still applicable in the case of image
    captioning. You can use a mental model of the caption as a summary of an image,
    as opposed to the summary of a paragraph in text summarization. There can be more than
    one way to express the summary, and ROUGE-L tries to capture the intent. There
    are two other commonly reported metrics:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: '**BLEU**: This stands for **Bilingual Evaluation Understudy** and is the most
    popular metric in machine translation. We can cast the image captioning problem
    as a machine translation problem as well. It relies on n-grams for computing the
    overlap of the predicted text with a number of reference texts and combines the
    results into one score.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CIDEr**: This stands for **Consensus-Based Image Description Evaluation**
    and was proposed in a paper by the same name in 2015\. It tries to deal with the
    difficulty of automatic evaluation when multiple captions could be reasonable
    by combining TF-IDF and n-grams. The metric tries to compare the captions generated
    by the model against multiple captions by human annotators and tries to score
    them based on consensus.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before wrapping up this chapter, let's spend a little time discussing ways to
    improve performance and state-of-the-art models.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance and state-of-the-art models
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's first talk through some simple experiments you can try to improve performance
    before talking about the latest models. Recall our discussion on positional encodings
    for inputs in the Encoder. Adding or removing positional encodings helps or hinders
    performance. In the previous chapter, we implemented the beam search algorithm
    for generating summaries. You can adapt the beam search code and see an improvement
    in the results with beam search. Another avenue of exploration is the ResNet50\.
    We used a pre-trained network and did not fine-tune it further. It is possible
    to build an architecture where ResNet is part of the architecture and not a pre-processing
    step. Image files are loaded in, and features are extracted from ResNet50 as part
    of the VisualEncoder. ResNet50 layers can be trained from the get-go, or only
    in the last few iterations. This idea is implemented in the `resnet-finetuning.py`
    file for you to try. Another line of thinking is using a different object detection
    model than ResNet50 or using the output from a different layer. You can try a
    more complex version of ResNet like ResNet152, or a different object detection
    model like Detectron from Facebook or other models. It should be quite easy to
    use a different model in our code as it is quite modular.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
- en: When you use a different model for extracting image features, the key will be
    to make sure tensor dimensions are flowing properly through the Encoder. The Decoder
    should not require any changes. Depending on the complexity of the model, you
    can either pre-process and store the image features or compute them on the fly.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Recall that we just used the pixels from the image directly. This was based
    on a paper published recently at CVPR titled *Pixel-BERT*. Most models use region
    proposals extracted from images instead of the pixels directly. Object detection
    in an image involves drawing a boundary around that object in the image. Another
    way to perform the same task is to classify each pixel into an object or background.
    These region proposals can be in the form of bounding boxes in an image. State-of-the-art
    models use bounding boxes or region proposals as input.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: The second-biggest gain in image captioning comes from pre-training. Recall
    that BERT and GPT are pre-trained on specific pre-training objectives. Models
    differ based on whether the Encoder is pre-trained or both the Encoder and Decoder
    are pre-trained. A common pre-training objective is a version of the BERT MLM
    task. Recall that BERT inputs are structured as `[CLS] I1 I2 … In [SEP] J1 J2
    … Jk [SEP]`, where some of the tokens from the input sequence are masked. This
    is adapted for image captioning, where the image features and caption tokens in
    the input are concatenated. Caption tokens are masked similar to how they are
    in the BERT model, and the pre-training objective is for the model to predict
    the masked token. After pre-training, the output of the CLS token can be used
    for classification or fed to the Decoder to generate the caption. Care must be
    exercised to not pre-train on the same dataset, like that for evaluation. An example
    of the setup could be using the Visual Genome and Flickr30k datasets for pre-training
    and COCO for fine-tuning.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Image captioning is an active area of research. The research is just getting
    started on multi-modal networks in general. Now, let's recap everything we've
    learned in this chapter.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the world of deep learning, specific architectures have been developed to
    handle specific modalities. **Convolutional Neural Networks** (**CNNs**) have
    been incredibly effective in processing images and is the standard architecture
    for CV tasks. However, the world of research is moving toward the world of multi-modal
    networks, which can take multiple types of inputs, like sounds, images, text,
    and so on and perform cognition like humans. After reviewing multi-modal networks,
    we dived into vision and language tasks as a specific focus. There are a number
    of problems in this particular area, including image captioning, visual question
    answering, VCR, and text-to-image, among others.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Building on our learnings from previous chapters on seq2seq architectures, custom
    TensorFlow layers and models, custom learning schedules, and custom training loops,
    we implemented a Transformer model from scratch. Transformers are state of the
    art at the time of writing. We took a quick look at the basic concepts of CNNs
    to help with the image side of things. We were able to build a model that may
    not be able to generate a thousand words for a picture but is definitely able
    to generate a human-readable caption. Its performance still needs improvement,
    and we discussed a number of possibilities so that we can try to do so, including
    the latest techniques.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: It is apparent that deep models perform very well when they contain a lot of
    data. The BERT and GPT models have shown the value of pre-training on massive
    amounts of data. It is still very hard to get good quality labeled data for use
    in pre-training or fine-tuning. In the world of NLP, we have a lot of text data,
    but not enough labeled data. The next chapter focuses on weak supervision to build
    classification models that can label data for pre-training or even fine-tuning
    tasks.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
