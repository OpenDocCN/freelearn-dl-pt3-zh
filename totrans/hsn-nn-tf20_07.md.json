["```\ndataset = tf.data.Dataset.from_tensor_slices({\n    \"a\": tf.random.uniform([4]),\n    \"b\": tf.random.uniform([4, 100], maxval=100, dtype=tf.int32)\n})\nfor value in dataset:\n    # Do something with the dict value\n    print(value[\"a\"])\n```", "```\ndef noise():\n    while True:\n        yield tf.random.uniform((100,))\n\ndataset = tf.data.Dataset.from_generator(noise, (tf.float32))\n```", "```\nbuffer_size = 10\nbatch_size = 32\ndataset = dataset.map(lambda x: x + 10).shuffle(buffer_size).batch(batch_size)\n```", "```\n{\n    \"height\": image.height,\n    \"width\": image.widht,\n    \"depth\": image.depth,\n    \"label\": label,\n    \"image\": image.bytes()\n}\n```", "```\nimport tensorflow as tf \nfrom tensorflow.keras.datasets import fashion_mnist \n\ndef train_dataset(batch_size=32, num_epochs=1): \n    (train_x, train_y), (test_x, test_y) = fashion_mnist.load_data()\n    input_x, input_y = train_x, train_y \n\n    def scale_fn(image, label): \n        return (tf.image.convert_image_dtype(image, tf.float32) - 0.5) * 2.0, label \n\n    dataset = tf.data.Dataset.from_tensor_slices( \n        (tf.expand_dims(input_x, -1), tf.expand_dims(input_y, -1)) \n    ).map(scale_fn) \n\n    dataset = dataset.cache().repeat(num_epochs)\n    dataset = dataset.shuffle(batch_size)\n\n    return dataset.batch(batch_size).prefetch(1)\n```", "```\ndef augment(image):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n    image = tf.image.random_brightness(image, max_delta=0.1)\n    return image\n```", "```\npip install tensorflow-datasets\n```", "```\nimport tensorflow_datasets as tfds\n\n# See available datasets\nprint(tfds.list_builders())\n# Construct 2 tf.data.Dataset objects\n# The training dataset and the test dataset\nds_train, ds_test = tfds.load(name=\"mnist\", split=[\"train\", \"test\"])\n```", "```\nbuilder = tfds.builder(\"mnist\")\nprint(builder.info)\n```", "```\ntfds.core.DatasetInfo(\n    name='mnist',\n    version=1.0.0,\n    description='The MNIST database of handwritten digits.',\n    urls=['http://yann.lecun.com/exdb/mnist/'],\n    features=FeaturesDict({\n        'image': Image(shape=(28, 28, 1), dtype=tf.uint8),\n        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10)\n    },\n    total_num_examples=70000,\n    splits={\n        'test': <tfds.core.SplitInfo num_examples=10000>,\n        'train': <tfds.core.SplitInfo num_examples=60000>\n    },\n    supervised_keys=('image', 'label'),\n    citation='\"\"\"\n        @article{lecun2010mnist,\n          title={MNIST handwritten digit database},\n          author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n          journal={ATT Labs [Online]. Available: http://yann. lecun. com/exdb/mnist},\n          volume={2},\n          year={2010}\n        }\n\n    \"\"\"',\n)\n```", "```\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(1e-5),\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy'])\n\nmodel.fit(train_dataset(num_epochs=10))\n```", "```\ndef train():\n    # Define the model\n    n_classes = 10\n    model = make_model(n_classes)\n\n    # Input data\n    dataset = train_dataset(num_epochs=10)\n\n    # Training parameters\n    loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n    step = tf.Variable(1, name=\"global_step\")\n    optimizer = tf.optimizers.Adam(1e-3)\n    accuracy = tf.metrics.Accuracy()\n\n    # Train step function\n    @tf.function\n    def train_step(inputs, labels):\n        with tf.GradientTape() as tape:\n            logits = model(inputs)\n            loss_value = loss(labels, logits)\n\n        gradients = tape.gradient(loss_value, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        step.assign_add(1)\n\n        accuracy_value = accuracy(labels, tf.argmax(logits, -1))\n        return loss_value, accuracy_value\n\n    @tf.function\n    def loop():\n        for features, labels in dataset:\n            loss_value, accuracy_value = train_step(features, labels)\n            if tf.equal(tf.math.mod(step, 10), 0):\n                tf.print(step, \": \", loss_value, \" - accuracy: \",\n                         accuracy_value)\n\n    loop()\n```", "```\n__init__(\n    model_fn,\n    model_dir=None,\n    config=None,\n    params=None,\n    warm_start_from=None\n)\n```", "```\nimport tensorflow as tf \nfrom tensorflow.keras.datasets import fashion_mnist \n\ndef get_input_fn(mode, batch_size=32, num_epochs=1): \n    (train_x, train_y), (test_x, test_y) = fashion_mnist.load_data() \n    half = test_x.shape[0] // 2 \n    if mode == tf.estimator.ModeKeys.TRAIN: \n        input_x, input_y = train_x, train_y \n        train = True \n    elif mode == tf.estimator.ModeKeys.EVAL: \n        input_x, input_y = test_x[:half], test_y[:half] \n        train = False \n    elif mode == tf.estimator.ModeKeys.PREDICT: \n        input_x, input_y = test_x[half:-1], test_y[half:-1] \n        train = False \n    else: \n        raise ValueError(\"tf.estimator.ModeKeys required!\") \n\n    def scale_fn(image, label): \n        return ( \n            (tf.image.convert_image_dtype(image, tf.float32) - 0.5) * 2.0, \n            tf.cast(label, tf.int32), \n        ) \n\n    def input_fn(): \n        dataset = tf.data.Dataset.from_tensor_slices( \n            (tf.expand_dims(input_x, -1), tf.expand_dims(input_y, -1)) \n        ).map(scale_fn) \n        if train: \n            dataset = dataset.shuffle(10).repeat(num_epochs) \n        dataset = dataset.batch(batch_size).prefetch(1) \n        return dataset \n\n    return input_fn\n```", "```\nmodel_fn(\n    features,\n    labels,\n    mode = None,\n    params = None,\n    config = None\n)\n```", "```\ndef model_fn(features, labels, mode): \n    v1 = tf.compat.v1 \n    model = make_model(10) \n    logits = model(features) \n\n    if mode == tf.estimator.ModeKeys.PREDICT: \n        # Extract the predictions \n        predictions = v1.argmax(logits, -1) \n        return tf.estimator.EstimatorSpec(mode, predictions=predictions) \n\n    loss = v1.reduce_mean( \n        v1.nn.sparse_softmax_cross_entropy_with_logits( \n            logits=logits, labels=v1.squeeze(labels) \n        ) \n    ) \n\n    global_step = v1.train.get_global_step() \n\n    # Compute evaluation metrics. \n    accuracy = v1.metrics.accuracy( \n        labels=labels, predictions=v1.argmax(logits, -1), name=\"accuracy\" \n    ) \n    # The metrics dictionary is used by the estimator during the evaluation \n    metrics = {\"accuracy\": accuracy} \n\n    if mode == tf.estimator.ModeKeys.EVAL: \n        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics) \n    if mode == tf.estimator.ModeKeys.TRAIN: \n        opt = v1.train.AdamOptimizer(1e-4) \n        train_op = opt.minimize( \n            loss, var_list=model.trainable_variables, global_step=global_step \n        ) \n\n        return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op) \n\n    raise NotImplementedError(f\"Unknown mode {mode}\")\n```", "```\nprint(\"Every log is on TensorBoard, please run TensorBoard --logidr log\") \nestimator = tf.estimator.Estimator(model_fn, model_dir=\"log\") \nfor epoch in range(50): \n    print(f\"Training for the {epoch}-th epoch\") \n    estimator.train(get_input_fn(tf.estimator.ModeKeys.TRAIN, num_epochs=1)) \n    print(\"Evaluating...\") \n    estimator.evaluate(get_input_fn(tf.estimator.ModeKeys.EVAL))\n```", "```\n# Define train & eval specs\ntrain_spec = tf.estimator.TrainSpec(input_fn=get_input_fn(tf.estimator.ModeKeys.TRAIN, num_epochs=50))\neval_spec = tf.estimator.EvalSpec(input_fn=get_input_fn(tf.estimator.ModeKeys.EVAL, num_epochs=1))\n\n# Get the Keras model\nmodel = make_model(10)\n# Compile it\nmodel.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Convert it to estimator\nestimator = tf.keras.estimator.model_to_estimator(\n  keras_model = model\n)\n\n# Train and evalution loop\ntf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n```", "```\ndata = tf.data.Dataset.range(100)\ndata2 = tf.data.Dataset.from_generator(lambda: range(100), (tf.int32))\n\ndef l1():\n    for v in data:\n        tf.print(v)\ndef l2():\n    for v in data2:\n        tf.print(v)\n```"]