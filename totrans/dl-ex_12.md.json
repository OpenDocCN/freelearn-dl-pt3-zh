["```\nfrom tensorflow.python.keras.models \nimport Sequential\nfrom tensorflow.python.keras.layers \nimport Dense, GRU, Embedding\nfrom tensorflow.python.keras.optimizers \nimport Adam\nfrom tensorflow.python.keras.preprocessing.text \nimport Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence \nimport pad_sequences\n```", "```\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, GRU, Embedding\nfrom tensorflow.python.keras.optimizers import Adam\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n```", "```\nimport imdb\nimdb.maybe_download_and_extract()\n\nOutput:\n- Download progress: 100.0%\nDownload finished. Extracting files.\nDone.\n```", "```\ninput_text_train, target_train = imdb.load_data(train=True)\ninput_text_test, target_test = imdb.load_data(train=False)\n```", "```\nprint(\"Size of the trainig set: \", len(input_text_train))\nprint(\"Size of the testing set:  \", len(input_text_test))\n\nOutput:\nSize of the trainig set: 25000\nSize of the testing set: 25000\n```", "```\n#combine dataset\ntext_data = input_text_train + input_text_test\ninput_text_train[1]\n\nOutput:\n'This is a really heart-warming family movie. It has absolutely brilliant animal training and \"acting\" (if you can call it like that) as well (just think about the dog in \"How the Grinch stole Christmas\"... it was plain bad training). The Paulie story is extremely well done, well reproduced and in general the characters are really elaborated too. Not more to say except that this is a GREAT MOVIE!<br /><br />My ratings: story 8.5/10, acting 7.5/10, animals+fx 8.5/10, cinematography 8/10.<br /><br />My overall rating: 8/10 - BIG FAMILY MOVIE AND VERY WORTH WATCHING!'\n\ntarget_train[1]\n\nOutput:\n1.0\n```", "```\nnum_top_words = 10000\ntokenizer_obj = Tokenizer(num_words=num_top_words)\n```", "```\ntokenizer_obj.fit_on_texts(text_data)\n```", "```\ntokenizer_obj.word_index\n\nOutput:\n{'britains': 33206,\n 'labcoats': 121364,\n 'steeled': 102939,\n 'geddon': 67551,\n \"rossilini's\": 91757,\n 'recreational': 27654,\n 'suffices': 43205,\n 'hallelujah': 30337,\n 'mallika': 30343,\n 'kilogram': 122493,\n 'elphic': 104809,\n 'feebly': 32818,\n 'unskillful': 91728,\n \"'mistress'\": 122218,\n \"yesterday's\": 25908,\n 'busco': 85664,\n 'goobacks': 85670,\n 'mcfeast': 71175,\n 'tamsin': 77763,\n \"petron's\": 72628,\n \"'lion\": 87485,\n 'sams': 58341,\n 'unbidden': 60042,\n \"principal's\": 44902,\n 'minutiae': 31453,\n 'smelled': 35009,\n 'history\\x97but': 75538,\n 'vehemently': 28626,\n 'leering': 14905,\n 'k√Ωnay': 107654,\n 'intendend': 101260,\n 'chomping': 21885,\n 'nietsze': 76308,\n 'browned': 83646,\n 'grosse': 17645,\n \"''gaslight''\": 74713,\n 'forseeing': 103637,\n 'asteroids': 30997,\n 'peevish': 49633,\n \"attic'\": 120936,\n 'genres': 4026,\n 'breckinridge': 17499,\n 'wrist': 13996,\n \"sopranos'\": 50345,\n 'embarasing': 92679,\n \"wednesday's\": 118413,\n 'cervi': 39092,\n 'felicity': 21570,\n \"''horror''\": 56254,\n 'alarms': 17764,\n \"'ol\": 29410,\n 'leper': 27793,\n 'once\\x85': 100641,\n 'iverson': 66834,\n 'triply': 117589,\n 'industries': 19176,\n 'brite': 16733,\n 'amateur': 2459,\n \"libby's\": 46942,\n 'eeeeevil': 120413,\n 'jbc33': 51111,\n 'wyoming': 12030,\n 'waned': 30059,\n 'uchida': 63203,\n 'uttter': 93299,\n 'irector': 123847,\n 'outriders': 95156,\n 'perd': 118465,\n.\n.\n.}\n```", "```\ntokenizer_obj.word_index['the']\n\nOutput:\n1\n```", "```\ntokenizer_obj.word_index['and']\n\nOutput:\n2\n```", "```\ntokenizer_obj.word_index['a']\n\nOutput:\n3\n```", "```\ntokenizer_obj.word_index['movie']\n\nOutput:\n17\n```", "```\ntokenizer_obj.word_index['film']\n\nOutput:\n19\n```", "```\ntokenizer_obj.word_index['romantic']\n\nOutput:\n743\n```", "```\ninput_text_train[1]\nOutput:\n'This is a really heart-warming family movie. It has absolutely brilliant animal training and \"acting\" (if you can call it like that) as well (just think about the dog in \"How the Grinch stole Christmas\"... it was plain bad training). The Paulie story is extremely well done, well reproduced and in general the characters are really elaborated too. Not more to say except that this is a GREAT MOVIE!<br /><br />My ratings: story 8.5/10, acting 7.5/10, animals+fx 8.5/10, cinematography 8/10.<br /><br />My overall rating: 8/10 - BIG FAMILY MOVIE AND VERY WORTH WATCHING!\n```", "```\nnp.array(input_train_tokens[1])\n\nOutput:\narray([ 11, 6, 3, 62, 488, 4679, 236, 17, 9, 45, 419,\n        513, 1717, 2425, 2, 113, 43, 22, 67, 654, 9, 37,\n         12, 14, 69, 39, 101, 42, 1, 826, 8, 85, 1,\n       6418, 3492, 1156, 9, 13, 1042, 74, 2425, 1, 6419, 64,\n          6, 568, 69, 221, 69, 2, 8, 825, 1, 102, 23,\n         62, 96, 21, 51, 5, 131, 556, 12, 11, 6, 3,\n         78, 17, 7, 7, 56, 2818, 64, 723, 447, 156, 113,\n        702, 447, 156, 1598, 3611, 723, 447, 156, 633, 723, 156,\n          7, 7, 56, 437, 670, 723, 156, 191, 236, 17, 2,\n         52, 278, 147])\n```", "```\ninput_test_tokens = tokenizer_obj.texts_to_sequences(input_text_test)\n```", "```\ntotal_num_tokens = [len(tokens) for tokens in input_train_tokens + input_test_tokens]\ntotal_num_tokens = np.array(total_num_tokens)\n\n#Get the average number of tokens\nnp.mean(total_num_tokens)\n\nOutput:\n221.27716\n```", "```\nnp.max(total_num_tokens)\n\nOutput:\n2208\n```", "```\nmax_num_tokens = np.mean(total_num_tokens) + 2 * np.std(total_num_tokens)\nmax_num_tokens = int(max_num_tokens)\nmax_num_tokens\n\nOutput:\n544\n```", "```\nnp.sum(total_num_tokens < max_num_tokens) / len(total_num_tokens)\n\nOutput:\n0.94532\n```", "```\nseq_pad = 'pre'\n\ninput_train_pad = pad_sequences(input_train_tokens, maxlen=max_num_tokens,\n padding=seq_pad, truncating=seq_pad)\n\ninput_test_pad = pad_sequences(input_test_tokens, maxlen=max_num_tokens,\n padding=seq_pad, truncating=seq_pad)\n```", "```\ninput_train_pad.shape\n\nOutput:\n(25000, 544)\n\ninput_test_pad.shape\n\nOutput:\n(25000, 544)\n```", "```\nnp.array(input_train_tokens[1])\n\nOutput:\narray([ 11, 6, 3, 62, 488, 4679, 236, 17, 9, 45, 419,\n        513, 1717, 2425, 2, 113, 43, 22, 67, 654, 9, 37,\n         12, 14, 69, 39, 101, 42, 1, 826, 8, 85, 1,\n       6418, 3492, 1156, 9, 13, 1042, 74, 2425, 1, 6419, 64,\n          6, 568, 69, 221, 69, 2, 8, 825, 1, 102, 23,\n         62, 96, 21, 51, 5, 131, 556, 12, 11, 6, 3,\n         78, 17, 7, 7, 56, 2818, 64, 723, 447, 156, 113,\n        702, 447, 156, 1598, 3611, 723, 447, 156, 633, 723, 156,\n          7, 7, 56, 437, 670, 723, 156, 191, 236, 17, 2,\n         52, 278, 147])\n```", "```\ninput_train_pad[1]\n\nOutput:\narray([ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 11, 6, 3, 62, 488, 4679, 236, 17, 9,\n         45, 419, 513, 1717, 2425, 2, 113, 43, 22, 67, 654,\n          9, 37, 12, 14, 69, 39, 101, 42, 1, 826, 8,\n         85, 1, 6418, 3492, 1156, 9, 13, 1042, 74, 2425, 1,\n       6419, 64, 6, 568, 69, 221, 69, 2, 8, 825, 1,\n        102, 23, 62, 96, 21, 51, 5, 131, 556, 12, 11,\n          6, 3, 78, 17, 7, 7, 56, 2818, 64, 723, 447,\n        156, 113, 702, 447, 156, 1598, 3611, 723, 447, 156, 633,\n        723, 156, 7, 7, 56, 437, 670, 723, 156, 191, 236,\n         17, 2, 52, 278, 147], dtype=int32)\n```", "```\nindex = tokenizer_obj.word_index\nindex_inverse_map = dict(zip(index.values(), index.keys()))\n```", "```\ndef convert_tokens_to_string(input_tokens):\n\n # Convert the tokens back to words\n input_words = [index_inverse_map[token] for token in input_tokens if token != 0]\n\n # join them all words.\n combined_text = \" \".join(input_words)\n\nreturn combined_text\n```", "```\ninput_text_train[1]\nOutput:\n\ninput_text_train[1]\n\n'This is a really heart-warming family movie. It has absolutely brilliant animal training and \"acting\" (if you can call it like that) as well (just think about the dog in \"How the Grinch stole Christmas\"... it was plain bad training). The Paulie story is extremely well done, well reproduced and in general the characters are really elaborated too. Not more to say except that this is a GREAT MOVIE!<br /><br />My ratings: story 8.5/10, acting 7.5/10, animals+fx 8.5/10, cinematography 8/10.<br /><br />My overall rating: 8/10 - BIG FAMILY MOVIE AND VERY WORTH WATCHING!'\n```", "```\nconvert_tokens_to_string(input_train_tokens[1])\n\n'this is a really heart warming family movie it has absolutely brilliant animal training and acting if you can call it like that as well just think about the dog in how the grinch stole christmas it was plain bad training the paulie story is extremely well done well and in general the characters are really too not more to say except that this is a great movie br br my ratings story 8 5 10 acting 7 5 10 animals fx 8 5 10 cinematography 8 10 br br my overall rating 8 10 big family movie and very worth watching'\n```", "```\nembedding_layer_size = 8\n\nrnn_type_model.add(Embedding(input_dim=num_top_words,\n                    output_dim=embedding_layer_size,\n                    input_length=max_num_tokens,\n                    name='embedding_layer'))\n```", "```\nrnn_type_model.add(GRU(units=16, return_sequences=True))\n```", "```\nrnn_type_model.add(GRU(units=8, return_sequences=True))\n```", "```\nrnn_type_model.add(GRU(units=4))\n```", "```\nrnn_type_model.add(Dense(1, activation='sigmoid'))\n```", "```\nmodel_optimizer = Adam(lr=1e-3)\n\nrnn_type_model.compile(loss='binary_crossentropy',\n              optimizer=model_optimizer,\n              metrics=['accuracy'])\n```", "```\nrnn_type_model.summary()\n\n_________________________________________________________________\nLayer (type) Output Shape Param # \n=================================================================\nembedding_layer (Embedding) (None, 544, 8) 80000 \n_________________________________________________________________\ngru_1 (GRU) (None, None, 16) 1200 \n_________________________________________________________________\ngru_2 (GRU) (None, None, 8) 600 \n_________________________________________________________________\ngru_3 (GRU) (None, 4) 156 \n_________________________________________________________________\ndense_1 (Dense) (None, 1) 5 \n=================================================================\nTotal params: 81,961\nTrainable params: 81,961\nNon-trainable params: 0\n_________________________\n```", "```\nOutput:\nrnn_type_model.fit(input_train_pad, target_train,\n          validation_split=0.05, epochs=3, batch_size=64)\n\nOutput:\nTrain on 23750 samples, validate on 1250 samples\nEpoch 1/3\n23750/23750 [==============================]23750/23750 [==============================] - 176s 7ms/step - loss: 0.6698 - acc: 0.5758 - val_loss: 0.5039 - val_acc: 0.7784\n\nEpoch 2/3\n23750/23750 [==============================]23750/23750 [==============================] - 175s 7ms/step - loss: 0.4631 - acc: 0.7834 - val_loss: 0.2571 - val_acc: 0.8960\n\nEpoch 3/3\n23750/23750 [==============================]23750/23750 [==============================] - 174s 7ms/step - loss: 0.3256 - acc: 0.8673 - val_loss: 0.3266 - val_acc: 0.8600\n```", "```\nmodel_result = rnn_type_model.evaluate(input_test_pad, target_test)\n\nOutput:\n25000/25000 [==============================]25000/25000 [==============================] - 60s 2ms/step\n\nprint(\"Accuracy: {0:.2%}\".format(model_result[1]))\nOutput:\nAccuracy: 85.26%\n```", "```\ntarget_predicted = rnn_type_model.predict(x=input_test_pad[0:1000])\ntarget_predicted = target_predicted.T[0]\n```", "```\nclass_predicted = np.array([1.0 if prob>0.5 else 0.0 for prob in target_predicted])\n```", "```\nclass_actual = np.array(target_test[0:1000])\n```", "```\nincorrect_samples = np.where(class_predicted != class_actual)\nincorrect_samples = incorrect_samples[0]\nlen(incorrect_samples)\n\nOutput:\n122\n```", "```\nindex = incorrect_samples[0]\nindex\n```", "```\nOutput:\n9\n\nincorrectly_predicted_text = input_text_test[index]\nincorrectly_predicted_text\n```", "```\nOutput:\n\n'I am not a big music video fan. I think music videos take away personal feelings about a particular song.. Any song. In other words, creative thinking goes out the window. Likewise, Personal feelings aside about MJ, toss aside. This was the best music video of alltime. Simply wonderful. It was a movie. Yes folks it was. Brilliant! You had awesome acting, awesome choreography, and awesome singing. This was spectacular. Simply a plot line of a beautiful young lady dating a man, but was he a man or something sinister. Vincent Price did his thing adding to the song and video. MJ was MJ, enough said about that. This song was to video, what Jaguars are for cars. Top of the line, PERFECTO. What was even better about this was, that we got the real MJ without the thousand facelifts. Though ironically enough, there was more than enough makeup and costumes to go around. Folks go to Youtube. Take 14 mins. out of your life and see for yourself what a wonderful work of art this particular video really is.'\n```", "```\ntarget_predicted[index]\n```", "```\nOutput:\n0.1529513\n\nclass_actual[index]\nOutput:\n1.0\n```", "```\ntest_sample_1 = \"This movie is fantastic! I really like it because it is so good!\"\ntest_sample_2 = \"Good movie!\"\ntest_sample_3 = \"Maybe I like this movie.\"\ntest_sample_4 = \"Meh ...\"\ntest_sample_5 = \"If I were a drunk teenager then this movie might be good.\"\ntest_sample_6 = \"Bad movie!\"\ntest_sample_7 = \"Not a good movie!\"\ntest_sample_8 = \"This movie really sucks! Can I get my money back please?\"\ntest_samples = [test_sample_1, test_sample_2, test_sample_3, test_sample_4, test_sample_5, test_sample_6, test_sample_7, test_sample_8]\n```", "```\ntest_samples_tokens = tokenizer_obj.texts_to_sequences(test_samples)\n```", "```\ntest_samples_tokens_pad = pad_sequences(test_samples_tokens, maxlen=max_num_tokens,\n                           padding=seq_pad, truncating=seq_pad)\ntest_samples_tokens_pad.shape\n\nOutput:\n(8, 544)\n```", "```\nrnn_type_model.predict(test_samples_tokens_pad)\n\nOutput:\narray([[0.9496784 ],\n [0.9552593 ],\n [0.9115685 ],\n [0.9464672 ],\n [0.87672734],\n [0.81883633],\n [0.33248223],\n [0.15345531 ]], dtype=float32)\n```"]