<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer137">
<h1 class="chapterNumber">2</h1>
<h1 class="chapterTitle" id="_idParaDest-52">Regression and Classification</h1>
<p class="normal">Regression and classification are two fundamental tasks ubiquitously present in almost all machine learning applications. They find application in varied fields ranging from engineering, physical science, biology, and the financial market, to the social sciences. They are the fundamental tools in the hands of statisticians and data scientists. In this chapter, we will cover the following topics:</p>
<ul>
<li class="bulletList">Regression</li>
<li class="bulletList">Classification</li>
<li class="bulletList">Difference between classification and regression</li>
<li class="bulletList">Linear regression</li>
<li class="bulletList">Different types of linear regression</li>
<li class="bulletList">Classification using the TensorFlow Keras API</li>
<li class="bulletList">Applying linear regression to estimate the price of a house</li>
<li class="bulletList">Applying logistic regression to identify handwritten digits</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp2"><span class="url">https://packt.link/dltfchp2</span></a></p>
</div>
<p class="normal">Let us first start with understanding what regression really is.</p>
<h1 class="heading-1" id="_idParaDest-53">What is regression?</h1>
<p class="normal">Regression is normally the first algorithm<a id="_idIndexMarker160"/> that people in machine learning work with. It allows us to make predictions from data by learning about the relationship between a given set of dependent and independent variables. It has its use in almost every field; anywhere that has an interest in drawing relationships between two or more things will find a use for regression.</p>
<p class="normal">Consider the case of house price<a id="_idIndexMarker161"/> estimation. There are many factors that can have an impact on the house price: the number of rooms, the floor area, the locality, the availability of amenities, the parking space, and so on. Regression analysis can help us in finding the mathematical relationship between these factors and the house price.</p>
<p class="normal">Let us imagine a simpler world where only the area of the house determines its price. Using regression, we could determine the relationship between the area of the house (<strong class="keyWord">independent variable</strong>: these are the variables that do not depend<a id="_idIndexMarker162"/> upon any other variables) and its price (<strong class="keyWord">dependent variable</strong>: these variables depend<a id="_idIndexMarker163"/> upon one or more independent variables). Later, we could use this relationship to predict the price of any house, given its area. To learn more about<a id="_idIndexMarker164"/> dependent and independent variables and how to identify them, you can refer to this post: <a href="https://medium.com/deeplearning-concepts-and-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db"><span class="url">https://medium.com/deeplearning-concepts-and-implementation/independent-and-dependent-variables-in-machine-learning-210b82f891db</span></a>. In machine learning, the independent variables are normally input into the model and the dependent variables are output from our model.</p>
<p class="normal">Depending upon the number of independent variables, the number of dependent variables, and the relationship type, we have many different types of regression. There are two important components of regression: the <em class="italic">relationship</em> between independent and dependent variables, and the <em class="italic">strength of impact</em> of different independent variables on dependent variables. In the following section, we will learn in detail about the widely used linear regression technique.</p>
<h1 class="heading-1" id="_idParaDest-54">Prediction using linear regression</h1>
<p class="normal"><strong class="keyWord">Linear regression</strong> is one of the most widely<a id="_idIndexMarker165"/> known modeling techniques. Existing for more than 200 years, it has been explored from almost all possible angles. Linear regression assumes a linear relationship between the input variable (<em class="italic">X</em>) and the output variable (<em class="italic">Y</em>). The basic idea of linear<a id="_idIndexMarker166"/> regression is building a model, using training data that can predict<a id="_idIndexMarker167"/> the output given the input, such that the predicted output <img alt="" height="42" src="../Images/B18331_02_001.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="25"/> is as near the observed training output <em class="italic">Y</em> for the input <em class="italic">X</em>. It involves finding a linear equation for the predicted value <img alt="" height="42" src="../Images/B18331_02_001.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="25"/> of the form:</p>
<p class="center"><img alt="" height="42" src="../Images/B18331_02_003.png" style="height: 1.05em !important; vertical-align: 0.26em !important;" width="233"/></p>
<p class="normal">where <img alt="" height="46" src="../Images/B18331_02_004.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="308"/> are the <em class="italic">n</em> input<a id="_idIndexMarker168"/> variables, and <img alt="" height="50" src="../Images/B18331_02_005.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="346"/> are the linear<a id="_idIndexMarker169"/> coefficients, with <em class="italic">b</em> as the bias term. We can also expand the preceding equation to:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_006.png" style="height: 3.33em !important;" width="275"/></p>
<p class="normal">The bias term allows our regression model to provide an output even in the absence of any input; it provides us with an option to shift our data for a better fit. The error between the observed values (<em class="italic">Y</em>) and predicted values (<img alt="" height="42" src="../Images/B18331_02_001.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="25"/>) for an input sample <em class="italic">i</em> is: </p>
<p class="center"><img alt="" height="50" src="../Images/B18331_02_008.png" style="height: 1.25em !important; vertical-align: 0.05em !important;" width="196"/></p>
<p class="normal">The goal is to find the best estimates for the coefficients <em class="italic">W</em> and bias <em class="italic">b</em>, such that the error between the observed values <em class="italic">Y</em> and the predicted values <img alt="" height="42" src="../Images/B18331_02_001.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="25"/> is minimized. Let’s go through some examples to better understand this.</p>
<h2 class="heading-2" id="_idParaDest-55">Simple linear regression</h2>
<p class="normal">If we consider only one independent variable<a id="_idIndexMarker170"/> and one dependent variable, what we<a id="_idIndexMarker171"/> get is a simple linear regression. Consider the case of house price prediction, defined in the preceding section; the area of the house (<em class="italic">A</em>) is the independent variable, and the price (<em class="italic">Y</em>) of the house is the dependent variable. We want to find a linear relationship between predicted price <img alt="" height="42" src="../Images/B18331_02_001.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="25"/> and <em class="italic">A</em>, of the form:</p>
<p class="center"><img alt="" height="42" src="../Images/B18331_02_011.png" style="height: 1.05em !important; vertical-align: 0.26em !important;" width="225"/></p>
<p class="normal">where <em class="italic">b</em> is the bias term. Thus, we need to determine <em class="italic">W</em> and <em class="italic">b</em>, such that the error between the price <em class="italic">Y</em> and the predicted price <img alt="" height="42" src="../Images/B18331_02_001.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="25"/> is minimized. The standard method used to estimate <em class="italic">W</em> and <em class="italic">b</em> is called the method of least squares, that is, we<a id="_idIndexMarker172"/> try to minimize the sum of the square of errors (<em class="italic">S</em>). For the preceding case, the expression becomes:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_013.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="771"/></p>
<p class="normal">We want to estimate the regression<a id="_idIndexMarker173"/> coefficients, <em class="italic">W</em> and <em class="italic">b</em>, such that <em class="italic">S</em> is minimized. We use<a id="_idIndexMarker174"/> the fact that the derivative of a function is 0 at its minima to get these two equations:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_014.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="592"/></p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_015.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="538"/></p>
<p class="normal">These two equations can be solved to find the two unknowns. To do so, we first expand the summation in the second equation: </p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_016.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="475"/></p>
<p class="normal">Take a look at the last term on the left-hand side; it just sums up a constant <em class="italic">N</em> time. Thus, we can rewrite it as:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_017.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="450"/></p>
<p class="normal">Reordering the terms, we get: </p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_018.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="388"/></p>
<p class="normal">The two terms on the right-hand side can be replaced by <img alt="" height="42" src="../Images/B18331_02_019.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/>, the average price (output), and <img alt="" height="42" src="../Images/B18331_02_020.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="29"/>, the average area (input), respectively, and thus we get: </p>
<p class="center"><img alt="" height="42" src="../Images/B18331_02_021.png" style="height: 1.05em !important; vertical-align: 0.26em !important;" width="213"/></p>
<p class="normal">In a similar fashion, we expand the partial differential equation of <em class="italic">S</em> with respect to weight <em class="italic">W</em>: </p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_022.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="458"/></p>
<p class="normal">Substitute<a id="_idIndexMarker175"/> the expression for the bias<a id="_idIndexMarker176"/> term <em class="italic">b</em>: </p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_023.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="608"/></p>
<p class="normal">Reordering the equation:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_02_024.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="667"/></p>
<p class="normal">Playing around with the mean definition, we can get from this the value of weight <em class="italic">W</em> as: </p>
<p class="center"><img alt="" height="108" src="../Images/B18331_02_025.png" style="height: 2.70em !important; vertical-align: 0.03em !important;" width="346"/></p>
<p class="normal">where <img alt="" height="42" src="../Images/B18331_02_019.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="25"/> and <img alt="" height="42" src="../Images/B18331_02_020.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="29"/> are the average price and area, respectively. Let us try this on some simple sample data:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">We import the necessary modules. It is a simple example, so we’ll be using only NumPy, pandas, and Matplotlib:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
</code></pre>
</li>
<li class="numberedList">Next, we generate random data with a linear relationship. To make it more realistic, we also add a random noise element. You can see the two variables (the cause, <code class="inlineCode">area</code>, and the effect, <code class="inlineCode">price</code>) follow a positive linear dependence:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Generate a random data</span>
np.random.seed(<span class="hljs-number">0</span>)
area = <span class="hljs-number">2.5</span> * np.random.randn(<span class="hljs-number">100</span>) + <span class="hljs-number">25</span>
price = <span class="hljs-number">25</span> * area + <span class="hljs-number">5</span> + np.random.randint(<span class="hljs-number">20</span>,<span class="hljs-number">50</span>, size = <span class="hljs-built_in">len</span>(area))
data = np.array([area, price])
data = pd.DataFrame(data = data.T, columns=[<span class="hljs-string">'area'</span>,<span class="hljs-string">'price'</span>])
plt.scatter(data[<span class="hljs-string">'area'</span>], data[<span class="hljs-string">'price'</span>])
plt.show()
</code></pre>
</li>
</ol>
<figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" height="450" src="../Images/B18331_02_01.png" width="702"/></figure>
<p class="packt_figref">Figure 2.1: Scatter plot between the area of the house and its price</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="3">Now, we calculate the two regression<a id="_idIndexMarker177"/> coefficients using the equations<a id="_idIndexMarker178"/> we defined. You can see the result is very much near the linear relationship we have simulated:
        <pre class="programlisting code"><code class="hljs-code">W = <span class="hljs-built_in">sum</span>(price*(area-np.mean(area))) / <span class="hljs-built_in">sum</span>((area-np.mean(area))**<span class="hljs-number">2</span>)
b = np.mean(price) - W*np.mean(area)
<span class="hljs-built_in">print</span>(<span class="hljs-string">"The regression coefficients are"</span>, W,b)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">-----------------------------------------------
The regression coefficients are 24.815544052284988 43.4989785533412
</code></pre>
</li>
<li class="numberedList">Let us now try predicting the new prices using the obtained weight and bias values:
        <pre class="programlisting code"><code class="hljs-code">y_pred = W * area + b
</code></pre>
</li>
<li class="numberedList">Next, we plot the predicted<a id="_idIndexMarker179"/> prices along with the actual price. You can<a id="_idIndexMarker180"/> see that predicted prices follow a linear relationship with the area:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(area, y_pred, color=<span class="hljs-string">'red'</span>,label=<span class="hljs-string">"Predicted Price"</span>)
plt.scatter(data[<span class="hljs-string">'area'</span>], data[<span class="hljs-string">'price'</span>], label=<span class="hljs-string">"Training Data"</span>)
plt.xlabel(<span class="hljs-string">"Area"</span>)
plt.ylabel(<span class="hljs-string">"Price"</span>)
plt.legend()
</code></pre>
<figure class="mediaobject"><img alt="A close up of a map  Description automatically generated" height="448" src="../Images/B18331_02_02.png" width="674"/></figure>
<p class="packt_figref">Figure 2.2: Predicted values vs the actual price</p>
</li>
</ol>
<p class="normal">From <em class="italic">Figure 2.2</em>, we can see that the predicted values follow the same trend as the actual house prices.</p>
<h2 class="heading-2" id="_idParaDest-56">Multiple linear regression</h2>
<p class="normal">The preceding example<a id="_idIndexMarker181"/> was simple, but that is rarely the case. In most<a id="_idIndexMarker182"/> problems, the dependent variables depend upon multiple independent variables. Multiple linear regression finds a linear relationship between the many independent input variables (<em class="italic">X</em>) and the dependent output variable (<em class="italic">Y</em>), such that they satisfy the predicted <em class="italic">Y</em> value of the form:</p>
<p class="center"><img alt="" height="42" src="../Images/B18331_02_003.png" style="height: 1.05em !important; vertical-align: 0.26em !important;" width="233"/></p>
<p class="normal">where <img alt="" height="50" src="../Images/B18331_02_029.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="308"/> are the <em class="italic">n</em> independent input variables, and <img alt="" height="50" src="../Images/B18331_02_005.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="346"/>are the linear coefficients, with <em class="italic">b</em> as the bias term.</p>
<p class="normal">As before, the linear coefficients <em class="italic">W</em><sub class="subscript">s</sub> are estimated using the method of least squares, that is, minimizing the sum of squared differences between predicted values (<img alt="" height="42" src="../Images/B18331_02_001.png" style="height: 1.05em !important; vertical-align: -0.02em !important;" width="25"/>) and observed values (<em class="italic">Y</em>). Thus, we try to minimize<a id="_idIndexMarker183"/> the loss function (also called squared error, and if we divide by <em class="italic">n</em>, it is the mean squared error):</p>
<p class="center"><img alt="" height="117" src="../Images/B18331_02_032.png" style="height: 2.92em !important;" width="333"/></p>
<p class="normal">where the sum is over all the training samples.</p>
<p class="normal">As you might have guessed, now, instead of two, we will have <em class="italic">n+1</em> equations, which we will need to simultaneously solve. An easier alternative will be to use the TensorFlow Keras API. We will learn shortly how to use the TensorFlow Keras API to perform the task of regression.</p>
<h2 class="heading-2" id="_idParaDest-57">Multivariate linear regression</h2>
<p class="normal">There can be cases where the independent<a id="_idIndexMarker184"/> variables affect more than one dependent <a id="_idIndexMarker185"/>variable. For example, consider the case where we want to predict a rocket’s speed and its carbon dioxide emission – these two will now be our dependent variables, and both will be affected by the sensors reading the fuel amount, engine type, rocket body, and so on. This is a case of multivariate linear regression. Mathematically, a multivariate regression model can be represented as: </p>
<p class="center"><img alt="" height="138" src="../Images/B18331_02_033.png" style="height: 3.45em !important;" width="367"/></p>
<p class="normal">where <img alt="" height="46" src="../Images/B18331_02_034.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="196"/> and <img alt="" height="50" src="../Images/B18331_02_035.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="208"/>. The term <img alt="" height="54" src="../Images/B18331_02_036.png" style="height: 1.35em !important; vertical-align: -0.39em !important;" width="42"/> represents the <em class="italic">j</em><sup class="superscript">th</sup> predicted output value corresponding to the <em class="italic">i</em><sup class="superscript">th</sup> input sample, <em class="italic">w</em> represents the regression coefficients, and <em class="italic">x</em><sub class="subscript">ik</sub> is the <em class="italic">k</em><sup class="superscript">th</sup> feature of the <em class="italic">i</em><sup class="superscript">th</sup> input sample. The number<a id="_idIndexMarker186"/> of equations needed to solve<a id="_idIndexMarker187"/> in this case will now be <em class="italic">n x m</em>. While we can solve these equations using matrices, the process will be computationally expensive as it will involve calculating the inverse and determinants. An easier way would be to use the gradient descent with the sum of least square error as the loss function and to use one of the many optimizers that the TensorFlow API includes.</p>
<p class="normal">In the next section, we will delve deeper into the TensorFlow Keras API, a versatile higher-level API to develop your model with ease.</p>
<h1 class="heading-1" id="_idParaDest-58">Neural networks for linear regression</h1>
<p class="normal">In the preceding sections, we used mathematical expressions<a id="_idIndexMarker188"/> for calculating the coefficients<a id="_idIndexMarker189"/> of a linear regression equation. In this section, we will see how we can use the neural networks to perform the task of regression and build a neural network model using the TensorFlow Keras API.</p>
<p class="normal">Before performing regression using neural networks, let us first review what a neural network is. Simply speaking, a neural network is a network of many artificial neurons. From <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, we know that the simplest neural network, the (simple) perceptron, can be mathematically represented as:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_02_037.png" style="height: 1.25em !important;" width="292"/></p>
<p class="normal">where <em class="italic">f</em> is the activation function. Consider, if we have <em class="italic">f</em> as a linear function, then the above expression is similar to the expression of linear regression that we learned in the previous section. In other words, we can say that a neural network, which is also called a function approximator, is a generalized<a id="_idIndexMarker190"/> regressor. Let us try to build a neural network simple regressor next using the TensorFlow Keras API.</p>
<h2 class="heading-2" id="_idParaDest-59">Simple linear regression using TensorFlow Keras</h2>
<p class="normal">In the first chapter, we learned<a id="_idIndexMarker191"/> about how to build a model<a id="_idIndexMarker192"/> in TensorFlow Keras. Here, we will use the same <code class="inlineCode">Sequential</code> API to build a single-layered perceptron (fully connected neural network) using the <code class="inlineCode">Dense</code> class. We will continue with the same problem, that is, predicting the price of a house given its area:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">We start with importing the packages we will need. Notice the addition of the <code class="inlineCode">Keras</code> module and the <code class="inlineCode">Dense</code> layer in importing packages:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> tensorflow.keras <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense
</code></pre>
</li>
<li class="numberedList">Next, we generate the data, as in the previous case:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Generate a random data</span>
np.random.seed(<span class="hljs-number">0</span>)
area = <span class="hljs-number">2.5</span> * np.random.randn(<span class="hljs-number">100</span>) + <span class="hljs-number">25</span>
price = <span class="hljs-number">25</span> * area + <span class="hljs-number">5</span> + np.random.randint(<span class="hljs-number">20</span>,<span class="hljs-number">50</span>, size = <span class="hljs-built_in">len</span>(area))
data = np.array([area, price])
data = pd.DataFrame(data = data.T, columns=[<span class="hljs-string">'area'</span>,<span class="hljs-string">'price'</span>])
plt.scatter(data[<span class="hljs-string">'</span><span class="hljs-string">area'</span>], data[<span class="hljs-string">'price'</span>])
plt.show()
</code></pre>
</li>
<li class="numberedList">The input to neural networks should be normalized; this is because input gets multiplied with weights, and if we have very large numbers, the result of multiplication will be large, and soon our metrics may cross infinity (the largest number your computer can handle):
        <pre class="programlisting code"><code class="hljs-code">data = (data - data.<span class="hljs-built_in">min</span>()) / (data.<span class="hljs-built_in">max</span>() - data.<span class="hljs-built_in">min</span>())  <span class="hljs-comment">#Normalize</span>
</code></pre>
</li>
<li class="numberedList">Let us now build the model; since it is a simple linear regressor, we use a <code class="inlineCode">Dense</code> layer with only one unit:
        <pre class="programlisting code"><code class="hljs-code">model = K.Sequential([
                      Dense(<span class="hljs-number">1</span>, input_shape = [<span class="hljs-number">1</span>,], activation=<span class="hljs-literal">None</span>)
])
model.summary()
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Model: "sequential"
____________________________________________________________
 Layer (type)           Output Shape              Param #   
============================================================
 dense (Dense)          (None, 1)                 2         
                                                            
============================================================
Total params: 2
Trainable params: 2
Non-trainable params: 0
____________________________________________________________
</code></pre>
</li>
<li class="numberedList">To train a model, we will need<a id="_idIndexMarker193"/> to define the loss function<a id="_idIndexMarker194"/> and optimizer. The loss function defines the quantity that our model tries to minimize, and the optimizer decides the minimization algorithm we are using. Additionally, we can also define metrics, which is the quantity we want to log as the model is trained. We define the loss function, <code class="inlineCode">optimizer</code> (see <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>), and metrics using the <code class="inlineCode">compile</code> function:
        <pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'mean_squared_error'</span>, optimizer=<span class="hljs-string">'sgd'</span>)
</code></pre>
</li>
<li class="numberedList">Now that model is defined, we just need to train it using the <code class="inlineCode">fit</code> function. Observe that we are using a <code class="inlineCode">batch_size</code> of 32 and splitting the data into training and validation datasets using the <code class="inlineCode">validation_spilt</code> argument of the <code class="inlineCode">fit</code> function:
        <pre class="programlisting code"><code class="hljs-code">model.fit(x=data[<span class="hljs-string">'area'</span>],y=data[<span class="hljs-string">'price'</span>], epochs=<span class="hljs-number">100</span>, batch_size=<span class="hljs-number">32</span>, verbose=<span class="hljs-number">1</span>, validation_split=<span class="hljs-number">0.2</span>)
</code></pre>
<pre class="programlisting con"><code class="hljs-con">model.fit(x=data['area'],y=data['price'], epochs=100, batch_size=32, verbose=1, validation_split=0.2)
Epoch 1/100
3/3 [==============================] - 0s 78ms/step - loss: 1.2643 - val_loss: 1.4828
Epoch 2/100
3/3 [==============================] - 0s 13ms/step - loss: 1.0987 - val_loss: 1.3029
Epoch 3/100
3/3 [==============================] - 0s 13ms/step - loss: 0.9576 - val_loss: 1.1494
Epoch 4/100
3/3 [==============================] - 0s 16ms/step - loss: 0.8376 - val_loss: 1.0156
Epoch 5/100
3/3 [==============================] - 0s 15ms/step - loss: 0.7339 - val_loss: 0.8971
Epoch 6/100
3/3 [==============================] - 0s 16ms/step - loss: 0.6444 - val_loss: 0.7989
Epoch 7/100
3/3 [==============================] - 0s 14ms/step - loss: 0.5689 - val_loss: 0.7082
.
.
.
Epoch 96/100
3/3 [==============================] - 0s 22ms/step - loss: 0.0827 - val_loss: 0.0755
Epoch 97/100
3/3 [==============================] - 0s 17ms/step - loss: 0.0824 - val_loss: 0.0750
Epoch 98/100
3/3 [==============================] - 0s 14ms/step - loss: 0.0821 - val_loss: 0.0747
Epoch 99/100
3/3 [==============================] - 0s 21ms/step - loss: 0.0818 - val_loss: 0.0740
Epoch 100/100
3/3 [==============================] - 0s 15ms/step - loss: 0.0815 - val_loss: 0.0740
&lt;keras.callbacks.History at 0x7f7228d6a790&gt;
</code></pre>
</li>
<li class="numberedList">Well, you have successfully<a id="_idIndexMarker195"/> trained a neural network<a id="_idIndexMarker196"/> to perform the task of linear regression. The mean squared error after training for 100 epochs is 0.0815 on training data and 0.074 on validation data. We can get the predicted value for a given input using the <code class="inlineCode">predict</code> function:
        <pre class="programlisting code"><code class="hljs-code">y_pred = model.predict(data[<span class="hljs-string">'area'</span>])
</code></pre>
</li>
<li class="numberedList">Next, we plot a graph of the predicted and the actual data:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(data[<span class="hljs-string">'area'</span>], y_pred, color=<span class="hljs-string">'red'</span>,label=<span class="hljs-string">"</span><span class="hljs-string">Predicted Price"</span>)
plt.scatter(data[<span class="hljs-string">'area'</span>], data[<span class="hljs-string">'price'</span>], label=<span class="hljs-string">"Training Data"</span>)
plt.xlabel(<span class="hljs-string">"Area"</span>)
plt.ylabel(<span class="hljs-string">"Price"</span>)
plt.legend()
</code></pre>
</li>
<li class="numberedList"><em class="italic">Figure 2.3</em> shows the plot between the predicted<a id="_idIndexMarker197"/> data and the actual<a id="_idIndexMarker198"/> data. You can see that, just like the linear regressor, we have got a nice linear fit:</li>
</ol>
<figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" height="362" src="../Images/B18331_02_03.png" width="532"/></figure>
<p class="packt_figref">Figure 2.3: Predicted price vs actual price </p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="10">In case you are interested in knowing the coefficients <code class="inlineCode">W</code> and <code class="inlineCode">b</code>, we can do it by printing the weights of the model using <code class="inlineCode">model.weights</code>:
        <pre class="programlisting con"><code class="hljs-con">[&lt;tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[-0.33806288]], dtype=float32)&gt;,
&lt;tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.68142694], dtype=float32)&gt;]
</code></pre>
</li>
</ol>
<p class="normal">We can see from the result above that our coefficients are <code class="inlineCode">W= 0.69</code> and bias <code class="inlineCode">b= 0.127</code>. Thus, using linear regression, we can<a id="_idIndexMarker199"/> find a linear relationship<a id="_idIndexMarker200"/> between the house price and its area. In the next section, we explore multiple and multivariate linear regression using the TensorFlow Keras API. </p>
<h2 class="heading-2" id="_idParaDest-60">Multiple and multivariate linear regression using the TensorFlow Keras API</h2>
<p class="normal">The example<a id="_idIndexMarker201"/> in the previous section<a id="_idIndexMarker202"/> had only one independent <a id="_idIndexMarker203"/>variable, the <em class="italic">area</em> of the house, and one<a id="_idIndexMarker204"/> dependent variable, the <em class="italic">price</em> of the house. However, problems in real life are not that simple; we may have more than one independent variable, and we may need to predict more than one dependent variable. As you must have realized from the discussion on multiple and multivariate regression, they involve solving multiple equations. We can make our tasks easier by using the Keras API for both tasks.</p>
<p class="normal">Additionally, we can have more<a id="_idIndexMarker205"/> than one neural network layer, that is, we can build a <strong class="keyWord">deep neural network</strong>. A deep neural network is like applying multiple function approximators:</p>
<p class="center"><img alt="" height="54" src="../Images/B18331_02_038.png" style="height: 1.35em !important; vertical-align: 0.04em !important;" width="475"/></p>
<p class="normal">with <img alt="" height="50" src="../Images/B18331_02_039.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/> being the function at layer <em class="italic">L</em>. From the expression above, we can see that if <em class="italic">f</em> was a linear function, adding multiple layers of a neural network was not useful; however, using a non-linear activation function (see <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, for more details) allows us to apply neural networks to the regression problems where dependent and independent variables are related in some non-linear fashion. In this section, we will use a deep neural network, built using TensorFlow Keras, to predict the fuel efficiency of a car, given its number of cylinders, displacement, acceleration, and so on. The data we use<a id="_idIndexMarker206"/> is available from the UCI ML repository (Blake, C., &amp; Merz, C. (1998), the UCI repository of machine learning databases (<a href="http://www.ics.uci.edu/~mlearn/MLRepository.xhtml"><span class="url">http://www.ics.uci.edu/</span><span class="url">~</span><span class="url">mlearn/MLRepository.xhtml</span></a>):</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">We start by importing the modules<a id="_idIndexMarker207"/> that we will need. In the previous<a id="_idIndexMarker208"/> example, we normalized<a id="_idIndexMarker209"/> our data using the DataFrame operations. In<a id="_idIndexMarker210"/> this example, we will make use of the Keras <code class="inlineCode">Normalization</code> layer. The <code class="inlineCode">Normalization</code> layer shifts the data to a zero mean and one standard deviation. Also, since we have more than one independent variable, we will use Seaborn to visualize the relationship between different variables:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> tensorflow.keras <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense, Normalization
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
</code></pre>
</li>
<li class="numberedList">Let us first download the data from the UCI ML repo.
        <pre class="programlisting code"><code class="hljs-code">url = <span class="hljs-string">'https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'</span>
column_names = [<span class="hljs-string">'mpg'</span>, <span class="hljs-string">'cylinders'</span>, <span class="hljs-string">'displacement'</span>, <span class="hljs-string">'</span><span class="hljs-string">horsepower'</span>, <span class="hljs-string">'weight'</span>, <span class="hljs-string">'acceleration'</span>, <span class="hljs-string">'model_year'</span>, <span class="hljs-string">'origin'</span>]
data = pd.read_csv(url, names=column_names, na_values=<span class="hljs-string">'?'</span>, comment=<span class="hljs-string">'\t'</span>, sep=<span class="hljs-string">' '</span>, skipinitialspace=<span class="hljs-literal">True</span>)
</code></pre>
</li>
<li class="numberedList">The data consists of eight features: mpg, cylinders, displacement, horsepower, weight, acceleration, model year, and origin. Though the origin of the vehicle can also affect the fuel efficiency “mpg” (<em class="italic">miles per gallon</em>), we use only seven features to predict the mpg value. Also, we drop any rows with NaN values:
        <pre class="programlisting code"><code class="hljs-code">data = data.drop(<span class="hljs-string">'origin'</span>, <span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(data.isna().<span class="hljs-built_in">sum</span>())
data = data.dropna()
</code></pre>
</li>
<li class="numberedList">We divide the dataset into training and test datasets. Here, we are keeping 80% of the 392 datapoints as training data and 20% as test dataset:
        <pre class="programlisting code"><code class="hljs-code">train_dataset = data.sample(frac=<span class="hljs-number">0.8</span>, random_state=<span class="hljs-number">0</span>)
test_dataset = data.drop(train_dataset.index)
</code></pre>
</li>
<li class="numberedList">Next, we use Seaborn’s <code class="inlineCode">pairplot</code> to visualize the relationship between the different variables:
        <pre class="programlisting code"><code class="hljs-code">sns.pairplot(train_dataset[[<span class="hljs-string">'</span><span class="hljs-string">mpg'</span>, <span class="hljs-string">'cylinders'</span>, <span class="hljs-string">'displacement'</span>,<span class="hljs-string">'horsepower'</span>, <span class="hljs-string">'weight'</span>, <span class="hljs-string">'acceleration'</span>, <span class="hljs-string">'model_year'</span>]], diag_kind=<span class="hljs-string">'kde'</span>)
</code></pre>
</li>
<li class="numberedList">We can see<a id="_idIndexMarker211"/> that mpg (fuel efficiency) has dependencies<a id="_idIndexMarker212"/> on all the other variables, and the<a id="_idIndexMarker213"/> dependency<a id="_idIndexMarker214"/> relationship is non-linear, as none of the curves are linear:</li>
</ol>
<figure class="mediaobject"><img alt="A picture containing text, electronics, display  Description automatically generated" height="810" src="../Images/B18331_02_04.png" width="810"/></figure>
<p class="packt_figref">Figure 2.4: Relationship among different variables of auto-mpg data</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="7">For<a id="_idIndexMarker215"/> convenience, we also separate<a id="_idIndexMarker216"/> the variables<a id="_idIndexMarker217"/> into input variables<a id="_idIndexMarker218"/> and the label that we want to predict:
        <pre class="programlisting code"><code class="hljs-code">train_features = train_dataset.copy()
test_features = test_dataset.copy() 
train_labels = train_features.pop(<span class="hljs-string">'mpg'</span>)
test_labels = test_features.pop(<span class="hljs-string">'mpg'</span>)
</code></pre>
</li>
<li class="numberedList">Now, we use the Normalization<a id="_idIndexMarker219"/> layer of Keras to normalize our data. Note that while<a id="_idIndexMarker220"/> we normalized our inputs<a id="_idIndexMarker221"/> to a value with<a id="_idIndexMarker222"/> mean 0 and standard deviation 1, the output prediction <code class="inlineCode">'mpg'</code> remains as it is:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">#Normalize</span>
data_normalizer = Normalization(axis=<span class="hljs-number">1</span>)
data_normalizer.adapt(np.array(train_features))
</code></pre>
</li>
<li class="numberedList">We build our model. The model has two hidden layers, with 64 and 32 neurons, respectively. For the hidden layers, we have used <strong class="keyWord">Rectified Linear Unit</strong> (<strong class="keyWord">ReLU</strong>) as our activation function; this should help in approximating the non-linear relation between fuel efficiency and the rest of the variables:
        <pre class="programlisting code"><code class="hljs-code">model = K.Sequential([
    data_normalizer,
    Dense(<span class="hljs-number">64</span>, activation=<span class="hljs-string">'relu'</span>),
    Dense(<span class="hljs-number">32</span>, activation=<span class="hljs-string">'relu'</span>),
    Dense(<span class="hljs-number">1</span>, activation=<span class="hljs-literal">None</span>)
])
model.summary()
</code></pre>
</li>
<li class="numberedList">Earlier, we used stochastic gradient as the optimizer; this time, we try the Adam optimizer (see <em class="chapterRef">Chapter 1</em>, <em class="italic">Neural Network Foundations with TF</em>, for more details). The loss function for the regression we chose is the mean squared error again:
        <pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'adam'</span>, loss=<span class="hljs-string">'mean_squared_error'</span>)
</code></pre>
</li>
<li class="numberedList">Next, we train the model for 100 epochs:
        <pre class="programlisting code"><code class="hljs-code">history = model.fit(x=train_features,y=train_labels, epochs=<span class="hljs-number">100</span>, verbose=<span class="hljs-number">1</span>, validation_split=<span class="hljs-number">0.2</span>)
</code></pre>
</li>
<li class="numberedList">Cool, now that the model is trained, we can check if our model is overfitted, underfitted, or properly fitted by plotting the loss curve. Both validation loss and training loss are near each other as we increase the training epochs; this suggests that our model is properly trained:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(history.history[<span class="hljs-string">'loss'</span>], label=<span class="hljs-string">'loss'</span>)
plt.plot(history.history[<span class="hljs-string">'val_loss'</span>], label=<span class="hljs-string">'val_loss'</span>)
plt.xlabel(<span class="hljs-string">'Epoch'</span>)
plt.ylabel(<span class="hljs-string">'Error [MPG]'</span>)
plt.legend()
plt.grid(<span class="hljs-literal">True</span>)
</code></pre>
</li>
</ol>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="354" src="../Images/B18331_02_05.png" width="525"/></figure>
<p class="packt_figref">Figure 2.5: Model error</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="13">Let us finally compare the predicted fuel efficiency<a id="_idIndexMarker223"/> and the true fuel efficiency<a id="_idIndexMarker224"/> on the test dataset. Remember<a id="_idIndexMarker225"/> that the model <a id="_idIndexMarker226"/>has not seen a test dataset ever, thus this prediction is from the model’s ability to generalize the relationship between inputs and fuel efficiency. If the model has learned the relationship well, the two should form a linear relationship:
        <pre class="programlisting code"><code class="hljs-code">y_pred = model.predict(test_features).flatten()
a = plt.axes(aspect=<span class="hljs-string">'equal'</span>)
plt.scatter(test_labels, y_pred)
plt.xlabel(<span class="hljs-string">'True Values [MPG]'</span>)
plt.ylabel(<span class="hljs-string">'Predictions [MPG]'</span>)
lims = [<span class="hljs-number">0</span>, <span class="hljs-number">50</span>]
plt.xlim(lims)
plt.ylim(lims)
plt.plot(lims, lims)
</code></pre>
</li>
</ol>
<figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" height="351" src="../Images/B18331_02_06.png" width="375"/></figure>
<p class="packt_figref">Figure 2.6: Plot between predicted fuel efficiency and actual values</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="14">Additionally, we<a id="_idIndexMarker227"/> can also plot<a id="_idIndexMarker228"/> the error<a id="_idIndexMarker229"/> between the predicted <a id="_idIndexMarker230"/>and true fuel efficiency:
        <pre class="programlisting code"><code class="hljs-code">error = y_pred - test_labels
plt.hist(error, bins=<span class="hljs-number">30</span>)
plt.xlabel(<span class="hljs-string">'Prediction Error [MPG]'</span>)
plt.ylabel(<span class="hljs-string">'Count'</span>)
</code></pre>
</li>
</ol>
<figure class="mediaobject"><img alt="Chart, histogram  Description automatically generated" height="416" src="../Images/B18331_02_07.png" width="605"/></figure>
<p class="packt_figref">Figure 2.7: Prediction error</p>
<p class="normal">In case we want to make<a id="_idIndexMarker231"/> more than one prediction, that is, dealing <a id="_idIndexMarker232"/>with a multivariate regression<a id="_idIndexMarker233"/> problem, the only change would<a id="_idIndexMarker234"/> be that instead of one unit in the last dense layer, we will have as many units as the number of variables to be predicted. Consider, for example, we want to build a model which takes into account a student’s SAT score, attendance, and some family parameters, and wants to predict the GPA score for all four undergraduate years; then we will have the output layer with four units. Now that you are familiar with regression, let us move toward the classification tasks.</p>
<h1 class="heading-1" id="_idParaDest-61">Classification tasks and decision boundaries</h1>
<p class="normal">Till now, the focus of the chapter<a id="_idIndexMarker235"/> was on regression. In this section, we will talk about another important task: the task of classification. Let us first understand the difference between<a id="_idIndexMarker236"/> regression (also sometimes<a id="_idIndexMarker237"/> referred to as prediction) and<a id="_idIndexMarker238"/> classification:</p>
<ul>
<li class="bulletList">In classification, the data is grouped into classes/categories, while in regression, the aim is to get a continuous numerical value for given data. For example, identifying the number of handwritten digits is a classification task; all handwritten digits will belong to one of the ten numbers lying between 0-9. The task of predicting the price of the house depending upon different input variables is a regression task.</li>
<li class="bulletList">In a classification task, the model finds the decision boundaries<a id="_idIndexMarker239"/> separating one class from another. In the regression task, the model approximates a function that fits the input-output relationship.</li>
<li class="bulletList">Classification is a subset of regression; here, we are predicting classes. Regression is much more general.</li>
</ul>
<p class="normal"><em class="italic">Figure 2.8</em> shows how classification and regression tasks differ. In classification, we need to find a line (or a plane or hyperplane in multidimensional space) separating the classes. In regression, the aim is to find a line (or plane or hyperplane) that fits the given input points:</p>
<figure class="mediaobject"><img alt="" height="318" src="../Images/B18331_02_08.png" width="625"/></figure>
<p class="packt_figref">Figure 2.8: Classification vs regression</p>
<p class="normal">In the following section, we will explain logistic regression, which is a very common and useful classification technique.</p>
<h2 class="heading-2" id="_idParaDest-62">Logistic regression</h2>
<p class="normal">Logistic regression is used to determine the probability<a id="_idIndexMarker240"/> of an event. Conventionally, the event is represented as a categorical dependent variable. The probability of the event is expressed using the sigmoid (or “logit”) function:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_02_040.png" style="height: 2.50em !important;" width="558"/></p>
<p class="normal">The goal now is to estimate weights <img alt="" height="50" src="../Images/B18331_02_005.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="346"/> and bias term <em class="italic">b</em>. In logistic regression, the coefficients are estimated using either the maximum likelihood estimator or stochastic gradient descent. If <em class="italic">p</em> is the total number of input data points, the loss is conventionally defined as a cross-entropy term given by:</p>
<p class="center"><img alt="" height="138" src="../Images/B18331_02_042.png" style="height: 3.45em !important;" width="700"/></p>
<p class="normal">Logistic regression is used in classification problems. For example, when looking at medical data, we can use logistic regression to classify whether a person has cancer or not. If the output categorical variable has two or more levels, we can use multinomial logistic regression. Another common technique used for two or more output variables is one versus all.</p>
<p class="normal">For multiclass logistic regression, the cross-entropy loss function is modified as:</p>
<p class="center"><img alt="" height="142" src="../Images/B18331_02_043.png" style="height: 3.55em !important;" width="392"/></p>
<p class="normal">where <em class="italic">K</em> is the total number of classes. You can<a id="_idIndexMarker241"/> read more about logistic regression at <a href="https://en.wikipedia.org/wiki/Logistic_regression"><span class="url">https://en.wikipedia.org/wiki/Logistic_regression</span></a>.</p>
<p class="normal">Now that you have some idea about logistic regression, let us see how we can apply it to any dataset.</p>
<h2 class="heading-2" id="_idParaDest-63">Logistic regression on the MNIST dataset</h2>
<p class="normal">Next, we will use TensorFlow Keras to classify handwritten<a id="_idIndexMarker242"/> digits using logistic<a id="_idIndexMarker243"/> regression. We will be using the <strong class="keyWord">MNIST</strong> (<strong class="keyWord">Modified National Institute of Standards and Technology</strong>) dataset. For those working in the field of deep learning, MNIST is not new, it is like the ABC of machine learning. It contains images of handwritten digits and a label for each image, indicating which digit it is. The label contains a value lying between 0-9 depending on the handwritten digit. Thus, it is a multiclass classification.</p>
<p class="normal">To implement the logistic regression, we will make a model with only one dense layer. Each class will be represented by a unit in the output, so since we have 10 classes, the number of units in the output would be 10. The probability function used in the logistic regression is similar to the sigmoid activation function; therefore, we use sigmoid activation.</p>
<p class="normal">Let us build our model:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">The first step is, as always, importing the modules needed. Notice that here we are using another useful layer from the Keras API, the <code class="inlineCode">Flatten</code> layer. The <code class="inlineCode">Flatten</code> layer helps us to resize the 28 x 28 two-dimensional input images of the MNIST dataset into a 784 flattened array:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">import</span> tensorflow.keras <span class="hljs-keyword">as</span> K
<span class="hljs-keyword">from</span> tensorflow.keras.layers <span class="hljs-keyword">import</span> Dense, Flatten
</code></pre>
</li>
<li class="numberedList">We take the input data of MNIST from the <code class="inlineCode">tensorflow.keras</code> dataset:
        <pre class="programlisting code"><code class="hljs-code">((train_data, train_labels),(test_data, test_labels)) = tf.keras.datasets.mnist.load_data()
</code></pre>
</li>
<li class="numberedList">Next, we preprocess the data. We normalize the images; the MNIST dataset images are black and white images with the intensity value of each pixel lying between 0-255. We divide it by 255, so that now the values lie between 0-1:
        <pre class="programlisting code"><code class="hljs-code">train_data = train_data/np.float32(<span class="hljs-number">255</span>)
train_labels = train_labels.astype(np.int32)  
test_data = test_data/np.float32(<span class="hljs-number">255</span>)
test_labels = test_labels.astype(np.int32)
</code></pre>
</li>
<li class="numberedList">Now, we define a very simple<a id="_idIndexMarker244"/> model; it has only one <code class="inlineCode">Dense</code> layer with <code class="inlineCode">10</code> units, and it takes an input of size 784. You can see from the output of the model summary that only the <code class="inlineCode">Dense</code> layer has trainable parameters:
        <pre class="programlisting code"><code class="hljs-code">model = K.Sequential([
    Flatten(input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)),
    Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'sigmoid'</span>)
])
model.summary()
</code></pre>
<pre class="programlisting con"><code class="hljs-con">Model: "sequential"
____________________________________________________________
 Layer (type)           Output Shape              Param #   
============================================================
 flatten (Flatten)      (None, 784)               0         
                                                            
 dense (Dense)          (None, 10)                7850      
                                                            
============================================================
Total params: 7,850
Trainable params: 7,850
Non-trainable params: 0
____________________________________________________________
</code></pre>
</li>
<li class="numberedList">Since the test labels are integral values, we will use <code class="inlineCode">SparseCategoricalCrossentropy</code> loss with <code class="inlineCode">logits</code> set to <code class="inlineCode">True</code>. The optimizer selected is Adam. Additionally, we also define accuracy as metrics to be logged as the model is trained. We train our model for 50 epochs, with a train-validation split of 80:20:
        <pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(optimizer=<span class="hljs-string">'adam'</span>, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=<span class="hljs-literal">True</span>), metrics=[<span class="hljs-string">'accuracy'</span>])
history = model.fit(x=train_data,y=train_labels, epochs=<span class="hljs-number">50</span>, verbose=<span class="hljs-number">1</span>, validation_split=<span class="hljs-number">0.2</span>)
</code></pre>
</li>
<li class="numberedList">Let us see how our simple model has fared by plotting the loss plot. You can see that since the validation loss and training loss are diverging, as the training loss is decreasing, the validation loss increases, thus the model is overfitting. You can improve the model performance by adding hidden layers:
        <pre class="programlisting code"><code class="hljs-code">plt.plot(history.history[<span class="hljs-string">'loss'</span>], label=<span class="hljs-string">'loss'</span>)
plt.plot(history.history[<span class="hljs-string">'val_loss'</span>], label=<span class="hljs-string">'val_loss'</span>)
plt.xlabel(<span class="hljs-string">'Epoch'</span>)
plt.ylabel(<span class="hljs-string">'Loss'</span>)
plt.legend()
plt.grid(<span class="hljs-literal">True</span>)
</code></pre>
</li>
</ol>
<figure class="mediaobject"><img alt="Chart, line chart  Description automatically generated" height="369" src="../Images/B18331_02_09.png" width="550"/></figure>
<p class="packt_figref">Figure 2.9: Loss plot</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="7">To better understand<a id="_idIndexMarker245"/> the result, we build two utility<a id="_idIndexMarker246"/> functions; these functions help us in visualizing the handwritten digits and the probability of the 10 units in the output:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span> <span class="hljs-title">plot_image</span>(<span class="hljs-params">i, predictions_array, true_label, img</span>):
    true_label, img = true_label[i], img[i]
    plt.grid(<span class="hljs-literal">False</span>)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(img, cmap=plt.cm.binary)
    predicted_label = np.argmax(predictions_array)
    <span class="hljs-keyword">if</span> predicted_label == true_label:
      color =<span class="hljs-string">'blue'</span>
    <span class="hljs-keyword">else</span>:
      color =<span class="hljs-string">'red'</span>
    plt.xlabel(<span class="hljs-string">"Pred {} Conf: {:2.0f}% True ({})"</span>.<span class="hljs-built_in">format</span>(predicted_label,
                                  <span class="hljs-number">100</span>*np.<span class="hljs-built_in">max</span>(predictions_array),
                                  true_label),
                                  color=color)
<span class="hljs-keyword">def</span> <span class="hljs-title">plot_value_array</span>(<span class="hljs-params">i, predictions_array, true_label</span>):
    true_label = true_label[i]
    plt.grid(<span class="hljs-literal">False</span>)
    plt.xticks(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>))
    plt.yticks([])
    thisplot = plt.bar(<span class="hljs-built_in">range</span>(<span class="hljs-number">10</span>), predictions_array,
    color<span class="hljs-string">"#777777"</span>)
    plt.ylim([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>])
    predicted_label = np.argmax(predictions_array)
    thisplot[predicted_label].set_color(<span class="hljs-string">'red'</span>)
    thisplot[true_label].set_color(<span class="hljs-string">'blue'</span>)
</code></pre>
</li>
<li class="numberedList">Using these utility functions, we plot the predictions:
        <pre class="programlisting code"><code class="hljs-code">predictions = model.predict(test_data)
i = <span class="hljs-number">56</span>
plt.figure(figsize=(<span class="hljs-number">10</span>,<span class="hljs-number">5</span>))
plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">1</span>)
plot_image(i, predictions[i], test_labels, test_data)
plt.subplot(<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)
plot_value_array(i, predictions[i],  test_labels)
plt.show()
</code></pre>
</li>
<li class="numberedList">The plot on the left<a id="_idIndexMarker247"/> is the image of the handwritten<a id="_idIndexMarker248"/> digit, with the predicted label, the confidence in the prediction, and the true label. The image on the right shows the probability (logistic) output of the 10 units; we can see that the unit which represents the number 4 has the highest probability:</li>
</ol>
<figure class="mediaobject"><img alt="A picture containing logo  Description automatically generated" height="391" src="../Images/B18331_02_10.png" width="741"/></figure>
<p class="packt_figref">Figure 2.10: Predicted digit and confidence value of the prediction</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="10">In this code, to stay true to logistic<a id="_idIndexMarker249"/> regression, we used a sigmoid activation<a id="_idIndexMarker250"/> function and only one <code class="inlineCode">Dense</code> layer. For better performance, adding dense layers and using softmax as the final activation function will be helpful. For example, the following model gives 97% accuracy on the validation dataset:
        <pre class="programlisting code"><code class="hljs-code">better_model = K.Sequential([
    Flatten(input_shape=(<span class="hljs-number">28</span>, <span class="hljs-number">28</span>)),
    Dense(<span class="hljs-number">128</span>,  activation=<span class="hljs-string">'relu'</span>),
    Dense(<span class="hljs-number">10</span>, activation=<span class="hljs-string">'softmax'</span>)
])
better_model.summary()
</code></pre>
</li>
</ol>
<p class="normal">You can experiment by adding more layers, or by changing the number of neurons in each layer, and even changing the optimizer. This will give you a better understanding of how these parameters influence the model performance.</p>
<h1 class="heading-1" id="_idParaDest-64">Summary</h1>
<p class="normal">This chapter dealt with different types of regression algorithms. We started with linear regression and used it to predict house prices for a simple one-input variable case. We built simple and multiple linear regression models using the TensorFlow Keras API. The chapter then moved toward logistic regression, which is a very important and useful technique for classifying tasks. The chapter explained the TensorFlow Keras API and used it to implement both linear and logistic regression for some classical datasets. The next chapter will introduce you to convolutional neural networks, the most commercially successful neural network models for image data.</p>
<h1 class="heading-1" id="_idParaDest-65">References</h1>
<p class="normal">Here are some good resources if you are interested in knowing more about the concepts we’ve covered in this chapter:</p>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">TensorFlow website: <a href="https://www.tensorflow.org/"><span class="url">https://www.tensorflow.org/</span></a></li>
<li class="numberedList"><em class="italic">Exploring bivariate numerical data</em>: <a href="https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data"><span class="url">https://www.khanacademy.org/math/statistics-probability/describing-relationships-quantitative-data</span></a></li>
<li class="numberedList">Murphy, K. P. (2022). <em class="italic">Probabilistic Machine Learning: An introduction</em>, MIT Press.</li>
<li class="numberedList">Blake, C., &amp; Merz, C. (1998). UCI repository of machine learning databases: <a href="http://www.ics.uci.edu/~mlearn/MLRepository.xhtml%20"><span class="url">http://www.ics.uci.edu/~mlearn/MLRepository.xhtml</span></a></li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="138" src="../Images/QR_Code18312172242788196871.png" width="177"/></p>
</div>
</div>
</body></html>