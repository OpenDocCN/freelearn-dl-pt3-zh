- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Case Study – The MAB Problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in the previous chapters, we have learned the fundamental concepts of
    reinforcement learning and also several interesting reinforcement learning algorithms.
    We learned about a model-based method called dynamic programming and a model-free
    method called the Monte Carlo method, and then we learned about the temporal difference
    method, which combines the advantages of dynamic programming and the Monte Carlo
    method.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn about one of the classic problems in reinforcement
    learning called the **multi-armed bandit** (**MAB**) problem. We start the chapter
    by understanding the MABproblem, and then we will learn about several exploration
    strategies, called epsilon-greedy, softmax exploration, upper confidence bound,
    and Thompson sampling, for solving the MAB problem. Following this, we will learn
    how a MAB is useful in real-world use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we will understand how to find the best advertisement banner
    that is clicked on most frequently by users by framing it as a MAB problem. At
    the end of the chapter, we will learn about contextual bandits and how they are
    used in different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The MAB problem
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The epsilon-greedy method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The upper confidence bound algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Thompson sampling algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications of MAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finding the best advertisement banner using MAB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contextual bandits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MAB problem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MAB problem is one of the classic problems in reinforcement learning. A
    MAB is a slot machine where we pull the arm (lever) and get a payout (reward)
    based on some probability distribution. A single slot machine is called a one-armed
    bandit and when there are multiple slot machines it is called a MAB or *k*-armed
    bandit, where *k* denotes the number of slot machines.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.1* shows a 3-armed bandit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: 3-armed bandit slot machines'
  prefs: []
  type: TYPE_NORMAL
- en: Slot machines are one of the most popular games in the casino, where we pull
    the arm and get a reward. If we get 0 reward then we lose the game, and if we
    get +1 reward then we win the game. There can be several slot machines, and each
    slot machine is referred to as an arm. For instance, slot machine 1 is referred
    to as arm 1, slot machine 2 is referred to as arm 2, and so on. Thus, whenever
    we say arm *n*, it actually means that we are referring to slot machine *n*.
  prefs: []
  type: TYPE_NORMAL
- en: Each arm has its own probability distribution indicating the probability of
    winning and losing the game. For example, let's suppose we have two arms. Let
    the probability of winning if we pull arm 1 (slot machine 1) be 0.7 and the probability
    of winning if we pull arm 2 (slot machine 2) be 0.5\.
  prefs: []
  type: TYPE_NORMAL
- en: Then, if we pull arm 1, 70% of the time we win the game and get the +1 reward,
    and if we pull arm 2, then 50% of the time we win the game and get the +1 reward.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we can say that pulling arm 1 is desirable as it makes us win the game
    70% of the time. However, this probability distribution of the arm (slot machine)
    will not be given to us. We need to find out which arm helps us to win the game
    most of the time and gives us a good reward.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, how can we find this?
  prefs: []
  type: TYPE_NORMAL
- en: Say we pulled arm 1 once and received a +1 reward, and we pulled arm 2 once
    and received a 0 reward. Since arm 1 gives a +1 reward, we cannot come to the
    conclusion that arm 1 is the best arm immediately after pulling it only once.
    We need to pull both of the arms many times and compute the average reward we
    obtain from each of the arms, and then we can select the arm that gives the maximum
    average reward as the best arm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s denote the arm by *a* and define the average reward by pulling the arm
    *a* as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_001.png)'
  prefs: []
  type: TYPE_IMG
- en: Where *Q*(*a*) denotes the average reward of arm *a*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The optimal arm *a** is the one that gives us the maximum average reward, that
    is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, we have learned that the arm that gives the maximum average reward is
    the optimal arm. But how can we find this?
  prefs: []
  type: TYPE_NORMAL
- en: We play the game for several rounds and we can pull only one arm in each round.
    Say in the first round we pull arm 1 and observe the reward, and in the second
    round we pull arm 2 and observe the reward. Similarly, in every round, we keep
    pulling arm 1 or arm 2 and observe the reward. After completing several rounds
    of the game, we compute the average reward of each of the arms, and then we select
    the arm that has the maximum average reward as the best arm.
  prefs: []
  type: TYPE_NORMAL
- en: But this is not a good approach to find the best arm. Say we have 20 arms; if
    we keep pulling a different arm in each round, then in most of the rounds we will
    lose the game and get a 0 reward. Along with finding the best arm, our goal should
    be to minimize the cost of identifying the best arm, and this is usually referred
    to as regret.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we need to find the best arm while minimizing regret. That is, we need
    to find the best arm, but we don't want to end up selecting the arms that make
    us lose the game in most of the rounds.
  prefs: []
  type: TYPE_NORMAL
- en: So, should we explore a different arm in each round, or should we select only
    the arm that got us a good reward in the previous rounds? This leads to a situation
    called the exploration-exploitation dilemma, which we learned about in *Chapter
    4*, *Monte Carlo Methods*. So, to resolve this, we use the epsilon-greedy method
    and select the arm that got us a good reward in the previous rounds with probability
    1-epsilon and select the random arm with probability epsilon. After completing
    several rounds, we select the best arm as the one that has the maximum average
    reward.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the epsilon-greedy method, there are several different exploration
    strategies that help us to overcome the exploration-exploitation dilemma. In the
    upcoming section, we will learn more about several different exploration strategies
    in detail and how they help us to find the optimal arm, but first let's look at
    creating a bandit.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a bandit in the Gym
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before going ahead, let's learn how to create a bandit environment with the
    Gym toolkit. The Gym does not come with a prepackaged bandit environment. So,
    we need to create a bandit environment and integrate it with the Gym. Instead
    of creating the bandit environment from scratch, we will use the open-source version
    of the bandit environment provided by Jesse Cooper.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s clone the Gym bandits repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can install it using `pip`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After installation, we import `gym_bandits` and also the `gym` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`gym_bandits` provides several versions of the bandit environment. We can examine
    the different bandit versions at [https://github.com/JKCooper2/gym-bandits](https://github.com/JKCooper2/gym-bandits).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s just create a simple 2-armed bandit whose environment ID is `BanditTwoArmedHighLowFixed-v0`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we created a 2-armed bandit, our action space will be `2` (as there are
    two arms), as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also check the probability distribution of the arm with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: It indicates that, with arm 1, we win the game 80% of the time and with arm
    2, we win the game 20% of the time. Our goal is to find out whether pulling arm
    1 or arm 2 makes us win the game most of the time.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to create bandit environments in the Gym, in the
    next section, we will explore different exploration strategies to solve the MAB
    problem and we will implement them with the Gym.
  prefs: []
  type: TYPE_NORMAL
- en: Exploration strategies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the beginning of the chapter, we learned about the exploration-exploitation
    dilemma in the MAB problem. To overcome this, we use different exploration strategies
    and find the best arm. The different exploration strategies are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon-greedy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Softmax exploration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upper confidence bound
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thomson sampling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will explore all of these exploration strategies in detail and implement
    them to find the best arm.
  prefs: []
  type: TYPE_NORMAL
- en: Epsilon-greedy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We learned about the epsilon-greedy algorithm in the previous chapters. With
    epsilon-greedy, we select the best arm with a probability 1-epsilon and we select
    a random arm with a probability epsilon. Let's take a simple example and learn
    how to find the best arm with the epsilon-greedy method in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Say we have two arms—arm 1 and arm 2\. Suppose with arm 1 we win the game 80% of
    the time and with arm 2 we win the game 20% of the time. So, we can say that arm
    1 is the best arm as it makes us win the game 80% of the time. Now, let's learn
    how to find this with the epsilon-greedy method.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we initialize the `count` (the number of times the arm is pulled), `sum_rewards`
    (the sum of rewards obtained from pulling the arm), and `Q` (the average reward
    obtained by pulling the arm), as *Table 6.1* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.1: Initialize the variables with zero'
  prefs: []
  type: TYPE_NORMAL
- en: '**Round 1**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say, in round 1 of the game, we select a random arm with a probability epsilon,
    and suppose we randomly pull arm 1 and observe the reward. Let the reward obtained
    by pulling arm 1 be 1\. So, we update our table with `count` of arm 1 set to 1,
    and `sum_rewards` of arm 1 set to 1, and thus the average reward `Q` of arm 1
    after round 1 is 1 as *Table 6.2* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.2: Results after round 1'
  prefs: []
  type: TYPE_NORMAL
- en: '**Round 2**:'
  prefs: []
  type: TYPE_NORMAL
- en: Say, in round 2, we select the best arm with a probability 1-epsilon. The best
    arm is the one that has the maximum average reward. So, we check our table to
    see which arm has the maximum average reward. Since arm 1 has the maximum average
    reward, we pull arm 1 and observe the reward and let the reward obtained from
    pulling arm 1 be 1\.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we update our table with `count` of arm 1 to 2 and `sum_rewards` of arm
    1 to 2, and thus the average reward `Q` of arm 1 after round 2 is 1 as *Table
    6.3* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.3: Results after round 2'
  prefs: []
  type: TYPE_NORMAL
- en: '**Round 3**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say, in round 3, we select a random arm with a probability epsilon. Suppose
    we randomly pull arm 2 and observe the reward. Let the reward obtained by pulling
    arm 2 be 0\. So, we update our table with `count` of arm 2 set to 1 and `sum_rewards`
    of arm 2 set to 0, and thus the average reward `Q` of arm 2 after round 3 is 0
    as *Table 6.4* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.4: Results after round 3'
  prefs: []
  type: TYPE_NORMAL
- en: '**Round 4**:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say, in round 4, we select the best arm with a probability 1-epsilon. So, we
    pull arm 1 since it has the maximum average reward. Let the reward obtained by
    pulling arm 1 be 0 this time. Now, we update our table with `count` of arm 1 to
    3 and `sum_rewards` of arm 2 to 2, and thus the average reward `Q` of arm 1 after
    round 4 will be 0.66 as *Table 6.5* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.5: Results after round 4'
  prefs: []
  type: TYPE_NORMAL
- en: We repeat this process for several rounds; that is, for several rounds of the
    game, we pull the best arm with a probability 1-epsilon and we pull a random arm
    with probability epsilon.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 6.6* shows the updated table after 100 rounds of the game:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.6: Results after 100 rounds'
  prefs: []
  type: TYPE_NORMAL
- en: From *Table 6.6*, we can conclude that arm 1 is the best arm since it has the
    maximum average reward.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing epsilon-greedy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s learn to implement the epsilon-greedy method to find the best arm.
    First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For better understanding, let''s create the bandit with only two arms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s check the probability distribution of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We can observe that with arm 1 we win the game with 80% probability and with
    arm 2 we win the game with 20% probability. Here, the best arm is arm 1, as with
    arm 1 we win the game with 80% probability. Now, let's see how to find this best
    arm using the epsilon-greedy method.
  prefs: []
  type: TYPE_NORMAL
- en: First, let's initialize the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the `count` for storing the number of times an arm is pulled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `sum_rewards` for storing the sum of rewards of each arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `Q` for storing the average reward of each arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of rounds (iterations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's define the `epsilon_greedy` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we generate a random number from a uniform distribution. If the random
    number is less than epsilon, then we pull the random arm; else, we pull the best
    arm that has the maximum average reward, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's play the game and try to find the best arm using the epsilon-greedy
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each round:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the arm based on the epsilon-greedy method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Pull the arm and store the reward and next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Increment the count of the arm by `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the sum of rewards of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the average reward of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the rounds, we look at the average reward obtained from each of the
    arms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can select the optimal arm as the one that has the maximum average
    reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_002.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Since arm 1 has a higher average reward than arm 2, our optimal arm will be
    arm 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we have found the optimal arm using the epsilon-greedy method.
  prefs: []
  type: TYPE_NORMAL
- en: Softmax exploration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Softmax exploration, also known as Boltzmann exploration, is another useful
    exploration strategy for finding the optimal arm.
  prefs: []
  type: TYPE_NORMAL
- en: In the epsilon-greedy policy, we learned that we select the best arm with probability
    1-epsilon and a random arm with probability epsilon. As you may have noticed,
    in the epsilon-greedy policy, all the non-best arms are explored equally. That
    is, all the non-best arms have a uniform probability of being selected. For example,
    say we have 4 arms and arm 1 is the best arm. Then we explore the non-best arms
    – [arm 2, arm 3, arm 4] – uniformly.
  prefs: []
  type: TYPE_NORMAL
- en: Say arm 3 is never a good arm and it always gives a reward of 0\. In this case,
    instead of exploring arm 3 again, we can spend more time exploring arm 2 and arm
    4\. But the problem with the epsilon-greedy method is that we explore all the
    non-best arms equally. So, all the non-best arms – [arm 2, arm 3, arm 4] – will
    be explored equally.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid this, if we can give priority to arm 2 and arm 4 over arm 3, then we
    can explore arm 2 and arm 4 more than arm 3.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, but how can we give priority to the arms? We can give priority to the
    arms by assigning a probability to all the arms based on the average reward *Q*.
    The arm that has the maximum average reward will have high probability, and all
    the non-best arms have a probability proportional to their average reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, as *Table 6.7* shows, arm 1 is the best arm as it has a high
    average reward *Q*. So, we assign a high probability to arm 1\. Arms 2, 3, and
    4 are the non-best arms, and we need to explore them. As we can observe, arm 3
    has an average reward of 0\. So, instead of selecting all the non-best arms uniformly,
    we give more priority to arms 2 and 4 than arm 3\. So, the probability of arm
    2 and 4 will be high compared to arm 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.7: Average reward for a 4-armed bandit'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, in softmax exploration, we select the arms based on a probability. The
    probability of each arm is directly proportional to its average reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'But wait, the probabilities should sum to 1, right? The average reward (Q value)
    will not sum to 1\. So, we convert them into probabilities with the softmax function,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_005.png)'
  prefs: []
  type: TYPE_IMG
- en: So, now the arm will be selected based on the probability. However, in the initial
    rounds we will not know the correct average reward of each arm, so selecting the
    arm based on the probability of average reward will be inaccurate in the initial
    rounds. To avoid this, we introduce a new parameter called *T*. *T* is called
    the temperature parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can rewrite the preceding equation with the temperature *T*, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Okay, how will this *T* help us? When *T* is high, all the arms have an equal
    probability of being selected and when *T* is low, the arm that has the maximum
    average reward will have a high probability. So, we set *T* to a high number in
    the initial rounds, and after a series of rounds we reduce the value of *T*. This
    means that in the initial round we explore all the arms equally and after a series
    of rounds, we select the best arm that has a high probability.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s understand this with a simple example. Say we have four arms, arm 1
    to arm 4. Suppose we pull arm 1 and receive a reward of 1\. Then the average reward
    of arm 1 will be 1 and the average reward of all other arms will be 0, as *Table
    6.8* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.8: Average reward for each arm'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we convert the average reward to probabilities using the softmax function
    given in equation (1), then our probabilities look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.9: Probability of each arm'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can observe, we have a 47% probability for arm 1 and a 17% probability
    for all other arms. But we cannot assign a high probability to arm 1 by just pulling
    arm 1 once. So, we set *T* to a high number, say *T* = 30, and calculate the probabilities
    based on equation (2). Now our probabilities become:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.10: Probability of each arm with T=30'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, now all the arms have equal probabilities of being selected.
    Now we explore the arms based on this probability and over a series of rounds,
    the *T* value will be reduced, and we will have a high probability to the best
    arm. Let''s suppose after some 30 rounds, the average reward of all the arms is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.11: Average reward for each arm after 30+ rounds'
  prefs: []
  type: TYPE_NORMAL
- en: 'We learned that the value of *T* is reduced over several rounds. Suppose the
    value of *T* is reduced and it is now 0.3 (*T*=0.3); then the probabilities will
    become:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Table 6.12: Probabilities for each arm with T now set to 0.3'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, arm 1 has a high probability compared to other arms. So, we select
    arm 1 as the best arm and explore the non-best arms – [arm 2, arm 3, arm 4] –
    based on their probabilities in the next rounds.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in the initial round, we don't know which arm is the best arm. So instead
    of assigning a high probability to the arm based on the average reward, we assign
    an equal probability to all the arms in the initial round with a high value of
    *T* and over a series of rounds, we reduce the value of *T* and assign a high
    probability to the arm that has a high average reward.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing softmax exploration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to implement softmax exploration to find the best arm.
    First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take the same two-armed bandit we saw in the epsilon-greedy section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's initialize the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize `count` for storing the number of times an arm is pulled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `sum_rewards` for storing the sum of rewards of each arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `Q` for storing the average reward of each arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of rounds (iterations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define the softmax function with the temperature *T*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_007.png)'
  prefs: []
  type: TYPE_IMG
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the probability of each arm based on the preceding equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the arm based on the computed probability distribution of arms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's play the game and try to find the best arm using the softmax exploration
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s begin by setting the temperature `T` to a high number, say, `50`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'For each round:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the arm based on the softmax exploration method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Pull the arm and store the reward and next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Increment the count of the arm by 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the sum of rewards of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the average reward of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Reduce the temperature `T`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the rounds, we check the Q value, that is, the average reward of
    all the arms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, arm 1 has a higher average reward than arm 2, so we select arm
    1 as the optimal arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code prints:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we have found the optimal arm using the softmax exploration method.
  prefs: []
  type: TYPE_NORMAL
- en: Upper confidence bound
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we will explore another interesting algorithm called **upper
    confidence bound** (**UCB**) for handling the exploration-exploitation dilemma.
    The UCB algorithm is based on a principle called optimism in the face of uncertainty.
    Let's take a simple example and understand how exactly the UCB algorithm works.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose we have two arms – arm 1 and arm 2\. Let's say we played the game for
    20 rounds by pulling arm 1 and arm 2 randomly and found that the mean reward of
    arm 1 is 0.6 and the mean reward of arm 2 is 0.5\. But how can we be sure that
    this mean reward is actually accurate? That is, how can we be sure that this mean
    reward represents the true mean (population mean)? This is where we use the confidence
    interval.
  prefs: []
  type: TYPE_NORMAL
- en: The confidence interval denotes the interval within which the true value lies.
    So, in our setting, the confidence interval denotes the interval within which
    the true mean reward of the arm lies.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, from *Figure 6.2*, we can see that the confidence interval of
    arm 1 is 0.2 to 0.9, which indicates that the mean reward of arm 1 lies in the
    range of 0.2 to 0.9\. 0.2 is the lower confidence bound and 0.9 is the upper confidence
    bound. Similarly, we can observe that the confidence interval of arm 2 is 0.5
    to 0.7, which indicates that the mean reward of arm 2 lies in the range of 0.5
    to 0.7\. where 0.5 is the lower confidence bound and 0.7 is the upper confidence
    bound:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Confidence intervals for arms 1 and 2'
  prefs: []
  type: TYPE_NORMAL
- en: Okay, from *Figure 6.2*, we can see the confidence intervals of arm 1 and arm
    2\. Now, how can we make a decision? That is, how can we decide whether to pull
    arm 1 or arm 2? If we look closely, we can see that the confidence interval of
    arm 1 is large and the confidence interval of arm 2 is small.
  prefs: []
  type: TYPE_NORMAL
- en: When the confidence interval is large, we are uncertain about the mean value.
    Since the confidence interval of arm 1 is large (0.2 to 0.9), we are not sure
    what reward we would obtain by pulling arm 1 because the average reward varies
    from as low as 0.2 to as high as 0.9\. So, there is a lot of uncertainty in arm
    1 and we are not sure whether arm 1 gives a high reward or a low reward.
  prefs: []
  type: TYPE_NORMAL
- en: When the confidence interval is small, then we are certain about the mean value.
    Since the confidence interval of arm 2 is small (0.5 to 0.7), we can be sure that
    we will get a good reward by pulling arm 2 as our average reward is in the range
    of 0.5 to 0.7.
  prefs: []
  type: TYPE_NORMAL
- en: But what is the reason for the confidence interval of arm 2 being small and
    the confidence interval of arm 1 being large? At the beginning of the section,
    we learned that we played the game for 20 rounds by pulling arm 1 and arm 2 randomly
    and computed the mean reward of arm 1 and arm 2\. Say arm 2 has been pulled 15
    times and arm 1 has been pulled only 5 times. Since arm 2 has been pulled many
    times, the confidence interval of arm 2 is small and it denotes a certain mean
    reward. Since arm 1 has been pulled fewer times, the confidence interval of the
    arm is large and it denotes an uncertain mean reward. Thus, it indicates that
    arm 2 has been explored a lot more than arm 1.
  prefs: []
  type: TYPE_NORMAL
- en: Okay, coming back to our question, should we pull arm 1 or arm 2? In UCB, we
    always select the arm that has a high upper confidence bound, so in our example,
    we select arm 1 since it has a high upper confidence bound of 0.9\. But why do
    we have to select the arm that has the highest upper confidence bound? Selecting
    the arm with the highest upper bound helps us to select the arm that gives the
    maximum reward.
  prefs: []
  type: TYPE_NORMAL
- en: But there is a small catch here. When the confidence interval is large, we will
    not be sure about the mean reward. For instance, in our example, we select arm
    1 since it has a high upper confidence bound of 0.9; however, since the confidence
    interval of arm 1 is large, our mean reward could be anywhere from 0.2 to 0.9,
    and so we can even get a low reward. But that's okay, we still select arm 1 as
    it promotes exploration. When the arm is explored well, then the confidence interval
    gets smaller.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we play the game for several rounds by selecting the arm that has a high
    UCB, our confidence interval of both arms will get narrower and denote a more
    accurate mean value. For instance, as we can see in *Figure 6.3*, after playing
    the game for several rounds, the confidence interval of both the arms becomes
    small and denotes a more accurate mean value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Confidence intervals for arms 1 and 2 after several rounds'
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 6.3*, we can see that the confidence interval of both arms is small
    and we have a more accurate mean, and since in UCB we select arm that has the
    highest UCB, we select arm 2 as the best arm.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in UCB, we always select the arm that has the highest upper confidence
    bound. In the initial rounds, we may not select the best arm as the confidence
    interval of the arms will be large in the initial round. But over a series of
    rounds, the confidence interval gets smaller and we select the best arm.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let *N*(*a*) be the number of times arm *a* was pulled and *t* be the total
    number of rounds, then the upper confidence bound of arm *a* can be computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_008.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We select the arm that has the highest upper confidence bound as the best arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_009.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The algorithm of UCB is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Select the arm whose upper confidence bound is high
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pull the arm and receive a reward
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the arm's mean reward and confidence interval
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 1* to *3* for several rounds
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing UCB
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now, let's learn how to implement the UCB algorithm to find the best arm.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create the same two-armed bandit we saw in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's initialize the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize `count` for storing the number of times an arm is pulled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `sum_rewards` for storing the sum of rewards of each arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `Q` for storing the average reward of each arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of rounds (iterations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we define the **UCB function**, which returns the best arm as the one
    that has the highest UCB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the `numpy` array for storing the UCB of all the arms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Before computing the UCB, we explore all the arms at least once, so for the
    first 2 rounds, we directly select the arm corresponding to the round number:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'If the round is greater than 2, then we compute the UCB of all the arms as
    specified in equation (3) and return the arm that has the highest UCB:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's play the game and try to find the best arm using the UCB method.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each round:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the arm based on the UCB method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Pull the arm and store the reward and next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Increment the count of the arm by `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the sum of rewards of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the average reward of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the rounds, we can select the optimal arm as the one that has the
    maximum average reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we found the optimal arm using the UCB method.
  prefs: []
  type: TYPE_NORMAL
- en: Thompson sampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Thompson sampling** (**TS**) is another interesting exploration strategy
    to overcome the exploration-exploitation dilemma and it is based on a beta distribution.
    So, before diving into Thompson sampling, let''s first understand the beta distribution.
    The beta distribution is a probability distribution function and it is expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_010.png)'
  prefs: []
  type: TYPE_IMG
- en: Where ![](img/B15558_06_011.png) and ![](img/B15558_06_012.png) is the gamma
    function.
  prefs: []
  type: TYPE_NORMAL
- en: The shape of the distribution is controlled by the two parameters ![](img/B15558_06_013.png)
    and ![](img/B15558_06_014.png). When the values of ![](img/B15558_05_055.png)
    and ![](img/B15558_06_016.png) are the same, then we will have a symmetric distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, as *Figure 6.4* shows, since the value of ![](img/B15558_05_055.png)
    and ![](img/B15558_06_018.png) is equal to two we have a symmetric distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Symmetric beta distribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the value of ![](img/B15558_06_019.png) is higher than ![](img/B15558_06_020.png)
    then we will have a probability closer to 1 than 0\. For instance, as *Figure
    6.5* shows, since the value of ![](img/B15558_06_021.png) and ![](img/B15558_06_022.png),
    we have a high probability closer to 1 than 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Beta distribution where ![](img/B15558_06_023.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'When the value of ![](img/B15558_06_024.png) is higher than ![](img/B15558_05_055.png)
    then we will have a high probability closer to 0 than 1\. For instance, as shown
    in the following plot, since the value of ![](img/B15558_06_026.png) and ![](img/B15558_06_027.png),
    we have a high probability closer to 0 than 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Gamma distribution where ![](img/B15558_06_028.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a basic idea of the beta distribution, let''s explore how
    Thompson sampling works and how it uses the beta distribution. Understanding the
    true distribution of each arm is very important because once we know the true
    distribution of the arm, then we can easily understand whether the arm will give
    us a good reward; that is, we can understand whether pulling the arm will help
    us to win the game. For example, let''s say we have two arms – arm 1 and arm 2\.
    *Figure 6.7* shows the true distribution of the two arms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: True distributions for arms 1 and 2'
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 6.7*, we can see that it is better to pull arm 1 than arm 2 because
    arm 1 has a high probability close to 1, but arm 2 has a high probability close
    to 0\. So, if we pull arm 1, we get a reward of 1 and win the game, but if we
    pull arm 2 we get a reward of 0 and lose the game. Thus, once we know the true
    distribution of the arms then we can understand which arm is the best arm.
  prefs: []
  type: TYPE_NORMAL
- en: But how can we learn the true distribution of arm 1 and arm 2? This is where
    we use the Thompson sampling method. Thompson sampling is a probabilistic method
    and it is based on a prior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we take *n* samples from arm 1 and arm 2 and compute their distribution.
    However, in the initial iterations, the computed distributions of arm 1 and arm
    2 will not be the same as the true distribution, and so we will call this the
    prior distribution. As *Figure 6.8* shows, we have the prior distribution of arm
    1 and arm 2, and it varies from the true distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_20.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.8: Prior distributions for arms 1 and 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'But over a series of iterations, we learn the true distribution of arm 1 and
    arm 2 and, as *Figure 6.9* shows, the prior distributions of the arms look the
    same as the true distribution after a series of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_21.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.9: The prior distributions move closer to the true distributions'
  prefs: []
  type: TYPE_NORMAL
- en: Once we have learned the true distributions of all the arms, then we can easily
    select the best arm. Okay, but how exactly do we learn the true distribution?
    Let's explore this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we use the beta distribution as a prior distribution. Say we have two
    arms, so we will have two beta distributions (prior distributions), and we initialize
    both ![](img/B15558_05_055.png) and ![](img/B15558_06_030.png) to the same value,
    say 3, as *Figure 6.10* shows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_22.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.10: Initialized prior distributions for arms 1 and 2 look the same'
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, since we initialized alpha and beta to the same value, the beta
    distributions of arm 1 and arm 2 look the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first round, we just randomly sample a value from these two distributions
    and select the arm that has the maximum sampled value. Let''s say the sampled
    value of arm 1 is high, so in this case, we pull arm 1\. Say we win the game by
    pulling arm 1, then we update the distribution of arm 1 by incrementing the alpha
    value of the distribution by 1; that is, we update the alpha value as ![](img/B15558_06_031.png).
    As *Figure 6.11* shows, the alpha value of the distribution of arm 1 is incremented,
    and as we can see, arm 1''s beta distribution has slightly high probability closer
    to 1 compared to arm 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_23.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.11: Prior distributions for arms 1 and 2 after round 1'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next round, we again sample a value randomly from these two distributions
    and select the arm that has the maximum sampled value. Suppose, in this round
    as well, we got the maximum sampled value from arm 1\. Then we pull the arm 1
    again. Say we win the game by pulling arm 1, then we update the distribution of
    arm 1 by updating the alpha value to ![](img/B15558_06_031.png). As *Figure 6.12*
    shows, the alpha value of arm 1''s distribution is incremented, and arm 1''s beta
    distribution has a slightly high probability close to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_24.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.12: Prior distributions for arms 1 and 2 after round 2'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, in the next round, we again randomly sample a value from these distributions
    and pull the arm that has the maximum value. Say this time we got the maximum
    value from arm 2, so we pull arm 2 and play the game. Suppose we lose the game
    by pulling arm 2\. Then we update the distribution of arm 2 by updating the beta
    value as ![](img/B15558_06_033.png). As *Figure 6.13* shows, the beta value of
    arm 2''s distribution is incremented and the beta distribution of arm 2 has a
    slightly high probability close to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_25.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.13: Prior distributions for arms 1 and 2 after round 3'
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, in the next round, we randomly sample a value from the beta distribution
    of arm 1 and arm 2\. Say the sampled value of arm 2 is high, so we pull arm 2\.
    Say we lose the game again by pulling arm 2\. Then we update the distribution
    of arm 2 by updating the beta value as ![](img/B15558_06_034.png). As *Figure
    6.14* shows, the beta value of arm 2''s distribution is incremented by 1 and also
    arm 2''s beta distribution has a slightly high probability close to 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Prior distributions for arms 1 and 2 after round 4'
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, so, did you notice what we are doing here? We are essentially increasing
    the alpha value of the distribution of the arm if we win the game by pulling that
    arm, else we increase the beta value. If we do this repeatedly for several rounds,
    then we can learn the true distribution of the arm. Say after several rounds,
    our distribution will look like *Figure 6.15*. As we can see, the distributions
    of both arms resemble the true distributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Prior distributions for arms 1 and 2 after several rounds'
  prefs: []
  type: TYPE_NORMAL
- en: Now if we sample a value from each of these distributions, then the sampled
    value will always be high from arm 1 and we always pull arm 1 and win the game.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps involved in the Thomson sampling method are given here:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the beta distribution with alpha and beta set to equal values for
    all *k* arms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sample a value from the beta distribution of all *k* arms
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pull the arm whose sampled value is high
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we win the game, then update the alpha value of the distribution to ![](img/B15558_06_035.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If we lose the game, then update the beta value of the distribution to ![](img/B15558_06_034.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Repeat *steps 2* to *5* for many rounds
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implementing Thompson sampling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Now, let's learn how to implement the Thompson sampling method to find the best
    arm.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'For better understanding, let''s create the same two-armed bandit we saw in
    the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's initialize the variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize `count` for storing the number of times an arm is pulled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `sum_rewards` for storing the sum of rewards of each arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `Q` for storing the average reward of each arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the alpha value as `1` for both arms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize the beta value as `1` for both arms:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the number of rounds (iterations):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's define the `thompson_sampling` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'As the following code shows, we randomly sample values from the beta distributions
    of both arms and return the arm that has the maximum sampled value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's play the game and try to find the best arm using the Thompson sampling
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each round:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the arm based on the Thompson sampling method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Pull the arm and store the reward and next state information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Increment the count of the arm by 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the sum of rewards of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Update the average reward of the arm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'If we win the game, that is, if the reward is equal to 1, then we update the
    value of alpha to ![](img/B15558_06_037.png), else we update the value of beta
    to ![](img/B15558_06_033.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the rounds, we can select the optimal arm as the one that has the
    highest average reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: Thus, we found the optimal arm using the Thompson sampling method.
  prefs: []
  type: TYPE_NORMAL
- en: Applications of MAB
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have learned about the MAB problem and how can we solve it using
    various exploration strategies. But our goal is not to just use these algorithms
    for playing slot machines. We can apply the various exploration strategies to
    several different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, bandits can be used as an alternative to AB testing. AB testing
    is one of the most commonly used classic methods of testing. Say we have two versions
    of the landing page of our website. Suppose we want to know which version of the
    landing page is most liked by the users. In this case, we conduct AB testing to
    understand which version of the landing page is most liked by the users. So, we
    show version 1 of the landing page to a particular set of users and version 2
    of the landing page to other set of users. Then we measure several metrics, such
    as click-through rate, average time spent on the website, and so on, to understand
    which version of the landing page is most liked by the users. Once we understand
    which version of the landing page is most liked by the users, then we will start
    showing that version to all the users.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, in AB testing, we schedule a separate time for exploration and exploitation.
    That is, AB testing has two different dedicated periods for exploration and exploitation.
    But the problem with AB testing is that it will incur high regret. We can minimize
    the regret using the various exploration strategies that we have used to solve
    the MAB problem. So, instead of performing complete exploration and exploitation
    separately, we can perform exploration and exploitation simultaneously in an adaptive
    fashion with the various exploration strategies we learned in the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: Bandits are widely used for website optimization, maximizing conversion rates,
    online advertisements, campaigning, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Finding the best advertisement banner using bandits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, let's see how to find the best advertisement banner using bandits.
    Suppose we are running a website and we have five different banners for a single
    advertisement on our website, and say we want to figure out which advertisement
    banner is most liked by the users.
  prefs: []
  type: TYPE_NORMAL
- en: We can frame this problem as a MAB problem. The five advertisement banners represent
    the five arms of the bandit, and we assign +1 reward if the user clicks the advertisement
    and 0 reward if the user does not click the advertisement. So, to find out which
    advertisement banner is most clicked by the users, that is, which advertisement
    banner can give us the maximum reward, we can use various exploration strategies.
    In this section, let's just use an epsilon-greedy method to find the best advertisement
    banner.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the necessary libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: Creating a dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s create a dataset. We generate a dataset with five columns denoting
    the five advertisement banners, and we generate 100,000 rows, where the values
    in the rows will be either 0 or 1, indicating whether the advertisement banner
    has been clicked (1) or not clicked (0) by the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s look at the first few rows of our dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will print the following. As we can see, we have the five
    advertisement banners (0 to 4) and the rows consisting of values of 0 or 1, indicating
    whether the banner has been clicked (1) or not clicked (0).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_28.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.14: Clicks per banner'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the variables
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, let's initialize some of the important variables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the number of iterations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the number of banners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `count` for storing the number of times the banner was clicked:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `sum_rewards` for storing the sum of rewards obtained from each
    banner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize `Q` for storing the mean reward of each banner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a list for storing the selected banners:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: Define the epsilon-greedy method
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s define the epsilon-greedy method. We generate a random value from
    a uniform distribution. If the random value is less than epsilon, then we select
    the random banner; else, we select the best banner that has the maximum average
    reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Run the bandit test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we run the epsilon-greedy policy to find out which advertisement banner
    is the best.
  prefs: []
  type: TYPE_NORMAL
- en: 'For each iteration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Select the banner using the epsilon-greedy policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Get the reward of the banner:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Increment the counter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the sum of rewards:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'Compute the average reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Store the banner to the banner selected list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'After all the rounds, we can select the best banner as the one that has the
    maximum average reward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will print:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also plot and see which banner is selected the most often:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code will plot the following. As we can see, banner 2 is selected
    most often:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15558_06_29.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.15: Banner 2 is the best advertisement banner'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, we have learned how to find the best advertisement banner by framing our
    problem as a MAB problem.
  prefs: []
  type: TYPE_NORMAL
- en: Contextual bandits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We just learned how to use bandits to find the best advertisement banner for
    the users. But the banner preference varies from user to user. That is, user A
    likes banner 1, but user B might like banner 3, and so on. Each user has their
    own preferences. So, we have to personalize advertisement banners according to
    each user. How can we do that? This is where we use contextual bandits.
  prefs: []
  type: TYPE_NORMAL
- en: In the MAB problem, we just perform the action and receive a reward. But with
    contextual bandits, we take actions based on the state of the environment and
    the state holds the context.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, in the advertisement banner example, the state specifies the user
    behavior and we will take action (show the banner) according to the state (user
    behavior) that will result in the maximum reward (ad clicks).
  prefs: []
  type: TYPE_NORMAL
- en: Contextual bandits are widely used for personalizing content according to the
    user's behavior. They are also used to solve the cold-start problems faced by
    recommendation systems. Netflix uses contextual bandits for personalizing the
    artwork for TV shows according to user behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started off the chapter by understanding what the MAB problem is and how
    it can be solved using several exploration strategies. We first learned about
    the epsilon-greedy method, where we select a random arm with a probability epsilon
    and select the best arm with a probability 1-epsilon. Next, we learned about the
    softmax exploration method, where we select the arm based on the probability distribution,
    and the probability of each arm is proportional to the average reward.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we learned about the UCB algorithm, where we select the arm
    that has the highest upper confidence bound. Then, we explored the Thomspon sampling
    method, where we learned the distributions of the arms based on the beta distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, we learned how MAB can be used as an alternative to AB testing
    and how can we find the best advertisement banner by framing the problem as a
    MAB problem. At the end of the chapter, we also had an overview of contextual
    bandits.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn about several interesting deep learning algorithms
    that are essential for deep reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s evaluate the knowledge we gained in this chapter by answering the following
    questions:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a MAB problem?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does the epsilon-greedy policy select an arm?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the significance of *T* in softmax exploration?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How do we compute the upper confidence bound?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What happens when the value of alpha is higher than the value of beta in the
    beta distribution?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the steps involved in Thompson sampling?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are contextual bandits?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information, check out these interesting resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction to Multi-Armed Bandits** by *Aleksandrs Slivkins*, [https://arxiv.org/pdf/1904.07272.pdf](https://arxiv.org/pdf/1904.07272.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A Survey on Practical Applications of Multi-Armed and Contextual Bandits**
    by *Djallel Bouneffouf, Irina Rish*, [https://arxiv.org/pdf/1904.10040.pdf](https://arxiv.org/pdf/1904.10040.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative Filtering Bandits** by *Shuai Li, Alexandros Karatzoglou, Claudio
    Gentile*, [https://arxiv.org/pdf/1502.03473.pdf](https://arxiv.org/pdf/1502.03473.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
