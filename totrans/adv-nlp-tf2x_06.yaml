- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Text Summarization with Seq2seq Attention and Transformer Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Seq2seq注意力机制和Transformer网络进行文本摘要
- en: 'Summarizing a piece of text challenges a deep learning model''s understanding
    of language. Summarization can be considered a uniquely human ability, where the
    gist of a piece of text needs to be understood and phrased. In the previous chapters,
    we have built components that can help in summarization. First, we used BERT to
    encode text and perform sentiment analysis. Then, we used a decoder architecture
    with GPT-2 to generate text. Putting the Encoder and Decoder together yields a
    summarization model. In this chapter, we will implement a seq2seq Encoder-Decoder
    with Bahdanau Attention. Specifically, we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一篇文本挑战了深度学习模型对语言的理解。摘要可以看作是一个独特的人类能力，需要理解文本的要点并加以表述。在前面的章节中，我们构建了有助于摘要的组件。首先，我们使用BERT对文本进行编码并执行情感分析。然后，我们使用GPT-2的解码器架构来生成文本。将编码器和解码器结合起来，形成了一个摘要模型。在本章中，我们将实现一个带有Bahdanau注意力机制的seq2seq编码器-解码器。具体来说，我们将涵盖以下主题：
- en: Overview of extractive and abstractive text summarization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取式和抽象式文本摘要概述
- en: Building a seq2seq model with attention to summarize text
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用带有注意力机制的seq2seq模型进行文本摘要
- en: Improving summarization with beam search
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过束搜索改进摘要
- en: Addressing beam search issues with length normalizations
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过长度归一化解决束搜索问题
- en: Measuring the performance of summarization with ROUGE metrics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ROUGE指标衡量摘要性能
- en: A review of state-of-the-art summarization
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最新摘要技术的回顾
- en: The first step of this journey begins with understanding the main ideas behind
    text summarization. It is important to understand the task before building a model.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程的第一步是理解文本摘要背后的主要思想。在构建模型之前，理解任务本身至关重要。
- en: Overview of text summarization
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本摘要概述
- en: The core idea in summarization is to condense long-form text or articles into
    a short representation. The shorter representation should contain the main idea
    of crucial information from the longer form. A single document can be summarized.
    This document could be long or may contain just a couple of sentences. An example
    of a short document summarization is generating a headline from the first few
    sentences of an article. This is called **sentence compression**. When multiple
    documents are being summarized, they are usually related. They could be the financial
    reports of a company or news reports about an event. The generated summary could
    itself be long or short. A shorter summary would be desirable when generating
    a headline. A lengthier summary would be something like an abstract and could
    have multiple sentences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要的核心思想是将长篇文本或文章浓缩为简短的表示形式。简短的表示应包含长文本中的关键信息。单个文档可以被总结，这个文档可以很长，也可以只有几句话。一个短文档摘要的例子是从文章的前几句话生成标题。这被称为**句子压缩**。当总结多个文档时，它们通常是相关的。它们可以是公司财务报告，或者关于某个事件的新闻报道。生成的摘要可以长也可以短。当生成标题时，通常希望摘要较短；而较长的摘要则像摘要部分，可能包含多句话。
- en: 'There are two main approaches when summarizing text:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 总结文本时有两种主要的方法：
- en: '**Extractive summarization**: Phrases or sentences from the articles are selected
    and put together to create a summary. A mental model for this approach is using
    a highlighter on the long-form text, and the summary is the highlights put together.
    Extractive summarization is a more straightforward approach as sentences from
    the source text can be copied, which leads to fewer grammatical issues. The quality
    of the summarization is also easier to measure using metrics such as ROUGE. This
    metric is detailed later in this chapter. Extractive summarization was the predominant
    approach before deep learning and neural networks.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提取式摘要**：从文章中选取短语或句子，并将其组合成摘要。这种方法的思维模型类似于在长篇文本上使用荧光笔，摘要即是这些重点内容的组合。提取式摘要是一种更直接的方法，因为可以直接复制源文本中的句子，从而减少语法问题。摘要的质量也可以通过诸如ROUGE等指标来衡量。该指标将在本章后面详细介绍。提取式摘要在深度学习和神经网络出现之前是主要的方法。'
- en: '**Abstractive summarization**: A person may use the full vocabulary available
    in a language while summarizing an article. They are not restricted to only using
    words from the article. The mental model is that the person is penning a new piece
    of text. The model must have some understanding of the meaning of different words
    so that the model can use them in the summary. Abstractive summarization is quite
    hard to implement and evaluate. The advent of the seq2seq architecture made significant
    improvements to the quality of abstractive summarization models.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**抽象式摘要**：在摘要一篇文章时，个人可能会使用该语言中所有可用的词汇。他们不局限于仅使用文章中的词语。其心理模型是，人们正在撰写一篇新的文本。模型必须对不同词语的意义有所理解，才能在摘要中使用这些词汇。抽象式摘要相当难以实现和评估。Seq2Seq架构的出现大大提升了抽象式摘要模型的质量。'
- en: 'This chapter focuses on abstractive summarization. Here are some examples of
    summaries that our model can generate:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点讨论抽象式摘要。以下是我们模型生成的摘要示例：
- en: '| Source text | Generated summary |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 原文 | 生成的摘要 |'
- en: '| american airlines group inc said on sunday it plans to raise ## billion by
    selling shares and convertible senior notes , to improve the airline''s liquidity
    as it grapples with travel restrictions caused by the coronavirus . | american
    airlines to raise ## **bln** convertible **bond issue** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 美国航空集团公司周日表示，它计划通过出售股票和可转换优先票据筹集##亿美元，以改善航空公司在应对因冠状病毒引起的旅行限制时的流动性。| 美国航空将通过**可转换债券发行**筹集##**亿美元**
    |'
- en: '| sales of newly-built single-family houses occurred at a seasonally adjusted
    annual rate of ## in may , that represented a #.#% increase from the downwardly
    revised pace of ## in april . | **new home** sales **rise** in may |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 新建独栋住宅的销售在5月按季节调整后的年化速度为##，与4月经过下调修正后的##相比，增长了#.#%。| **新房**销售在5月**上涨** |'
- en: '| jc penney will close another ## stores for good . the department store chain
    , which filed for bankruptcy last month , is inching toward its target of closing
    ## stores . | jc penney to close **more** stores |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| JC Penney 将永久关闭更多##家门店。这家上个月申请破产的百货商店连锁，正朝着关闭##家门店的目标迈进。| JC Penney 将关闭**更多**门店
    |'
- en: The source text was pre-processed to be all in lowercase, and numbers were replaced
    with placeholder tokens to prevent the model from inventing numbers in the summary.
    The generated summaries have some words highlighted. Those words were not present
    in the source text. The model was able to propose these words in the summary.
    Thus, the model is an abstractive summarization model. So, how can such a model
    be built?
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 原文在预处理时已全部转换为小写，并且将数字替换为占位符标记，以防止模型在摘要中捏造数字。生成的摘要中有些词被高亮显示，这些词在原文中并未出现。模型能够在摘要中提出这些词。因此，模型是一个抽象式摘要模型。那么，如何构建这样一个模型呢？
- en: 'One way of looking at the summarization problem is that the model is *translating*
    an input sequence of tokens into a smaller set of output tokens. The model learns
    the output lengths based on the supervised examples provided. Another well-known
    problem is mapping an input sequence to an output sequence – the problem of Neural
    Machine Translation or NMT. In NMT, the input sequence could be a sentence from
    the source language, and the output could be a sequence of tokens in the target
    language. The process for translation is as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一种看待摘要生成问题的方法是，模型将输入的标记序列*转换*为较小的输出标记集合。模型根据提供的监督样本学习输出长度。另一个著名的问题是将输入序列映射到输出序列——即神经机器翻译（NMT）问题。在NMT中，输入序列可能是源语言中的一句话，输出则可能是目标语言中的一系列标记。翻译过程如下：
- en: Convert the input text into tokens
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入文本转换为标记
- en: Learn embeddings for these tokens
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为这些标记学习嵌入
- en: Pass the token embeddings through an encoder to calculate the hidden states
    and outputs
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过编码器传递标记嵌入，计算隐藏状态和输出
- en: Use the hidden states with the attention mechanism for generating a context
    vector for the inputs
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用带有注意力机制的隐藏状态生成输入的上下文向量
- en: Pass encoder outputs, hidden states, and context vectors to the decoder part
    of the network
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器输出、隐藏状态和上下文向量传递给网络的解码器部分
- en: Generate the outputs from left to right using an autoregressive model
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用自回归模型从左到右生成输出
- en: Google AI published a tutorial on NMT using a seq2seq attention model in July
    2017\. This model uses a left-to-right encoder with GRU cells. The Decoder also
    uses GRU cells. In summarization, the piece of text to be summarized is a prerequisite.
    This may or may not be valid for machine translation. In some cases, the translation
    is performed on the fly. In that case, a left-to-right encoder is useful. However,
    if the entire text to be translated or summarized is available from the outset,
    a bi-directional Encoder can encode context from both sides of a given token.
    BiRNN in the Encoder leads to much better performance of the overall model. The
    NMT tutorial code serves as inspiration for the seq2seq attention model and the
    attention tutorial referenced previously. Before we work on the model, let's look
    at the datasets that are used for this purpose.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Data loading and pre-processing
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several summarization-related datasets available for training. These
    datasets are available through the TensorFlow Datasets or `tfds` package, which
    we have used in the previous chapters as well. The datasets that are available
    differ in length and style. The CNN/DailyMail dataset is one of the most commonly
    used datasets. It was published in 2015, with approximately a total of 1 million
    news articles. Articles from CNN, starting in 2007, and Daily Mail, starting in
    2010, were collected until 2015\. The summaries are usually multi-sentence. The
    Newsroom dataset, available from [https://summari.es](https://summari.es), contains
    over 1.3 million news articles from 38 publications. However, this dataset requires
    that you register to download it, which is why it is not used in this book. The
    wikiHow data set contains full Wikipedia article pages and the summary sentences
    for those articles. The LCSTS data set contains Chinese language data collected
    from Sina Weibo with paragraphs and their one-sentence summaries.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Another popular dataset is the Gigaword dataset. It provides the first one or
    two sentences of a news story and has the headline of the story as the summary.
    This dataset is quite large, with just under 4 million rows. This dataset was
    published in a paper titled *Annotated Gigaword* by Napoles et al. in 2011\. It
    is quite easy to import this dataset using `tfds`. Given the large size of the
    dataset and long training times for the model, the training code is stored in
    Python files, while the inference code is in an IPython notebook. This pattern
    was used in the previous chapter as well. The code for training is in the `s2s-training.py`
    file. The top part of the file contains the imports and a method called `setupGPU()`
    to initialize the GPU. The file contains a main function, which provides the control
    flow, and several functions that perform specific actions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset needs to be loaded first. The code for loading the data is in the
    `load_data()` function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The corresponding section in the main function looks like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Only the training dataset is being loaded. The validation dataset contains
    approximately 190,000 examples, while the test split contains over 1,900 examples.
    In contrast, the training set contains over 3.8 million examples. Depending on
    the internet connection, downloading the dataset may take a while:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 仅加载训练数据集。验证数据集包含约 190,000 个样本，而测试集包含超过 1,900 个样本。相比之下，训练集包含超过 380 万个样本。根据网络连接情况，下载数据集可能需要一些时间：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The warning about insecure requests can be safely ignored. The data is now ready
    to be tokenized and vectorized.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不安全请求的警告可以安全忽略。数据现在已经准备好进行分词和向量化处理。
- en: Data tokenization and vectorization
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据分词和向量化
- en: 'The Gigaword dataset has been already cleaned, normalized, and tokenized using
    the StanfordNLP tokenizer. All the data is converted into lowercase and normalized
    using the StanfordNLP tokenizer, as seen in the preceding examples. The main task
    in this step is to create a vocabulary. A word-based tokenizer is the most common
    choice in summarization. However, we will use a subword tokenizer in this chapter.
    A subword tokenizer provides the benefit of limiting the size of the vocabulary
    while minimizing the number of unknown words. *Chapter 3*, *Named Entity Recognition
    (NER) with BiLSTMs, CRFs, and Viterbi Decoding*, on BERT, described different
    types of tokenizers. Consequently, models such specifically the part as BERT and
    GPT-2 use some variant of a subword tokenizer. The `tfds` package provides a way
    for us to create a subword tokenizer, initialized from a corpus of text. Since
    generating the vocabulary requires running it over all of the training data, this
    process can be slow. After initialization, the tokenizer can be persisted to disk
    for future use. The code for this process is defined in the `get_tokenizer()`
    function:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Gigaword 数据集已经使用 StanfordNLP 分词器进行了清洗、规范化和分词。所有数据都已转换为小写，并使用 StanfordNLP 分词器进行了规范化，正如前面的示例所示。本步骤的主要任务是创建词汇表。基于单词的分词器是摘要生成中最常见的选择。然而，本章将使用子词分词器。子词分词器的优点是可以在最小化未知词数量的同时限制词汇表的大小。*第3章*，*使用
    BiLSTMs、CRFs 和维特比解码进行命名实体识别（NER）*，介绍了不同类型的分词器。因此，像 BERT 和 GPT-2 这样的模型使用某种变体的子词分词器。`tfds`
    包提供了一种方法，可以从文本语料库初始化并创建一个子词分词器。由于生成词汇表需要遍历所有训练数据，因此这个过程可能比较慢。初始化后，分词器可以保存到磁盘以供将来使用。这个过程的代码在
    `get_tokenizer()` 函数中定义：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This method checks to see if a subword tokenizer is saved and loads it. If no
    tokenizer exists on disk, it creates one by feeding in the articles and summaries
    combined. Note that creating a new tokenizer took over 20 minutes on my machine.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法检查是否已保存子词分词器并加载它。如果磁盘上没有分词器，则通过将文章和摘要合并输入来创建一个新的分词器。请注意，在我的机器上创建新分词器花费了超过
    20 分钟。
- en: Hence, it is a good idea to do this process only once and persist the results
    for future use. The GitHub folder for this chapter contains a saved version of
    the tokenizer to save some of your time.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最好只执行一次这个过程，并将结果保存以供将来使用。本章的 GitHub 文件夹包含了已保存的分词器版本，以节省一些时间。
- en: 'Two additional tokens that denote the start and end of a sequence are added
    to the vocabulary after its creation. These tokens help the model start and end
    the inputs and outputs. The end of sequence token provides a way for the Decoder,
    which generates the summary, to signal the end of the summary. The main method
    at this point looks like so:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建词汇表后，会向其中添加两个额外的标记，表示序列的开始和结束。这些标记帮助模型开始和结束输入输出。结束标记为生成摘要的解码器提供了一种信号，表示摘要的结束。此时，主要方法如下所示：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Articles and their summaries can be tokenized using the tokenizer. Articles
    can be of varying lengths and will need to be truncated at a maximum length. A
    maximum token length of 128 has been chosen as the Gigaword dataset only contains
    a few sentences from the article. Note that 128 tokens are not the same as 128
    words due to the subword tokenizer. Using a subword tokenizer minimizes the presence
    of unknown tokens during summary generation.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 文章及其摘要可以使用分词器进行分词。文章的长度各异，需要在最大长度处进行截断。由于 Gigaword 数据集仅包含文章中的少数几句话，因此选择了最大令牌长度为
    128。请注意，128 个令牌并不等同于 128 个单词，因为使用的是子词分词器。使用子词分词器可以最小化生成摘要时出现未知令牌的情况。
- en: 'Once the tokenizer is ready, both the article and summary texts need to be
    tokenized. Since the summary will be fed to the Decoder one token at a time, the
    provided summary text will be shifted right by adding a `start` token, as shown
    previously. An `end` token will be appended to the summary to let the Decoder
    learn how to signal the end of the summary''s generation. The `encode()` method
    in the file `seq2seq.py` defines the vectorization step:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分词器准备好，文章和摘要文本都需要进行分词。由于摘要将一次性传递给解码器一个标记，提供的摘要文本会通过添加一个 `start` 标记向右偏移，正如之前所示。一个
    `end` 标记将被附加到摘要末尾，以便让解码器学会如何标识摘要生成的结束。文件 `seq2seq.py` 中的 `encode()` 方法定义了向量化步骤：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Since this is a Python function working on the contents of the text of tensors,
    another function needs to be defined. This can be passed to the dataset to be
    applied to all the rows of the data. This function is also defined in the same
    file as the `encode` function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个处理张量文本内容的 Python 函数，因此需要定义另一个函数。这个函数可以传递给数据集，以便应用到数据的所有行。这个函数也在与 `encode`
    函数相同的文件中定义：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Going back to the main function in the `s2s-training.py` file, the dataset
    can be vectorized with the help of the preceding functions like so:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 `s2s-training.py` 文件中的主函数，数据集可以借助之前的函数进行向量化，如下所示：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note that shuffling the dataset is recommended. By shuffling the dataset, it
    is easier for the model to converge and not overfit to batches. However, this
    adds to the training time. This has been commented out here as this is an optional
    step. Shuffling records in batches while training models for production use cases
    is recommended. The last step in preparing the data is batching it, as shown in
    the last step here. Now, we are ready to build the model and train it.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，建议对数据集进行打乱。通过打乱数据集，模型更容易收敛，并且不容易对批次过拟合。然而，这会增加训练时间。这里将其注释掉，因为这是一个可选步骤。在为生产用例训练模型时，建议在批次中打乱记录。准备数据的最后一步是将其分批，如此处的最后一步所示。现在，我们可以开始构建模型并进行训练。
- en: Seq2seq model with attention
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Seq2seq 模型与注意力机制
- en: 'The summarization model has an Encoder part with a bidirectional RNN and a
    unidirectional decoder part. There is an attention layer that helps the Decoder
    focus on specific parts of the input while generating an output token. The overall
    architecture is shown in the following diagram:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要模型有一个包含双向 RNN 的编码器部分和一个单向解码器部分。存在一个注意力层，它帮助解码器在生成输出标记时集中关注输入的特定部分。整体架构如下图所示：
- en: '![](img/B16252_06_01.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B16252_06_01.png)'
- en: 'Figure 6.1: Seq2seq and attention model'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：Seq2seq 和注意力模型
- en: 'These layers are detailed in the following subsections. All the code for these
    parts of the model are in the file `seq2seq.py`. All the layers use common hyperparameters
    specified in the main function in the `s2s-training.py` file:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层将在以下小节中详细介绍。模型的所有代码都在文件 `seq2seq.py` 中。这些层使用在 `s2s-training.py` 文件主函数中指定的通用超参数：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code and architecture for this section have been inspired by the paper
    titled *Get To The Point: Summarization with Pointer-Generator Networks* by Abigail
    See, Peter Liu, and Chris Manning, published in April 2017\. The fundamental architecture
    is easy to follow and provides impressive performance for a model that can be
    trained on a desktop with a commodity GPU.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '这一部分的代码和架构灵感来自2017年4月由 Abigail See、Peter Liu 和 Chris Manning 发表的论文 *Get To
    The Point: Summarization with Pointer-Generator Networks*。基础架构易于理解，并且对于可以在普通桌面
    GPU 上训练的模型提供了令人印象深刻的性能。'
- en: Encoder model
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器模型
- en: 'The detailed architecture of the Encoder layer is shown in the following diagram.
    Tokenized and vectorized input is fed through an embedding layer. Embeddings for
    the tokens generated by the tokenizer are learned from scratch. It is possible
    to use a set of pre-trained embeddings like GloVe and use the corresponding tokenizer.
    While using a pre-trained set of embeddings can help with the accuracy of the
    model, a word-based vocabulary would have many unknown tokens, as we saw in the
    IMDb example and GloVe vectors earlier. The unknown tokens would impact the ability
    of the model to create summaries with words it hasn''t seen before. If the summarization
    model is used on daily news, there can be several unknown words, like names of
    people, places, or new products:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_06_02.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: Encoder architecture'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The embedding layer has a dimension of 128, as configured in the hyperparameters.
    These hyperparameters have been chosen to resemble those in the paper. We then
    create an embedding singleton that can be used by both the Encoder and the Decoder.
    The code for the class is in the `seq2seq.py` file:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Input sequences will be padded to a fixed length of 128\. Hence, a masking
    parameter is passed to the embedding layer so that the embedding layer ignores
    the mask tokens. Next, let''s define an `Encoder` class and instantiate the embedding
    layer in the constructor:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The constructor takes a number of parameters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: '**Size of the vocabulary**: In the present case, this is 32,899 tokens.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding dimensions**: This is 128 dimensions. Feel free to experiment with
    a larger or smaller embedding dimension. Smaller dimensions would reduce the model''s
    size and memory required for training the model.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder units**: The number of forward and backward units in the bidirectional
    layer. 256 units will be used for a total of 512 units.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: The size of the input batches. 64 records will be in one batch.
    A larger batch would make training go faster but would need more memory on the
    GPU. So, this number can be adjusted based on the capacity of the training hardware.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The output of the embedding layer is fed to a bidirectional RNN layer. There
    are 256 GRU units in each direction. The bidirectional layer in Keras provides
    options on how to combine the output of the forward and backward layer. In this
    case, we concatenate the outputs of the forward and backward GRU cells. Hence,
    the output will be 512-dimensional. Furthermore, the hidden states are also needed
    for the attention mechanism to work, so a parameter is passed to retrieve the
    output states. The bidirectional GRU layer is configured like so:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A dense layer with ReLU activation is also set up. The two layers return their
    hidden layers. However, the Decoder and attention layers require one vector of
    hidden states. We pass the hidden states through the dense layer and convert the
    dimensions from 512 into 256, which is expected by the Decoder and attention modules.
    This completes the constructor for the Encoder class. Given this is a custom model
    with specific ways to compute the model, a `call()` method is defined that operates
    on a batch of inputs to produce the output and hidden states. This method takes
    in hidden states to seed the bidirectional layer:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'First, the input is passed through the embedding layer. The output is fed to
    the bidirectional layer, and the output and hidden states are retrieved. The two
    hidden states are concatenated and fed through the dense layer to create the output
    hidden state. Lastly, a utility method to return initial hidden states is defined:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This completes the code for the Encoder. Before going into the Decoder, an attention
    layer needs to be defined, which will be used in the Decoder. Bahdanau's attention
    formulation will be used for this. Note that TensorFlow/Keras does not provide
    an attention layer out of the box. However, this simple attention layer code should
    be entirely reusable.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: Bahdanau attention layer
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Bahdanau et al. published this form of global attention in 2015\. It has been
    widely used in Transformer models, as we saw in the previous chapters. Now, we
    are going to implement an attention layer from scratch. This part of the code
    is inspired by the NMT tutorial published by the TensorFlow team.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The core idea behind attention is to let the Decoder see all the inputs and
    focus on the most relevant inputs while predicting the output token. A global
    attention mechanism allows the Decoder to see all the inputs. This global version
    of the attention mechanism will be implemented. At an abstract level, the purpose
    of the attention mechanism maps a set of values to a given query. It does this
    by providing a relevance score of each of these values for a given query.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'In our case, the query is the Decoder''s hidden state, and the values are the
    Encoder outputs. We are interested in figuring out which inputs can best help
    in generating the next token from the Decoder. The first step is computing a score
    using the Encoder output and the Decoder''s previous hidden state. If this is
    the first step of decoding, then the hidden states from the Encoder are used to
    seed the Decoder. A corresponding weight matrix is multiplied by the Encoder''s
    output and Decoder''s hidden state. The output is passed through a *tanh* activation
    function and multiplied by another weight matrix to produce the final score. The
    following equation shows this formulation:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_06_001.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
- en: 'Matrices *V*, *W*[1], and *W*[2] are trainable. Then, to understand the alignment
    between the Decoder output and the Encoder outputs, a softmax is computed:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_06_002.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
- en: 'The last step is to produce a context vector. The context vector is produced
    by multiplying the attention weights by the Encoder outputs:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_06_003.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
- en: These are all the computations in the attention layer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is setting up the constructor for the attention class:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `call()` method of the `BahdanauAttention` class implements the equations
    shown previously with some additional code to manage the tensor shapes. This is
    shown here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The only thing we have left to do is implement the Decoder model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: Decoder model
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The detailed Decoder model is shown in the following diagram:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_06_03.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: Detailed decoder architecture'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Hidden states from the Encoder are used to initialize the hidden states of the
    Decoder. The start token initiates the summaries being generated. The hidden states
    of the Decoder, along with the Encoder output, are used to compute the attention
    weights and the context vector. The context vector, along with the embeddings
    of the output token, are concatenated and passed through the unidirectional GRU
    cell. The output of the GRU cell is passed through a dense layer, with a softmax
    activation function to get the output token. This process is repeated token by
    token.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: Note that the Decoder functions differently during training and inference. During
    training, the output token from the Decoder is used to calculate the loss but
    is not fed back into the Decoder to produce the next token. Instead, the next
    token from the ground truth is fed into the Decoder at each time step. This process
    is called **teacher forcing**. The output tokens generated by the Decoder are
    only fed back in during inference when summaries are being generated.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'A `Decoder` class is defined in the `seq2seq.py` file. The constructor for
    this class sets up the dimensions and the various layers:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The embedding layer in the Decoder is not shared with the Encoder. This is a
    design choice. It is common in summarization to use a shared embedding layer.
    The structure of the articles and their summaries is slightly different in the
    Gigaword dataset as news headlines are not proper sentences but fragments of sentences.
    During training, using different embedding layers gave better results than shared
    embeddings. It is possible that, on the CNN/DailyMail dataset, shared embeddings
    give better results than on the Gigaword dataset. In the case of machine translation,
    the Encoder and Decoder are seeing different languages, so having separate embedding
    layers is a best practice. You are encouraged to try out both versions on different
    datasets and build your own intuition. The preceding commented code makes it easy
    to switch back and forth between shared and separate embeddings between the Encoder
    and Decoder.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'The next part of the Decoder is the computation that calculates the output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The computation is fairly straightforward. The model looks like this:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The Encoder model contains 4.9M parameters, while the Decoder model contains
    13.5M parameters for a total of 18.4M parameters. Now, we are ready to train the
    model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: Training the model
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of steps to be performed in training that require a custom
    training loop. First, let''s define a method that executes one step of the training
    loop. This method is defined in the `s2s-training.py` file:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This is a custom training loop that uses `GradientTape`, which tracks the different
    variables of the model and calculates the gradients. The preceding function runs
    once for each batch of inputs. Inputs are passed through the Encoder to get the
    final encoding and the last hidden state. The Decoder is initialized with the
    last Encoder hidden state, and summaries are generated one token at a time. However,
    the generated token is not fed back into the Decoder. Instead, the actual token
    is fed back. This method is known as **Teacher Forcing**. A custom loss function
    is defined in the `seq2seq.py` file:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The key to the loss function is to use a mask to handle summaries of varying
    lengths. The last part of the model is using an optimizer. The Adam optimizer
    is being used here, with a learning rate schedule that reduces the learning rate
    over epochs of training. The concept of learning rate annealing was covered in
    previous chapters. The code for the optimizer is inside the main function in the
    `s2s-training.py` file:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Since the model is going to be trained for a long time, it is important to
    set up checkpoints that can be used to restart training in case issues occur.
    Checkpoints also provide us with an opportunity to adjust some of the training
    parameters across runs. The next part of the main function sets up the checkpointing
    system. We looked at checkpoints in the previous chapter. We will extend what
    we''ve learned and set up an optional command-line argument that specifies if
    training needs to be restarted from a specific checkpoint:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: If training needs to be restarted from a checkpoint, then a command-line argument
    in the form `–-checkpoint <dir>` can be specified while invoking the training
    script. If no argument is supplied, then a new checkpoint directory will be created.
    Training with 1.5M records takes over 3 hours. Running 10 iterations will take
    over a day and a half. The Pointer-Generator model we referenced earlier in this
    chapter was trained for 33 epochs, which took over 4 days of training. However,
    it is possible to see some results after 4 epochs of training.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the last part of the main function is to start the training process:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The training loop prints the loss every 100 batches and saves a checkpoint
    every second epoch. Feel free to adjust these settings as needed. The following
    command can be used to start training:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output of this script should be something similar to:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This sample run used only 2,000 samples since we edited this line:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'If training is being restarted from a checkpoint, then the command line will
    be:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: With this comment, the model is hydrated from the checkpoint directory we used
    in the training step. Training continues from that point. Once the model has finished
    training, we are ready to generate the summaries. Note that the model we'll be
    using in the next section was trained for 8 epochs with 1.5M records. Using all
    3.8M records and training for more epochs would give better results.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Generating summaries
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The critical thing to note while generating summaries is that a new inference
    loop will need to be built. Recall that *teacher forcing* was used during training,
    and the output of the Decoder was not used in predicting the next token. While
    generating summaries, we would like to use the generated tokens in predicting
    the next token. Since we would like to play with various input texts and generate
    summaries, we will use the code in the `generating-summaries.ipynb` IPython notebook.
    After importing and setting everything up, the tokenizer needs to be instantiated.
    The *Setup Tokenization* section of the notebook loads the tokenizers and sets
    up the vocabulary by adding start and end token IDs. Similar to when we loaded
    the data, the data encoding method is set up to encode the input articles.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we must hydrate the model from the saved checkpoint. All of the model
    objects are created first:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, a checkpoint with the appropriate checkpoint directory is defined:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Then, the last checkpoint is checked:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Since checkpoints are stored after every alternate epoch, this checkpoint corresponds
    to 8 epochs of training. Checkpoints can be loaded and tested with the following
    code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: That's it! The model is now ready for inference.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '**Checkpoints and variable names**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible that the second command may give an error if it cannot match
    the names of the variables in the checkpoint with the names in the model. This
    can happen as we did not explicitly name the layers when they were instantiated
    in the model. TensorFlow will provide a dynamically generated name for the layer
    when the model is instantiated:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Variable names in the checkpoint can be inspected with:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'If the model is instantiated again, these names may change, and restore from
    checkpoint may fail. There are two solutions to prevent this. A quick fix is to
    restart the notebook kernel. A better fix is to edit the code and add names to
    each layer in the Encoder and Decoder constructors before training. This ensures
    that checkpoints will always find the variables. An example of this approach is
    shown for the `fc1` layer in the Decoder:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Inference can be done via the greedy search or beam search algorithms. Both
    of these methods will be demonstrated here. Before going into the code for generating
    summaries, a convenience method for plotting attention weights will be defined.
    This helps in providing some intuition on what inputs contributed to a given token
    being generated in the summary:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: A plot is configured with the input sequence as the columns and the output summary
    tokens as the rows. Feel free to play with different color scales to get a better
    idea of the strength of the association between the tokens.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: We have covered much ground and possibly trained a network for hours. It is
    time to see the fruits of our labor!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Greedy search
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Greedy search uses the highest probability token at each time step to construct
    the sequence. The predicted token is fed back into the model to generate the next
    token. This is the same model that was used in the previous chapter while generating
    characters in the char-RNN model:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The first part of the code encodes the inputs the same way they were encoded
    during training. These inputs are passed through the Encoder to the final encoder
    output and the last hidden state. The Decoder''s initial hidden state is set to
    the last hidden state of the Encoder. Now, the process of generating the output
    tokens begins. First, the inputs are fed to the Decoder, which generates a prediction,
    the hidden state, and the attention weights. Attention weights are added to a
    running list of attention weights per time step. This generation continues until
    whichever comes earlier; producing an end-of-sequence token, or producing 50 tokens.
    The resulting summary and attention plot are returned. A summarization method
    is defined, which calls this greedy search algorithm, plots the attention weights,
    and converts the generated tokens into proper words:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The preceding method has a spot where we can plug in beam search later. Let''s
    test the model:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![](img/B16252_06_04.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Attention plot for an example summary'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the generated summary:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '*bulgarian president summons french ambassador over remarks on iraq*'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: It is a pretty good summary! The most surprising part is that the model was
    able to identify the Bulgarian president, even though Bulgaria is not mentioned
    anywhere in the source text. It contains other words not found in the original
    text. These are highlighted in the preceding output. The model was able to change
    the tense of the word *summoned* to *summons*. The word *remarks* never appears
    in the source text. The model was able to infer this from a number of input tokens.
    The notebook contains many examples, both good and bad, of summaries generated
    by the model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of a piece of challenging text for the model:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: *charles kennedy , leader of britain''s third-ranked liberal democrats
    , announced saturday he was quitting with immediate effect and would not stand
    in a new leadership election . us president george w. bush on saturday called
    for extending tax cuts adopted in his first term , which he said had bolstered
    economic growth.*'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicted summary**: *kennedy quits to be a step toward new term*'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this article, there are two seemingly unrelated sentences. The model is
    trying to make sense of them but messes it up. There are other examples where
    the model doesn''t do so well:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: *jc penney will close another ## stores for good . the department
    store chain , which filed for bankruptcy last month , is inching toward its target
    of closing ## stores.*'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicted summary**: *jc penney to close another ## stores for #nd stores*'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, the model repeats itself, attending to the same positions.
    In fact, this is a common problem with summarization models. One solution to prevent
    repetition is to add coverage loss. Coverage loss keeps a running total of the
    attention weights across time steps and feeds it back to the attention mechanism,
    as a way to clue it in to previously attended positions. Furthermore, coverage
    loss terms are added to the overall loss equation to penalize repetition. Training
    the model for much longer would also help in this particular case. Note that Transformer-based
    models suffer a little less from repetition.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'The second example is the model inventing something:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: *the german engineering giant siemens is working on a revamped version
    of its defective tram car , of which the ### units sold so far worldwide are being
    recalled owing to a technical fault , a company spokeswoman said on tuesday.*'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Predicted summary**: *siemens to launch reb-made cars*'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The model invents *reb-made*, which is incorrect:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_06_05.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: Model invents the word "reb-made"'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the preceding attention plot, the new word is being generated by
    attending to *revamped*, *version*, *defective*, and *tram*. This made-up word
    garbles the summary generated.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: As noted earlier, using beam search can help in further improving the accuracy
    of the translations. We will try some of these challenging examples after implementing
    the beam search algorithm.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Beam search
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Beam search uses multiple paths or beams to generate tokens and tries to minimize
    the overall conditional probability. At each time step, all the options are evaluated,
    and the cumulative conditional probabilities are evaluated over all the time steps
    so far. Only the top *k* beams, where *k* is the beam width, are kept; the rest
    are pruned for the next time step. Greedy search is a special case of beam search
    with a beam width of 1\. In fact, this property serves as a test case for the
    beam search algorithm. The code for this section can be found in the *Beam Search*
    section of the IPython notebook.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'A new method called `beam_search()` is defined. The first part of this method
    is similar to greedy search, where inputs are tokenized and passed through the
    Encoder. The main difference between this algorithm and the greedy search algorithm
    is the core loop, which processes one token at a time. In beam search, a token
    needs to be generated for every beam. This makes beam search slower than greedy
    search, and running time increases in proportion to beam width. At each time step,
    for each of the *k* beams, the top *k* tokens are generated, sorted, and pruned
    back to *k* items. This step is performed until each beam generates an end of
    sequence token or has generated the maximum number of tokens. If there are *m*
    tokens to be generated, then beam search would require *k * m* runs of the Decoder
    to generate the output sequence. The main loop is shown in the following code:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'At the start, there is only one beam within the start token. A list to keep
    track of the beams generated is then defined. The list of tuples stores the attention
    plots, tokens, last hidden state, and the overall cost of the beam. Conditional
    probability requires a product of all probabilities. Given that all probabilities
    are numbers between 0 and 1, the conditional probability could become very small.
    Instead, logs of the probabilities are added together, as shown in the preceding
    highlighted code. The best beams minimize this score. Finally, a small section
    is inserted that prints all the top beams with their scores once the function
    completes its execution. This part is optional and can be removed:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'At the end, the function returns the best beam:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The `summarize()` method is extended so that you can generate greedy and beam
    search, like so:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let''s re-run the Siemens tram car example:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy search summary**: *siemens to launch reb-made cars*'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beam search summary**: *siemens working on revamped european tram car*'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The beam search summary contains more detail and represents the text better.
    It introduces a new word, *european*, which may or may not be accurate in the
    current context. Contrast the following attention plot with the one shown previously:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_06_06.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.6: Attention plot of summary generated by beam search'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'The summary generated by beam search covers more concepts from the source text.
    For the JC Penney example, beam search makes the output better:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy search summary**: *jc penney to close another ## stores stores for
    #nd stores*'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beam search summary**: *jc penney to close ## more stores*'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The beam search summary is more concise and grammatically correct. These examples
    were generated with a beam width of 3\. The notebook contains several other examples
    for you to play with. You will notice that generally, beam search improves the
    results, but it reduces the length of the output. Beam search suffers from issues
    where the score of sequences is not normalized for the sequence length, and repeatedly
    attending to the same input tokens has no penalty.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: The most significant improvement at this point will come from training the model
    for longer and on more examples. The model that was used for these examples was
    trained for 22 epochs on 1.5M samples out of 3.8M from the Gigaword dataset. However,
    it is important to have beam search and various penalties in your back pocket
    to improve the quality of your model.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: There are two specific penalties that address these issues, both of which will
    be discussed in the next section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Decoding penalties with beam search
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Wu et al. proposed two penalties in the seminal paper *Google''s Neural Machine
    Translation System*, published in 2016\. These penalties are:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '**Length normalization**: Aimed at encouraging longer or short summaries.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coverage normalization**: Aimed at penalizing generation if the output focuses
    too much on the same part of the input sequence. As per the pointer-generator
    paper, this is best added during training for the last few iterations of training.
    This will not be implemented in this section.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These methods are inspired by NMT and must be adapted for the needs of summarization.
    At a high level, the score can be represented by the following formula:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_06_004.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: 'For example, the beam search algorithm naturally produces shorter sequences.
    The length penalty is important for NMT as the output sequence should address
    the input text. This is different from summarization, where shorter outputs are
    preferred. Length normalization computes a factor based on a parameter and the
    current token number. The cost of the beam is divided by this factor to calculate
    a length-normalized score. The paper proposes the following empirical formula:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B16252_06_005.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Smaller values of alpha produce shorter sequences, and larger values produce
    longer sequences. Values of ![](img/B16252_06_006.png) are between 0 and 1\. The
    conditional probability score is divided by the preceding quantity to give the
    normalized score for a beam. The `length_wu()` method normalizes the score using
    this parameter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that all the code for this part is in the *Beam Search with Length Normalizations*
    section of the notebook:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'It is easy to implement in the code. A new beam search method with normalizations
    is created. Most of the code is the same as in the previous implementation. The
    key change for enabling length normalization involves adding an alpha parameter
    to the method signature and updating the computation of the score so that it uses
    the aforementioned method:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Next, the score is normalized like so (around line 60 in the code):'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Let''s try the settings out on some examples. First, we will try placing a
    length normalization penalty on the Siemens example:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '**Greedy search**: *siemens to launch reb-made cars*'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beam search**: *siemens working on revamped european tram car*'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beam search with length penalties**: *siemens working on new version of defective
    tram car*'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A beam size of 5 and an alpha of 0.8 was used to generate the preceding example.
    Length normalization generates longer summaries, which corrects some of the challenges
    that are faced by the summaries generated purely by beam search:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a cell phone  Description automatically generated](img/B16252_06_07.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.7: Beam search with length normalization produces a great summary'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at a more complex contemporary example, which is not
    in the training set at all:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: *the uk on friday said that it would allow a quarantine-free international
    travel to some low-risk countries falling in its green zone list of an estimated
    ## nations . uk transport secretary said that the us will fall within the red
    zone*'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Greedy search**: *uk to allow free travel to low-risk countries*'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beam search**: *britain to allow free travel to low-risk countries*'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beam search with length normalization**: *britain to allow quarantines free
    travel to low-risk countries*'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The best summary uses beam search and length normalization. Note that beam search
    alone was removing a very important word, "quarantines", before "free travel."
    This changed the meaning of the summary. With length normalization, the summary
    contains all the right details. Note that the Gigaword dataset has very short summaries
    in general, and beam search is making them even shorter. Hence, we use larger
    values of alpha. Generally, smaller values of alpha are used for summarization
    and larger values for NMT. You can try different values of the length normalization
    parameter and beam width to build some intuition. Note that the formulation for
    the length penalty was empirical. It should also be experimented with.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: The penalty adds a new parameter that needs to be tuned in addition to beam
    size. Selecting the right parameters requires a better way of evaluating summaries
    than human inspection. This is the focus of the next section.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating summaries
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When people write summaries, they use inventive language. Human-written summaries
    often use words that are not present in the vocabulary of the text being summarized.
    When models generate abstractive summaries, they may also use words that are different
    from the words used in the ground truth summaries provided. There is no real way
    to do an effective semantic comparison of the ground truth summary and the generated
    summary. In summarization problems, a human evaluation step is often involved,
    which is where a qualitative check of the generated summaries is done. This method
    is both unscalable and expensive. There are approximations that uses n-gram overlaps
    and the longest common subsequence matches after stemming and lemmatization. The
    hope is that such pre-processing helps bring ground truth and generated summaries
    closer together for evaluation. The most common metric used for evaluating summaries
    is **Recall-Oriented Understudy for Gisting Evaluation**, also referred to as
    **ROUGE**. In machine translation, metrics such as **Bilingual Evaluation Understudy**
    (**BLEU**) and **Metric for Evaluation of Translation with Explicit Ordering**
    (**METEOR**) are used. BLEU relies mainly on precision, as precision is very important
    for translation. In summarization, recall is more important. Consequently, ROUGE
    is the metric of choice for evaluating summarization models. It was proposed by
    Chin-Yew Lin in 2004 in a paper titled *Rouge: A Package for Automatic Evaluation
    of Summaries*.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: ROUGE metric evaluation
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A summary that''s generated by a model should be readable, coherent, and factually
    correct. In addition, it should be grammatically correct. Human evaluation of
    summaries can be a mammoth task. If a person took 30 seconds to evaluate one summary
    in the Gigaword dataset, then it would take over 26 hours for one person to check
    the validation set. Since abstractive summaries are being generated, this human
    evaluation work will need to be done every time summaries are produced. The ROUGE
    metric tries to measure various aspects of an abstractive summary. It is a collection
    of four metrics:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '**ROUGE-N** is the n-gram recall between a generated summary and the ground
    truth or reference summary. "N" at the end of the name specifies the length of
    the n-gram. It is common to report ROUGE-1 and ROUGE-2\. The metric is calculated
    as the ratio of matching n-grams between the ground truth summary and the generated
    summary, divided by the total number of n-grams in the ground truth. This formulation
    is oriented toward recall. If multiple reference summaries exist, the ROUGE-N
    metric is calculated pairwise for each reference summary, and the maximum score
    is taken. In our example, only one reference summary exists.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROUGE-L** uses the **longest common subsequence** (**LCS**) between the generated
    summary and the ground truth to calculate the metric. Often, the sequences are
    stemmed prior to computing the LCS. Once the length of the LCS is known, precision
    is calculated by dividing it by the length of the reference summary; recall is
    calculated by dividing by the length of the generated score. The F1 score, which
    is the harmonic mean of precision and recall, is also calculated and reported.
    The F1 score provides a way for us to balance precision and recall. Since the
    LCS already includes common n-grams, choosing an n-gram length is not required.
    This particular version of ROUGE-L is called the sentence-level LCS score. There
    is a summary-level score as well, for cases when the summary contains more than
    one sentence. It is used for the CNN and DailyMail datasets, among others. The
    summary-level score matches each sentence in the ground truth with all the generated
    sentences to calculate the union LCS precision and recall. Details of the method
    can be found in the paper referenced previously.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROUGE-W** is a weighted version of the previous metric, where contiguous
    matches in the LCS are weighted higher than if the tokens were separated by some
    other tokens in the middle.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROUGE-S** uses skip-bigram co-occurrence statistics. A skip-bigram allows
    there to be arbitrary gaps between two tokens. Precision and recall are calculated
    using this measure.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The paper that proposed these metrics also contained code, in Perl, for calculating
    these metrics. This requires generating text files with references and generating
    summaries. Google Research has published a full Python implementation that is
    available from their GitHub repository: [https://github.com/google-research/google-research](https://github.com/google-research/google-research).
    The `rouge/` directory contains the code for these metrics. Please follow the
    installation instructions from the repository. Once installed, we can evaluate
    greedy search, beam search, and beam search with length normalization to judge
    their quality using the ROUGE-L metric. The code for this part is in the ROUGE
    Evaluation section.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'The scorer library can be imported and initialized like so:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'A version of the `summarize()` method, called `summarize_quietly()`, is used
    to summarize pieces of text without printing any outputs like attention plots.
    Random samples from the validation test will be used to measure the performance.
    The code for loading the data and the quiet summarization method can be found
    in the notebook and should be run prior to running metrics. Evaluation can be
    run using a greedy search, as shown in the following code fragment:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'While the validation set contains close to 190,000 records, the preceding code
    runs metrics on 1,000 records. The code also randomly prints out summaries for
    about 1% of the samples. The results of this evaluation should look similar to
    these:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This is not a bad start since we have high precision, but the recall is low.
    The current leaderboard for the Gigaword dataset has 36.74 as the highest ROUGE-L
    F1 score, as per paperswithcode.com. Let''s run the same test with beam search
    and see the results. The code here is identical to the preceding code, with the
    only difference being that a beam width of 3 is being used:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'It seems that the precision has improved considerably at the expense of recall.
    Overall, the F1 score shows a slight decrease. Beam search does produce shorter
    summaries, which could be the reason for the decrease in recall. Adjusting length
    normalization could help with this. Another hypothesis could be to try bigger
    beams. Trying a bigger beam size of 5 produces this result:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'There is a significant improvement in precision and a further decrease in recall.
    Now, let''s try some length normalization. Running beam search with an alpha of
    0.7 gives us the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'By running a larger beam width of 5 with the same alpha, we obtain this result:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: There is a considerable increase in recall due to there being a decline in precision.
    Overall, for a basic model trained only on a slice of data, the performance is
    quite good. A score of 27.3 would yield a spot on the top 20 of the leaderboard.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Seq2seq-based text summarization was the main approach prior to the advent of
    Transformer-based models. Now, Transformer-based models, which include both the Encoder
    and Decoder parts, are used for summarization. The next section reviews state-of-the-art
    approaches to summarization.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Summarization – state of the art
  id: totrans-260
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, the predominant approach to summarization uses the full Transformer
    architecture. Such models are quite big, often ranging from 223M parameters to
    over a billion in the case of GPT-3\. Google Research published a paper at ICML
    in June 2020 titled *PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive
    Summarization*. This paper sets the benchmark for state-of-the-art results as
    of the time of writing. The key innovation proposed by this model is a specific
    pre-training objective for summarization. Recall that BERT was pre-trained using
    a **masked language model** (**MLM**) objective, where tokens were randomly masked
    and the model had to predict them. The PEGASUS model proposed a **Gap Sentence
    Generation** (**GSG**) pre-training objective, where important sentences are completely
    replaced with a special masking token, and the model has to generate the sequence.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: The importance of the sentence is judged using the ROUGE1-F1 score of a given
    score compared to the entire document. A certain number of top-scoring sentences
    are masked from the input, and the model needs to predict them. Additional details
    can be found in the aforementioned paper. The base Transformer model is very similar
    to the BERT configurations. The pre-training objective makes a significant difference
    to the ROUGE1/2/L-F1 scores and sets new records on many of the datasets.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: These models are quite large and training them on a desktop is not realistic.
    Often, the models are pre-trained on humongous datasets for several days at a
    time. Thankfully, pre-trained versions of such models are available through libraries
    like HuggingFace.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Summarizing text is considered a uniquely human trait. Deep learning NLP models
    have made great strides in this area in the past 2-3 years. Summarization remains
    a very hot area of research within many applications. In this chapter, we built
    a seq2seq model from scratch that can summarize sentences from news articles and
    generate a headline. This model obtains fairly good results due to its simplicity.
    We were able to train the model for a long period of time due to learning rate
    annealing. By checkpointing the model, training was made resilient as it could
    be restarted from the last checkpoint in case of failure. Post-training, we improved
    our generated summaries through a custom implementation of beam search. As beam
    search has a tendency to provide short summaries, length normalization techniques
    were used to make the summaries even better.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'Measuring the quality of generated summaries is a challenge in abstractive
    summarization. Here is a random example from the validation dataset:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '**Input**: *the french soccer star david ginola on saturday launched his anti-land
    mines campaign on behalf of the international committee for the red cross which
    has taken him on as a sort of poster boy for the cause .*'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ground truth**: *soccer star joins red cross effort against land mines*'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Beam search (5/0.7)**: *former french star ginola launches anti-land mine
    campaign*'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The generated summary is very comparable to the ground truth. However, matching
    token by token would give us a very low score. ROUGE metrics that use n-grams
    and the LCS allow us to measure the quality of the summaries.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we took a quick look at the current state-of-the-art models for summarization.
    Large models that are pre-trained on even larger datasets are ruling the roost.
    Unfortunately, training a model of such size is often beyond the resources of
    a single individual.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move on to a very new and exciting area of research – multi-modal
    networks. Thus far, we have only treated text in isolation. But is a picture really
    worth a thousand words? We shall find out when we try to caption images and answer
    questions about them in the next chapter.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
