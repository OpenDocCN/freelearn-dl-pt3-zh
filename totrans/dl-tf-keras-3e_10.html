<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer441">
<h1 class="chapterNumber">10</h1>
<h1 class="chapterTitle" id="_idParaDest-262">Self-Supervised Learning</h1>
<p class="normal">Imagine that you are in the middle of the ocean, and you are thirsty. There is water all around you, but you cannot drink any of it. But what if you had the resources to boil the salt out of the water and thereby make it drinkable? Of course, the energy costs associated with the process can be quite high, so you will likely use the process in moderation. However, if your energy costs effectively became free, for example, if you were harnessing the power of the sun, the process might be more attractive for you to do on a larger scale.</p>
<p class="normal">In our somewhat simplistic situation described above, the first scenario is roughly analogous to supervised learning, and the second to the class of unsupervised / semi-supervised learning techniques we will cover in this chapter. The biggest problem with supervised learning techniques is the time and expense associated with the collection of labeled training data. As a result, labeled datasets are often relatively small.</p>
<p class="normal">Deep learning trades off computation against manual feature engineering, and while this can be very effective, deep learning models typically need more data to train than traditional (non-deep learning) models. Deep learning models tend to be more complex and have more learnable parameters, which results in them performing better at various tasks. However, more complex models also require more data to train. Because the creation of training data is expensive, this effectively limits us from scaling up Deep learning models using supervised learning.</p>
<p class="normal">Unfortunately, completely unsupervised learning techniques that do not need labeled data have had limited success so far. Self-supervised techniques that leverage the structure of data in the wild to create labeled data to feed supervised learning models offer a middle ground. In this chapter, we will learn about various self-supervised techniques and some of their applications in the areas of natural language processing, computer vision, and audio signal processing.</p>
<p class="normal">The chapter covers the following topics:</p>
<ul>
<li class="bulletList">Previous work</li>
<li class="bulletList">Self-supervised learning</li>
<li class="bulletList">Self-prediction</li>
<li class="bulletList">Contrastive learning</li>
<li class="bulletList">Pretext tasks</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at https://packt.link/dltfchp10</p>
</div>
<p class="normal">Self-supervised learning is the process of imaginatively reusing labels that already exist implicitly in your data. In this chapter, we will learn about some common strategies for self-supervised learning and examples of their use to solve real-life problems. Let’s begin.</p>
<h1 class="heading-1" id="_idParaDest-263">Previous work</h1>
<p class="normal">Self-supervised learning is not a new concept. However, the term became popular with the advent of transformer-based models such as BERT and GPT-2, which were trained in a semi-supervised manner on large quantities of unlabeled text. In the past, self-supervised learning was often labeled as unsupervised learning. However, there were many earlier models that attempted to leverage regularities in the input data to produce results comparable to that using supervised learning. You have encountered some of them in previous chapters already, but we will briefly cover them again in this section.</p>
<p class="normal">The <strong class="keyWord">Restricted Boltzmann Machine</strong> (<strong class="keyWord">RBM</strong>) is a generative neural model that can learn<a id="_idIndexMarker1012"/> a probability distribution over its inputs. It was invented in 1986 and subsequently improved in the mid-2000s. It can be trained in either supervised or unsupervised mode and can be applied to many downstream tasks, such as dimensionality reduction, classification, etc.</p>
<p class="normal"><strong class="keyWord">Autoencoders</strong> (<strong class="keyWord">AEs</strong>) are unsupervised learning models that attempt<a id="_idIndexMarker1013"/> to learn an efficient latent representation of input data by learning to reconstruct its input. The latent representation can be used to encode the input for downstream tasks. There are several variants of the model. Sparse, denoising, and contrastive AEs are effective in learning representations for downstream classification tasks, whereas variational AEs are more useful as generative models.</p>
<p class="normal">The Word2Vec model is another great example of what we would now call self-supervised learning. The CBOW and skip-gram models used to build the latent representation of words in a corpus, attempt to learn mappings of neighbors to words and words to neighbors respectively. However, the latent representation can now be used as word embeddings for a variety of downstream tasks. Similarly, the GloVe model is also a self-supervised model, which uses word co-occurrences and matrix factorization to generate word embeddings useful for downstream tasks.</p>
<p class="normal"><strong class="keyWord">Autoregressive</strong> (<strong class="keyWord">AR</strong>) models predict future behavior based<a id="_idIndexMarker1014"/> on past behavior. We cover them in this chapter in the <em class="italic">Self-prediction</em> section. However, AR models have their roots in time series analysis<a id="_idIndexMarker1015"/> in statistics, hidden Markov models in pre-neural natural language processing, <strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>) in neural (but pre-transformer) NLP.</p>
<p class="normal"><strong class="keyWord">Contrastive Learning</strong> (<strong class="keyWord">CL</strong>) models try to learn representations whereby<a id="_idIndexMarker1016"/> similar pairs of items cluster together and dissimilar pairs are pushed far apart. </p>
<p class="normal">CL models are also<a id="_idIndexMarker1017"/> covered in this chapter in the <em class="italic">Contrastive learning</em> section. However, <strong class="keyWord">Self Organizing Maps</strong> (<strong class="keyWord">SOMs</strong>) and Siamese networks use very similar ideas and may have been a precursor of current CL models.</p>
<h1 class="heading-1" id="_idParaDest-264">Self-supervised learning</h1>
<p class="normal">In self-supervised learning, the network is trained using supervised<a id="_idIndexMarker1018"/> learning, but the labels are obtained in an automated manner by leveraging some property of the data and without human labeling effort. Usually, this automation is achieved by leveraging how parts of the data sample interact with each other and learning to predict that. In other words, the data itself provides the supervision for the learning process.</p>
<p class="normal">One class of techniques involves leveraging co-occurrences within parts of the same data sample or co-occurrences between the same data sample at different points in time. These techniques are discussed in more detail in the <em class="italic">Self-prediction</em> section.</p>
<p class="normal">Another class of techniques involves leveraging co-occurring modality for a given data sample, for example, between a piece of text and its associated audio stream, or an image and its caption. Examples of this technique are discussed in the sections on joint learning.</p>
<p class="normal">Yet another class of self-supervised learning techniques involves exploiting relationships between pairs of data samples. These pairs are selected from the dataset based on some domain-level heuristic. Examples of these techniques are covered in the <em class="italic">Contrastive learning</em> section.</p>
<p class="normal">These techniques can either<a id="_idIndexMarker1019"/> be used to train a model to learn to solve a business task (such as sentiment analysis, classification, etc.) directly or to learn a latent (embedding) representation of the data that can then be used to generate features to learn to solve a downstream business task. The latter class of tasks that are used to indirectly learn the latent representation of the data are called pretext tasks. The <em class="italic">Pretext tasks</em> section will cover this subject, with examples, in more detail.</p>
<p class="normal">The advantages<a id="_idIndexMarker1020"/> of self-supervised learning are twofold. First, as noted already, supervised learning involves the manual labeling of data, which is very expensive to create, and therefore it is difficult to get high-quality labeled data. Second, self-supervised tasks may not address a business task directly but can be used to learn a good representation of the data, which can then be applied to transfer this information to actual business tasks downstream.</p>
<h1 class="heading-1" id="_idParaDest-265">Self-prediction</h1>
<p class="normal">The idea behind self-prediction<a id="_idIndexMarker1021"/> is to predict one part of a data sample given another part. For the purposes of prediction, we pretend that the part to be predicted is hidden or missing and learn to predict it. Obviously, both parts are known, and the part to be predicted serves as the data label. The model is trained in a supervised manner, using the non-hidden part as the input and the hidden part as the label, learning to predict the hidden part accurately. Essentially, it is to pretend that there is a part of the input that you don’t know and predict that.</p>
<p class="normal">The idea can also be extended to reversing the pipeline, for example, deliberately adding noise to an image and using the original image as the label and the corrupted image as the input.</p>
<h2 class="heading-2" id="_idParaDest-266">Autoregressive generation</h2>
<p class="normal"><strong class="keyWord">Autoregressive</strong> (<strong class="keyWord">AR</strong>) models attempt to predict a future<a id="_idIndexMarker1022"/> event, behavior, or property<a id="_idIndexMarker1023"/> based on past events, behavior, or properties. Any data that comes with some innate sequential order can be modeled using AR generation. Unlike latent variable models such as VAEs or GANs, AR models make no assumptions of independence.</p>
<h3 class="heading-3" id="_idParaDest-267">PixelRNN</h3>
<p class="normal">The PixelRNN [1] AR model uses two-dimensional <strong class="keyWord">Recurrent Neural Networks</strong> (<strong class="keyWord">RNNs</strong>) to model images<a id="_idIndexMarker1024"/> on a large scale. The idea<a id="_idIndexMarker1025"/> is to learn to generate a pixel<a id="_idIndexMarker1026"/> by conditioning on all pixels to the left and above it. A convolution operation is used to compute all the states along each dimension at once. The LSTM layers used in PixelRNN are one of two types – the Row LSTM and the Diagonal BiLSTM. In the row LSTM, the convolution is applied along each row, and in the Diagonal BiLSTM, the convolutions are applied along the diagonals of the image:</p>
<table class="table-container" id="table001-4">
<tbody>
<tr>
<td class="table-cell">
<figure class="mediaobject"><img alt="" height="338" src="../Images/B18331_10_01.png" width="337"/></figure>
</td>
<td class="table-cell">
<p class="normal"><img alt="" height="133" src="../Images/B18331_10_001.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="454"/></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Figure 10.1: PixelRNN tries to predict a pixel by conditioning on all pixels to the left and above it. From the paper Pixel Recurrent Neural Networks [1]</p>
<h3 class="heading-3" id="_idParaDest-268">Image GPT (IPT)</h3>
<p class="normal">Image GPT (IPT) [14] is similar to PixelRNN except it works<a id="_idIndexMarker1027"/> on patches, and each patch<a id="_idIndexMarker1028"/> is treated as a word. The Image GPT is based on the Transformer model and is trained on images from the ImageNet dataset. The images are corrupted in multiple different ways (super-resolution, bicubic interpolation, adding noise, etc.) and pretrained to predict the original image. The core of the IPT model consisted of a transformer encoder decoder<a id="_idIndexMarker1029"/> pair but had multiple heads and tails <a id="_idIndexMarker1030"/>to extract features from the corrupted input image and format the decoder output into the output image respectively. The multiple heads and tails were specialized for each of the different tasks IPT is trained to do (denoising, de-raining, x2 and x4 super-resolution, etc.):</p>
<figure class="mediaobject"><img alt="" height="403" src="../Images/B18331_10_02.png" width="877"/></figure>
<p class="packt_figref">Figure 10.2: Architecture of the Image GPT (IPT) AR model. From the paper Pre-trained Image Processing Transformer [14]</p>
<h3 class="heading-3" id="_idParaDest-269">GPT-3</h3>
<p class="normal">The GPT-3, or Generative Pre-trained Transformer [9] model from OpenAI is an AR language model that can<a id="_idIndexMarker1031"/> generate human-like text. It generates sequences<a id="_idIndexMarker1032"/> of words, code, and other data, starting from a human-provided prompt. The first version of GPT used 110 million learning parameters, GPT-2 used 1.5 billion, and GPT-3 used 175 billion parameters. The model is trained on unlabeled text such as Wikipedia that is readily available on the internet, initially in English but later in other languages as well. The GPT-3 model has a wide variety of use cases, including summarization, translation, grammar correction, question answering, chatbots, and email composition.</p>
<p class="normal">The popularity of GPT-3 has given<a id="_idIndexMarker1033"/> rise to a new profession called prompt<a id="_idIndexMarker1034"/> engineering [39], which is basically<a id="_idIndexMarker1035"/> to create the most effective prompts to start GPT-3 on various tasks. A partial list of possible applications for GPT-3<a id="_idIndexMarker1036"/> can be found on the OpenAI GPT-3 examples page (<a href="https://beta.openai.com/examples/"><span class="url">https://beta.openai.com/examples/</span></a>).</p>
<h3 class="heading-3" id="_idParaDest-270">XLNet</h3>
<p class="normal">XLNet [38] is similar to GPT-3 in that it is a generalized<a id="_idIndexMarker1037"/> AR model. However, it leverages<a id="_idIndexMarker1038"/> both AR language modeling and <strong class="keyWord">AutoEncoding</strong> while avoiding their limitations. Instead<a id="_idIndexMarker1039"/> of using only tokens from the left or right context to predict the next token, it uses all possible permutations of the tokens from the left and right contexts, thus using tokens from both the left and right contexts for prediction. Secondly, unlike AE approaches such as BERT, it does not depend on input corruption (as in masked language modeling) since it is a generalized AR language model. Empirically, under comparable experimental settings, XLNet consistently outperforms BERT on a wide spectrum of tasks.</p>
<h3 class="heading-3" id="_idParaDest-271">WaveNet</h3>
<p class="normal">WaveNet [3] is an AR generative<a id="_idIndexMarker1040"/> model based on PixelCNN’s architecture but operates<a id="_idIndexMarker1041"/> on the raw audio waveform. As with PixelCNN, an audio sample at a particular point in time is conditioned on the samples at all previous timesteps. The conditional probability distribution is modeled as a stack of convolutional layers. The main ingredient of the WaveNet is causal convolutions. The predictions emitted by the model at a time step cannot depend on any future timesteps. When applied to text to speech, WaveNet yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding for English and Mandarin than comparable text-to-speech models.</p>
<h3 class="heading-3" id="_idParaDest-272">WaveRNN</h3>
<p class="normal">WaveRNN [28] is an AR generative model<a id="_idIndexMarker1042"/> that learns the joint probability<a id="_idIndexMarker1043"/> of the data by factorizing the distribution into a product of conditional probabilities over each sample. The convolutional layers of the WaveNet architecture are replaced with a single-layer RNN. It also uses more efficient sampling<a id="_idIndexMarker1044"/> techniques that, overall, reduce<a id="_idIndexMarker1045"/> the number of operations to perform and result in approximately 4x speedup over WaveNet.</p>
<h2 class="heading-2" id="_idParaDest-273">Masked generation</h2>
<p class="normal">Masked generation models mask some random portion<a id="_idIndexMarker1046"/> of themselves and pretend<a id="_idIndexMarker1047"/> it is missing, and the models learn to predict the masked information using the unmasked information available to them. Unlike autoregressive models, in the case of masked generation models, there is no need for the masked information to be located before or after the unmasked information; it can be anywhere in the input.</p>
<h3 class="heading-3" id="_idParaDest-274">BERT</h3>
<p class="normal"><strong class="keyWord">BERT</strong> [16], or <strong class="keyWord">Bidirectional Encoder Representation from Transformers</strong>, is<a id="_idIndexMarker1048"/> a transformer-based language<a id="_idIndexMarker1049"/> model that was trained using text from<a id="_idIndexMarker1050"/> the internet by a team from<a id="_idIndexMarker1051"/> Google. It uses two objectives during the pretraining phase – <strong class="keyWord">Masked Language Modeling</strong> (<strong class="keyWord">MLM</strong>) and <strong class="keyWord">Next Sentence Prediction</strong> (<strong class="keyWord">NSP</strong>). During training, 15% of the input tokens are masked and the model learns to predict the masked token. Since the model is transformer based, it can use context from anywhere in the sentence to help with predicting the masked tokens. BERT models, once pretrained, can be fine-tuned with smaller supervised datasets for a variety of downstream tasks such as classification, sentiment analysis, textual entailment, etc. BERT is covered in more depth in <em class="chapterRef">Chapter 6</em>, <em class="italic">Transformers</em>.</p>
<p class="normal">You can see BERT’s masked generation in action using a pretrained BERT model from the Hugging Face Transformers library and the code snippet shown below. Here, we ask a pretrained BERT transformer model to predict the masked token <code class="inlineCode">[MASK]</code> in the sentence <code class="inlineCode">"The capital of France is [MASK]."</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> BertTokenizer, TFBertForMaskedLM
<span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
tokenizer = BertTokenizer.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)
model = TFBertForMaskedLM.from_pretrained(<span class="hljs-string">"bert-base-cased"</span>)
inputs = tokenizer(<span class="hljs-string">"The capital of France is [MASK]."</span>, return_tensors=<span class="hljs-string">"tf"</span>)
logits = model(**inputs).logits
mask_token_index = tf.where(inputs.input_ids == tokenizer.mask_token_id)[<span class="hljs-number">0</span>][<span class="hljs-number">1</span>]
predicted_token_id = tf.math.argmax(logits[:, mask_token_index], axis=-<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(tokenizer.convert_ids_to_tokens(predicted_token_id)[<span class="hljs-number">0</span>])
</code></pre>
<p class="normal">Somewhat<a id="_idIndexMarker1052"/> predictably, the output<a id="_idIndexMarker1053"/> of this code block is <code class="inlineCode">"Paris"</code>.</p>
<h3 class="heading-3" id="_idParaDest-275">Stacked denoising autoencoder</h3>
<p class="normal">A stacked denoising autoencoder (AE) [29] adds random noise<a id="_idIndexMarker1054"/> to images and uses them<a id="_idIndexMarker1055"/> as input to a denoising AE to predict the original image. Multiple layers of denoising AEs are each individually trained and stacked. This results in the composition of several levels of non-linearity and is key to achieving better generalization performance on difficult image recognition tasks. Higher-level representations learned in this purely unsupervised manner can be used as image features to boost the performance of downstream SVM based image classifiers. Each layer functions like a regular AE, i.e., it takes an image as input and tries to reconstruct it after it passes through a “bottleneck” layer. The bottleneck layer learns a compact feature representation of the input image. Unfortunately, AEs usually end up only learning how to compress the image without learning a semantically meaningful representation. Denoising AEs address this issue by corrupting the input and requiring the network to undo the corruption and hence learn a better semantic representation of the input image.</p>
<h3 class="heading-3" id="_idParaDest-276">Context autoencoder</h3>
<p class="normal">The context autoencoder [12] masks<a id="_idIndexMarker1056"/> out a region of the image<a id="_idIndexMarker1057"/> and uses it to train a convolutional neural network (the context AE) to regress the missing pixel values to predict the original image. The task of a context AE is even harder than that of a denoising AE since it has to fill in larger missing areas and cannot use information from immediately neighboring pixels. This requires a much deeper semantic understanding of the image, and the ability to generate high-level features over large spatial areas. In a sense, the context AE is a more powerful generative model<a id="_idIndexMarker1058"/> since it needs to fill in the missing<a id="_idIndexMarker1059"/> region while maintaining coherence with the supplied context. </p>
<p class="normal">For that reason, the context AE is trained to reconstruct a combination of reconstruction loss and adversarial loss. This results in sharper predictions than training on reconstruction (L2) loss alone:</p>
<figure class="mediaobject"><img alt="" height="910" src="../Images/B18331_10_03.png" width="804"/></figure>
<p class="packt_figref">Figure 10.3: Qualitative illustration of the context encoder task (from Context Encoders: Feature Learning by Inpainting [10]) </p>
<p class="normal">Context does not have to be image<a id="_idIndexMarker1060"/> features, it could also<a id="_idIndexMarker1061"/> be color, as we will see in the next section.</p>
<h3 class="heading-3" id="_idParaDest-277">Colorization</h3>
<p class="normal">The paper <em class="italic">Colorization as a Proxy Task for Visual Understanding</em> [12] uses colorization<a id="_idIndexMarker1062"/> as a way to learn image<a id="_idIndexMarker1063"/> representations. Color images are converted to their grayscale equivalent, which is then used as input to predict the original color image. The model can be used to automatically colorize grayscale images, as well as learn a representation that can help in downstream tasks such as image classification and segmentation. In functional terms, the model predicts the <em class="italic">a</em> and <em class="italic">b</em> (color information) channels in their <em class="italic">Lab</em> encoding given their <em class="italic">L</em> (grayscale) channel. Experiments on the ImageNet dataset by the authors of this paper have resulted in models that produce state-of-the-art results against datasets for semantic segmentation and image classification for models that don’t use ImageNet labels, and even surpass some earlier models that have been trained on ImageNet using supervised learning.</p>
<h2 class="heading-2" id="_idParaDest-278">Innate relationship prediction</h2>
<p class="normal">Models using this technique attempt<a id="_idIndexMarker1064"/> to learn visual common-sense tasks<a id="_idIndexMarker1065"/> by leveraging innate relationships between parts of an input image. Weights from these learned models could be used to generate semantic representations of images for other downstream tasks.</p>
<h3 class="heading-3" id="_idParaDest-279">Relative position</h3>
<p class="normal">The paper <em class="italic">Unsupervised Visual Representation Learning by Context Prediction</em> [8] predicts the relative position<a id="_idIndexMarker1066"/> of one patch in an image<a id="_idIndexMarker1067"/> with respect to another. Effectively, this approach uses spatial context as a source of self-supervision for training visual representations. Given a large unlabeled image collection, random pairs of patches are extracted from each image as shown in <em class="italic">Figure 10.4</em>. Each pair is labeled depending on the orientation of the second patch with respect to the central one. A convolutional network is trained to predict the position of the second patch relative to the first. The feature representation learned is found to capture the notion of visual similarity across images. Using this representation, it has been shown to aid in visual data mining, i.e., discovering image fragments that depict the same semantic object, against the Pascal VOC 2007 dataset:</p>
<figure class="mediaobject"><img alt="" height="350" src="../Images/B18331_10_04.png" width="395"/></figure>
<p class="packt_figref">Figure 10.4: Illustration of relative position prediction. The model must predict the configuration of the second patch relative to the (central) first patch. From the paper Unsupervised Visual Representation Learning by Context Prediction [8]</p>
<h3 class="heading-3" id="_idParaDest-280">Solving jigsaw puzzles</h3>
<p class="normal">The paper <em class="italic">Unsupervised Learning of Visual Representations by Solving Jigsaw Puzzles</em> [26] describes<a id="_idIndexMarker1068"/> an approach somewhat similar to the previous<a id="_idIndexMarker1069"/> approach of predicting relative position. This method attempts to learn the visual representation of images by solving jigsaw puzzles of natural images. Patches are extracted from the input image and shuffled to form a jigsaw puzzle. The network learns to reconstruct the original image from the jigsaw<a id="_idIndexMarker1070"/> puzzle, i.e., to solve the jigsaw puzzle. The network used is a <strong class="keyWord">Context Free Network</strong> (<strong class="keyWord">CFN</strong>), an n-way Siamese network. Each patch corresponds to a column in the n-way CFN. The shared layers in each column are implemented exactly as in AlexNet. The classification head predicts the original index of the patch (before shuffling). On the Pascal VOC dataset, it outperforms<a id="_idIndexMarker1071"/> all previous self-supervised models<a id="_idIndexMarker1072"/> in image classification and object detection tasks:</p>
<figure class="mediaobject"><img alt="" height="285" src="../Images/B18331_10_05.png" width="877"/></figure>
<p class="packt_figref">Figure 10.5: The image is split up into patches and shuffled, and the model learns to put the shuffled patches back in the correct order. From the paper Unsupervised Learning of Visual Representations [26]</p>
<h3 class="heading-3" id="_idParaDest-281">Rotation</h3>
<p class="normal">The RotNet model [34] learns an image<a id="_idIndexMarker1073"/> representation by using rotation<a id="_idIndexMarker1074"/> as a self-supervision signal. Input images<a id="_idIndexMarker1075"/> are rotated by 0, 90, 180, and 270 degrees, and a convolutional network (RotNet) is trained to learn to predict the rotation angle as one of 4 target classes. It turns out that this apparently simple task provides a very powerful supervisory signal for semantic feature learning. RotNet features were used as input for image classification against the CIFAR-10 dataset and resulted in classification accuracy of only 1.6% less than the state-of-the-art result obtained using supervised learning. It also obtained state-of-the-art results at the time for some classification tasks against ImageNet, and some classification and object detection tasks against Pascal VOC.</p>
<h2 class="heading-2" id="_idParaDest-282">Hybrid self-prediction</h2>
<p class="normal">With hybrid self-prediction models, self-prediction<a id="_idIndexMarker1076"/> is achieved using not one but<a id="_idIndexMarker1077"/> multiple self-prediction strategies. For example, our first two examples, Jukebox and DALL-E, achieve self-prediction by first reducing the input data to a more manageable format using one self-supervision technique (VQ-VAE or Vector Quantized Variational AutoEncoder [35]) and then use another (AR) on the reduced image to produce the final prediction. In our third example, the predictions from the VQ-VAE component are further refined using a discriminator trained in an adversarial manner.</p>
<h3 class="heading-3" id="_idParaDest-283">VQ-VAE</h3>
<p class="normal">Since the VQ-VAE is common<a id="_idIndexMarker1078"/> to all our hybrid self-prediction<a id="_idIndexMarker1079"/> models, let us try to understand what it does at a high level. You have already read about autoencoders and variational autoencoders in <em class="chapterRef">Chapter 8</em>, <em class="italic">Autoencoders</em>. Autoencoders try to learn to reconstruct their input by first encoding the input onto a smaller dimension and then decoding the output of the smaller dimension. However, autoencoders typically just end up compressing the input and do not learn a good semantic representation. </p>
<p class="normal"><strong class="keyWord">Variational Autoencoders</strong> (<strong class="keyWord">VAEs</strong>) can do better in this respect<a id="_idIndexMarker1080"/> by enforcing a probabilistic prior, generally in the form of a standard Gaussian distribution, and by minimizing not only the reconstruction loss but also the KL divergence between the prior distribution and posterior distribution (the actual distribution in the latent space).</p>
<p class="normal">While the VAE learns a continuous latent distribution, the VQ-VAE learns a discrete latent distribution. This is useful because transformers are designed to take discrete data as input. VQ-VAE extends VAE by adding a discrete codebook component to the network, which is used to quantize the latent vectors output by the encoder by choosing the vector in the codebook that is closest to each latent vector by Euclidean distance. The VQ-VAE decoder is then tasked with reconstructing the input from the discretized latent vector.</p>
<h3 class="heading-3" id="_idParaDest-284">Jukebox</h3>
<p class="normal">Our first example<a id="_idIndexMarker1081"/> is the Jukebox<a id="_idIndexMarker1082"/> paper [32], which is a generative model for music, similar to how GPT-3 is a generative model for text and Image-GPT is a generative model for images. That is, given a musical (voice and music) prompt, Jukebox can create the music that might follow this prompt. Early attempts at generative models for audio attempted symbolic music generation in the form of a piano roll, since the problem with generating raw audio directly is the extremely large amount of information it contains and consequently, the extreme long-range dependencies that need to be modeled. The VQ-VAE addresses this problem by learning a lower-dimensional encoding of the audio with the goal of losing the least important information but retaining most of the useful information. </p>
<p class="normal">Jukebox uses hierarchical VQ-VAEs<a id="_idIndexMarker1083"/> to discretize the input signal into different<a id="_idIndexMarker1084"/> temporal resolutions, then generates a new sequence at each resolution, and finally combines the generated sequence at each level into the final prediction.</p>
<h3 class="heading-3" id="_idParaDest-285">DALL-E</h3>
<p class="normal">Our second example<a id="_idIndexMarker1085"/> of hybrid prediction models<a id="_idIndexMarker1086"/> is the DALL-E model [5] from OpenAI. DALL-E can also be classified as a joint learning (multimodal) model, since it attempts to learn to create images from text captions, using pairs of text and image as training input. However, we classify it here as a hybrid prediction model because, like Jukebox, it attempts to address the high dimensionality of image information (compared with the dimensionality of the associated text) using a VQ-VAE.</p>
<p class="normal">DALL-E receives text and images as a single stream of data. DALL-E uses a two-stage training regime. In the first stage, a VQ-VAE is trained to compress each input RGB image of size (256, 256, 3) into a grid of image tokens of size (32, 32), each element of which can assume one of 8,192 possible discrete values. This reduces the size of the image input by a factor of 192 without a corresponding loss in image quality. </p>
<p class="normal">In the second stage, the text<a id="_idIndexMarker1087"/> is BPE-encoded and truncated to 256 tokens. <strong class="keyWord">Byte Pair Encoding</strong> (<strong class="keyWord">BPE</strong>) is a hybrid character/word encoding that can represent large corpora using a relatively small vocabulary by encoding common byte pairs. This encoding is then concatenated with the flattened sequence of 1,024 (32 x 32) image tokens. This combined sequence is used to train an autoregressive transformer to model the joint distribution over the text and image tokens. The first stage learns the visual codebook in the VQ-VAE and the second stage learns the prior of the discrete latent distribution over the text and image tokens. The trained DALL-E model can then be used to generate images given a text prompt.</p>
<p class="normal">Text-to-image generation is getting quite popular. A newer version of DALL-E, called DALL-E 2, was recently released<a id="_idIndexMarker1088"/> by OpenAI. It has 35 billion parameters compared to DALL-E’s 12 billion. Even though they are named similarly, DALL-E is a version of GPT-3 trained to generate images from text descriptions, and DALL-E 2 is an encoder-decoder pipeline that uses CLIP to encode the text description into a CLIP embedding, and then decode the embedding back to an image using a diffusion model that you learned about in <em class="chapterRef">Chapter 9</em>, <em class="italic">Generative Models</em>. As expected, DALL-E 2 generates more realistic and accurate images than DALL-E.</p>
<p class="normal">Even more recently, Google Research<a id="_idIndexMarker1089"/> has released Imagen, another model<a id="_idIndexMarker1090"/> in this space that competes with DALL-E 2. Like DALL-E 2, Imagen uses a T5-XXL encoder to map input text into embeddings and a diffusion model to decode the embedding into an image.</p>
<h3 class="heading-3" id="_idParaDest-286">VQ-GAN</h3>
<p class="normal">The VQ-GAN [30] uses an encoder-decoder framework<a id="_idIndexMarker1091"/> where the encoder uses a VQ-VAE style<a id="_idIndexMarker1092"/> encoder that learns a discrete latent representation, but the decoder<a id="_idIndexMarker1093"/> is a discriminator component of a <strong class="keyWord">Generative Adversarial Network</strong> (<strong class="keyWord">GAN</strong>). Instead of the L2 loss used in the VQ-VAE, the VQ-GAN uses a combination of perceptual loss and discriminator loss, which helps in keeping good perceptual quality at increased compression rates. The use of a GAN architecture rather than a traditional VAE decoder helps with training efficiency.</p>
<p class="normal">Like VQ-VAE, the VQ-GAN learns a codebook of context-rich visual components, which are used to compose sequences for training the autoregressive component. The VQ-GAN has been found to outperform the VQ-VAE-2 model<a id="_idIndexMarker1094"/> on images from ImageNet using the <strong class="keyWord">Fréchet Inception Distance</strong> (<strong class="keyWord">FID</strong>), which measures the distance between feature vectors of real versus fake images) metric, even though it uses approximately 10x fewer parameters:</p>
<figure class="mediaobject"><img alt="" height="353" src="../Images/B18331_10_06.png" width="882"/></figure>
<p class="packt_figref">Figure 10.6: Architecture of the VQ-GAN. From the paper: Taming Transformers for High Resolution Image Synthesis [30]</p>
<p class="normal">Next, we will look at another popular self-supervised technique called contrastive learning.</p>
<h1 class="heading-1" id="_idParaDest-287">Contrastive learning</h1>
<p class="normal"><strong class="keyWord">Contrastive Learning</strong> (<strong class="keyWord">CL</strong>) tries to predict<a id="_idIndexMarker1095"/> the relationship between a pair of input samples. The goal of CL is to learn an embedding space where pairs of similar samples are pulled close together and dissimilar samples are pushed far apart. Inputs to train CL models are in the form of <em class="italic">pairs of data points</em>. CL can be used in both supervised and unsupervised settings.</p>
<p class="normal">When used in an unsupervised setting, it can be a very powerful self-supervised learning approach. Similar pairs are found from existing data in a self-supervised manner, and dissimilar pairs are found from pairs of similar pairs of data. The model learns to predict if a pair of data points are similar or different.</p>
<p class="normal">A taxonomy of CL can be derived by considering the techniques used to generate contrastive examples. Before we do that, we will take a brief detour to explore the various training objectives that are popular in CL.</p>
<h2 class="heading-2" id="_idParaDest-288">Training objectives</h2>
<p class="normal">Early CL models<a id="_idIndexMarker1096"/> used data points consisting of a single positive<a id="_idIndexMarker1097"/> and a single negative example to learn from. However, the trend in more recent CL models is to learn from multiple positive and negative samples in a single batch. In this section, we will cover some<a id="_idIndexMarker1098"/> training objectives (also called loss functions) that are commonly used for training CL models.</p>
<h3 class="heading-3" id="_idParaDest-289">Contrastive loss</h3>
<p class="normal">Contrastive loss [35] is one of the earliest training<a id="_idIndexMarker1099"/> objectives to be used for learning<a id="_idIndexMarker1100"/> using CL techniques. It tries to encode data into an embedding space such that examples from the same class have similar embeddings and examples from different classes have dissimilar embeddings. Thus, given two data pairs, (<em class="italic">x</em><sub class="italic">i</sub><em class="italic">, y</em><sub class="italic">i</sub>) and (<em class="italic">x</em><sub class="italic">j</sub><em class="italic">, y</em><sub class="italic">j</sub>), the contrastive loss objective is described using the following formula:</p>
<p class="center"><img alt="" height="83" src="../Images/B18331_10_002.png" style="height: 2.08em !important;" width="1446"/></p>
<p class="normal">The first term is activated when the pairs <em class="italic">i</em> and <em class="italic">j</em> are similar, and the second term is activated when the pair is dissimilar. The objective is designed to maximize the square of the differences in the first term and minimize the square of differences in the second term (thus maximizing the second term in the case of dissimilar pairs). The <img alt="" height="42" src="../Images/B18331_10_003.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> is a hyperparameter and represents a margin of the minimum allowable distance between samples of different classes.</p>
<h3 class="heading-3" id="_idParaDest-290">Triplet loss</h3>
<p class="normal">Triplet loss [11] is an enhancement of contrastive loss<a id="_idIndexMarker1101"/> in that it uses three data points<a id="_idIndexMarker1102"/> instead of two – the anchor point, the positive point, and the negative point. Thus, given an anchor point <em class="italic">x</em>, we select a positive sample <img alt="" height="42" src="../Images/B18331_10_004.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="42"/> and one negative sample <img alt="" height="42" src="../Images/B18331_10_005.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="42"/>, where <em class="italic">x</em> and <img alt="" height="42" src="../Images/B18331_10_004.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="42"/> belong to the same class and <em class="italic">x</em> and <img alt="" height="42" src="../Images/B18331_10_005.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="42"/> belong to different classes. Triplet loss learns to minimize the distance between the anchor <em class="italic">x</em> and positive sample <img alt="" height="42" src="../Images/B18331_10_004.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="42"/> and maximize the distance between <em class="italic">x</em> and negative sample <img alt="" height="42" src="../Images/B18331_10_005.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="42"/>. This is illustrated in <em class="italic">Figure 10.7</em>:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="159" src="../Images/B18331_10_07.png" width="630"/></figure>
<p class="packt_figref">Figure 10.7: Illustration of triplet loss. Based on the paper: FaceNet: A Unified Embedding for Face Recognition and Clustering [11]</p>
<p class="normal">The equation for triplet loss<a id="_idIndexMarker1103"/> is shown below. As with contrastive loss, the <img alt="" height="42" src="../Images/B18331_10_003.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> is a hyperparameter<a id="_idIndexMarker1104"/> representing the minimum allowed difference between distances between similar and dissimilar pairs. Triplet loss-based models typically need challenging values for <img alt="" height="42" src="../Images/B18331_10_005.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="42"/>, the so-called hard negatives, to provide good<a id="_idIndexMarker1105"/> representations: </p>
<p class="center"><img alt="" height="108" src="../Images/B18331_10_012.png" style="height: 2.70em !important;" width="1142"/></p>
<h3 class="heading-3" id="_idParaDest-291">N-pair loss</h3>
<p class="normal">N-pair loss [21] generalizes triplet loss to incorporate comparison<a id="_idIndexMarker1106"/> with multiple negative samples<a id="_idIndexMarker1107"/> instead of just one. Thus, given an <em class="italic">(N+1)</em> tuple of training samples, {<em class="italic">x</em>, <em class="italic">x</em><sup class="superscript">+</sup>, <em class="italic">x</em><sub class="subscript">1</sub><sup class="superscript">-</sup>, <em class="italic">x</em><sub class="subscript">2</sub><sup class="superscript">-</sup>, …, <em class="italic">x</em><sub class="subscript">N+1</sub><sup class="superscript">-</sup>}, where there is one positive sample and <em class="italic">N-1</em> negative ones, the N-pair loss is defined using the following equation:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_10_013.png" style="height: 2.70em !important; vertical-align: 0.03em !important;" width="1204"/></p>
<h3 class="heading-3" id="_idParaDest-292">Lifted structural loss</h3>
<p class="normal">Lifted structured loss [15] is another generalization<a id="_idIndexMarker1108"/> of triplet loss where it uses<a id="_idIndexMarker1109"/> all pairwise edges within a training batch. This leads to better training performance. <em class="italic">Figure 10.8</em> illustrates the idea behind lifted structural loss, and how it evolved from contrastive and triplet loss. Red edges connect similar pairs<a id="_idIndexMarker1110"/> and blue edges connect<a id="_idIndexMarker1111"/> dissimilar pairs:</p>
<figure class="mediaobject"><img alt="Chart, diagram, schematic  Description automatically generated" height="432" src="../Images/B18331_10_08.png" width="488"/></figure>
<p class="packt_figref">Figure 10.8: Illustration of the idea of Lifted Structured Loss. Based on the paper: Deep Metric Learning via Lifted Structured Feature Embedding [15]</p>
<h3 class="heading-3" id="_idParaDest-293">NCE loss</h3>
<p class="normal"><strong class="keyWord">Noise Contrastive Estimation</strong> (<strong class="keyWord">NCE</strong>) loss [27] uses logistic regression<a id="_idIndexMarker1112"/> to distinguish positive and negative (noise) examples. The NCE loss<a id="_idIndexMarker1113"/> attempts to maximize the log odds (logits) of positive examples <em class="italic">x</em> and minimize the log odds of negative examples <img alt="" height="50" src="../Images/B18331_10_014.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="29"/> . The equation for NCE loss is shown below:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_10_015.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="1204"/></p>
<h3 class="heading-3" id="_idParaDest-294">InfoNCE loss</h3>
<p class="normal">InfoNCE loss [2] was inspired by NCE<a id="_idIndexMarker1114"/> loss (described in the previous section) and uses<a id="_idIndexMarker1115"/> categorical cross-entropy loss to identify the positive sample from the set of unrelated noise samples. Given some context vector <em class="italic">c</em>, the positive sample should be drawn from the conditional probability distribution <em class="italic">p(x|c)</em>, while the <em class="italic">N-1</em> negative examples can be drawn from the distribution <em class="italic">p(x)</em> independent of the context <em class="italic">c</em>. The InfoNCE loss optimizes the negative log probability of classifying the positive sample correctly. </p>
<p class="normal">The InfoNCE loss is given by the following equation, where <em class="italic">f(x, c)</em> estimates the density ratio <em class="italic">p(x|c) / p(x)</em>:</p>
<p class="center"><img alt="" height="108" src="../Images/B18331_10_016.png" style="height: 2.70em !important; vertical-align: 0.03em !important;" width="454"/></p>
<h3 class="heading-3" id="_idParaDest-295">Soft nearest neighbors loss</h3>
<p class="normal">Soft nearest neighbors loss [33] further extends<a id="_idIndexMarker1116"/> the idea of contrastive loss<a id="_idIndexMarker1117"/> to include multiple positive samples given known labels. Given a batch of samples, <img alt="" height="50" src="../Images/B18331_10_017.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="192"/> where <em class="italic">y</em><sub class="italic">i</sub> is the class label of <em class="italic">x</em><sub class="italic">i</sub>, and a similarity function <em class="italic">f</em> that measures similarity between two inputs, the soft nearest neighbor loss is given by the equation:</p>
<p class="center"><img alt="" height="133" src="../Images/B18331_10_018.png" style="height: 3.33em !important; vertical-align: 0.03em !important;" width="892"/></p>
<p class="normal">The temperature <img alt="" height="42" src="../Images/B18331_10_019.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> is a hyperparameter and is used for tuning how concentrated the features are in the representation space. Thus, at low temperatures, the contribution of faraway points in the representation space to the soft nearest neighbors loss is also low.</p>
<h2 class="heading-2" id="_idParaDest-296">Instance transformation</h2>
<p class="normal">CL models that use instance transformation<a id="_idIndexMarker1118"/> generally rely on data augmentation<a id="_idIndexMarker1119"/> techniques to generate positive pairs and negative mining to generate negative pairs from pairs of positive pairs. Many such models rely on generating in-batch negative and innovative techniques for mining hard negatives.</p>
<p class="normal">Data augmentation techniques<a id="_idIndexMarker1120"/> are used to create pairs of the original data point and its noisy version. This introduces non-essential variation into the examples without modifying semantic meaning, which the model then learns during training.</p>
<p class="normal">In-batch negative sampling is a technique for generating negative samples by combining information from examples within a single batch. For each positive pair (<em class="italic">x</em><sub class="italic">i</sub><em class="italic">, y</em><sub class="italic">i</sub>) in the batch, all pairs (<em class="italic">x</em><sub class="italic">i</sub><em class="italic">, y</em><sub class="italic">j</sub>) and (<em class="italic">x</em><sub class="italic">j</sub><em class="italic">, y</em><sub class="italic">i</sub>) for all <img alt="" height="50" src="../Images/B18331_10_020.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="79"/> can be considered as negative pairs. In effect, negative pairs are created by combining elements from two random positive pairs in the same batch. This technique is practical and can be implemented efficiently on GPUs and is therefore widely used.</p>
<p class="normal">Some models require hard negative samples to learn how to perform their tasks well. Hard negatives are pairs that have different labels, but whose embedding features are very close to each other. You can visualize them as points that lie very close to each other in the embedding space but on opposite sides of the decision boundary. Identifying hard negatives for a given task is relatively easy for supervised learning. For unsupervised learning, one approach is to increase the batch size, which will introduce more hard negative samples. Another technique [19] is to increase the sampling probability of the candidate negative sample by its similarity with the anchor sample.</p>
<h3 class="heading-3" id="_idParaDest-297">SimCLR</h3>
<p class="normal">The SimCLR model [36] presents a simple framework<a id="_idIndexMarker1121"/> for contrastive learning of visual<a id="_idIndexMarker1122"/> representations. Each input image (<em class="italic">x</em>) is augmented in two different ways (<em class="italic">x</em><sub class="italic">i</sub> and <em class="italic">x</em><sub class="italic">j</sub>) using the same family of image augmentation strategies, resulting in <em class="italic">2N</em> positive samples. </p>
<p class="normal">In-batch negative sampling is used, so for each positive example, we have <em class="italic">(2N-1)</em> negative samples. A base encoder (<em class="italic">f</em>) is applied to the pair of data points in each example, and a projection head (<em class="italic">g</em>) attempts to maximize the agreement for positive pairs and minimize it for negative pairs. For good performance, SimCLR needs to use large batch sizes so as to incorporate enough negative examples in the training regime. SimCLR achieved state-of-the-art results for self-supervised and semi-supervised models on ImageNet and matches the performance of a supervised ResNet-50. <em class="italic">Figure 10.9</em> shows the architecture of the SimCLR model:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="409" src="../Images/B18331_10_09.png" width="531"/></figure>
<p class="packt_figref">Figure 10.9: Architecture of the SimCLR model. From the paper: A Simple Framework for Contrastive Learning of Visual Representations [36]</p>
<h3 class="heading-3" id="_idParaDest-298">Barlow Twins</h3>
<p class="normal">The idea behind the Barlow Twins [20] model<a id="_idIndexMarker1123"/> has its roots in neuroscience, i.e., the goal<a id="_idIndexMarker1124"/> of sensory processing is to re-code highly redundant sensory inputs into a factorial code, or a code with statistically independent components. In this model, an image is distorted into two versions of itself. The distorted versions are fed into the same network to extract features and learn to make the cross-correlation matrix between these two features as close to the identity matrix as possible. In line with the neuroscience idea, the goal of this model is to reduce the redundancy between the two distorted versions of the sample by reducing the redundancy between these vectors. This is reflected in its somewhat unique loss function – in the first equation, the first term represents the difference between the identity matrix and the cross-correlation matrix, and the second term represents the redundancy reduction term. The second equation defines each element of the cross-correlation matrix <em class="italic">C</em>:</p>
<p class="center"><img alt="" height="113" src="../Images/B18331_10_021.png" style="height: 2.83em !important; vertical-align: 0.03em !important;" width="529"/></p>
<p class="center"><img alt="" height="158" src="../Images/B18331_10_022.png" style="height: 3.95em !important; vertical-align: 0.04em !important;" width="471"/></p>
<p class="normal">Some notable differences between <a id="_idIndexMarker1125"/>the Barlow Twins model and other models in this genre are that the Barlow Twins model<a id="_idIndexMarker1126"/> doesn’t require a large number of negative samples and can thus operate on smaller batches, and that it benefits from high-dimensional embeddings. The Barlow Twins model outperforms some previous semi-supervised models trained on ImageNet and is on par with some supervised ImageNet models.</p>
<h3 class="heading-3" id="_idParaDest-299">BYOL</h3>
<p class="normal">The <strong class="keyWord">Bootstrap Your Own Latent</strong> (<strong class="keyWord">BYOL</strong>) model [17] is unique in that it does<a id="_idIndexMarker1127"/> not use negative samples at all. It relies on two<a id="_idIndexMarker1128"/> neural networks, the online and target networks, that interact and learn from each other. The goal of BYOL is to learn a representation <img alt="" height="50" src="../Images/B18331_10_023.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="38"/>that can be used for downstream tasks. The online network is parameterized by a set of weights <img alt="" height="42" src="../Images/B18331_10_024.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> and comprises three stages – an encoder <img alt="" height="50" src="../Images/B18331_10_025.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/>, a projector <img alt="" height="50" src="../Images/B18331_10_026.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="42"/>, and a predictor <img alt="" height="46" src="../Images/B18331_10_027.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="38"/> The target network has the same architecture as the online network but uses a different set of weights <img alt="" height="46" src="../Images/B18331_10_028.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/>. The target network provides the regression targets to train the online network, and its parameters <img alt="" height="46" src="../Images/B18331_10_028.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="21"/> are an exponential moving average of the online parameters <img alt="" height="42" src="../Images/B18331_10_024.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/> After every training step, the following update is performed:</p>
<p class="center"><img alt="" height="46" src="../Images/B18331_10_031.png" style="height: 1.15em !important; vertical-align: 0.10em !important;" width="304"/></p>
<p class="normal">BYOL produces two augmented views of each image. From the first augmented view, the online network outputs a representation <img alt="" height="50" src="../Images/B18331_10_032.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="38"/> and a projection <img alt="" height="46" src="../Images/B18331_10_033.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/>. Similarly, the target network outputs a representation <img alt="" height="54" src="../Images/B18331_10_034.png" style="height: 1.35em !important; vertical-align: -0.43em !important;" width="38"/> and a projection <img alt="" height="54" src="../Images/B18331_10_035.png" style="height: 1.35em !important; vertical-align: -0.43em !important;" width="33"/> BYOL attempts to minimize the error between the L2 normalized online and target projections <img alt="" height="46" src="../Images/B18331_10_033.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="33"/> and <img alt="" height="54" src="../Images/B18331_10_035.png" style="height: 1.35em !important; vertical-align: -0.43em !important;" width="33"/>. At the end of the training, we only retain the online network (the encoder).</p>
<p class="normal">BYOL achieves<a id="_idIndexMarker1129"/> competitive results against<a id="_idIndexMarker1130"/> semi-supervised or transfer learning models on ImageNet. It is also less sensitive to changes in batch size and the type of image augmentations used compared to other models in this genre. However, later work [4] indicates that the batch normalization component in BYOL may implicitly cause a form of contrastive learning by implicitly creating negative samples as a result of data redistribution it causes.</p>
<h3 class="heading-3" id="_idParaDest-300">Feature clustering</h3>
<p class="normal">Feature clustering involves finding<a id="_idIndexMarker1131"/> similar data samples by clustering<a id="_idIndexMarker1132"/> them. This can be useful when data augmentation techniques are not feasible. The idea here is to use clustering algorithms to assign pseudo-labels to samples such that we can run intra-sample CL. Although similar, feature clustering differs from CL in that it relaxes the instance discrimination problem – rather than learn to distinguish between a pair of transformations on a single input image, feature clustering learns to discriminate between groups of images with similar features.</p>
<h3 class="heading-3" id="_idParaDest-301">DeepCluster</h3>
<p class="normal">The DeepCluster [24] paper<a id="_idIndexMarker1133"/> is predicated on the fact that datasets for supervised<a id="_idIndexMarker1134"/> learning such as ImageNet are “too small” to account for general-purpose features that go beyond image classification. For learning general-purpose features, it is necessary to train on billions of images at internet scales. However, labeling such large datasets is not feasible, so DeepCluster presents a clustering method that jointly learns the parameters of the neural network and the cluster assignments of the resulting features. DeepCluster iteratively groups these features using the K-Means clustering algorithm and uses the cluster assignments as pseudo labels to learn the parameters of the ConvNet. The end product of the training is the weights of the ConvNet. These weights have been shown to be useful general-purpose visual features and have outperformed the best published numbers on many downstream tasks regardless of the dataset.</p>
<h3 class="heading-3" id="_idParaDest-302">SwAV</h3>
<p class="normal">In the <strong class="keyWord">SwAV</strong> (<strong class="keyWord">SWapping Assignments between multiple Views</strong>) [25] model, features are learned by predicting<a id="_idIndexMarker1135"/> the cluster assignment (pseudo-label) for <a id="_idIndexMarker1136"/>a view from the representation of another view. SwAV uses a variant of the architecture used in CL models. The images <em class="italic">x</em><sub class="subscript">1</sub> and <em class="italic">x</em><sub class="subscript">2</sub> are transformations of the same input image <em class="italic">x</em>, which are sent through an encoder <img alt="" height="50" src="../Images/B18331_10_025.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="33"/> to produce a representation <em class="italic">z</em><sub class="subscript">1</sub> and <em class="italic">z</em><sub class="subscript">2</sub>. In the case of SwAV, <em class="italic">z</em><sub class="subscript">1</sub> and <em class="italic">z</em><sub class="subscript">2</sub> are used to compute <em class="italic">q</em><sub class="subscript">1</sub> and <em class="italic">q</em><sub class="subscript">2</sub> by matching their features to a set of <em class="italic">K</em> prototype vectors <em class="italic">{c</em><sub class="subscript">1</sub><em class="italic">, …, c</em><sub class="italic">K</sub><em class="italic">}</em>, which are then used to predict the cluster assignment for <em class="italic">x</em><sub class="subscript">2</sub> and <em class="italic">x</em><sub class="subscript">1</sub> respectively.</p>
<p class="normal">Unlike DeepCluster, SwAV does online clustering (clustering of data that arrives continuously in a streaming manner and is not known before the clustering process begins) and can therefore scale to potentially unlimited amounts of data. SwAV also works well with both large and small batch sizes. The SwAV paper also proposes a new multi-crop strategy to increase the number of views of an image with no computational or memory overhead. It achieves 75% top-1 accuracy on ImageNet with ResNet50 (a supervised learning method) as well as surpassing results of supervised pretraining in all the considered transfer tasks.</p>
<h3 class="heading-3" id="_idParaDest-303">InterCLR</h3>
<p class="normal">InterCLR [18] is a hybrid model<a id="_idIndexMarker1137"/> that jointly learns a visual representation by leveraging<a id="_idIndexMarker1138"/> intra-image as well as inter-image invariance. It has two invariance learning branches in its pipeline, one for intra-image, and the other for inter-image. The intra-image branch constructs contrastive pairs by standard CL methods such as generating a pair of transformations from an input image. The inter-image branch constructs contrastive pairs using pseudo-labels obtained from clustering – two items within the same cluster constitute a positive pair, and two items from different clusters form a negative pair. </p>
<p class="normal">A variant of the InfoNCE loss function is used to compute the contrastive loss and the network is trained through back-propagation:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="398" src="../Images/B18331_10_10.png" width="877"/></figure>
<p class="packt_figref">Figure 10.10: Architecture of the InterCLR model. From the paper: Delving into Inter-Image Invariance for Unsupervised Visual Representation [18]</p>
<p class="normal">The InterCLR paper also addresses<a id="_idIndexMarker1139"/> some special considerations around pseudo label<a id="_idIndexMarker1140"/> maintenance, sampling strategy, and decision boundary design for the inter-image branch, which we will skip here in the interests of space. The InterCLR model shows many improvements over state-of-the-art intra-image invariance learning methods on multiple standard benchmarks.</p>
<h2 class="heading-2" id="_idParaDest-304">Multiview coding</h2>
<p class="normal">Multiview coding<a id="_idIndexMarker1141"/> has become a mainstream CL method in recent years<a id="_idIndexMarker1142"/> and involves constructing positive contrastive examples using two or more views of the same object. The objective is to maximize the mutual information between the representations of the multiple views of the data for positive examples and minimize it for negative examples. This requires the model to learn higher-level features whose influence spans multiple views.</p>
<h3 class="heading-3" id="_idParaDest-305">AMDIM</h3>
<p class="normal"><strong class="keyWord">Augmented Multiscale Deep InfoMax</strong> (<strong class="keyWord">AMDIM</strong>) [31] is a model for<a id="_idIndexMarker1143"/> self-supervised representational<a id="_idIndexMarker1144"/> learning based on an earlier local Deep InfoMax method, which attempts to maximize the mutual information between a global summary feature that depends on the entire input, and a collection<a id="_idIndexMarker1145"/> of local features that are extracted<a id="_idIndexMarker1146"/> from intermediate layers in the encoder. AMDIM extends DIM by predicting features across independently augmented features of each input and simultaneously across multiple scales, as well as using a more powerful encoder. </p>
<p class="normal">The paper also considers other ways of producing contrastive pairs, such as instance transformation and multimodal (discussed in the next section), but it is described here because it also considers constructing contrastive pairs using multiview coding. The model beats several benchmarks for self-supervised learning objectives.</p>
<h3 class="heading-3" id="_idParaDest-306">CMC</h3>
<p class="normal">The <strong class="keyWord">Contrastive Multiview Coding</strong> (<strong class="keyWord">CMC</strong>) [37] model is based<a id="_idIndexMarker1147"/> on the idea that when an object<a id="_idIndexMarker1148"/> is represented by multiple views, each of these views is noisy and incomplete, but important factors such as the physics, geometry, and semantics of the object are usually shared across all the views. The goal of CMC is to learn a compact representation of the object that captures these important factors. CMC achieves this by using CL to learn a representation such that views of the same scene map to nearby points, whereas views of different scenes map to distant points.</p>
<h2 class="heading-2" id="_idParaDest-307">Multimodal models</h2>
<p class="normal">The class of models<a id="_idIndexMarker1149"/> covered in this section includes models<a id="_idIndexMarker1150"/> that use paired inputs from two or more modalities of the same data. The input to such a model could be an image and a caption, a video and text, an audio clip and its transcript, etc. These models learn a joint embedding across multiple modalities. In this class of models, we will cover the CLIP [6] and CodeSearchNet [13] models as examples.</p>
<p class="normal">Another class of multimodal models is frameworks that can be used to do self-supervised learning across multiple modalities. The Data2Vec [7] model is an example of such a model.</p>
<h3 class="heading-3" id="_idParaDest-308">CLIP</h3>
<p class="normal">The CLIP model [6] learns image<a id="_idIndexMarker1151"/> representations by learning to predict<a id="_idIndexMarker1152"/> which image goes with which caption. It is pretrained with 400 million image-text pairs from the internet. After pretraining, the model can use natural language queries to refer to learned visual concepts. CLIP can be used in zero-shot mode for downstream tasks such as image classification, text-to-image, and image-to-image image search. The model is competitive for natural images with a fully supervised baseline without the need for any additional fine-tuning. For example, CLIP can match the accuracy of the original ResNet50 on ImageNet in zero-shot mode, i.e., without additional fine-tuning. CLIP can also be fine-tuned with specialized image datasets for specific downstream tasks, such as learning visual representations for satellite imagery or tumor detection.</p>
<p class="normal"><em class="italic">Figure 10.11</em> shows the architecture of the CLIP model for training and inference. Both image and text encoders are transformer-based encoders. The objective of pretraining is to solve the task of predicting which text as a whole is paired with which image. Thus, given a batch of <em class="italic">N</em> image-text pairs, CLIP learns to predict which of the <em class="italic">N </em>x<em class="italic"> N</em> possible image-text pairs across the batch actually occurred. CLIP learns a multi-modal joint embedding space by maximizing the cosine similarity of the image and text embeddings of the N real pairs in the batch while minimizing the cosine similarity of the rest of the <em class="italic">N</em><sup class="superscript">2</sup><em class="italic"> - N</em> incorrect pairs. </p>
<p class="normal">During inference, the input of one modality can be used to predict the output of the other, i.e., given an image, it can predict the image class as text:</p>
<figure class="mediaobject"><img alt="" height="330" src="../Images/B18331_10_11.png" width="873"/></figure>
<p class="packt_figref">Figure 10.11: Architecture of the CLIP model. From the paper: Learning Transferable Visual Models from Natural Language Supervision [34x]</p>
<p class="normal">The code snippet below demonstrates<a id="_idIndexMarker1153"/> the CLIP model’s ability to compare images<a id="_idIndexMarker1154"/> and text. Here, we take an image of two cats side by side and compare it to two text strings: <code class="inlineCode">"</code><code class="inlineCode">a photo of a cat"</code> and <code class="inlineCode">"a photo of a dog"</code>. CLIP can compare the image with the two text strings and correctly determine that the probability that the image is similar to the string <code class="inlineCode">"a photo of a cat"</code> is 0.995 as opposed to a probability of 0.005 for the image being similar to the string <code class="inlineCode">"a photo of a dog"</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">from</span> PIL <span class="hljs-keyword">import</span> Image
<span class="hljs-keyword">import</span> requests
<span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> CLIPProcessor, TFCLIPModel
model = TFCLIPModel.from_pretrained(<span class="hljs-string">"openai/clip-vit-base-patch32"</span>)
processor = CLIPProcessor.from_pretrained(<span class="hljs-string">"openai/clip-vit-base-patch32"</span>)
url = <span class="hljs-string">"http://images.cocodataset.org/val2017/000000039769.jpg"</span>
image = Image.<span class="hljs-built_in">open</span>(requests.get(url, stream=<span class="hljs-literal">True</span>).raw)
texts = [<span class="hljs-string">"a photo of a cat"</span>, <span class="hljs-string">"a photo of a dog"</span>]
inputs = processor(text=texts, images=image, return_tensors=<span class="hljs-string">"tf"</span>, padding=<span class="hljs-literal">True</span>)
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = tf.nn.softmax(logits_per_image, axis=<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(probs.numpy())
</code></pre>
<p class="normal">The CLIP model does this by projecting both text and image to a single embedding space. Using this common embedding approach, CLIP is also able to compute the similarity between two images and a text. It also offers the ability to extract encodings of text and images.</p>
<h3 class="heading-3" id="_idParaDest-309">CodeSearchNet</h3>
<p class="normal">The CodeSearchNet model [13] uses code snippets<a id="_idIndexMarker1155"/> representing functions or methods<a id="_idIndexMarker1156"/> in multiple programming languages (Go, Java, JavaScript, Python, PHP, and Ruby), and pairs them with (manually augmented) natural language comments describing the code to create positive examples. The corpus consists of approximately 2 million code-documentation pairs across all the different languages. As with CLIP, the goal of the CodeSearchNet model is to learn a joint embedding space of code and documentation, which can then be queried to return the appropriate code snippet (functions or methods) that satisfy some natural language query. The code and the natural language query are encoded using two separate encoders, and the model tries to learn a joint embedding that maximizes the inner product of the code<a id="_idIndexMarker1157"/> and query encodings<a id="_idIndexMarker1158"/> for positive pairs and minimizes it for negative pairs.</p>
<h3 class="heading-3" id="_idParaDest-310">Data2Vec</h3>
<p class="normal">Data2Vec [7] is a little different in that it proposes<a id="_idIndexMarker1159"/> a common framework to do self-supervised learning<a id="_idIndexMarker1160"/> across multiple modalities. It uses masked prediction to apply the same learning method for either speech, language, or computer vision. The core idea is to predict latent representations of the full input based on a masked view of the input. Instead of predicting modality-specific targets such as words, visual tokens, etc., it predicts contextualized latent representations that contain information for the entire input. It uses a teacher-student architecture – first, a representation of the full input data is built, which serves as the target for the learning task (teacher mode). Then a masked version of the input sample is encoded, with which the full data representation is predicted (student mode). The teacher’s parameters are updated using exponentially decaying average weights of the student. At the end of the training, the teacher’s weights are used as the learned embedding.</p>
<p class="normal">Experiments using this framework against major benchmarks in speech recognition, image classification, and natural language understanding show either state-of-the-art performance or competitive performance to popular approaches:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="234" src="../Images/B18331_10_12.png" width="882"/></figure>
<p class="packt_figref">Figure 10.12: Architecture of the Data2Vec model. From the paper: data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language [7]</p>
<h1 class="heading-1" id="_idParaDest-311">Pretext tasks</h1>
<p class="normal">Pretext tasks are tasks<a id="_idIndexMarker1161"/> that self-supervised learning models attempt to solve by leveraging some pattern inherent in the unlabeled data they train on. Such tasks are not necessarily useful in and of themselves, but they help the system learn a useful latent representation, or embeddings, that can then be used, either as-is or after fine-tuning, on some other downstream tasks. Training to solve pretext tasks usually happens as a precursor to building the actual model, and for that<a id="_idIndexMarker1162"/> reason, it is also referred to as pretraining.</p>
<p class="normal">Almost all the techniques we have discussed in this chapter have been pretext tasks. While some tasks may end up being useful in and of themselves, such as colorization or super-resolution, they also result in embeddings that end up learning the semantics of the data distribution of the unlabeled data that it was trained on, in the form of learned weights. These weights can then be applied to downstream tasks.</p>
<p class="normal">This is not a new concept – for<a id="_idIndexMarker1163"/> example, the Word2Vec algorithm, which is widely used for finding “synonyms,” is based on an embedding space where words used in similar contexts cluster together. It is trained using either the skip-gram or CBOW algorithm, which attempt to predict a context word given a word, or vice versa. Neither of these objectives are useful in and of themselves, but in the process, the network ends up learning a good latent representation of the words in the input data. This representation can then be directly used to find “synonyms” for words or do word analogies, as well as being used to produce useful vector representations of words and sequences of words (such as sentences and documents) for downstream tasks, such as text classification or sentiment analysis.</p>
<p class="normal">The biggest advantage of pretext tasks<a id="_idIndexMarker1164"/> is that the training of models for downstream tasks can be done with relatively smaller amounts of labeled data. The model learns a lot about the domain (the broad strokes) based on solving the pretext task using large quantities of readily available unlabeled data. It requires relatively smaller amounts of labeled data to learn to solve more specific downstream tasks based on what it already knows about the domain. Because labeled data is hard to come by and expensive to create, this two-step approach can often make some machine learning models possible, if not more practical.</p>
<h1 class="heading-1" id="_idParaDest-312">Summary</h1>
<p class="normal">In this chapter, we saw various self-supervised strategies for leveraging data to learn the data distribution in the form of specialized embedding spaces, which in turn can be used for solving downstream tasks. We have looked at self-prediction, contrastive learning, and pretext tasks as specific approaches for self-supervision.</p>
<p class="normal">In the next chapter, we will look at reinforcement learning, an approach that uses rewards as a feedback mechanism to train models for specific tasks.</p>
<h1 class="heading-1" id="_idParaDest-313">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Aaron van den Oord, Nal Kalchbrenner, and Koray Kavucuoglu (2016). Pixel Recurrent Neural Networks Proceedings MLR Press: <a href="http://proceedings.mlr.press/v48/oord16.pdf"><span class="url">http://proceedings.mlr.press/v48/oord16.pdf</span></a></li>
<li class="numberedList">Aaron van den Oord, Yazhe Li, and Oriol Vinyals. <em class="italic">Representation Learning with Contrastive Predictive Coding</em>. Arxiv Preprint, arXiv 1807.03748 [cs.LG]: <a href="https://arxiv.org/pdf/1807.03748.pdf"><span class="url">https://arxiv.org/pdf/1807.03748.pdf</span></a></li>
<li class="numberedList">Aaron van den Oord, et al. (2016). <em class="italic">WaveNet: A Generative Model for Raw Audio</em>. Arxiv Preprint, arXiv:1609.03499v2 [cs.SD]: <a href="https://arxiv.org/pdf/1609.03499.pdf"><span class="url">https://arxiv.org/pdf/1609.03499.pdf</span></a></li>
<li class="numberedList">Abe Fetterman and Josh Albrecht. (2020). <em class="italic">Understanding Self-Supervised and Contrastive Learning</em> with “Bootstrap your Own Latent” (BYOL). Blog post: <a href="https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/"><span class="url">https://generallyintelligent.ai/blog/2020-08-24-understanding-self-supervised-contrastive-learning/</span></a> </li>
<li class="numberedList">Aditya Ramesh, et al. <em class="italic">Zero Shot Text to Image generation</em>. Arxiv Preprint, arXiv 2102.12092v2 [cs.CV]: <a href="https://arxiv.org/pdf/2102.12092.pdf"><span class="url">https://arxiv.org/pdf/2102.12092.pdf</span></a> </li>
<li class="numberedList">Alec Radford, et al. (2021). <em class="italic">Learning Transferable Visual Models from Natural Language Supervision</em>. Proceedings of Machine Learning Research (PMLR): <a href="http://proceedings.mlr.press/v139/radford21a/radford21a.pdf"><span class="url">http://proceedings.mlr.press/v139/radford21a/radford21a.pdf</span></a></li>
<li class="numberedList">Alexei Baevsky, et al. (2022). <em class="italic">data2vec: A General Framework for Self-Supervised Learning in Speech, Vision and Language</em>. Arxiv Preprint, arXiv 2202.03555v1 [cs.LG]: <a href="https://arxiv.org/pdf/2202.03555.pdf"><span class="url">https://arxiv.org/pdf/2202.03555.pdf</span></a> </li>
<li class="numberedList">Carl Doersch, Abhinav Gupta and Alexei Efros. (2015). <em class="italic">Unsupervised Visual Representation by Context Prediction</em>. International Conference on Computer Vision (ICCV): <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf"><span class="url">https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Doersch_Unsupervised_Visual_Representation_ICCV_2015_paper.pdf</span></a> </li>
<li class="numberedList">Chuan Li. (2020). <em class="italic">OpenAI’s GPT-3 Language Model – a Technical Overview</em>. LambdaLabs Blog post: <a href="https://lambdalabs.com/blog/demystifying-gpt-3/"><span class="url">https://lambdalabs.com/blog/demystifying-gpt-3/</span></a> </li>
<li class="numberedList">Deepak Pathak, et al. (2016). <em class="italic">Context Encoders: Feature Learning by Inpainting</em>: <a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf"><span class="url">https://openaccess.thecvf.com/content_cvpr_2016/papers/Pathak_Context_Encoders_Feature_CVPR_2016_paper.pdf</span></a> </li>
<li class="numberedList">Florian Schroff, Dmitry Kalenichenko and James Philbin. (2025).<em class="italic"> FaceNet: A Unified Embedding for Face Recognition and Clustering</em>. ArXiv Preprint, arXiv 1503.03832 [cs.CV]: <a href="https://arxiv.org/pdf/1503.03832.pdf"><span class="url">https://arxiv.org/pdf/1503.03832.pdf</span></a> </li>
<li class="numberedList">Gustav Larsson, Michael Maire and Gregory Shakhnarovich. (2017). <em class="italic">Colorization as a Proxy Task for Visual Understanding</em>: <a href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Colorization_as_a_CVPR_2017_paper.pdf"><span class="url">https://openaccess.thecvf.com/content_cvpr_2017/papers/Larsson_Colorization_as_a_CVPR_2017_paper.pdf</span></a> </li>
<li class="numberedList">Hamel Husain, et al. (2020). <em class="italic">CodeSearchNet Challenge: Evaluating the State of Semantic Code Search</em>. Arxiv Preprint, arXiv: 1909.09436 [cs.LG]: <a href="https://arxiv.org/pdf/1909.09436.pdf"><span class="url">https://arxiv.org/pdf/1909.09436.pdf</span></a> </li>
<li class="numberedList">Hanting Chen, et al. (2021). <em class="italic">Pre-trained Image Processing Transformer</em>. Conference on Computer Vision and Pattern Recognition (CVPR): <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf"><span class="url">https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Pre-Trained_Image_Processing_Transformer_CVPR_2021_paper.pdf</span></a> </li>
<li class="numberedList">Hyun Oh Song, Yu Xiang, Stefanie Jegelka and Silvio Savarese. (2015). <em class="italic">Deep Metric Learning via Lifted Structured Feature Embedding</em>. Arxiv Preprint, arXiv 1511.06452 [cs.CV]: <a href="https://arxiv.org/pdf/1511.06452.pdf"><span class="url">https://arxiv.org/pdf/1511.06452.pdf</span></a> </li>
<li class="numberedList">Jacob Devlin, et al. (2019). <em class="italic">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>. Arxiv Preprint, arXiv: 1810.04805v2 [cs.CL]: <a href="https://arxiv.org/pdf/1810.04805.pdf"><span class="url">https://arxiv.org/pdf/1810.04805.pdf</span></a> </li>
<li class="numberedList">Jean-Bastien Grill, et al. (2020). <em class="italic">Bootstrap your own latent: A new approach to self-supervised learning</em>. Arxiv Preprint, arXiv 2006.07733 [cs.LG]: <a href="https://arxiv.org/pdf/2006.07733.pdf"><span class="url">https://arxiv.org/pdf/2006.07733.pdf</span></a> </li>
<li class="numberedList">Jiahao Xie, et al. (2021). <em class="italic">Delving into Inter-Image Invariance for Unsupervised Visual Representations</em>. Arxiv Preprint, arXiv: 2008.11702 [cs.CV]: <a href="https://arxiv.org/pdf/2008.11702.pdf"><span class="url">https://arxiv.org/pdf/2008.11702.pdf</span></a> </li>
<li class="numberedList">Joshua Robinson, Ching-Yao Chuang, Suvrit Sra and Stefanie Jegelka. (2021). <em class="italic">Contrastive Learning with Hard Negative Samples</em>. Arxiv Preprint, arXiv 2010.04592 [cs.LG]: <a href="https://arxiv.org/pdf/2010.04592.pdf"><span class="url">https://arxiv.org/pdf/2010.04592.pdf</span></a> </li>
<li class="numberedList">Jure Zobontar, et al. (2021). Barlow Twins: Self-Supervised Learning via Redundancy Reduction. Arxiv Preprint, arXiv 2103.03230 [cs.CV]: <a href="https://arxiv.org/pdf/2103.03230.pdf"><span class="url">https://arxiv.org/pdf/2103.03230.pdf</span></a> </li>
<li class="numberedList">Kihyuk Sohn. (2016). <em class="italic">Improved Deep Metric Learning with Multi-class N-pair Loss Objective</em>. Advances in Neural Information Processing Systems: <a href="https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf</span></a> </li>
<li class="numberedList">Lilian Weng and Jong Wook Kim. (2021). <em class="italic">Self-supervised Learning: Self Prediction and Contrastive Learning</em>. NeurIPS Tutorial: <a href="https://neurips.cc/media/neurips-2021/Slides/21895.pdf"><span class="url">https://neurips.cc/media/neurips-2021/Slides/21895.pdf</span></a></li>
<li class="numberedList">Lilian Weng. (Blog post 2021). Contrastive Representation Learning: <a href="https://lilianweng.github.io/posts/2021-05-31-contrastive/"><span class="url">https://lilianweng.github.io/posts/2021-05-31-contrastive/</span></a></li>
<li class="numberedList">Mathilde Caron, Piotr Bojanowsky, Armand Joulin and Matthijs Douze. (2019). <em class="italic">Deep Clustering for Unsupervised Learning of Visual Features</em>. Arxiv Preprint, arXiv: 1807.05520 [cs.CV]: <a href="https://arxiv.org/pdf/1807.05520.pdf"><span class="url">https://arxiv.org/pdf/1807.05520.pdf</span></a> </li>
<li class="numberedList">Mathilde Caron, et al. (2020). <em class="italic">Unsupervised Learning of Visual Features by Contrasting Cluster Assignments</em>. Arxiv Preprint, arXiv: 2006.099882 [cs.CV]: <a href="https://arxiv.org/pdf/2006.09882.pdf"><span class="url">https://arxiv.org/pdf/2006.09882.pdf</span></a> </li>
<li class="numberedList">Mehdi Noroozi and Paolo Favaro. (2016). <em class="italic">Unsupervised Learning of Visual Representations by solving Jigsaw Puzzles</em>. European Conference on Computer Vision: <a href="https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5"><span class="url">https://link.springer.com/chapter/10.1007/978-3-319-46466-4_5</span></a></li>
<li class="numberedList">Michael Gutmann, Aapo Hyvarinen. (2010). <em class="italic">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</em>. Proceedings of Machine Learning Research (PMLR): <a href="http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf"><span class="url">http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf</span></a> </li>
<li class="numberedList">Nal Kalchbrenner, et al. (2018). <em class="italic">Efficient Neural Audio Synthesis</em>. Proceedings MLR Press: <a href="http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf"><span class="url">http://proceedings.mlr.press/v80/kalchbrenner18a/kalchbrenner18a.pdf</span></a></li>
<li class="numberedList">Pascal Vincent, et al. (2010). <em class="italic">Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion</em>. Journal of Machine Learning Research (JMLR): <a href="https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com"><span class="url">https://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?ref=https://githubhelp.com</span></a> </li>
<li class="numberedList">Patrick Esser, Robin Rombach and Bjorn Ommer. (2021). Taming Transformers for High-Resolution Image Synthesis. Computer Vision and Pattern Recognition (CVPR): <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf"><span class="url">https://openaccess.thecvf.com/content/CVPR2021/papers/Esser_Taming_Transformers_for_High-Resolution_Image_Synthesis_CVPR_2021_paper.pdf</span></a> </li>
<li class="numberedList">Philip Bachman, R Devon Hjelm and William Buchwalter. (2019). <em class="italic">Learning Representations by Maximizing Mutual Information across Views. Advances in Neural Information Processing Systems (NeurIPS)</em>: <a href="https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/2019/file/ddf354219aac374f1d40b7e760ee5bb7-Paper.pdf</span></a> </li>
<li class="numberedList">Prafulla Dhariwal, et al. (2020). Jukebox: <em class="italic">A Generative Model for Music</em>. Arxiv Preprint, arXiv 2005.00341v1 [eess.AS]: <a href="https://arxiv.org/pdf/2005.00341.pdf"><span class="url">https://arxiv.org/pdf/2005.00341.pdf</span></a> </li>
<li class="numberedList">Ruslan Salakhutdinov and Geoff Hinton. (2007). <em class="italic">Learning a Nonlinear Embedding by Preserving Class Neighborhood Structure.</em> Proceedings of Machine Learning Research (PMLR): <a href="http://proceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf"><span class="url">http://proceedings.mlr.press/v2/salakhutdinov07a/salakhutdinov07a.pdf</span></a> </li>
<li class="numberedList">Spyros Gidaris, Praveer Singh and Nicos Komodakis. (2018). <em class="italic">Unsupervised Representation Learning by Predicting Image Rotations</em>. Arxiv Preprint, arXiv 1803.07728v1 [cs.CV]: <a href="https://arxiv.org/pdf/1803.07728.pdf"><span class="url">https://arxiv.org/pdf/1803.07728.pdf</span></a> </li>
<li class="numberedList">Sumit Chopra, et al. (2005). <em class="italic">Learning a Similarity Metric Discriminatively, with application to Face Verification</em>. IEEE Computer Society: <a href="http://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf"><span class="url">http://www.cs.utoronto.ca/~hinton/csc2535_06/readings/chopra-05.pdf</span></a> </li>
<li class="numberedList">Ting Chen, Simon Kornblith, Mohammed Norouzi and Geoffrey Hinton. (2020). <em class="italic">A Simple Framework for Contrastive Learning</em>. Arxiv Preprint, arXiv 2002.05709 [cs.LG]: <a href="https://arxiv.org/pdf/2002.05709.pdf"><span class="url">https://arxiv.org/pdf/2002.05709.pdf</span></a> </li>
<li class="numberedList">Yonglong Tian, Dilip Krishnan and Philip Isola. (2020). <em class="italic">Contrastive Multiview Coding</em>. Arxiv Preprint, arXiv: 1906.05849 [cs.CV]: <a href="https://arxiv.org/pdf/1906.05849.pdf?ref=https://githubhelp.com"><span class="url">https://arxiv.org/pdf/1906.05849.pdf?ref=https://githubhelp.com</span></a> </li>
<li class="numberedList">Zhilin Yang, et al. (2019). <em class="italic">XLNet: Generalized Autoregressive Pre-training for Language Understanding</em>: <a href="https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf"><span class="url">https://proceedings.neurips.cc/paper/2019/file/dc6a7e655d7e5840e66733e9ee67cc69-Paper.pdf</span></a> </li>
<li class="numberedList"><em class="italic">Prompt Engineering</em>. (7th July 2022). Wikipedia, Wikimedia Foundation: <a href="https://en.wikipedia.org/wiki/Prompt_engineering"><span class="url">https://en.wikipedia.org/wiki/Prompt_engineering</span></a></li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>