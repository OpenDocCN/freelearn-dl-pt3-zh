<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Autoencoders – Feature Extraction and Denoising</h1>
                </header>
            
            <article>
                
<p class="calibre2">An autoencoder network is nowadays one of the widely used deep learning architectures. It's mainly used for unsupervised learning of efficient decoding tasks. It can also be used for dimensionality reduction by learning an encoding or a representation for a specific dataset. Using autoencoders in this chapter, we'll show how to denoise your dataset by constructing another dataset with the same dimensions but less noise. To use this concept in practice, we will extract the important features from the MNIST dataset and try to see how the performance will be significantly enhanced by this.</p>
<p class="calibre2">The following topics will be covered in this chapter:</p>
<ul class="calibre7">
<li class="calibre8">Introduction to autoencoders</li>
<li class="calibre8">Examples of autoencoders</li>
<li class="calibre8">Autoencoder architectures</li>
<li class="calibre8">Compressing the MNIST dataset</li>
<li class="calibre8">Convolutional autoencoders</li>
<li class="calibre8">Denoising autoencoders</li>
<li class="calibre8">Applications of autoencoders</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction to autoencoders</h1>
                </header>
            
            <article>
                
<p class="calibre2">An autoencoder is yet another deep learning architecture that can be used for many interesting tasks, but it can also be considered as a variation of the vanilla feed-forward neural network, where the output has the same dimensions as the input. As shown in <em class="calibre19">Figure 1</em>, the way autoencoders work is by feeding data samples <em class="calibre19">(x<sub class="calibre28">1</sub>,...,x<sub class="calibre28">6</sub>) </em>to the network. It will try to learn a lower representation of this data in layer <em class="calibre19">L2</em>, which you might call a way of encoding your dataset in a lower representation. Then, the second part of the network, which you might call a decoder, is responsible for constructing an output from this representation <img class="fm-editor-equation32" src="assets/f1bff437-7017-445b-93d4-7886dd3f1623.png"/>. You can think of the intermediate lower representation that the network learns from the input data as a compressed version of it.</p>
<p class="calibre2">Not very different from all the other deep learning architectures that we have seen so far, autoencoders use backpropagation algorithms.</p>
<p class="calibre2">An autoencoder neural network is an unsupervised learning algorithm that applies backpropagation, setting the target values to be equal to the inputs:</p>
<div class="CDPAlignCenter"><img src="assets/e447a3cc-4dd7-4ae1-bba0-fd6c845ebbe5.png" class="calibre153"/></div>
<div class="CDPAlignCenter1">Figure 1: General autoencoder architecture</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Examples of autoencoders</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we will demonstrate some examples of different variations of autoencoders using the MNIST dataset. As a concrete example, suppose the inputs <em class="calibre19">x</em> are the pixel intensity values from a 28 x 28 image (784 pixels); so the number of input data samples is <em class="calibre19">n=784</em>. There are <em class="calibre19">s2=392</em> hidden units in layer <em class="calibre19">L2</em>. And since the output will be of the same dimensions as the input data samples, <em class="calibre19">y ∈ R784</em>. The number of neurons in the input layer will be <em class="calibre19">784</em>, followed by <em class="calibre19">392</em> neurons in the middle layer <em class="calibre19">L2</em>; so the network will be a lower representation, which is a compressed version of the output. The network will then feed this compressed lower representation of the input <em class="calibre19">a(L2) ∈ R392</em> to the second part of the network, which will try hard to reconstruct the input pixels <em class="calibre19">784</em> from this compressed version.</p>
<p class="calibre2">Autoencoders rely on the fact that the input samples represented by the image pixels will be somehow correlated and then it will use this fact to reconstruct them. So autoencoders are a bit similar to dimensionality reduction techniques, because they learn a lower representation of the input data as well.</p>
<p class="calibre2">To sum up, a typical autoencoder will consist of three parts:</p>
<ol class="calibre16">
<li class="calibre8">The encoder part, which is responsible for compressing the input into a lower representation</li>
<li class="calibre8">The code, which is the intermediate result of the encoder</li>
<li class="calibre8">The decoder, which is responsible for reconstructing the the original input using this code</li>
</ol>
<p class="calibre2">The following figure shows the three <span class="calibre10">main</span><span class="calibre10"> </span><span class="calibre10">components of a typical autoencoder:</span></p>
<div class="CDPAlignCenter"><img src="assets/3f678096-4330-4dfb-b907-c4f22b8c3971.png" class="calibre154"/></div>
<div class="CDPAlignCenter1">Figure 2: How encoders function over an image</div>
<p class="calibre2">As we mentioned, autoencoders part learn a compressed representation of the input that are then fed to the third part, which tries to reconstruct the input. The reconstructed input will be similar to the output but it won't be exactly the same as the original output, so autoencoders can't be used for compression tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Autoencoder architectures</h1>
                </header>
            
            <article>
                
<p class="calibre2">As we mentioned, a typical autoencoder consists of three parts. Let's explore these three parts in more detail. To motivate you, we are not going to reinvent the wheel here in this chapter. The encoder-decoder part is nothing but a fully connected neural network, and the code part is another neural network but it's not fully connected. The dimensionality of this code part is controllable and we can treat it as a hyperparameter:</p>
<div class="CDPAlignCenter"><img src="assets/552fb5f6-0459-4e13-887b-d55d288e0f72.png" class="calibre155"/></div>
<div class="CDPAlignCenter1">Figure 3: General encoder-decoder architecture of autoencoders</div>
<p class="calibre2">Before diving into using autoencoders for compressing the MNIST dataset, we are going to list the set of hyperparameters that we can use to fine-tune the autoencoder model. There are m<span class="calibre10">ainly </span>four hyperparameters:</p>
<ol class="calibre16">
<li class="calibre8"><strong class="calibre1">Code part size</strong>: This is the number of units in the middle layer. The lower the number of units we have in this layer, the more compressed the representation of the input we get.</li>
<li class="calibre8"><strong class="calibre1">Number of layers in the encoder and decoder</strong>: As we mentioned, the encoder and decoder are nothing but a fully connected neural network that we can make as deep as we can by adding more layers.</li>
<li class="calibre8"><strong class="calibre1">Number of units per layer</strong>: We can also use a different number of units in each layer. The shape of the encoder and decoder is very similar to DeconvNets, where the number of layers in the encoders decreases as we approach the code part and then starts to increase as we approach the final layer of the decoder.</li>
<li class="calibre8"><strong class="calibre1">Model loss function</strong>: We can use different loss functions as well, such as MSE or cross-entropy.</li>
</ol>
<p class="calibre2">After defining these hyperparameters and giving them initial values, we can train the network using a backpropagation algorithm.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Compressing the MNIST dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this part, we'll build a simple autoencoder that can be used to compress the MNIST dataset. So we will feed the images of this dataset to the encoder part, which will try to learn a lower compressed representation for them; then we will try to construct the input images again in the decoder part.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The MNIST dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">We will start the implementation by getting the MNIST dataset, using the helper functions of TensorFlow.</p>
<p class="calibre2">Let's import the necessary packages for this implementation:</p>
<pre class="calibre21">%matplotlib inline<br class="title-page-name"/><br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<pre class="calibre21"><br class="title-page-name"/>from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/>mnist_dataset = input_data.read_data_sets('MNIST_data', validation_size=0)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>Extracting MNIST_data/train-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/train-labels-idx1-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</pre>
<p class="calibre2">Let's start off by plotting some examples from the MNIST dataset:</p>
<pre class="calibre21"># Plotting one image from the training set.<br class="title-page-name"/>image = mnist_dataset.train.images[2]<br class="title-page-name"/>plt.imshow(image.reshape((28, 28)), cmap='Greys_r')</pre>
<pre class="calibre21">Output:</pre>
<div class="CDPAlignCenter"><img src="assets/9ca739a8-c53b-48ee-b8b3-18f455abca36.png" class="calibre156"/></div>
<div class="CDPAlignCenter1">Figure 4: Example image from the MNIST dataset</div>
<div class="title-page-name">
<pre class="calibre21"># Plotting one image from the training set.<br class="title-page-name"/>image = mnist_dataset.train.images[2]<br class="title-page-name"/>plt.imshow(image.reshape((28, 28)), cmap='Greys_r')<br class="title-page-name"/><br class="title-page-name"/>Output:</pre></div>
<div class="CDPAlignCenter"><img src="assets/25c86e3f-00cd-4eda-a9de-ed311292e709.png" class="calibre157"/></div>
<div class="title-page-name">
<div class="CDPAlignCenter1">Figure 5: Example image from the MNIST dataset</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<p class="calibre2">In order to build the encoder, we need to figure out how many pixels each MNIST image will have so that we can figure out the size of the input layer of the encoder. Each image from the MNIST dataset is 28 by 28 pixels, so we will reshape this matrix to a vector of 28 x 28 = 784 pixel values. We don't have to normalize the images of MNIST because they are<span class="calibre10"> </span><span class="calibre10">already <span class="calibre10">normalized</span></span><span class="calibre10">.</span></p>
<p class="calibre2">Let's start off building our three components of the model. In this implementation, we will use a very simple architecture of a single hidden layer followed by ReLU activation, as shown in the following figure:</p>
<div class="CDPAlignCenter"><img src="assets/a887c40b-4430-4ce7-95c3-8ebf56ad2b69.png" class="calibre158"/></div>
<div class="CDPAlignCenter1">Figure 6: Encoder-decoder architecture for MNIST implementation</div>
<p class="calibre2">Let's go ahead and implement this simple encoder-decoder architecture according to the preceding explanation:</p>
<pre class="calibre21"># The size of the encoding layer or the hidden layer.<br class="title-page-name"/>encoding_layer_dim = 32 <br class="title-page-name"/><br class="title-page-name"/>img_size = mnist_dataset.train.images.shape[1]<br class="title-page-name"/><br class="title-page-name"/># defining placeholder variables of the input and target values<br class="title-page-name"/>inputs_values = tf.placeholder(tf.float32, (None, img_size), name="inputs_values")<br class="title-page-name"/>targets_values = tf.placeholder(tf.float32, (None, img_size), name="targets_values")<br class="title-page-name"/><br class="title-page-name"/># Defining an encoding layer which takes the input values and incode them.<br class="title-page-name"/>encoding_layer = tf.layers.dense(inputs_values, encoding_layer_dim, activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># Defining the logit layer, which is a fully-connected layer but without any activation applied to its output<br class="title-page-name"/>logits_layer = tf.layers.dense(encoding_layer, img_size, activation=None)<br class="title-page-name"/><br class="title-page-name"/># Adding a sigmoid layer after the logit layer<br class="title-page-name"/>decoding_layer = tf.sigmoid(logits_layer, name = "decoding_layer")<br class="title-page-name"/><br class="title-page-name"/># use the sigmoid cross entropy as a loss function<br class="title-page-name"/>model_loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=logits_layer, labels=targets_values)<br class="title-page-name"/><br class="title-page-name"/># Averaging the loss values accross the input data<br class="title-page-name"/>model_cost = tf.reduce_mean(model_loss)<br class="title-page-name"/><br class="title-page-name"/># Now we have a cost functiont that we need to optimize using Adam Optimizer<br class="title-page-name"/>model_optimizier = tf.train.AdamOptimizer().minimize(model_cost)</pre>
<p class="calibre2">Now we have defined our model and also used a binary cross-entropy since the images, pixels are already normalized.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this section, we'll kick off the training process. We'll use the helper function of the <kbd class="calibre12">mnist_dataset</kbd> object in order to get a random batch from the dataset with a specific size; then we'll run the optimizer on this batch of images.</p>
<p class="calibre2">Let's start this section by creating the session variable, which will be responsible for executing the computational graph that we defined earlier:</p>
<pre class="calibre21"># creating the session<br class="title-page-name"/> sess = tf.Session()</pre>
<p class="calibre2">Next up, let's kick off the training process:</p>
<pre class="calibre21">num_epochs = 20<br class="title-page-name"/>train_batch_size = 200<br class="title-page-name"/><br class="title-page-name"/>sess.run(tf.global_variables_initializer())<br class="title-page-name"/>for e in range(num_epochs):<br class="title-page-name"/>    for ii in range(mnist_dataset.train.num_examples//train_batch_size):<br class="title-page-name"/>        input_batch = mnist_dataset.train.next_batch(train_batch_size)<br class="title-page-name"/>        feed_dict = {inputs_values: input_batch[0], targets_values: input_batch[0]}<br class="title-page-name"/>        input_batch_cost, _ = sess.run([model_cost, model_optimizier], feed_dict=feed_dict)<br class="title-page-name"/><br class="title-page-name"/>        print("Epoch: {}/{}...".format(e+1, num_epochs),<br class="title-page-name"/>              "Training loss: {:.3f}".format(input_batch_cost))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.089<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.096<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.096<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.092<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.096<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.089<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.090<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.088<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.090<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.091<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.095<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.094<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.092<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.092<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.093</pre>
<p class="calibre2">After running the preceding code snippet for 20 epochs, we will get a trained model that is able to generate or reconstruct images from the test set of the MNIST data. Bear in mind that if we feed images that are not similar to the ones that the model was trained on, then the reconstruction process just won't work because autoencoders are data-specific.</p>
<p class="calibre2">Let's test the trained model by feeding some images from the test set and see how the model is able to reconstruct them in the decoder part:</p>
<pre class="calibre21">fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))<br class="title-page-name"/><br class="title-page-name"/>input_images = mnist_dataset.test.images[:10]<br class="title-page-name"/>reconstructed_images, compressed_images = sess.run([decoding_layer, encoding_layer], feed_dict={inputs_values: input_images})<br class="title-page-name"/><br class="title-page-name"/>for imgs, row in zip([input_images, reconstructed_images], axes):<br class="title-page-name"/>    for img, ax in zip(imgs, row):<br class="title-page-name"/>        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')<br class="title-page-name"/>        ax.get_xaxis().set_visible(False)<br class="title-page-name"/>        ax.get_yaxis().set_visible(False)<br class="title-page-name"/><br class="title-page-name"/>fig.tight_layout(pad=0.1)</pre>
<p class="calibre2">Output:</p>
<div class="CDPAlignCenter"><img src="assets/7167052e-0b8f-4049-9738-4cca4e2c0e0f.png" class="calibre159"/></div>
<div class="CDPAlignCenter1">Figure 7: Examples of the original test images (first row) and their constructions (second row)</div>
<p class="calibre2">As you can see, the reconstructed images are very close to the input ones, but we can probably get better images using convolution layers in the <span class="calibre10">encoder-</span>decoder part.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Convolutional autoencoder</h1>
                </header>
            
            <article>
                
<p class="calibre2">The previous simple implementation did a good job while trying to reconstruct input images from the MNIST dataset, but we can get a better performance through a convolution layer in the encoder and the decoder parts of the autoencoder. The resulting network of this replacement is called <strong class="calibre13">convolutional autoencoder</strong> (<strong class="calibre13">CAE</strong>). This flexibility of being able to replace layers is a great advantage of autoencoders and makes them applicable to different domains. </p>
<p class="calibre2">The architecture that we'll be using for the CAE will contain upsampling layers in the decoder part of the network to get the reconstructed version of the image.<br class="calibre20"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Dataset</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this implementation, we can use any kind of imaging dataset and see how the convolutional version of the autoencoder will make a difference. We will still be using the MNIST dataset for this, so let's start off by getting the dataset using the TensorFlow helpers:</p>
<pre class="calibre21">%matplotlib inline<br class="title-page-name"/><br class="title-page-name"/>import numpy as np<br class="title-page-name"/>import tensorflow as tf<br class="title-page-name"/>import matplotlib.pyplot as plt</pre>
<pre class="calibre21">from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/>mnist_dataset = input_data.read_data_sets('MNIST_data', validation_size=0)<br class="title-page-name"/><br class="title-page-name"/>Output:<br class="title-page-name"/>from tensorflow.examples.tutorials.mnist import input_data<br class="title-page-name"/><br class="title-page-name"/>mnist_dataset = input_data.read_data_sets('MNIST_data', validation_size=0)<br class="title-page-name"/><br class="title-page-name"/>Extracting MNIST_data/train-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/train-labels-idx1-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-images-idx3-ubyte.gz<br class="title-page-name"/>Extracting MNIST_data/t10k-labels-idx1-ubyte.gz</pre>
<p class="calibre2">Let's show one digit from the dataset:</p>
<pre class="calibre21"># Plotting one image from the training set.<br class="title-page-name"/>image = mnist_dataset.train.images[2]<br class="title-page-name"/>plt.imshow(image.reshape((28, 28)), cmap='Greys_r')</pre>
<p class="calibre2">Output:</p>
<div class="CDPAlignCenter"><img src="assets/2105032e-0770-41d3-af6a-85e6f825f43f.png" class="calibre160"/></div>
<div class="CDPAlignCenter1">Figure 8: Example image from the MNIST dataset</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">In this implementation, we will be using convolution layers with stride 1, and the padding parameter is set to be the same. By this, we won't change the height or width of the image. Also, we are using a set of max pooling layers to reduce the width and height of the image and hence building a compressed lower representation of the image.</p>
<p class="calibre2">So let's go ahead and build the core of our network:</p>
</div>
<pre class="calibre21">learning_rate = 0.001<br class="title-page-name"/><br class="title-page-name"/># Define the placeholder variable sfor the input and target values<br class="title-page-name"/>inputs_values = tf.placeholder(tf.float32, (None, 28,28,1), name="inputs_values")<br class="title-page-name"/>targets_values = tf.placeholder(tf.float32, (None, 28,28,1), name="targets_values")<br class="title-page-name"/><br class="title-page-name"/># Defining the Encoder part of the netowrk<br class="title-page-name"/># Defining the first convolution layer in the encoder parrt<br class="title-page-name"/># The output tenosor will be in the shape of 28x28x16<br class="title-page-name"/>conv_layer_1 = tf.layers.conv2d(inputs=inputs_values, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x16<br class="title-page-name"/>maxpool_layer_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x8<br class="title-page-name"/>conv_layer_2 = tf.layers.conv2d(inputs=maxpool_layer_1, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x8<br class="title-page-name"/>maxpool_layer_2 = tf.layers.max_pooling2d(conv_layer_2, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x8<br class="title-page-name"/>conv_layer_3 = tf.layers.conv2d(inputs=maxpool_layer_2, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 4x4x8<br class="title-page-name"/>encoded_layer = tf.layers.max_pooling2d(conv_layer_3, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># Defining the Decoder part of the netowrk<br class="title-page-name"/># Defining the first upsampling layer in the decoder part<br class="title-page-name"/># The output tenosor will be in the shape of 7x7x8<br class="title-page-name"/>upsample_layer_1 = tf.image.resize_images(encoded_layer, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x8<br class="title-page-name"/>conv_layer_4 = tf.layers.conv2d(inputs=upsample_layer_1, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x8<br class="title-page-name"/>upsample_layer_2 = tf.image.resize_images(conv_layer_4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x8<br class="title-page-name"/>conv_layer_5 = tf.layers.conv2d(inputs=upsample_layer_2, filters=8, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x8<br class="title-page-name"/>upsample_layer_3 = tf.image.resize_images(conv_layer_5, size=(28,28), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x16<br class="title-page-name"/>conv6 = tf.layers.conv2d(inputs=upsample_layer_3, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x1<br class="title-page-name"/>logits_layer = tf.layers.conv2d(inputs=conv6, filters=1, kernel_size=(3,3), padding='same', activation=None)<br class="title-page-name"/><br class="title-page-name"/># feeding the logits values to the sigmoid activation function to get the reconstructed images<br class="title-page-name"/>decoded_layer = tf.nn.sigmoid(logits_layer)<br class="title-page-name"/><br class="title-page-name"/># feeding the logits to sigmoid while calculating the cross entropy<br class="title-page-name"/>model_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_values, logits=logits_layer)<br class="title-page-name"/><br class="title-page-name"/># Getting the model cost and defining the optimizer to minimize it<br class="title-page-name"/>model_cost = tf.reduce_mean(model_loss)<br class="title-page-name"/>model_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_cost)</pre>
<p class="calibre2">Now we are good to go. We've built the decoder-decoder part of the convolutional neural network while showing how the dimensions of the input image will be reconstructed in the decoder part.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training</h1>
                </header>
            
            <article>
                
<p class="calibre2">Now that we have the model built, we can kick off the learning process by generating random batches form the MNIST dataset and feed them to the optimizer <span class="calibre10">defined</span><span class="calibre10"> </span><span class="calibre10">earlier</span><span class="calibre10">.</span></p>
<p class="calibre2">Let's start off by creating the session variable; it will be responsible for executing the computational graph that we defined earlier:</p>
<pre class="calibre21">sess = tf.Session()<br class="title-page-name"/>num_epochs = 20<br class="title-page-name"/>train_batch_size = 200<br class="title-page-name"/>sess.run(tf.global_variables_initializer())<br class="title-page-name"/><br class="title-page-name"/>for e in range(num_epochs):<br class="title-page-name"/>    for ii in range(mnist_dataset.train.num_examples//train_batch_size):<br class="title-page-name"/>        input_batch = mnist_dataset.train.next_batch(train_batch_size)<br class="title-page-name"/>        input_images = input_batch[0].reshape((-1, 28, 28, 1))<br class="title-page-name"/>        input_batch_cost, _ = sess.run([model_cost, model_optimizer], feed_dict={inputs_values: input_images,targets_values: input_images})<br class="title-page-name"/><br class="title-page-name"/>        print("Epoch: {}/{}...".format(e+1, num_epochs),<br class="title-page-name"/>              "Training loss: {:.3f}".format(input_batch_cost))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.105<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.096<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.103<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.104<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.101<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.099<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.098<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.100<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.097<br class="title-page-name"/>Epoch: 20/20... Training loss: 0.102</pre>
<p class="calibre2">After running the preceding code snippet for 20 epochs, we'll get a trained CAE, so let's go ahead and test this model by feeding similar images from the MNIST dataset:</p>
<div class="title-page-name">
<pre class="calibre21">fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))<br class="title-page-name"/>input_images = mnist_dataset.test.images[:10]<br class="title-page-name"/>reconstructed_images = sess.run(decoded_layer, feed_dict={inputs_values: input_images.reshape((10, 28, 28, 1))})<br class="title-page-name"/><br class="title-page-name"/>for imgs, row in zip([input_images, reconstructed_images], axes):<br class="title-page-name"/>    for img, ax in zip(imgs, row):<br class="title-page-name"/>        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')<br class="title-page-name"/>        ax.get_xaxis().set_visible(False)<br class="title-page-name"/>        ax.get_yaxis().set_visible(False)<br class="title-page-name"/><br class="title-page-name"/>fig.tight_layout(pad=0.1)<br class="title-page-name"/><br class="title-page-name"/>Output:</pre>
<div class="CDPAlignCenter"><img src="assets/38895773-80a4-4f5d-b572-2f728dac20dd.png" class="calibre161"/></div>
<div class="CDPAlignCenter1">Figure 9: Examples of the original test images (first row) and their constructions (second row) using the convolution autoencoder</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Denoising autoencoders</h1>
                </header>
            
            <article>
                
<p class="calibre2">We can take the autoencoder architecture further by forcing it to learn more important features about the input data. By adding noise to the input images and having the original ones as the target, the model will try to remove this noise and learn important features about them in order to come up with meaningful reconstructed images in the output. This kind of CAE architecture can be used to remove noise from input images. This specific variation of autoencoders is called <strong class="calibre13">denoising autoencoder</strong>:</p>
<div class="CDPAlignCenter"><img src="assets/5100d26b-63c5-4c69-93cd-4fb8df5ddcb2.png" class="calibre162"/></div>
<div class="CDPAlignCenter1">Figure 10: Examples of original images and the same images after adding a bit of Gaussian noise</div>
<p class="calibre2">So let's start off by implementing the architecture in the following figure. The only extra thing that we have added to this denoising autoencoder architecture is some noise in the original input image:</p>
<div class="CDPAlignCenter"><img src="assets/5a04b580-1641-457a-8609-cf2392f35c28.png" class="calibre163"/></div>
<div class="CDPAlignCenter1">Figure 11: General denoising architecture of autoencoders</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building the model</h1>
                </header>
            
            <article>
                
<div class="title-page-name">
<p class="calibre2">In this implementation, we will be using more layers in the encoder and decoder part, and the reason for this is the new complexity that we have added to the input.</p>
<p class="calibre2">The next model is exactly the same as the previous CAE but with extra layers that will help us to reconstruct a noise-free image from a noisy one.</p>
</div>
<p class="calibre2">So let's go ahead and build this architecture:</p>
<pre class="calibre21">learning_rate = 0.001<br class="title-page-name"/><br class="title-page-name"/># Define the placeholder variable sfor the input and target values<br class="title-page-name"/>inputs_values = tf.placeholder(tf.float32, (None, 28, 28, 1), name='inputs_values')<br class="title-page-name"/>targets_values = tf.placeholder(tf.float32, (None, 28, 28, 1), name='targets_values')<br class="title-page-name"/><br class="title-page-name"/># Defining the Encoder part of the netowrk<br class="title-page-name"/># Defining the first convolution layer in the encoder parrt<br class="title-page-name"/># The output tenosor will be in the shape of 28x28x32<br class="title-page-name"/>conv_layer_1 = tf.layers.conv2d(inputs=inputs_values, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x32<br class="title-page-name"/>maxpool_layer_1 = tf.layers.max_pooling2d(conv_layer_1, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x32<br class="title-page-name"/>conv_layer_2 = tf.layers.conv2d(inputs=maxpool_layer_1, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x32<br class="title-page-name"/>maxpool_layer_2 = tf.layers.max_pooling2d(conv_layer_2, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x16<br class="title-page-name"/>conv_layer_3 = tf.layers.conv2d(inputs=maxpool_layer_2, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 4x4x16<br class="title-page-name"/>encoding_layer = tf.layers.max_pooling2d(conv_layer_3, pool_size=(2,2), strides=(2,2), padding='same')<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># Defining the Decoder part of the netowrk<br class="title-page-name"/># Defining the first upsampling layer in the decoder part<br class="title-page-name"/># The output tenosor will be in the shape of 7x7x16<br class="title-page-name"/>upsample_layer_1 = tf.image.resize_images(encoding_layer, size=(7,7), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 7x7x16<br class="title-page-name"/>conv_layer_4 = tf.layers.conv2d(inputs=upsample_layer_1, filters=16, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x16<br class="title-page-name"/>upsample_layer_2 = tf.image.resize_images(conv_layer_4, size=(14,14), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 14x14x32<br class="title-page-name"/>conv_layer_5 = tf.layers.conv2d(inputs=upsample_layer_2, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x32<br class="title-page-name"/>upsample_layer_3 = tf.image.resize_images(conv_layer_5, size=(28,28), method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x32<br class="title-page-name"/>conv_layer_6 = tf.layers.conv2d(inputs=upsample_layer_3, filters=32, kernel_size=(3,3), padding='same', activation=tf.nn.relu)<br class="title-page-name"/><br class="title-page-name"/># The output tenosor will be in the shape of 28x28x1<br class="title-page-name"/>logits_layer = tf.layers.conv2d(inputs=conv_layer_6, filters=1, kernel_size=(3,3), padding='same', activation=None)<br class="title-page-name"/><br class="title-page-name"/><br class="title-page-name"/># feeding the logits values to the sigmoid activation function to get the reconstructed images<br class="title-page-name"/>decoding_layer = tf.nn.sigmoid(logits_layer)<br class="title-page-name"/><br class="title-page-name"/># feeding the logits to sigmoid while calculating the cross entropy<br class="title-page-name"/>model_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=targets_values, logits=logits_layer)<br class="title-page-name"/><br class="title-page-name"/># Getting the model cost and defining the optimizer to minimize it<br class="title-page-name"/>model_cost = tf.reduce_mean(model_loss)<br class="title-page-name"/>model_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(model_cost)</pre>
<p class="calibre2">Now we have a more complex or deeper version of the convolutional model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Model training</h1>
                </header>
            
            <article>
                
<p class="calibre2">It's time to start training this deeper network, which in turn will take more time to converge by reconstructing noise-free images from the noisy input.</p>
<p class="calibre2">So let's start off by creating the session variable:</p>
<pre class="calibre21">sess = tf.Session()</pre>
<p class="calibre2">Next up, we will kick off the training process but for more number of epochs:</p>
<pre class="calibre21">num_epochs = 100<br class="title-page-name"/>train_batch_size = 200<br class="title-page-name"/><br class="title-page-name"/># Defining a noise factor to be added to MNIST dataset<br class="title-page-name"/>mnist_noise_factor = 0.5<br class="title-page-name"/>sess.run(tf.global_variables_initializer())<br class="title-page-name"/><br class="title-page-name"/>for e in range(num_epochs):<br class="title-page-name"/>    for ii in range(mnist_dataset.train.num_examples//train_batch_size):<br class="title-page-name"/>        input_batch = mnist_dataset.train.next_batch(train_batch_size)<br class="title-page-name"/>        <br class="title-page-name"/>        # Getting and reshape the images from the corresponding batch<br class="title-page-name"/>        batch_images = input_batch[0].reshape((-1, 28, 28, 1))<br class="title-page-name"/>        <br class="title-page-name"/>        # Add random noise to the input images<br class="title-page-name"/>        noisy_images = batch_images + mnist_noise_factor * np.random.randn(*batch_images.shape)<br class="title-page-name"/>        <br class="title-page-name"/>        # Clipping all the values that are above 0 or above 1<br class="title-page-name"/>        noisy_images = np.clip(noisy_images, 0., 1.)<br class="title-page-name"/>        <br class="title-page-name"/>        # Set the input images to be the noisy ones and the original images to be the target<br class="title-page-name"/>        input_batch_cost, _ = sess.run([model_cost, model_optimizer], feed_dict={inputs_values: noisy_images,<br class="title-page-name"/>                                                         targets_values: batch_images})<br class="title-page-name"/><br class="title-page-name"/>        print("Epoch: {}/{}...".format(e+1, num_epochs),<br class="title-page-name"/>              "Training loss: {:.3f}".format(input_batch_cost))</pre>
<pre class="calibre21">Output:<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>.<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.103<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.103<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.096<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.103<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.096<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.102<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.103<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.098<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.097<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.099<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.100<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101<br class="title-page-name"/>Epoch: 100/100... Training loss: 0.101</pre>
<p class="calibre2">Now we have trained the model to be able to produce noise-free images, which makes autoencoders applicable to many domains.</p>
<p class="calibre2">In the next snippet of code, we will not feed the row images of the MNIST test set to the model as we need to add noise to these images first to see how the trained model will be able to produce noise-free images.</p>
<p class="calibre2">Here I'm adding noise to the test images and passing them through the autoencoder. It does a surprisingly great job of removing the noise, even though it's sometimes difficult to tell what the original number is:</p>
<pre class="calibre21">#Defining some figures<br class="title-page-name"/>fig, axes = plt.subplots(nrows=2, ncols=10, sharex=True, sharey=True, figsize=(20,4))<br class="title-page-name"/><br class="title-page-name"/>#Visualizing some images<br class="title-page-name"/>input_images = mnist_dataset.test.images[:10]<br class="title-page-name"/>noisy_imgs = input_images + mnist_noise_factor * np.random.randn(*input_images.shape)<br class="title-page-name"/><br class="title-page-name"/>#Clipping and reshaping the noisy images<br class="title-page-name"/>noisy_images = np.clip(noisy_images, 0., 1.).reshape((10, 28, 28, 1))<br class="title-page-name"/><br class="title-page-name"/>#Getting the reconstructed images<br class="title-page-name"/>reconstructed_images = sess.run(decoding_layer, feed_dict={inputs_values: noisy_images})<br class="title-page-name"/><br class="title-page-name"/>#Visualizing the input images and the noisy ones<br class="title-page-name"/>for imgs, row in zip([noisy_images, reconstructed_images], axes):<br class="title-page-name"/>    for img, ax in zip(imgs, row):<br class="title-page-name"/>        ax.imshow(img.reshape((28, 28)), cmap='Greys_r')<br class="title-page-name"/>        ax.get_xaxis().set_visible(False)<br class="title-page-name"/>        ax.get_yaxis().set_visible(False)<br class="title-page-name"/><br class="title-page-name"/>fig.tight_layout(pad=0.1)</pre>
<pre class="calibre21"><br class="title-page-name"/>Output:</pre>
<div class="CDPAlignCenter"><img src="assets/14c7f119-bf36-4b37-aa0a-777b02b5703a.png" class="calibre164"/></div>
<div class="CDPAlignCenter1">Figure 12: Examples of original test images with some Gaussian noise (top row) and their construction based on the trained denoising autoencoder</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applications of autoencoders</h1>
                </header>
            
            <article>
                
<p class="calibre2">In the previous example of constructing images from a lower representation, we saw it was very similar to the original input, and also we saw the benefits of CANs while denoising the noisy dataset. This kind of example we have implemented above is really useful for the image construction applications and dataset denoising. So you can generalize the above implementation to any other example of interest to you.</p>
<p class="calibre2">Also, throughout this chapter, we have seen how flexible the autoencoder architecture <span class="calibre10">is</span><span class="calibre10"> </span><span class="calibre10">and how we can make different changes to it. We have even tested it to solve harder problems of removing noise from input images. This kind of flexibility opens the door to many more applications that auoencoders will be a great fit for.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Image colorization</h1>
                </header>
            
            <article>
                
<p class="calibre2">Autoencoders<span class="calibre10">—</span>especially the convolutional version—can be used for harder tasks such as image colorization. In the following example, we feed the model with an input image without any colors, and the reconstructed version of this image will be colorized by the autoencoder model:</p>
<div class="CDPAlignCenter"><img src="assets/4d095b47-f875-4eed-89ec-378f9c56067d.jpeg" class="calibre165"/></div>
<div class="CDPAlignCenter1">Figure 13: The CAE is trained to colorize the image</div>
<div class="CDPAlignCenter"><img src="assets/8e56cfc3-2c87-4af8-b844-081f5549e6ff.jpg" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 14: Colorization paper architecture</div>
<p class="calibre2">Now that our autoencoder is trained, we can use it to colorize pictures we have never seen before!</p>
<p class="calibre2">This kind of application can be used to color very old images that were taken in the early days of the camera.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">More applications</h1>
                </header>
            
            <article>
                
<p class="calibre2">Another interesting application can be producing images with higher resolution, or neural image enhancement, like the following <span class="calibre10">figures </span>show.</p>
<p class="calibre2">These figures show more realistic versions of image colorization by Richard Zhang:</p>
<div class="CDPAlignCenter"><img src="assets/737b6da4-0bd7-437c-8a6b-e79adbe19b63.png" class="calibre166"/></div>
<div class="CDPAlignCenter1">Figure 15: Colorful image colorization by Richard Zhang, Phillip Isola, and Alexei A. Efros</div>
<p class="calibre2">This figure shows another application of autoencoders to make image enhancements:</p>
<div class="CDPAlignCenter"><img src="assets/1eae3e42-4da8-4cea-b961-30bbe5229382.png" class="calibre31"/></div>
<div class="CDPAlignCenter1">Figure 16: Neural enhancement by Alexjc (https://github.com/alexjc/neural-enhance)</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p class="calibre2">In this chapter, we introduced a totally new architecture that can be used for many interesting applications. Autoencoders are very flexible, so feel free to come up with your own problem in the area of image enhancement, colorization, or construction. Also, there are more variations of autoencoders, called <strong class="calibre13">variational autoencoders</strong>. They are also used for very interesting applications, such as image generation.</p>


            </article>

            
        </section>
    </body></html>