<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer533">
<h1 class="chapterNumber">12</h1>
<h1 class="chapterTitle" id="_idParaDest-337">Probabilistic TensorFlow</h1>
<p class="normal">Uncertainty is a fact of life; whether you are doing a classification task or a regression task, it is important to know how confident your model is in its prediction. Till now, we have covered the traditional deep learning models, and while they are great at many tasks, they are not able to handle uncertainty. Instead, they are deterministic in nature. In this chapter, you will learn how to leverage TensorFlow Probability to build models that can handle uncertainty, specifically probabilistic deep learning models and Bayesian networks. The chapter will include:</p>
<ul>
<li class="bulletList">TensorFlow Probability</li>
<li class="bulletList">Distributions, events, and shapes in TensorFlow Probability </li>
<li class="bulletList">Bayesian networks using TensorFlow Probability</li>
<li class="bulletList">Understand uncertainty in machine learning models</li>
<li class="bulletList">Model aleatory and epistemic uncertainty using TensorFlow Probability</li>
</ul>
<div class="note">
<p class="normal">All the code files for this chapter can be found at <a href="https://packt.link/dltfchp12"><span class="url">https://packt.link/dltfchp12</span></a></p>
</div>
<p class="normal">Let’s start with first understanding TensorFlow Probability.</p>
<h1 class="heading-1" id="_idParaDest-338">TensorFlow Probability</h1>
<p class="normal"><strong class="keyWord">TensorFlow Probability</strong> (<strong class="keyWord">TFP</strong>), a part <a id="_idIndexMarker1262"/>of the TensorFlow ecosystem, is a library that provides tools for developing probabilistic models. It can be used to perform probabilistic reasoning and statistical analysis. It is built over TensorFlow and provides the same computational advantage. </p>
<p class="normal"><em class="italic">Figure 12.1</em> shows the major components constituting TensorFlow Probability:</p>
<figure class="mediaobject"><img alt="Graphical user interface, application  Description automatically generated" height="460" src="../Images/B18331_12_01.png" width="666"/></figure>
<p class="packt_figref">Figure 12.1: Different components of TensorFlow Probability </p>
<p class="normal">At the root, we have <a id="_idIndexMarker1263"/>all numerical operations supported by TensorFlow, specifically the <code class="inlineCode">LinearOperator</code> class (part of <code class="inlineCode">tf.linalg</code>) – it contains all the methods that can be performed on a matrix, without the need to actually materialize the matrix. This provides computationally efficient matrix-free computations. TFP includes a large collection of probability distributions and their related statistical computations. It also has <code class="inlineCode">tfp.bijectors</code>, which offers a wide range of transformed distributions.</p>
<div class="note">
<p class="normal">Bijectors encapsulate the change of variables for probability density. That is, when one transforms one variable from space A to space B, we need a way to map the probability distributions of the variables as well. Bijectors provide us with all the tools needed to do so.</p>
</div>
<p class="normal">TensorFlow Probability also provides <code class="inlineCode">JointDistribution</code>, which allows the user to draw a joint sample and compute a joint log-density (log probability density function). The standard TFP distributions work on tensors, but <code class="inlineCode">JointDistribution</code> works on the structure of tensors. <code class="inlineCode">tfp.layers</code> provides neural network layers that can be used to extend the standard TensorFlow layers and add uncertainty to them. And finally, it provides a wide <a id="_idIndexMarker1264"/>range of tools for probabilistic inference. In this chapter, we will go through some of these functions and classes; let us first start with installation. To install TFP in your working environment, just run:</p>
<pre class="programlisting con"><code class="hljs-con">pip install tensorflow-probability
</code></pre>
<p class="normal">Let us have some fun with TFP. To use TFP, we will need to import it. Additionally, we are going to do some plots. So, we import some additional modules:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> tensorflow_probability <span class="hljs-keyword">as</span> tfp
<span class="hljs-keyword">import</span> functools, inspect, sys
</code></pre>
<p class="normal">Next, we explore the different classes of distributions available in <code class="inlineCode">tfp.distributions</code>:</p>
<pre class="programlisting code"><code class="hljs-code">tfd = tfp.distributions
distribution_class =  tfp.distributions.Distribution
distributions = [name <span class="hljs-keyword">for</span> name, obj <span class="hljs-keyword">in</span> inspect.getmembers(tfd)
                <span class="hljs-keyword">if</span> inspect.isclass(obj) <span class="hljs-keyword">and</span> <span class="hljs-built_in">issubclass</span>(obj, distribution_class)]
print(distributions)
</code></pre>
<p class="normal">Here is the output:</p>
<pre class="programlisting con"><code class="hljs-con">['Autoregressive', 'BatchBroadcast', 'BatchConcat', 'BatchReshape', 'Bates', 'Bernoulli', 'Beta', 'BetaBinomial', 'BetaQuotient', 'Binomial', 'Blockwise', 'Categorical', 'Cauchy', 'Chi', 'Chi2', 'CholeskyLKJ', 'ContinuousBernoulli', 'DeterminantalPointProcess', 'Deterministic', 'Dirichlet', 'DirichletMultinomial', 'Distribution', 'DoublesidedMaxwell', 'Empirical', 'ExpGamma', 'ExpInverseGamma', 'ExpRelaxedOneHotCategorical', 'Exponential', 'ExponentiallyModifiedGaussian', 'FiniteDiscrete', 'Gamma', 'GammaGamma', 'GaussianProcess', 'GaussianProcessRegressionModel', 'GeneralizedExtremeValue', 'GeneralizedNormal', 'GeneralizedPareto', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'HalfStudentT', 'HiddenMarkovModel', 'Horseshoe', 'Independent', 'InverseGamma', 'InverseGaussian', 'JohnsonSU', 'JointDistribution', 'JointDistributionCoroutine', 'JointDistributionCoroutineAutoBatched', 'JointDistributionNamed', 'JointDistributionNamedAutoBatched', 'JointDistributionSequential', 'JointDistributionSequentialAutoBatched', 'Kumaraswamy', 'LKJ', 'LambertWDistribution', 'LambertWNormal', 'Laplace', 'LinearGaussianStateSpaceModel', 'LogLogistic', 'LogNormal', 'Logistic', 'LogitNormal', 'MarkovChain', 'Masked', 'MatrixNormalLinearOperator', 'MatrixTLinearOperator', 'Mixture', 'MixtureSameFamily', 'Moyal', 'Multinomial', 'MultivariateNormalDiag', 'MultivariateNormalDiagPlusLowRank', 'MultivariateNormalDiagPlusLowRankCovariance', 'MultivariateNormalFullCovariance', 'MultivariateNormalLinearOperator', 'MultivariateNormalTriL', 'MultivariateStudentTLinearOperator', 'NegativeBinomial', 'Normal', 'NormalInverseGaussian', 'OneHotCategorical', 'OrderedLogistic', 'PERT', 'Pareto', 'PixelCNN', 'PlackettLuce', 'Poisson', 'PoissonLogNormalQuadratureCompound', 'PowerSpherical', 'ProbitBernoulli', 'QuantizedDistribution', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'Sample', 'SigmoidBeta', 'SinhArcsinh', 'Skellam', 'SphericalUniform', 'StoppingRatioLogistic', 'StudentT', 'StudentTProcess', 'StudentTProcessRegressionModel', 'TransformedDistribution', 'Triangular', 'TruncatedCauchy', 'TruncatedNormal', 'Uniform', 'VariationalGaussianProcess', 'VectorDeterministic', 'VonMises', 'VonMisesFisher', 'Weibull', 'WishartLinearOperator', 'WishartTriL', 'Zipf']
</code></pre>
<p class="normal">You can see <a id="_idIndexMarker1265"/>that a rich range of distributions is available in TFP. Let us now try one of the distributions:</p>
<pre class="programlisting code"><code class="hljs-code">normal = tfd.Normal(loc=<span class="hljs-number">0.</span>, scale=<span class="hljs-number">1.</span>)
</code></pre>
<p class="normal">This statement declares that we want to have a normal distribution with <strong class="keyWord">mean</strong> (<code class="inlineCode">loc</code>) zero and <strong class="keyWord">standard deviation</strong> (<code class="inlineCode">scale</code>) 1. We can generate random samples following this distribution using the sample method. The following code snippet generates such <code class="inlineCode">N</code> samples and plots them:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">plot_normal</span><span class="hljs-function">(</span><span class="hljs-params">N</span><span class="hljs-function">):</span>
  samples = normal.sample(N)
  sns.distplot(samples)
  plt.title(<span class="hljs-string">f"Normal Distribution with zero mean, and 1 std. dev </span><span class="hljs-subst">{N}</span><span class="hljs-string"> samples"</span>)
  plt.show()
</code></pre>
<p class="normal">You can <a id="_idIndexMarker1266"/>see that as <code class="inlineCode">N</code> increases, the plot follows a nice normal distribution:</p>
<table class="table-container" id="table001-6">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">N=100</strong></p>
</td>
<td class="table-cell">
<p class="normal"><img alt="Chart, histogram  Description automatically generated" height="335" src="../Images/B18331_12_02_1.png" width="531"/></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">N=1000</strong></p>
</td>
<td class="table-cell">
<p class="normal"><img alt="Chart, histogram  Description automatically generated" height="336" src="../Images/B18331_12_02_2.png" width="531"/></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">N=10000</strong></p>
</td>
<td class="table-cell">
<p class="normal"><img alt="A picture containing histogram  Description automatically generated" height="329" src="../Images/B18331_12_02_3.png" width="531"/></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Figure 12.2: Normal distribution from randomly generated samples of sizes 100, 1,000, and 10,000. The distribution has a mean of zero and a standard deviation of one</p>
<p class="normal">Let us now <a id="_idIndexMarker1267"/>explore the different distributions available with TFP.</p>
<h1 class="heading-1" id="_idParaDest-339">TensorFlow Probability distributions</h1>
<p class="normal">Every <a id="_idIndexMarker1268"/>distribution in TFP has a shape, batch, and event size associated with it. The shape is the sample size; it represents independent and identically distributed draws or observations. Consider the normal distribution that we defined in the previous section:</p>
<pre class="programlisting code"><code class="hljs-code">normal = tfd.Normal(loc=<span class="hljs-number">0.</span>, scale=<span class="hljs-number">1.</span>)
</code></pre>
<p class="normal">This defines a single normal distribution, with mean zero and standard deviation one. When we use the <code class="inlineCode">sample</code> function, we do a random draw from this distribution.</p>
<p class="normal">Notice the details regarding <code class="inlineCode">batch_shape</code> and <code class="inlineCode">event_shape</code> if you print the object <code class="inlineCode">normal</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-built_in">print</span>(normal)
</code></pre>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> tfp.distributions.Normal(<span class="hljs-con-string">"Normal"</span>, batch_shape=[], event_shape=[], dtype=float32)
</code></pre>
<p class="normal">Let us try and define a second <code class="inlineCode">normal</code> object, but this time, <code class="inlineCode">loc</code> and <code class="inlineCode">scale</code> are lists:</p>
<pre class="programlisting code"><code class="hljs-code">normal_2 = tfd.Normal(loc=[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>], scale=[<span class="hljs-number">1.</span>, <span class="hljs-number">3.</span>])
<span class="hljs-built_in">print</span>(normal_2)
</code></pre>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> tfp.distributions.Normal(<span class="hljs-con-string">"Normal"</span>, batch_shape=[<span class="hljs-con-number">2</span>], event_shape=[], dtype=float32)
</code></pre>
<p class="normal">Did you notice the change in <code class="inlineCode">batch_shape</code>? Now, if we draw a single sample from it, we will draw from two normal distributions, one with a mean of zero and standard deviation of one, and the other with a mean of zero and standard deviation of three. Thus, the <a id="_idIndexMarker1269"/>batch shape determines the number of observations from the same distribution family. The two normal distributions are independent; thus, it is a batch of distributions of the same family.</p>
<div class="note">
<p class="normal">You can have batches of the same type of distribution family, like in the preceding example of having two normal distributions. You cannot create a batch of, say, a normal and a Gaussian distribution.</p>
</div>
<p class="normal">What if we need a single normal distribution that is dependent on two variables, each with a different mean? This is made possible using <code class="inlineCode">MultivariateNormalDiag</code>, and this influences the event shape – it is the atomic shape of a single draw or observation from this distribution:</p>
<pre class="programlisting code"><code class="hljs-code">normal_3 = tfd.MultivariateNormalDiag(loc = [[<span class="hljs-number">1.0</span>, <span class="hljs-number">0.3</span>]])
<span class="hljs-built_in">print</span>(normal_3)
</code></pre>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> tfp.distributions.MultivariateNormalDiag(<span class="hljs-con-string">"MultivariateNormalDiag"</span>, batch_shape=[<span class="hljs-con-number">1</span>], event_shape=[<span class="hljs-con-number">2</span>], dtype=float32)
</code></pre>
<p class="normal">We can see that in the above output the <code class="inlineCode">event_shape</code> has changed.</p>
<h2 class="heading-2" id="_idParaDest-340">Using TFP distributions</h2>
<p class="normal">Once you <a id="_idIndexMarker1270"/>have defined a distribution, you can do a lot more. TFP provides a good range of functions to perform various operations. We have already used the <code class="inlineCode">Normal</code> distribution and <code class="inlineCode">sample</code> method. The section above also demonstrated how we can use TFP for creating univariate, multivariate, or independent distribution/s. TFP provides many important methods to interact <a id="_idIndexMarker1271"/>with the created distributions. Some of the important ones include:</p>
<ul>
<li class="bulletList"><code class="inlineCode">sample(n)</code>: It samples <code class="inlineCode">n</code> observations from the distribution.</li>
<li class="bulletList"><code class="inlineCode">prob(value)</code>: It provides probability (discrete) or probability density (continuous) for the value.</li>
<li class="bulletList"><code class="inlineCode">log_prob(values)</code>: Provides log probability or log-likelihood for the values.</li>
<li class="bulletList"><code class="inlineCode">mean()</code>: It gives the mean of the distribution.</li>
<li class="bulletList"><code class="inlineCode">stddev()</code>: It provides the standard deviation of the distribution.</li>
</ul>
<h3 class="heading-3" id="_idParaDest-341">Coin Flip Example</h3>
<p class="normal">Let us now use some of the features of TFP to describe data by looking at an example: the standard <a id="_idIndexMarker1272"/>coin-flipping example we are familiar with from our school days. We know that if we flip a coin, there <a id="_idIndexMarker1273"/>are only two possibilities – we can have either a head or a tail. Such a distribution, where we have only two discrete values, is <a id="_idIndexMarker1274"/>called a <strong class="keyWord">Bernoulli</strong> distribution. So let us consider different scenarios:</p>
<h4 class="heading-4">Scenario 1</h4>
<p class="normal">A fair <a id="_idIndexMarker1275"/>coin with a <code class="inlineCode">0.5</code> probability of heads and <code class="inlineCode">0.5</code> probability of tails.</p>
<p class="normal">Let us create the distribution:</p>
<pre class="programlisting code"><code class="hljs-code">coin_flip = tfd.Bernoulli(probs=<span class="hljs-number">0.5</span>, dtype=tf.int32)
</code></pre>
<p class="normal">Now get some samples:</p>
<pre class="programlisting code"><code class="hljs-code">coin_flip_data = coin_flip.sample(<span class="hljs-number">2000</span>)
</code></pre>
<p class="normal">Let us visualize the samples:</p>
<pre class="programlisting code"><code class="hljs-code">plt.hist(coin_flip_data)
</code></pre>
<figure class="mediaobject"><img alt="Shape  Description automatically generated" height="390" src="../Images/B18331_12_03.png" width="600"/></figure>
<p class="packt_figref">Figure 12.3: Distribution of heads and tails from 2,000 observations</p>
<p class="normal">You can <a id="_idIndexMarker1276"/>see that we have both heads and tails in equal numbers; after all, it is a fair coin. The probability of heads and tails as <code class="inlineCode">0.5</code>:</p>
<pre class="programlisting code"><code class="hljs-code">coin_flip.prob(<span class="hljs-number">0</span>) <span class="hljs-comment">## Probability of tail</span>
</code></pre>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="hljs-con-number">0.5</span>&gt;
</code></pre>
<h4 class="heading-4">Scenario 2</h4>
<p class="normal">A biased coin with a 0.8 probability of heads and 0.2 probability of tails.</p>
<p class="normal">Now, since the coin is biased, with the probability of heads being <code class="inlineCode">0.8</code>, the distribution would be created using:</p>
<pre class="programlisting code"><code class="hljs-code">bias_coin_flip = tfd.Bernoulli(probs=<span class="hljs-number">0.8</span>, dtype=tf.int32)
</code></pre>
<p class="normal">Now get some samples:</p>
<pre class="programlisting code"><code class="hljs-code">bias_coin_flip_data = bias_coin_flip.sample(<span class="hljs-number">2000</span>)
</code></pre>
<p class="normal">Let us visualize the samples:</p>
<pre class="programlisting code"><code class="hljs-code">plt.hist(bias_coin_flip_data)
</code></pre>
<figure class="mediaobject"><img alt="Shape  Description automatically generated with medium confidence" height="412" src="../Images/B18331_12_04.png" width="633"/></figure>
<p class="packt_figref">Figure 12.4: Distribution of heads and tails from 2,000 coin flips of a biased coin</p>
<p class="normal">We can <a id="_idIndexMarker1277"/>see that now heads are much larger in number than tails. Thus, the probability of tails is no longer <code class="inlineCode">0.5</code>:</p>
<pre class="programlisting code"><code class="hljs-code">bias_coin_flip.prob(<span class="hljs-number">0</span>) <span class="hljs-comment">## Probability of tail</span>
</code></pre>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">&gt;&gt;&gt;</span> &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="hljs-con-number">0.19999999</span>&gt;
</code></pre>
<p class="normal">You will probably get a number close to <code class="inlineCode">0.2</code>.</p>
<h4 class="heading-4">Scenario 3 </h4>
<p class="normal">Two coins with one biased toward heads with a <code class="inlineCode">0.8</code> probability, and the other biased toward heads with a <code class="inlineCode">0.6</code> probability.</p>
<p class="normal">Now, we have two independent coins. Since the coins are biased, with the probabilities of heads being <code class="inlineCode">0.8</code> and <code class="inlineCode">0.6</code> respectively, we create a distribution using:</p>
<pre class="programlisting code"><code class="hljs-code">two_bias_coins_flip = tfd.Bernoulli(probs=[<span class="hljs-number">0.8</span>, <span class="hljs-number">0.6</span>], dtype=tf.int32)
</code></pre>
<p class="normal">Now get some samples:</p>
<pre class="programlisting code"><code class="hljs-code">two_bias_coins_flip_data = two_bias_coins_flip.sample(<span class="hljs-number">2000</span>)
</code></pre>
<p class="normal">Let us <a id="_idIndexMarker1278"/>visualize the samples:</p>
<pre class="programlisting code"><code class="hljs-code">plt.hist(two_bias_coins_flip_data[:,<span class="hljs-number">0</span>], alpha=<span class="hljs-number">0.8</span>, label=<span class="hljs-string">'Coin 1'</span>)
plt.hist(two_bias_coins_flip_data[:,<span class="hljs-number">1</span>], alpha=<span class="hljs-number">0.5</span>, label=<span class="hljs-string">'Coin 2'</span>)
plt.legend(loc=<span class="hljs-string">'center'</span>)
</code></pre>
<figure class="mediaobject"><img alt="Graphical user interface  Description automatically generated" height="421" src="../Images/B18331_12_05.png" width="648"/></figure>
<p class="packt_figref">Figure 12.5: Distribution of heads and tails from 2,000 flips for two independent coins</p>
<p class="normal">The bar in blue corresponds to Coin 1, and the bar in orange corresponds to Coin 2. The brown part of the graphs is the area where the results of the two coins overlap. You can see that for Coin 1, the number of heads is much larger as compared to Coin 2, as expected.</p>
<h3 class="heading-3" id="_idParaDest-342">Normal distribution</h3>
<p class="normal">We can <a id="_idIndexMarker1279"/>use the Bernoulli distribution where the data can have only two possible discrete values: heads and tails, good and bad, spam and ham, and so on. However, a large amount of data in our daily lives <a id="_idIndexMarker1280"/>is continuous in range, with the normal distribution being very common. So let us also explore different normal distributions.</p>
<p class="normal">Mathematically, the probability density function of a normal distribution can be expressed as:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_12_001.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="642"/></p>
<p class="normal">where <img alt="" height="46" src="../Images/B18331_08_023.png" style="height: 1.15em !important; vertical-align: -0.18em !important;" width="25"/> is the mean of the distribution, and <img alt="" height="42" src="../Images/B18331_07_010.png" style="height: 1.05em !important; vertical-align: -0.08em !important;" width="25"/> is the standard deviation.</p>
<p class="normal">In TFP, the parameter <code class="inlineCode">loc</code> represents the mean and the parameter <code class="inlineCode">scale</code> represents the standard deviation. Now, to illustrate the use of how we can use distribution, let us consider that we want to represent the weather data of a location for a particular season, say summer in Delhi, India.</p>
<h4 class="heading-4">Univariate normal</h4>
<p class="normal">We can think that weather depends only on temperature. So, by having a sample of temperature <a id="_idIndexMarker1281"/>in the summer months over many years, we can <a id="_idIndexMarker1282"/>get a good representation of data. That is, we can have a univariate normal distribution. </p>
<p class="normal">Now, based on weather data, the average high temperature in the month of June in Delhi is 35 degrees Celsius, with a standard deviation of 4 degrees Celsius. So, we can create a normal distribution using:</p>
<pre class="programlisting code"><code class="hljs-code">temperature = tfd.Normal(loc=<span class="hljs-number">35</span>, scale = <span class="hljs-number">4</span>)
</code></pre>
<p class="normal">Get some observation samples from it:</p>
<pre class="programlisting code"><code class="hljs-code">temperature_data = temperature.sample(<span class="hljs-number">1000</span>)
</code></pre>
<p class="normal">And let us now visualize it:</p>
<pre class="programlisting code"><code class="hljs-code">sns.displot(temperature_data, kde= <span class="hljs-literal">True</span>)
</code></pre>
<figure class="mediaobject"><img alt="Chart, histogram  Description automatically generated" height="477" src="../Images/B18331_12_06.png" width="477"/></figure>
<p class="packt_figref">Figure 12.6: Probability density function for the temperature of Delhi in the month of June</p>
<p class="normal">It would be <a id="_idIndexMarker1283"/>good to verify if the mean and standard <a id="_idIndexMarker1284"/>deviation of our sample data is close to the values we described.</p>
<p class="normal">Using the distribution, we can find the mean and standard deviation using:</p>
<pre class="programlisting code"><code class="hljs-code">temperature.mean()
</code></pre>
<pre class="programlisting con"><code class="hljs-con"># output
<span class="hljs-con-meta">&gt;&gt;&gt;</span> &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="hljs-con-number">35.0</span>&gt;
</code></pre>
<pre class="programlisting code"><code class="hljs-code">temperature.stddev()
</code></pre>
<pre class="programlisting con"><code class="hljs-con"># output
<span class="hljs-con-meta">&gt;&gt;&gt;</span> &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="hljs-con-number">4.0</span>&gt;
</code></pre>
<p class="normal">And from the sampled data, we can verify using:</p>
<pre class="programlisting code"><code class="hljs-code">tf.math.reduce_mean(temperature_data) 
</code></pre>
<pre class="programlisting con"><code class="hljs-con"># output
<span class="hljs-con-meta">&gt;&gt;&gt;</span> &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="hljs-con-number">35.00873</span>&gt;
</code></pre>
<pre class="programlisting code"><code class="hljs-code">tf.math.reduce_std(temperature_data)
</code></pre>
<pre class="programlisting con"><code class="hljs-con"># output
<span class="hljs-con-meta">&gt;&gt;&gt;</span> &lt;tf.Tensor: shape=(), dtype=float32, numpy=<span class="hljs-con-number">3.9290223</span>&gt;
</code></pre>
<p class="normal">Thus, the <a id="_idIndexMarker1285"/>sampled data is following the same <a id="_idIndexMarker1286"/>mean and standard deviation.</p>
<h4 class="heading-4">Multivariate distribution</h4>
<p class="normal">All is good so far. I show my distribution to a friend working in meteorology, and he says that <a id="_idIndexMarker1287"/>using only temperature is not <a id="_idIndexMarker1288"/>sufficient; the humidity is also important. So now, each weather point depends on two parameters – the temperature of the day and the humidity of the day. This type of data distribution can be obtained using the <code class="inlineCode">MultivariateNormalDiag</code> distribution class, as defined in TFP:</p>
<pre class="programlisting code"><code class="hljs-code">weather = tfd.MultivariateNormalDiag(loc = [<span class="hljs-number">35</span>, <span class="hljs-number">56</span>], scale_diag=[<span class="hljs-number">4</span>, <span class="hljs-number">15</span>])
weather_data = weather.sample(<span class="hljs-number">1000</span>)
plt.scatter(weather_data[:, <span class="hljs-number">0</span>], weather_data[:, <span class="hljs-number">1</span>], color=<span class="hljs-string">'blue'</span>, alpha=<span class="hljs-number">0.4</span>)
plt.xlabel(<span class="hljs-string">"Temperature Degree Celsius"</span>)
plt.ylabel(<span class="hljs-string">"Humidity %"</span>)
</code></pre>
<p class="normal"><em class="italic">Figure 12.7</em>, shows the multivariate normal distribution of two variables, temperature and humidity, generated using TFP:</p>
<figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" height="402" src="../Images/B18331_12_07.png" width="597"/></figure>
<p class="packt_figref">Figure 12.7: Multivariate normal distribution with the x-axis representing temperature and the y-axis humidity</p>
<p class="normal">Using <a id="_idIndexMarker1289"/>the different distributions <a id="_idIndexMarker1290"/>and bijectors available in TFP, we can generate synthetic data that follows the same joint distribution as real data to train the model.</p>
<h2 class="heading-2" id="_idParaDest-343">Bayesian networks</h2>
<p class="normal"><strong class="keyWord">Bayesian Networks</strong> (<strong class="keyWord">BNs</strong>) make <a id="_idIndexMarker1291"/>use of the concepts from graph theory, probability, and statistics to encapsulate complex causal relationships. Here, we <a id="_idIndexMarker1292"/>build a <strong class="keyWord">Directed Acyclic Graph</strong> (<strong class="keyWord">DAG</strong>), where nodes, called factors (random variables), are connected by the arrows representing cause-effect relationships. Each node represents a variable with an associated <a id="_idIndexMarker1293"/>probability (also called a <strong class="keyWord">Conditional Probability Table</strong> (<strong class="keyWord">CPT</strong>)). The links tell us about the dependence of one node over another. Though they were first proposed by Pearl in 1988, they have regained attention in recent years. The main cause of this renowned interest in BNs is that standard deep learning models are not able to represent the cause-effect relationship.</p>
<p class="normal">Their strength lies in the fact that they can be used to model uncertainties combined with expert knowledge and data. They have been employed in diverse fields for their power to do probabilistic and causal reasoning. At the heart of the Bayesian network is Bayes’ rule:</p>
<p class="center"><img alt="" height="100" src="../Images/B18331_12_004.png" style="height: 2.50em !important; vertical-align: 0.05em !important;" width="383"/></p>
<p class="normal">Bayes’ rule is used to determine the joint probability of an event given certain conditions. The simplest way to understand the BN is that the BN can determine the causal relationship between <a id="_idIndexMarker1294"/>the hypothesis and evidence. There is some unknown hypothesis H, about which we want to assess the uncertainty and make some decisions. We start with some prior belief about hypothesis H, and then based on evidence E, we update our belief about H.</p>
<p class="normal">Let us try to understand it by example. We consider a very standard example: a garden with grass and a sprinkler. Now, using common sense, we know that if the sprinkler is on, the grass is wet. Let us now reverse the logic: what if you come back home and find that the grass is wet, what is the probability that the sprinkler is on, and what is the probability that it actually rained? Interesting, right? Let us add further evidence – you find that the sky is cloudy. Now, what do you think is the reason for the grass being wet?</p>
<p class="normal">This sort of reasoning based on evidence is encompassed by BNs in the form of DAGs, also <a id="_idIndexMarker1295"/>called causal graphs – because they provide an insight into the cause-effect relationship.</p>
<p class="normal">To model the problem, we make use of the <code class="inlineCode">JointDistributionCoroutine</code> distribution class. This distribution allows both the sampling of data and computation of the joint probability from a single model specification. Let us make some assumptions to build the model:</p>
<ul>
<li class="bulletList">The probability that it is cloudy is <code class="inlineCode">0.2</code></li>
<li class="bulletList">The probability that it is cloudy and it rains is <code class="inlineCode">0.8</code>, and the probability that it is not cloudy but it rains is <code class="inlineCode">0.1</code></li>
<li class="bulletList">The probability that it is cloudy and the sprinkler is on is <code class="inlineCode">0.1</code>, and the probability that it is not cloudy and the sprinkler is on is <code class="inlineCode">0.5</code></li>
<li class="bulletList">Now, for the grass, we have four possibilities:</li>
</ul>
<table class="table-container" id="table002-2">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Sprinkler</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Rain</strong></p>
</td>
<td class="table-cell">
<p class="normal"><strong class="keyWord">Grass Wet</strong></p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">F</p>
</td>
<td class="table-cell">
<p class="normal">F</p>
</td>
<td class="table-cell">
<p class="normal">0</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">F</p>
</td>
<td class="table-cell">
<p class="normal">T</p>
</td>
<td class="table-cell">
<p class="normal">0.8</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">T</p>
</td>
<td class="table-cell">
<p class="normal">F</p>
</td>
<td class="table-cell">
<p class="normal">0.9</p>
</td>
</tr>
<tr>
<td class="table-cell">
<p class="normal">T</p>
</td>
<td class="table-cell">
<p class="normal">T</p>
</td>
<td class="table-cell">
<p class="normal">0.99</p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Table 12.1: The conditional probability table for the Sprinkler-Rain-Grass scenario</p>
<p class="normal"><em class="italic">Figure 12.8</em> shows the <a id="_idIndexMarker1296"/>corresponding BN DAG:</p>
<figure class="mediaobject"><img alt="Diagram  Description automatically generated" height="429" src="../Images/B18331_12_08.png" width="357"/></figure>
<p class="packt_figref">Figure 12.8: Bayesian Network for our toy problem</p>
<p class="normal">This information can be represented by the following model:</p>
<pre class="programlisting code"><code class="hljs-code">Root = tfd.JointDistributionCoroutine.Root
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">model</span><span class="hljs-function">():</span>
<span class="hljs-comment">  # generate the distribution for cloudy weather</span>
  cloudy = <span class="hljs-keyword">yield</span> Root(tfd.Bernoulli(probs=<span class="hljs-number">0.2</span>, dtype=tf.int32))
  <span class="hljs-comment"># define sprinkler probability table</span>
  sprinkler_prob = [<span class="hljs-number">0.5</span>, <span class="hljs-number">0.1</span>]
  sprinkler_prob = tf.gather(sprinkler_prob, cloudy)
  sprinkler = <span class="hljs-keyword">yield</span> tfd.Bernoulli(probs=sprinkler_prob, dtype=tf.int32)
  <span class="hljs-comment"># define rain probability table</span>
  raining_prob = [<span class="hljs-number">0.1</span>, <span class="hljs-number">0.8</span>]
  raining_prob = tf.gather(raining_prob, cloudy)
  raining = <span class="hljs-keyword">yield</span> tfd.Bernoulli(probs=raining_prob, dtype=tf.int32)
  #Conditional Probability table for wet grass
  grass_wet_prob = [[<span class="hljs-number">0.0</span>, <span class="hljs-number">0.8</span>],
                    [<span class="hljs-number">0.9</span>, <span class="hljs-number">0.99</span>]]
  grass_wet_prob = tf.gather_nd(grass_wet_prob, _stack(sprinkler, raining))
  grass_wet = <span class="hljs-keyword">yield</span> tfd.Bernoulli(probs=grass_wet_prob, dtype=tf.int32)
</code></pre>
<p class="normal">The above model <a id="_idIndexMarker1297"/>will function like a data generator. The <code class="inlineCode">Root</code> function is used to tell the node in the graph without any parent. We define a few utility functions, <code class="inlineCode">broadcast</code> and <code class="inlineCode">stack</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">_conform</span><span class="hljs-function">(</span><span class="hljs-params">ts</span><span class="hljs-function">):</span>
  <span class="hljs-string">"""Broadcast all arguments to a common shape."""</span>
  shape = functools.reduce(
      tf.broadcast_static_shape, [a.shape <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> ts])
  <span class="hljs-keyword">return</span> [tf.broadcast_to(a, shape) <span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> ts]
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">_stack</span><span class="hljs-function">(</span><span class="hljs-params">*ts</span><span class="hljs-function">):</span>
  <span class="hljs-keyword">return</span> tf.stack(_conform(ts), axis=-<span class="hljs-number">1</span>)
</code></pre>
<p class="normal">To do inferences, we make use of the <code class="inlineCode">MarginalizableJointDistributionCoroutine</code> class, as this allows us to compute marginalized probabilities:</p>
<pre class="programlisting code"><code class="hljs-code">d = marginalize.MarginalizableJointDistributionCoroutine(model)
</code></pre>
<p class="normal">Now, based on our observations, we can obtain the probability of other factors.</p>
<h4 class="heading-4">Case 1:</h4>
<p class="normal">We observe <a id="_idIndexMarker1298"/>that the grass is wet (the observation corresponding to this is 1 – if the grass was dry, we would set it to 0), we have no idea about the state of the clouds or the state of the sprinkler (the observation corresponding to an unknown state is set to “marginalize”), and we want to know the probability of rain (the observation corresponding to the probability we want to find is set to “tabulate”). Converting this into observations:</p>
<pre class="programlisting code"><code class="hljs-code">observations = [<span class="hljs-string">'marginalize'</span>, <span class="hljs-comment"># We don't know the cloudy state</span>
                <span class="hljs-string">'tabulate'</span>, <span class="hljs-comment"># We want to know the probability of rain</span>
                <span class="hljs-string">'marginalize'</span>, <span class="hljs-comment"># We don't know the sprinkler state.</span>
                <span class="hljs-number">1</span>]             <span class="hljs-comment"># We observed a wet lawn.</span>
</code></pre>
<p class="normal">Now we get the probability of rain using:</p>
<pre class="programlisting code"><code class="hljs-code">p = tf.exp(d.marginalized_log_prob(observations))
p = p / tf.reduce_sum(p)
</code></pre>
<p class="normal">The result is <code class="inlineCode">array([0.27761015, 0.72238994], dtype=float32)</code>, that is, there is a 0.722 probability that it rained.</p>
<h4 class="heading-4">Case 2:</h4>
<p class="normal">We observe <a id="_idIndexMarker1299"/>that the grass is wet, we have no idea about the state of the clouds or rain, and we want to know the probability of whether the sprinkler is on. Converting this into observations:</p>
<pre class="programlisting code"><code class="hljs-code">observations = [<span class="hljs-string">'marginalize'</span>,  
                <span class="hljs-string">'marginalize'</span>, 
                <span class="hljs-string">'tabulate'</span>,  
                <span class="hljs-number">1</span>]
</code></pre>
<p class="normal">This results in probabilities <code class="inlineCode">array([0.61783344, 0.38216656], dtype=float32)</code>, that is, there is a <code class="inlineCode">0.382</code> probability that the sprinkler is on.</p>
<h4 class="heading-4">Case 3:</h4>
<p class="normal">What if we observe that there is no rain, and the sprinkler is off? What do you think is the state of the grass? Logic says the grass should not be wet. Let us confirm this from the model by sending it the observations:</p>
<pre class="programlisting code"><code class="hljs-code">observations = [<span class="hljs-string">'marginalize'</span>,  
                 <span class="hljs-number">0</span>,
                 <span class="hljs-number">0</span>, 
                <span class="hljs-string">'tabulate'</span>]
</code></pre>
<p class="normal">This results in the probabilities <code class="inlineCode">array([1., 0], dtype=float32)</code>, that is, there is a 100% probability that the grass is dry, just the way we expected.</p>
<p class="normal">As you can see, once we know the state of the parents, we do not need to know the state of the parent’s parents – that is, the BN follows the local Markov property. In the example <a id="_idIndexMarker1300"/>that we covered here, we started with the structure, and we had the conditional probabilities available to us. We demonstrate how we can do inference based on the model, and how despite the same model and CPDs, the evidence <a id="_idIndexMarker1301"/>changes the <strong class="keyWord">posterior probabilities</strong>.</p>
<div class="note">
<p class="normal">In Bayesian networks, the structure (the nodes and how they are interconnected) and the parameters (the conditional probabilities of each node) are learned from the data. They are referred to as structured learning and parameter learning respectively. Covering the algorithms for structured learning and parameter learning are beyond the scope of this chapter.</p>
</div>
<h2 class="heading-2" id="_idParaDest-344">Handling uncertainty in predictions using TensorFlow Probability</h2>
<p class="normal">At <a id="_idIndexMarker1302"/>the beginning <a id="_idIndexMarker1303"/>of this chapter, we talked about the uncertainties in prediction by deep learning models and how the existing deep learning architectures are not able to account for those uncertainties. In this chapter, we will use the layers provided by TFP to model uncertainty.</p>
<p class="normal">Before adding the TFP layers, let us first understand the uncertainties a bit. There are two classes of uncertainty.</p>
<h3 class="heading-3" id="_idParaDest-345">Aleatory uncertainty</h3>
<p class="normal">This exists because <a id="_idIndexMarker1304"/>of the random nature of the natural processes. It is <a id="_idIndexMarker1305"/>inherent uncertainty, present due to the probabilistic variability. For example, when tossing a coin, there will always be a certain degree of uncertainty in predicting whether the next toss will be heads or tails. There is no way to remove this uncertainty. In essence, every time you repeat the experiment, the results will have certain variations.</p>
<h3 class="heading-3" id="_idParaDest-346">Epistemic uncertainty</h3>
<p class="normal">This uncertainty <a id="_idIndexMarker1306"/>comes from a lack of knowledge. There can be <a id="_idIndexMarker1307"/>various reasons for this lack of knowledge, for example, an inadequate understanding of the underlying processes, an incomplete knowledge of the phenomena, and so on. This type of uncertainty can be reduced by understanding the reason, for example, to get more data, we conduct more experiments.</p>
<p class="normal">The presence of these uncertainties increases risk. We require a way to quantify these uncertainties and, hence, quantify the risk.</p>
<h3 class="heading-3" id="_idParaDest-347">Creating a synthetic dataset</h3>
<p class="normal">In this <a id="_idIndexMarker1308"/>section, we will learn how to modify the standard deep neural networks to quantify uncertainties. Let us start with creating <a id="_idIndexMarker1309"/>a synthetic dataset. To create the dataset, we consider that output prediction y depends on input x linearly, as given by the following expression:</p>
<p class="center"><img alt="" height="50" src="../Images/B18331_12_005.png" style="height: 1.25em !important;" width="383"/></p>
<p class="normal">Here, <img alt="" height="50" src="../Images/B18331_12_006.png" style="height: 1.25em !important; vertical-align: -0.29em !important;" width="163"/> follows a normal distribution with mean zero and standard deviation 1 around x. The function below will generate this synthetic data for us. Do observe that to generate this data, we made use of the <code class="inlineCode">Uniform</code> distribution and <code class="inlineCode">Normal</code> distributions available as part of TFP distributions:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">create_dataset</span><span class="hljs-function">(</span><span class="hljs-params">n, x_range</span><span class="hljs-function">):</span>
    x_uniform_dist = tfd.Uniform(low=x_range[<span class="hljs-number">0</span>], high=x_range[<span class="hljs-number">1</span>])
    x = x_uniform_dist.sample(n).numpy() [:, np.newaxis] 
    y_true = <span class="hljs-number">2.7</span>*x+<span class="hljs-number">3</span>
    eps_uniform_dist = tfd.Normal(loc=<span class="hljs-number">0</span>, scale=<span class="hljs-number">1</span>)
    eps = eps_uniform_dist.sample(n).numpy() [:, np.newaxis] *<span class="hljs-number">0.74</span>*x
    y = y_true + eps
    <span class="hljs-keyword">return</span> x, y, y_true
</code></pre>
<p class="normal"><code class="inlineCode">y_true</code> is the value without including the normal distributed noise <img alt="" height="42" src="../Images/B18331_10_003.png" style="height: 1.05em !important; vertical-align: -0.07em !important;" width="21"/>.</p>
<p class="normal">Now we use it to create a training dataset and a validation dataset:</p>
<pre class="programlisting code"><code class="hljs-code">x_train, y_train, y_true = create_dataset(<span class="hljs-number">2000</span>, [-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>])
x_val, y_val, _ = create_dataset(<span class="hljs-number">500</span>, [-<span class="hljs-number">10</span>, <span class="hljs-number">10</span>])
</code></pre>
<p class="normal">This will give us 2,000 datapoints for training and 500 datapoints for validation. <em class="italic">Figure 12.9</em> shows the <a id="_idIndexMarker1310"/>plots of the two datasets, with <a id="_idIndexMarker1311"/>ground truth (the value of <em class="italic">y</em> in the absence of any noise) in the background:</p>
<figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" height="437" src="../Images/B18331_12_09.png" width="447"/></figure>
<p class="packt_figref">Figure 12.9: Plot of the synthetic dataset</p>
<h3 class="heading-3" id="_idParaDest-348">Building a regression model using TensorFlow</h3>
<p class="normal">We can <a id="_idIndexMarker1312"/>build a simple Keras model to perform <a id="_idIndexMarker1313"/>the task of regression on the synthetic dataset created in the preceding section:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Model Architecture</span>
model = Sequential([Dense(<span class="hljs-number">1</span>, input_shape=(<span class="hljs-number">1</span>,))])
<span class="hljs-comment"># Compile </span>
model.<span class="hljs-built_in">compile</span>(loss=<span class="hljs-string">'mse'</span>, optimizer=<span class="hljs-string">'adam'</span>)
<span class="hljs-comment"># Fit</span>
model.fit(x_train, y_train, epochs=<span class="hljs-number">100</span>, verbose=<span class="hljs-number">1</span>)
</code></pre>
<p class="normal">Let us see how good the fitted model works on the test dataset:</p>
<figure class="mediaobject"><img alt="Chart, scatter chart  Description automatically generated" height="428" src="../Images/B18331_12_10.png" width="438"/></figure>
<p class="packt_figref">Figure 12.10: Ground truth and fitted regression line</p>
<p class="normal">It was <a id="_idIndexMarker1314"/>a simple problem, and we can see that <a id="_idIndexMarker1315"/>the fitted regression line almost overlaps the ground truth. However, there is no way to tell the uncertainty of predictions.</p>
<h3 class="heading-3" id="_idParaDest-349">Probabilistic neural networks for aleatory uncertainty</h3>
<p class="normal">What if instead of linear regression, we build a model that can fit the distribution? In our synthetic <a id="_idIndexMarker1316"/>dataset, the source <a id="_idIndexMarker1317"/>of aleatory uncertainty is the noise, and we know that our noise follows a normal distribution, which is characterized by two parameters: the mean and standard deviation. So, we can modify our model to predict the mean and standard deviation distributions instead of actual <em class="italic">y</em> values. We can accomplish this using either the <code class="inlineCode">IndependentNormal</code> TFP layer or the <code class="inlineCode">DistributionLambda</code> TFP layer. The following code defines the modified model architecture:</p>
<pre class="programlisting code"><code class="hljs-code">model = Sequential([Dense(<span class="hljs-number">2</span>, input_shape = (<span class="hljs-number">1</span>,)),
    tfp.layers.DistributionLambda(<span class="hljs-keyword">lambda</span> t: tfd.Normal(loc=t[..., :<span class="hljs-number">1</span>], scale=<span class="hljs-number">0.3</span>+tf.math.<span class="hljs-built_in">abs</span>(t[...,<span class="hljs-number">1</span>:])))
])
</code></pre>
<p class="normal">We will <a id="_idIndexMarker1318"/>need to make one more <a id="_idIndexMarker1319"/>change. Earlier, we predicted the <em class="italic">y</em> value; therefore, the mean square error loss was a good choice. Now, we are predicting the distribution; therefore, a better choice is the negative log-likelihood as the loss function:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Define negative loglikelihood loss function</span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">neg_loglik</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> -y_pred.log_prob(y_true)
</code></pre>
<p class="normal">Let us now train this new model:</p>
<pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(loss=neg_loglik, optimizer=<span class="hljs-string">'adam'</span>)
<span class="hljs-comment"># Fit</span>
model.fit(x_train, y_train, epochs=<span class="hljs-number">500</span>, verbose=<span class="hljs-number">1</span>)
</code></pre>
<p class="normal">Since now our model returns a distribution, we require the statistics mean and standard deviation for the test dataset:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Summary Statistics</span>
y_mean = model(x_test).mean()
y_std = model(x_test).stddev()
</code></pre>
<p class="normal">Note that the predicted mean now corresponds to the fitted line in the first case. Let us now see the plots:</p>
<pre class="programlisting code"><code class="hljs-code">fig = plt.figure(figsize = (<span class="hljs-number">20</span>, <span class="hljs-number">10</span>))
plt.scatter(x_train, y_train, marker=<span class="hljs-string">'+'</span>, label=<span class="hljs-string">'Training Data'</span>, alpha=<span class="hljs-number">0.5</span>)
plt.plot(x_train, y_true, color=<span class="hljs-string">'k'</span>, label=<span class="hljs-string">'Ground Truth'</span>)
plt.plot(x_test, y_mean, color=<span class="hljs-string">'r'</span>, label=<span class="hljs-string">'</span><span class="hljs-string">Predicted Mean'</span>)
plt.fill_between(np.squeeze(x_test), np.squeeze(y_mean+<span class="hljs-number">1</span>*y_std), np.squeeze(y_mean-<span class="hljs-number">1</span>*y_std),  alpha=<span class="hljs-number">0.6</span>, label=<span class="hljs-string">'Aleatory Uncertainty (1SD)'</span>)
plt.fill_between(np.squeeze(x_test), np.squeeze(y_mean+<span class="hljs-number">2</span>*y_std), np.squeeze(y_mean-<span class="hljs-number">2</span>*y_std),  alpha=<span class="hljs-number">0.4</span>, label=<span class="hljs-string">'Aleatory Uncertainty (2SD)'</span>)
plt.title(<span class="hljs-string">'Aleatory Uncertainty'</span>)
plt.xlabel(<span class="hljs-string">'</span><span class="hljs-string">$x$'</span>)
plt.ylabel(<span class="hljs-string">'$y$'</span>)
plt.legend()
plt.show()
</code></pre>
<p class="normal">The <a id="_idIndexMarker1320"/>following curve shows the <a id="_idIndexMarker1321"/>fitted line, along with the aleatory uncertainty:</p>
<figure class="mediaobject"><img alt="Line chart  Description automatically generated" height="358" src="../Images/B18331_12_11.png" width="681"/></figure>
<p class="packt_figref">Figure 12.11: Modelling aleatory uncertainty using TFP layers</p>
<p class="normal">You can see that our model shows less uncertainty near the origin, but as we move further away, the uncertainty increases.</p>
<h3 class="heading-3" id="_idParaDest-350">Accounting for the epistemic uncertainty</h3>
<p class="normal">In conventional neural networks, each weight is represented by a single number, and it is updated <a id="_idIndexMarker1322"/>such that the loss of the model with respect to its weight is minimized. We assume that weights so learned are the optimum weights. But are they? To answer this question, we replace each weight with a distribution, and instead of learning a single value, we will now make our model learn a set of parameters for each weight distribution. This is accomplished by replacing the Keras <code class="inlineCode">Dense</code> layer with the <code class="inlineCode">DenseVariational</code> layer. The <code class="inlineCode">DenseVariational</code> layer uses a variational posterior over the weights to represent the uncertainty in their values. It tries to regularize the posterior to be close to the prior distribution. Hence, to use the <code class="inlineCode">DenseVariational</code> layer, we will need to define two functions, one prior generating function and another posterior generating function. We use the posterior and prior functions defined at <a href="https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression"><span class="url">https://www.tensorflow.org/probability/examples/Probabilistic_Layers_Regression</span></a>. </p>
<p class="normal">Our model <a id="_idIndexMarker1323"/>now has two layers, a <code class="inlineCode">DenseVariational</code> layer followed by a <code class="inlineCode">DistributionLambda</code> layer:</p>
<pre class="programlisting code"><code class="hljs-code">model = Sequential([
  tfp.layers.DenseVariational(<span class="hljs-number">1</span>, posterior_mean_field, prior_trainable, kl_weight=<span class="hljs-number">1</span>/x_train.shape[<span class="hljs-number">0</span>]),
  tfp.layers.DistributionLambda(<span class="hljs-keyword">lambda</span> t: tfd.Normal(loc=t, scale=<span class="hljs-number">1</span>)),
])
</code></pre>
<p class="normal">Again, as we are looking for distributions, the loss function that we use is the negative log-likelihood function:</p>
<pre class="programlisting code"><code class="hljs-code">model.<span class="hljs-built_in">compile</span>(optimizer=tf.optimizers.Adam(learning_rate=<span class="hljs-number">0.01</span>), loss=negloglik)
</code></pre>
<p class="normal">We continue with the same synthetic data that we created earlier and train the model:</p>
<pre class="programlisting code"><code class="hljs-code">model.fit(x_train, y_train, epochs=<span class="hljs-number">100</span>, verbose=<span class="hljs-number">1</span>)
</code></pre>
<p class="normal">Now that the model has been trained, we make the prediction, and to understand the concept of uncertainty, we make multiple predictions for the same input ranges. We can see the difference in variance in the result in the following graphs:</p>
<table class="table-container" id="table003-2">
<tbody>
<tr>
<td class="table-cell">
<p class="normal"><img alt="Chart, scatter chart  Description automatically generated" height="347" src="../Images/B18331_12_12_1.png" width="369"/></p>
</td>
<td class="table-cell">
<p class="normal"><img alt="Chart, scatter chart  Description automatically generated" height="343" src="../Images/B18331_12_12_2.png" width="365"/></p>
</td>
</tr>
</tbody>
</table>
<p class="packt_figref">Figure 12.12: Epistemic uncertainty</p>
<p class="normal"><em class="italic">Figure 12.12</em> shows two graphs, one when only 200 training data points were used to build the model, and the second when 2,000 data points were used to train the model. We can see <a id="_idIndexMarker1324"/>that when there is more data, the variance and, hence, the epistemic uncertainty reduces. Here, <em class="italic">overall mean</em> refers to the mean of all the predictions (100 in number), and in the case of <em class="italic">ensemble mean</em>, we considered only the first 15 predictions. All machine learning models suffer from some level of uncertainty in predicting outcomes. Getting an estimate or quantifiable range of uncertainty in the prediction will help AI users build more confidence in their AI predictions and will boost overall AI adoption. </p>
<h1 class="heading-1" id="_idParaDest-351">Summary</h1>
<p class="normal">This chapter introduced TensorFlow Probability, the library built over TensorFlow to perform probabilistic reasoning and statistical analysis. The chapter started with the need for probabilistic reasoning – the uncertainties both due to the inherent nature of data and due to a lack of knowledge. We demonstrated how to use TensorFlow Probability distributions to generate different data distributions. We learned how to build a Bayesian network and perform inference. Then, we built Bayesian neural networks using TFP layers to take into account aleatory uncertainty. Finally, we learned how to account for epistemic uncertainty with the help of the <code class="inlineCode">DenseVariational</code> TFP layer. </p>
<p class="normal">In the next chapter, we will learn about TensorFlow AutoML frameworks.</p>
<h1 class="heading-1" id="_idParaDest-352">References</h1>
<ol class="numberedList" style="list-style-type: decimal;">
<li class="numberedList" value="1">Dillon, J. V., Langmore, I., Tran, D., Brevdo, E., Vasudevan, S., Moore, D., Patton, B., Alemi, A., Hoffman, M., and Saurous, R. A. (2017). <em class="italic">TensorFlow distributions</em>. arXiv preprint arXiv:1711.10604.</li>
<li class="numberedList">Piponi, D., Moore, D., and Dillon, J. V. (2020). <em class="italic">Joint distributions for TensorFlow probability</em>. arXiv preprint arXiv:2001.11819.</li>
<li class="numberedList">Fox, C. R. and Ülkümen, G. (2011). <em class="italic">Distinguishing Two Dimensions of Uncertainty</em>, in Essays in Judgment and Decision Making, Brun, W., Kirkebøen, G. and Montgomery, H., eds. Oslo: Universitetsforlaget.</li>
<li class="numberedList">Hüllermeier, E. and Waegeman, W. (2021). <em class="italic">Aleatoric and epistemic uncertainty in machine learning: An introduction to concepts and methods</em>. Machine Learning 110, no. 3: 457–506.</li>
</ol>
<h1 class="heading-1">Join our book’s Discord space</h1>
<p class="normal">Join our Discord community to meet like-minded people and learn alongside more than 2000 members at: <a href="https://packt.link/keras"><span class="url">https://packt.link/keras</span></a></p>
<p class="normal"><img alt="" height="177" src="../Images/QR_Code1831217224278819687.png" width="177"/></p>
</div>
</div>
</body></html>