["```\nimport tensorflow as tf\nfrom tensorflow import keras\nNB_CLASSES = 10\nRESHAPED = 784\nmodel = tf.keras.models.Sequential()\nmodel.add(keras.layers.Dense(NB_CLASSES,\n            input_shape=(RESHAPED,), kernel_initializer='zeros',\n            name='dense_layer', activation='softmax')) \n```", "```\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\n# Network and training parameters.\nEPOCHS = 200\nBATCH_SIZE = 128\nVERBOSE = 1\nNB_CLASSES = 10   # number of outputs = number of digits\nN_HIDDEN = 128\nVALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n# Loading MNIST dataset.\n# verify\n# You can verify that the split between train and test is 60,000, and 10,000 respectively. \n# Labels have one-hot representation.is automatically applied\nmnist = keras.datasets.mnist\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n# X_train is 60000 rows of 28x28 values; we  --> reshape it to 60000 x 784.\nRESHAPED = 784\n#\nX_train = X_train.reshape(60000, RESHAPED)\nX_test = X_test.reshape(10000, RESHAPED)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n# Normalize inputs to be within in [0, 1].\nX_train /= 255\nX_test /= 255\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n# One-hot representation of the labels.\nY_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\nY_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES) \n```", "```\n# Build the model.\nmodel = tf.keras.models.Sequential()\nmodel.add(keras.layers.Dense(NB_CLASSES,\n            input_shape=(RESHAPED,),\n            name='dense_layer', \n            activation='softmax')) \n```", "```\n# Compiling the model.\nmodel.compile(optimizer='SGD', \n              loss='categorical_crossentropy',\n              metrics=['accuracy']) \n```", "```\n# Training the model.\nmodel.fit(X_train, Y_train,\n          batch_size=BATCH_SIZE, epochs=EPOCHS,\n          verbose=VERBOSE, validation_split=VALIDATION_SPLIT) \n```", "```\n#evaluate the model\ntest_loss, test_acc = model.evaluate(X_test, Y_test)\nprint('Test accuracy:', test_acc) \n```", "```\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                Output Shape              Param #    \n=================================================================\ndense_layer (Dense)         (None, 10)                7850       \n\n=================================================================\nTotal params: 7,850\nTrainable params: 7,850\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/200\n48000/48000 [==============================] - 1s 31us/sample - loss: 2.1276 - accuracy: 0.2322 - val_loss: 1.9508 - val_accuracy: 0.3908\nEpoch 2/200\n48000/48000 [==============================] - 1s 23us/sample - loss: 1.8251 - accuracy: 0.5141 - val_loss: 1.6848 - val_accuracy: 0.6277\nEpoch 3/200\n48000/48000 [==============================] - 1s 25us/sample - loss: 1.5992 - accuracy: 0.6531 - val_loss: 1.4838 - val_accuracy: 0.7150\nEpoch 4/200\n48000/48000 [==============================] - 1s 27us/sample - loss: 1.4281 - accuracy: 0.7115 - val_loss: 1.3304 - val_accuracy: 0.7551\nEpoch 5/200 \n```", "```\nEpoch 199/200\n48000/48000 [==============================] - 1s 22us/sample - loss: 0.3684 - accuracy: 0.8995 - val_loss: 0.3464 - val_accuracy: 0.9071\nEpoch 200/200\n48000/48000 [==============================] - 1s 23us/sample - loss: 0.3680 - accuracy: 0.8996 - val_loss: 0.3461 - val_accuracy: 0.9070\n10000/10000 [==============================] - 1s 54us/sample - loss: 0.3465 - accuracy: 0.9071\nTest accuracy: 0.9071 \n```", "```\nimport tensorflow as tf\nfrom tensorflow import keras\n# Network and training.\nEPOCHS = 50\nBATCH_SIZE = 128\nVERBOSE = 1\nNB_CLASSES = 10   # number of outputs = number of digits\nN_HIDDEN = 128\nVALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\n# Loading MNIST dataset.\n# Labels have one-hot representation.\nmnist = keras.datasets.mnist\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\nRESHAPED = 784\n#\nX_train = X_train.reshape(60000, RESHAPED)\nX_test = X_test.reshape(10000, RESHAPED)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n# Normalize inputs to be within in [0, 1].\nX_train, X_test = X_train / 255.0, X_test / 255.0\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n# Labels have one-hot representation.\nY_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\nY_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n# Build the model.\nmodel = tf.keras.models.Sequential()\nmodel.add(keras.layers.Dense(N_HIDDEN,\n             input_shape=(RESHAPED,),\n             name='dense_layer', activation='relu'))\nmodel.add(keras.layers.Dense(N_HIDDEN,\n             name='dense_layer_2', activation='relu'))\nmodel.add(keras.layers.Dense(NB_CLASSES,\n             name='dense_layer_3', activation='softmax'))\n# Summary of the model.\nmodel.summary()\n# Compiling the model.\nmodel.compile(optimizer='SGD', \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n# Training the model.\nmodel.fit(X_train, Y_train,\n          batch_size=BATCH_SIZE, epochs=EPOCHS,\n          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n# Evaluating the model.\ntest_loss, test_acc = model.evaluate(X_test, Y_test)\nprint('Test accuracy:', test_acc) \n```", "```\n> labels\narray([0, 2, 1, 2, 0]) \n```", "```\nto_categorical(labels)\narray([[ 1.,  0.,  0.],\n       [ 0.,  0.,  1.],\n       [ 0.,  1.,  0.],\n       [ 0.,  0.,  1.],\n       [ 1.,  0.,  0.]], dtype=float32) \n```", "```\n_________________________________________________________________\nLayer (type)                Output Shape              Param #    \n=================================================================\ndense_layer (Dense)         (None, 128)               100480     \n\ndense_layer_2 (Dense)       (None, 128)               16512      \n\ndense_layer_3 (Dense)       (None, 10)                1290       \n\n=================================================================\nTotal params: 118,282\nTrainable params: 118,282\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/50\n48000/48000 [==============================] - 3s 63us/sample - loss: 2.2507 - accuracy: 0.2086 - val_loss: 2.1592 - val_accuracy: 0.3266 \n```", "```\nEpoch 49/50\n48000/48000 [==============================] - 1s 30us/sample - loss: 0.3347 - accuracy: 0.9075 - val_loss: 0.3126 - val_accuracy: 0.9136\nEpoch 50/50\n48000/48000 [==============================] - 1s 28us/sample - loss: 0.3326 - accuracy: 0.9081 - val_loss: 0.3107 - val_accuracy: 0.9140\n10000/10000 [==============================] - 0s 40us/sample - loss: 0.3164 - accuracy: 0.9118\nTest accuracy: 0.9118 \n```", "```\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow import keras\n# Network and training.\nEPOCHS = 200\nBATCH_SIZE = 128\nVERBOSE = 1\nNB_CLASSES = 10   # number of outputs = number of digits\nN_HIDDEN = 128\nVALIDATION_SPLIT = 0.2 # how much TRAIN is reserved for VALIDATION\nDROPOUT = 0.3\n# Loading MNIST dataset.\n# Labels have one-hot representation.\nmnist = keras.datasets.mnist\n(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n# X_train is 60000 rows of 28x28 values; we reshape it to 60000 x 784.\nRESHAPED = 784\n#\nX_train = X_train.reshape(60000, RESHAPED)\nX_test = X_test.reshape(10000, RESHAPED)\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n# Normalize inputs within [0, 1].\nX_train, X_test = X_train / 255.0, X_test / 255.0\nprint(X_train.shape[0], 'train samples')\nprint(X_test.shape[0], 'test samples')\n# One-hot representations for labels.\nY_train = tf.keras.utils.to_categorical(Y_train, NB_CLASSES)\nY_test = tf.keras.utils.to_categorical(Y_test, NB_CLASSES)\n# Building the model.\nmodel = tf.keras.models.Sequential()\nmodel.add(keras.layers.Dense(N_HIDDEN,\n              input_shape=(RESHAPED,),\n              name='dense_layer', activation='relu'))\nmodel.add(keras.layers.Dropout(DROPOUT))\nmodel.add(keras.layers.Dense(N_HIDDEN,\n              name='dense_layer_2', activation='relu'))\nmodel.add(keras.layers.Dropout(DROPOUT))\nmodel.add(keras.layers.Dense(NB_CLASSES,\n              name='dense_layer_3', activation='softmax'))\n# Summary of the model.\nmodel.summary()\n# Compiling the model.\nmodel.compile(optimizer='SGD', \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n# Training the model.\nmodel.fit(X_train, Y_train,\n          batch_size=BATCH_SIZE, epochs=EPOCHS,\n          verbose=VERBOSE, validation_split=VALIDATION_SPLIT)\n# Evaluating the model.\ntest_loss, test_acc = model.evaluate(X_test, Y_test)\nprint('Test accuracy:', test_acc) \n```", "```\nEpoch 199/200\n48000/48000 [==============================] - 2s 45us/sample - loss: 0.2850 - accuracy: 0.9177 - val_loss: 0.1922 - val_accuracy: 0.9442\nEpoch 200/200\n48000/48000 [==============================] - 2s 42us/sample - loss: 0.2845 - accuracy: 0.9170 - val_loss: 0.1917 - val_accuracy: 0.9442\n10000/10000 [==============================] - 1s 61us/sample - loss: 0.1927 - accuracy: 0.9415\nTest accuracy: 0.9415 \n```", "```\n# Compiling the model.\nmodel.compile(optimizer='RMSProp', \n              loss='categorical_crossentropy', metrics=['accuracy']) \n```", "```\n_________________________________________________________________\nLayer (type)                Output Shape              Param #    \n=================================================================\ndense_layer (Dense)         (None, 128)               100480     \n\ndropout_2 (Dropout)         (None, 128)               0          \n\ndense_layer_2 (Dense)       (None, 128)               16512      \n\ndropout_3 (Dropout)         (None, 128)               0          \n\ndense_layer_3 (Dense)       (None, 10)                1290       \n\n=================================================================\nTotal params: 118,282\nTrainable params: 118,282\nNon-trainable params: 0\n_________________________________________________________________\nTrain on 48000 samples, validate on 12000 samples\nEpoch 1/10\n48000/48000 [==============================] - 2s 48us/sample - loss: 0.4715 - accuracy: 0.8575 - val_loss: 0.1820 - val_accuracy: 0.9471\nEpoch 2/10\n48000/48000 [==============================] - 2s 36us/sample - loss: 0.2215 - accuracy: 0.9341 - val_loss: 0.1268 - val_accuracy: 0.9361\nEpoch 3/10\n48000/48000 [==============================] - 2s 39us/sample - loss: 0.1684 - accuracy: 0.9497 - val_loss: 0.1198 - val_accuracy: 0.9651\nEpoch 4/10\n48000/48000 [==============================] - 2s 43us/sample - loss: 0.1459 - accuracy: 0.9569 - val_loss: 0.1059 - val_accuracy: 0.9710\nEpoch 5/10\n48000/48000 [==============================] - 2s 39us/sample - loss: 0.1273 - accuracy: 0.9623 - val_loss: 0.1059 - val_accuracy: 0.9696\nEpoch 6/10\n48000/48000 [==============================] - 2s 36us/sample - loss: 0.1177 - accuracy: 0.9659 - val_loss: 0.0941 - val_accuracy: 0.9731\nEpoch 7/10\n48000/48000 [==============================] - 2s 35us/sample - loss: 0.1083 - accuracy: 0.9671 - val_loss: 0.1009 - val_accuracy: 0.9715\nEpoch 8/10\n48000/48000 [==============================] - 2s 35us/sample - loss: 0.0971 - accuracy: 0.9706 - val_loss: 0.0950 - val_accuracy: 0.9758\nEpoch 9/10\n48000/48000 [==============================] - 2s 35us/sample - loss: 0.0969 - accuracy: 0.9718 - val_loss: 0.0985 - val_accuracy: 0.9745\nEpoch 10/10\n48000/48000 [==============================] - 2s 35us/sample - loss: 0.0873 - accuracy: 0.9743 - val_loss: 0.0966 - val_accuracy: 0.9762\n10000/10000 [==============================] - 1s 2ms/sample - loss: 0.0922 - accuracy: 0.9764\nTest accuracy: 0.9764 \n```", "```\nEpoch 248/250\n48000/48000 [==============================] - 2s 40us/sample - loss: 0.0506 - accuracy: 0.9904 - val_loss: 0.3465 - val_accuracy: 0.9762\nEpoch 249/250\n48000/48000 [==============================] - 2s 40us/sample - loss: 0.0490 - accuracy: 0.9905 - val_loss: 0.3645 - val_accuracy: 0.9765\nEpoch 250/250\n48000/48000 [==============================] - 2s 39us/sample - loss: 0.0547 - accuracy: 0.9899 - val_loss: 0.3353 - val_accuracy: 0.9766\n10000/10000 [==============================] - 1s 58us/sample - loss: 0.3184 - accuracy: 0.9779\nTest accuracy: 0.9779 \n```", "```\n# Compiling the model.\nmodel.compile(optimizer='Adam', \n              loss='categorical_crossentropy',\n              metrics=['accuracy']) \n```", "```\nEpoch 49/50\n48000/48000 [==============================] - 3s 55us/sample - loss: 0.0313 - accuracy: 0.9894 - val_loss: 0.0868 - val_accuracy: 0.9808\nEpoch 50/50\n48000/48000 [==============================] - 2s 51s/sample - loss: 0.0321 - accuracy: 0.9894 - val_loss: 0.0983 - val_accuracy: 0.9789\n10000/10000 [==============================] - 1s 66us/step - loss: 0.0964 - accuracy: 0.9782\nTest accuracy: 0.9782 \n```", "```\nfrom tf.keras.regularizers import l2, activity_l2\nmodel.add(Dense(64, input_dim=64, W_regularizer=l2(0.01),\n    activity_regularizer=activity_l2(0.01))) \n```", "```\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models, preprocessing\nimport tensorflow_datasets as tfds\nmax_len = 200\nn_words = 10000\ndim_embedding = 256\nEPOCHS = 20\nBATCH_SIZE = 500\ndef load_data():\n    # Load data.\n    (X_train, y_train), (X_test, y_test) = datasets.imdb.load_data(num_words=n_words)\n    # Pad sequences with max_len.\n    X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n    X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n    return (X_train, y_train), (X_test, y_test) \n```", "```\ndef build_model():\n    model = models.Sequential()\n    # Input: - eEmbedding Layer.\n    # The model will take as input an integer matrix of size (batch, input_length).\n    # The model will output dimension (input_length, dim_embedding).\n    # The largest integer in the input should be no larger\n    # than n_words (vocabulary size).\n    model.add(layers.Embedding(n_words, \n        dim_embedding, input_length=max_len))\n    model.add(layers.Dropout(0.3))\n    # Takes the maximum value of either feature vector from each of the n_words features.\n    model.add(layers.GlobalMaxPooling1D())\n    model.add(layers.Dense(128, activation='relu'))\n    model.add(layers.Dropout(0.5))\n    model.add(layers.Dense(1, activation='sigmoid'))\n    return model \n```", "```\n(X_train, y_train), (X_test, y_test) = load_data()\nmodel = build_model()\nmodel.summary()\nmodel.compile(optimizer = \"adam\", loss = \"binary_crossentropy\",\n metrics = [\"accuracy\"]\n)\nscore = model.fit(X_train, y_train,\n epochs = EPOCHS,\n batch_size = BATCH_SIZE,\n validation_data = (X_test, y_test)\n)\nscore = model.evaluate(X_test, y_test, batch_size=BATCH_SIZE)\nprint(\"\\nTest score:\", score[0])\nprint('Test accuracy:', score[1]) \n```", "```\n___________________________________________________________________\nLayer (type)                  Output Shape              Param #    \n===================================================================\nembedding (Embedding)         (None, 200, 256)          2560000    \n\ndropout (Dropout)             (None, 200, 256)          0          \n\nglobal_max_pooling1d (Global  (None, 256)               0          \n\ndense (Dense)                 (None, 128)               32896      \n\ndropout_1 (Dropout)           (None, 128)               0          \n\ndense_1 (Dense)               (None, 1)                 129        \n\n===================================================================\nTotal params: 2,593,025\nTrainable params: 2,593,025\nNon-trainable params: 0 \n```", "```\nEpoch 20/20\n25000/25000 [==============================] - 23s 925ms/sample - loss: 0.0053 - accuracy: 0.9991 - val_loss: 0.4993 - val_accuracy: 0.8503\n25000/25000 [==============================] - 2s 74us/sample - loss: 0.4993 - accuracy: 0.88503\nTest score: 0.4992710727453232\nTest accuracy: 0.85028 \n```", "```\n# Making predictions.\npredictions = model.predict(X) \n```", "```\n# X_train is 60000 rows of 28x28 values; we  --> reshape it as in 60000 x 784.\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784) \n```"]