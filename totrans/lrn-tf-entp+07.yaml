- en: Section 3 â€“ Scaling and Tuning ML Works
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having covered how to set up a training job through various means of TensorFlow
    Enterprise model development, now is the time to scale the training process by
    using a cluster of GPUs or TPUs. You will learn how to leverage distributed training
    strategies and implement hyperparameter tuning to scale and improve your model
    training experiment.
  prefs: []
  type: TYPE_NORMAL
- en: In this part, you will learn about how to set up GPUs and TPUs in a GCP environment
    for submitting a model training job in GCP. You also will learn about the latest
    hyperparameter tuning API and run it at scale using GCP resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B16070_05_Final_JM_ePub.xhtml#_idTextAnchor145), *Training at
    Scale*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B16070_06_Final_JM_ePub.xhtml#_idTextAnchor177), *Hyperparameter
    Tuning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
