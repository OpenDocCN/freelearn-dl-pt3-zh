["```\npi = policy(states) # actions probability for each action\nonehot_action = tf.one_hot(actions, depth=env.action_space.n) \npi_log = tf.reduce_sum(onehot_action * tf.math.log(pi), axis=1)\n\npi_loss = -tf.reduce_mean(pi_log * Q_function(states, actions))\n\n# calculate the gradients of pi_loss with respect to the variables\ngradients = tf.gradient(pi_loss, variables)\n\n# or optimize directly pi_loss with Adam (or any other SGD optimizer)\n# pi_opt = tf.train.AdamOptimizer(lr).minimize(pi_loss) #\n```", "```\npi = policy(states) # actions probability for each action\nonehot_action = tf.one_hot(actions, depth=env.action_space.n) \n\npi_log = tf.reduce_sum(onehot_action * tf.nn.log_softmax(pi), axis=1) # instead of tf.math.log(pi)\n\npi_loss = -tf.reduce_mean(pi_log * Q_function(states, actions))\ngradients = tf.gradient(pi_loss, variables)\n```", "```\ndef discounted_rewards(rews, gamma):\n    rtg = np.zeros_like(rews, dtype=np.float32)\n    rtg[-1] = rews[-1]\n    for i in reversed(range(len(rews)-1)):\n        rtg[i] = rews[i] + gamma*rtg[i+1]\n    return rtg\n```", "```\nInitialize  with random weight\n\nfor episode 1..M do\n    Initialize environment \n    Initialize empty buffer\n\n    *> Generate a few episodes*\n    for step 1..MaxSteps do\n        *> Collect experience by acting on the environment*\n\n        if :\n\n       *     > Compute the reward to go* \n             # for each t\n            *> Store the episode in the buffer*\n             # where  is the length of the episode\n    *> REINFORCE update step using all the experience in ![](img/56af0138-16a3-45ac-a415-8e84bd262d9e.png) following formula (6.5)\n*    \n```", "```\ndef REINFORCE(env_name, hidden_sizes=[32], lr=5e-3, num_epochs=50, gamma=0.99, steps_per_epoch=100):\n```", "```\ndef REINFORCE(env_name, hidden_sizes=[32], lr=5e-3, num_epochs=50, gamma=0.99, steps_per_epoch=100):\n\n    tf.reset_default_graph()\n\n    env = gym.make(env_name) \n    obs_dim = env.observation_space.shape\n    act_dim = env.action_space.n \n\n    obs_ph = tf.placeholder(shape=(None, obs_dim[0]), dtype=tf.float32, name='obs')\n    act_ph = tf.placeholder(shape=(None,), dtype=tf.int32, name='act')\n    ret_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='ret')\n\n    p_logits = mlp(obs_ph, hidden_sizes, act_dim, activation=tf.tanh)\n```", "```\n act_multn = tf.squeeze(tf.random.multinomial(p_logits, 1))\n actions_mask = tf.one_hot(act_ph, depth=act_dim)\n p_log = tf.reduce_sum(actions_mask * tf.nn.log_softmax(p_logits), axis=1)\n p_loss = -tf.reduce_mean(p_log*ret_ph)\n p_opt = tf.train.AdamOptimizer(lr).minimize(p_loss)\n```", "```\n    sess = tf.Session()\n    sess.run(tf.global_variables_initializer())\n    step_count = 0\n    train_rewards = []\n    train_ep_len = []\n```", "```\n    for ep in range(num_epochs):\n        obs = env.reset()\n        buffer = Buffer(gamma)\n        env_buf = []\n        ep_rews = []\n\n        while len(buffer) < steps_per_epoch:\n\n            # run the policy \n            act = sess.run(act_multn, feed_dict={obs_ph:[obs]})\n            # take a step in the environment\n            obs2, rew, done, _ = env.step(np.squeeze(act))\n\n            env_buf.append([obs.copy(), rew, act])\n            obs = obs2.copy()\n            step_count += 1\n            ep_rews.append(rew)\n\n            if done:\n                # add the full trajectory to the environment\n                buffer.store(np.array(env_buf))\n                env_buf = []\n                train_rewards.append(np.sum(ep_rews))\n                train_ep_len.append(len(ep_rews))\n                obs = env.reset()\n                ep_rews = []\n\n        obs_batch, act_batch, ret_batch = buffer.get_batch()\n        # Policy optimization\n        sess.run(p_opt, feed_dict={obs_ph:obs_batch, act_ph:act_batch, ret_ph:ret_batch})\n\n        # Print some statistics\n        if ep % 10 == 0:\n            print('Ep:%d MnRew:%.2f MxRew:%.1f EpLen:%.1f Buffer:%d -- Step:%d --' % (ep, np.mean(train_rewards), np.max(train_rewards), np.mean(train_ep_len), len(buffer), step_count))\n            train_rewards = []\n            train_ep_len = []\n    env.close()\n```", "```\nclass Buffer():\n    def __init__(self, gamma=0.99):\n        self.gamma = gamma\n        self.obs = []\n        self.act = []\n        self.ret = []\n\n    def store(self, temp_traj):\n        if len(temp_traj) > 0:\n            self.obs.extend(temp_traj[:,0])\n            ret = discounted_rewards(temp_traj[:,1], self.gamma)\n            self.ret.extend(ret)\n            self.act.extend(temp_traj[:,2])\n\n    def get_batch(self):\n        return self.obs, self.act, self.ret\n\n    def __len__(self):\n        assert(len(self.obs) == len(self.act) == len(self.ret))\n        return len(self.obs)\n```", "```\ndef mlp(x, hidden_layers, output_size, activation=tf.nn.relu, last_activation=None):\nfor l in hidden_layers:\n    x = tf.layers.dense(x, units=l, activation=activation)\n    return tf.layers.dense(x, units=output_size, activation=last_activation)\n```", "```\nREINFORCE('LunarLander-v2', hidden_sizes=[64], lr=8e-3, gamma=0.99, num_epochs=1000, steps_per_epoch=1000)\n```", "```\n    def get_batch(self):\n        b_ret = self.ret - np.mean(self.ret)\n        return self.obs, self.act, b_ret\n```", "```\n    ...\n    # placeholder that will contain the reward to go values (i.e. the y values)\n    rtg_ph = tf.placeholder(shape=(None,), dtype=tf.float32, name='rtg')\n\n    # MLP value function\n    s_values = tf.squeeze(mlp(obs_ph, hidden_sizes, 1, activation=tf.tanh))\n\n    # MSE loss function\n    v_loss = tf.reduce_mean((rtg_ph - s_values)**2)\n\n    # value function optimization\n    v_opt = tf.train.AdamOptimizer(vf_lr).minimize(v_loss)\n    ...\n```", "```\n            ...\n            # besides act_multn, run also s_values\n            act, val = sess.run([act_multn, s_values], feed_dict={obs_ph:[obs]})\n            obs2, rew, done, _ = env.step(np.squeeze(act))\n\n            # add the new transition, included the state value predictions\n            env_buf.append([obs.copy(), rew, act, np.squeeze(val)])\n            ...\n\n```", "```\n        obs_batch, act_batch, ret_batch, rtg_batch = buffer.get_batch() \n        sess.run([p_opt, v_opt], feed_dict={obs_ph:obs_batch, act_ph:act_batch, ret_ph:ret_batch, rtg_ph:rtg_batch})\n```", "```\n    def store(self, temp_traj):\n        if len(temp_traj) > 0:\n            self.obs.extend(temp_traj[:,0])\n            rtg = discounted_rewards(temp_traj[:,1], self.gamma)\n            # ret = G - V\n            self.ret.extend(rtg - temp_traj[:,3])\n            self.rtg.extend(rtg)\n            self.act.extend(temp_traj[:,2])\n\n    def get_batch(self):\n        return self.obs, self.act, self.ret, self.rtg\n```", "```\nInitialize  with random weight\nInitialize environment \nfor episode 1..M do\n    Initialize empty buffer\n\n    *> Generate a few episodes*\n    for step 1..MaxSteps do\n        *> Collect experience by acting on the environment*\n\n        if :\n\n            *> Compute the n-step reward to go* \n             # for each t\n            *> Compute the advantage values*\n             # for each t\n            *> Store the episode in the buffer*\n             # where  is the lenght of the episode\n    *> Actor update step using all the experience in ![](img/c782ce23-3d71-4d58-9cc7-31cb78ce9b55.png)\n*    \n    *> Critic update using all the experience in **D***\n\n```", "```\ndef discounted_rewards(rews, last_sv, gamma):\n    rtg = np.zeros_like(rews, dtype=np.float32)\n    rtg[-1] = rews[-1] + gamma*last_sv    # Bootstrap with the estimate next state value \n\n    for i in reversed(range(len(rews)-1)):\n        rtg[i] = rews[i] + gamma*rtg[i+1]\n    return rtg\n\n```", "```\n    obs = env.reset()\n    ep_rews = []\n\n    for ep in range(num_epochs):\n        buffer = Buffer(gamma)\n        env_buf = []\n\n        for _ in range(steps_per_env):\n            act, val = sess.run([act_multn, s_values], feed_dict={obs_ph:[obs]})\n            obs2, rew, done, _ = env.step(np.squeeze(act))\n\n            env_buf.append([obs.copy(), rew, act, np.squeeze(val)])\n            obs = obs2.copy()\n            step_count += 1\n            last_test_step += 1\n            ep_rews.append(rew)\n\n            if done:\n\n                buffer.store(np.array(env_buf), 0)\n                env_buf = []\n\n                train_rewards.append(np.sum(ep_rews))\n                train_ep_len.append(len(ep_rews))\n                obs = env.reset()\n                ep_rews = []\n\n        if len(env_buf) > 0:\n            last_sv = sess.run(s_values, feed_dict={obs_ph:[obs]})\n            buffer.store(np.array(env_buf), last_sv)\n\n        obs_batch, act_batch, ret_batch, rtg_batch = buffer.get_batch()\n        sess.run([p_opt, v_opt], feed_dict={obs_ph:obs_batch, act_ph:act_batch, ret_ph:ret_batch,         rtg_ph:rtg_batch})\n        ...\n```", "```\n    def store(self, temp_traj, last_sv):\n        if len(temp_traj) > 0:\n            self.obs.extend(temp_traj[:,0])\n            rtg = discounted_rewards(temp_traj[:,1], last_sv, self.gamma)\n            self.ret.extend(rtg - temp_traj[:,3])\n            self.rtg.extend(rtg)\n            self.act.extend(temp_traj[:,2])\n```", "```\nAC('LunarLander-v2', hidden_sizes=[64], ac_lr=4e-3, cr_lr=1.5e-2, gamma=0.99, steps_per_epoch=100, num_epochs=8000)\n```"]