- en: Object Detection and Image Segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml), *Advanced Convolutional
    Networks*, we discussed some of the most popular and best performing **convolutional
    neural network **(**CNN**) models. To focus on the architecture specifics of each
    network, we viewed the models in the straightforward context of the classification
    problem. In the universe of computer vision tasks, classification is fairly straightforward,
    as it assigns a single label to an image. In this chapter, we'll shift our focus
    to two more interesting computer vision tasks—object detection and semantic segmentation,
    while the network architecture will take a backseat. We can say that these tasks
    are more complex compared to classification, because the model has to obtain a
    more comprehensive understanding of the image. It has to be able to detect different
    objects as well as their positions on the image. At the same time, the task complexity
    allows for more creative solutions. In this chapter, we'll discuss some of them.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Introduction to object detection:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Approaches to object detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: YOLO
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Faster R-CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image segmentation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: U-Net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mask R-CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Object detection is the process of finding object instances of a certain class,
    such as faces, cars, and trees, in images or videos. Unlike classification, object
    detection can detect multiple objects as well as their location in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'An object detector would return a list of detected objects with the following
    information for each object:'
  prefs: []
  type: TYPE_NORMAL
- en: The class of the object (person, car, tree, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Probability (or confidence score) in the [0, 1] range, which conveys how confident
    the detector is that the object exists in that location. This is similar to the
    output of a regular classifier.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The coordinates of the rectangular region of the image where the object is located.
    This rectangle is called a **bounding box**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see the typical output of an object-detection algorithm in the following
    photograph. The object type and confidence score are above each bounding box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e3e8ed91-d2f9-4711-9aef-361fce94c5c8.png)'
  prefs: []
  type: TYPE_IMG
- en: The output of an object detector
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's outline the different approaches to solving an object detection
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches to object detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll outline three approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Classic sliding window**: Here, we''ll use a regular classification network
    (classifier). This approach can work with any type of classification algorithm,
    but it''s relatively slow and error-prone:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Build an image pyramid: This is a combination of different scales of the same
    image (see the following photograph). For example, each scaled image can be two
    times smaller than the previous one. In this way, we''ll be able to detect objects
    regardless of their size in the original image.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Slide the classifier across the whole image: That is, we''ll use each location
    of the image as an input to the classifier, and the result will determine the
    type of object that is in the location. The bounding box of the location is just
    the image region that we used as input.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll have multiple overlapping bounding boxes for each object: We''ll use
    some heuristics to combine them in a single prediction.'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here is a diagram of the sliding window approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4781245d-f125-4945-a2fd-51a3d4da219e.png)'
  prefs: []
  type: TYPE_IMG
- en: Sliding window plus image pyramid object detection
  prefs: []
  type: TYPE_NORMAL
- en: '**Two-stage detection methods**: These methods are very accurate, but relatively
    slow. As the name suggests, they involve two steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A special type of CNN, called a **Region Proposal Network** (**RPN**), scans
    the image and proposes a number of possible bounding boxes, or regions of interest
    (**RoI**), where objects might be located. However, this network doesn't detect
    the type of the object, but only whether an object is present in the region.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The regions of interest are sent to the second stage for object classification,
    which determines the actual object in each bounding box.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**One-stage** (**or one-shot**) **detection methods**: Here, a single CNN produces
    both the object type and the bounding box. These approaches are usually faster,
    but less accurate compared to the two-stage methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we'll introduce the YOLO—an accurate, yet efficient one-stage
    detection algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Object detection with YOLOv3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll discuss one of the most popular detection algorithms,
    called YOLO. The name is an acronym for the popular motto **you only live once**,
    which reflects the one-stage nature of the algorithm. The authors have released three versions
    with incremental improvements of the algorithm. We''ll only discuss the latest,
    v3 ( for more details, see *YOLOv3: An Incremental Improvement*, [https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767)).'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm starts with the so-called **backbone** network called **Darknet-53** (after
    the number of convolutional layers). It is trained to classify the ImageNet dataset,
    just as the networks in [Chapter 3](433225cc-e19a-4ecb-9874-8de71338142d.xhtml),
    *Advanced Convolutional Networks*. It is fully convolutional (no pooling layers)
    and uses residual connections.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the backbone architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bd1d4252-1a0c-4f41-8a0e-310cdfd4afc5.png)'
  prefs: []
  type: TYPE_IMG
- en: The Darknet-53 model (source: https://arxiv.org/abs/1804.02767)
  prefs: []
  type: TYPE_NORMAL
- en: Once the network is trained, it will serve as a base for the following object
    detection training phase. This is a case of feature extraction transfer learning,
    which we described in [Chapter 2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml),
    *Understanding Convolutional Networks*. The fully connected layers of the backbone
    are replaced with new randomly initialized convolutional and fully connected layers.
    The new fully connected layers will output the bounding boxes, object classes,
    and confidence scores of all detected objects in just a single pass.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the bounding boxes in the image of people on the crosswalk at
    the beginning of this section were generated using a single network pass. YOLOv3
    predicts boxes at three different scales. The system extracts features from those
    scales using a similar concept to feature pyramid networks (for more information,
    see *Feature Pyramid Networks for Object Detection*, [https://arxiv.org/abs/1612.03144](https://arxiv.org/abs/1612.03144)).
    In the detection phase, the network is trained with the common objects in context (*Microsoft
    COCO: Common Objects in Context*, [https://arxiv.org/abs/1405.0312](https://arxiv.org/abs/1405.0312), [http://cocodataset.org](http://cocodataset.org))
    object detection dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s see how YOLO works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Split the image into a grid of *S×S* cells (in the following diagram, we can
    see a 3×3 grid):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The network treats the center of each grid cell as the center of the region,
    where an object might be located.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An object might lie entirely within a cell. Then, its bounding box will be smaller
    than the cell. Alternatively, it can span over multiple cells and the bounding
    box will be larger. YOLO covers both cases.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The algorithm can detect multiple objects in a grid cell with the help of **anchor
    boxes** (more on that later), but an object is associated with one cell only (a
    one-to-*n* relation). That is, if the bounding box of the object covers multiple
    cells, we'll associate the object with the cell, where the center of the bounding
    box lies. For example, the two objects in the following diagram span multiple
    cells, but they are both assigned to the central cell, because their centers lie
    in it.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of the cells may contain an object and others might not. We are only interested
    in the ones that do.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows a 3×3 cell grid with 2 objects and their bounding
    boxes (dashed lines). Both objects are associated with the middle cell, because
    the centers of their bounding boxes lie in that cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/51d5c3d5-1325-4153-b367-4c45da92f509.png)'
  prefs: []
  type: TYPE_IMG
- en: An object detection YOLO example with a 3x3 cell grid and 2 objects
  prefs: []
  type: TYPE_NORMAL
- en: 'Тhe network will output multiple possible detected objects for each grid cell.
    For example, if the grid is 3×3, then the output will contain 9 possible detected
    objects. For the sake of clarity, let''s discuss the output data (and its corresponding
    label) for a single grid cell/detected object. It is an array with values, *[b[x],
    b[y], b[h], b[w], p[c], c[1], c[2], ..., c[n]]*, where the values are as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*b[x], b[y], b[h], b[w]* describes the object bounding box, if an object exists,
    then *b[x]* and *b[y]* are the coordinates of the center of the box. They are
    normalized in the [0, 1] range with respect to the size of the image. That is,
    if the image is of size 100 x 100 and *b[x] = 20* and *b[y] = 50*, their normalized
    values will be 0.2 and 0.5\. Basically, *b[h]* and *b[w]* represent the box height
    and width. They are normalized with respect to the grid cell. If the bounding
    box is larger than the cell, its value will be greater than 1\. Predicting the
    box parameters is a regression task.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p[c]* is a confidence score in the [0, 1] range. The labels for the confidence
    score are either 0 (not present) or 1 (present), making this part of the output
    a classification task. If an object is not present, we can discard the rest of
    the array values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '*c[1], c[2], ..., c[n]* is a one-hot encoding of the object class. For example,
    if we have car, person, tree, cat, and dog classes, and the current object is
    of the cat type, its encoding will be *[0, 0, 0, 1, 0]*. If we have *n* possible
    classes, the size of the output array for one cell will be *5 + n* (9 in our example).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The network output/labels will contain *S×S* such arrays. For example, the length
    of the YOLO output for a 3×3 cell grid and four classes would be *3*3*9 = 81*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's address the scenario with multiple objects in the same cell. Thankfully,
    YOLO proposes an elegant solution to this problem. We'll have multiple candidate
    boxes (known as **anchor boxes** or priors) with a slightly different shape associated
    with each cell. In the following diagram, we can see the grid cell (the square,
    uninterrupted line) and two anchor boxes—vertical and horizontal (the dashed lines).
    If we have multiple objects in the same cell, we'll associate each object with
    one of the anchor boxes. Conversely, if an anchor box doesn't have an associated
    object, it will have a confidence score of zero. This arrangement will also change
    the network output. We'll have multiple output arrays per grid cell (one output
    array per anchor box). To extend our previous example, let's assume we have a
    *3×3* cell grid with 4 classes and 2 anchor boxes per cell. Then, we'll have *3*3*2
    = 18* output bounding boxes and a total output length of *3*3*2*9 = 162*. Because we
    have a fixed number of cells (*S×S*) and a fixed number of anchor boxes per cell,
    the size of the network output doesn't change with the number of detected objects.
    Instead, the output will indicate whether an object is present in all possible
    anchor boxes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the following diagram, we can see a grid cell with two anchor boxes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d15b17d8-2f69-4320-ac15-753e15fcf584.png)'
  prefs: []
  type: TYPE_IMG
- en: Grid cell (the square, uninterrupted line) with two anchor boxes (the dashed
    lines)
  prefs: []
  type: TYPE_NORMAL
- en: 'The only question now is how to choose the proper anchor box for an object
    during training (during inference, the network will choose by itself). We''ll
    do this with the help of **Intersection over Union** (**IoU**). This is just the
    ratio between the area of the intersection of the object bounding box/anchor box
    and the area of their union:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0468041c-b463-4ca2-8b08-cf907bd50aec.png)'
  prefs: []
  type: TYPE_IMG
- en: Intersection over Union
  prefs: []
  type: TYPE_NORMAL
- en: We'll compare the bounding box of each object to all anchor boxes, and assign
    the object to the anchor box with the highest IoU. Since the anchor boxes have
    varying sizes and shapes, IoU assures that the object will be assigned to the
    anchor box that most closely resembles its footprint on the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we (hopefully) know how YOLO works, we can use it for predictions.
    However, the output of the network might be noisy—that is, the output includes
    all possible anchor boxes for each cell, regardless of whether an object is present
    in them. Many of the boxes will overlap and actually predict the same object.
    We''ll get rid of the noise using **non-maximum suppression**. Here''s how it
    works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Discard all bounding boxes with a confidence score of less than or equal to 0.6.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: From the remaining bounding boxes, pick the one with the highest possible confidence
    score.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Discard any box whose IoU >= 0.5 with the box we selected in the previous step.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: If you are worried that the network output/groundtruth data will become too
    complex or large, don't be. CNNs work well with the ImageNet dataset, which has
    1,000 categories, and therefore 1,000 outputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about YOLO, check out the original sequence of papers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*You Only Look Once: Unified, Real-Time Object Detection* ([https://arxiv.org/abs/1506.02640](https://arxiv.org/abs/1506.02640))
    by Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*YOLO9000: Better, Faster, Stronger* ([https://arxiv.org/abs/1612.08242](https://arxiv.org/abs/1612.08242))
    by Joseph Redmon and Ali Farhadi'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*YOLOv3: An Incremental Improvement* ([https://arxiv.org/abs/1804.02767](https://arxiv.org/abs/1804.02767))
    by Joseph Redmon and Ali Farhadi'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we've introduced the theory of the YOLO algorithm, in the next section,
    we'll discuss how to use it in practice.
  prefs: []
  type: TYPE_NORMAL
- en: A code example of YOLOv3 with OpenCV
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll demonstrate how to use the YOLOv3 object detector with
    OpenCV. For this example, you''ll need `opencv-python` 4.1.1 or higher, and 250
    MB of disk space for the pretrained YOLO network. Let''s begin with the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Add some boilerplate code that downloads and stores several configuration and
    data files. We''ll start with the YOLOv3 network configuration `yolo_config` and
    `weights`, and we''ll use them to initialize the `net` network. We''ll use the
    YOLO author''s GitHub and personal website to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll download the names of the COCO dataset classes that the network
    can detect. We''ll also load them from the file. The dataset as presented in the
    COCO paper contains 91 categories. However, the dataset on the website contains
    only 80\. YOLO uses the 80-category version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, download a test image from Wikipedia. We''ll also load the image from
    the file in the `blob` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Feed the image to the network and do the inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterate over the classes and anchor boxes and prepare them for the next step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove the noise with non-max suppression. You can experiment with different
    values for `score_threshold` and `nms_threshold` to see how the detected objects
    change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Draw the bounding boxes and their captions on the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can display the detected objects with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If everything goes alright, this code block will produce the same image that
    we saw at the beginning of the *Introduction to object detection* section.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion about YOLO. In the next section, we'll introduce
    a two-stage object detector called Faster R-CNN (R-CNN stands for Regions with
    CNN).
  prefs: []
  type: TYPE_NORMAL
- en: Object detection with Faster R-CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll discuss a two-stage object detection algorithm called
    Faster R-CNN (*Faster R-CNN: Towards Real-Time Object Detection with Region Proposal
    Networks*, [https://arxiv.org/abs/1506.01497](https://arxiv.org/abs/1506.01497)).
    It is an evolution of the earlier two-stage detectors Fast R-CNN ([https://arxiv.org/abs/1504.08083](https://arxiv.org/abs/1504.08083))
    and R-CNN (*Rich feature hierarchies for accurate object detection and semantic
    segmentation*, [https://arxiv.org/abs/1311.2524](https://arxiv.org/abs/1311.2524)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start by outlining the general structure of Faster R-CNN, which is displayed
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7796c454-91ea-4eb6-b3e9-824fe7451d48.png)'
  prefs: []
  type: TYPE_IMG
- en: The structure of Faster R-CNN; source: https://arxiv.org/abs/1506.01497
  prefs: []
  type: TYPE_NORMAL
- en: Let's keep that figure in mind while we explain the algorithm. Like YOLO, Faster
    R-CNN starts with a backbone classification network trained on ImageNet, which serves
    as a base for the different modules of the model. The authors of the paper experimented
    with VGG16 and ZF net (*Visualizing and Understanding Convolutional Networks*, [https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf](https://cs.nyu.edu/~fergus/papers/zeilerECCV2014.pdf))
    backbones. However, recent implementations use more contemporary architectures
    such as ResNets. The backbone net serves as a backbone (get it?) to the two other
    components of the model—the **region proposal network** (**RPN**) and the detection
    network. In the next section, we'll discuss the RPN.
  prefs: []
  type: TYPE_NORMAL
- en: Region proposal network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the first stage, the RPN takes an image (of any size) as input and will output a
    set of rectangular regions of interest (RoIs), where an object might be located.
    The RPN itself is created by taking the first *p* (13 in the case of VGG and 5
    for ZF net) convolutional layers of the backbone model (see the preceding diagram).
    Once the input image is propagated to the last shared convolutional layer, the
    algorithm takes the feature map of that layer and slides another small net over
    each location of the feature map. The small net outputs whether an object is present at
    any of the *k* anchor boxes over each location (the concept of anchor box is the
    same as in YOLO). This concept is illustrated on the left-hand side image of the
    following diagram, which shows a single location of the RPN sliding over a single
    feature map of the last convolutional layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e60633c-846c-4c74-9a36-ff7c0633a697.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: RPN proposals over a single location; Right: example detections using
    RPN proposals (the labels are artificially enhanced). Source: https://arxiv.org/abs/1506.01497'
  prefs: []
  type: TYPE_NORMAL
- en: 'The small net is fully connected to an *n×n* region at the same location over
    all input feature maps (*n = 3* according to the paper).  For example, if the
    final convolutional layer has 512 feature maps, then the small net input size
    at one location is 512 x 3 x 3 = 4,608\. Each sliding window is mapped to a lower
    dimensional (512 for VGG and 256 for ZF net) vector. This vector itself serves
    as input to the following two parallel fully connected layers:'
  prefs: []
  type: TYPE_NORMAL
- en: A classification layer with *2k* units organized into *k* 2-unit binary softmax
    outputs. The output of each softmax represents a confidence score of whether an
    object is located in each of the *k* anchor boxes. The paper refers to the confidence
    score as **objectness**, which measures whether the anchor box content belongs
    to a set of objects versus background. During training, an object is assigned
    to an anchor box based on the IoU formula in the same way as in YOLO.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A regression layer with *4k* units organized into *k* 4-unit RoI coordinates.
    2 of the 4 units represent the coordinates of the RoI center in the [0:1] range
    relative to the whole image. The other two coordinates represent the height and
    width of the region, relative to the whole image (again, similar to YOLO).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The authors of the paper experimented with three scales and three aspect ratios,
    resulting in nine possible anchor boxes over each location. The typical H×W size
    of the final feature map is around 2,400, which results in 2,400*9 = 21,600 anchor
    boxes.
  prefs: []
  type: TYPE_NORMAL
- en: In theory, we slide the small net over the feature map of the last convolutional
    layer. However, the small net weights are shared along all locations. Because
    of this, the sliding can be implemented as a cross-channel convolution. Therefore,
    the network can produce output for all anchor boxes in a single image pass. This
    is an improvement over Fast R-CNN, which requires a separate network pass for
    each anchor box.
  prefs: []
  type: TYPE_NORMAL
- en: 'The RPN is trained with backpropagation and stochastic gradient descent (what
    a surprise!). The shared convolutional layers are initialized with the weights
    of the backbone net and the rest are initialized randomly. The samples of each
    mini-batch are extracted from a single image, which contains many positive (objects)
    and negative (background) anchor boxes. The sampling ratio between the two types is
    1:1. Each anchor is assigned a binary class label (of being an object or not).
    There are two kinds of anchors with positive labels: the anchor/anchors with the
    highest IoU overlap with a groundtruth box or an anchor that has an IoU overlap
    of higher than 0.7 with any groundtruth box. If the IoU ratio of an anchor is
    lower than 0.3, the box is assigned a negative label. Anchors that are neither
    positive nor negative do not participate in the training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As the RPN has two output layers (classification and regression), the training
    uses the following composite cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9b2d900-ba01-43b4-9ece-37e05fb81db0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s discuss it in detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '*i* is the index of the anchor in the mini-batch.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p[i]* is the classification output, which represents the predicted probability
    of anchor *i* being an object. Note *p[i]^** is the target data for the same (0
    or 1).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*t[i]* is the regression output vector of size 4, which represents the RoI
    parameters. As in YOLO, the  *t[i]^** is the target vector for the same.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L[cls]* is a cross-entropy loss for the classification layer. *N[cls]* is
    a normalization term equal to the mini-batch size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*L[reg]* is the regression loss. ![](img/2e31c521-9a23-4e85-a1a2-cdd4d558d832.png),
    where R is the mean absolute error (see the *Cost functions *section in [Chapter
    1](b94f711b-daab-4de7-97b7-b7efccd0b392.xhtml), *The Nuts and Bolts of Neural
    Networks*). *N[reg]* is a normalization term equal to the total number of anchor
    locations (around 2,400).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the classification and regression components of the cost function are
    combined with the help of the *λ* parameter*.* Since *N[reg]* ~ 2400 and *N[cls]* =
    256, *λ* is set to 10 to preserve the balance between the two losses.
  prefs: []
  type: TYPE_NORMAL
- en: Detection network
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've discussed the RPN, let's focus on the detection network. To do
    this, we'll go back to the diagram *The structure of Faster R-CNN* at the beginning
    of the *Object detection with Faster R-CNN* section. Let's recall that in the
    first stage, the RPN already generated the RoI coordinates. The detection network
    is a regular classifier, which determines the type of object (or background) in
    the current RoI. Both the RPN and the detection net share their first convolutional
    layers, borrowed from the backbone net. But the detection net also incorporates
    the proposed regions from the RPN, along with the feature maps of the last shared
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'But how do we combine the inputs? We can do this with the help of **Region
    of Interest** (**RoI**) max pooling, which is the first layer of the second part
    of the detection network. An example of this operation is displayed in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f742144-c9af-41a6-9594-7b195e81742c.png)'
  prefs: []
  type: TYPE_IMG
- en: An example of *2×2* RoI max pooling with a 10×7 feature map and a 5×5 region
    of interest (blue rectangle)
  prefs: []
  type: TYPE_NORMAL
- en: For the sake of simplicity, we'll assume that we have a single *10×7* feature
    map and a single RoI. As we learned in the *Region proposal network* section,
    the RoI is defined by its coordinates, width, and height. The operation converts
    these parameters to actual coordinates on the feature map. In this example, the
    region size is *h×w = 5×5*. The RoI max pooling is further defined by its output
    height, *H*, and width, *W*. In this example, *H×W = 2×2*, but in practice the
    values could be larger, such as 7×7\. The operation splits the *h×w* RoI into
    a grid with (*h / H)×(w / W)* subregions.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the example, the subregions might have different sizes. Once
    this is done, each subregion is downsampled to a single output cell by taking
    the maximum value of that region. In other words, RoI pooling can transform inputs
    with arbitrary sizes into a fixed-size output window. In this way, the transformed
    data can propagate through the network in a consistent format.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned in the *Object detection with Faster R-CNN* section, the RPN
    and the detection network share their initial layers. However, they start their
    life as separate networks. The training alternates between the two in a four-step
    process:'
  prefs: []
  type: TYPE_NORMAL
- en: Train the RPN, which is initialized with the ImageNet weights of the backbone
    net.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the detection network, using the proposals from the freshly trained RPN
    from *step 1*. The training also starts with the weights of the ImageNet backbone
    net. At this point, the two networks don't share weights.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the detection net shared layers to initialize the weights of the RPN. Then,
    train the RPN again, but freeze the shared layers and fine-tune the RPN-specific
    layers only. The two networks share weights now.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the detection net by freezing the shared layers and fine-tuning the detection-net-specific
    layers only.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we've introduced Faster R-CNN, in the next section, we'll discuss how
    to use it in practice with the help of a pretrained PyTorch model.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Faster R-CNN with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll use a pretrained PyTorch Faster R-CNN with a ResNet50
    backbone for object detection. This example requires PyTorch 1.3.1, `torchvision`
    0.4.2, and `python-opencv` 4.1.1:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll start with the imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll continue with downloading the input image, and we'll define the
    class names in the COCO dataset. This step is the same as the one we implemented
    in the *A code example of YOLOv3 with OpenCV* section. The path to the download
    image is stored in the  `image_file = 'source_2.png'` variable, and the class
    names are stored in the `classes` list. This implementation uses the full 91 COCO
    categories.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We''ll load the pretrained Faster R-CNN model, and we''ll set it to evaluation
    mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we''ll read the image file with OpenCV:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll define the PyTorch `transform`  sequence, we''ll transform the image
    to a PyTorch compatible tensor, and we''ll feed it to the net. The network output
    is stored in the `output` variable. As we discussed in the *Region Proposal Network* section, `output`
    contains three components: `boxes` for the bounding box parameters, `classes`
    for the object class, and `scores` for confidence scores. The model applies NMS
    internally, and there is no need to do it in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we continue with displaying the detected objects, we''ll define a set
    of random colors for each class of the COCO dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We iterate over each bounding box and draw it on the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Drawing the bounding boxes involves the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Filter the boxes with a confidence score of less than 0.5 to prevent noisy detections.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The bounding `box` parameters (extracted from `output['boxes']`) contain the
    top-left and bottom-right absolute (pixel) coordinates of the bounding box on
    the image. They are only transformed in tuples to fit the OpenCV format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extract the class name and the color for the bounding box.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Draw the bounding box and the class name.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we can display the detection result with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will produce the following result (the passengers on the bus are
    also detected):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/11a81f52-51d6-4535-9387-97d7612aa4a8.png)'
  prefs: []
  type: TYPE_IMG
- en: Faster R-CNN object detection
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the section about object detection. To summarize, we discussed
    two of the most popular detection models—YOLO and Faster R-CNN. In the next section,
    we'll talk about image segmentation—you can think of it as classification on the
    pixel level.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing image segmentation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Image segmentation is the process of assigning a class label (such as person,
    car, or tree) to each pixel of an image. You can think of it as classification,
    but on a pixel level—instead of classifying the entire image under one label,
    we''ll classify each pixel separately. There are two types of segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Semantic segmentation**: This assigns a class to each pixel, but doesn''t
    differentiate between object instances. For example, the middle image in the following
    screenshot shows a semantic segmentation mask, where the pixels of each vehicle
    have the same value. Semantic segmentation can tell us that a pixel is part of
    a vehicle, but cannot make a distinction between two vehicles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Instance segmentation**: This assigns a class to each pixel and differentiates
    between object instances. For example, the image on the right in the following
    screenshot shows an instance segmentation mask, where each vehicle is segmented
    as a separate object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of semantic and instance segmentation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6dd1e561-e943-41ad-a682-4edee1fede0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Left: input image; middle: semantic segmentation; right: instance segmentation;
    source: http://sceneparsing.csail.mit.edu/'
  prefs: []
  type: TYPE_NORMAL
- en: To train a segmentation algorithm, we'll need a special type of groundtruth
    data, where the labels of each image are the segmented version of the image.
  prefs: []
  type: TYPE_NORMAL
- en: The easiest way to segment an image is by using the familiar sliding-window
    technique, which we described in the *Approaches to object detection* section.
    That is, we'll use a regular classifier and we'll slide it in either direction
    with stride 1\. After we get the prediction for a location, we'll take the pixel
    that lies in the middle of the input region and we'll assign it with the predicted
    class. Predictably, this approach is very slow because of the large number of
    pixels in an image (even a 1024×1024 image has more than 1 million pixels). Thankfully,
    there are faster and more accurate algorithms, which we'll discuss in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation with U-Net
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first approach to segmentation we''ll discuss is called U-Net (*U-Net:
    Convolutional Networks for Biomedical Image Segmentation*, [https://arxiv.org/abs/1505.04597](https://arxiv.org/abs/1505.04597)).
    The name comes from the visualization of the network architecture. U-Net is a
    type of **fully convolutional network** (**FCN**), called so because it contains
    only convolutional layers and doesn''t have any fully connected layers. An FCN
    takes the whole image as input, and outputs its segmentation map in a single pass. We
    can separate an FCN into two virtual components (in reality, this is just a single
    network):'
  prefs: []
  type: TYPE_NORMAL
- en: The encoder is the first part of the network. It is similar to a regular CNN,
    without the fully connected layers at the end. The role of the encoder is to learn
    highly abstract representations of the input image (nothing new here).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The decoder is the second part of the network. It starts after the encoder and
    uses it as input. The role of the decoder is to translate these abstract representations
    into the segmented groundtruth data. To do this, the decoder uses the opposite
    of the encoder operations. This includes transposed convolutions (the opposite
    of convolutions) and unpooling (the opposite of pooling).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With that introduction, here is U-Net in all its glory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e64e443f-6ab9-4879-8f65-4640edd938c9.png)'
  prefs: []
  type: TYPE_IMG
- en: The U-Net architecture; source: https://arxiv.org/abs/1505.04597
  prefs: []
  type: TYPE_NORMAL
- en: Each blue box corresponds to a multichannel feature map. The number of channels
    is denoted on top of the box, and the feature map size is at the lower-left edge
    of the box. White boxes represent copied feature maps. The arrows denote the different
    operations (displayed on the legend as well). The left part of the *U* is the
    encoder, and the right part is the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s segment (get it?) the U-Net modules:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Encoder**: the network takes as input a 572×572 RGB image. From there, it
    continues like a regular CNN with alternating convolutional and max pooling layers.
    The encoder consists of four blocks of the following layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two consecutive cross-channel unpadded 3×3 convolutions with stride 1.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A 2×2 max pooling layer.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: ReLU activations.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each downsampling step doubles the number of feature maps.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The final encoder convolution ends with 1,024 28×28 feature maps.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Decoder**: This is symmetrical to the encoder. The decoder takes the innermost
    28×28 feature maps and simultaneously upsamples and converts them to a 388×388
    segmentation map. It contains four upsampling blocks:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The upsampling works with 2×2 transposed convolutions with stride 2 ([Chapter
    2](d94e220f-820e-40da-8bb5-9593e0790b21.xhtml)*, Understanding Convolutional Networks*),
    denoted by green vertical arrows.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of each upsampling step is concatenated with the cropped high-resolution
    feature maps of the corresponding encoder step (grey horizontal arrows). The cropping
    is necessary because of the loss of border pixels in every convolution.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Each transposed convolution is followed by two regular convolutions to smooth
    the expanded image.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The upsampling steps halve the number of feature maps. The final output uses
    a 1×1 bottleneck convolution to map the 64-component feature map tensor to the
    desired number of classes. The authors of the paper have demonstrated the binary
    segmentation of medical images of cells.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The network output is a softmax over each pixel. That is, the output contains
    as many independent softmax operations as the number of pixels. The softmax output
    for one pixel determines the pixel class. The U-Net is trained like a regular
    classification network. However, the cost function is a combination of the cross-entropy
    losses of the softmax outputs over all pixels.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that because of the valid (unpadded) convolutions of the network,
    the output segmentation map is smaller than the input image (388 versus 572).
    However, the output map is not a rescaled version of the input image. Instead,
    it has a one-to-one scale compared to the input, but only covers the central part
    of the input tile.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c8e4167-93be-4ae8-9044-c7872536d575.png)'
  prefs: []
  type: TYPE_IMG
- en: An overlap-tile strategy for segmenting large images; source: https://arxiv.org/abs/1505.04597
  prefs: []
  type: TYPE_NORMAL
- en: The unpadded convolutions are necessary, so the network doesn't produce noisy
    artifacts at the borders of the segmentation map. This makes it possible to segment
    images with arbitrary large sizes using the so called overlap-tile strategy. The
    input image is split in overlapping input tiles, like the one on the left of the
    preceding diagram. The segmentation map of the small light area in the image on
    the right requires the large light area (one tile) on the left image as input.
  prefs: []
  type: TYPE_NORMAL
- en: The next input tile overlaps with the previous one in such a way that their
    segmentation maps cover adjacent areas of the image. To predict the pixels in
    the border region of the image, the missing context is extrapolated by mirroring
    the input image. In the next section, we'll discuss Mask R-CNN—a model, which
    extends Faster R-CNN for instance segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Instance segmentation with Mask R-CNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mask R-CNN ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870))
    is an extension of Faster R-CNN for instance segmentation. Faster R-CNN has two
    outputs for each candidate object: bounding box parameters and class labels. In
    addition to these, Mask R-CNN adds a third output—an FCN that produces a binary
    segmentation mask for each RoI. The following diagram shows the structure of Mask
    R-CNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8e225a2f-e379-4d89-8080-6c256cd70229.png)'
  prefs: []
  type: TYPE_IMG
- en: Mask R-CNN
  prefs: []
  type: TYPE_NORMAL
- en: The RPN produces anchors in five scales and three aspect ratios. The segmentation
    and classification paths both use the RoI predictions of the RPN, but otherwise
    are independent of each other. The segmentation path produces *I* *m×m* binary
    segmentation masks, one for each of the *I* classes. At training or inference,
    only the mask related to the predicted class of the classification path is considered
    and the rest are discarded. The class prediction and segmentation are parallel
    and decoupled—the classification path predicts the class of the segmented object,
    and the segmentation path determines the mask.
  prefs: []
  type: TYPE_NORMAL
- en: 'Mask R-CNN replaces the RoI max pooling operation with a more accurate RoI
    align layer. The RPN outputs the anchor box center, and its height and width as
    four floating point numbers. Then, the RoI pooling layer translates them to integer
    feature map cell coordinates (quantization). Additionally, the division of the
    RoI to *H×W* bins also involves quantization. The RoI example from the *Object
    detection with Faster R-CNN* section shows that the bins have different sizes
    (3×3, 3×2, 2×3, 2×2). These two quantization levels can introduce misalignment
    between the RoI and the extracted features. The following diagram shows how RoI
    alignment solves this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e676d19c-e656-4212-82e0-98b12eba6157.png)'
  prefs: []
  type: TYPE_IMG
- en: RoI align example; source: https://arxiv.org/abs/1703.06870
  prefs: []
  type: TYPE_NORMAL
- en: The dashed lines represent the feature map cells. The region with solid lines
    in the middle is a 2×2 RoI overlaid on the feature map. Note that it doesn't match
    the cells exactly. Instead, it is located according to the RPN prediction without
    quantization. In the same way, a cell of the RoI (the black dots) doesn't match
    one particular cell of the feature map. The RoI align operation computes the value
    of an RoI cell with a bilinear interpolation of its adjacent cells. In this way,
    RoI align is more accurate than RoI pooling.
  prefs: []
  type: TYPE_NORMAL
- en: At training, an RoI is assigned a positive label if it has IoU with a groundtruth
    box of at least 0.5, and negative otherwise. The mask target is the intersection
    between an RoI and its associated groundtruth mask. Only the positive RoIs participate
    in the segmentation path training.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Mask R-CNN with PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll use a pretrained PyTorch Mask R-CNN with a ResNet50
    backbone for instance segmentation. This example requires PyTorch 1.1.0, torchvision
    0.3.0, and OpenCV 3.4.2\. This example is very similar to the one we implemented
    in the *Implementing Faster R-CNN with PyTorch* section. Because of this, we''ll
    omit some parts of the code to avoid repetition. Let''s start:'
  prefs: []
  type: TYPE_NORMAL
- en: The imports, `classes`, and `image_file` are the same as in the Faster R-CNN
    example.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The first difference between the two examples is that we''ll load the Mask
    R-CNN pretrained model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We feed the input image to the network and obtain the `output` variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Besides `boxes`, `classes`, and `scores`, `output` contains an additional `masks` component
    for the predicted segmentation masks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We iterate over the masks and overlay them on the image. The image and the
    mask are `numpy` arrays, and we can implement the overlay as a vector operation.
    We''ll display both the bounding boxes and the segmentation masks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can display the segmentation result as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This example will produce the image on the right as follows (the original on
    the left is for comparison):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc987e79-92a8-46e3-b97d-7810f262710b.png)'
  prefs: []
  type: TYPE_IMG
- en: Mask R-CNN instance segmentation
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see that each segmentation mask is defined only within its bounding
    box, where all values of the segmentation mask are greater than zero. To obtain
    the actual pixels that belong to the object, we apply the mask only over the pixels,
    whose segmentation confidence score is greater than 0.5 (this code snippet is
    part of step 4 of the Mask R-CNN code example):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This concludes the section of the chapter devoted to image segmentation (in
    fact, it concludes the chapter itself).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed object detection and image segmentation. We started
    with the one-shot detection algorithm, YOLO, and then we continued with the two-stage
    Faster R-CNN algorithm. Next, we discussed the semantic segmentation network architecture,
    U-Net. Finally, we talked about Mask R-CNN—an extension of Faster R-CNN for instance
    segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll explore new types of ML algorithms called generative
    models. We can use them to generate new content, such as images. Stay tuned—it
    will be fun!
  prefs: []
  type: TYPE_NORMAL
