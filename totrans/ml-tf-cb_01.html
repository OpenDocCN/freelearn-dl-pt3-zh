<html><head></head><body>
  <div id="_idContainer020">
    <h1 class="chapterNumber">1</h1>
    <h1 id="_idParaDest-13" class="chapterTitle">Getting Started with TensorFlow 2.x</h1>
    <p class="normal">Google's TensorFlow engine has a unique way of solving problems, allowing us to solve machine learning problems very efficiently. Nowadays, machine learning is used in almost all areas of life and work, with famous applications in computer vision, speech recognition, language translations, healthcare, and many more. We will cover the basic steps to understand how TensorFlow operates and eventually build up to production code techniques later in the pages of this book. For the moment, the fundamentals presented in this chapter are paramount in order to provide you with a core understanding for the recipes found in the rest of this book.</p>
    <p class="normal">In this chapter, we'll start by covering some basic recipes and helping you to understand how TensorFlow 2.x works. You'll also learn how to access the data used to run the examples in this book, and how to get additional resources. By the end of this chapter, you should have knowledge of the following:</p>
    <ul>
      <li class="bullet">Understanding how TensorFlow 2.x works</li>
      <li class="bullet">Declaring and using variables and tensors</li>
      <li class="bullet">Working with matrices</li>
      <li class="bullet">Declaring operations</li>
      <li class="bullet">Implementing activation functions</li>
      <li class="bullet">Working with data sources</li>
      <li class="bullet">Finding additional resources</li>
    </ul>
    <p class="normal">Without any further ado, let's begin with the first recipe, which presents in an easy fashion the way TensorFlow deals with data and computations.</p>
    <h1 id="_idParaDest-14" class="title">How TensorFlow works</h1>
    <p class="normal">Started as an internal project<a id="_idIndexMarker000"/> by researchers and engineers from the Google Brain team, initially named <strong class="keyword">DistBelief</strong>, an open source framework for high performance numerical <a id="_idIndexMarker001"/>computations was released in November 2015 under the name TensorFlow (tensors are a generalization of scalars, vectors, matrices, and higher dimensionality matrices). You can read the original paper on the project here: <a href="http://download.tensorflow.org/paper/whitepaper2015.pdf"><span class="url">http://download.tensorflow.org/paper/whitepaper2015.pdf</span></a>. After the appearance of version 1.0 in 2017, last year, Google released TensorFlow 2.0, which continues the development and improvement of TensorFlow by making it more user-friendly and accessible.</p>
    <p class="normal">Production-oriented and capable of handling different computational architectures (CPUs, GPUs, and now TPUs), TensorFlow is a framework for any kind of computation that requires high performance and easy distribution. It excels at deep learning, making it possible to create everything from shallow networks (neural networks made of a few layers) to complex deep networks for image recognition and natural language processing.</p>
    <p class="normal">In this book, we're going to present a series of recipes that will help you use TensorFlow for your deep learning projects in a more efficient way, cutting through complexities and helping you achieve both a wider scope of applications and much better results.</p>
    <p class="normal">At first, computation in TensorFlow may seem needlessly complicated. But there is a reason for it: because of how TensorFlow deals with computation, when you become accustomed to TensorFlow style, developing more complicated algorithms becomes relatively easy. This recipe will guide us through the pseudocode of a TensorFlow algorithm.</p>
    <h2 id="_idParaDest-15" class="title">Getting ready</h2>
    <p class="normal">Currently, TensorFlow is tested and supported on the following 64-bit systems: Ubuntu 16.04 or later, macOS 10.12.6 (Sierra) or later (no GPU support, though), Raspbian 9.0 or later, and Windows 7 or later. The code for this book has been developed and tested on an Ubuntu system, but it should run fine on any other system as well. The code for the book is available on GitHub at <a href="https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook</span></a>, which acts as the book repository for all the code and some data.</p>
    <p class="normal">Throughout this book, we'll only concern ourselves with the Python library wrapper of TensorFlow, although most of the original core code for TensorFlow is written in C++. TensorFlow operates nicely with Python, ranging from version 3.7 to 3.8. This book will use Python 3.7 (you can get the plain interpreter at <a href="https://www.python.org"><span class="url">https://www.python.org</span></a>) and TensorFlow 2.2.0 (you can find all the necessary instructions to install it at <a href="https://www.tensorflow.org/install"><span class="url">https://www.tensorflow.org/install</span></a>).</p>
    <p class="normal">While TensorFlow can run on the CPU, most algorithms run faster if processed on a GPU, and it is supported on graphics cards with Nvidia Compute Capability 3.5 or higher (preferable when running complex networks that are more computationally intensive).</p>
    <div class="note">
      <p class="Information-Box--PACKT-">All the recipes you'll find in the book are compatible with TensorFlow 2.2.0. Where necessary, we'll point out the differences in syntax and execution with the previous 2.1 and 2.0 versions.</p>
    </div>
    <p class="normal">Popular GPUs for running<a id="_idIndexMarker002"/> scripts based on TensorFlow on a workstation are Nvidia Titan RTX and Nvidia Quadro RTX models, whereas in data centers, we instead commonly find Nvidia Tesla architectures with at least 24 GB of memory (for instance, Google Cloud Platform offers GPU Nvidia Tesla K80, P4, T4, P100 and V100 models). To run properly on a GPU, you will also need to download and install the Nvidia CUDA toolkit, version 5.x+ (<a href="https://developer.nvidia.com/cuda-downloads"><span class="url">https://developer.nvidia.com/cuda-downloads</span></a>).</p>
    <p class="normal">Some of the recipes in this chapter will rely on an installation of the current versions of SciPy, NumPy, and Scikit-learn Python packages. These accompanying packages are also included in the Anaconda package (<span class="url">https://www.anaconda.com/products/individual#Downloads</span>).</p>
    <h2 id="_idParaDest-16" class="title">How to do it…</h2>
    <p class="normal">Here, we'll introduce the general flow of TensorFlow algorithms. Most recipes will follow this outline:</p>
    <ol>
      <li class="numbered"><strong class="keyword">Import or generate datasets</strong>: All of our machine learning algorithms will depend on datasets. In this book, we'll either generate data or use an outside source of datasets. Sometimes, it's better to rely on generated data because we can control how to vary and verify the expected outcome. Most of the time, we will access public datasets for the given recipe. The details on accessing these datasets can be found in the <em class="italic">Additional resources</em> recipe at the end of this chapter:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">data</span> = tfds.load(<span class="hljs-string">"iris"</span>, split=<span class="hljs-string">"train"</span>)
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Transform and normalize data</strong>: Generally, input datasets do not come in the exact form we want for what we intend to achieve. TensorFlow expects us to transform the data into the accepted shape and data type. In fact, the data is usually not in the correct dimension or type that our algorithms expect, and we will have to transform it properly before we can use it. Most algorithms also expect normalized <a id="_idIndexMarker003"/>data (which implies variables whose mean is zero and whose standard deviation is one) and we will look at how to accomplish this here as well. TensorFlow offers built-in functions that can load your data, split your data into batches, and allow you to transform variables and normalize each batch using simple NumPy functions, including the following:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> batch <span class="hljs-keyword">in</span> data.batch(batch_size, drop_remainder=<span class="hljs-literal">True</span>):
    labels = tf.one_hot(batch[<span class="hljs-string">'label'</span>], <span class="hljs-number">3</span>)
    X = batch[<span class="hljs-string">'features'</span>]
    X = (X - np.mean(X)) / np.std(X) 
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Partition the dataset into training, test, and validation sets</strong>: We generally want to test our algorithms on different sets that we have trained on. Many algorithms also require hyperparameter tuning, so we set aside a validation set for determining the best set of hyperparameters.</li>
      <li class="numbered"><strong class="keyword">Set algorithm parameters (hyperparameters)</strong>: Our algorithms usually have a set of parameters that we hold constant throughout the procedure. For example, this could be the number of iterations, the learning rate, or other fixed parameters of our choice. It's considered good practice to initialize these together using global variables, so that the reader or user can easily find them, as follows:
        <pre class="programlisting code"><code class="hljs-code">epochs = <span class="hljs-number">1000</span> 
batch_size = <span class="hljs-number">32</span>
input_size = <span class="hljs-number">4</span>
output_size = <span class="hljs-number">3</span>
learning_rate = <span class="hljs-number">0.001</span>
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Initialize variables</strong>: TensorFlow depends on knowing what it can and cannot modify. TensorFlow will modify/adjust the variables (model weights/biases) during optimization to minimize a loss function. To accomplish this, we feed in data through input variables. We need to initialize both variables and placeholders with size and type so that TensorFlow knows what to expect. TensorFlow also needs to know the type of data to expect. For most of this book, we will use <code class="Code-In-Text--PACKT-">float32</code>. TensorFlow also provides <code class="Code-In-Text--PACKT-">float64</code> and <code class="Code-In-Text--PACKT-">float16</code> data types. Note that more bytes are used for precision results in slower algorithms, but fewer bytes results in less precision of the resulting algorithm. Refer to the following code for a simple example of how to set up an array of weights and a vector of biases in TensorFlow:
        <pre class="programlisting code"><code class="hljs-code">weights = tf.Variable(tf.random.normal(shape=(input_size, 
                                              output_size), 
                                        dtype=tf.float32))
biases  = tf.Variable(tf.random.normal(shape=(output_size,), 
                                       dtype=tf.float32))
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Define the model structure</strong>: After we have the data, and have initialized our variables, we have to define the <a id="_idIndexMarker004"/>model. This is done by building a computational graph. The model for this example will be a logistic regression model (logit <em class="italic">E</em>(<em class="italic">Y</em>) = b<em class="italic">X</em> + a):
        <pre class="programlisting code"><code class="hljs-code">logits = tf.add(tf.matmul(X, weights), biases) 
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Declare the loss functions</strong>: After defining the model, we must be able to evaluate the output. This is where we declare the loss function. The loss function is very important as it tells us how far off our predictions are from the actual values. The different types of loss function are explored in greater detail in the <em class="italic">Implementing Backpropagation</em> recipe in <em class="chapterRef">Chapter 2</em>, <em class="italic">The TensorFlow Way. </em>Here, as an example, we implement the cross entropy with logits, which computes softmax cross entropy between logits and labels:
        <pre class="programlisting code"><code class="hljs-code">loss = tf.reduce_mean(
    tf.nn.softmax_cross_entropy_with_logits(labels, logits)) 
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Initialize and train the model</strong>: Now that we have everything in place, we need to create an instance of our graph, feed in the data, and let TensorFlow change the variables to predict our training data better. Here is one way to initialize the computational graph and, by means of multiple iterations, converge the weights in the model structure using the SDG optimizer:
        <pre class="programlisting code"><code class="hljs-code">optimizer = tf.optimizers.SGD(learning_rate)
<span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
   logits = tf.add(tf.matmul(X, weights), biases)
   loss = tf.reduce_mean(
      tf.nn.softmax_cross_entropy_with_logits(labels, logits))
gradients = tape.gradient(loss, [weights, biases])
optimizer.apply_gradients(zip(gradients, [weights, biases]))
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Evaluate the model</strong>: Once we've built and trained the model, we should evaluate the model by looking at how well it does with new data through some specified criteria. We evaluate on the training and test set, and these evaluations will allow us to see whether the model is under or overfitting. We will address this in later recipes. In this simple example, we evaluate the final loss and compare the fitted values against the ground truth training ones:
        <pre class="programlisting code"><code class="hljs-code">print(<span class="hljs-string">f"final loss is: </span><span class="hljs-subst">{loss.numpy():</span><span class="hljs-number">.3</span><span class="hljs-subst">f}</span><span class="hljs-string">"</span>)
preds = tf.math.argmax(tf.add(tf.matmul(X, weights), biases), axis=<span class="hljs-number">1</span>)
ground_truth = tf.math.argmax(labels, axis=<span class="hljs-number">1</span>)
<span class="hljs-keyword">for</span> y_true, y_pred <span class="hljs-keyword">in</span> zip(ground_truth.numpy(), preds.numpy()):
    print(<span class="hljs-string">f"real label: </span><span class="hljs-subst">{y_true}</span><span class="hljs-string"> fitted: </span><span class="hljs-subst">{y_pred}</span><span class="hljs-string">"</span>)
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Tune hyperparameters</strong>: Most of the time, we will want to go back and change some of the hyperparameters, checking<a id="_idIndexMarker005"/> the model's performance based on our tests. We then repeat the previous steps with different hyperparameters and evaluate the model on the validation set.</li>
      <li class="numbered"><strong class="keyword">Deploy/predict new outcomes</strong>: It is also a key requirement to know how to make predictions on new and unseen data. We can achieve this easily with TensorFlow with all of our models once we have them trained.</li>
    </ol>
    <h2 id="_idParaDest-17" class="title">How it works…</h2>
    <p class="normal">In TensorFlow, we have to set up the data, input variables, and model structure before we can tell the program to train and tune its weights to improve predictions. TensorFlow accomplishes this through computational graphs. These computational graphs are directed graphs with no recursion, which allows for computational parallelism.</p>
    <p class="normal">To do this, we need to create a loss function for TensorFlow to minimize. TensorFlow accomplishes this by modifying the variables in the computational graph. TensorFlow knows how to modify the variables because it keeps track of the computations in the model and automatically computes the variable gradients (how to change each variable) to minimize the loss. Because of this, we can see how easy it can be to make changes and try different data sources.</p>
    <h2 id="_idParaDest-18" class="title">See also</h2>
    <ul>
      <li class="bullet">For a further introduction to TensorFlow and more on its resources, refer to the official documentation <a id="_idIndexMarker006"/>and tutorials at TensorFlow's official page: <a href="https://www.tensorflow.org/"><span class="url">https://www.tensorflow.org/</span></a></li>
      <li class="bullet"><a id="_idIndexMarker007"/>Within the official pages, a more encyclopedic place to start with is the official Python API documentation, <a href="https://www.tensorflow.org/api_docs/python/"><span class="url">https://www.tensorflow.org/api_docs/python/</span></a>, where you will find all the possible commands enumerated</li>
      <li class="bullet">There are also tutorials available: <a href="https://www.tensorflow.org/tutorials/"><span class="url">https://www.tensorflow.org/tutorials/</span></a></li>
      <li class="bullet">Besides that, an unofficial collection of TensorFlow tutorials, projects, presentations, and code repositories can be found here: <a href="https://github.com/dragen1860/TensorFlow-2.x-Tutorials "><span class="url">https://github.com/dragen1860/TensorFlow-2.x-Tutorials</span></a></li>
    </ul>
    <h1 id="_idParaDest-19" class="title">Declaring variables and tensors</h1>
    <p class="normal">Tensors are the primary data structure that TensorFlow uses to operate on the computational graph. Even if now, in TensorFlow 2.x, this aspect is hidden, the data flow graph is still operating behind the scenes. This means that the logic of building a neural network doesn't change<a id="_idIndexMarker008"/> all that much between TensorFlow 1.x and TensorFlow 2.x. The most eye-catching aspect is that you no longer have to deal with placeholders, the<a id="_idIndexMarker009"/> previous entry gates for data in a TensorFlow 1.x graph.</p>
    <p class="normal">Now, you simply declare tensors as variables and proceed to building your graph.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">A <em class="italic">tensor</em> is a mathematical term that refers to generalized vectors or matrices. If vectors are one-dimensional and matrices are two-dimensional, a tensor is <em class="italic">n</em>-dimensional (where <em class="italic">n</em> could be 1, 2, or even larger).</p>
    </div>
    <p class="normal">We can declare these tensors as variables and use them for our computations. To do this, first, we must learn how to create tensors.</p>
    <h2 id="_idParaDest-20" class="title">Getting ready</h2>
    <p class="normal">When we create a tensor and declare it as a variable, TensorFlow creates several graph structures in our computation graph. It is also important to point out that just by creating a tensor, TensorFlow is not adding anything to the computational graph. TensorFlow does this only after running an operation to initialize the variables. See the next section, on variables and placeholders, for more information.</p>
    <h2 id="_idParaDest-21" class="title">How to do it…</h2>
    <p class="normal">Here, we will cover the four main ways in which we can create tensors in TensorFlow.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">We will not be unnecessarily exhaustive in this recipe or others. We will tend to illustrate only the mandatory parameters of the different API calls, unless you might find it interesting for the recipe to cover any optional parameter; when that happens, we'll justify the reasoning behind it.</p>
    </div>
    <ol>
      <li class="numbered" value="1"> Fixed size tensors:<ul>
          <li class="bullet-l2">In the following <a id="_idIndexMarker010"/>code, we create a zero-filled tensor:</li>
        </ul>
        <pre class="programlisting code"><code class="hljs-code">row_dim, col_dim = <span class="hljs-number">3</span>, <span class="hljs-number">3</span>
zero_tsr = tf.zeros(shape=[row_dim, col_dim], dtype=tf.float32) 
</code></pre>
        <ul>
          <li class="bullet-l2">In the following code, we <a id="_idIndexMarker011"/>create a one-filled tensor:</li>
        </ul>
        <pre class="programlisting code"><code class="hljs-code">ones_tsr = tf.ones([row_dim, col_dim]) 
</code></pre>
        <ul>
          <li class="bullet-l2">In the following <a id="_idIndexMarker012"/>code, we create a constant-filled tensor:</li>
        </ul>
        <pre class="programlisting code"><code class="hljs-code">filled_tsr = tf.fill([row_dim, col_dim], <span class="hljs-number">42</span>) 
</code></pre>
        <ul>
          <li class="bullet-l2">In the following code, we create a tensor out of an existing constant:</li>
        </ul>
        <pre class="programlisting code"><code class="hljs-code">constant_tsr = tf.constant([1,2,3])
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">Note that the <code class="Code-In-Text--PACKT-">tf.constant()</code> function can be used to broadcast a value into an array, mimicking the behavior of <code class="Code-In-Text--PACKT-">tf.fill()</code> by writing <code class="Code-In-Text--PACKT-">tf.constant(42, [row_dim, col_dim])</code>.</p>
        </div>
      </li>
      <li class="numbered"><strong class="keyword">Tensors of similar shape</strong>: We can<a id="_idIndexMarker013"/> also initialize variables based on the shape of other tensors, as follows:
        <pre class="programlisting code"><code class="hljs-code">zeros_similar = tf.zeros_like(constant_tsr) 
ones_similar = tf.ones_like(constant_tsr) 
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">Note that since these tensors depend on prior tensors, we must initialize them in order. Attempting to initialize the tensors in a random order will result in an error. </p>
        </div>
      </li>
      <li class="numbered"><strong class="keyword">Sequence tensors</strong>: In TensorFlow, all <a id="_idIndexMarker014"/>parameters are documented as tensors. Even when scalars are required, the API mentions these as zero-dimensional scalars. It won't therefore be a surprise that TensorFlow allows us to specify tensors that contain defined intervals. The following functions behave very similarly to NumPy's <code class="Code-In-Text--PACKT-">linspace()</code> outputs and <code class="Code-In-Text--PACKT-">range()</code> outputs (for reference: <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html"><span class="url">https://docs.scipy.org/doc/numpy/reference/generated/numpy.linspace.html</span></a>). See the following function:
        <pre class="programlisting code"><code class="hljs-code">linear_tsr = tf.linspace(start=<span class="hljs-number">0.0</span>, stop=<span class="hljs-number">1.0</span>, num=<span class="hljs-number">3</span>)
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">Note that the start and stop parameters should be float values, and that <code class="Code-In-Text--PACKT-">num</code> should be an integer. </p>
        </div>
        <p class="bullet-para">The resultant tensor has a sequence of [0.0, 0.5, 1.0] (the <code class="Code-In-Text--PACKT-">print(linear_tsr</code> command will provide the necessary output). Note that this function includes the specified stop value. See the following <code class="Code-In-Text--PACKT-">tf.range</code> function for comparison:</p>
        <pre class="programlisting code"><code class="hljs-code">integer_seq_tsr = tf.range(start=6, limit=15, delta=3) 
</code></pre>
        <p class="bullet-para">The result is the sequence [6, 9, 12]. Note that this function does not include the limit value and it can <a id="_idIndexMarker015"/>operate with both integer and float values for the start and limit parameters.</p>
      </li>
      <li class="numbered"><strong class="keyword">Random tensors</strong>: The<a id="_idIndexMarker016"/> following generated random numbers are from a <a id="_idIndexMarker017"/>uniform distribution:
        <pre class="programlisting code"><code class="hljs-code">randunif_tsr = tf.random.uniform([row_dim, col_dim], 
                                 minval=<span class="hljs-number">0</span>, maxval=<span class="hljs-number">1</span>) 
</code></pre>
      </li>
    </ol>
    <p class="normal">Note that this random uniform distribution draws from the interval that includes <code class="Code-In-Text--PACKT-">minval</code> but not <code class="Code-In-Text--PACKT-">maxval</code> (<code class="Code-In-Text--PACKT-">minval &lt;= x &lt; maxval</code>). Therefore, in this case, the output range is [0, 1). If, instead, you need to draw only integers and not floats, just add the <code class="Code-In-Text--PACKT-">dtype=tf.int32</code> parameter when calling the function.</p>
    <p class="normal">To get a tensor with random draws from a normal distribution, you can run the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">randnorm_tsr = tf.random.normal([row_dim, col_dim], 
                                 mean=<span class="hljs-number">0.0</span>, stddev=<span class="hljs-number">1.0</span>) 
</code></pre>
    <p class="normal">There are also times where we want to generate normal random values that are assured within certain bounds. The <code class="Code-In-Text--PACKT-">truncated_normal()</code> function always picks normal values within two standard deviations of the specified mean:</p>
    <pre class="programlisting code"><code class="hljs-code">runcnorm_tsr = tf.random.truncated_normal([row_dim, col_dim], 
                                          mean=<span class="hljs-number">0.0</span>, stddev=<span class="hljs-number">1.0</span>) 
</code></pre>
    <p class="normal">We might also be <a id="_idIndexMarker018"/>interested in randomizing entries of arrays. To accomplish this, two functions can help us: <code class="Code-In-Text--PACKT-">random.shuffle()</code>and <code class="Code-In-Text--PACKT-">image.random_crop()</code>. The following code performs this:</p>
    <pre class="programlisting code"><code class="hljs-code">shuffled_output = tf.random.shuffle(input_tensor) 
cropped_output = tf.image.random_crop(input_tensor, crop_size) 
</code></pre>
    <p class="normal">Later on in this book, we'll be interested<a id="_idIndexMarker019"/> in randomly cropping images of size (height, width, 3) where<a id="_idIndexMarker020"/> there are three-color spectrums. To fix a dimension in <code class="Code-In-Text--PACKT-">cropped_output</code>, you must give it the maximum size in that dimension:</p>
    <pre class="programlisting code"><code class="hljs-code">height, width = (<span class="hljs-number">64</span>, <span class="hljs-number">64</span>)
my_image = tf.random.uniform([height, width, <span class="hljs-number">3</span>], minval=<span class="hljs-number">0</span>,
         maxval=<span class="hljs-number">255</span>, dtype=tf.int32)
cropped_image = tf.image.random_crop(my_image, 
       [height//<span class="hljs-number">2</span>, width//<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) 
</code></pre>
    <p class="normal">This code snippet will generate random noise images that will be cropped, halving both the height and width, but the depth dimension will be untouched because you fixed its maximum value as a parameter. </p>
    <h2 id="_idParaDest-22" class="title">How it works…</h2>
    <p class="normal">Once we have decided how to create the tensors, we may also create the corresponding variables by wrapping the tensor in the <code class="Code-In-Text--PACKT-">Variable()</code> function, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">my_var = tf.Variable(tf.zeros([row_dim, col_dim])) 
</code></pre>
    <p class="normal">There's more on this in the following recipes.</p>
    <h2 id="_idParaDest-23" class="title">There's more…</h2>
    <p class="normal">We are not limited to the built-in functions: we can convert any NumPy array into a Python list, or a constant into<a id="_idIndexMarker021"/> a tensor using the <code class="Code-In-Text--PACKT-">convert_to_tensor()</code> function. Note that this function also accepts tensors as an input in case we wish to generalize a computation inside a function.</p>
    <h1 id="_idParaDest-24" class="title">Using eager execution</h1>
    <p class="normal">When developing deep and<a id="_idIndexMarker022"/> complex neural networks, you need to continuously experiment with architectures and data. This proved difficult in TensorFlow 1.0 because you always need to run your code from the beginning to end in order to check whether it worked. TensorFlow 2.x works in eager execution mode as default, which means that you develop and check your code step by step as you progress into your project. This is great news; now we just have to understand how to experiment with eager execution, so we can use this TensorFlow 2.x feature to our advantage. This recipe will provide you with the basics to get started.</p>
    <h2 id="_idParaDest-25" class="title">Getting ready</h2>
    <p class="normal">TensorFlow 1.x performed optimally because it executed its computations after compiling a static computational graph. All computations were distributed and connected into a graph as you compiled your network and that graph helped TensorFlow to execute computations, leveraging the available resources (multi-core CPUs of multiple GPUs) in the best way, and splitting operations between the resources in the most timely and efficient way. That also meant, in any case, that once you defined and compiled your graph, you could not change it at runtime but had to instantiate it from scratch, thereby incurring some extra work. </p>
    <p class="normal">In TensorFlow 2.x, you can still define your network, compile it, and run it optimally, but the team of TensorFlow developers has now favored, by default, a more experimental approach, allowing immediate evaluation of operations, thus making it easier to debug and to try network variations. This is<a id="_idIndexMarker023"/> called eager execution. Operations now return concrete values instead of pointers to parts of a computational graph to be built later. More importantly, you can now have all the functionality of the host language available while your model is executing, making it easier to write more complex and sophisticated deep learning solutions.</p>
    <h2 id="_idParaDest-26" class="title">How to do it…</h2>
    <p class="normal">You basically don't have to do anything; eager execution is the default way of operating in TensorFlow 2.x. When you<a id="_idIndexMarker024"/> import TensorFlow and start using its functions, you operate in eager execution since you can perform checks when executing:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.executing_eagerly()
<span class="hljs-literal">True</span>
</code></pre>
    <p class="normal">That's all you need to do.</p>
    <h2 id="_idParaDest-27" class="title">How it works…</h2>
    <p class="normal">Just run TensorFlow operations and the results will return immediately:</p>
    <pre class="programlisting code"><code class="hljs-code">x = [[<span class="hljs-number">2.</span>]]
m = tf.matmul(x, x)
print(<span class="hljs-string">"the result is {}"</span>.format(m))
the result is [[4.]]
</code></pre>
    <p class="normal">That's all there is to it!</p>
    <h2 id="_idParaDest-28" class="title">There's more…</h2>
    <p class="normal">As TensorFlow is now set on eager execution as default, you won't be surprised to hear that <code class="Code-In-Text--PACKT-">tf.Session</code> has been removed from the TensorFlow API. You no longer need to build a computational graph before running a computation; all you have to do now is build your network and test it along the way. This opens the road to common software best practices, such as documenting the code, using object-oriented programming when scripting your code, and organizing it into reusable self-contained modules.</p>
    <h1 id="_idParaDest-29" class="title">Working with matrices</h1>
    <p class="normal">Understanding how <a id="_idIndexMarker025"/>TensorFlow works with matrices is very important when developing the flow of data through computational graphs. In this recipe, we will cover the creation of matrices and the basic operations that can be performed on them with TensorFlow.</p>
    <p class="normal">It is worth emphasizing the importance of matrices in machine learning (and mathematics in general): machine learning algorithms are computationally expressed as matrix operations. Knowing how to perform matrix computations is a plus when working with TensorFlow, though you may not need it often; its high-end module, Keras, can deal with most of the matrix algebra stuff behind the scenes (more on Keras in <em class="chapterRef">Chapter 3,</em><em class="italic"> Keras</em>).</p>
    <p class="normal">This book does not cover the mathematical background on matrix properties and matrix algebra (linear algebra), so the unfamiliar reader is strongly encouraged to learn enough about matrices to be comfortable with matrix algebra. In the <em class="italic">See also</em> section, you can find a couple of resources to help you to revise your calculus skills or build them from scratch, and get even more out of TensorFlow.</p>
    <h2 id="_idParaDest-30" class="title">Getting ready</h2>
    <p class="normal">Many algorithms depend <a id="_idIndexMarker026"/>on matrix operations. TensorFlow gives us easy-to-use operations to perform such matrix calculations. You just need to import TensorFlow and follow this section to the end; if you're not a matrix algebra expert, please first have a look at the <em class="italic">See also</em> section of this recipe for resources to help you to get the most out of the following recipe.</p>
    <h2 id="_idParaDest-31" class="title">How to do it…</h2>
    <p class="normal">We proceed as follows:</p>
    <ol>
      <li class="numbered" value="1"><strong class="keyword">Creating matrices</strong>: We can create two-dimensional matrices from NumPy arrays or nested lists, as described in the <em class="italic">Declaring and using variables and tensors</em> recipe at the beginning of this chapter. We can use the tensor creation functions and specify a two-dimensional shape for functions such as <code class="Code-In-Text--PACKT-">zeros()</code>, <code class="Code-In-Text--PACKT-">ones()</code>, and <code class="Code-In-Text--PACKT-">truncated_normal()</code>. TensorFlow also allows us to create a diagonal matrix from a one-dimensional array or list using the <code class="Code-In-Text--PACKT-">diag()</code> function, as follows:
        <pre class="programlisting code"><code class="hljs-code">identity_matrix = tf.linalg.diag([<span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>, <span class="hljs-number">1.0</span>]) 
A = tf.random.truncated_normal([<span class="hljs-number">2</span>, <span class="hljs-number">3</span>]) 
B = tf.fill([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>], <span class="hljs-number">5.0</span>) 
C = tf.random.uniform([<span class="hljs-number">3</span>,<span class="hljs-number">2</span>]) 
D = tf.convert_to_tensor(np.array([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
                                   [<span class="hljs-number">-3.</span>, <span class="hljs-number">-7.</span>, <span class="hljs-number">-1.</span>],
                                   [<span class="hljs-number">0.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">-2.</span>]]), 
                         dtype=tf.float32) 
print(identity_matrix)
 
[[ 1.  0.  0.] 
 [ 0.  1.  0.] 
 [ 0.  0.  1.]] 
print(A) 
[[ 0.96751703  0.11397751 -0.3438891 ] 
 [-0.10132604 -0.8432678   0.29810596]] 
print(B) 
[[ 5.  5.  5.] 
 [ 5.  5.  5.]] 
print(C)
[[ 0.33184157  0.08907614] 
 [ 0.53189191  0.67605299] 
 [ 0.95889051 0.67061249]] 
</code></pre>
        <div class="packt_tip">
          <p class="Tip--PACKT-">Please note that the C tensor is created in a random way, and it will probably differ in your session from what is represented in this book. </p>
        </div>
        <pre class="programlisting code"><code class="hljs-code">print(D) 
[[ 1.  2.  3.] 
 [-3. -7. -1.] 
 [ 0.  5. -2.]] 
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Addition, subtraction, and multiplication</strong>: To add, subtract, or multiply matrices of the same dimension, TensorFlow<a id="_idIndexMarker027"/> uses the following function:
        <pre class="programlisting code"><code class="hljs-code">print(A+B) 
[[ 4.61596632  5.39771316  4.4325695 ] 
 [ 3.26702736  5.14477345  4.98265553]] 
print(B-B)
[[ 0.  0.  0.] 
 [ 0.  0.  0.]] 
print(tf.matmul(B, identity_matrix)) 
[[ 5.  5.  5.] 
 [ 5.  5.  5.]] 
</code></pre>
        <p class="bullet-para">It is important to note that the <code class="Code-In-Text--PACKT-">matmul()</code> function has arguments that specify whether or not to transpose the arguments before multiplication (the Boolean parameters, <code class="Code-In-Text--PACKT-">transpose_a</code> and <code class="Code-In-Text--PACKT-">transpose_b</code>), or whether each matrix is sparse (<code class="Code-In-Text--PACKT-">a_is_sparse</code> and <code class="Code-In-Text--PACKT-">b_is_sparse</code>).</p>
        <p class="bullet-para">If, instead, you need element-wise <a id="_idIndexMarker028"/>multiplication between two matrices of the same shape and type (this is very important or you will get an error), you just use the <code class="Code-In-Text--PACKT-">tf.multiply</code> function:</p>
        <pre class="programlisting code"><code class="hljs-code">print(tf.multiply(D, identity_matrix))
[[ 1.  0.  0.] 
 [-0. -7. -0.] 
 [ 0.  0. -2.]] 
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">Note that matrix division is not explicitly defined. While many define matrix division as multiplying by the inverse, it is fundamentally different from real-numbered division.</p>
        </div>
      </li>
      <li class="numbered"><strong class="keyword">The transpose</strong>: Transpose a matrix (flip the columns and rows) as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.transpose(C)) 
[[0.33184157 0.53189191 0.95889051]
 [0.08907614 0.67605299 0.67061249]]
</code></pre>
        <p class="bullet-para">Again, it is worth mentioning that reinitializing gives us different values than before.</p>
      </li>
      <li class="numbered"><strong class="keyword">Determinant:</strong> To calculate the determinant, use the following code:
        <pre class="programlisting code"><code class="hljs-code">print(tf.linalg.det(D))
-38.0 
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Inverse</strong>: To find the inverse of a square matrix, see the following:
        <pre class="programlisting code"><code class="hljs-code">print(tf.linalg.inv(D))
[[-0.5        -0.5        -0.5       ] 
 [ 0.15789474  0.05263158  0.21052632] 
 [ 0.39473684  0.13157895  0.02631579]] 
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">The inverse method is based on Cholesky decomposition only if the matrix is symmetric positive definite. If the matrix is not symmetric positive definite, then it is based on LU decomposition.</p>
        </div>
      </li>
      <li class="numbered"><strong class="keyword">Decompositions</strong>: For Cholesky <a id="_idIndexMarker029"/>decomposition, use the following code:
        <pre class="programlisting code"><code class="hljs-code">print(tf.linalg.cholesky(identity_matrix))
[[ 1.  0.  1.] 
 [ 0.  1.  0.] 
 [ 0.  0.  1.]] 
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Eigenvalues and eigenvectors</strong>: For eigenvalues and eigenvectors, use the following code:
        <pre class="programlisting code"><code class="hljs-code">print(tf.linalg.eigh(D))
[[-10.65907521  -0.22750691   2.88658212] 
 [  0.21749542   0.63250104  -0.74339638] 
 [  0.84526515   0.2587998    0.46749277] 
 [ -0.4880805    0.73004459   0.47834331]] 
</code></pre>
      </li>
    </ol>
    <p class="normal">Note that the <code class="Code-In-Text--PACKT-">tf.linalg.eigh()</code> function outputs two tensors: in the first, you find the <strong class="keyword">eigenvalues</strong> and, in the<a id="_idIndexMarker030"/> second tensor, you have the <strong class="keyword">eigenvectors</strong>. In mathematics, such an <a id="_idIndexMarker031"/>operation is known as the <strong class="keyword">eigendecomposition</strong> of a<a id="_idIndexMarker032"/> matrix.</p>
    <h2 id="_idParaDest-32" class="title">How it works…</h2>
    <p class="normal">TensorFlow provides all the tools for us to get started with numerical computations and adding these computations to our neural networks.</p>
    <h2 id="_idParaDest-33" class="title">See also</h2>
    <p class="normal">If you need to build your calculus skills quickly and understand more about TensorFlow operations, we suggest the following resources:</p>
    <ul>
      <li class="bullet">The free book <em class="italic">Mathematics for Machine Learning</em>, which can be found here: <a href="https://mml-book.github.io/"><span class="url">https://mml-book.github.io/</span></a>. This contains everything you need to know if you want to operate successfully with machine learning in general.</li>
      <li class="bullet">For an even more<a id="_idIndexMarker033"/> accessible source, watch the lessons about vectors and matrices from the Kahn Academy (<a href="https://www.khanacademy.org/math/precalculus"><span class="url">https://www.khanacademy.org/math/precalculus</span></a>) to get to work with the most basic data elements of a neural network. </li>
    </ul>
    <h1 id="_idParaDest-34" class="title">Declaring operations</h1>
    <p class="normal">Apart from <a id="_idIndexMarker034"/>matrix operations, there are hosts of other TensorFlow operations we must at least be aware of. This recipe will provide you with a quick and essential glance at what you really need to know.</p>
    <h2 id="_idParaDest-35" class="title">Getting ready</h2>
    <p class="normal">Besides the standard arithmetic operations, TensorFlow provides us with more operations that we should be aware of. We should acknowledge them and learn how to use them before proceeding. Again, we just import TensorFlow: </p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
</code></pre>
    <p class="normal">Now we're ready to run the code to be found in the following section.</p>
    <h2 id="_idParaDest-36" class="title">How to do it…</h2>
    <p class="normal">TensorFlow has the standard operations on tensors, that is, <code class="Code-In-Text--PACKT-">add()</code>, <code class="Code-In-Text--PACKT-">subtract()</code>, <code class="Code-In-Text--PACKT-">multiply()</code>, and <code class="Code-In-Text--PACKT-">division()</code> in its <code class="Code-In-Text--PACKT-">math</code> module. Note that all of the operations in this section will evaluate the inputs elementwise, unless specified otherwise:</p>
    <ol>
      <li class="numbered" value="1">TensorFlow provides some variations of <code class="Code-In-Text--PACKT-">division()</code> and the relevant functions.</li>
      <li class="numbered">It is worth mentioning that <code class="Code-In-Text--PACKT-">division()</code> returns the same type as the inputs. This means that it really returns the floor of the division (akin to Python 2) if the inputs are integers. To return the Python 3 version, which casts integers into floats before dividing and always returns a float, TensorFlow provides the <code class="Code-In-Text--PACKT-">truediv()</code> function, as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.math.divide(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>))
0.75 
print(tf.math.truediv(<span class="hljs-number">3</span>, <span class="hljs-number">4</span>)) 
tf.Tensor(0.75, shape=(), dtype=float64) 
</code></pre>
      </li>
      <li class="numbered">If we have floats and <a id="_idIndexMarker035"/>want integer division, we can use the <code class="Code-In-Text--PACKT-">floordiv()</code> function. Note that this will still return a float, but it will be rounded down to the nearest integer. This function is as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.math.floordiv(<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>)) 
tf.Tensor(0.0, shape=(), dtype=float32) 
</code></pre>
      </li>
      <li class="numbered">Another important function is <code class="Code-In-Text--PACKT-">mod()</code>. This function returns the remainder after division. It is as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.math.mod(<span class="hljs-number">22.0</span>, <span class="hljs-number">5.0</span>))
tf.Tensor(2.0, shape=(), dtype=float32) 
</code></pre>
      </li>
      <li class="numbered">The cross product between two tensors is achieved by the <code class="Code-In-Text--PACKT-">cross()</code> function. Remember that the cross product is only defined for two three-dimensional vectors, so it only accepts two three-dimensional tensors. The following code illustrates this use:
        <pre class="programlisting code"><code class="hljs-code">print(tf.linalg.cross([<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>], [<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>]))
tf.Tensor([0. 0. 1.], shape=(3,), dtype=float32) 
</code></pre>
      </li>
      <li class="numbered">Here's a compact list of the more common math functions. All of these functions operate elementwise:<table id="table001" class="No-Table-Style _idGenTablePara-1">
          <colgroup>
            <col/>
            <col/>
          </colgroup>
          <tbody>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="Table-Column-Heading--PACKT-">Function</p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Heading--PACKT-">Operation</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.abs()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Absolute value of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.ceil()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Ceiling function of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.cos()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Cosine function of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.exp()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Base <em class="italic">e</em> exponential of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.floor()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Floor function of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.linalg.inv()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Multiplicative inverse (1/x) of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.log()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Natural logarithm of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.maximum()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Elementwise maximum of two tensors</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.minimum()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Elementwise minimum of two tensors</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.negative()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Negative of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.pow()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">The first tensor raised to the second tensor elementwise</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.round()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Rounds one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.rsqrt()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">The reciprocal of the square root of one tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.sign()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Returns -1, 0, or 1, depending on the sign of the tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.sin()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Sine function of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.sqrt()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Square root of one input tensor</p>
              </td>
            </tr>
            <tr class="No-Table-Style">
              <td class="No-Table-Style">
                <p class="normal"><code class="Code-In-Text--PACKT-">tf.math.square()</code></p>
              </td>
              <td class="No-Table-Style">
                <p class="Table-Column-Content--PACKT-">Square of one input tensor</p>
              </td>
            </tr>
          </tbody>
        </table>
      </li>
      <li class="numbered"><strong class="keyword">Specialty mathematical functions</strong>: There are some special math functions that are often used in machine learning that <a id="_idIndexMarker036"/>are worth mentioning, and<a id="_idIndexMarker037"/> TensorFlow has built-in functions for them. Again, these functions operate elementwise, unless specified otherwise:</li>
    </ol>
    <table id="table002" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tf.math.digamma()</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Psi function, the derivative of the <code class="Code-In-Text--PACKT-">lgamma()</code> function</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tf.math.erf()</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Gaussian error function, element-wise, of one tensor</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tf.math.erfc()</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Complementary error function of one tensor</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tf.math.igamma()</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Lower regularized incomplete gamma function</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tf.math.igammac()</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Upper regularized incomplete gamma function</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tf.math.lbeta()</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Natural logarithm of the absolute value of the beta function</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tf.math.lgamma()</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Natural logarithm of the absolute value of the gamma function</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-"><code class="Code-In-Text--PACKT-">tf.math.squared_difference()</code></p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Computes the square of the differences between two tensors</p>
          </td>
        </tr>
      </tbody>
    </table>
    <h2 id="_idParaDest-37" class="title">How it works…</h2>
    <p class="normal">It is important to know which functions are available to us so that we can add them to our computational graphs. We<a id="_idIndexMarker038"/> will mainly be concerned with the preceding functions. We can also generate many different custom functions as compositions of the preceding, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Tangent function (tan(pi/4)=1) </span>
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">pi_tan</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.tan(<span class="hljs-number">3.1416</span>/x)
print(pi_tan(<span class="hljs-number">4</span>))
tf.Tensor(1.0000036, shape=(), dtype=float32) 
</code></pre>
    <p class="normal">The complex layers that constitute a deep neural network are just composed of the preceding functions, so now, thanks to this recipe, you have all the basics you need to create anything you want.</p>
    <h2 id="_idParaDest-38" class="title">There's more…</h2>
    <p class="normal">If we wish to add other operations to our graphs that are not listed here, we must create our own from the preceding functions. Here is an example of an operation that wasn't used previously that we can add to our graph. We can add a custom polynomial function, <em class="italic">3 * x^2 - x + 10</em>, using the following code:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">custom_polynomial</span><span class="hljs-function">(</span><span class="hljs-params">value</span><span class="hljs-function">):</span> 
    <span class="hljs-keyword">return</span> tf.math.subtract(<span class="hljs-number">3</span> * tf.math.square(value), value) + <span class="hljs-number">10</span>
print(custom_polynomial(<span class="hljs-number">11</span>))
tf.Tensor(362, shape=(), dtype=int32) 
</code></pre>
    <p class="normal">There's no limit to the custom functions you can create now, though I always recommend that you first <a id="_idIndexMarker039"/>consult the TensorFlow documentation. Often, you don't need to reinvent the wheel; you can find that what you need has already been coded.</p>
    <h1 id="_idParaDest-39" class="title">Implementing activation functions</h1>
    <p class="normal">Activation functions <a id="_idIndexMarker040"/>are the key for neural networks to approximate non-linear outputs and adapt to non-linear <a id="_idIndexMarker041"/>features. They introduce non-linear operations into neural networks. If we're careful as to which activation functions are selected and where we put them, they're very powerful operations that we can tell TensorFlow to fit and optimize.</p>
    <h2 id="_idParaDest-40" class="title">Getting ready</h2>
    <p class="normal">When we start to use neural networks, we'll use activation functions regularly because activation functions are an essential part of any neural network. The goal of an activation function is just to adjust weight and bias. In TensorFlow, activation functions are non-linear operations that act on tensors. They are functions that operate in a similar way to the previous mathematical operations. Activation functions serve many purposes, but the main concept is that they introduce a non-linearity into the graph while normalizing the outputs. </p>
    <h2 id="_idParaDest-41" class="title">How to do it…</h2>
    <p class="normal">The activation<a id="_idIndexMarker042"/> functions live in the <strong class="keyword">neural network</strong> (<strong class="keyword">nn</strong>) library in TensorFlow. Besides using built-in activation functions, we can also design our own using TensorFlow operations. We can import the predefined activation functions (from<code class="Code-In-Text--PACKT-"> tensorflow import nn</code>) or be explicit and write <code class="Code-In-Text--PACKT-">nn</code> in our function calls. Here, we'll choose to be explicit with each function call:</p>
    <ol>
      <li class="numbered" value="1">The rectified linear unit, known as ReLU, is the most common and basic way to introduce non-linearity into neural networks. This function is just called <code class="Code-In-Text--PACKT-">max(0,x)</code>. It is continuous, but not smooth. It appears as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.nn.relu([<span class="hljs-number">-3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">10.</span>]))
tf.Tensor([ 0.  3. 10.], shape=(3,), dtype=float32) 
</code></pre>
      </li>
      <li class="numbered">There are times where we'll want to cap the linearly increasing part of the preceding ReLU activation function. We can do this by nesting the <code class="Code-In-Text--PACKT-">max(0,x)</code> function in a <code class="Code-In-Text--PACKT-">min()</code> function. The implementation that TensorFlow has is called the ReLU6 function. This is <a id="_idIndexMarker043"/>defined as <code class="Code-In-Text--PACKT-">min(max(0,x),6)</code>. This is a version of the hard-sigmoid function, is computationally faster, and does not suffer from vanishing (infinitesimally near zero) or exploding values. This will come in handy when we discuss<a id="_idIndexMarker044"/> deeper neural networks in later chapters on convolutional neural networks and recurrent ones. It appears as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.nn.relu6([<span class="hljs-number">-3.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">10.</span>]))
tf.Tensor([ 0.  3. 6.], shape=(3,), dtype=float32)
</code></pre>
      </li>
      <li class="numbered">The sigmoid function is the most common continuous and smooth activation function. It is also called a logistic function and has the form <em class="italic">1 / (1 + exp(-x))</em>. The sigmoid function is not used very often because of its tendency to zero-out the backpropagation terms during training. It appears as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.nn.sigmoid([<span class="hljs-number">-1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>]))
tf.Tensor([0.26894143 0.5 0.7310586 ], shape=(3,), dtype=float32) 
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">We should be aware that some activation functions, such as the sigmoid, are not zero-centered. This will require us to zero-mean data prior to using it in most computational graph algorithms.</p>
        </div>
      </li>
      <li class="numbered">Another smooth activation function is the hyper tangent. The hyper tangent function is very similar to the sigmoid except that instead of having a range between 0 and 1, it has a range between -1 and 1. This function has the form of the ratio of the hyperbolic sine over the hyperbolic cosine. Another way to write this is as follows:
        <pre class="programlisting code"><code class="hljs-code">((<span class="hljs-builtin-name">exp</span>(<span class="hljs-name">x</span>) – exp(<span class="hljs-name">-x</span>))/(<span class="hljs-builtin-name">exp</span>(<span class="hljs-name">x</span>) + exp(<span class="hljs-name">-x</span>)) 
</code></pre>
        <p class="bullet-para">This activation function is as follows:</p>
        <pre class="programlisting code"><code class="hljs-code">print(tf.nn.tanh([<span class="hljs-number">-1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>]))
tf.Tensor([-0.7615942  0. 0.7615942], shape=(3,), dtype=float32) 
</code></pre>
      </li>
      <li class="numbered">The <code class="Code-In-Text--PACKT-">softsign</code> function is also used as an activation function. The form of this function is <em class="italic">x/(|x| + 1)</em>. The <code class="Code-In-Text--PACKT-">softsign</code> function is supposed to be a continuous (but not smooth) approximation to the sign function. See the following code:
        <pre class="programlisting code"><code class="hljs-code">print(tf.nn.softsign([<span class="hljs-number">-1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">-1.</span>]))
tf.Tensor([-0.5  0.  -0.5], shape=(3,), dtype=float32) 
</code></pre>
      </li>
      <li class="numbered">Another function, the <code class="Code-In-Text--PACKT-">softplus</code> function, is a smooth version of the ReLU function. The form of this<a id="_idIndexMarker045"/> function is <em class="italic">log(exp(x) + 1)</em>. It appears as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.nn.softplus([<span class="hljs-number">-1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">-1.</span>]))
tf.Tensor([0.31326166 0.6931472  0.31326166], shape=(3,), dtype=float32) 
</code></pre>
        <div class="note">
          <p class="Information-Box--PACKT-">The <code class="Code-In-Text--PACKT-">softplus</code> function goes to infinity as the input increases, whereas the <code class="Code-In-Text--PACKT-">softsign</code> function goes to 1. As the input gets smaller, however, the <code class="Code-In-Text--PACKT-">softplus</code> function approaches zero and the <code class="Code-In-Text--PACKT-">softsign</code> function goes to -1.</p>
        </div>
      </li>
      <li class="numbered">The <strong class="keyword">Exponential Linear Unit</strong> (<strong class="keyword">ELU</strong>) is very similar to the softplus function except that the bottom <a id="_idIndexMarker046"/>asymptote is -1 instead of 0. The form is <em class="italic">(exp(x) + 1)</em> if <em class="italic">x &lt; 0,</em> else <em class="italic">x</em>. It appears as follows:
        <pre class="programlisting code"><code class="hljs-code">print(tf.nn.elu([<span class="hljs-number">-1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">-1.</span>])) 
tf.Tensor([-0.63212055  0. -0.63212055], shape=(3,), dtype=float32) 
</code></pre>
      </li>
      <li class="numbered">Now, from this recipe, you should understand the basic key activations. Our list of the existing activation functions is not exhaustive, and you may discover that for certain problems, you need to try some of the lesser known among them. Apart from the activations from this recipe, you can find even more activations on the Keras<a id="_idIndexMarker047"/> activation pages: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/activations"><span class="url">https://www.tensorflow.org/api_docs/python/tf/keras/activations</span></a></li>
    </ol>
    <h2 id="_idParaDest-42" class="title">How it works…</h2>
    <p class="normal">These activation functions are ways that we can introduce non-linearity in neural networks or other computational graphs in the future. It is important to note where in our network we are using activation functions. If the activation function has a range between 0 and 1 (sigmoid), then the computational graph can only output values between 0 and 1. If the activation functions are inside and hidden between nodes, then we want to be aware of the effect that the range can have on our tensors as we pass them through. If our tensors were scaled to have a mean of zero, we will want to use an activation function that preserves as much variance as possible around zero. </p>
    <p class="normal">This would imply that we want to<a id="_idIndexMarker048"/> choose an activation function such as the <strong class="keyword">hyperbolic tangent</strong> (<strong class="keyword">tanh</strong>) or the <strong class="keyword">softsign</strong>. If the tensors <a id="_idIndexMarker049"/>were all scaled to be positive, then <a id="_idIndexMarker050"/>we would ideally choose an activation function that preserves variance in the positive domain.</p>
    <h2 id="_idParaDest-43" class="title">There's more…</h2>
    <p class="normal">We can even easily create custom activations such as the Swish, which is <em class="italic">x</em>*sigmoid(<em class="italic">x</em>) (see <em class="italic">Swish: a Self-Gated Activation Function</em>, Ramachandran et al., 2017, <a href="https://arxiv.org/abs/1710.05941"><span class="url">https://arxiv.org/abs/1710.05941</span></a>), which can be used as a more performing replacement for ReLU activations in image and tabular data problems:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">swish</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> x * tf.nn.sigmoid(x)
print(swish([<span class="hljs-number">-1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>]))
tf.Tensor([-0.26894143  0.  0.7310586 ], shape=(3,), dtype=float32)
</code></pre>
    <p class="normal">After having tried the activations proposed by TensorFlow, your next natural step will be to replicate the ones you find on deep learning papers or that you create by yourself.</p>
    <h1 id="_idParaDest-44" class="title">Working with data sources</h1>
    <p class="normal">For most of this book, we<a id="_idIndexMarker051"/> will rely on the use of datasets to fit machine learning algorithms. This section has instructions on how to access each of these datasets through TensorFlow and Python.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Some of the data sources rely on the maintenance of outside websites so that you can access the data. If these websites change or remove this data, then some of the following code in this section may need to be updated. You can find the updated code on this book's GitHub page:</p>
      <p class="Information-Box--PACKT-"><a href="https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook</span></a></p>
    </div>
    <h2 id="_idParaDest-45" class="title">Getting ready</h2>
    <p class="normal">Throughout the book, the majority of the datasets that we will be using are accessible using TensorFlow Datasets, whereas some others will require some extra effort by using a Python script to download, or by manually downloading them through the internet.</p>
    <p class="normal"><strong class="keyword">TensorFlow Datasets</strong> (<strong class="keyword">TFDS</strong>) is a collection of <a id="_idIndexMarker052"/>datasets ready to use (you can find the complete list here: <a href="https://www.tensorflow.org/datasets/catalog/overview"><span class="url">https://www.tensorflow.org/datasets/catalog/overview</span></a>). It automatically handles downloading and preparation of the data and, being a wrapper around <code class="Code-In-Text--PACKT-">tf.data</code>, constructs efficient and fast data pipelines.</p>
    <p class="normal">In order to install TFDS, just run the following installation command on your console:</p>
    <pre class="programlisting con"><code class="hljs-con">pip install tensorflow-datasets
</code></pre>
    <p class="normal">We can now move on to<a id="_idIndexMarker053"/> explore the core datasets that you will be using in this book (not all of these datasets are included here, just the most common ones. Some other very specific datasets will be introduced in different chapters throughout the book).</p>
    <h2 id="_idParaDest-46" class="title">How to do it…</h2>
    <ol>
      <li class="numbered" value="1"><strong class="keyword">Iris data</strong>: This dataset is<a id="_idIndexMarker054"/> arguably the classic structured dataset used in machine learning and perhaps in all examples of statistics. It is a dataset that measures sepal length, sepal width, petal length, and petal width of three different types of iris flowers: <em class="italic">Iris setosa</em>, <em class="italic">Iris virginica,</em> and <em class="italic">Iris versicolor</em>. There are 150 measurements in total, which means that there are 50 measurements for each species. To load the dataset in Python, we will use TFDS functions, as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
iris = tfds.load(<span class="hljs-string">'iris'</span>, split=<span class="hljs-string">'train'</span>)
</code></pre>
        <div class="packt_tip">
          <p class="Tip--PACKT-">When you are importing a dataset for the first time, a bar will point out where you are as you download the dataset. If you prefer, you can deactivate it if you type the following:</p>
          <p class="Tip-Within-Bullet--PACKT-"><code class="Code-In-Text--PACKT-">tfds.disable_progress_bar()</code></p>
        </div>
      </li>
      <li class="numbered"><strong class="keyword">Birth weight data</strong>: This data<a id="_idIndexMarker055"/> was originally from Baystate Medical Center, Springfield, Mass, 1986. This dataset contains measurements including childbirth weight and other demographic and medical measurements of the mother and the family history. There are 189 observations of eleven variables. The following code<a id="_idIndexMarker056"/> shows you how you can access this data as <code class="Code-In-Text--PACKT-">tf.data.dataset</code>:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
birthdata_url = <span class="hljs-string">'https://raw.githubusercontent.com/PacktPublishing/TensorFlow-2-Machine-Learning-Cookbook-Third-Edition/master/birthweight.dat'</span> 
path = tf.keras.utils.get_file(birthdata_url.split(<span class="hljs-string">"/"</span>)[<span class="hljs-number">-1</span>], birthdata_url)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">map_line</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.strings.to_number(tf.strings.split(x))
birth_file = (tf.data
              .TextLineDataset(path)
              .skip(<span class="hljs-number">1</span>)     <span class="hljs-comment"># Skip first header line</span>
              .map(map_line)
             )
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Boston housing data</strong>: Carnegie Mellon University maintains a library of datasets in their <code class="Code-In-Text--PACKT-">StatLib</code> Library. This data is <a id="_idIndexMarker057"/>easily accessible via The University of California at Irvine's machine learning repository (<a href="https://archive.ics.uci.edu/ml/index.php"><span class="url">https://archive.ics.uci.edu/ml/index.php</span></a>). There are 506 observations of house worth, along with various <a id="_idIndexMarker058"/>demographic data and housing attributes (14 variables). The following code shows you how to access this data in TensorFlow:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
housing_url = <span class="hljs-string">'http://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data'</span>
path = tf.keras.utils.get_file(housing_url.split(<span class="hljs-string">"/"</span>)[<span class="hljs-number">-1</span>], housing_url)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">map_line</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.strings.to_number(tf.strings.split(x))
housing = (tf.data
           .TextLineDataset(path)
           .map(map_line)
          )
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">MNIST handwriting data</strong>: The <strong class="keyword">Mixed National Institute of Standards and Technology</strong> (<strong class="keyword">MNIST</strong>) dataset is a <a id="_idIndexMarker059"/>subset of the larger<a id="_idIndexMarker060"/> NIST handwriting database. The MNIST handwriting dataset is hosted on Yann LeCun's website (<a href="http://yann.lecun.com/exdb/mnist/"><span class="url">http://yann.lecun.com/exdb/mnist/</span></a>). It is a database of 70,000 images of single-digit numbers (0-9), with about 60,000 annotated for a training set and 10,000 for a test set. This dataset is used so often in image recognition that TensorFlow provides built-in functions to access this data. In machine learning, it is also important to <a id="_idIndexMarker061"/>provide validation data to prevent overfitting (target leakage). Because of this, TensorFlow sets aside 5,000 images of the training set in a validation set. The following code shows you how to access this data in TensorFlow:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
mnist = tfds.load(<span class="hljs-string">'mnist'</span>, split=<span class="hljs-literal">None</span>)
mnist_train = mnist[<span class="hljs-string">'train'</span>]
mnist_test = mnist[<span class="hljs-string">'test'</span>]
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Spam-ham text data</strong>. UCI's machine<a id="_idIndexMarker062"/> learning dataset library also holds a spam-ham text message dataset. We can access this <code class="Code-In-Text--PACKT-">.zip</code> file and get the spam-ham text data as follows:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
zip_url = <span class="hljs-string">'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'</span>
path = tf.keras.utils.get_file(zip_url.split(<span class="hljs-string">"/"</span>)[<span class="hljs-number">-1</span>], zip_url, extract=<span class="hljs-literal">True</span>)
path = path.replace(<span class="hljs-string">"smsspamcollection.zip"</span>, <span class="hljs-string">"SMSSpamCollection"</span>)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">split_text</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.strings.split(x, sep=<span class="hljs-string">'\t'</span>)
text_data = (tf.data
             .TextLineDataset(path)
             .map(split_text)
            )
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">Movie review data</strong>: Bo Pang from Cornell has released a movie review dataset that classifies reviews as good <a id="_idIndexMarker063"/>or bad. You can find the data on the Cornell University website: <a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/"><span class="url">http://www.cs.cornell.edu/people/pabo/movie-review-data/</span></a>. To download, extract, and transform this data, we can run the following code:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
movie_data_url = <span class="hljs-string">'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'</span>
path = tf.keras.utils.get_file(movie_data_url.split(<span class="hljs-string">"/"</span>)[<span class="hljs-number">-1</span>], movie_data_url, extract=<span class="hljs-literal">True</span>)
path = path.replace(<span class="hljs-string">'.tar.gz'</span>, <span class="hljs-string">''</span>)
<span class="hljs-keyword">with</span> open(path+filename, <span class="hljs-string">'r'</span>, encoding=<span class="hljs-string">'utf-8'</span>, errors=<span class="hljs-string">'ignore'</span>) <span class="hljs-keyword">as</span> movie_file:
    <span class="hljs-keyword">for</span> response, filename <span class="hljs-keyword">in</span> enumerate([<span class="hljs-string">'\\rt-polarity.neg'</span>, <span class="hljs-string">'\\rt-polarity.pos'</span>]):
        <span class="hljs-keyword">with</span> open(path+filename, <span class="hljs-string">'r'</span>) <span class="hljs-keyword">as</span> movie_file:
            <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> movie_file:
                review_file.write(str(response) + <span class="hljs-string">'\t'</span> + line.encode(<span class="hljs-string">'utf-8'</span>).decode())
                
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">split_text</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.strings.split(x, sep=<span class="hljs-string">'\t'</span>)
movies = (tf.data
          .TextLineDataset(<span class="hljs-string">'movie_reviews.txt'</span>)
          .map(split_text)
         )
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">CIFAR-10 image data</strong>: The Canadian Institute for Advanced Research has released an image set that<a id="_idIndexMarker064"/> contains 80 million labeled colored<a id="_idIndexMarker065"/> images (each image is scaled to 32 x 32 pixels). There are 10 different target classes (airplane, automobile, bird, and so on). CIFAR-10 is a subset that includes 60,000 images. There are 50,000 images in the training set, and 10,000 in the test set. Since we will be using this dataset in multiple ways, and because it is one of our larger datasets, we will not run a script each time we need it. To get this dataset, just execute the following code to download the CIFAR-10 dataset (this may take a long time):
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
ds, info = tfds.load(<span class="hljs-string">'cifar10'</span>, shuffle_files=<span class="hljs-literal">True</span>, with_info=<span class="hljs-literal">True</span>)
print(info)
cifar_train = ds[<span class="hljs-string">'train'</span>]
cifar_test = ds[<span class="hljs-string">'test'</span>] 
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">The works of Shakespeare text data</strong>: Project Gutenberg is a project that releases electronic<a id="_idIndexMarker066"/> versions of free books. They have compiled all of the<a id="_idIndexMarker067"/> works of Shakespeare together. The following code shows you how to access this text file through TensorFlow:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
shakespeare_url = <span class="hljs-string">'https://raw.githubusercontent.com/PacktPublishing/TensorFlow-2-Machine-Learning-Cookbook-Third-Edition/master/shakespeare.txt'</span>
path = tf.keras.utils.get_file(shakespeare_url.split(<span class="hljs-string">"/"</span>)[<span class="hljs-number">-1</span>], shakespeare_url)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">split_text</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.strings.split(x, sep=<span class="hljs-string">'\n'</span>)
shakespeare_text = (tf.data
                    .TextLineDataset(path)
                    .map(split_text)
                   )
</code></pre>
      </li>
      <li class="numbered"><strong class="keyword">English-German sentence translation data</strong>: The Tatoeba project (<a href="http://tatoeba.org"><span class="url">http://tatoeba.org</span></a>) collects<a id="_idIndexMarker068"/> sentence translations in many languages. Their data has been released under the Creative Commons license. From this data, ManyThings.org (<a href="http://www.manythings.org"><span class="url">http://www.manythings.org</span></a>) has compiled sentence-to-sentence translations in text files that are available for download. Here, we will use the English-German translation file, but you can change the URL to whichever languages you would like to use:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> os
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
<span class="hljs-keyword">from</span> zipfile <span class="hljs-keyword">import</span> ZipFile
<span class="hljs-keyword">from</span> urllib.request <span class="hljs-keyword">import</span> urlopen, Request
<span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
sentence_url = <span class="hljs-string">'https://www.manythings.org/anki/deu-eng.zip'</span>
r = Request(sentence_url, headers={<span class="hljs-string">'User-Agent'</span>: <span class="hljs-string">'Mozilla/5.0 (X11; U; Linux i686) Gecko/20071127 Firefox/2.0.0.11'</span>})
b2 = [z <span class="hljs-keyword">for</span> z <span class="hljs-keyword">in</span> sentence_url.split(<span class="hljs-string">'/'</span>) <span class="hljs-keyword">if</span> <span class="hljs-string">'.zip'</span> <span class="hljs-keyword">in</span> z][<span class="hljs-number">0</span>] <span class="hljs-comment">#gets just the '.zip' part of the url</span>
<span class="hljs-keyword">with</span> open(b2, <span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> target:
    target.write(urlopen(r).read()) <span class="hljs-comment">#saves to file to disk</span>
<span class="hljs-keyword">with</span> ZipFile(b2) <span class="hljs-keyword">as</span> z:
    deu = [line.split(<span class="hljs-string">'\t'</span>)[:<span class="hljs-number">2</span>] <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> z.open(<span class="hljs-string">'deu.txt'</span>).read().decode().split(<span class="hljs-string">'\n'</span>)]
os.remove(b2) <span class="hljs-comment">#removes the zip file</span>
<span class="hljs-comment"># saving to disk prepared en-de sentence file</span>
<span class="hljs-keyword">with</span> open(<span class="hljs-string">"deu.txt"</span>, <span class="hljs-string">"wb"</span>) <span class="hljs-keyword">as</span> deu_file:
    <span class="hljs-keyword">for</span> line <span class="hljs-keyword">in</span> deu:
        data = <span class="hljs-string">","</span>.join(line)+<span class="hljs-string">'\n'</span>
        deu_file.write(data.encode(<span class="hljs-string">'utf-8'</span>))
        
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">split_text</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.strings.split(x, sep=<span class="hljs-string">','</span>)
text_data = (tf.data
             .TextLineDataset(<span class="hljs-string">"deu.txt"</span>)
             .map(split_text)
            ) 
</code></pre>
      </li>
    </ol>
    <p class="normal">With this last dataset, we <a id="_idIndexMarker069"/>have completed our review of the datasets that you will most frequently encounter when using the recipes you will find in this book. At the start of each recipe, we'll remind you how to download the relevant dataset and explain why it is relevant for the recipe in question.</p>
    <h2 id="_idParaDest-47" class="title">How it works…</h2>
    <p class="normal">When it comes to using one of these datasets in a recipe, we'll refer you to this section and assume that the <a id="_idIndexMarker070"/>data is loaded in the ways we've just described. If further data transformation or preprocessing is necessary, then that code will be provided in the recipe itself.</p>
    <p class="normal">Usually, the approach will simply be as follows when we use data from TensorFlow datasets:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> tensorflow_datasets <span class="hljs-keyword">as</span> tfds
dataset_name = <span class="hljs-string">"..."</span>
data = tfds.load(dataset_name, split=<span class="hljs-literal">None</span>)
train = data[<span class="hljs-string">'train'</span>]
test = data[<span class="hljs-string">'test'</span>]
</code></pre>
    <p class="normal">In any case, depending on the location of the data, it may turn out to be necessary to download it, extract it, and transform it. </p>
    <h2 id="_idParaDest-48" class="title">See also</h2>
    <p class="normal">Here are some additional references for the data <a id="_idIndexMarker071"/>resources we use in this book:</p>
    <ul>
      <li class="bullet">Hosmer, D.W., Lemeshow, S., and Sturdivant, R. X. (2013) <em class="italic">Applied Logistic Regression: 3rd Edition</em></li>
      <li class="bullet">Lichman, M. (2013). <em class="italic">UCI machine learning repository</em>: <a href="http://archive.ics.uci.edu/ml"><span class="url">http://archive.ics.uci.edu/ml</span></a>. Irvine, CA: University of California, School of Information and Computer Science</li>
      <li class="bullet">Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan, <em class="italic">Thumbs up? Sentiment classification using machine learning techniques</em>, Proceedings of EMNLP 2002: <a href="http://www.cs.cornell.edu/people/pabo/movie-review-data/"><span class="url">http://www.cs.cornell.edu/people/pabo/movie-review-data/</span></a></li>
      <li class="bullet">Krizhevsky. (2009). <em class="italic">Learning Multiple Layers of Features from Tiny Images</em>: <a href="http://www.cs.toronto.edu/~kriz/cifar.html"><span class="url">http://www.cs.toronto.edu/~kriz/cifar.html</span></a></li>
      <li class="bullet"><em class="italic">Project Gutenberg. Accessed</em> April 2016: <a href="http://www.gutenberg.org/ "><span class="url">http://www.gutenberg.org/</span></a></li>
    </ul>
    <h1 id="_idParaDest-49" class="title">Additional resources</h1>
    <p class="normal">In this section, you <a id="_idIndexMarker072"/>will find additional links, documentation sources, and tutorials that will be of great assistance when learning and using TensorFlow.</p>
    <h2 id="_idParaDest-50" class="title">Getting ready</h2>
    <p class="normal">When learning how to use TensorFlow, it helps to know where to turn for assistance or pointers. This section lists some resources to get TensorFlow running and to troubleshoot problems.</p>
    <h2 id="_idParaDest-51" class="title">How to do it…</h2>
    <p class="normal">Here is a list of TensorFlow resources:</p>
    <ul>
      <li class="bullet">The code for this<a id="_idIndexMarker073"/> book is available online at the Packt repository: <a href="https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook</span></a></li>
      <li class="bullet">The official TensorFlow Python API documentation is located at <a href="https://www.tensorflow.org/api_docs/python"><span class="url">https://www.tensorflow.org/api_docs/python</span></a>. Here, there is documentation and examples of all of the functions, objects, and methods in TensorFlow.</li>
      <li class="bullet">TensorFlow's official tutorials are very thorough and detailed. They are located at <a href="https://www.tensorflow.org/tutorials/index.html"><span class="url">https://www.tensorflow.org/tutorials/index.html</span></a>. They start covering image recognition models, and work through Word2Vec, RNN models, and sequence-to-sequence models. They also have additional tutorials for generating fractals and solving PDE systems. Note that they are continually adding more tutorials and examples to this collection.</li>
      <li class="bullet">TensorFlow's official GitHub repository is available via <a href="https://github.com/tensorflow/tensorflow"><span class="url">https://github.com/tensorflow/tensorflow</span></a>. Here, you can view the open source code and even fork or clone the most current version of the code if you want. You can also see current filed issues if you navigate to the <code class="Code-In-Text--PACKT-">issues</code> directory.</li>
      <li class="bullet">A public Docker container that is kept up to date by TensorFlow is available on Dockerhub at <a href="https://hub.docker.com/r/tensorflow/tensorflow/"><span class="url">https://hub.docker.com/r/tensorflow/tensorflow/</span></a>.</li>
      <li class="bullet">A great source for community help is Stack Overflow. There is a tag for TensorFlow. This tag seems to be growing in interest as TensorFlow is gaining in popularity. To view activity on this tag, visit <a href="http://stackoverflow.com/questions/tagged/Tensorflow"><span class="url">http://stackoverflow.com/questions/tagged/Tensorflow</span></a>.</li>
      <li class="bullet">While TensorFlow is very agile and can be used for many things, the most common use of TensorFlow is deep learning. To understand the basis of deep learning, how the underlying mathematics works, and to develop more intuition on deep learning, Google has created an online course that's available on Udacity. To sign up and take this video lecture course, visit <a href="https://www.udacity.com/course/deep-learning--ud730"><span class="url">https://www.udacity.com/course/deep-learning--ud730</span></a>.</li>
      <li class="bullet">TensorFlow has also made a site where you can visually explore training a neural network while changing the parameters and datasets. Visit <a href="http://playground.tensorflow.org/"><span class="url">http://playground.tensorflow.org/</span></a> to explore how different settings affect the training of neural networks.</li>
      <li class="bullet">Andrew Ng teaches an online course called Neural Networks and Deep Learning : <a href="https://www.coursera.org/learn/neural-networks-deep-learning"><span class="url">https://www.coursera.org/learn/neural-networks-deep-learning</span></a></li>
      <li class="bullet">Stanford University has an online syllabus and detailed course notes for <em class="italic">Convolutional Neural Networks for Visual Recognition</em>: <a href="http://cs231n.stanford.edu/"><span class="url">http://cs231n.stanford.edu/</span></a></li>
    </ul>
  </div>
</body></html>