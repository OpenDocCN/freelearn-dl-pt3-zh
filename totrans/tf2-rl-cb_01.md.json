["```\nimport copy\nimport sys\nimport gym\nimport numpy as np\n```", "```\n    EMPTY = BLACK = 0\n    WALL = GRAY = 1\n    AGENT = BLUE = 2\n    MINE = RED = 3\n    GOAL = GREEN = 4\n    SUCCESS = PINK = 5\n    ```", "```\n    COLOR_MAP = {\n        BLACK: [0.0, 0.0, 0.0],\n        GRAY: [0.5, 0.5, 0.5],\n        BLUE: [0.0, 0.0, 1.0],\n        RED: [1.0, 0.0, 0.0],\n        GREEN: [0.0, 1.0, 0.0],\n        PINK: [1.0, 0.0, 1.0],\n    }\n    ```", "```\n    NOOP = 0\n    DOWN = 1\n    UP = 2\n    LEFT = 3\n    RIGHT = 4\n    ```", "```\n    class GridworldEnv():\n    \tdef __init__(self):\n    ```", "```\n    \tself.grid_layout = \"\"\"\n            1 1 1 1 1 1 1 1\n            1 2 0 0 0 0 0 1\n            1 0 1 1 1 0 0 1\n            1 0 1 0 1 0 0 1\n            1 0 1 4 1 0 0 1\n            1 0 3 0 0 0 0 1\n            1 0 0 0 0 0 0 1\n            1 1 1 1 1 1 1 1\n            \"\"\"\n    ```", "```\n    \tself.initial_grid_state = np.fromstring(\n                        self.grid_layout, dtype=int, sep=\" \")\n    \tself.initial_grid_state = \\\n                        self.initial_grid_state.reshape(8, 8)\n    \tself.grid_state = copy.deepcopy(\n                                     self.initial_grid_state)\n    \tself.observation_space = gym.spaces.Box(\n    \t\tlow=0, high=6, shape=self.grid_state.shape\n    \t)\n    \tself.img_shape = [256, 256, 3]\n    \tself.metadata = {\"render.modes\": [\"human\"]}\n    ```", "```\n    \t   self.action_space = gym.spaces.Discrete(5)\n            self.actions = [NOOP, UP, DOWN, LEFT, RIGHT]\n            self.action_pos_dict = {\n                NOOP: [0, 0],\n                UP: [-1, 0],\n                DOWN: [1, 0],\n                LEFT: [0, -1],\n                RIGHT: [0, 1],\n            }\n    ```", "```\n    (self.agent_start_state, self.agent_goal_state,) = \\\n                                             self.get_state()\n    ```", "```\n    def get_state(self):\n            start_state = np.where(self.grid_state == AGENT)\n            goal_state = np.where(self.grid_state == GOAL)\n            start_or_goal_not_found = not (start_state[0] \\\n                                           and goal_state[0])\n            if start_or_goal_not_found:\n                sys.exit(\n                    \"Start and/or Goal state not present in \n                     the Gridworld. \"\n                    \"Check the Grid layout\"\n                )\n            start_state = (start_state[0][0], \n                           start_state[1][0])\n            goal_state = (goal_state[0][0], goal_state[1][0])\n            return start_state, goal_state\n    ```", "```\n    def step(self, action):\n            \"\"\"return next observation, reward, done, info\"\"\"\n            action = int(action)\n            info = {\"success\": True}\n            done = False\n            reward = 0.0\n            next_obs = (\n                self.agent_state[0] + \\\n                    self.action_pos_dict[action][0],\n                self.agent_state[1] + \\\n                    self.action_pos_dict[action][1],\n            )\n    ```", "```\n     # Determine the reward\n            if action == NOOP:\n                return self.grid_state, reward, False, info\n            next_state_valid = (\n                next_obs[0] < 0 or next_obs[0] >= \\\n                                    self.grid_state.shape[0]\n            ) or (next_obs[1] < 0 or next_obs[1] >= \\\n                                    self.grid_state.shape[1])\n            if next_state_valid:\n                info[\"success\"] = False\n                return self.grid_state, reward, False, info\n            next_state = self.grid_state[next_obs[0], \n                                         next_obs[1]]\n            if next_state == EMPTY:\n                self.grid_state[next_obs[0], \n                                next_obs[1]] = AGENT\n            elif next_state == WALL:\n                info[\"success\"] = False\n                reward = -0.1\n                return self.grid_state, reward, False, info\n            elif next_state == GOAL:\n                done = True\n                reward = 1\n            elif next_state == MINE:\n                done = True\n                reward = -1        # self._render(\"human\")\n            self.grid_state[self.agent_state[0], \n                            self.agent_state[1]] = EMPTY\n            self.agent_state = copy.deepcopy(next_obs)\n            return self.grid_state, reward, done, info\n    ```", "```\n    def reset(self):\n            self.grid_state = copy.deepcopy(\n                                     self.initial_grid_state)\n            (self.agent_state, self.agent_goal_state,) = \\\n                                             self.get_state()\n            return self.grid_state\n    ```", "```\n    def gridarray_to_image(self, img_shape=None):\n            if img_shape is None:\n                img_shape = self.img_shape\n            observation = np.random.randn(*img_shape) * 0.0\n            scale_x = int(observation.shape[0] / self.grid_\\\n                                             state.shape[0])\n            scale_y = int(observation.shape[1] / self.grid_\\\n                                             state.shape[1])\n            for i in range(self.grid_state.shape[0]):\n                for j in range(self.grid_state.shape[1]):\n                    for k in range(3):  # 3-channel RGB image\n                        pixel_value = \\\n                          COLOR_MAP[self.grid_state[i, j]][k]\n                        observation[\n                            i * scale_x : (i + 1) * scale_x,\n                            j * scale_y : (j + 1) * scale_y,\n                            k,\n                        ] = pixel_value\n            return (255 * observation).astype(np.uint8)\n        def render(self, mode=\"human\", close=False):\n            if close:\n                if self.viewer is not None:\n                    self.viewer.close()\n                    self.viewer = None\n                return\n            img = self.gridarray_to_image()\n            if mode == \"rgb_array\":\n                return img\n            elif mode == \"human\":\n                from gym.envs.classic_control import \\\n                   rendering\n                if self.viewer is None:\n                    self.viewer = \\\n                            rendering.SimpleImageViewer()\n                self.viewer.imshow(img)\n    ```", "```\n    if __name__ == \"__main__\":\n    \tenv = GridworldEnv()\n    \tobs = env.reset()\n    \t# Sample a random action from the action space\n    \taction = env.action_space.sample()\n    \tnext_obs, reward, done, info = env.step(action)\n    \tprint(f\"reward:{reward} done:{done} info:{info}\")\n    \tenv.render()\n    \tenv.close()\n    ```", "```\n    reward:0.0 done:False info:{'success': True}\n    ```", "```\npip install --upgrade numpy tensorflow tensorflow_probability seaborn \nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow_probability as tfp\n```", "```\n    binary_policy = tfp.distributions.Bernoulli(probs=0.5)\n    for i in range(5):\n        action = binary_policy.sample(1)\n        print(\"Action:\", action)\n    ```", "```\n    Action: tf.Tensor([0], shape=(1,), dtype=int32)\n    Action: tf.Tensor([1], shape=(1,), dtype=int32)\n    Action: tf.Tensor([0], shape=(1,), dtype=int32)\n    Action: tf.Tensor([1], shape=(1,), dtype=int32)\n    Action: tf.Tensor([1], shape=(1,), dtype=int32)\n    ```", "```\n    # Sample 500 actions from the binary policy distribution\n    sample_actions = binary_policy.sample(500)\n    sns.distplot(sample_actions)\n    ```", "```\n    action_dim = 4  # Dimension of the discrete action space\n    action_probabilities = [0.25, 0.25, 0.25, 0.25]\n    discrete_policy = tfp.distributions.Multinomial(probs=action_probabilities, total_count=1)\n    for i in range(5):\n        action = discrete_policy.sample(1)\n        print(action)\n    ```", "```\n    tf.Tensor([[0\\. 0\\. 0\\. 1.]], shape=(1, 4), dtype=float32)\n    tf.Tensor([[0\\. 0\\. 1\\. 0.]], shape=(1, 4), dtype=float32)\n    tf.Tensor([[0\\. 0\\. 1\\. 0.]], shape=(1, 4), dtype=float32)\n    tf.Tensor([[1\\. 0\\. 0\\. 0.]], shape=(1, 4), dtype=float32)\n    tf.Tensor([[0\\. 1\\. 0\\. 0.]], shape=(1, 4), dtype=float32)\n    ```", "```\n    sns.distplot(discrete_policy.sample(1))\n    ```", "```\n    def entropy(action_probs):\n        return -tf.reduce_sum(action_probs * \\\n                          tf.math.log(action_probs), axis=-1)\n    action_probabilities = [0.25, 0.25, 0.25, 0.25]\n    print(entropy(action_probabilities))\n    ```", "```\n    class DiscretePolicy(object):\n        def __init__(self, num_actions):\n            self.action_dim = num_actions\n        def sample(self, actino_logits):\n            self.distribution = tfp.distributions.Multinomial(logits=action_logits, total_count=1)\n            return self.distribution.sample(1)\n        def get_action(self, action_logits):\n            action = self.sample(action_logits)\n            return np.where(action)[-1]  \n            # Return the action index\n        def entropy(self, action_probabilities):\n            return – tf.reduce_sum(action_probabilities * tf.math.log(action_probabilities), axis=-1)\n    ```", "```\n    def evaluate(agent, env, render=True):\n        obs, episode_reward, done, step_num = env.reset(), \n                                              0.0, False, 0\n        while not done:\n            action = agent.get_action(obs)\n            obs, reward, done, info = env.step(action)\n            episode_reward += reward\n            step_num += 1\n            if render:\n                env.render()\n        return step_num, episode_reward, done, info\n    ```", "```\n    class Brain(keras.Model):\n        def __init__(self, action_dim=5, \n                     input_shape=(1, 8 * 8)):\n            \"\"\"Initialize the Agent's Brain model\n            Args:\n                action_dim (int): Number of actions\n            \"\"\"\n            super(Brain, self).__init__()\n            self.dense1 = layers.Dense(32, input_shape=\\\n                              input_shape, activation=\"relu\")\n            self.logits = layers.Dense(action_dim)\n        def call(self, inputs):\n            x = tf.convert_to_tensor(inputs)\n            if len(x.shape) >= 2 and x.shape[0] != 1:\n                x = tf.reshape(x, (1, -1))\n            return self.logits(self.dense1(x))\n        def process(self, observations):\n    # Process batch observations using `call(inputs)` behind-the-scenes\n            action_logits = \\\n                         self.predict_on_batch(observations)\n            return action_logits\n    ```", "```\n    class Agent(object):\n        def __init__(self, action_dim=5, \n                     input_dim=(1, 8 * 8)):\n            self.brain = Brain(action_dim, input_dim)\n            self.policy = DiscretePolicy(action_dim)\n        def get_action(self, obs):\n            action_logits = self.brain.process(obs)\n            action = self.policy.get_action(\n                                np.squeeze(action_logits, 0))\n            return action\n    ```", "```\n    from envs.gridworld import GridworldEnv\n    env = GridworldEnv()\n    agent = Agent(env.action_space.n, \n                  env.observation_space.shape)\n    steps, reward, done, info = evaluate(agent, env)\n    print(f\"steps:{steps} reward:{reward} done:{done} info:{info}\")\n    env.close()\n    ```", "```\npip install --upgrade tensorflow_probability\nimport tensorflow_probability as tfp\nimport seaborn as sns\n```", "```\n    sample_actions = continuous_policy.sample(500)\n    sns.distplot(sample_actions)\n    ```", "```\n    sample_actions = continuous_policy.sample(500)\n    sns.distplot(sample_actions)\n    ```", "```\n    mu = 0.0  # Mean = 0.0\n    sigma = 1.0  # Std deviation = 1.0\n    continuous_policy = tfp.distributions.Normal(loc=mu,\n                                                 scale=sigma)\n    # action = continuous_policy.sample(10)\n    for i in range(10):\n        action = continuous_policy.sample(1)\n        print(action)\n    ```", "```\n    tf.Tensor([-0.2527136], shape=(1,), dtype=float32)\n    tf.Tensor([1.3262751], shape=(1,), dtype=float32)\n    tf.Tensor([0.81889665], shape=(1,), dtype=float32)\n    tf.Tensor([1.754675], shape=(1,), dtype=float32)\n    tf.Tensor([0.30025303], shape=(1,), dtype=float32)\n    tf.Tensor([-0.61728036], shape=(1,), dtype=float32)\n    tf.Tensor([0.40142158], shape=(1,), dtype=float32)\n    tf.Tensor([1.3219402], shape=(1,), dtype=float32)\n    tf.Tensor([0.8791297], shape=(1,), dtype=float32)\n    tf.Tensor([0.30356944], shape=(1,), dtype=float32)\n    ```", "```\n    mu = [0.0, 0.0]\n    covariance_diag = [3.0, 3.0]\n    continuous_multidim_policy = tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=covariance_diag)\n    # action = continuous_multidim_policy.sample(10)\n    for i in range(10):\n        action = continuous_multidim_policy.sample(1)\n        print(action)\n    ```", "```\n     tf.Tensor([[ 1.7003113 -2.5801306]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[ 2.744986  -0.5607129]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[ 6.696332  -3.3528223]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[ 1.2496299 -8.301748 ]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[2.0009246 3.557394 ]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[-4.491785  -1.0101566]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[ 3.0810184 -0.9008362]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[1.4185237 2.2145705]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[-1.9961193 -2.1251974]], shape=(1, 2), dtype=float32)\n    tf.Tensor([[-1.2200387 -4.3516426]], shape=(1, 2), dtype=float32)\n    ```", "```\n    sample_actions = continuous_multidim_policy.sample(500)\n    sns.jointplot(sample_actions[:, 0], sample_actions[:, 1], kind='scatter')\n    ```", "```\n    class ContinuousPolicy(object):\n        def __init__(self, action_dim):\n            self.action_dim = action_dim\n        def sample(self, mu, var):\n            self.distribution = \\\n                tfp.distributions.Normal(loc=mu, scale=sigma)\n            return self.distribution.sample(1)\n        def get_action(self, mu, var):\n            action = self.sample(mu, var)\n            return action\n    ```", "```\n    import tensorflow_probability as tfp\n    import numpy as np\n    class ContinuousMultiDimensionalPolicy(object):\n        def __init__(self, num_actions):\n            self.action_dim = num_actions\n        def sample(self, mu, covariance_diag):\n            self.distribution = tfp.distributions.\\\n                             MultivariateNormalDiag(loc=mu,\n                             scale_diag=covariance_diag)\n            return self.distribution.sample(1)\n        def get_action(self, mu, covariance_diag):\n            action = self.sample(mu, covariance_diag)\n            return action\n    ```", "```\n    def evaluate(agent, env, render=True):\n        obs, episode_reward, done, step_num = env.reset(),\n                                              0.0, False, 0\n        while not done:\n            action = agent.get_action(obs)\n            obs, reward, done, info = env.step(action)\n            episode_reward += reward\n            step_num += 1\n            if render:\n                env.render()\n        return step_num, episode_reward, done, info\n    ```", "```\n    from neural_agent import Brain\n    import gym\n    env = gym.make(\"MountainCarContinuous-v0\")Implementing a Neural-network Brain class using TensorFlow 2.x. \n              class Brain(keras.Model):\n        def __init__(self, action_dim=5, \n                     input_shape=(1, 8 * 8)):\n            \"\"\"Initialize the Agent's Brain model\n            Args:\n                action_dim (int): Number of actions\n            \"\"\"\n            super(Brain, self).__init__()\n            self.dense1 = layers.Dense(32, \n                  input_shape=input_shape, activation=\"relu\")\n            self.logits = layers.Dense(action_dim)\n        def call(self, inputs):\n            x = tf.convert_to_tensor(inputs)\n            if len(x.shape) >= 2 and x.shape[0] != 1:\n                x = tf.reshape(x, (1, -1))\n            return self.logits(self.dense1(x))\n        def process(self, observations):\n            # Process batch observations using `call(inputs)`\n            # behind-the-scenes\n            action_logits = \\\n                self.predict_on_batch(observations)\n            return action_logits\n    ```", "```\n    class Agent(object):\n        def __init__(self, action_dim=5, \n                     input_dim=(1, 8 * 8)):\n            self.brain = Brain(action_dim, input_dim)\n            self.policy = ContinuousPolicy(action_dim)\n        def get_action(self, obs):\n            action_logits = self.brain.process(obs)\n            action = self.policy.get_action(*np.\\\n                            squeeze(action_logits, 0))\n            return action\n    ```", "```\n    from neural_agent import Brain\n    import gym\n    env = gym.make(\"MountainCarContinuous-v0\") \n    action_dim = 2 * env.action_space.shape[0]  \n        # 2 values (mu & sigma) for one action dim\n    agent = Agent(action_dim, env.observation_space.shape)\n    steps, reward, done, info = evaluate(agent, env)\n    print(f\"steps:{steps} reward:{reward} done:{done} info:{info}\")\n    env.close()\n    ```", "```\npip install gym[atari]\n```", "```\n    #!/usr/bin/env python\n    from gym import envs\n    env_names = [spec.id for spec in envs.registry.all()]\n    for name in sorted(env_names):\n        print(name)\n    ```", "```\n    #!/usr/bin/env python\n    import gym\n    import sys\n    def run_gym_env(argv):\n        env = gym.make(argv[1]) # Name of the environment \n                                # supplied as 1st argument\n        env.reset()\n        for _ in range(int(argv[2])):\n            env.render()\n            env.step(env.action_space.sample())\n        env.close()\n    if __name__ == \"__main__\":\n        run_gym_env(sys.argv)\n    ```", "```\n    Alien-v4 environment, which should look like the following screenshot:\n    ```", "```\npip install tensorflow gym tqdm  # Run this line in a terminal\n```", "```\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport gym\nimport envs\nfrom tqdm import tqdm\n```", "```\n    class Brain(keras.Model):\n        def __init__(self, action_dim=5, \n                       input_shape=(1, 8 * 8)):\n            \"\"\"Initialize the Agent's Brain model\n            Args:\n                action_dim (int): Number of actions\n            \"\"\"\n            super(Brain, self).__init__()\n            self.dense1 = layers.Dense(32, input_shape= \\\n                              input_shape, activation=\"relu\")\n            self.logits = layers.Dense(action_dim)\n    ```", "```\n    def call(self, inputs):\n            x = tf.convert_to_tensor(inputs)\n            if len(x.shape) >= 2 and x.shape[0] != 1:\n                x = tf.reshape(x, (1, -1))\n            return self.logits(self.dense1(x))\n    ```", "```\n    def process(self, observations):\n            # Process batch observations using `call(inputs)`\n            # behind-the-scenes\n            action_logits = \\\n                self.predict_on_batch(observations)\n            return action_logits\n    ```", "```\n    class Agent(object):\n        def __init__(self, action_dim=5, \n                     input_shape=(1, 8 * 8)):\n            \"\"\"Agent with a neural-network brain powered\n               policy\n            Args:\n                brain (keras.Model): Neural Network based \n            model\n            \"\"\"\n            self.brain = Brain(action_dim, input_shape)\n            self.policy = self.policy_mlp\n    ```", "```\n    def policy_mlp(self, observations):\n            observations = observations.reshape(1, -1)\n            # action_logits = self.brain(observations)\n            action_logits = self.brain.process(observations)\n            action = tf.random.categorical(tf.math.\\\n                           log(action_logits), num_samples=1)\n            return tf.squeeze(action, axis=1)\n    ```", "```\n    def get_action(self, observations):\n            return self.policy(observations)\n    ```", "```\n    def learn(self, samples):\n            raise NotImplementedError\n    ```", "```\n    def evaluate(agent, env, render=True):\n        obs, episode_reward, done, step_num = env.reset(),\n                                              0.0, False, 0\n        while not done:\n            action = agent.get_action(obs)\n            obs, reward, done, info = env.step(action)\n            episode_reward += reward\n            step_num += 1\n            if render:\n                env.render()\n        return step_num, episode_reward, done, info\n    ```", "```\n    if __name__ == \"__main__\":\n        env = gym.make(\"Gridworld-v0\")\n        agent = Agent(env.action_space.n, \n                      env.observation_space.shape)\n        for episode in tqdm(range(10)):\n            steps, episode_reward, done, info = \\\n                                         evaluate(agent, env)\n            print(f\"EpReward:{episode_reward:.2f}\\\n                   steps:{steps} done:{done} info:{info}\")\n        env.close()\n    ```", "```\n    python neural_agent.py\n    ```", "```\nself.brain = Brain(action_dim, input_shape)\n```", "```\nfrom collections import namedtuple\nimport gym\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tqdm import tqdm\nimport envs\n```", "```\n    from neural_agent import Agent, Brain\n    from envs.gridworld import GridworldEnv\n    ```", "```\n    def rollout(agent, env, render=False):\n        obs, episode_reward, done, step_num = env.reset(),\n    \t\t\t\t\t\t\t 0.0, False, 0\n        observations, actions = [], []\n        episode_reward = 0.0\n        while not done:\n            action = agent.get_action(obs)\n            next_obs, reward, done, info = env.step(action)\n            # Save experience\n            observations.append(np.array(obs).reshape(1, -1))  \t        # Convert to numpy & reshape (8, 8) to (1, 64)\n            actions.append(action)\n            episode_reward += reward\n\n            obs = next_obs\n            step_num += 1\n            if render:\n                env.render()\n        env.close()\n        return observations, actions, episode_reward\n    ```", "```\n    env = GridworldEnv()\n    # input_shape = (env.observation_space.shape[0] * \\\n                     env.observation_space.shape[1], )\n    brain = Brain(env.action_space.n)\n    agent = Agent(brain)\n    obs_batch, actions_batch, episode_reward = rollout(agent,\n                                                       env)\n    ```", "```\n    assert len(obs_batch) == len(actions_batch)\n    ```", "```\n    # Trajectory: (obs_batch, actions_batch, episode_reward)\n    # Rollout 100 episodes; Maximum possible steps = 100 * 100 = 10e4\n    trajectories = [rollout(agent, env, render=True) \\\n                    for _ in tqdm(range(100))]\n    ```", "```\n    from tqdm.auto import tqdm\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    sample_ep_rewards = [rollout(agent, env)[-1] for _ in \\\n                         tqdm(range(100))]\n    plt.hist(sample_ep_rewards, bins=10, histtype=\"bar\");\n    ```", "```\n    from collections import namedtuple\n    Trajectory = namedtuple(\"Trajectory\", [\"obs\", \"actions\",\n                                           \"reward\"])\n    # Example for understanding the operations:\n    print(Trajectory(*(1, 2, 3)))\n    # Explanation: `*` unpacks the tuples into individual \n    # values\n    Trajectory(*(1, 2, 3)) == Trajectory(1, 2, 3)\n    # The rollout(...) function returns a tuple of 3 values: \n    # (obs, actions, rewards)\n    # The Trajectory namedtuple can be used to collect \n    # and store mini batch of experience to train the neuro \n    # evolution agent\n    trajectories = [Trajectory(*rollout(agent, env)) \\\n                    for _ in range(2)]\n    ```", "```\n    def gather_elite_xp(trajectories, elitism_criterion):\n        \"\"\"Gather elite trajectories from the batch of \n           trajectories\n        Args:\n            batch_trajectories (List): List of episode \\\n            trajectories containing experiences (obs,\n                                      actions,episode_reward)\n        Returns:\n            elite_batch_obs\n            elite_batch_actions\n            elite_reard_threshold\n\n        \"\"\"\n        batch_obs, batch_actions, \n        batch_rewards = zip(*trajectories)\n        reward_threshold = np.percentile(batch_rewards,\n                                         elitism_criterion)\n        indices = [index for index, value in enumerate(\n                 batch_rewards) if value >= reward_threshold]\n\n        elite_batch_obs = [batch_obs[i] for i in indices]\n        elite_batch_actions = [batch_actions[i] for i in \\\n                                indices]\n        unpacked_elite_batch_obs = [item for items in \\\n                           elite_batch_obs for item in items]\n        unpacked_elite_batch_actions = [item for items in \\\n                       elite_batch_actions for item in items]\n        return np.array(unpacked_elite_batch_obs), \\\n               np.array(unpacked_elite_batch_actions), \\\n               reward_threshold\n    ```", "```\n    elite_obs, elite_actions, reward_threshold = gather_elite_xp(trajectories, elitism_criterion=75)\n    ```", "```\n    def gen_action_distribution(action_index, action_dim=5):\n        action_distribution = np.zeros(action_dim).\\\n                                   astype(type(action_index))\n        action_distribution[action_index] = 1\n        action_distribution = \\\n                       np.expand_dims(action_distribution, 0)\n        return action_distribution\n    ```", "```\n    elite_action_distributions = np.array([gen_action_distribution(a.item()) for a in elite_actions])\n    ```", "```\n    brain = Brain(env.action_space.n)\n    brain.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    ```", "```\n    elite_obs, elite_action_distributions = elite_obs.astype(\"float16\"), elite_action_distributions.astype(\"float16\")\n    brain.fit(elite_obs, elite_action_distributions, batch_size=128, epochs=1);\n    ```", "```\n    1/1 [==============================] - 0s 960us/step - loss: 0.8060 - accuracy: 0.4900\n    ```", "```\n    class Agent(object):\n        def __init__(self, brain):\n            \"\"\"Agent with a neural-network brain powered \n               policy\n            Args:\n                brain (keras.Model): Neural Network based \\\n                model\n            \"\"\"\n            self.brain = brain\n            self.policy = self.policy_mlp\n        def policy_mlp(self, observations):\n            observations = observations.reshape(1, -1)\n            action_logits = self.brain.process(observations)\n            action = tf.random.categorical(\n                   tf.math.log(action_logits), num_samples=1)\n            return tf.squeeze(action, axis=1)\n        def get_action(self, observations):\n            return self.policy(observations)\n    ```", "```\n    def evaluate(agent, env, render=True):\n        obs, episode_reward, done, step_num = env.reset(),\n                                              0.0, False, 0\n        while not done:\n            action = agent.get_action(obs)\n            obs, reward, done, info = env.step(action)\n            episode_reward += reward\n            step_num += 1\n            if render:\n                env.render()\n        return step_num, episode_reward, done, info\n    ```", "```\n    env = GridworldEnv()\n    agent = Agent(brain)\n    for episode in tqdm(range(10)):\n        steps, episode_reward, done, info = evaluate(agent,\n                                                     env)\n    env.close()\n    ```", "```\n    total_trajectory_rollouts = 70\n    elitism_criterion = 70  # percentile\n    num_epochs = 200\n    mean_rewards = []\n    elite_reward_thresholds = []\n    ```", "```\n    env = GridworldEnv()\n    input_shape = (env.observation_space.shape[0] * \\\n                   env.observation_space.shape[1], )\n    brain = Brain(env.action_space.n)\n    brain.compile(loss=\"categorical_crossentropy\",\n                  optimizer=\"adam\", metrics=[\"accuracy\"])\n    agent = Agent(brain)\n    for i in tqdm(range(num_epochs)):\n        trajectories = [Trajectory(*rollout(agent, env)) \\\n                   for _ in range(total_trajectory_rollouts)]\n        _, _, batch_rewards = zip(*trajectories)\n        elite_obs, elite_actions, elite_threshold = \\\n                       gather_elite_xp(trajectories, \n                       elitism_criterion=elitism_criterion)\n        elite_action_distributions = \\\n            np.array([gen_action_distribution(a.item()) \\\n                         for a in elite_actions])\n        elite_obs, elite_action_distributions = \\\n            elite_obs.astype(\"float16\"), \n            elite_action_distributions.astype(\"float16\")\n        brain.fit(elite_obs, elite_action_distributions, \n                  batch_size=128, epochs=3, verbose=0);\n        mean_rewards.append(np.mean(batch_rewards))\n        elite_reward_thresholds.append(elite_threshold)\n        print(f\"Episode#:{i + 1} elite-reward-\\\n              threshold:{elite_reward_thresholds[-1]:.2f} \\\n              reward:{mean_rewards[-1]:.2f} \")\n    plt.plot(mean_rewards, 'r', label=\"mean_reward\")\n    plt.plot(elite_reward_thresholds, 'g', \n             label=\"elites_reward_threshold\")\n    plt.legend()\n    plt.grid()\n    plt.show()\n    ```"]