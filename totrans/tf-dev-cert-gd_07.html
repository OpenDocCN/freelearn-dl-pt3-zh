<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer122">
<h1 class="chapter-number" id="_idParaDest-125"><a id="_idTextAnchor146"/>7</h1>
<h1 id="_idParaDest-126"><a id="_idTextAnchor147"/>Image Classification with Convolutional Neural Networks</h1>
<p><strong class="bold">Convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) are the go-to algorithms when it comes to image<a id="_idIndexMarker355"/> classification. In the 1960s, neuroscientists Hubel and Wiesel conducted a study on the visual cortex in cats and monkeys. Their work unraveled how we visually process information in a hierarchical structure, showing how visual systems are organized into a series of layers where each layer is responsible for a different aspect of visual processing. This earned them a Nobel Prize, but more importantly, it served as the basis upon which CNNs are built. CNNs, by virtue of their nature, are well designed to work with data with spatial structures such <span class="No-Break">as images.</span></p>
<p>However, in the early days, CNNs did not have the limelight due to a number of factors, such as insufficient training data, underdeveloped network architecture, insufficient computational resources, and the absence of modern techniques such as data augmentation and dropout. In the 2012 ImageNet Large Scale Visual Recognition Challenge, the <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) community <a id="_idIndexMarker356"/>was taken by storm when a CNN architecture called AlexNet outperformed all other methods by a large margin. Today, ML practitioners apply CNNs to achieve state-of-the-art performance on computer vision tasks such as image classification, image segmentation, and object detection, <span class="No-Break">among others.</span></p>
<p>In this chapter, we will examine CNNs to see how they do things differently from the fully connected neural networks we have used so far. We will start with the challenges faced by fully connected networks when working with image data, after which we will explore the anatomy of CNNs. We will look at the core building blocks of CNN architecture and their overall impact on the performance of the network. Next, we will build an image classifier using a CNN architecture with the Fashion MNIST dataset, then move on to building a real-world image classifier. We will be working with color images of different sizes, and our target objects are in different positions within <span class="No-Break">the image.</span></p>
<p>By the end of this chapter, you will have a sound understanding of what CNNs are and why they are superior to fully connected networks when it comes to image classification tasks. Also, you will be able to effectively build, train, tune, and test CNN models on real-world image <span class="No-Break">classification problems.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
<ul>
<li>The anatomy <span class="No-Break">of CNNs</span></li>
<li>Fashion MNIST <span class="No-Break">with CNNs</span></li>
<li><span class="No-Break">Real-world images</span></li>
<li>Weather <span class="No-Break">data classification</span></li>
<li>Applying hyperparameters to improve the <span class="No-Break">model’s performance</span></li>
<li>Evaluating <span class="No-Break">image classifiers</span></li>
</ul>
<h1 id="_idParaDest-127"><a id="_idTextAnchor148"/>Challenges of image recognition with fully connected networks</h1>
<p>In <a href="B18118_05.xhtml#_idTextAnchor105"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Image Classification with Neural Networks</em>, we<a id="_idIndexMarker357"/> applied a <strong class="bold">deep neural network</strong> (<strong class="bold">DNN</strong>) to the<a id="_idIndexMarker358"/> Fashion MNIST dataset. We saw how every neuron in the input layer is connected to every neuron in the hidden layer and those in the hidden layer are connected to neurons in the output layer, hence the name <em class="italic">fully connected</em>. While this architecture can solve many ML problems, they are not well suited for modeling image classification tasks, due to the spatial nature of image data. Let’s say you are looking at a picture of a face; the positioning and orientation of the features on the face enable you to know it is a human face even when you just focus on a specific feature, such as the eyes. Instinctively, you know it’s a face by virtue of the spatial relationship between the features of the face; however, DNNs do not see this bigger picture when looking at images. They process each pixel in the image as independent features, without taking the spatial relationships between these features <span class="No-Break">into consideration.</span></p>
<p>Another issue with using fully connected architectures is the curse of dimensionality. Let’s say we are working with a real-world image of size 150 x 150 with 3 color channels, <strong class="bold">red, green, and blue</strong> (<strong class="bold">RGB</strong>); we <a id="_idIndexMarker359"/>will have an input size of 67,500. As all the neurons are connected to neurons in the next layer, if we feed these values into a hidden layer with 500 neurons, we will have 67,500 x 500 = 33,750,000 parameters, and this number of parameters will grow exponentially as we add more layers, making it resource intensive to apply this type of network to image classification tasks. Another accompanying problem we could stumble upon is overfitting; this happens due to the large number of parameters in our network. If we have images of larger sizes or we add more neurons to our networks, the number of trainable parameters will grow exponentially, and it could become impractical to train such a network due to cost and resource requirements. In light of these challenges, there is a need for a more sophisticated architecture, and this is where CNNs come in with their ability to uncover spatial relationships and hierarchies, ensuring features are recognized irrespective of where they are located within <span class="No-Break">an image.</span></p>
<p class="callout-heading">Note</p>
<p class="callout"><strong class="bold">Spatial relationship</strong> refers to<a id="_idIndexMarker360"/> how features within an image are arranged in relation to each other in terms of position, distance, <span class="No-Break">and orientation.</span></p>
<h1 id="_idParaDest-128"><a id="_idTextAnchor149"/>Anatomy of CNNs</h1>
<p>In the last section, we <a id="_idIndexMarker361"/>saw some of the challenges DNNs grappled with when dealing with visual recognition tasks. These issues include the lack of spatial awareness, high dimensionality, computational inefficiency, and the risk of overfitting. How do we overcome these challenges? This is where CNNs come into the picture. CNNs by design are uniquely positioned to handle image data. Let's go through <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> and uncover why and how CNNs <span class="No-Break">stand out:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<img alt="Figure 7.1 – The anatomy of a CNN" height="796" src="image/B18118_07_01.jpg" width="1610"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The anatomy of a CNN</p>
<p>Let’s break down <a id="_idIndexMarker362"/>the different layers in <span class="No-Break">the diagram:</span></p>
<ol>
<li><strong class="bold">Convolutional layer – the eyes of the network</strong>: Our journey begins with us feeding in images into the convolutional layer; this layer can be viewed as the “eyes of our network.” Their job is primarily to extract vital features. Unlike DNNs, where each neuron is connected to every neuron in the next layer, CNNs apply filters (also known as kernels) to capture local patterns within an image in a hierarchical fashion. The output of the interactions between a segment of the input image that the filter slides over is called a feature map. As shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>, we can see that each feature map highlights specific patterns in the shirt that we passed into the network. Images go through CNNs in a hierarchical fashion with filters in the earlier layers adept at capturing simple features, while those in subsequent layers capture more complex patterns, mimicking the hierarchical structure of a human’s visual cortex. Another important property of CNNs is parameter sharing – this happens because patterns are only learned once and applied everywhere else across an image. This ensures that the visual ability of the model is not location-specific. In ML, we refer to this concept as <strong class="bold">translation invariance</strong> – the network’s ability to detect a shirt regardless of whether it is aligned to the right or left or centered within <span class="No-Break">an image.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer102">
<img alt="Figure 7.2 – Visualization of features captured by a convolutional layer" height="446" src="image/B18118_07_02.jpg" width="612"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Visualization of features captured by a convolutional layer</p>
<ol>
<li value="2"><strong class="bold">Pooling layer – the summarizer</strong>: After the convolutional layer comes the pooling layer. This <a id="_idIndexMarker363"/>layer can be viewed as a summarizer in CNNs as it focuses on condensing the overall dimensionality of the feature maps while retaining important features, as illustrated in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>. By methodically downsampling the feature maps, CNNs significantly not only reduce the number of parameters required for image processing but also improve the overall computational efficiency <span class="No-Break">of CNNs.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer103">
<img alt="Figure 7.3 – An example of the pooling operation, preserving essential details" height="467" src="image/B18118_07_03.jpg" width="910"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – An example of the pooling operation, preserving essential details</p>
<ol>
<li value="3"><strong class="bold">Fully connected layer – the decision maker</strong>: Our image traverses a series of convolution and pooling layers that extract features and reduce the dimensionality of feature maps, eventually reaching the fully connected layer. This layer can be viewed as the decision maker. This layer offers high-level reasoning as it brings together all the important details collected through the layers and uses them to make the final classification verdict. One of the hallmarks of CNNs is its end-to-end learning process, which seamlessly integrates feature extraction and image classification. This methodological and hierarchical learning approach makes CNN a well-suited tool for image recognition <span class="No-Break">and analysis.</span></li>
</ol>
<p>We have only scratched<a id="_idIndexMarker364"/> the surface of how CNNs work. Let's now drill down into the key operations that take place within the different layers, starting <span class="No-Break">with convolu<a id="_idTextAnchor150"/>tions.</span></p>
<h2 id="_idParaDest-129"><a id="_idTextAnchor151"/>Convolutions</h2>
<p>We now know that <a id="_idIndexMarker365"/>convolutional layers apply filters, which slide over patches of the input image. A typical CNN applies multiple filters, with each filter learning a specific kind of feature by interacting with the input image. By combining the detected features, a CNN arrives at a comprehensive understanding of the image features and uses this detailed information to classify the input image. Mathematically, this convolution process involves the dot product between a patch of the input image and the filter (a small matrix), as illustrated in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.4</em>. This process yields an output<a id="_idIndexMarker366"/> known as<a id="_idIndexMarker367"/> the <strong class="bold">activation map</strong> or <span class="No-Break"><strong class="bold">feature map</strong></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<img alt="Figure 7.4 – Convolution operation – applying a filter to an input image to generate a feature map" height="601" src="image/B18118_07_04.jpg" width="817"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Convolution operation – applying a filter to an input image to generate a feature map</p>
<p>As the filter slides<a id="_idIndexMarker368"/> over the patches of the image, it produces a feature map for each dot operation. Feature maps are a representation of the input image in which certain visual patterns are enhanced by the filter, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.2</em>. When we stack feature maps from all the filters in the network, we arrive at a rich, multi-faceted view of the input image, which gives later layers adequate information to learn more <span class="No-Break">complex patterns.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<img alt="Figure 7.5  – a (top) and b (bottom): Dot product computation" height="516" src="image/B18118_07_05.jpg" width="716"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5  – a (top) and b (bottom): Dot product computation</p>
<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5 a</em>, we see a dot product operation in progress, as a filter slides over a section of the input image resulting in a destination pixel value of 13. If we move the filter 1 pixel to the right, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5 b</em>, we will arrive at the next destination pixel value of 14. If we continue sliding the filter one pixel at a time over the input image, we will achieve the complete output shown in <span class="No-Break"><em class="italic">Figure </em></span><span class="No-Break"><em class="italic">7</em></span><span class="No-Break"><em class="italic">.5 b</em></span><span class="No-Break">.</span></p>
<p>We have now seen how convolution operations work; however, there are various types of convolutional layers that we can apply in CNNs. For image classification, we typically use 2D convolutional layers, while we apply 1D convolution layers for audio processing and 3D convolutional layers for video processing. When designing our convolutional layer, there are a number of adjustable hyperparameters that can impact the performance of our network, such as the number of filters, the size of the filters, stride, and padding. It is <a id="_idIndexMarker369"/>pertinent to explore how these hyperparameters impact <span class="No-Break">our network.</span></p>
<p>Let’s begin this exploration by looking at the impact of the number of filters in a <span class="No-Break">convolutional layer.</span></p>
<h2 id="_idParaDest-130">Impact of the number of <a id="_idTextAnchor152"/>filters</h2>
<p>By increasing the<a id="_idIndexMarker370"/> number of filters within a CNN, we empower it to learn a richer and more diverse representation of the input image. The more filters we have, the more representation will be learned. However, more filters mean more parameters to train, and this could not only increase the computational cost but also slow down the training process and increase the risk of overfitting. When deciding on the number of filters to apply to your network, it is important to consider the type of data in use. If the data has a lot of variability, you may need more filters to capture the diversity in your data, whereas with smaller <a id="_idIndexMarker371"/>datasets, you should be more conservative to reduce the risk <span class="No-Break">of overfitting.</span></p>
<h2 id="_idParaDest-131"><a id="_idTextAnchor153"/>Impact of the size of the filter</h2>
<p>We now know that filters <a id="_idIndexMarker372"/>are small matrices that slide over our input image to produce feature maps. The size of the filter we apply to the input image will determine the level and type of features that will be extracted from the input image. The filter size is the dimension of the filter – that is, the height and width of the filter matrix. Typically, you will come across 3x3, 5x5, and 7x7 filters. Smaller filters will cover a smaller patch of the input image, while a larger filter will cover a more extensive section of the <span class="No-Break">input image:</span></p>
<ul>
<li><strong class="bold">Granularity of features</strong> – Smaller filters such as 3x3 filters can be applied to capture finer and more local details of an image such as edges, textures, and corners, while larger filters such as 7x7 filters can learn broader patterns such as face shapes or <span class="No-Break">object parts.</span></li>
<li><strong class="bold">Computational efficiency</strong> – Smaller filters cover a smaller receptive field of the input image, as illustrated in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.6</em>, which means they will require <span class="No-Break">more operations.</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer106">
<img alt="Figure 7.6 – Convolution operation with a 3x3 filter" height="435" src="image/B18118_07_06.jpg" width="945"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Convolution operation with a 3x3 filter</p>
<p>On the other hand, a larger filter covers a large segment of the input image, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.7</em>. However, many modern CNN architectures (for example, VGG) use 3x3 filters. Stacking these smaller filters together would increase the depth of the network and enhance the capabilities of these filters to capture more complex <a id="_idIndexMarker373"/>patterns with a smaller number of parameters in comparison to using a large filter, which makes smaller filters easier <span class="No-Break">to train.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer107">
<img alt="Figure 7.7 – Convolution operation with a 5x5 filter" height="317" src="image/B18118_07_07.jpg" width="743"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Convolution operation with a 5x5 filter</p>
<ul>
<li><strong class="bold">Parameter count</strong> – A larger filter typically has more weight in comparison to a smaller filter; for example, a 5x5 filter will have 25 parameters and a 3x3 filter will have 9 parameters. Here, we are ignoring the depth for the sake of simplicity. Hence, larger filters will contribute to making the model more complex in <a id="_idIndexMarker374"/>comparison to <span class="No-Break">smaller filters.</span></li>
</ul>
<h2 id="_idParaDest-132">Impact<a id="_idTextAnchor154"/><a id="_idTextAnchor155"/> of stride</h2>
<p>Stride is an important<a id="_idIndexMarker375"/> hyperparameter in CNNs. It determines the number of pixels a filter moves over an input image. We can liken stride to the step we take when walking; if we take small steps, it will take us a longer time to reach our destination, while larger steps will ensure we reach it much quicker. In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.8</em>, we apply a stride of 1, which means the filter moves over the input image 1 pixel at <span class="No-Break">a time.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<img alt="Figure 7.8 – Convolution operation with a stride of 1" height="429" src="image/B18118_07_08.jpg" width="940"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Convolution operation with a stride of 1</p>
<p>If we apply a stride of 2, it means the filter will move 2 pixels at a time, as illustrated in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.9</em>. We see that a large stride will lead to a reduced spatial dimension of the output feature map. We can see this when we compare the output of <span class="No-Break">both figures.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<img alt="Figure 7.9 – Convolution operation with a stride of 2" height="467" src="image/B18118_07_09.jpg" width="940"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Convolution operation with a stride of 2</p>
<p>When we apply a larger stride, it can increase the computational efficiency, but it also reduces the spatial <a id="_idIndexMarker376"/>resolution of the input image. Hence, we need to consider this trade-off when selecting the right stride for our network. Next, let's examine the <span class="No-Break">border effect.</span></p>
<h2 id="_idParaDest-133">The bounda<a id="_idTextAnchor156"/>ry problem</h2>
<p>When a filter slides <a id="_idIndexMarker377"/>over the input image performing convolution operations, it soon reaches the borders or the edges, where it becomes difficult to perform dot product operations due to the absence of pixels outside the image boundaries. This results in the output feature map being smaller than the input image as a result of loss of information around the edges or borders. This issue is referred to <a id="_idIndexMarker378"/>as the <strong class="bold">edge effect</strong> or the <strong class="bold">boundary problem</strong> in ML. In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.10</em>, we can<a id="_idIndexMarker379"/> observe that we are unable to perform a dot product operation on the bottom-left corner as we cannot center the filter over the highlighted pixel value of 3 without some part of the filter falling out of the defined <span class="No-Break">image boundary.</span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer110">
<img alt="Figure 7.10 – Showing the boundary problem" height="726" src="image/B18118_07_10.jpg" width="1235"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – Showing the boundary problem</p>
<p>To fix the boundary issue and preserve the spatial dimension of the output feature map, we may want to apply <a id="_idIndexMarker380"/>padding. Let's discuss this <span class="No-Break">concept next.</span></p>
<h2 id="_idParaDest-134"><a id="_idTextAnchor157"/>Impact of padding</h2>
<p><strong class="bold">Padding</strong> is a technique<a id="_idIndexMarker381"/> we can apply<a id="_idIndexMarker382"/> to our convolution process to prevent the boundary effect by adding extra pixels to the edges, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<img alt="Figure 7.11 – A padded image undergoing convolution operation" height="486" src="image/B18118_07_11.jpg" width="1013"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – A padded image undergoing convolution operation</p>
<p>We can now perform dot product operations on pixels at the edges, hence preserving information at the edges. Padding can also be applied to maintain the spatial dimension pre- and post-convolution. This could prove useful in deep CNN architecture with several convolution layers. Let’s look at the two main types <span class="No-Break">of padding:</span></p>
<ul>
<li><strong class="bold">Valid Padding (No Padding)</strong>: Here, no<a id="_idIndexMarker383"/> padding is applied. This can be useful when we want to achieve a reduced spatial dimensionality, especially in <span class="No-Break">deeper layers.</span></li>
<li><strong class="bold">Same Padding</strong>: Here, we set padding to ensure the output feature map and the input image dimensions are the same. We use this when maintaining spatial dimensionality is of <span class="No-Break">paramount importance.</span></li>
</ul>
<p>Before we move on to <a id="_idIndexMarker384"/>examining <a id="_idIndexMarker385"/>the pooling layer, let's put together the different hyperparameters we have discussed in the convolutional layer and se<a id="_idTextAnchor158"/>e them <span class="No-Break">in action.</span></p>
<h2 id="_idParaDest-135"><a id="_idTextAnchor159"/>Putting it all together</h2>
<p>In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.12</em>, we have a 7x7 input image and a 3x3 filter. Here, we use a stride of 1 and set padding to Valid (<span class="No-Break">no padding).</span></p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<img alt="Figure 7.12 – Setting the hyperparameters" height="310" src="image/B18118_07_12.jpg" width="739"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – Setting the hyperparameters</p>
<p>To compute the output feature map of a convolutional operation, we can apply the <span class="No-Break">following formula:</span></p>
<p><span class="_-----MathTools-_Math_Base">(</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">W</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">−</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">F</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">2</span><span class="_-----MathTools-_Math_Variable">P</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">_</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable">S</span><span class="_-----MathTools-_Math_Variable"> </span><span class="_-----MathTools-_Math_Base">)</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator">+</span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number">1</span></p>
<p>In this formula, the <span class="No-Break">following applies:</span></p>
<ul>
<li><em class="italic">W</em> represents the size of the <span class="No-Break">input image</span></li>
<li><em class="italic">F</em> stands for the <span class="No-Break">filter size</span></li>
<li><em class="italic">S</em> <span class="No-Break">represents stride</span></li>
<li><em class="italic">P</em> stands <span class="No-Break">for padding</span></li>
</ul>
<p>When we input the respective values into the equation, we get a resulting value of 5, which means we will have a 5x5 output feature map. If we alter any of the values, it will impact the size of the output feature map one way or the other. For example, if we increase the stride size, we will have a smaller output feature map, while if we set padding to same, this will increase the size of the output. We can now move on from the convolution operations and explo<a id="_idTextAnchor160"/>re <span class="No-Break">pooling next.</span></p>
<h1 id="_idParaDest-136"><a id="_idTextAnchor161"/>Pooling</h1>
<p><strong class="bold">Pooling</strong> is an important<a id="_idIndexMarker386"/> operation that takes play in the pooling layer of a CNN. It is a technique used to downsample the spatial dimension of individual feature maps generated by the convolutional layers. Let's examine some important types of pooling layers. We’ll begin by exploring max pooling, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.13</em>. Here, we see how max pooling operations work. The pooling layer simply takes the highest value from each region of the <span class="No-Break">input data.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer113">
<img alt="Figure 7.13 – A max pooling operation" height="305" src="image/B18118_07_13.jpg" width="644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – A max pooling operation</p>
<p>Max pooling enjoys several benefits as it is intuitive and easy to implement. It is also efficient since it simply extracts the highest value in a region, and it has been applied with good effect across <span class="No-Break">diverse tasks.</span></p>
<p>Average pooling, as<a id="_idIndexMarker387"/> the name suggests, reduces the data dimensionality by taking the average value for a designated region, as illustrated in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer114">
<img alt="Figure 7.14 – An average pooling operation" height="315" src="image/B18118_07_14.jpg" width="646"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – An average pooling operation</p>
<p>On the other hand, min pooling extracts the minimum value in a specified region of the input data. Pooling reduces the spatial size of the output feature maps and this, in turn, reduces the memory requirement for storing intermediate representations. Pooling can be beneficial to a <a id="_idIndexMarker388"/>network; however, excessive pooling can be counterproductive as this could lead to information loss. After the pooling layer, we arrive at the fully connected layer, the decision maker of <span class="No-Break">our network.</span></p>
<h2 id="_idParaDest-137">The full<a id="_idTextAnchor162"/>y connected layer</h2>
<p>The final component<a id="_idIndexMarker389"/> of our CNN architecture is the fully connected layer. Unlike the convolutional layer, here, every neuron is connected to every neuron in the next layer. This layer is responsible for decision-making, such as classifying whether our input image is a shirt or a hat. The fully connected layer takes the learned features from the earlier layers and maps them to their corresponding labels. We have now covered CNNs in theory; let's now proceed by applying them to our <span class="No-Break">fashion dataset.</span></p>
<h1 id="_idParaDest-138"><a id="_idTextAnchor163"/>Fashion MNIST 2.0</h1>
<p>By now, you are already <a id="_idIndexMarker390"/>familiar with this dataset, as we used it in <a href="B18118_05.xhtml#_idTextAnchor105"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, <em class="italic">Image Classification with Neural Networks</em>, and <a href="B18118_06.xhtml#_idTextAnchor129"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Improving the Model</em>. Now, let's see how CNNs compare to the simple neural networks we have worked with so far. We will continue in the same spirit as before. We start by importing the <span class="No-Break">required libraries:</span></p>
<ol>
<li>We will import the requisite libraries for preprocessing, modeling, and visualizing our ML model <span class="No-Break">using TensorFlow:</span><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre></li>
<li>Next, we will load the Fashion MNIST dataset from TensorFlow Datasets using the <strong class="source-inline">load_data()</strong> function. This function returns our training and testing data consisting of NumPy arrays. The training data consists of <strong class="source-inline">x_train</strong> and <strong class="source-inline">y_train</strong>, and the test data is made up of  <strong class="source-inline">x_test</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">y_test</strong></span><span class="No-Break">:</span><pre class="source-code">
(x_train,y_train),(x_test,y_test) = tf.keras.datasets.fashion_mnist.load_data()</pre></li>
<li>We can confirm the data size by using the <strong class="source-inline">len</strong> function on our training and <span class="No-Break">testing data:</span><pre class="source-code">
len(x_train), len(x_test)</pre></li>
</ol>
<p>When we run the code, we get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
(60000, 10000)</pre>
<p>We can see that we have a training data size of 60,000 images and test data of <span class="No-Break">10,000 images.</span></p>
<ol>
<li value="4">In CNNs, unlike the DNNs we used previously, we need to account for the color channels of the input images. Currently, our training and testing data has a shape of <strong class="source-inline">(batch_size, height, width)</strong> for grayscale images, with a single channel. However, CNN models require a 4D input tensor, made up of <strong class="source-inline">batch_size</strong>, <strong class="source-inline">height</strong>, <strong class="source-inline">width</strong>, and <strong class="source-inline">channels</strong>. We can fix this data mismatch by simply reshaping our data and converting the elements to <span class="No-Break"><strong class="source-inline">float32</strong></span><span class="No-Break"> values:</span><pre class="source-code">
# Reshape the images(batch_size, height, width, channels)</pre><pre class="source-code">
x_train = x_train.reshape(x_train.shape[0],</pre><pre class="source-code">
    28, 28, 1).astype('float32')</pre><pre class="source-code">
x_test = x_test.reshape(x_test.shape[0],</pre><pre class="source-code">
    28, 28, 1).astype('float32')</pre></li>
</ol>
<p>This preprocessing<a id="_idIndexMarker391"/> step is standard before training an ML model, as most models require floating-point input. Since our images are grayscale, there is only one color channel, which is why we reshape the data to include a single <span class="No-Break">channel dimension.</span></p>
<ol>
<li value="5">The pixel value of our data (training and testing data) ranges from <strong class="source-inline">0</strong> to <strong class="source-inline">255</strong>, where <strong class="source-inline">0</strong> represents black and <strong class="source-inline">255</strong> represents white. We normalize our data by dividing the pixel values by 255 to bring the pixel values in our data to a scale of between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. We do this to enable our model to converge faster and <span class="No-Break">perform better:</span><pre class="source-code">
# Normalize the pixel values</pre><pre class="source-code">
x_train /= 255</pre><pre class="source-code">
x_test /= 255</pre></li>
<li>We use the <strong class="source-inline">to_categorical</strong> function from the <strong class="source-inline">utils</strong> module of <strong class="source-inline">tf.keras</strong> to convert our labels (<strong class="source-inline">y_train</strong> and <strong class="source-inline">y_test</strong>) that have an integer value of <strong class="source-inline">0</strong> to <strong class="source-inline">9</strong> into one-hot encoded arrays. The <strong class="source-inline">to_categorical</strong> function takes two arguments: the labels to be converted, and the number of classes; it returns a one-hot encoded array, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.15</em></span><span class="No-Break">.</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer115">
<img alt="Figure 7.15 – A one-hot encoded array" height="248" src="image/B18118_07_15.jpg" width="753"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – A one-hot encoded array</p>
<p>The one-hot encoded vectors will have a length of <strong class="source-inline">10</strong>, with a number <strong class="source-inline">1</strong> in the index that corresponds<a id="_idIndexMarker392"/> to the label for a given data point, and <strong class="source-inline">0</strong> in all <span class="No-Break">other indices:</span></p>
<pre class="source-code">
# Convert the labels to one hot encoding format
y_train = tf.keras.utils.to_categorical(y_train, 10)
y_test = tf.keras.utils.to_ca<a id="_idTextAnchor164"/>tegorical(y_test, 10)</pre>
<ol>
<li value="7">Using the Sequential Model API from <strong class="source-inline">tf.keras.model</strong>, we will create a <span class="No-Break">CNN architecture:</span><pre class="source-code">
# Build the Sequential model</pre><pre class="source-code">
model = tf.keras.models.Sequential()</pre><pre class="source-code">
# Add convolutional layer</pre><pre class="source-code">
model.add(tf.keras.layers.Conv2D(64,kernel_size=(3,3),</pre><pre class="source-code">
    activation='relu',</pre><pre class="source-code">
    input_shape=(28, 28, 1)))</pre><pre class="source-code">
# Add max pooling layer</pre><pre class="source-code">
model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))</pre><pre class="source-code">
# Flatten the data</pre><pre class="source-code">
model.add(tf.keras.layers.Flatten())</pre><pre class="source-code">
# Add fully connected layer</pre><pre class="source-code">
model.add(tf.keras.layers.Dense(128,</pre><pre class="source-code">
                                activation='relu'))</pre><pre class="source-code">
# Apply softmax</pre><pre class="source-code">
model.add(tf.keras.layers.Dense(10,</pre><pre class="source-code">
                                activation='softmax'))</pre></li>
</ol>
<p>The first layer is a convolution layer composed of 64 filters of size 3x3 to process the input images, which have a shape of 28x28 pixels and 1 channel (grayscale). ReLU is used as the <a id="_idIndexMarker393"/>activation function. The subsequent max pooling layer is a 2D pooling layer that applies max pooling to downsample the output of the convolution layer, reducing the dimensionality of the feature maps. The <strong class="source-inline">flatten</strong> layer takes the output of the pooling layer and flattens it into a 1D array, which is then processed by the fully connected layer. The output layer contains <strong class="source-inline">softmax</strong> activation for multiclass classification and 10 neurons, one for <span class="No-Break">each class.</span></p>
<ol>
<li value="8">Next, we compile and fit the model on our <span class="No-Break">training data:</span><pre class="source-code">
# Compile and fit the model</pre><pre class="source-code">
model.compile(loss='categorical_crossentropy',</pre><pre class="source-code">
              optimizer='adam', metrics=['accuracy'])</pre><pre class="source-code">
model.fit(x_train, y_train, epochs=10,</pre><pre class="source-code">
          validation_split=0.2)</pre></li>
</ol>
<p>The <strong class="source-inline">compile()</strong> function takes three arguments: loss function (<strong class="source-inline">categorical_crossentropy</strong>, since this is a multi-class classification task), optimizer (<strong class="source-inline">adam</strong>), and metrics (<strong class="source-inline">accuracy</strong>). After compiling the model, we used the <strong class="source-inline">fit()</strong> function to train the model on the training data. We specified the number of epochs as <strong class="source-inline">10</strong> and used 20% of the training data for <span class="No-Break">validation purposes.</span></p>
<p>In 10 epochs, we arrive<a id="_idIndexMarker394"/> at a training accuracy of <strong class="source-inline">0.9785</strong> and a validation accuracy <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.9133</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
Epoch 6/10
1500/1500 [==============================] - 5s 3ms/step - loss: 0.1267 - accuracy: 0.9532 - val_loss: 0.2548 - val_accuracy: 0.9158
Epoch 7/10
1500/1500 [==============================] - 5s 4ms/step - loss: 0.1061 - accuracy: 0.9606 - val_loss: 0.2767 - val_accuracy: 0.9159
Epoch 8/10
1500/1500 [==============================] - 6s 4ms/step - loss: 0.0880 - accuracy: 0.9681 - val_loss: 0.2957 - val_accuracy: 0.9146
Epoch 9/10
1500/1500 [==============================] - 6s 4ms/step - loss: 0.0697 - accuracy: 0.9749 - val_loss: 0.3177 - val_accuracy: 0.9135
Epoch 10/10
1500/1500 [==============================] - 6s 4ms/step - loss: 0.0588 - accuracy: 0.9785 - val_loss: 0.3472 - val_accuracy: 0.9133</pre>
<ol>
<li value="9">The <strong class="source-inline">summary</strong> function is a very useful way to get a high-level overview of the model’s architecture and understand the number of parameters and the shape of the <span class="No-Break">output tensors:</span><pre class="source-code">
model.summary()</pre></li>
</ol>
<p>The output returns the five layers that make up our current model architecture. It also displays the output shape and the number of parameters of each layer. The total number of parameters is 1,386,506. From the output, we see that the output shape from the convolution layer is 26x26 as a result of the border effect since we did not apply padding. Next, the max pooling layer halves the pixel size after <a id="_idIndexMarker395"/>which we flatten the data and <span class="No-Break">generate predictions:</span></p>
<pre class="source-code">
Model: "sequential"
______________________________________________________
 Layer (type)             Output Shape         Param #
======================================================
 conv2d (Conv2D)          (None, 26, 26,64)    640
 max_pooling2d (MaxPooling2D  (None,13,13,64)  0    )
 flatten (Flatten)            (None, 10816)    0
 dense (Dense)                (None, 128)      1384576
 dense_1 (Dense)              (None, 10)       1290
======================================================
Total params: 1,386,506
Trainable params: 1,386,506
Non-trainable params: 0
_________________________________________________________________</pre>
<ol>
<li value="10">Finally, we will use the <strong class="source-inline">evaluate</strong> function to evaluate our model on test data. The <strong class="source-inline">evaluate</strong> function returns the loss and the accuracy of the model on <span class="No-Break">test data:</span><pre class="source-code">
# Evaluate the model</pre><pre class="source-code">
score = model.evaluate(x_test, y_test)</pre></li>
</ol>
<p>Our model achieved an accuracy of <strong class="source-inline">0.9079</strong> on the test data, surpassing the performance of the architectures used in <a href="B18118_06.xhtml#_idTextAnchor129"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">, Improving the Model</em>. We can try to further improve the model’s performance <a id="_idIndexMarker396"/>by adjusting the hyperparameters and applying data augmentation. Let's turn our attention to real-world images, where CNNs clearly outsh<a id="_idTextAnchor165"/>ine our <span class="No-Break">previous models.</span></p>
<h1 id="_idParaDest-139"><a id="_idTextAnchor166"/>Working with real-world images</h1>
<p>Real-world images pose <a id="_idIndexMarker397"/>a different type of challenge as these images are usually colored images with three color channels (red, green, and blue), unlike the grayscale images we used from our fashion MNIST dataset. In <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.16</em>, where we see an example of real-world images from the weather dataset that we will be modeling shortly, you will notice the images are of varying sizes. This introduces another layer of complexity that requires additional preprocessing steps such as resizing or cropping to ensure all our images are of uniform dimensions before we feed them into our <span class="No-Break">neural network.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<img alt="Figure 7.16 – Images from the weather dataset" height="167" src="image/B18118_07_16.jpg" width="851"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Images from the weather dataset</p>
<p>Another issue we may encounter when working with real-world images is the presence of various noise sources. For example, we may have images in our dataset taken in conditions with uneven lighting or unintended blurring. Again, we could have images with multiple objects or other unintended distractions in the background among the images in our <span class="No-Break">real-world dataset.</span></p>
<p>To address these issues, we could apply noise reduction techniques such as denoising to improve the quality of our data. We could also use object detection techniques such as bounding boxes or segmentation to help us identify the target object within an image with multiple objects. The good part is TensorFlow is well equipped with a comprehensive set of tools tailored to handling these challenges. One important tool from TensorFlow is the <strong class="source-inline">tf.image</strong> module, which offers an array of image preprocessing functionalities such as resizing various adjustments (for example, brightness, contrast, hue, and saturation), application of bounding boxes, cropping, flipping, and <span class="No-Break">much more.</span></p>
<p>However, this module is beyond the scope of this book and the exam itself. But, if you wish to learn more about this module, you can visit the TensorFlow documentation at <a href="https://www.tensorflow.org/api_docs/python/tf/image">https://www.tensorflow.org/api_docs/python/tf/image</a>. Another tool in TensorFlow’s arsenal is <strong class="source-inline">ImageDataGenerator</strong>, which enables us to perform data augmentation on the fly, offering us the ability to preprocess and perform augmentative actions (such as rotation and flipping images) in real time as we feed these images into our training pipeline. Let's proceed<a id="_idIndexMarker398"/> to work with our real-world image dataset and see <strong class="source-inline">ImageDataGenerator</strong> <span class="No-Break">in action.</span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor167"/>Weather dataset classification</h2>
<p>In this case study, we<a id="_idIndexMarker399"/> will be working as a computer vision consultant for an emerging start-up called WeatherBIG. You have been assigned the responsibility of developing an image classification system that will be used to identify different weather conditions; the dataset for this task can be found on Kaggle using this link: <a href="https://www.kaggle.com/datasets/rahul29g/weatherdataset">https://www.kaggle.com/datasets/rahul29g/weatherdataset</a>. The dataset has been packaged into three folders made up of a training folder, a validation folder, and a testing folder. Each of these folders has subfolders with each weather class. Let’s get started with <span class="No-Break">the task:</span></p>
<ol>
<li>We start by importing several libraries to build our <span class="No-Break">image classifier:</span><pre class="source-code">
import os</pre><pre class="source-code">
import pathlib</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><pre class="source-code">
import matplotlib.image as mpimg</pre><pre class="source-code">
import random</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
import tensorflow as tf</pre><pre class="source-code">
from tensorflow import keras</pre><pre class="source-code">
from tensorflow.keras.preprocessing.imag<a id="_idTextAnchor168"/>e import ImageDataGenerator</pre></li>
</ol>
<p>We have used several of these libraries in our previous experiments; however, let's address the functionalities of a few libraries that we will be using for the first time. The <strong class="source-inline">os</strong> module acts as a bridge to our operating system. It gives us the ability to read from and write to our filesystem, while <strong class="source-inline">pathlib</strong> offers us an intuitive, object-oriented way to streamline our file navigation tasks. For image manipulation, we<a id="_idIndexMarker400"/> use <strong class="source-inline">PIL</strong>, and we also have the <strong class="source-inline">ImageDataGenerator</strong> class from the <strong class="source-inline">tensorflow.keras.preprocessing.image</strong> module for our data preprocessing steps, batch generati<a id="_idTextAnchor169"/>on, and <span class="No-Break">data augmentation.</span></p>
<ol>
<li value="2">You can access/download the dataset for this case study from <a href="https://www.kaggle.com/datasets/rahul29g/weatherdataset">https://www.kaggle.com/datasets/rahul29g/weatherdataset</a> and upload it to Google Drive. Once you do this, you can easily follow along with the code in this section. In my case, the data is stored in this root directory: <strong class="source-inline">/content/drive/MyDrive/weather dataset</strong>. In your case, your root directory will be different, so make sure you change the directory path to match the directory where the dataset is stored in your Google Drive: <strong class="source-inline">root_dir = "/</strong><span class="No-Break"><strong class="source-inline">content/drive/MyDrive/weather dataset"</strong></span><span class="No-Break">.</span></li>
<li>Next, we apply the <strong class="source-inline">os.walk</strong> function to access the root directory and generate information about the content of all the directories <span class="No-Break">and subdirectories:</span><pre class="source-code">
for dirpath, dirnames, filenames in os.walk(root_dir):</pre><pre class="source-code">
    print(f"Directory: {dirpath}")</pre><pre class="source-code">
    print(f"Number of images: {len(filenames)}")</pre><pre class="source-code">
    print()</pre></li>
</ol>
<p>Running the code returns a tuple made up of the path of each directory and the number of images within each of them, as illustrated in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.17</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<img alt="Figure 7.17 – A snapshot directory and subdirectories" height="371" src="image/B18118_07_17.jpg" width="617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – A snapshot directory and subdirectories</p>
<p>We use this step to <a id="_idIndexMarker401"/>get a sense of the contents of each<a id="_idTextAnchor170"/> directory <span class="No-Break">and subdirectory.</span></p>
<ol>
<li value="4">We use the <strong class="source-inline">retrieve_labels</strong> function to fetch and display labels and their corresponding counts from the training, test, and validation directories. To craft this function, we use the <strong class="source-inline">listdir</strong> method from the <strong class="source-inline">os</strong> module and we pass in the respective directory paths (<strong class="source-inline">train_<a id="_idTextAnchor171"/>dir</strong>, <strong class="source-inline">test_dir</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">val_dir</strong></span><span class="No-Break">):</span><pre class="source-code">
def retrieve_labels(train_dir, test_dir, val_dir):</pre><pre class="source-code">
    # Retrieve labels from training directory</pre><pre class="source-code">
    train_labels = os.listdir(train_dir)</pre><pre class="source-code">
    print(f"Training labels: {train_labels}")</pre><pre class="source-code">
    print(f"Number of training labels: {len(train_labels)}")</pre><pre class="source-code">
    print()</pre><pre class="source-code">
    # Retrieve labels from test directory</pre><pre class="source-code">
    test_labels = os.listdir(test_dir)</pre><pre class="source-code">
    print(f"Test labels: {test_labels}")</pre><pre class="source-code">
    print(f"Number of test labels: {len(test_labels)}")</pre><pre class="source-code">
    print()</pre><pre class="source-code">
    # Retrieve labels from validation directory</pre><pre class="source-code">
    val_labels = os.listdir(val_dir)</pre><pre class="source-code">
    print(f"Validation labels: {val_labels}")</pre><pre class="source-code">
    print(f"Number of validation labels: {len(val_labels)}")</pre><pre class="source-code">
    print()</pre></li>
<li>We specify the<a id="_idIndexMarker402"/> path to the training, test, and validation directories in the <strong class="source-inline">train_dir</strong>, <strong class="source-inline">test_dir</strong>, and <strong class="source-inline">val_dir</strong> <span class="No-Break">arguments, respectively:</span><pre class="source-code">
train_dir = "/content/drive/MyDrive/weather dataset/train"</pre><pre class="source-code">
test_dir = "/content/drive/MyDrive/weather dataset/test"</pre><pre class="source-code">
val_dir = "/content/drive/MyDrive/weather dataset/validation"</pre><pre class="source-code">
retrieve_labels(train_dir, test_dir, val_dir)</pre></li>
</ol>
<p>When we run the code, it returns the training data, test data, validation data labels, and the number <span class="No-Break">of labels:</span></p>
<pre class="source-code">
Training labels: ['cloud', 'shine', 'rain', 'sunrise']
Number of training labels: 4
Test labels: ['sunrise', 'shine', 'cloud', 'rain']
Number of test labels: 4
Validation labels: ['shine', 'sunrise', 'cloud', 'rain']
Number of validation labels: 4</pre>
<ol>
<li value="6">For our exploration, let's craft a function called <strong class="source-inline">view_random_images</strong> to randomly access and display images from the subdirectories within our dataset. The function takes in the main directory that holds the subdirectories housing our images and the number of images we want to display. We apply <strong class="source-inline">listdir</strong> to access the subdirectories and to introduce randomness in the selection process. We <a id="_idIndexMarker403"/>use the <strong class="source-inline">shuffle</strong> function from the <strong class="source-inline">random</strong> library for shuffling and selecting images randomly. Matplotlib is used to display the specified number of<a id="_idTextAnchor172"/> random images in <span class="No-Break">our function:</span><pre class="source-code">
def view_random_images(target_dir, num_images):</pre><pre class="source-code">
  """</pre><pre class="source-code">
  View num_images random images from the subdirectories of target_dir as a subplot.</pre><pre class="source-code">
  """</pre><pre class="source-code">
  # Get list of subdirectories</pre><pre class="source-code">
    subdirs = [d for d in os.listdir(</pre><pre class="source-code">
        target_dir) if os.path.isdir(os.path.join(</pre><pre class="source-code">
            target_dir, d))]</pre><pre class="source-code">
  # Select num_images random subdirectories</pre><pre class="source-code">
    random.shuffle(subdirs)</pre><pre class="source-code">
    selected_subdirs = subdirs[:num_images]</pre><pre class="source-code">
  # Create a subplot</pre><pre class="source-code">
    fig, axes = plt.subplots(1, num_images, figsize=(15,9))</pre><pre class="source-code">
    for i, subdir in enumerate(selected_subdirs):</pre><pre class="source-code">
      # Get list of images in subdirectory</pre><pre class="source-code">
        image_paths = [f for f in os.listdir(</pre><pre class="source-code">
            os.path.join(target_dir, subdir))]</pre><pre class="source-code">
      # Select a random image</pre><pre class="source-code">
        image_path = random.choice(image_paths)</pre><pre class="source-code">
      # Load image</pre><pre class="source-code">
        image = plt.imread(os.path.join(target_dir,</pre><pre class="source-code">
            subdir, image_path))</pre><pre class="source-code">
      # Display image in subplot</pre><pre class="source-code">
        axes[i].imshow(image)</pre><pre class="source-code">
        axes[i].axis("off")</pre><pre class="source-code">
        axes[i].set_title(subdir)</pre><pre class="source-code">
    print(f"Shape of image: {image.shape}")    </pre><pre class="source-code">
    #width,height, colour chDNNels</pre><pre class="source-code">
    plt.show()</pre></li>
<li>Let's try out<a id="_idIndexMarker404"/> the function by setting <strong class="source-inline">num_images</strong> to <strong class="source-inline">4</strong> and examine some data in our <span class="No-Break"><strong class="source-inline">train</strong></span><span class="No-Break"> directory:</span><pre class="source-code">
view_random_images(target_dir="/content/drive/MyDrive/weather dataset/train/", num_images=4)</pre></li>
</ol>
<p>This returns four randomly selected images, as <span class="No-Break">illustrated here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<img alt="7.18 – Randomly selected images from the weather dataset" height="166" src="image/B18118_07_18.jpg" width="851"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">7.18 – Randomly selected images from the weather dataset</p>
<p>From the data displayed, we can see the images come in various sizes (height and weight) and we will need to fix<a id="_idIndexMarker405"/> this preprocessing issue. We will be using the <strong class="source-inline">ImageDataGenerator</strong> class from TensorFlow. Let's discuss <span class="No-Break">this<a id="_idTextAnchor173"/> next.</span></p>
<h2 id="_idParaDest-141"><a id="_idTextAnchor174"/>Image data preprocessing</h2>
<p>We saw, in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.18</em>, that<a id="_idIndexMarker406"/> our training images are of different sizes. Here, we will be resizing and normalizing our data before training. Also, we want to develop an efficient method of loading our training data in batches, ensuring optimized memory usage with seamless integration with our model’s training process. To achieve all of this, we will be utilizing the <strong class="source-inline">ImageDataGenerator</strong> class from the <strong class="source-inline">TensorFlow.keras.preprocessing.image</strong> module. In <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Handling Overfitting,</em> we will take our application of <strong class="source-inline">ImageDataGenerator</strong> further by using it to enlarge our training dataset by developing variants of our image data by rotating, flipping, and zooming. This could help our model become more robust and reduce the risk <span class="No-Break">of overfitting.</span></p>
<p>Another useful tool to aid our data preprocessing task is the <strong class="source-inline">flow_from_directory</strong> method. We can use this method to build data pipelines. It is especially useful when we are working on large-scale, real-world data because of its ability to automate reading, resizing, and batching images for model training or inference. The <strong class="source-inline">flow_from_directory</strong> method takes three main arguments. The first is the directory path that contains our image data. Next, we specify the desired size of the images before we feed them into our neural network. Then, we also have to specify the batch size to determine the number of images we want to process simultaneously. We can tailor the process more by specifying other parameters, such as color mode, class mode, and shuffle. Let's now take a look at a typical directory structure for a multiclass classification problem, as illustrated in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.19</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<img alt="Figure 7.19 – The directory structure for a multiclass classification problem" height="853" src="image/B18118_07_19.jpg" width="1659"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – The directory structure for a multiclass classification problem</p>
<p>When <a id="_idIndexMarker407"/>applying the <strong class="source-inline">flow_from_directory</strong> method, it is important that we organize our images in a well-structured directory, with subdirectories for each unique class label as displayed in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.19</em>. Here, we have four subdirectories, one for each class label in our weather dataset. Once all the images are in the appropriate subdirectories, we can apply <strong class="source-inline">flow_from_directory</strong> to set up an iterator. This iterator is adjustable so that we can define parameters such as the image size and batch size and decide whether we want to shuffle our data or not. Let’s apply these new ideas to our current <span class="No-Break">case study:</span></p>
<pre class="source-code">
# Preprocess data (get all of the pixel values between 1 and 0, also called scaling/normalization)
train_datagen = ImageDataGenerator(rescale=1./255)
valid_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = Im<a id="_idTextAnchor175"/><a id="_idTextAnchor176"/>ageDataGenerator(rescale=1./255)</pre>
<p>Here, we define three instances of the <strong class="source-inline">ImageDataGenerator</strong> class: one for training, one for validation, and one for testing. We apply a rescaling factor of 1/255 to the pixel values of images in <a id="_idIndexMarker408"/>each instance to normalize <span class="No-Break">our data:</span></p>
<pre class="source-code">
# Import data from directories and turn it into batches
train_data = train_datagen.flow_from_directory(train_dir,
    batch_size=64, # number of images to process at a time
    target_size=(224,224), # convert all images to be 224 x 224
    class_mode="categorical")
valid_data = valid_datagen.flow_from_directory(val_dir,
    batch_size=64,
    target_size=(224,224),
    class_mode="categorical")
test_data = test_datagen.flow_from_directory(test_dir,
    batch_size=64,
    target_size=(224,22<a id="_idTextAnchor177"/>4),
    class_mode="categorical")</pre>
<p>We use <strong class="source-inline">flow_from_directory</strong> to import images from the respective training, validation, and testing directories, and the resulting data is stored in our <strong class="source-inline">train_data</strong>, <strong class="source-inline">valid_data</strong>, and <strong class="source-inline">test_data</strong> variables. In addition to specifying the directories in our <strong class="source-inline">flow_from_directory</strong> method, you will notice we also specified not just the target size (224 x244) and batch size (64) but also the type of problem we are tackling as <strong class="source-inline">categorical</strong> because we are dealing with a multi-classification use case. We have now successfully completed<a id="_idIndexMarker409"/> our data preprocessing steps. Let's move on to modeling <span class="No-Break">our data:</span></p>
<pre class="source-code">
mode<a id="_idTextAnchor178"/>l_1 = tf.keras.models.Sequential([
    tf.keras.layers.Conv2D(filters=16,
        kernel_size=3, # can also be (3, 3)
        activation="relu",
        input_shape=(224, 224, 3)),
        #(height, width, colour channels)
tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Conv2D(32, 3, activation="relu"),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Conv2D(64, 3, activation="relu"),
    tf.keras.layers.MaxPool2D(2,2),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1050, activation="relu"),
    tf.keras.layers.Dense(4, activation="softmax")
    # binary activation output
])
# Compile the model
model_1.compile(loss="CategoricalCrossentropy",
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"])
# Fit the model
history_1 = model_1.fit(train_data,
    epochs=10,
    validation_data=valid_data,
    )</pre>
<p>Here, we use a CNN architecture made up of three sets of convolutional and pooling layers. In the first convolutional layer, we apply 16 filters with a filter size of 3. Notice the input shape also matches the shape defined in our preprocessing step. After the first convolutional layer, we apply max pooling of 2x2. Next, we reach the second convolutional layer, which utilizes 32 filters, each 3x3 in size, followed by another 2x2 max pooling layer. The final convolutional layer has 64 filters, each 3x3 in size, followed by another max pooling layer, which further downsamples <span class="No-Break">the data.</span></p>
<p>Next, we reach the fully connected layers. Here, we first flatten the 3D output of the earlier layers into a 1D array. Then, we feed the data into dense layers for final classification. We proceed by compiling and fitting our model to our data. It’s important to note that in our <strong class="source-inline">compile</strong> step, we use <strong class="source-inline">CategoricalCrossentropy</strong> for our <strong class="source-inline">loss</strong> function as we are dealing with a task with multiple classes, and we set <strong class="source-inline">metrics</strong> to <strong class="source-inline">accuracy</strong>. The resulting output is a probability distribution over the four classes in our dataset, with the class <a id="_idIndexMarker410"/>with the highest p<a id="_idTextAnchor179"/>robability being the <span class="No-Break">predicted label:</span></p>
<pre class="source-code">
Epoch 6/10
13/13 [==============================] - 8s 622ms/step - loss: 0.1961 - accuracy: 0.9368 - val_loss: 0.2428 - val_accuracy: 0.8994
Epoch 7/10
13/13 [==============================] - 8s 653ms/step - loss: 0.1897 - accuracy: 0.9241 - val_loss: 0.2967 - val_accuracy: 0.9218
Epoch 8/10
13/13 [==============================] - 8s 613ms/step - loss: 0.1093 - accuracy: 0.9671 - val_loss: 0.3447 - val_accuracy: 0.8939
Epoch 9/10
13/13 [==============================] - 8s 604ms/step - loss: 0.1756 - accuracy: 0.9381 - val_loss: 0.6276 - val_accuracy: 0.8324
Epoch 10/10
13/13 [==============================] - 8s 629ms/step - loss: 0.1472 - accuracy: 0.9418 - val_<a id="_idTextAnchor180"/>loss: 0.2633 - val_accuracy: 0.9106</pre>
<p>We train our model for 10 epochs, attaining a training accuracy of around 94% on training data and 91% on validation data. We use the <strong class="source-inline">summary</strong> method to obtain information about the different layers in the model. This information includes the layer-wise overview, output<a id="_idIndexMarker411"/> shape, and number of parameters used (trainable <span class="No-Break">and non-trainable):</span></p>
<pre class="source-code">
Model: "sequential"
___________________________________________________________
 Layer (type)                 Output Shape         Param #
===========================================================
 conv2d (Conv2D)              (None, 222, 222, 16) 448
 max_pooling2d (MaxPooling2D) (None, 111, 111, 16) 0
 conv2d_1 (Conv2D)            (None, 109, 109, 32) 4640
 max_pooling2d_1 (MaxPooling  (None, 54, 54, 32)   0
 2D)
 conv2d_2 (Conv2D)            (None, 52, 52, 64)   18496
 max_pooling2d_2 (MaxPooling  (None, 26, 26, 64)   0
 2D)
 flatten (Flatten)            (None, 43264)        0
 dense (Dense)                (None, 1050)         45428250
 dense_1 (Dense)              (None, 4)            4204
===========================================================
Total params: 45,456,038
Trainable params: 45,456,038
Non-trainable params: 0
_______________________<a id="_idTextAnchor181"/>____________________________________</pre>
<p>From our model’s summary, we see our architecture has three convolutional (<strong class="source-inline">Conv2D</strong>) layers, each accompanied by a pooling (<strong class="source-inline">MaxPooling2D</strong>) layer. Information flows from these layers into the <a id="_idIndexMarker412"/>fully connected layer, where the final classification is carried out. Let's drill down into each of the layers and unpack the information they provide us with. The first convolutional layer is with an output shape of <strong class="source-inline">(None, 222, 222, 16)</strong>. Here, <strong class="source-inline">None</strong> means we didn’t hardcode the batch size, which gives us the flexibility to use different batch sizes with ease. Next, we have <strong class="source-inline">222, 222</strong>, which represents the dimension of the output feature map; we lose 2 pixels in height and weight because of the boundary effect if we do not apply padding. Finally, <strong class="source-inline">16</strong> represents the number of filters or kernels used, which means we will have an output of 16 different feature maps from each of the filters. You will also notice this layer has <strong class="source-inline">448</strong> parameters. To calculate the number of parameters in the convolutional layers, we use the <span class="No-Break">following formula:</span></p>
<p><em class="italic">(Filter width × Filter height × Input channels + 1(for bias)) × Number of filters = Total number of parameters in the </em><span class="No-Break"><em class="italic">convolutional layer</em></span></p>
<p>When we key in the values into the formula, we arrive at (3 × 3 × 3 + 1) × 16 = <span class="No-Break">448 parameters.</span></p>
<p>The next layer is the first pooling layer, which is a <strong class="source-inline">MaxPooling2D</strong> layer that downsamples the output feature maps from the convolutional layer. Here, we have an output shape of <strong class="source-inline">(None, 111, 111, 16)</strong>. From the output, you can see that the spatial dimension has been reduced to half, and it is also important to note that pooling layers have no parameters, as you will observe with all the pooling layers in our <span class="No-Break">model’s summary.</span></p>
<p>Next, we reach the second convolutional layer and notice the depth of our output has increased to <strong class="source-inline">32</strong>. This happens because we employed 32 filters in this layer; hence, we will have 32 different feature maps returned. Also, we have the spatial dimension of the feature maps again reduced by two pixels because of the boundary effect. We can easily calculate the number of parameters in this layer as follows: (3 × 3 × 16 + 1) × 32 = <span class="No-Break">4,640 parameters.</span></p>
<p>Next, we reach the second pooling layer, which downsamples the feature maps further to <strong class="source-inline">(None, 54, 54, 32)</strong>. The final convolutional layer uses 64 filters, so it has an output shape of <strong class="source-inline">(None, 52, 52, 64)</strong> and 18,496 parameters. The final pooling layer again reduces the dimension of our data to <strong class="source-inline">(None, 26, 26, 64)</strong>. The output of the final pooling layer is fed into the <strong class="source-inline">Flatten</strong> layer, which reshapes the data from a 3D tensor into a 1D tensor with a size of 26 x 26 x 64 = 43,264. This is fed into the first <strong class="source-inline">Dense</strong> layer, which has an output shape of <strong class="source-inline">(None, 1050)</strong>. To calculate the number of parameters in the <strong class="source-inline">Dense</strong> layer, we use <span class="No-Break">this formula:</span></p>
<p><em class="italic">(Number of input nodes + 1) × Number of </em><span class="No-Break"><em class="italic">output nodes</em></span></p>
<p>When we input the<a id="_idIndexMarker413"/> values, we get (43,264 + 1) × 1,050 = 45,428,250 parameters. The final <strong class="source-inline">Dense</strong> layer is the output layer and it has a shape of <strong class="source-inline">(None, 4)</strong>, where <strong class="source-inline">4</strong> represents the number of unique classes in our data that we want to predict. This layer has (1,050 + 1) × 4 = 4,204 parameters due to its connections, biases, and the number of <span class="No-Break">output neurons.</span></p>
<p>Next, we evaluate our model using the <span class="No-Break"><strong class="source-inline">evaluate</strong></span><span class="No-Break"> method:</span></p>
<pre class="source-code">
model_1.evaluate(test_data)</pre>
<p>We reach an accuracy of 91% on our <span class="No-Break">test data.</span></p>
<p>Let's compare our CNN architecture with <span class="No-Break">two DNNs:</span></p>
<pre class="source-code">
model_2 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(224, 224, 3)),
    tf.keras.layers.Dense(1200, activation='relu'),
    tf.keras.layers.Dense(600, activation='relu'),
    tf.keras.layers.Dense(300, activation='relu'),
    tf.keras.layers.Dense(4, activation='softmax')
])
# Compile the model
model_2.compile(loss='categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"])
# Fit the model
history_2 = model_2.fit(train_data,
    epochs=10,
    validation_data=valid_data)</pre>
<p>We build a DNN called <strong class="source-inline">model_2</strong> made up of 4 <strong class="source-inline">Dense</strong> layers, with <strong class="source-inline">1200</strong>, <strong class="source-inline">600</strong>, <strong class="source-inline">300</strong>, and <strong class="source-inline">4</strong> neurons, respectively. Apart from the output layer, which uses the <strong class="source-inline">softmax</strong> function for <a id="_idIndexMarker414"/>classification, all the other layers use ReLU as their activation function. We compile and fit <strong class="source-inline">model_2</strong> in the same way <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">model_1</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
Epoch 6/10
13/13 [==============================] - 8s 625ms/step - loss: 2.2083 - accuracy: 0.6953 - val_loss: 0.9884 - val_accuracy: 0.7933
Epoch 7/10
13/13 [==============================] - 8s 606ms/step - loss: 2.7116 - accuracy: 0.6435 - val_loss: 2.0749 - val_accuracy: 0.6704
Epoch 8/10
13/13 [==============================] - 8s 636ms/step - loss: 2.8324 - accuracy: 0.6877 - val_loss: 1.7241 - val_accuracy: 0.7430
Epoch 9/10
13/13 [==============================] - 8s 599ms/step - loss: 1.8597 - accuracy: 0.6890 - val_loss: 1.1507 - val_accuracy: 0.7877
Epoch 10/10
13/13 [==============================] - 8s 612ms/step - loss: 1.0902 - accuracy: 0.7813 - val_loss: 0.9915 - val_accuracy: 0.7486</pre>
<p>After 10 epochs, we reach a validation accuracy of 74.86% and when we examine the model’s summary, we see that we have used a total of 181,536,904 parameters, which is 4 times the size of our CNN <span class="No-Break">architecture parameters.</span></p>
<p>Next, let's look at<a id="_idIndexMarker415"/> another <span class="No-Break">DNN architecture:</span></p>
<pre class="source-code">
model_3 = tf.keras.Sequential([
    tf.keras.layers.Flatten(input_shape=(224, 224, 3)),
    tf.keras.layers.Dense(1000, activation='relu'),
    tf.keras.layers.Dense(500, activation='relu'),
    tf.keras.layers.Dense(500, activation='relu'),
    tf.keras.layers.Dense(4, activation='softmax')
])
# Compile the model
model_3.compile(loss='categorical_crossentropy',
    optimizer=tf.keras.optimizers.Adam(),
    metrics=["accuracy"])
# Fit the model
history_3 = model_3.fit(train_data,
    epochs=10,
    validation_data=valid_data)</pre>
<p>We use another set of 4 <strong class="source-inline">Dense</strong> layers, with <strong class="source-inline">1000</strong>, <strong class="source-inline">500</strong>, <strong class="source-inline">500</strong>, and <strong class="source-inline">4</strong> neurons, respectively. We fit and compile <strong class="source-inline">model_3</strong> as well for <span class="No-Break">10 epochs:</span></p>
<pre class="source-code">
Epoch 6/10
13/13 [==============================] - 9s 665ms/step - loss: 1.6911 - accuracy: 0.6814 - val_loss: 0.5861 - val_accuracy: 0.7877
Epoch 7/10
13/13 [==============================] - 8s 606ms/step - loss: 0.7309 - accuracy: 0.7952 - val_loss: 0.5100 - val_accuracy: 0.8268
Epoch 8/10
13/13 [==============================] - 8s 572ms/step - loss: 0.6797 - accuracy: 0.7863 - val_loss: 0.9520 - val_accuracy: 0.7263
Epoch 9/10
13/13 [==============================] - 8s 632ms/step - loss: 0.7430 - accuracy: 0.7724 - val_loss: 0.5220 - val_accuracy: 0.7933
Epoch 10/10
13/13 [==============================] - 8s 620ms/step - loss: 0.5845 - accuracy: 0.7737 - val_loss: 0.5881 - val_accuracy: 0.7765</pre>
<p>We reach a validation<a id="_idIndexMarker416"/> accuracy of 77.65% after 10 epochs and this model has around 151,282,004 parameters; the results are not close to those of our CNN architecture. Let's proceed to compare all three models on test data, which is what we want to be judging our models on. To do this, we will write a function to generate a DataFrame showing the names, the loss, and the accuracy of <span class="No-Break">the models:</span></p>
<pre class="source-code">
def evaluate_models(models, model_names,test_data):
    # Initialize lists for the results
    losses = []
    accuracies = []
    # Iterate over the models
    for model in models:
        # Evaluate the model
        loss, accuracy = model.evaluate(test_data)
        losses.append(loss)
        accuracies.append(accuracy)
       # Convert the results to percentages
    losses = [round(loss * 100, 2) for loss in losses]
    accuracies = [round(accuracy * 100, 2) for accuracy in accuracies]
    # Create a dataframe with the results
    results = pd.DataFrame({"Model": model_names,
        "Loss": losses,
        "Accuracy": accuracies})
    return results</pre>
<p>The <strong class="source-inline">evaluate_models()</strong> function <a id="_idIndexMarker417"/>takes a list of models, model names, and test data as input and returns a DataFrame with the evaluation results for each model <span class="No-Break">as percentages:</span></p>
<pre class="source-code">
# Define the models and model names
models = [model_1, model_2, model_3]
model_names = ["Model 1", "Model 2", "Model 3"]
# Evaluate the models
results = evaluate_models(models, model_names,test_data)
# Display the results
results</pre>
<p>When we run the code, it generates the table shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.20</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<img alt="Figure 7.20 – A DataFrame showing the experimental results of all three models" height="149" src="image/B18118_07_20.jpg" width="261"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – A DataFrame showing the experimental results of all three models</p>
<p>From the results, we can clearly see that Model 1 is ahead. You may wish to experiment with larger DNNs but you will soon run out of memory. For larger datasets, the results may be much worse<a id="_idIndexMarker418"/> for DNNs. Next, let's look at how we fared on our training and validation data with <span class="No-Break">Model 1:</span></p>
<pre class="source-code">
def plot_loss_accuracy(history_1):
  # Extract the loss and accuracy history for both training and validation data
    loss = history_1.history['loss']
    val_loss = history_1.history['val_loss']
    acc = history_1.history['accuracy']
    val_acc = history_1.history['val_accuracy']
  # Create subplots
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 6))
  # Plot the loss history
    ax1.plot(loss, label='Training loss')
    ax1.plot(val_loss, label='Validation loss')
    ax1.set_title('Loss history')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.legend()
  # Plot the accuracy history
    ax2.plot(acc, label='Training accuracy')
    ax2.plot(val_acc, label='Validation accuracy')
    ax2.set_title('Accuracy history')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    plt.show()</pre>
<p>We created a <a id="_idIndexMarker419"/>function to plot the training and validation loss and accuracy using matplotlib. We pass <strong class="source-inline">history_1</strong> into <span class="No-Break">our function:</span></p>
<pre class="source-code">
# Lets plot the training and validation loss and accuracy
plot_loss_accuracy(history_1)</pre>
<p>This will generate the <span class="No-Break">following output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<img alt="Figure 7.21 – Loss and accuracy plot for Model 1" height="387" src="image/B18118_07_21.jpg" width="553"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure<a id="_idTextAnchor182"/> 7.21 – Loss and accuracy plot for Model 1</p>
<p>From the plot, we can see that our training accuracy rises steadily but falls below its highest point just<a id="_idIndexMarker420"/> before the 10th epoch. Also, our validation data experiences a sharp fall in accuracy. Our loss veers off from the <span class="No-Break">fourth epoch.</span></p>
<h1 id="_idParaDest-142"><a id="_idTextAnchor183"/>Summary</h1>
<p>In this chapter, we saw the power of CNNs. We began by examining the challenges faced by DNNs for visual recognition tasks. Next, we journeyed through the anatomy of CNNs, zooming in on the various moving parts, such as the convolutional, pooling, and fully connected layers. Here, we saw the impact and effect of different hyperparameters, and we also discussed the boundary effect. Next, we moved on to using all we learned to build a real-world weather classifier using two DNNs and a CNN. Our CNN model outperformed the DNNs, showcasing the strength of CNNs in handling image-based problems. Also, we discussed and applied some TensorFlow functions that streamline data preprocessing and modeling when we are working with <span class="No-Break">image data.</span></p>
<p>By now you should have a good understanding of the structure and operations of CNNs and how to use them to solve real-world image classification problems, as well as utilizing various tools in TensorFlow to effectively and efficiently preprocess image data for improved model performance. In the next chapter, we will address the issue of overfitting in neural networks and explore various techniques to overcome this challenge, ensuring that our models generalize well to <span class="No-Break">unseen data.</span></p>
<p>In the next chapter, we will use some old tricks such as callbacks and hyperparameter tuning to see whether we can improve our model’s performance. We will also experiment with data augmentation and other new techniques to improve our model’s performance. We draw the curtains on our task for now until <a href="B18118_08.xhtml#_idTextAnchor186"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><em class="italic">, </em><span class="No-Break"><em class="italic">Handling Overfitting</em></span><span class="No-Break">.</span></p>
<h1 id="_idParaDest-143"><a id="_idTextAnchor184"/>Questions</h1>
<p>Let’s test what we have learned in <span class="No-Break">this chapter:</span></p>
<ol>
<li>What are the components of a typical <span class="No-Break">CNN architecture?</span></li>
<li>How does a convolutional layer work in a <span class="No-Break">CNN architecture?</span></li>
<li>What is pooling and why is it used in a <span class="No-Break">CNN architecture?</span></li>
<li>What is the purpose of a fully connected layer in a <span class="No-Break">CNN architecture?</span></li>
<li>What is the impact of the padding on a <span class="No-Break">convolution operation?</span></li>
<li>What are the advantages of using TensorFlow image <span class="No-Break">data generators?</span></li>
</ol>
<h1 id="_idParaDest-144"><a id="_idTextAnchor185"/>Further reading</h1>
<p>To learn more, you can check out the <span class="No-Break">following resources:</span></p>
<ul>
<li>Dumoulin, V., &amp; Visin, F. (2016). <em class="italic">A guide to convolution arithmetic for deep </em><span class="No-Break"><em class="italic">learning</em></span><span class="No-Break">. </span><a href="http://arxiv.org/abs/1603.07285"><span class="No-Break">http://arxiv.org/abs/1603.07285</span></a></li>
<li>Gulli, A., Kapoor, A. and Pal, S., 2019. <em class="italic">Deep Learning with TensorFlow 2 and Keras</em>. Birmingham: Packt <span class="No-Break">Publishing Ltd</span></li>
<li>Kapoor, A., Gulli, A. and Pal, S. (2020) <em class="italic">Deep Learning with TensorFlow and Keras Third Edition: Build and deploy supervised, unsupervised, deep, and reinforcement learning models</em>. Packt <span class="No-Break">Publishing Ltd</span></li>
<li>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). <em class="italic">ImageNet classification with deep convolutional neural networks</em>. In Advances in neural information processing systems (<span class="No-Break">pp. 1097-1105)</span></li>
<li>Zhang, Y., &amp; Yang, H. (2018). <em class="italic">Food classification with convolutional neural networks and multi-class linear discernment analysis</em>. In 2018 IEEE International Conference on Information Reuse and Integration (IRI) (pp. <span class="No-Break">1-5). IEEE</span></li>
<li>Zhang, Z., Ma, H., Fu, H., &amp; Zha, C. (2020). <em class="italic">Scene-Free Multi-Class Weather Classification on Single Images</em>. IEEE Access, 8, <span class="No-Break">146038-146049. doi:10.1109</span></li>
<li>The <strong class="source-inline">tf.image</strong> <span class="No-Break">module: </span><a href="https://www.tensorflow.org/api_docs/python/tf/image"><span class="No-Break">https://www.tensorflow.org/api_docs/python/tf/image</span></a></li>
</ul>
</div>
</div></body></html>