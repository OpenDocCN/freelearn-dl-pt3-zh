<html><head></head><body>
  <div id="_idContainer036">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-52" class="chapterTitle">The TensorFlow Way</h1>
    <p class="normal">In <em class="chapterRef">Chapter 1</em>,<em class="italic"> Getting Started with TensorFlow 2.x</em> we introduced how TensorFlow creates tensors and uses variables. In this chapter, we'll introduce how to put together all these objects using eager execution, thus dynamically setting up a computational graph. From this, we can set up a simple classifier and see how well it performs.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Also, remember that the current and updated code from this book is available online on GitHub at <a href="https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook"><span class="url">https://github.com/PacktPublishing/Machine-Learning-Using-TensorFlow-Cookbook</span></a>.</p>
    </div>
    <p class="normal">Over the course of this chapter, we'll introduce the key components of how TensorFlow operates. Then, we'll tie it together to create a simple classifier and evaluate the outcomes. By the end of the chapter, you should have learned about the following:</p>
    <ul>
      <li class="bullet">Operations using eager execution</li>
      <li class="bullet">Layering nested operations</li>
      <li class="bullet">Working with multiple layers</li>
      <li class="bullet">Implementing loss functions</li>
      <li class="bullet">Implementing backpropagation</li>
      <li class="bullet">Working with batch and stochastic training</li>
      <li class="bullet">Combining everything together</li>
    </ul>
    <p class="normal">Let's start working our way through more and more complex recipes, demonstrating the TensorFlow way of handling and solving data problems.</p>
    <h1 id="_idParaDest-53" class="title">Operations using eager execution</h1>
    <p class="normal">Thanks to <em class="chapterRef">Chapter 1</em>,<em class="italic"> Getting Started with TensorFlow 2.x</em> we can <a id="_idIndexMarker074"/>already create objects such as <a id="_idIndexMarker075"/>variables in TensorFlow. Now we will introduce operations that act on such objects. In order to do so, we'll return to eager execution with a new basic recipe showing how to manipulate matrices. This recipe, and the following ones, are still basic ones, but over the course of the chapter, we'll combine these basic recipes into more complex ones.</p>
    <h2 id="_idParaDest-54" class="title">Getting ready</h2>
    <p class="normal">To start, we load TensorFlow and NumPy, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> TensorFlow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> NumPy <span class="hljs-keyword">as</span> np 
</code></pre>
    <p class="normal">That's all we need to get started; now we can proceed.</p>
    <h2 id="_idParaDest-55" class="title">How to do it...</h2>
    <p class="normal">In this example, we'll use what we have learned so far, and send each number in a list to be computed by TensorFlow commands and print the output.</p>
    <p class="normal">First, we declare our tensors and variables. Here, out of all the various ways we could feed data into the variable using TensorFlow, we will create a NumPy array to feed into our variable and then use it for our operation:</p>
    <pre class="programlisting code"><code class="hljs-code">x_vals = np.array([<span class="hljs-number">1.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">9.</span>])
x_data = tf.Variable(x_vals, dtype=tf.float32)
m_const = tf.constant(<span class="hljs-number">3.</span>)
operation = tf.multiply(x_data, m_const)
<span class="hljs-keyword">for</span> result <span class="hljs-keyword">in</span> operation:
    print(result.NumPy()) 
</code></pre>
    <p class="normal">The output of the preceding code is as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">3.0 
9.0 
15.0 
21.0 
27.0 
</code></pre>
    <p class="normal">Once you get accustomed to working with TensorFlow variables, constants, and functions, it will become natural to start from NumPy array data, progress to scripting data structures and operations, and test their results as you go.</p>
    <h2 id="_idParaDest-56" class="title">How it works...</h2>
    <p class="normal">Using eager execution, TensorFlow immediately evaluates the operation values, instead of manipulating the symbolic <a id="_idIndexMarker076"/>handles referred to the <a id="_idIndexMarker077"/>nodes of a computational graph to be later compiled and executed. You can therefore just iterate through the results of the multiplicative operation and print the resulting values using the <code class="Code-In-Text--PACKT-">.NumPy</code> method, which returns a NumPy object from a TensorFlow tensor.</p>
    <h1 id="_idParaDest-57" class="title">Layering nested operations</h1>
    <p class="normal">In this recipe, we'll learn <a id="_idIndexMarker078"/>how to put multiple operations to work; it is important to know how to chain operations together. This will set up layered operations to be executed by our network. In this recipe, we will multiply a placeholder by two matrices and then perform addition. We will feed in two matrices in the form of a three-dimensional NumPy array.</p>
    <p class="normal">This is another easy-peasy recipe to give you ideas about how to code in TensorFlow using common constructs such as functions or classes, improving readability and code modularity. Even if the final product is a neural network, we're still writing a computer program, and we should abide by programming best practices.</p>
    <h2 id="_idParaDest-58" class="title">Getting ready</h2>
    <p class="normal">As usual, we just need to import TensorFlow and NumPy, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> TensorFlow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> NumPy <span class="hljs-keyword">as</span> np 
</code></pre>
    <p class="normal">We're now ready to move forward with our recipe. </p>
    <h2 id="_idParaDest-59" class="title">How to do it...</h2>
    <p class="normal">We will feed in two NumPy arrays of size <em class="italic">3</em> x <em class="italic">5</em>. We will multiply each matrix by a constant of size <em class="italic">5</em> x <em class="italic">1,</em> which will result in a matrix of size <em class="italic">3</em> x <em class="italic">1</em>. We will then multiply this by a <em class="italic">1</em> x <em class="italic">1</em> matrix resulting in a <em class="italic">3</em> x <em class="italic">1</em> matrix again. Finally, we add a <em class="italic">3</em> x <em class="italic">1</em> matrix at the end, as follows:</p>
    <ol>
      <li class="numbered">First, we create the data to feed in and the corresponding placeholder:
        <pre class="programlisting code"><code class="hljs-code">my_array = np.array([[<span class="hljs-number">1.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">9.</span>], 
                     [<span class="hljs-number">-2.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">4.</span>, <span class="hljs-number">6.</span>], 
                     [<span class="hljs-number">-6.</span>, <span class="hljs-number">-3.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">6.</span>]]) 
x_vals = np.array([my_array, my_array + <span class="hljs-number">1</span>])
x_data = tf.Variable(x_vals, dtype=tf.float32)
</code></pre>
      </li>
      <li class="numbered">Next, we create the constants that we will use for matrix multiplication and addition:
        <pre class="programlisting code"><code class="hljs-code">m1 = tf.constant([[<span class="hljs-number">1.</span>], [<span class="hljs-number">0.</span>], [<span class="hljs-number">-1.</span>], [<span class="hljs-number">2.</span>], [<span class="hljs-number">4.</span>]]) 
m2 = tf.constant([[<span class="hljs-number">2.</span>]]) 
a1 = tf.constant([[<span class="hljs-number">10.</span>]]) 
</code></pre>
      </li>
      <li class="numbered">Now, we declare the <a id="_idIndexMarker079"/>operations to be eagerly executed. As good practice, we create functions that execute the operations we need:
        <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">prod1</span><span class="hljs-function">(</span><span class="hljs-params">a, b</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.matmul(a, b)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">prod2</span><span class="hljs-function">(</span><span class="hljs-params">a, b</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.matmul(a, b) 
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">add1</span><span class="hljs-function">(</span><span class="hljs-params">a, b</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.add(a, b)
</code></pre>
      </li>
      <li class="numbered">Finally, we nest our functions and display the result:
        <pre class="programlisting code"><code class="hljs-code">result = add1(prod2(prod1(x_data, m1), m2), a1)
print(result.NumPy()) 
[[ 102.] 
 [  66.] 
 [  58.]] 
[[ 114.] 
 [  78.] 
 [  70.]] 
</code></pre>
      </li>
    </ol>
    <p class="normal">Using functions (and also classes, as we are going to cover) will help you write clearer code. That makes debugging more effective and allows easy maintenance and reuse of code. </p>
    <h2 id="_idParaDest-60" class="title">How it works...</h2>
    <p class="normal">Thanks to <a id="_idIndexMarker080"/>eager execution, there's no longer a need to resort to the "kitchen sink" programming style (meaning that you put almost everything in the global scope of the program; see <a href="https://stackoverflow.com/questions/33779296/what-is-exact-meaning-of-kitchen-sink-in-programming"><span class="url">https://stackoverflow.com/questions/33779296/what-is-exact-meaning-of-kitchen-sink-in-programming</span></a>) that was so common when using TensorFlow 1.x. At the moment, you can adopt either a functional programming style or an object-oriented one, such as the one we present in this brief example, where you can arrange all your operations and computations in a more logical and understandable way:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">class</span><span class="hljs-class"> </span><span class="hljs-title">Operations</span><span class="hljs-class">():</span>  
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">__init__</span><span class="hljs-function">(</span><span class="hljs-params">self, a</span><span class="hljs-function">):</span>
        self.result = a
    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">apply</span><span class="hljs-function">(</span><span class="hljs-params">self, func, b</span><span class="hljs-function">):</span>
        self.result = func(self.result, b)
        <span class="hljs-keyword">return</span> self
        
operation = (Operations(a=x_data)
             .apply(prod1, b=m1)
             .apply(prod2, b=m2)
             .apply(add1, b=a1))
print(operation.result.NumPy())
</code></pre>
    <p class="normal">Classes can help you organize your code and reuse it better than functions, thanks to class inheritance.</p>
    <h2 id="_idParaDest-61" class="title">There's more...</h2>
    <p class="normal">In all the examples in this recipe, we've had to declare the data shape and know the outcome shape of the operations before we run the data through the operations. This is not always the case. There may be a dimension or two that we do not know beforehand or some that can vary during our data processing. To take this into account, we designate the dimension or dimensions that can vary (or are unknown) as value <code class="Code-In-Text--PACKT-">None</code>. </p>
    <p class="normal">For example, to initialize a variable to have an unknown amount of rows, we would write the following line and then we can assign values of arbitrary row numbers:</p>
    <pre class="programlisting code"><code class="hljs-code">v = tf.Variable(initial_value=tf.random.normal(shape=(<span class="hljs-number">1</span>, <span class="hljs-number">5</span>)),
                shape=tf.TensorShape((<span class="hljs-literal">None</span>, <span class="hljs-number">5</span>)))
v.assign(tf.random.normal(shape=(<span class="hljs-number">10</span>, <span class="hljs-number">5</span>)))
</code></pre>
    <p class="normal">It is fine for matrix multiplication to have flexible rows because that won't affect the arrangement of our operations. This will come in handy in later chapters when we are feeding data in multiple batches of varying batch sizes.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">While the use of <em class="italic">None</em> as a dimension allows us to use variably-sized dimensions, I always recommend that you be as explicit as possible when filling out dimensions. If the size of our data is known in advance, then we should explicitly write that size as the dimensions. The use of <code class="Code-In-Text--PACKT-">None</code> as a dimension is recommended to be limited to the batch size of the data (or however many data points we are computing on at once).</p>
    </div>
    <h1 id="_idParaDest-62" class="title">Working with multiple layers</h1>
    <p class="normal">Now that we have <a id="_idIndexMarker081"/>covered multiple operations, we will cover how to connect various layers that have data propagating through them. In this recipe, we will introduce how to best connect various layers, including custom layers. The data we will generate and use will be representative of small random images. It is best to understand this type of operation with a simple example and see how we can use some built-in layers to perform calculations. The first layer we will explore <a id="_idIndexMarker082"/>is called a <strong class="keyword">moving window</strong>. We will perform a small moving window average across a 2D image and then the second layer will be a custom operation layer.</p>
    <p class="normal">Moving windows are useful for everything related to time series. Though there are layers specialized for sequences, a moving window may prove useful when you are analyzing, for instance, MRI scans (neuroimages) or sound spectrograms.</p>
    <p class="normal">Moreover, we will see that the computational graph can get large and hard to look at. To address this, we will also introduce ways to name operations and create scopes for layers.</p>
    <h2 id="_idParaDest-63" class="title">Getting ready</h2>
    <p class="normal">To start, you have to load the usual packages – NumPy and TensorFlow – using the following:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> TensorFlow <span class="hljs-keyword">as</span> tf
<span class="hljs-keyword">import</span> NumPy <span class="hljs-keyword">as</span> np
</code></pre>
    <p class="normal">Let's now progress to the recipe. This time things are getting more complex and interesting.</p>
    <h2 id="_idParaDest-64" class="title">How to do it...</h2>
    <p class="normal">We proceed with the recipe as follows.</p>
    <p class="normal">First, we create our sample 2D image with NumPy. This image will be a <em class="italic">4</em> x <em class="italic">4</em> pixel image. We will create it in four dimensions; the first and last dimensions will have a size of <code class="Code-In-Text--PACKT-">1</code> (we keep the batch dimension distinct, so you can experiment with changing its size). Note that some TensorFlow image functions will operate on four-dimensional images. Those four dimensions are image number, height, width, and channel, and to make it work with one channel, we explicitly set the last dimension to <code class="Code-In-Text--PACKT-">1</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = [<span class="hljs-number">1</span>]
x_shape = [<span class="hljs-number">4</span>, <span class="hljs-number">4</span>, <span class="hljs-number">1</span>]
x_data = tf.random.uniform(shape=batch_size + x_shape)
</code></pre>
    <p class="normal">To create a moving <a id="_idIndexMarker083"/>window average across our <em class="italic">4</em> x <em class="italic">4</em> image, we will use a built-in function that will convolute a constant across a window of the shape <em class="italic">2</em> x <em class="italic">2</em>. The function we will use is <code class="Code-In-Text--PACKT-">conv2d()</code>; this function is quite commonly used in image processing and in TensorFlow. </p>
    <p class="normal">This function takes a piecewise product of the window and a filter we specify. We must also specify a stride for the moving window in both directions. Here, we will compute four moving window averages: the upper-left, upper-right, lower-left, and lower-right four pixels. We do this by creating a <em class="italic">2</em> x <em class="italic">2</em> window and having strides of length <code class="Code-In-Text--PACKT-">2</code> in each direction. To take the average, we will convolute the <em class="italic">2</em> x <em class="italic">2</em> window with a constant of <code class="Code-In-Text--PACKT-">0.25</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">mov_avg_layer</span><span class="hljs-function">(</span><span class="hljs-params">x</span><span class="hljs-function">):</span>
    my_filter = tf.constant(<span class="hljs-number">0.25</span>, shape=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]) 
    my_strides = [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>] 
    layer = tf.nn.conv2d(x, my_filter, my_strides, 
                         padding='SAME', name='Moving_Avg_Window')
    <span class="hljs-keyword">return</span> layer
</code></pre>
    <div class="note">
      <p class="Information-Box--PACKT-">Note that we are also naming this layer <code class="Code-In-Text--PACKT-">Moving_Avg_Window</code> by using the name argument of the function.</p>
      <p class="Information-Box--PACKT-">To figure out the output size of a convolutional layer, we can use the following formula: Output = (<em class="italic">W</em> – <em class="italic">F</em> + 2<em class="italic">P</em>)/<em class="italic">S</em> + 1), where <em class="italic">W</em> is the input size, <em class="italic">F</em> is the filter size, <em class="italic">P</em> is the padding of zeros, and <em class="italic">S</em> is the stride.</p>
    </div>
    <p class="normal">Now, we define a custom layer that will operate on the <em class="italic">2</em> x <em class="italic">2</em> output of the moving window average. The custom function will first multiply the input by another <em class="italic">2</em> x <em class="italic">2</em> matrix tensor, and then add <code class="Code-In-Text--PACKT-">1</code> to each entry. After this, we take the sigmoid of each element and return the <em class="italic">2</em> x <em class="italic">2</em> matrix. Since matrix multiplication only operates on two-dimensional matrices, we need to drop the extra dimensions of our image that are of size <code class="Code-In-Text--PACKT-">1</code>. TensorFlow can do this with the built-in <code class="Code-In-Text--PACKT-">squeeze()</code> function. Here, we define the new layer:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">custom_layer</span><span class="hljs-function">(</span><span class="hljs-params">input_matrix</span><span class="hljs-function">):</span> 
        input_matrix_sqeezed = tf.squeeze(input_matrix) 
        A = tf.constant([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>], [<span class="hljs-number">-1.</span>, <span class="hljs-number">3.</span>]]) 
        b = tf.constant(<span class="hljs-number">1.</span>, shape=[<span class="hljs-number">2</span>, <span class="hljs-number">2</span>]) 
        temp1 = tf.matmul(A, input_matrix_sqeezed) 
        temp = tf.add(temp1, b) <span class="hljs-comment"># Ax + b </span>
        <span class="hljs-keyword">return</span> tf.sigmoid(temp)  
</code></pre>
    <p class="normal">Now, we have to <a id="_idIndexMarker084"/>arrange the two layers in the network. We will do this by calling one layer function after the other, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">first_layer = mov_avg_layer(x_data) 
second_layer = custom_layer(first_layer)
</code></pre>
    <p class="normal">Now, we just feed in the <em class="italic">4</em> x <em class="italic">4</em> image into the functions. Finally, we can check the result, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code">print(second_layer)
 
tf.Tensor(
[[0.9385519  0.90720266]
 [0.9247799  0.82272065]], shape=(2, 2), dtype=float32)
</code></pre>
    <p class="normal">Let's now understand more in depth how it works.</p>
    <h2 id="_idParaDest-65" class="title">How it works...</h2>
    <p class="normal">The first layer is named <code class="Code-In-Text--PACKT-">Moving_Avg_Window</code>. The second is a collection of operations called <code class="Code-In-Text--PACKT-">Custom_Layer</code>. Data <a id="_idIndexMarker085"/>processed by these two layers is first collapsed on the left and then expanded on the right. As shown by the example, you can wrap all the layers into functions and call them, one after the other, in a way that later layers process the outputs of previous ones.</p>
    <h1 id="_idParaDest-66" class="title">Implementing loss functions</h1>
    <p class="normal">For this recipe, we will <a id="_idIndexMarker086"/>cover some of the main loss functions that we can use in TensorFlow. Loss functions are a key aspect of machine learning algorithms. They measure the distance between the model outputs and the target (truth) values.</p>
    <p class="normal">In order to optimize our machine learning algorithms, we will need to evaluate the outcomes. Evaluating outcomes in TensorFlow depends on specifying a loss function. A loss function tells TensorFlow how good or bad the predictions are compared to the desired result. In most cases, we will have a set of data and a target on which to train our algorithm. The loss function compares the target to the prediction (it measures the distance between the model outputs and the target truth values) and provides a numerical quantification between the two.</p>
    <h2 id="_idParaDest-67" class="title">Getting ready</h2>
    <p class="normal">We will first start a computational graph and load <code class="Code-In-Text--PACKT-">matplotlib</code>, a Python plotting package, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt 
<span class="hljs-keyword">import</span> TensorFlow <span class="hljs-keyword">as</span> tf 
</code></pre>
    <p class="normal">Now that we are ready to plot, let's proceed to the recipe without further ado.</p>
    <h2 id="_idParaDest-68" class="title">How to do it...</h2>
    <p class="normal">First, we will talk about loss functions for regression, which means predicting a continuous dependent variable. To start, we will create a sequence of our predictions and a target as a tensor. We will output the results across 500 x values between <code class="Code-In-Text--PACKT-">-1</code> and <code class="Code-In-Text--PACKT-">1</code>. See the <em class="italic">How it works...</em> section for a plot of the outputs. Use the following code:</p>
    <pre class="programlisting code"><code class="hljs-code">x_vals = tf.linspace(<span class="hljs-number">-1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">500</span>) 
target = tf.constant(<span class="hljs-number">0.</span>) 
</code></pre>
    <p class="normal">The L2 norm loss is also known as the Euclidean loss function. It is just the square of the distance to the target. Here, we will compute the loss function as if the target is zero. The L2 norm is a great loss function because it is very curved near the target and algorithms can use this fact to converge to the target more slowly the closer it gets to zero. We can implement this as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">l2</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.square(y_true - y_pred) 
</code></pre>
    <div class="note">
      <p class="Information-Box--PACKT-">TensorFlow has a built-in form of the L2 norm, called <code class="Code-In-Text--PACKT-">tf.nn.l2_loss()</code>. This function is actually half the L2 norm. In other words, it is the same as the previous one but divided by 2.</p>
    </div>
    <p class="normal">The L1 norm loss is also <a id="_idIndexMarker087"/>known as the <strong class="keyword">absolute loss function</strong>. Instead of squaring the difference, we take the <a id="_idIndexMarker088"/>absolute value. The L1 norm is better for outliers than the L2 norm because it is not as steep for larger values. One issue to be aware of is that the L1 norm is not smooth at the target, and this can result in algorithms not converging well. It appears as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">l1</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.abs(y_true - y_pred)
</code></pre>
    <p class="normal">Pseudo-Huber loss is a continuous and smooth approximation to the <strong class="keyword">Huber loss function</strong>. This loss function <a id="_idIndexMarker089"/>attempts to take the best of the L1 and L2 norms by being convex near the target and less steep for extreme values. The form depends on an extra parameter, <code class="Code-In-Text--PACKT-">delta</code>, which dictates how steep it will be. We will plot two forms, <em class="italic">delta1 = 0.25</em> and <em class="italic">delta2 = 5</em>, to show the difference, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">phuber1</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    delta1 = tf.constant(<span class="hljs-number">0.25</span>) 
    <span class="hljs-keyword">return</span> tf.multiply(tf.square(delta1), tf.sqrt(<span class="hljs-number">1.</span> +  
                        tf.square((y_true - y_pred)/delta1)) - <span class="hljs-number">1.</span>) 
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">phuber2</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    delta2 = tf.constant(<span class="hljs-number">5.</span>) 
    <span class="hljs-keyword">return</span> tf.multiply(tf.square(delta2), tf.sqrt(<span class="hljs-number">1.</span> +  
                        tf.square((y_true - y_pred)/delta2)) - <span class="hljs-number">1.</span>) 
</code></pre>
    <p class="normal">Now, we'll move on to loss functions for classification problems. Classification loss functions are used to evaluate loss when predicting categorical outcomes. Usually, the output of our model for a class category is a real-value number between <code class="Code-In-Text--PACKT-">0</code> and <code class="Code-In-Text--PACKT-">1</code>. Then, we choose a cutoff (0.5 is commonly chosen) and classify the outcome as being in that category if the number is above the cutoff. Next, we'll consider various loss functions for categorical outputs.</p>
    <p class="normal">To start, we will need to redefine our predictions <code class="Code-In-Text--PACKT-">(x_vals)</code> and <code class="Code-In-Text--PACKT-">target</code>. We will save the outputs and plot them in the next section. Use the following:</p>
    <pre class="programlisting code"><code class="hljs-code">x_vals = tf.linspace(<span class="hljs-number">-3.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">500</span>) 
target = tf.fill([<span class="hljs-number">500</span>,], <span class="hljs-number">1.</span>)
</code></pre>
    <p class="normal">Hinge loss is mostly used for support vector machines but can be used in neural networks as well. It is meant to compute a loss among two target classes, <code class="Code-In-Text--PACKT-">1</code> and -<code class="Code-In-Text--PACKT-">1</code>. In the following code, we are using the target value <code class="Code-In-Text--PACKT-">1</code>, so the closer our predictions are to <code class="Code-In-Text--PACKT-">1</code>, the lower the loss value:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">hinge</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.maximum(<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span> - tf.multiply(y_true, y_pred))
</code></pre>
    <p class="normal">Cross-entropy loss for a binary case is also <a id="_idIndexMarker090"/>sometimes referred to as the <strong class="keyword">logistic loss function</strong>. It comes about when we are predicting the two classes <code class="Code-In-Text--PACKT-">0</code> or <code class="Code-In-Text--PACKT-">1</code>. We wish to measure a distance from the actual class (<code class="Code-In-Text--PACKT-">0</code> or <code class="Code-In-Text--PACKT-">1</code>) to the predicted value, which is usually a real number between <code class="Code-In-Text--PACKT-">0</code> and <code class="Code-In-Text--PACKT-">1</code>. To measure this distance, we can use the cross-entropy formula from information theory, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">xentropy</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> (- tf.multiply(y_true, tf.math.log(y_pred)) -   
          tf.multiply((<span class="hljs-number">1.</span> - y_true), tf.math.log(<span class="hljs-number">1.</span> - y_pred))) 
</code></pre>
    <p class="normal">Sigmoid cross-entropy <a id="_idIndexMarker091"/>loss is very similar to the previous loss function except we transform the <code class="Code-In-Text--PACKT-">x</code> values using the sigmoid function before we put them in the cross-entropy loss, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">xentropy_sigmoid</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true,  
                                                   logits=y_pred) 
</code></pre>
    <p class="normal">Weighted cross-entropy loss is a weighted version of sigmoid cross-entropy loss. We provide a weight on the positive target. For an example, we will weight the positive target by <code class="Code-In-Text--PACKT-">0.5</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">xentropy_weighted</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    weight = tf.constant(<span class="hljs-number">0.5</span>) 
    <span class="hljs-keyword">return</span> tf.nn.weighted_cross_entropy_with_logits(labels=y_true,
                                                    logits=y_pred,  
                                                pos_weight=weight)
</code></pre>
    <p class="normal">Softmax cross-entropy loss operates on non-normalized outputs. This function is used to measure a loss when there is only one target category instead of multiple. Because of this, the function transforms the outputs into a probability distribution via the softmax function and then computes the loss function from a true probability distribution, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">softmax_xentropy</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.nn.softmax_cross_entropy_with_logits(labels=y_true,                                                    logits=y_pred)
    
unscaled_logits = tf.constant([[<span class="hljs-number">1.</span>, <span class="hljs-number">-3.</span>, <span class="hljs-number">10.</span>]]) 
target_dist = tf.constant([[<span class="hljs-number">0.1</span>, <span class="hljs-number">0.02</span>, <span class="hljs-number">0.88</span>]])
print(softmax_xentropy(y_true=target_dist,                        y_pred=unscaled_logits))
[ 1.16012561] 
</code></pre>
    <p class="normal">Sparse softmax cross-entropy loss is almost the same as softmax cross-entropy loss, except instead of the <a id="_idIndexMarker092"/>target being a probability distribution, it is an index of which category is <code class="Code-In-Text--PACKT-">true</code>. Instead of a sparse all-zero target vector with one value of <code class="Code-In-Text--PACKT-">1</code>, we just pass in the index of the category that is the <code class="Code-In-Text--PACKT-">true</code> value, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">sparse_xentropy</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.nn.sparse_softmax_cross_entropy_with_logits(
                                                    labels=y_true,
                                                    logits=y_pred) 
unscaled_logits = tf.constant([[<span class="hljs-number">1.</span>, <span class="hljs-number">-3.</span>, <span class="hljs-number">10.</span>]]) 
sparse_target_dist = tf.constant([<span class="hljs-number">2</span>]) 
print(sparse_xentropy(y_true=sparse_target_dist,  
                      y_pred=unscaled_logits))
[ 0.00012564] 
</code></pre>
    <p class="normal">Now let's understand better how such loss functions operate by plotting them on a graph.</p>
    <h2 id="_idParaDest-69" class="title">How it works...</h2>
    <p class="normal">Here is how <a id="_idIndexMarker093"/>to use <code class="Code-In-Text--PACKT-">matplotlib</code> to plot the regression loss functions:</p>
    <pre class="programlisting code"><code class="hljs-code">x_vals = tf.linspace(<span class="hljs-number">-1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">500</span>) 
target = tf.constant(<span class="hljs-number">0.</span>) 
funcs = [(l2, 'b-', 'L2 Loss'),
         (l1, 'r--', 'L1 Loss'),
         (phuber1, 'k-.', 'P-Huber Loss (<span class="hljs-number">0.25</span>)'),
         (phuber2, 'g:', 'P-Huber Loss (<span class="hljs-number">5.0</span>)')]
<span class="hljs-keyword">for</span> func, line_type, func_name <span class="hljs-keyword">in</span> funcs:
    plt.plot(x_vals, func(y_true=target, y_pred=x_vals), 
             line_type, label=func_name)
plt.ylim(<span class="hljs-number">-0.2</span>, <span class="hljs-number">0.4</span>) 
plt.legend(loc='lower right', prop={'size': <span class="hljs-number">11</span>}) 
plt.show()
</code></pre>
    <p class="normal">We get the <a id="_idIndexMarker094"/>following plot as output from the preceding code:</p>
    <figure class="mediaobject"><img src="../Images/B16254_02_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.1: Plotting various regression loss functions</p>
    <p class="normal">Here is how to use <code class="Code-In-Text--PACKT-">matplotlib</code> to plot the various classification loss functions:</p>
    <pre class="programlisting code"><code class="hljs-code">x_vals = tf.linspace(<span class="hljs-number">-3.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">500</span>)  
target = tf.fill([<span class="hljs-number">500</span>,], <span class="hljs-number">1.</span>)
funcs = [(hinge, 'b-', 'Hinge Loss'),
         (xentropy, 'r--', 'Cross Entropy Loss'),
         (xentropy_sigmoid, 'k-.', 'Cross Entropy Sigmoid Loss'),
         (xentropy_weighted, 'g:', 'Weighted Cross Enropy Loss            (x0<span class="hljs-number">.5</span>)')]
<span class="hljs-keyword">for</span> func, line_type, func_name <span class="hljs-keyword">in</span> funcs:
    plt.plot(x_vals, func(y_true=target, y_pred=x_vals), 
             line_type, label=func_name)
plt.ylim(<span class="hljs-number">-1.5</span>, <span class="hljs-number">3</span>) 
plt.legend(loc='lower right', prop={'size': <span class="hljs-number">11</span>}) 
plt.show()
</code></pre>
    <p class="normal">We get the <a id="_idIndexMarker095"/>following plot from the preceding code:</p>
    <figure class="mediaobject"><img src="../Images/B16254_02_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.2: Plots of classification loss functions</p>
    <p class="normal">Each of these loss curves provides different advantages to the neural network optimizing it. We are now going to discuss this a little bit more.</p>
    <h2 id="_idParaDest-70" class="title">There's more...</h2>
    <p class="normal">Here is a table <a id="_idIndexMarker096"/>summarizing the properties and benefits of the <a id="_idIndexMarker097"/>different loss functions that we have just graphically described:</p>
    <table id="table001-1" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Loss function</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Use</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Benefits</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Disadvantages</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">L2</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Regression</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">More stable</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Less robust</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">L1</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Regression</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">More robust</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Less stable</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Pseudo-Huber</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Regression</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">More robust and stable</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">One more parameter</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Hinge</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Classification</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Creates a max margin for use in SVM</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Unbounded loss affected by outliers</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Cross-entropy</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Classification</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">More stable</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Unbounded loss, less robust</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">The remaining classification loss functions all have to do with the type of cross-entropy loss. The cross-entropy sigmoid loss function is for <a id="_idIndexMarker098"/>use on unscaled logits and is preferred over computing the sigmoid loss and then the cross-entropy loss, because TensorFlow has better built-in ways to handle numerical edge cases. The same goes for softmax cross-entropy and sparse softmax cross-entropy.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Most of the classification loss functions described here are for two-class predictions. This can be extended to multiple classes by summing the cross-entropy terms over each prediction/target.</p>
    </div>
    <p class="normal">There are also many other <a id="_idIndexMarker099"/>metrics to look at when evaluating a model. Here is a list of some more to consider:</p>
    <table id="table002-1" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Model metric</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Description</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">R-squared (coefficient of determination)</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">For linear models, this is the proportion of variance in the dependent variable that is explained by the independent data. For models with a larger number of features, consider using adjusted R-squared.</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Root mean squared error</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">For continuous models, this measures the difference between prediction and actual via the square root of the average squared error.</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Confusion matrix</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">For categorical models, we look at a matrix of predicted categories versus actual categories. A perfect model has all the counts along the diagonal.</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Recall</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">For categorical models, this is the fraction of true positives over all predicted positives.</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Precision</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">For categorical models, this is the fraction of true positives over all actual positives.</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">F-score</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">For categorical models, this is the harmonic mean of precision and recall.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">In your choice of the right metric, you have <a id="_idIndexMarker100"/>to both evaluate the problem you have to solve (because each metric will behave differently and, depending on the problem at hand, some loss minimization strategies will prove better than others for our problem), and to experiment with the behavior of the neural network.</p>
    <h1 id="_idParaDest-71" class="title">Implementing backpropagation</h1>
    <p class="normal">One of the benefits of <a id="_idIndexMarker101"/>using TensorFlow is that it can keep track of operations and automatically update model variables based on backpropagation. In this recipe, we will introduce how to use this aspect to our advantage when training machine learning models.</p>
    <h2 id="_idParaDest-72" class="title">Getting ready</h2>
    <p class="normal">Now, we will introduce how to change our variables in the model in such a way that a loss function is minimized. We have learned how to use objects and operations, and how to create loss functions that will measure the distance between our predictions and targets. Now, we just have to tell TensorFlow how to backpropagate errors through our network in order to update the variables in such a way to minimize the loss function. This is achieved by declaring an optimization function. Once we have an optimization function declared, TensorFlow will go through and figure out the backpropagation terms for all of our computations in the graph. When we feed data in and minimize the loss function, TensorFlow will modify our variables in the network accordingly.</p>
    <p class="normal">For this recipe, we will do a very simple regression algorithm. We will sample random numbers from a normal distribution, with mean 1 and standard deviation 0.1. Then, we will run the numbers through one operation, which will be to multiply them by a weight tensor and then adding a bias tensor. From this, the loss function will be the L2 norm between the output and the target. Our target will show a high correlation with our input, so the task won't be too complex, yet the recipe will be interestingly demonstrative, and easily reusable for more complex problems.</p>
    <p class="normal">The second example is a very simple binary classification algorithm. Here, we will generate 100 numbers from two normal distributions, <em class="italic">N(-3,1)</em> and <em class="italic">N(3,1)</em>. All the numbers from <em class="italic">N(-3, 1)</em> will be in target class <code class="Code-In-Text--PACKT-">0</code>, and all the numbers from <em class="italic">N(3, 1)</em> will be in target class <code class="Code-In-Text--PACKT-">1</code>. The model to differentiate these classes (which are perfectly separable) will again be a linear model optimized accordingly to the sigmoid cross-entropy loss function, thus, at first operating a sigmoid transformation on the model result and then computing the cross-entropy loss function.</p>
    <p class="normal">While specifying a good learning rate helps the convergence of algorithms, we must also specify a type of optimization. From the preceding two examples, we are using standard gradient descent. This is implemented with the <code class="Code-In-Text--PACKT-">tf.optimizers.SGD</code> TensorFlow function.</p>
    <h2 id="_idParaDest-73" class="title">How to do it...</h2>
    <p class="normal">We'll start with the <a id="_idIndexMarker102"/>regression example. First, we load the usual numerical Python packages that always accompany our recipes, <code class="Code-In-Text--PACKT-">NumPy</code> and <code class="Code-In-Text--PACKT-">TensorFlow</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> NumPy <span class="hljs-keyword">as</span> np 
<span class="hljs-keyword">import</span> TensorFlow <span class="hljs-keyword">as</span> tf 
</code></pre>
    <p class="normal">Next, we create the data. In order to make everything easily replicable, we want to set the random seed to a specific value. We will always repeat this in our recipes, so we exactly obtain the same results; check yourself how chance may vary the results in the recipes, by simply changing the seed number. </p>
    <p class="normal">Moreover, in order to get assurance that the target and input have a good correlation, plot a scatterplot of the two variables:</p>
    <pre class="programlisting code"><code class="hljs-code">np.random.seed(<span class="hljs-number">0</span>)
x_vals = np.random.normal(<span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">100</span>).astype(np.float32) 
y_vals = (x_vals * (np.random.normal(<span class="hljs-number">1</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">100</span>) - <span class="hljs-number">0.5</span>)).astype(np.float32)
plt.scatter(x_vals, y_vals)
plt.show()
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_02_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.3: Scatterplot of x_vals and y_vals</p>
    <p class="normal">We add the structure <a id="_idIndexMarker103"/>of the network (a linear model of the type <em class="italic">bX + a</em>) as a function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">my_output</span><span class="hljs-function">(</span><span class="hljs-params">X, weights, biases</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.add(tf.multiply(X, weights), biases)
</code></pre>
    <p class="normal">Next, we add our L2 Loss function to be applied to the results of the network:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">loss_func</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.reduce_mean(tf.square(y_pred - y_true))
</code></pre>
    <p class="normal">Now, we have to declare a way to optimize the variables in our graph. We declare an optimization algorithm. Most optimization algorithms need to know how far to step in each iteration. Such a distance is controlled by the learning rate. Setting it to a correct value is specific to the problem we are dealing with, so we can figure out a suitable setting only by experimenting. Anyway, if our learning rate is too high, our algorithm might overshoot the minimum, but if our learning rate is too low, our algorithm might take too long to converge. </p>
    <p class="normal">The learning rate has a big influence on convergence and we will discuss it again at the end of the section. While we're using the standard gradient descent algorithm, there are many other alternative options. There are, for instance, optimization algorithms that operate differently and can achieve a better or worse optimum depending on the problem. For a great overview of different optimization algorithms, see the paper by Sebastian Ruder in the <em class="italic">See also</em> section at the end of this recipe:</p>
    <pre class="programlisting code"><code class="hljs-code">my_opt = tf.optimizers.SGD(learning_rate=<span class="hljs-number">0.02</span>)
</code></pre>
    <div class="packt_tip">
      <p class="Tip--PACKT-">There is a lot of theory on which learning rates are best. This is one of the harder things to figure out in machine learning algorithms. Good papers to read about how learning rates are related to specific optimization algorithms are listed in the <em class="italic">See also</em> section at the end of this recipe.</p>
    </div>
    <p class="normal">Now we can initialize our network variables (<code class="Code-In-Text--PACKT-">weights</code> and <code class="Code-In-Text--PACKT-">biases</code>) and set a recording list (named <code class="Code-In-Text--PACKT-">history</code>) to help us visualize the optimization steps:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.random.set_seed(<span class="hljs-number">1</span>)
np.random.seed(<span class="hljs-number">0</span>)
weights = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>])) 
biases = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>])) 
history = list()
</code></pre>
    <p class="normal">The final step is to loop <a id="_idIndexMarker104"/>through our training algorithm and tell TensorFlow to train many times. We will do this 100 times and print out results every 25<sup class="Superscript--PACKT-">th</sup> iteration. To train, we will select a random <em class="italic">x</em> and <em class="italic">y</em> entry and feed it through the graph. TensorFlow will automatically compute the loss, and slightly change the weights and biases to minimize the loss:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>): 
    rand_index = np.random.choice(<span class="hljs-number">100</span>) 
    rand_x = [x_vals[rand_index]] 
    rand_y = [y_vals[rand_index]]
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        predictions = my_output(rand_x, weights, biases)
        loss = loss_func(rand_y, predictions)
    history.append(loss.NumPy())
    gradients = tape.gradient(loss, [weights, biases])
    my_opt.apply_gradients(zip(gradients, [weights, biases]))
    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">25</span> == <span class="hljs-number">0</span>: 
        print(f'Step <span class="hljs-comment"># {i+1} Weights: {weights.NumPy()} Biases: {biases.NumPy()}')</span>
        print(f'Loss = {loss.NumPy()}') 
Step # 25 Weights: [-0.58009654] Biases: [0.91217995]
Loss = 0.13842473924160004
Step # 50 Weights: [-0.5050226] Biases: [0.9813488]
Loss = 0.006441597361117601
Step # 75 Weights: [-0.4791306] Biases: [0.9942327]
Loss = 0.01728087291121483
Step # 100 Weights: [-0.4777394] Biases: [0.9807473]
Loss = 0.05371852591633797
</code></pre>
    <p class="normal">In the loops, <code class="Code-In-Text--PACKT-">tf.GradientTape()</code> allows TensorFlow to track the computations and calculate the gradient with respect to the observed variables. Every variable that is within the <code class="Code-In-Text--PACKT-">GradientTape()</code> scope is monitored (please keep in mind that constants are not monitored, unless you explicitly state it with the command <code class="Code-In-Text--PACKT-">tape.watch(constant)</code>). Once you've completed the monitoring, you can compute the gradient of a target in respect of a list of sources (using the command <code class="Code-In-Text--PACKT-">tape.gradient(target, sources)</code>) and get back an eager tensor of <a id="_idIndexMarker105"/>the gradients that you can apply to the minimization process. The operation is automatically concluded with the updating of your sources (in our case, the <code class="Code-In-Text--PACKT-">weights</code> and <code class="Code-In-Text--PACKT-">biases</code> variables) with new values.</p>
    <p class="normal">When the training is completed, we can visualize how the optimization process operates over successive gradient applications:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.plot(history)
plt.xlabel('iterations')
plt.ylabel('loss')
plt.show()
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_02_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.4: L2 loss through iterations in our recipe</p>
    <p class="normal">At this point, we will introduce the code for the simple classification example. We can use the same TensorFlow script, with some updates. Remember, we will attempt to find an optimal set of weights and biases that will separate the data into two different classes.</p>
    <p class="normal">First, we pull in the data from two different normal distributions, <code class="Code-In-Text--PACKT-">N(-3, 1)</code> and <code class="Code-In-Text--PACKT-">N(3, 1)</code>. We will also generate the target labels and visualize how the two classes are distributed along our predictor <a id="_idIndexMarker106"/>variable:</p>
    <pre class="programlisting code"><code class="hljs-code">np.random.seed(<span class="hljs-number">0</span>)
x_vals = np.concatenate((np.random.normal(<span class="hljs-number">-3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>), 
                         np.random.normal(<span class="hljs-number">3</span>, <span class="hljs-number">1</span>, <span class="hljs-number">50</span>))
                    ).astype(np.float32) 
y_vals = np.concatenate((np.repeat(<span class="hljs-number">0.</span>, <span class="hljs-number">50</span>), np.repeat(<span class="hljs-number">1.</span>, <span class="hljs-number">50</span>))).astype(np.float32) 
plt.hist(x_vals[y_vals==<span class="hljs-number">1</span>], color='b')
plt.hist(x_vals[y_vals==<span class="hljs-number">0</span>], color='r')
plt.show()
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_02_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.5: Class distribution on x_vals</p>
    <p class="normal">Because the specific loss function for this problem is sigmoid cross-entropy, we update our loss function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">loss_func</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.reduce_mean(
        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, 
                                                logits=y_pred))
</code></pre>
    <p class="normal">Next, we initialize our variables:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.random.set_seed(<span class="hljs-number">1</span>)
np.random.seed(<span class="hljs-number">0</span>)
weights = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>])) 
biases = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>])) 
history = list()
</code></pre>
    <p class="normal">Finally, we loop through a <a id="_idIndexMarker107"/>randomly selected data point several hundred times and update the <code class="Code-In-Text--PACKT-">weights</code> and <code class="Code-In-Text--PACKT-">biases</code> variables accordingly. As we did before, every 25 iterations we will print out the value of our variables and the loss:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">100</span>):    
    rand_index = np.random.choice(<span class="hljs-number">100</span>) 
    rand_x = [x_vals[rand_index]] 
    rand_y = [y_vals[rand_index]]
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        predictions = my_output(rand_x, weights, biases)
        loss = loss_func(rand_y, predictions)
    history.append(loss.NumPy())
    gradients = tape.gradient(loss, [weights, biases])
    my_opt.apply_gradients(zip(gradients, [weights, biases]))
    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">25</span> == <span class="hljs-number">0</span>: 
        print(f'Step {i+1} Weights: {weights.NumPy()} Biases: {biases.NumPy()}')
        print(f'Loss = {loss.NumPy()}')
Step # 25 Weights: [-0.01804185] Biases: [0.44081175]
Loss = 0.5967269539833069
Step # 50 Weights: [0.49321094] Biases: [0.37732077]
Loss = 0.3199256658554077
Step # 75 Weights: [0.7071932] Biases: [0.32154965]
Loss = 0.03642747551202774
Step # 100 Weights: [0.8395616] Biases: [0.30409005]
Loss = 0.028119442984461784
</code></pre>
    <p class="normal">A plot, also in this case, will reveal how the optimization proceeded:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.plot(history)
plt.xlabel('iterations')
plt.ylabel('loss')
plt.show()
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_02_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.6: Sigmoid cross-entropy loss through iterations in our recipe</p>
    <p class="normal">The directionality of the plot is <a id="_idIndexMarker108"/>clear, though the trajectory is a bit bumpy because we are learning one example at a time, thus making the learning process decisively stochastic. The graph could also point out the need to try to decrease the learning rate a bit.</p>
    <h2 id="_idParaDest-74" class="title">How it works...</h2>
    <p class="normal">For a recap and <a id="_idIndexMarker109"/>explanation, for both examples, we did the following:</p>
    <ol>
      <li class="numbered" value="1">We created the data. Both examples needed to load data into specific variables used by the function that computes the network.</li>
      <li class="numbered">We initialized variables. We used some random Gaussian values, but initialization is a topic on its own, since much of the final results may depend on how we initialize our network (just change the random seed before initialization to find it out).</li>
      <li class="numbered">We created a loss function. We used the L2 loss for regression and the cross-entropy loss for classification.</li>
      <li class="numbered">We defined an optimization algorithm. Both algorithms used gradient descent.</li>
      <li class="numbered">We iterated across random <a id="_idIndexMarker110"/>data samples to iteratively update our variables.</li>
    </ol>
    <h2 id="_idParaDest-75" class="title">There's more...</h2>
    <p class="normal">As we mentioned before, the optimization algorithm is sensitive to the choice of learning rate. It is <a id="_idIndexMarker111"/>important to summarize the effect of this choice in a <a id="_idIndexMarker112"/>concise manner:</p>
    <table id="table003" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Learning rate size</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Advantages/disadvantages</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Uses</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="normal"><strong class="keyword">Smaller learning rate</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="normal">Converges slower but more accurate results</p>
          </td>
          <td class="No-Table-Style">
            <p class="normal">If the solution is unstable, try lowering the learning rate first</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="normal"><strong class="keyword">Larger learning rate</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="normal">Less accurate, but converges faster</p>
          </td>
          <td class="No-Table-Style">
            <p class="normal">For some problems, helps prevent solutions from stagnating</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">Sometimes, the standard gradient descent algorithm can be stuck or slow down significantly. This can happen when the optimization is stuck in the flat spot of a saddle. To combat this, the solution is taking into account a momentum term, which adds on a fraction of the prior step's gradient descent value. You can access this solution by setting the momentum and the Nesterov parameters, along with your learning rate, in <code class="Code-In-Text--PACKT-">tf.optimizers.SGD</code> (see <a href="https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/SGD"><span class="url">https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/SGD</span></a> for more details).</p>
    <p class="normal">Another variant is to vary the optimizer step for each variable in our models. Ideally, we would like to take larger steps for smaller moving variables and shorter steps for faster changing variables. We will not go into the mathematics of this approach, but a common implementation of this idea is <a id="_idIndexMarker113"/>called the <strong class="keyword">Adagrad algorithm</strong>. This algorithm takes into account the whole history of the variable gradients. The function in <a id="_idIndexMarker114"/>TensorFlow for this is called <code class="Code-In-Text--PACKT-">AdagradOptimizer()</code> (<a href="https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adagrad"><span class="url">https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adagrad</span></a>).</p>
    <p class="normal">Sometimes, Adagrad forces the gradients to zero too soon because it takes into account the whole history. A solution to this is to limit how many steps we use. This is called the <strong class="keyword">Adadelta algorithm</strong>. We <a id="_idIndexMarker115"/>can apply this by using the <code class="Code-In-Text--PACKT-">AdadeltaOptimizer()</code> function (<a href="https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adadelta"><span class="url">https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers/Adadelta</span></a>).</p>
    <p class="normal">There are a few other implementations of different gradient descent algorithms. For these, refer to the TensorFlow documentation at <a href="https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers"><span class="url">https://www.TensorFlow.org/api_docs/python/tf/keras/optimizers</span></a>.</p>
    <h2 id="_idParaDest-76" class="title">See also</h2>
    <p class="normal">For some references on optimization algorithms and learning rates, see the <a id="_idIndexMarker116"/>following papers and articles:</p>
    <ul>
      <li class="bullet">Recipes from this chapter, as follows:<ul>
          <li class="bullet-l2">The Implementing Loss Functions section.</li>
          <li class="bullet-l2">The Implementing Backpropagation section.</li>
        </ul>
      </li>
      <li class="bullet"><strong class="keyword">Kingma, D., Jimmy, L. Adam</strong>: <em class="italic">A Method for Stochastic Optimization. ICLR</em> 2015 <a href="https://arxiv.org/pdf/1412.6980.pdf"><span class="url">https://arxiv.org/pdf/1412.6980.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Ruder, S.</strong> <em class="italic">An Overview of Gradient Descent Optimization Algorithms</em>. 2016 <a href="https://arxiv.org/pdf/1609.04747v1.pdf"><span class="url">https://arxiv.org/pdf/1609.04747v1.pdf</span></a></li>
      <li class="bullet"><strong class="keyword">Zeiler, M.</strong> <em class="italic">ADADelta: An Adaptive Learning Rate Method</em>. 2012 <a href="https://arxiv.org/pdf/1212.5701.pdf"><span class="url">https://arxiv.org/pdf/1212.5701.pdf</span></a></li>
    </ul>
    <h1 id="_idParaDest-77" class="title">Working with batch and stochastic training</h1>
    <p class="normal">While TensorFlow <a id="_idIndexMarker117"/>updates our model variables according to <a id="_idIndexMarker118"/>backpropagation, it can operate on anything from a one-datum observation (as we did in the previous recipe) to a large batch of data at once. Operating on one training example can make for a very erratic learning process, while using too large a batch can be computationally expensive. Choosing the right type of training is crucial for getting our machine learning algorithms to converge to a solution.</p>
    <h2 id="_idParaDest-78" class="title">Getting ready</h2>
    <p class="normal">In order for TensorFlow to compute the variable gradients for backpropagation to work, we have to measure the loss on a sample or multiple samples. Stochastic training only works on one randomly sampled data-target pair at a time, just as we did in the previous recipe. Another option is to put a larger portion of the training examples in at a time and average the loss for the gradient calculation. The sizes of the training batch can vary, up to and including the whole dataset at once. Here, we will show how to extend the prior regression example, which used stochastic training, to batch training.</p>
    <p class="normal">We will start by loading <code class="Code-In-Text--PACKT-">NumPy</code>, <code class="Code-In-Text--PACKT-">matplotlib</code>, and <code class="Code-In-Text--PACKT-">TensorFlow</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib <span class="hljs-keyword">as</span> plt 
<span class="hljs-keyword">import</span> NumPy <span class="hljs-keyword">as</span> np 
<span class="hljs-keyword">import</span> TensorFlow <span class="hljs-keyword">as</span> tf 
</code></pre>
    <p class="normal">Now we just have to script our code and test our recipe in the <em class="italic">How to do it…</em> section.</p>
    <h2 id="_idParaDest-79" class="title">How to do it...</h2>
    <p class="normal">We start by <a id="_idIndexMarker119"/>declaring a batch size. This will be how many data observations we will <a id="_idIndexMarker120"/>feed through the computational graph at one time:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">20</span>
</code></pre>
    <p class="normal">Next, we just apply small modifications to the code used before for the regression problem:</p>
    <pre class="programlisting code"><code class="hljs-code">np.random.seed(<span class="hljs-number">0</span>)
x_vals = np.random.normal(<span class="hljs-number">1</span>, <span class="hljs-number">0.1</span>, <span class="hljs-number">100</span>).astype(np.float32) 
y_vals = (x_vals * (np.random.normal(<span class="hljs-number">1</span>, <span class="hljs-number">0.05</span>, <span class="hljs-number">100</span>) - <span class="hljs-number">0.5</span>)).astype(np.float32)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">loss_func</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.reduce_mean(tf.square(y_pred - y_true))
tf.random.set_seed(<span class="hljs-number">1</span>)
np.random.seed(<span class="hljs-number">0</span>)
weights = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>])) 
biases = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>])) 
history_batch = list()
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">50</span>):    
    rand_index = np.random.choice(<span class="hljs-number">100</span>, size=batch_size) 
    rand_x = [x_vals[rand_index]] 
    rand_y = [y_vals[rand_index]]
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        predictions = my_output(rand_x, weights, biases)
        loss = loss_func(rand_y, predictions)
    history_batch.append(loss.NumPy())
    gradients = tape.gradient(loss, [weights, biases])
    my_opt.apply_gradients(zip(gradients, [weights, biases]))
    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">25</span> == <span class="hljs-number">0</span>: 
        print(f'Step <span class="hljs-comment"># {i+1} Weights: {weights.NumPy()} \</span>
              Biases: {biases.NumPy()}')
        print(f'Loss = {loss.NumPy()}')
</code></pre>
    <p class="normal">Since our previous recipe, we have learned how to use matrix multiplication in our network and in our cost function. At this point, we just need to deal with inputs that are made of more rows <a id="_idIndexMarker121"/>as batches instead of single examples. We can even compare it with the previous approach, which <a id="_idIndexMarker122"/>we can now name stochastic optimization:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.random.set_seed(<span class="hljs-number">1</span>)
np.random.seed(<span class="hljs-number">0</span>)
weights = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>])) 
biases = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>])) 
history_stochastic = list()
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">50</span>):    
    rand_index = np.random.choice(<span class="hljs-number">100</span>, size=<span class="hljs-number">1</span>) 
    rand_x = [x_vals[rand_index]] 
    rand_y = [y_vals[rand_index]]
    <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
        predictions = my_output(rand_x, weights, biases)
        loss = loss_func(rand_y, predictions)
    history_stochastic.append(loss.NumPy())
    gradients = tape.gradient(loss, [weights, biases])
    my_opt.apply_gradients(zip(gradients, [weights, biases]))
    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">25</span> == <span class="hljs-number">0</span>: 
        print(f'Step <span class="hljs-comment"># {i+1} Weights: {weights.NumPy()} \</span>
              Biases: {biases.NumPy()}')
        print(f'Loss = {loss.NumPy()}')
</code></pre>
    <p class="normal">Just running the code will retrain our network using batches. At this point, we need to evaluate the results, get some intuition about how it works, and reflect on the results. Let's proceed to the next section.</p>
    <h2 id="_idParaDest-80" class="title">How it works...</h2>
    <p class="normal">Batch training and stochastic training differ in their optimization methods and their convergence. Finding a good batch size can be difficult. To see how convergence differs between batch training and stochastic training, you are encouraged to change the batch size to various levels.</p>
    <p class="normal">A visual comparison of the two approaches will explain better how using batches for this problem resulted in <a id="_idIndexMarker123"/>the same optimization as stochastic training, though there were fewer fluctuations during the <a id="_idIndexMarker124"/>process. Here is the code to produce the plot of both the stochastic and batch losses for the same regression problem. Note that the batch loss is much smoother and the stochastic loss is much more erratic:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.plot(history_stochastic, 'b-', label='Stochastic Loss') 
plt.plot(history_batch, 'r--', label='Batch Loss') 
plt.legend(loc='upper right', prop={'size': <span class="hljs-number">11</span>}) 
plt.show() 
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_02_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.7: Comparison of L2 loss when using stochastic and batch optimization</p>
    <p class="normal">Now our graph displays a smoother trend line. The persistent presence of bumps could be solved by reducing the learning rate and adjusting the batch size.</p>
    <h2 id="_idParaDest-81" class="title">There's more...</h2>
    <table id="table004" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Type of training</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Advantages</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Heading--PACKT-">Disadvantages</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Stochastic</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Randomness may help move out of local minimums</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Generally needs more iterations to converge</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Batch</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Finds minimums quicker</p>
          </td>
          <td class="No-Table-Style">
            <p class="Table-Column-Content--PACKT-">Takes more resources to compute</p>
          </td>
        </tr>
      </tbody>
    </table>
    <h1 id="_idParaDest-82" class="title">Combining everything together</h1>
    <p class="normal">In this section, we will combine everything we have illustrated so far <a id="_idIndexMarker125"/>and create a classifier <a id="_idIndexMarker126"/>for the iris dataset. The iris dataset is <a id="_idIndexMarker127"/>described in more detail in the <em class="italic">Working with data sources</em> recipe in <em class="chapterRef">Chapter 1</em>, <em class="italic">Getting Started with TensorFlow</em>. We will load this data and make a simple binary classifier to predict whether a flower is the species Iris setosa or not. To be clear, this dataset has three species, but we will only predict whether a flower is a single species, Iris setosa or not, giving us a binary classifier.</p>
    <h2 id="_idParaDest-83" class="title">Getting ready</h2>
    <p class="normal">We will start by loading the libraries and data and then transform the target accordingly. First, we load the libraries needed for our recipe. For the Iris dataset, we need the TensorFlow Datasets module, which we haven't used before in our recipes. Note that we also load <code class="Code-In-Text--PACKT-">matplotlib</code> here, because we would like to plot the resultant line afterward:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt 
<span class="hljs-keyword">import</span> NumPy <span class="hljs-keyword">as</span> np 
<span class="hljs-keyword">import</span> TensorFlow <span class="hljs-keyword">as</span> tf 
<span class="hljs-keyword">import</span> TensorFlow_datasets <span class="hljs-keyword">as</span> tfds
</code></pre>
    <h2 id="_idParaDest-84" class="title">How to do it...</h2>
    <p class="normal">As a starting point, let's first declare our batch size using a global variable:</p>
    <pre class="programlisting code"><code class="hljs-code">batch_size = <span class="hljs-number">20</span> 
</code></pre>
    <p class="normal">Next, we load the iris data. We will also need to transform the target data to be just <code class="Code-In-Text--PACKT-">1</code> or <code class="Code-In-Text--PACKT-">0</code>, whether the target is setosa or not. Since the iris dataset marks setosa as a <code class="Code-In-Text--PACKT-">0</code>, we will change all targets with the value <code class="Code-In-Text--PACKT-">0</code> to <code class="Code-In-Text--PACKT-">1</code>, and the other values all to <code class="Code-In-Text--PACKT-">0</code>. We will also only use two features, petal length and petal width. These two features are the third and fourth entry in each row of the dataset:</p>
    <pre class="programlisting code"><code class="hljs-code">iris = tfds.load('iris', split='train[:<span class="hljs-number">90</span>%]', W)
iris_test = tfds.load('iris', split='train[<span class="hljs-number">90</span>%:]', as_supervised=<span class="hljs-literal">True</span>)
<span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">iris2d</span><span class="hljs-function">(</span><span class="hljs-params">features, label</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> features[<span class="hljs-number">2</span>:], tf.cast((label == <span class="hljs-number">0</span>), dtype=tf.float32)
train_generator = (iris
                   .map(iris2d)
                   .shuffle(buffer_size=<span class="hljs-number">100</span>)
                   .batch(batch_size)
                  )
test_generator = iris_test.map(iris2d).batch(<span class="hljs-number">1</span>)
</code></pre>
    <p class="normal">As shown in the <a id="_idIndexMarker128"/>previous chapter, we use the TensorFlow <a id="_idIndexMarker129"/>dataset functions to both load and operate the necessary transformations by creating a data generator that can dynamically feed our network with data, instead of keeping it in an in-memory NumPy matrix. As a first step, we load the data, specifying that we want to split it (using the parameters <code class="Code-In-Text--PACKT-">split='train[:90%]'</code> and <code class="Code-In-Text--PACKT-">split='train[90%:]'</code>). This allows us to reserve a part (10%) of the dataset for the model evaluation, using data that has not been part of the training phase.</p>
    <p class="normal">We also specify the parameter, <code class="Code-In-Text--PACKT-">as_supervised=True</code>, that will allow us to access the data as tuples of features and labels when iterating from the dataset.</p>
    <p class="normal">Now we transform the dataset into an iterable generator by applying successive transformations. We shuffle the data, we define the batch to be returned by the iterable, and, most important, we apply a custom function that filters and transforms the features and labels returned from the dataset at the same time.</p>
    <p class="normal">Then, we define the linear model. The model will take the usual form <em class="italic">bX+a</em>. Remember that TensorFlow has loss functions with the sigmoid built in, so we just need to define the output of the model prior to the sigmoid function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">linear_model</span><span class="hljs-function">(</span><span class="hljs-params">X, A, b</span><span class="hljs-function">):</span>
    my_output = tf.add(tf.matmul(X, A), b) 
    <span class="hljs-keyword">return</span> tf.squeeze(my_output)
</code></pre>
    <p class="normal">Now, we add our sigmoid cross-entropy loss function with TensorFlow's built-in <code class="Code-In-Text--PACKT-">sigmoid_cross_entropy_with_logits()</code> function:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">def</span><span class="hljs-function"> </span><span class="hljs-title">xentropy</span><span class="hljs-function">(</span><span class="hljs-params">y_true, y_pred</span><span class="hljs-function">):</span>
    <span class="hljs-keyword">return</span> tf.reduce_mean(
        tf.nn.sigmoid_cross_entropy_with_logits(labels=y_true, 
                                                logits=y_pred))
</code></pre>
    <p class="normal">We also have to tell TensorFlow how to optimize our computational graph by declaring an optimizing method. We will want to minimize the cross-entropy loss. We will also choose <code class="Code-In-Text--PACKT-">0.02</code> as our learning rate:</p>
    <pre class="programlisting code"><code class="hljs-code">my_opt = tf.optimizers.SGD(learning_rate=<span class="hljs-number">0.02</span>) 
</code></pre>
    <p class="normal">Now, we <a id="_idIndexMarker130"/>will train our linear model with 300 iterations. We will feed in the three data points that we <a id="_idIndexMarker131"/>require: petal length, petal width, and the target variable. Every 30 iterations, we will print the variable values:</p>
    <pre class="programlisting code"><code class="hljs-code">tf.random.set_seed(<span class="hljs-number">1</span>)
np.random.seed(<span class="hljs-number">0</span>)
A = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">2</span>, <span class="hljs-number">1</span>])) 
b = tf.Variable(tf.random.normal(shape=[<span class="hljs-number">1</span>]))
history = list()
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">300</span>):
    iteration_loss = list()
    <span class="hljs-keyword">for</span> features, label <span class="hljs-keyword">in</span> train_generator:
        <span class="hljs-keyword">with</span> tf.GradientTape() <span class="hljs-keyword">as</span> tape:
            predictions = linear_model(features, A, b)
            loss = xentropy(label, predictions)
        iteration_loss.append(loss.NumPy())
        gradients = tape.gradient(loss, [A, b])
        my_opt.apply_gradients(zip(gradients, [A, b]))
    history.append(np.mean(iteration_loss))
    <span class="hljs-keyword">if</span> (i + <span class="hljs-number">1</span>) % <span class="hljs-number">30</span> == <span class="hljs-number">0</span>:
        print(f'Step <span class="hljs-comment"># {i+1} Weights: {A.NumPy().T} \</span>
              Biases: {b.NumPy()}')
        print(f'Loss = {loss.NumPy()}')
Step # 30 Weights: [[-1.1206311  1.2985772]] Biases: [1.0116111]
Loss = 0.4503694772720337
…
Step # 300 Weights: [[-1.5611029   0.11102282]] Biases: [3.6908474]
Loss = 0.10326375812292099
</code></pre>
    <p class="normal">If we plot the loss against the iterations, we can acknowledge from the smoothness of the reduction of the <a id="_idIndexMarker132"/>loss over time how the learning has <a id="_idIndexMarker133"/>been quite an easy task for the linear model:</p>
    <pre class="programlisting code"><code class="hljs-code">plt.plot(history)
plt.xlabel('iterations')
plt.ylabel('loss')
plt.show() 
</code></pre>
    <figure class="mediaobject"><img src="../Images/B16254_02_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.8: Cross-entropy error for the Iris setosa data</p>
    <p class="normal">We'll conclude by checking the performance on our reserved test data. This time we just take the examples from the test dataset. As expected, the resulting cross-entropy value is analogous to the training one:</p>
    <pre class="programlisting code"><code class="hljs-code">predictions = list()
labels = list()
<span class="hljs-keyword">for</span> features, label <span class="hljs-keyword">in</span> test_generator:
    predictions.append(linear_model(features, A, b).NumPy())
    labels.append(label.NumPy()[<span class="hljs-number">0</span>])
    
test_loss = xentropy(np.array(labels), np.array(predictions)).NumPy()
print(f"test cross-entropy <span class="hljs-keyword">is</span> {test_loss}")
test cross-entropy is 0.10227929800748825
</code></pre>
    <p class="normal">The next set of <a id="_idIndexMarker134"/>commands extracts the model variables and plots the <a id="_idIndexMarker135"/>line on a graph:</p>
    <pre class="programlisting code"><code class="hljs-code">coefficients = np.ravel(A.NumPy())
intercept = b.NumPy()
<span class="hljs-comment"># Plotting batches of examples</span>
<span class="hljs-keyword">for</span> j, (features, label) <span class="hljs-keyword">in</span> enumerate(train_generator):
    setosa_mask = label.NumPy() == <span class="hljs-number">1</span>
    setosa = features.NumPy()[setosa_mask]
    non_setosa = features.NumPy()[~setosa_mask]
    plt.scatter(setosa[:,<span class="hljs-number">0</span>], setosa[:,<span class="hljs-number">1</span>], c='red', label='setosa')
    plt.scatter(non_setosa[:,<span class="hljs-number">0</span>], non_setosa[:,<span class="hljs-number">1</span>], c='blue', label='Non-setosa')
    <span class="hljs-keyword">if</span> j==<span class="hljs-number">0</span>:
        plt.legend(loc='lower right')
<span class="hljs-comment"># Computing and plotting the decision function</span>
a = -coefficients[<span class="hljs-number">0</span>] / coefficients[<span class="hljs-number">1</span>]
xx = np.linspace(plt.xlim()[<span class="hljs-number">0</span>], plt.xlim()[<span class="hljs-number">1</span>], num=<span class="hljs-number">10000</span>)
yy = a * xx - intercept / coefficients[<span class="hljs-number">1</span>]
on_the_plot = (yy &gt; plt.ylim()[<span class="hljs-number">0</span>]) &amp; (yy &lt; plt.ylim()[<span class="hljs-number">1</span>])
plt.plot(xx[on_the_plot], yy[on_the_plot], 'k--')
plt.xlabel('Petal Length') 
plt.ylabel('Petal Width') 
plt.show() 
</code></pre>
    <p class="normal">The resultant graph is in the <em class="italic">How it works...</em> section, where we also discuss the validity and reproducibility of the obtained results.</p>
    <h2 id="_idParaDest-85" class="title">How it works...</h2>
    <p class="normal">Our goal was to fit a line between the Iris setosa points and the other two species using only petal width and petal length. If we plot the points, and separate the area of the plot where classifications are zero from the area where classifications are one with a line, we see that we have achieved this:</p>
    <figure class="mediaobject"><img src="../Images/B16254_02_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 2.9: Plot of Iris setosa and non-setosa for petal width versus petal length; the solid line is the linear separator that we achieved after 300 iterations</p>
    <p class="normal">The way the separating line <a id="_idIndexMarker136"/>is defined depends on the data, the <a id="_idIndexMarker137"/>network architecture, and the learning process. Different starting situations, even due to the random initialization of the neural network's weights, may provide you with a slightly different solution. </p>
    <h2 id="_idParaDest-86" class="title">There's more...</h2>
    <p class="normal">While we achieved our objective of separating the two classes with a line, it may not be the best model for separating two classes. For instance, after adding new observations, we may realize that our solution badly separates the two classes. As we progress into the next chapter, we will start dealing with recipes that address these problems by providing testing, randomization, and specialized layers that will increase the generalization capabilities of our recipes.</p>
    <h2 id="_idParaDest-87" class="title">See also</h2>
    <ul>
      <li class="bullet">For information about <a id="_idIndexMarker138"/>the Iris dataset, see the documentation at <a href="https://archive.ics.uci.edu/ml/datasets/iris"><span class="url">https://archive.ics.uci.edu/ml/datasets/iris</span></a>.</li>
      <li class="bullet">If you want to understand more about decision boundaries drawing for machine learning algorithms, we warmly suggest this excellent Medium article from Navoneel Chakrabarty: <a href="https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d"><span class="url">https://towardsdatascience.com/decision-boundary-visualization-a-z-6a63ae9cca7d</span></a></li>
    </ul>
  </div>
</body></html>