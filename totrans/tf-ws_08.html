<html><head></head><body>
		<div>
			<div id="_idContainer300" class="Content">
			</div>
		</div>
		<div id="_idContainer301" class="Content">
			<h1 id="_idParaDest-153"><a id="_idTextAnchor176"/>8. Pre-Trained Networks</h1>
		</div>
		<div id="_idContainer309" class="Content">
			<p class="callout-heading"><a id="_idTextAnchor177"/>Overview</p>
			<p class="callout">In this chapter, you will analyze pre-trained models. You will get hands-on experience using the different state-of-the-art model architectures available on TensorFlow. You will explore concepts such as transfer learning and fine-tuning and look at TensorFlow Hub and its published deep learning resources.</p>
			<p class="callout">By the end of the chapter, you will be able to use pre-trained models directly from TensorFlow and TensorFlow Hub. </p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor178"/>Introduction</h1>
			<p>In the previous chapter, you learned how <strong class="bold">convolution neural networks</strong> (<strong class="bold">CNNs</strong>) analyze images and learn relevant patterns to classify their main subjects or identify objects within them. You also saw the different types of layers used for such models.</p>
			<p>But rather than training a model from scratch, it would be more efficient if you could reuse existing models with pre-calculated weights. This is exactly what <strong class="bold">transfer learning</strong> and <strong class="bold">fine-tuning</strong> are about. You will learn how to apply these techniques to your own projects and datasets in this chapter.</p>
			<p>You will also look at the ImageNet competition and the corresponding dataset that is used by deep learning researchers to benchmark their models against state-of-the-art algorithms. Finally, you will learn how to use TensorFlow Hub's resources to build your own model.</p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor179"/>ImageNet</h1>
			<p>ImageNet is a large dataset containing more than 14 million images annotated for image classification or object detection. It was first consolidated by Fei-Fei Li and her team in 2007. The goal was to build a dataset that computer vision researchers could benefit from.</p>
			<p>The dataset was presented for the first time in 2009, and every year since 2010, an annual competition called the <strong class="bold">ImageNet Large-Scale Visual Recognition Challenge</strong> (<strong class="bold">ILSVRC</strong>) has been organized for image classification and object detection tasks.</p>
			<div>
				<div id="_idContainer302" class="IMG---Figure">
					<img src="image/B16341_08_01.jpg" alt="Figure 8.1: Examples of images from ImageNet&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1: Examples of images from ImageNet</p>
			<p>Over the years, some of the most famous CNN architectures (such as AlexNet, Inception, VGG, and ResNet) have achieved amazing results in this ILSVRC competition. In the following graph, you can see how some of the most famous CNN architectures performed in this competition. In less than 10 years, performance increased from 50% accuracy to almost 90%.</p>
			<div>
				<div id="_idContainer303" class="IMG---Figure">
					<img src="image/B16341_08_02.jpg" alt="Figure 8.2: Model benchmarking from paperswithcode.com&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.2: Model benchmarking from paperswithcode.com</p>
			<p>You will see in the next section how you can use transfer learning with these models.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor180"/>Transfer Learning</h1>
			<p>In the previous chapter, you got hands-on practice training different CNN models for image classification purposes. Even though you achieved good results, the models took quite some time to learn the relevant parameters. If you kept training the models, you could have achieved even better results. Using <strong class="bold">graphical processing units</strong> (<strong class="bold">GPUs</strong>) can shorten the training time, but it will still take a bit of time, especially for bigger or more complex datasets.</p>
			<p>Deep learning researchers have published their work for the benefit of the community. Everyone can benefit by taking existing model architectures and customizing them, rather than designing architectures from scratch. More than this though, researchers also share the weights of their models. You can then not only reuse an architecture but also leverage all the training performed on it. This is what transfer learning is about. By reusing pre-trained models, you don't have to start from scratch. These models are trained on a large dataset such as ImageNet and have learned how to recognize thousands of different categories of objects. You can reuse these state-of-the-art models straight out of the box without having to train them. Isn't that amazing? Rather than training a model for weeks, you can now just use an existing model.</p>
			<p>TensorFlow provides a list of state-of-the-art models pre-trained on the ImageNet dataset for transfer learning in its Keras API.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the full list of pre-trained models available in TensorFlow at the following link: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications">https://www.tensorflow.org/api_docs/python/tf/keras/applications</a>.</p>
			<p>Importing a pre-trained model is quite simple in TensorFlow, as shown with the following example, where you load the <strong class="source-inline">InceptionV3</strong> model:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.applications import InceptionV3</p>
			<p>Now that you have imported the class for the pre-trained model, you need to instantiate it by specifying the dimensions of the input image and <strong class="source-inline">imagenet</strong> as the pre-trained weights to be loaded:</p>
			<p class="source-code">model = InceptionV3(input_shape=(224, 224, 3), \</p>
			<p class="source-code">                    weights='imagenet', include_top=True)</p>
			<p>The <strong class="source-inline">include_top=True</strong> parameter specifies that you will be re-using the exact same top layer (which is the final layer) as for the original model trained on ImageNet. This means that the last layer is designed to predict the 1,000 classes that are in this dataset.</p>
			<p>Now that you have instantiated your pre-trained model, you can make predictions from it:</p>
			<p class="source-code">model.predict(input_image)</p>
			<p>If you want to use this pre-trained model to predict different categories than the ones from ImageNet, you will need to replace the top layer with another one that will be trained to recognize the specific categories of the input dataset.</p>
			<p>First, you need to remove this layer by specifying <strong class="source-inline">include_top=False</strong>:</p>
			<p class="source-code">model = InceptionV3(input_shape=(224, 224, 3), \</p>
			<p class="source-code">                    weights='imagenet', include_top=False)</p>
			<p>In the preceding example, you have loaded an <strong class="source-inline">InceptionV3</strong> model. The next step will be to <em class="italic">freeze</em> all the layers from this model so that their weights will not be updated:</p>
			<p class="source-code">model.trainable = False</p>
			<p>After this, you will instantiate a new fully connected layer with the number of units and activation function of your choice. In the following example, you want to predict 50 different classes. To do this, you create a dense layer with <strong class="source-inline">20</strong> units and use softmax as the activation function:</p>
			<p class="source-code">top_layer = tf.keras.layers.Dense(20, activation='softmax')</p>
			<p>Then you need to add this fully connected layer to your base model with the Sequential API from Keras:</p>
			<p class="source-code">new_model = tf.keras.Sequential([model, top_layer])</p>
			<p>Now, you can train this model and only the top-layer weights will be updated. All the other layers have been frozen:</p>
			<p class="source-code">new_model.compile(loss='sparse_categorical_crossentropy', \</p>
			<p class="source-code">                  optimizer=tf.keras.optimizers.Adam(0.001))</p>
			<p class="source-code">new_model.fit(X_train, t_train, epochs=50)</p>
			<p>In just a few lines of code, you have loaded the Inception V3 model, which is a state-of-the-art model that won the ILSVRC competition in 2016. You learned how to adapt it to your own project and dataset.</p>
			<p>In the next exercise, you will have hands-on practice on transfer learning.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor181"/>Exercise 8.01: Classifying Cats and Dogs with Transfer Learning</h2>
			<p>In this exercise, you will use transfer learning to correctly classify images as either cats or dogs. You will use a pre-trained model, NASNet-Mobile, that is already available in TensorFlow. This model comes with pre-trained weights on ImageNet. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The original dataset used in this exercise has been provided by Google. It contains 25,000 images of dogs and cats. It can be found here: <a href="https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip">https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip</a>.</p>
			<ol>
				<li>Open a new Jupyter notebook.</li>
				<li>Import the TensorFlow library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> containing a link to the dataset:<p class="source-code">file_url = 'https://storage.googleapis.com'\</p><p class="source-code">          '/mledu-datasets/cats_and_dogs_filtered.zip'</p></li>
				<li>Download the dataset using <strong class="source-inline">tf.keras.get_file</strong>, with <strong class="source-inline">'cats_and_dogs.zip'</strong>, <strong class="source-inline">origin=file_url</strong>, and <strong class="source-inline">extract=True</strong> as parameters, and save the result to a variable called <strong class="source-inline">zip_dir</strong>:<p class="source-code">zip_dir = tf.keras.utils.get_file('cats_and_dogs.zip', \</p><p class="source-code">                                  origin=file_url, extract=True)</p></li>
				<li>Import the <strong class="source-inline">pathlib</strong> library:<p class="source-code">import pathlib</p></li>
				<li>Create a variable called <strong class="source-inline">path</strong> containing the full path to the <strong class="source-inline">cats_and_dogs_filtered</strong> directory using <strong class="source-inline">pathlib.Path(zip_dir).parent</strong>:<p class="source-code">path = pathlib.Path(zip_dir).parent / 'cats_and_dogs_filtered'</p></li>
				<li>Create two variables called <strong class="source-inline">train_dir</strong> and <strong class="source-inline">validation_dir</strong> that take the full path to the <strong class="source-inline">train</strong> and <strong class="source-inline">validation</strong> folders, respectively:<p class="source-code">train_dir = path / 'train'</p><p class="source-code">validation_dir = path / 'validation'</p></li>
				<li>Create four variables called <strong class="source-inline">train_cats_dir</strong>, <strong class="source-inline">train_dogs_dir</strong>, <strong class="source-inline">validation_cats_dir</strong>, and <strong class="source-inline">validation_dogs_dir</strong> that take the full path to the <strong class="source-inline">cats</strong> and <strong class="source-inline">dogs</strong> folders for the train and validation sets, respectively:<p class="source-code">train_cats_dir = train_dir / 'cats'</p><p class="source-code">train_dogs_dir = train_dir /'dogs'</p><p class="source-code">validation_cats_dir = validation_dir / 'cats'</p><p class="source-code">validation_dogs_dir = validation_dir / 'dogs'</p></li>
				<li>Import the <strong class="source-inline">os</strong> package. In the next step, you will need to count the number of images from a folder:<p class="source-code">import os</p></li>
				<li>Create two variables called <strong class="source-inline">total_train</strong> and <strong class="source-inline">total_val</strong> that get the number of images for the training and validation sets:<p class="source-code">total_train = len(os.listdir(train_cats_dir)) \</p><p class="source-code">              + len(os.listdir(train_dogs_dir))</p><p class="source-code">total_val = len(os.listdir(validation_cats_dir)) \</p><p class="source-code">            + len(os.listdir(validation_dogs_dir))</p></li>
				<li>Import <strong class="source-inline">ImageDataGenerator</strong> from <strong class="source-inline">tensorflow.keras.preprocessing</strong>:<p class="source-code">from tensorflow.keras.preprocessing.image</p><p class="source-code">    import ImageDataGenerator</p></li>
				<li>Instantiate two <strong class="source-inline">ImageDataGenerator</strong> classes and call them <strong class="source-inline">train_image_generator</strong> and <strong class="source-inline">validation_image_generator</strong>. These will rescale images by dividing by <strong class="source-inline">255</strong>:<p class="source-code">train_image_generator = ImageDataGenerator(rescale=1./255)</p><p class="source-code">validation_image_generator = ImageDataGenerator(rescale=1./255)</p></li>
				<li>Create three variables called <strong class="source-inline">batch_size</strong>, <strong class="source-inline">img_height</strong>, and <strong class="source-inline">img_width</strong> that take the values <strong class="source-inline">16</strong>, <strong class="source-inline">224</strong>, and <strong class="source-inline">224</strong>, respectively:<p class="source-code">batch_size = 16</p><p class="source-code">img_height = 224</p><p class="source-code">img_width = 224</p></li>
				<li>Create a data generator called <strong class="source-inline">train_data_gen</strong> using <strong class="source-inline">flow_from_directory()</strong> method, and specify the batch size, the path to the training folder, the size of the target, and the mode of the class:<p class="source-code">train_data_gen = train_image_generator.flow_from_directory\</p><p class="source-code">                 (batch_size = batch_size, \</p><p class="source-code">                  directory = train_dir, \</p><p class="source-code">                  shuffle=True, \</p><p class="source-code">                  target_size = (img_height, img_width), \</p><p class="source-code">                  class_mode='binary')</p></li>
				<li>Create a data generator called <strong class="source-inline">val_data_gen</strong> using <strong class="source-inline">flow_from_directory()</strong> method and specify the batch size, the path to the validation folder, the size of the target, and the mode of the class:<p class="source-code">val_data_gen = validation_image_generator.flow_from_directory\</p><p class="source-code">               (batch_size = batch_size, \</p><p class="source-code">                directory = validation_dir, \</p><p class="source-code">                target_size=(img_height, img_width), \</p><p class="source-code">                class_mode='binary')</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> (this is totally arbitrary) as <strong class="source-inline">seed</strong> for NumPy and TensorFlow:<p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Import the <strong class="source-inline">NASNETMobile</strong> model from <strong class="source-inline">tensorflow.keras.applications</strong>:<p class="source-code">from tensorflow.keras.applications import NASNetMobile</p></li>
				<li>Instantiate the model with the ImageNet weights, remove the top layer, and specify the correct input dimensions:<p class="source-code">base_model = NASNetMobile(include_top=False, \</p><p class="source-code">                          input_shape=(img_height, img_width, 3),\</p><p class="source-code">                          weights='imagenet')</p></li>
				<li>Freeze all the layers of this model:<p class="source-code">base_model.trainable = False</p></li>
				<li>Print a summary of the model using the <strong class="source-inline">summary()</strong> method:<p class="source-code">base_model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer304" class="IMG---Figure"><img src="image/B16341_08_03.jpg" alt="Figure 8.3: Summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 8.3: Summary of the model</p></li>
				<li>Create a new model that combines the <strong class="source-inline">NASNETMobile</strong> model with two new top layers with <strong class="source-inline">500</strong> and <strong class="source-inline">1</strong> unit(s) and ReLu and sigmoid as the activation functions:<p class="source-code">model = tf.keras.Sequential([base_model,\</p><p class="source-code">                             layers.Flatten(),</p><p class="source-code">                             layers.Dense(500, \</p><p class="source-code">                                          activation='relu'),</p><p class="source-code">                             layers.Dense(1, \</p><p class="source-code">                                          activation='sigmoid')])</p></li>
				<li>Compile the model by providing <strong class="source-inline">binary_crossentropy</strong> as the <strong class="source-inline">loss</strong> function, an Adam optimizer with a learning rate of <strong class="source-inline">0.001</strong>, and <strong class="source-inline">accuracy</strong> as the metric to be displayed:<p class="source-code">model.compile(loss='binary_crossentropy', \</p><p class="source-code">              optimizer=tf.keras.optimizers.Adam(0.001), \</p><p class="source-code">              metrics=['accuracy'])</p></li>
				<li>Fit the model, provide the train and validation data generators, and run it for five epochs:<p class="source-code">model.fit(train_data_gen, \</p><p class="source-code">          steps_per_epoch = total_train // batch_size, \</p><p class="source-code">          epochs=5, \</p><p class="source-code">          validation_data = val_data_gen, \</p><p class="source-code">          validation_steps = total_val // batch_size)</p><p>The expected output is as follows:</p><div id="_idContainer305" class="IMG---Figure"><img src="image/B16341_08_04.jpg" alt="Figure 8.4: Model training output&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.4: Model training output</p>
			<p>You can observe that the model achieved an accuracy score of <strong class="source-inline">0.99</strong> on the training set and <strong class="source-inline">0.98</strong> on the validation set. This is quite a remarkable result given that you only trained the last two layers, and it took less than a minute. This is the benefit of applying transfer learning and using pre-trained state-of-the-art models.</p>
			<p>In the next section, you will see how you can apply fine-tuning to a pre-trained model.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor182"/>Fine-Tuning</h1>
			<p>Previously, you used transfer learning to leverage pre-trained models on your own dataset. You used the weights of state-of-the-art models that have been trained on large datasets such as ImageNet. These models learned the relevant parameters to recognize different patterns from images and helped you to achieve amazing results on different datasets.</p>
			<p>But there is a catch with this approach. Transfer learning works well in general if the classes you are trying to predict belong to the same list as that of ImageNet. If this is the case, the weight learned from ImageNet will also be relevant to your dataset. For example, the <strong class="source-inline">cats</strong> and <strong class="source-inline">dogs</strong> classes from the preceding exercise are present in ImageNet, so its weights will also be relevant for this dataset.</p>
			<p>However, if your dataset is very different from ImageNet, then the weights from these pre-trained models may not all be relevant. For example, if your dataset contains satellite images, and you are trying to determine whether a house has solar panels installed on its roof, this will be very different compared to ImageNet. The weights from the last layers will be very specific to the classes from ImageNet, such as cat whiskers or car wheels (which are not very useful for the satellite image dataset case), while the ones from earlier layers will be more generic, such as for detecting shapes, colors, or texture (which can be applied to the satellite image dataset). </p>
			<p>So, it will be great to still leverage some of the weights from earlier layers but train the final layers so that your models can learn the specific patterns relevant to your dataset and improve its performance. </p>
			<p>This technique is called fine-tuning. The idea behind it is quite simple: you freeze early layers and update the weights of the final layers only. Let's see how you can achieve this in TensorFlow:</p>
			<ol>
				<li value="1">First, instantiate a pre-trained <strong class="source-inline">MobileNetV2</strong> model without the top layer:<p class="source-code">from tensorflow.keras.applications import MobileNetV2</p><p class="source-code">base_model = MobileNetV2(input_shape=(224, 224, 3), \</p><p class="source-code">                         weights='imagenet', include_top=False)</p></li>
				<li>Next, iterate through the first layers and freeze them by setting them as non-trainable. In the following example, you will freeze only the first <strong class="source-inline">100</strong> layers:<p class="source-code">for layer in base_model.layers[:100]:</p><p class="source-code">    layer.trainable = False</p></li>
				<li>Now you need to add your custom top layer to your base model. In the following example, you will be predicting 20 different classes, so you need to add a fully connected layer of <strong class="source-inline">20</strong> units with the softmax activation function:<p class="source-code">prediction_layer = tf.keras.layers.Dense(20, activation='softmax')</p><p class="source-code">model = tf.keras.Sequential([base_model, prediction_layer])</p></li>
				<li>Finally, you will compile and then train this model:<p class="source-code">model.compile(loss='sparse_categorical_crossentropy', \</p><p class="source-code">              optimizer = tf.keras.optimizers.Adam(0.001))</p><p class="source-code">model.fit(features_train, label_train, epochs=5)</p><p>This will display a number of logs, as seen in the following screenshot:</p><div id="_idContainer306" class="IMG---Figure"><img src="image/B16341_08_05.jpg" alt="Figure 8.5: Fine-tuning results on a pre-trained MobileNetV2 model&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.5: Fine-tuning results on a pre-trained MobileNetV2 model</p>
			<p>That's it. You have just performed fine-tuning on a pre-trained MobileNetV2 model. You have used the first 100 pre-trained weights from ImageNet and only updated the weights from layer 100 onward according to your dataset. </p>
			<p>In the next activity, you will put into practice what you have just learned and apply fine-tuning to a pre-trained model.</p>
			<h2 id="_idParaDest-159">Activ<a id="_idTextAnchor183"/>ity 8.01: Fruit Classification with Fine-Tuning</h2>
			<p>The <strong class="source-inline">Fruits 360</strong> dataset (<a href="https://arxiv.org/abs/1712.00580">https://arxiv.org/abs/1712.00580</a>), which was originally shared by <em class="italic">Horea Muresan and Mihai Oltean, Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018</em>, contains more than 82,000 images of 120 different types of fruit. You will be using a subset of this dataset with more than 16,000 images. The numbers of images in the training and validation sets are <strong class="source-inline">11398</strong> and <strong class="source-inline">4752</strong> respectively.</p>
			<p>In this activity, you are tasked with training a <strong class="source-inline">NASNetMobile</strong> model to recognize images of different varieties of fruits (classification into 120 different classes). You will use fine-tuning to train the final layers of this model. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be found here: <a href="http://packt.link/OFUJj">http://packt.link/OFUJj</a>.</p>
			<p>The following steps will help you to complete this activity:</p>
			<ol>
				<li value="1">Import the dataset and unzip the file using TensorFlow.</li>
				<li>Create a data generator with the following data augmentation:<p class="source-code">Rescale = 1./255, </p><p class="source-code">rotation_range = 40, </p><p class="source-code">width_shift_range = 0.1, </p><p class="source-code">height_shift_range = 0.1, </p><p class="source-code">shear_range = 0.2, </p><p class="source-code">zoom_range = 0.2, </p><p class="source-code">horizontal_flip = True, </p><p class="source-code">fill_mode = 'nearest</p></li>
				<li>Load a pre-trained <strong class="source-inline">NASNetMobile</strong> model from TensorFlow.</li>
				<li>Freeze the first <strong class="source-inline">600</strong> layers of the model.</li>
				<li>Add two fully connected layers on top of <strong class="source-inline">NASNetMobile</strong>:<p>– A fully connected layer with <strong class="source-inline">Dense(1000, activation=relu)</strong></p><p>– A fully connected layer with <strong class="source-inline">Dense(120, activation='softmax')</strong></p></li>
				<li>Specify an Adam optimizer with a learning rate of <strong class="source-inline">0.001</strong>.</li>
				<li>Train the model.</li>
				<li>Evaluate the model on the test set.<p>The expected output is as follows:</p><div id="_idContainer307" class="IMG---Figure"><img src="image/B16341_08_06.jpg" alt="Figure 8.6: Expected output of the activity&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.6: Expected output of the activity</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor274">this link</a>. </p>
			<p>Now that you know how to use pre-trained models from TensorFlow, you will learn how models can be accessed from TensorFlow Hub in the following section.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor184"/>TensorFlow Hub</h1>
			<p>TensorFlow Hub is a repository of TensorFlow modules shared by publishers such as Google, NVIDIA, and Kaggle. TensorFlow modules are self-contained models built on TensorFlow that can be reused for different tasks. Put simply, it is an external collection of published TensorFlow modules for transfer learning and fine-tuning. With TensorFlow Hub, you can access different deep learning models or weights than the ones provided directly from TensorFlow's core API.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find more information about TensorFlow Hub here: <a href="https://tfhub.dev/">https://tfhub.dev/</a>.</p>
			<p>In order to use it, you first need to install it:</p>
			<p class="source-code">pip install tensorflow-hub</p>
			<p>Once it's installed, you can load available classification models with the <strong class="source-inline">load()</strong> method by specifying the link to a module:</p>
			<p class="source-code">import tensorflow_hub as hub</p>
			<p class="source-code">MODULE_HANDLE = 'https://tfhub.dev/tensorflow/efficientnet'\</p>
			<p class="source-code">                '/b0/classification/1'</p>
			<p class="source-code">module = hub.load(MODULE_HANDLE)</p>
			<p>In the preceding example, you have loaded the <strong class="bold">EfficientNet B0</strong> model, which was trained on ImageNet. You can find more details on this at the TensorFlow Hub page: <a href="https://tfhub.dev/tensorflow/efficientnet/b0/classification/1">https://tfhub.dev/tensorflow/efficientnet/b0/classification/1</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">TensorFlow Hub provides a search engine to find a specific module: <a href="https://tfhub.dev/s?subtype=module,placeholder">https://tfhub.dev/s?subtype=module,placeholder</a>.</p>
			<p>By default, modules loaded from TensorFlow Hub contain the final layer of a model without an activation function. For classification purposes, you need to add an activation layer of your choice. To do so, you can use the Sequential API from Keras. You just need to convert your model into a Keras layer with the <strong class="source-inline">KerasLayer</strong> class:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">model = tf.keras.Sequential([</p>
			<p class="source-code">    hub.KerasLayer(MODULE_HANDLE,input_shape=(224, 224, 3)),</p>
			<p class="source-code">    tf.keras.layers.Activation('softmax')</p>
			<p class="source-code">])</p>
			<p>Then, you can use your final model to perform predictions:</p>
			<p class="source-code">model.predict(data)</p>
			<p>You just performed transfer learning with a model from TensorFlow Hub. This is very similar to what you learned previously using the Keras API, where you loaded an entire model with <strong class="source-inline">include_top=True</strong>. With TensorFlow Hub, you can access a library of pre-trained models for object detection or image segmentation.</p>
			<p>In the next section, you will learn how to extract features from TensorFlow Hub pre-trained modules.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor185"/>Feature Extraction</h1>
			<p>TensorFlow Hub provides the option of downloading a model without the final layer. In this case, you will be using a TensorFlow module as a feature extractor; you can design your custom final layers on top of it. In TensorFlow Hub, a module used for feature extraction is known as a feature vector:</p>
			<p class="source-code">import tensorflow_hub as hub</p>
			<p class="source-code">MODULE_HANDLE = 'https://tfhub.dev/google/efficientnet/b0'\</p>
			<p class="source-code">                '/feature-vector/1'</p>
			<p class="source-code">module = hub.load(MODULE_HANDLE)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To find all the available feature vectors on TensorFlow Hub, you can use its search engine: <a href="https://tfhub.dev/s?module-type=image-feature-vector&amp;tf-version=tf2">https://tfhub.dev/s?module-type=image-feature-vector&amp;tf-version=tf2</a>.</p>
			<p>Once loaded, you can add your own final layer to the feature vector with the Sequential API:</p>
			<p class="source-code">model = tf.keras.Sequential([</p>
			<p class="source-code">    hub.KerasLayer(MODULE_HANDLE, input_shape=(224, 224, 3)),</p>
			<p class="source-code">    tf.keras.layers.Dense(20, activation='softmax')</p>
			<p class="source-code">])</p>
			<p>In the preceding example, you added a fully connected layer of <strong class="source-inline">20</strong> units with the softmax activation function. Next, you need to compile and train your model:</p>
			<p class="source-code">model.compile(optimizer=optimizer, \</p>
			<p class="source-code">              loss='sparse_categorical_crossentropy', \</p>
			<p class="source-code">              metrics=['accuracy'])</p>
			<p class="source-code">model.fit(X_train, epochs=5)</p>
			<p>And with that, you just used a feature vector from TensorFlow Hub and added your custom final layer to train the final model on your dataset.</p>
			<p>Now, test the knowledge you have gained so far in the next activity.</p>
			<h2 id="_idParaDest-162">Activi<a id="_idTextAnchor186"/>ty 8.02: Transfer Learning with TensorFlow Hub</h2>
			<p>In this activity, you are required to correctly classify images of cats and dogs using transfer learning. Rather than training a model from scratch, you will benefit from the <strong class="bold">EfficientNet B0</strong> feature vector from TensorFlow Hub, which contains pre-computed weights that can recognize different types of objects.</p>
			<p>You can find the dataset here: <a href="https://packt.link/RAAtm">https://packt.link/RAAtm</a>.</p>
			<p>The following steps will help you to complete this activity:</p>
			<ol>
				<li value="1">Import the dataset and unzip the file using TensorFlow.</li>
				<li>Create a data generator that will perform rescaling.</li>
				<li>Load a pre-trained <strong class="bold">EfficientNet B0</strong> feature vector from TensorFlow Hub.</li>
				<li>Add two fully connected layers on top of the feature vector:<p>– A fully connected layer with <strong class="source-inline">Dense(500, activation=relu)</strong></p><p>– A fully connected layer with <strong class="source-inline">Dense(1, activation='sigmoid')</strong></p></li>
				<li>Specify an Adam optimizer with a learning rate of <strong class="source-inline">0.001</strong>.</li>
				<li>Train the model.</li>
				<li>Evaluate the model on the test set.<p>The expected output is as follows:</p><div id="_idContainer308" class="IMG---Figure"><img src="image/B16341_08_07.jpg" alt="Figure 8.7: Expected output of the activity &#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 8.7: Expected output of the activity </p>
			<p>The expected accuracy scores should be around <strong class="source-inline">1.0</strong> for the training and validation sets. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution to this activity can be found via <a href="B16341_Solution_ePub.xhtml#_idTextAnchor277">this link</a>.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor187"/>Summary</h1>
			<p>In this chapter, you learned two very important concepts: transfer learning and fine-tuning. Both help deep learning practitioners to leverage existing pre-trained models and adapt them to their own projects and datasets. </p>
			<p>Transfer learning is the re-use of models that have been trained on large datasets such as ImageNet (which contains more than 14 million images). TensorFlow provides a list of such pre-trained models in its core API. You can also access other models from renowned publishers such as Google and NVIDIA through TensorFlow Hub.</p>
			<p>Finally, you got some hands-on practice fine-tuning a pre-trained model. You learned how to freeze the early layers of a model and only train the last layers according to the specificities of the input dataset.</p>
			<p>These two techniques were a major breakthrough for the community as they facilitated access to state-of-the-art models for anyone interested in applying deep learning models.</p>
			<p>In the next chapter, you will look at another type of model architecture, <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>). This type of architecture is well suited for sequential data such as time series or text.</p>
		</div>
	</body></html>