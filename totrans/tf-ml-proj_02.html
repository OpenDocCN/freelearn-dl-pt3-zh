<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Using Machine Learning to Detect Exoplanets in Outer Space</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we shall learn how to detect exoplanets in outer space using ensemble methods that are based on decision trees.</p>
<p class="mce-root">Decision trees are a family of non-parametric supervised learning methods. In a decision tree algorithm, the data is divided into two partitions by using a simple rule. The rule is applied again and again to further partition the data, thus forming a tree of decisions.</p>
<p class="mce-root">Ensemble methods combine the learning from multiple learning algorithms to improve predictions and reduce errors. These ensembles are differentiated on the basis of what kind of learners they use and how they structure those learns in the ensemble.</p>
<p class="mce-root">The two most popular ensemble methods based on decision trees are known as gradient boosted trees and random forests. </p>
<p class="mce-root">The following topics will be covered in this chapter:</p>
<ul>
<li>What is a decision tree?</li>
<li>Why we need ensembles?</li>
<li>Decision tree-based ensemble methods
<ul>
<li>Random forests</li>
<li>Gradient boosting</li>
</ul>
</li>
<li>Decision tree-based ensembles in TensorFlow</li>
<li>Building a TensorFlow boosted tree model for exoplanet detection</li>
</ul>
<div class="packt_infobox"><span>The code from this chapter is available in Jupyter Notebook as</span> <kbd>ch-02_Detecting_Explonaets_in_Outer_Space.ipynb</kbd> in the code bundle.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">What is a decision tree?</h1>
                </header>
            
            <article>
                
<p><span>Decision trees are a family of non-parametric supervised learning methods. In the decision tree algorithm, we start with the complete dataset and split it into two partitions based on a simple rule. The splitting continues until a specified criterion is met. The nodes at which the split is made are called interior nodes and the final endpoints are called terminal or leaf nodes.</span></p>
<p>As an example, let us look at the following tree:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1084 image-border" src="assets/3bba725c-c1a9-4574-bec6-5b2b904469b8.png" style="width:35.83em;height:17.50em;"/></p>
<p>Here, we are assuming that the exoplanet data has only two properties: <strong>flux.1</strong> and <strong>flux.2</strong>. First, we make a decision if <strong>flux.1 &gt; 400</strong> and then divide the data into two partitions. Then we divide the data again based on <strong>flux.2</strong> feature, and that division decides whether the planet is an exoplanet or not. How did we decide that condition <strong>flux.1 &gt; 400</strong>? We did not. This was just to<span> </span>demonstrate a decision tree. During the training phase, that's what the model learns – the parameters of conditions that divide the data into partitions.</p>
<p>For classification problems, the decision tree has leaf nodes that shows the result as the discrete classification of the data and for regression problems, the leaf nodes show the results as a predicted number. Decision trees, thus, are also <span>popularly</span><span> </span>known as <strong>Classification and Regression Trees</strong> (<strong>CART</strong>).</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Why do we need ensembles?</h1>
                </header>
            
            <article>
                
<p>Decision trees are prone to overfitting training data and suffer from high variance, thus, providing poor predictions from new unseen data. However, using an ensemble of<span> </span>decision<span> </span>trees helps alleviate the shortcoming of using a single decision tree model. In an ensemble, many weak learners come together to create a strong learner.</p>
<p>Among the many ways that we can combine decision trees to make ensembles, the two methods that have been popular due to their performance for predictive modeling are:</p>
<ul>
<li>Gradient boosting (also known as gradient tree boosting)</li>
<li>Random decision trees (also known as random forests)</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision tree-based ensemble methods</h1>
                </header>
            
            <article>
                
<p><span>In this section let us </span>explore<span> briefly two kinds of ensemble methods for decision trees: random forests and gradient boosting.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Random forests</h1>
                </header>
            
            <article>
                
<p>Random forests is a technique where you construct multiple trees, and then use those trees to learn the classification and regression models, but the results are aggregated from the trees to produce<span> </span>a final<span> </span>resu<span>lt.</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1081 image-border" src="assets/1ba7f1c9-51bf-4f83-b967-7041953cbb9c.png" style="width:32.75em;height:21.83em;"/></p>
<p>Random forests are an<span> </span>ensemble<span> </span>of random, uncorrelated, and fully-grown decision trees. The decision trees used in the random forest model are fully grown, thus, having low bias and high variance. The trees are uncorrelated in nature, which results in a maximum decrease in the variance. <span>By uncorrelated, we imply that each decision tree in the random forest is given a randomly selected subset of features and a randomly selected subset of the dataset for the selected features.</span></p>
<div class="packt_tip packt_infobox"><span>The original paper describing random forests is available at the following link: </span><a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf</a><span>.</span></div>
<p>The random forest technique does not reduce bias and as a result, has a slightly higher bias as compared to the individual trees in the ensemble. </p>
<div class="packt_infobox">Random forests were invented by Leo Breiman and have been trademarked by Leo Breiman and Adele Cutler. More information is available at the following link: <a href="https://www.stat.berkeley.edu/~breiman/RandomForests">https://www.stat.berkeley.edu/~breiman/RandomForests</a>.</div>
<p>Intuitively, in the random forest model, a large number of decision trees are trained on different samples of data, that either fit or overfit. By averaging the individual decision trees, overfitting cancels out. </p>
<div class="packt_tip">Random forests seem similar to bagging, aka bootstrap aggregating, but they are different. In bagging, a random sample with replacement is selected to train every tree in the ensemble. The tree is trained on all the features. In random forests, the features are also sampled randomly, and at each candidate that is split, a subset of features is used to train the model.</div>
<p><span>For predicting values in case of regression problems, the random forest model averages the predictions from individual decision trees. For predicting classes in case of a classification problem, the random forest model takes a majority vote from the results of individual decision trees.</span></p>
<div class="packt_infobox"><span>An interesting explanation of random forests can be found at the following link: </span><a href="https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/">https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/</a></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Gradient boosting</h1>
                </header>
            
            <article>
                
<p>Gradient boosted trees are an ensemble of shallow trees (or weak learners). The shallow decision trees could be as small as a tree with just two leaves (also known as decision stump). The boosting methods help in reducing bias mainly but also help reduce variance slightly.</p>
<div class="packt_infobox">Original papers by Breiman and Friedman who developed the idea of gradient boosting are available at following links:<br/>
<ul>
<li><span><em>Prediction Games and Arcing Algorithms</em> by Breiman, L at <a href="https://www.stat.berkeley.edu/~breiman/games.pdf">https://www.stat.berkeley.edu/~breiman/games.pdf</a></span></li>
<li><em>Arcing The Edge </em>by Breiman, L <span>at <a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf">http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf</a></span></li>
<li><em>Greedy Function Approximation: A Gradient Boosting Machine</em> by <span>Friedman, J. H. at <a href="http://statweb.stanford.edu/~jhf/ftp/trebst.pdf">http://statweb.stanford.edu/~jhf/ftp/trebst.pdf</a></span></li>
<li><em>Stochastic Gradient Boosting</em><span> </span><span>by </span><span>Friedman, J. H. at <a href="https://statweb.stanford.edu/~jhf/ftp/stobst.pdf">https://statweb.stanford.edu/~jhf/ftp/stobst.pdf</a></span></li>
</ul>
</div>
<p><span>Intuitively, in the gradient boosting model, the decision trees in the ensemble are trained in several iterations as shown in the following image. A new decision tree is added at each iteration. Every additional decision tree is trained to improve the trained ensemble model in previous iterations. This is different from the random forest model where each decision tree is trained independently from the other decision trees in the ensemble.</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1082 image-border" src="assets/1a5d0ce4-b378-4921-8df5-c88b5ae2297b.png" style="width:41.25em;height:48.00em;"/></p>
<p><span>The gradient boosting model has lesser number of trees as compared to the random forests model but ends up with a very large number of hyperparameters that need to be tuned to get a decent gradient boosting model.</span></p>
<p class="mce-root"/>
<div class="packt_infobox">An interesting explanation of gradient boosting can be found at the following link: <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/</a>.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Decision tree-based ensembles in TensorFlow</h1>
                </header>
            
            <article>
                
<p>In this chapter, we shall<span> </span>use<span> </span>the gradient boosted trees and random forest implementation as pre-made estimators in TensorFlow from the Google<span> </span>TensorFlow<span> </span>team. Let us learn the details of their implementation in the upcoming sections.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorForest Estimator</h1>
                </header>
            
            <article>
                
<p>TensorForest is a highly scalable implementation of random forests built by combining a variety of online HoeffdingTree algorithms with the extremely randomized approach.</p>
<div class="packt_infobox">Google published the details of the TensorForest implementation in the following paper: <em>TensorForest: Scalable Random Forests on TensorFlow</em><span> </span>by Thomas Colthurst, D. Sculley, Gibert Hendry, Zack Nado, presented at Machine Learning Systems Workshop at the Conference on <strong>Neural Information Processing Systems</strong> (<strong>NIPS</strong>) 2016. The paper is available at the following link: <a href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE">https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE</a>.</div>
<p>TensorForest estimators are used to implementing the following algorithm:</p>
<pre>Initialize the variables and sets<br/>    Tree = [root]<br/>    Fertile = {root}<br/>    Stats(root) = 0<br/>    Splits[root] = []<br/><br/>Divide training data into batches.<br/>For each batch of training data:<br/>    Compute leaf assignment <img class="fm-editor-equation" src="assets/f8ced87d-0906-47b2-b9af-7fb4f556547c.png" style="width:1.00em;height:1.58em;"/> for each feature vector<br/>    Update the leaf stats in Stats(<img class="fm-editor-equation" src="assets/e240ea2e-ce40-4c70-9b3c-803b2bb028b2.png" style="width:1.00em;height:1.58em;"/>)<br/>    For each <img class="fm-editor-equation" src="assets/9d38a814-b781-4b16-837d-054f436edcf7.png" style="width:1.00em;height:1.58em;"/> in Fertile set:<br/>        if |Splits(<img class="fm-editor-equation" src="assets/27e5a3d0-9ee8-4f79-bd13-39f845927beb.png" style="width:1.00em;height:1.58em;"/>)| &lt; max_splits<br/>            then add the split on a randomly selected feature to Splits(<img class="fm-editor-equation" src="assets/c36eef7e-be58-4a0f-9364-addd933bd805.png" style="width:1.00em;height:1.58em;"/>)<br/>        else if <img class="fm-editor-equation" src="assets/a88de7f4-cbf0-4dc5-9df4-d82bac5f280f.png" style="width:1.00em;height:1.58em;"/> is fertile and |Splits(<img class="fm-editor-equation" src="assets/bf75de10-dc11-4996-832a-a86e2ce4807e.png" style="width:1.00em;height:1.58em;"/>)| = max_splits<br/>            then update the split stats for <img class="fm-editor-equation" src="assets/909857df-1545-4e3d-936b-bdc1a01b0cb6.png" style="width:1.00em;height:1.58em;"/><br/>    Calculate the fertile leaves that are finished. <br/>    For every non-stale finished leaf:<br/>        turn the leaf into an internal node with its best scoring split <br/>        remove the leaf from Fertile<br/>        add the leaf's two children to Tree as leaves<br/>    If |Fertile| &lt; max_fertile<br/>        Then add the max_fertile − |Fertile| leaves with <br/>        the highest weighted leaf scores to Fertile and <br/>        initialize their Splits and split statistics. <br/>Until |Tree| = max_nodes or |Tree| stays the same for max_batches_to_grow batches </pre>
<p>More details of this algorithm implementation can be found in the TensorForest paper.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">TensorFlow boosted trees estimator</h1>
                </header>
            
            <article>
                
<p><strong>TensorFlow Boosted Trees</strong> (<strong>TFBT</strong>) is an improved scalable ensemble model built on top of generic gradient boosting trees. </p>
<div class="packt_infobox"><span>Google published the details of the TensorFlow boosted trees implementation in the following paper: <em>A scalable TensorFlow based framework for gradient boosting</em> by Natalia Ponomareva, Soroush Radpour, Gilbert Hendry, Salem Haykal, Thomas Colthurst, Petr Mitrichev, Alexander Grushetsky, presented at the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD) 2017</span><span>. The paper is available at the following link: <a href="http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf">http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf</a><a href="http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf">.</a></span></div>
<p>The gradient boosting algorithm is implemented by various libraries such as <kbd>sklearn</kbd>, <kbd>MLLib</kbd>, and <kbd>XGBoost</kbd>. TensorFlow's implementation is different from these implementations as described in the following table extracted from the TFBT research paper:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-1083 image-border" src="assets/bce85b46-1f54-4c08-a435-80f5f5acee49.png" style="width:59.42em;height:33.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">TFBT Research Paper from Google</div>
<p>The TFBT model can be extended by writing custom loss functions in TensorFlow. The differentiation for these custom loss functions is automatically provided by TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detecting exoplanets in outer space</h1>
                </header>
            
            <article>
                
<p>For the project<span> </span>explained<span> </span>in this chapter, we use the <em>Kepler labeled time series data</em> from Kaggle:<a href="https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data/home"> https://www.kaggle.com/keplersmachines/kepler-labelled-time-series-data/home</a>. This dataset is derived mainly from the Campaign 3 observations of the mission by NASA's Kepler space telescope.</p>
<p>In the dataset, column 1 values are the labels and columns 2 to 3198 values are the flux values over time. The training set has 5087 data points, 37 confirmed exoplanets, and 5050 non-exoplanet stars. The test set has 570 data points, 5 confirmed exoplanets, and 565 non-exoplanet stars.</p>
<p>We will carry out the following steps to download, and then preprocess our data to create the train and test datasets: </p>
<ol>
<li>Download the dataset using the Kaggle API. The following code will be used for the same:</li>
</ol>
<pre style="padding-left: 60px">armando@librenix:~/datasets/kaggle-kepler$ kaggle datasets download -d keplersmachines/kepler-labelled-time-series-data<br/><br/>Downloading kepler-labelled-time-series-data.zip to /mnt/disk1tb/datasets/kaggle-kepler<br/>100%|██████████████████████████████████████| 57.4M/57.4M [00:03&lt;00:00, 18.3MB/s]</pre>
<p style="padding-left: 60px">The folder contains the following two files:</p>
<pre style="padding-left: 60px">exoTest.csv
exoTrain.csv</pre>
<ol start="2">
<li>Link the folder<span> </span><kbd>datasets</kbd><span> </span>to our home folder so we can access it from the <kbd>~/datasets/kaggle-kepler</kbd> path and then we define the folder path and list the contents of the folder through the Notebook to confirm if we have access to the data files through the Notebook:</li>
</ol>
<pre style="padding-left: 60px">dsroot = os.path.join(os.path.expanduser(<span>'~'</span>),<span>'datasets'</span>,<span>'kaggle-kepler'</span>)<br/>os.listdir(dsroot)</pre>
<p style="padding-left: 60px">We get the following output:</p>
<pre style="padding-left: 60px">[<span>'exoTest.csv'</span>, <span>'kepler-labelled-time-series-data.zip'</span>, <span>'exoTrain.csv'</span>]</pre>
<div class="packt_infobox">The ZIP file is just a leftover of the download process because the Kaggle API begins by downloading the ZIP file and then proceeds to unzip the contents in the same folder.</div>
<ol start="3">
<li>We will then read the two<span> </span><kbd>.csv</kbd><span> </span>data files in the <kbd>pandas</kbd> DataFrames named<span> </span><kbd>train</kbd><span> </span>and<span> </span><kbd>test</kbd><span> </span>respectively:</li>
</ol>
<pre style="padding-left: 60px"><span>import </span>pandas <span>as </span>pd<br/>train = pd.read_csv(os.path.join(dsroot,<span>'exoTrain.csv'</span>))<br/>test = pd.read_csv(os.path.join(dsroot,<span>'exoTest.csv'</span>))<br/><span>print</span>(<span>'Training data</span><span>\n</span><span>'</span>,train.head())<br/><span>print</span>(<span>'Test data</span><span>\n</span><span>'</span>,test.head())</pre>
<p>The first five lines of the <kbd>training</kbd> and <kbd>test data</kbd> look similar to the following:</p>
<pre>Training data<br/>    LABEL   FLUX.1   FLUX.2   FLUX.3  \
0      2    93.85    83.81    20.10     
1      2   -38.88   -33.83   -58.54   
2      2   532.64   535.92   513.73    
3      2   326.52   347.39   302.35    
4      2 -1107.21 -1112.59 -1118.95 <br/>     FLUX.4   FLUX.5   FLUX.6  FLUX.7  \
0    -26.98   -39.56  -124.71 -135.18   
1    -40.09   -79.31   -72.81  -86.55   
2    496.92   456.45   466.00  464.50   
3    298.13   317.74   312.70  322.33   
4  -1095.10 -1057.55 -1034.48 -998.34   
<br/>    FLUX.8  FLUX.9    ...      FLUX.3188  \
0   -96.27  -79.89    ...         -78.07      
1   -85.33  -83.97    ...          -3.28   
2   486.39  436.56    ...         -71.69  
3   311.31  312.42    ...           5.71      
4 -1022.71 -989.57    ...        -594.37    <br/><br/>    FLUX.3189  FLUX.3190  FLUX.3191  \
0     -102.15    -102.15      25.13   
1      -32.21     -32.21     -24.89   
2       13.31      13.31     -29.89   
3       -3.73      -3.73      30.05   
4     -401.66    -401.66    -357.24  <br/><br/>   FLUX.3192  FLUX.3193  FLUX.3194  
0      48.57      92.54      39.32  
1      -4.86       0.76     -11.70     
2     -20.88       5.06     -11.80    
3      20.03     -12.67      -8.77      
4    -443.76    -438.54    -399.71   

   FLUX.3195  FLUX.3196  FLUX.3197  
0      61.42       5.08     -39.54  
1       6.46      16.00      19.93  
2     -28.91     -70.02     -96.67  
3     -17.31     -17.35      13.98  
4    -384.65    -411.79    -510.54  

[5 rows x 3198 columns]<br/><br/>Test data<br/><br/>    LABEL   FLUX.1   FLUX.2   FLUX.3   \
0      2   119.88   100.21    86.46      
1      2  5736.59  5699.98  5717.16    
2      2   844.48   817.49   770.07    
3      2  -826.00  -827.31  -846.12    
4      2   -39.57   -15.88    -9.16      
<br/>       FLUX.4   FLUX.5   FLUX.6   FLUX.7  \
0       48.68    46.12    39.39    18.57   
1     5692.73  5663.83  5631.16  5626.39   
2      675.01   605.52   499.45   440.77   
3     -836.03  -745.50  -784.69  -791.22   
4       -6.37   -16.13   -24.05    -0.90   

    FLUX.8   FLUX.9    ...      FLUX.3188  \
0     6.98     6.63    ...          14.52       
1  5569.47  5550.44    ...        -581.91    
2   362.95   207.27    ...          17.82     
3  -746.50  -709.53    ...         122.34       
4   -45.20    -5.04    ...         -37.87     <br/>    FLUX.3189  FLUX.3190  FLUX.3191  \
0       19.29      14.44      -1.62   
1     -984.09   -1230.89   -1600.45   
2      -51.66     -48.29     -59.99   
3       93.03      93.03      68.81   
4      -61.85     -27.15     -21.18     

   FLUX.3192  FLUX.3193  FLUX.3194  \  
0      13.33      45.50      31.93    
1   -1824.53   -2061.17   -2265.98     
2     -82.10    -174.54     -95.23      
3       9.81      20.75      20.25    
4     -33.76     -85.34     -81.46    
<br/>   FLUX.3195  FLUX.3196  FLUX.3197  
0      35.78     269.43      57.72  
1   -2366.19   -2294.86   -2034.72  
2    -162.68     -36.79      30.63  
3    -120.81    -257.56    -215.41  
4     -61.98     -69.34     -17.84  <br/><br/>[5 rows x 3198 columns]</pre>
<p>The training and test datasets have labels in the first column and 3197 features in the next columns. Now let us split the training and test data into labels and features with the following code:</p>
<pre>x_train = train.drop(<span>'LABEL'</span>, <span>axis</span>=<span>1</span>)<br/>y_train = train.LABEL-<span>1 </span><span>#subtract one because of TGBT<br/></span>x_test = test.drop(<span>'LABEL'</span>, <span>axis</span>=<span>1</span>)<br/>y_test = test.LABEL-<span>1</span></pre>
<p>In the preceding code, we subtract <kbd>1</kbd> from the labels, since the TFBT estimator assumes labels starting with numerical zero while the features in the datasets are numbers 1 and 2.</p>
<p>Now that we have the label and feature vectors for training and test data, let us build the boosted tree models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a TFBT model for exoplanet detection</h1>
                </header>
            
            <article>
                
<p>In this section, we shall<span> </span>build<span> </span>the gradient boosted trees model for detecting exoplanets using the Kepler dataset. Let us follow these steps in the Jupyter Notebook to build and train the exoplanet finder model:</p>
<ol>
<li><span>We will s</span>ave the names of all the features in a vector with the following code:</li>
</ol>
<pre style="padding-left: 60px">numeric_column_headers = x_train.columns.values.tolist()</pre>
<ol start="2">
<li>We will then bucketize the feature columns into two buckets around the mean since the TFBT estimator only takes bucketed features with the following code:</li>
</ol>
<pre style="padding-left: 60px">bc_fn = tf.feature_column.bucketized_column<br/>nc_fn = tf.feature_column.numeric_column<br/>bucketized_features = [bc_fn(<span>source_column</span>=nc_fn(<span>key</span>=column),<br/>                             <span>boundaries</span>=[x_train[column].mean()])<br/>                       <span>for </span>column <span>in </span>numeric_column_headers]</pre>
<ol start="3">
<li>Since we only have numeric bucketized features and no other kinds of features, we store them in the <kbd>all_features</kbd><span> </span>variable with the following code:</li>
</ol>
<pre style="padding-left: 60px">all_features = bucketized_features</pre>
<ol start="4">
<li>We will then define the batch size and create a function that will provide inputs from the label and feature vectors created from the training data. For creating this function we use a convenience function<span> </span><kbd>tf.estimator.inputs.pandas_input_fn()</kbd><span> </span>provided by TensorFlow. We will use the following code:</li>
</ol>
<pre style="padding-left: 60px">batch_size = <span>32<br/></span>pi_fn = tf.estimator.inputs.pandas_input_fn<br/>train_input_fn = pi_fn(<span>x </span>= x_train,<br/>                       <span>y </span>= y_train,<br/>                       <span>batch_size </span>= batch_size,<br/>                       <span>shuffle </span>= <span>True</span>,<br/>                       <span>num_epochs </span>= <span>None</span>)</pre>
<ol start="5">
<li>Similarly, we will create another data input function that would be used to evaluate the model from the test features and label vectors and name it<span> </span><kbd>eval_input_fn</kbd> using the following code:</li>
</ol>
<pre style="padding-left: 60px">eval_input_fn = pi_fn(<span>x </span>= x_test,<br/>                      <span>y </span>= y_test,<br/>                      <span>batch_size </span>= batch_size,<br/>                      <span>shuffle </span>= <span>False</span>,<br/>                      <span>num_epochs </span>= <span>1</span>)</pre>
<ol start="6">
<li>We will define the number of trees to be created as <kbd>100</kbd> and the number of steps to be used for training as <kbd>100</kbd>. We also define the <kbd>BoostedTreeClassifier</kbd> as the <kbd>estimator</kbd> using the following code:</li>
</ol>
<pre style="padding-left: 60px">n_trees = <span>100<br/></span>n_steps = <span>100<br/></span><span><br/></span>m_fn = tf.estimator.BoostedTreesClassifier<br/>model = m_fn(<span>feature_columns</span>=all_features,<br/>             <span>n_trees </span>= n_trees,<br/>             <span>n_batches_per_layer </span>= batch_size,<br/>             <span>model_dir</span>=<span>'./tfbtmodel'</span>)</pre>
<div class="packt_tip">Since we are doing classification, hence we use the <kbd>BoostedTreesClassifier</kbd>, for regression problems where a value needs to be predicted, TensorFlow also has an <kbd>estimator</kbd> named <kbd>BoostedTreesRegressor</kbd>.</div>
<p style="padding-left: 60px">One of the parameters provided to the <kbd>estimator</kbd> function is<span> </span><kbd>model_dir</kbd><span> </span>that defines where the trained model would be stored. The estimators are built such that they look for the model in that folder in further invocations for using them for inference and prediction. We name the folder as <kbd>tfbtmodel</kbd><span> </span>to save the model.</p>
<div class="packt_infobox">We have used the minimum number of models to define the <kbd>BoostedTreesClassifier</kbd>. Please look up the definition of this estimator in the TensorFlow API documentation to find various other parameters that can be provided to further customize the estimator.</div>
<p style="padding-left: 60px">The following output in the Jupyter Notebook describes the classifier estimator and its various settings:</p>
<pre style="padding-left: 60px">INFO:tensorflow:Using default config.
INFO:tensorflow:Using config: {'_model_dir': './tfbtmodel', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': &lt;tensorflow.python.training.server_lib.ClusterSpec object at 0x7fdd48c93b38&gt;, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}</pre>
<ol start="7">
<li>Post this, we will train the model using the<span> </span><kbd>train_input_fn</kbd><span> function </span>that provides the exoplanets input data using 100 steps with the following code:</li>
</ol>
<pre style="padding-left: 60px">model.train(<span>input_fn</span>=train_input_fn, <span>steps</span>=n_steps)</pre>
<p style="padding-left: 60px">The Jupyter Notebook shows the following output to indicate the training in progress:</p>
<pre style="padding-left: 60px">INFO:tensorflow:Calling model_fn.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Create CheckpointSaverHook.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from ./tfbtmodel/model.ckpt-19201
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Saving checkpoints for 19201 into ./tfbtmodel/model.ckpt.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:loss = 1.0475121e-05, step = 19201
INFO:tensorflow:Saving checkpoints for 19202 into ./tfbtmodel/model.ckpt.
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Loss for final step: 1.0475121e-05.</pre>
<ol start="8">
<li>Use the<span> </span><kbd>eval_input_fn</kbd><span> </span>that provides batches from the <kbd>test</kbd> dataset to evaluate the model with the following code:</li>
</ol>
<pre style="padding-left: 60px">results = model.evaluate(<span>input_fn</span>=eval_input_fn)</pre>
<p style="padding-left: 60px">The Jupyter Notebook shows the following output as the progress of the evaluation:</p>
<pre style="padding-left: 60px">INFO:tensorflow:Calling model_fn.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to "careful_interpolation" instead.
WARNING:tensorflow:Trapezoidal rule is known to produce incorrect PR-AUCs; please switch to "careful_interpolation" instead.
INFO:tensorflow:Done calling model_fn.
INFO:tensorflow:Starting evaluation at 2018-09-07-04:23:31
INFO:tensorflow:Graph was finalized.
INFO:tensorflow:Restoring parameters from ./tfbtmodel/model.ckpt-19203
INFO:tensorflow:Running local_init_op.
INFO:tensorflow:Done running local_init_op.
INFO:tensorflow:Finished evaluation at 2018-09-07-04:23:50
INFO:tensorflow:Saving dict for global step 19203: accuracy = 0.99122804, accuracy_baseline = 0.99122804, auc = 0.49911517, auc_precision_recall = 0.004386465, average_loss = 0.09851996, global_step = 19203, label/mean = 0.00877193, loss = 0.09749381, precision = 0.0, prediction/mean = 4.402521e-05, recall = 0.0
WARNING:tensorflow:Issue encountered when serializing resources.
Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.
'_Resource' object has no attribute 'name'
INFO:tensorflow:Saving 'checkpoint_path' summary for global step 19203: ./tfbtmodel/model.ckpt-19203</pre>
<p style="padding-left: 60px">Note that during the evaluation the estimator loads the parameters saved in the checkpoint file: </p>
<pre style="padding-left: 60px">INFO:tensorflow:Restoring parameters from ./tfbtmodel/model.ckpt-19203</pre>
<ol start="9">
<li>The results of the evaluation are stored in the <kbd>results</kbd> collection. Let us print each item in the <kbd>results</kbd> collection using the <kbd>for</kbd> loop in the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>for </span>key,value <span>in </span><span>sorted</span>(results.items()):<br/>    <span>print</span>(<span>'{}: {}'</span>.format(key, value))</pre>
<p>The Notebook shows the following results:</p>
<pre style="padding-left: 60px">accuracy: 0.9912280440330505
accuracy_baseline: 0.9912280440330505
auc: 0.4991151690483093
auc_precision_recall: 0.004386465065181255
average_loss: 0.0985199585556984
global_step: 19203
label/mean: 0.008771929889917374
loss: 0.09749381244182587
precision: 0.0
prediction/mean: 4.4025211536791176e-05
recall: 0.0</pre>
<p>It is observed that we<span> </span>achieve<span> </span>an accuracy of almost 99% with the first model itself. This is because the estimators are prewritten with several optimizations and we did not need to set various values of hyperparameters ourselves. For some datasets, the default hyperparameter values in the estimators will work out of the box, but for other datasets, you will have to play with various inputs to the estimators.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we learned what a decision tree<span> is</span> and two broad classes of creating ensembles from the decision trees. The ensembles we took a look at were random forests and gradient boosting trees.</p>
<p>We also learned about the Kepler dataset from Kaggle competitions. We used the Kepler dataset to build an exoplanet detection model using TensorFlow's prebuilt estimator for gradient boosting trees known as the <kbd>BoostedTreesClassifier</kbd>. The <kbd>BoostedTreesClassifier</kbd> estimator is part of the machine learning toolkit recently released by the TensorFlow team. As for now, the TensorFlow team is working on releasing prebuilt estimators based on <strong>support vector machine</strong> (<strong>SVM</strong>) and extreme random forests as part of the <kbd>tf.estimators</kbd> API.</p>
<p>In the next chapter, we shall learn how to use TensorFlow in the browser using the <kbd>TensorFlow.js</kbd> API for sentiment analysis.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ul>
<li>How is gradient boosting different from random forests?</li>
<li>How can you improve the performance of random forests?</li>
<li>How can you improve the performance of gradient boosting trees?</li>
<li>How to ensure that gradient boosting trees and random forests do not overfit?</li>
<li>Modify the model in this chapter with different parameters such as the number of trees, batch size, number of epochs and number of steps and observe their effect on training time and different levels of accuracy.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><a href="https://www.tensorflow.org/tutorials/">https://www.tensorflow.org/tutorials/</a></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/RandomForests">https://www.stat.berkeley.edu/~breiman/RandomForests</a></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf">https://www.stat.berkeley.edu/~breiman/randomforest2001.pdf</a></li>
<li><a href="https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/">https://machinelearning-blog.com/2018/02/06/the-random-forest-algorithm/</a></li>
<li><a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/">http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/</a> <a href="http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/"/></li>
<li><a href="https://www.stat.berkeley.edu/~breiman/games.pdf"><span>https://www.stat.berkeley.edu/~breiman/games.pdf</span></a></li>
<li><a href="http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf"><span>http://statistics.berkeley.edu/sites/default/files/tech-reports/486.pdf</span></a></li>
<li><a href="http://statweb.stanford.edu/~jhf/ftp/trebst.pdf"><span>http://statweb.stanford.edu/~jhf/ftp/trebst.pdf</span></a></li>
<li><a href="https://statweb.stanford.edu/~jhf/ftp/stobst.pdf"><span>https://statweb.stanford.edu/~jhf/ftp/stobst.pdf</span></a></li>
<li><a href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE">https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxtbHN5c25pcHMyMDE2fGd4OjFlNTRiOWU2OGM2YzA4MjE</a></li>
<li><a href="http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf">http://ecmlpkdd2017.ijs.si/papers/paperID705.pdf</a></li>
</ul>


            </article>

            
        </section>
    </body></html>